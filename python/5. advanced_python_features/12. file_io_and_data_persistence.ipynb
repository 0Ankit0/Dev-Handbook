{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: File I/O and Data Persistence\n",
    "\n",
    "Most applications must persist data beyond the runtime of the program, whether writing configuration files, processing log data, or serializing objects for network transmission. Python provides a rich ecosystem for file handling, from low-level byte streams to high-level path abstractions and structured data formats.\n",
    "\n",
    "This chapter examines the context management protocol that ensures resources are properly released, modern file handling techniques using `pathlib`, and serialization strategies for JSON, CSV, and binary formats. We emphasize contemporary best practices\u2014using `pathlib` over string manipulation, explicit encoding declarations, and understanding the security implications of deserialization.\n",
    "\n",
    "## 12.1 Context Managers: Resource Management with `with`\n",
    "\n",
    "Resource management is error-prone. Files left open after exceptions, locks held during crashes, and database connections leaking cause system instability. **Context managers** solve this through the `with` statement, guaranteeing cleanup code executes regardless of success or failure.\n",
    "\n",
    "### The Context Management Protocol\n",
    "\n",
    "Context managers implement two methods:\n",
    "*   `__enter__()`: Called when entering the `with` block; returns the resource\n",
    "*   `__exit__(exc_type, exc_val, exc_tb)`: Called when exiting; receives exception info if one occurred\n",
    "\n",
    "```python\n",
    "from typing import Optional, Type, Self\n",
    "import time\n",
    "\n",
    "class DatabaseConnection:\n",
    "    \"\"\"\n",
    "    Custom context manager demonstrating the protocol.\n",
    "    \n",
    "    Ensures connections are closed even if queries fail.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string: str, timeout: int = 30) -> None:\n",
    "        self.connection_string: str = connection_string\n",
    "        self.timeout: int = timeout\n",
    "        self.connection: Optional[object] = None\n",
    "        self.is_connected: bool = False\n",
    "    \n",
    "    def __enter__(self) -> Self:\n",
    "        \"\"\"Acquire resource and return it for use.\"\"\"\n",
    "        print(f\"Connecting to {self.connection_string}...\")\n",
    "        # Simulate connection establishment\n",
    "        self.connection = object()  # Stand-in for actual connection\n",
    "        self.is_connected = True\n",
    "        print(\"Connection established\")\n",
    "        return self  # This becomes the 'as' variable\n",
    "    \n",
    "    def __exit__(\n",
    "        self,\n",
    "        exc_type: Optional[Type[BaseException]],\n",
    "        exc_val: Optional[BaseException],\n",
    "        exc_tb: Optional[object]\n",
    "    ) -> Optional[bool]:\n",
    "        \"\"\"\n",
    "        Release resource and handle exceptions if needed.\n",
    "        \n",
    "        Args:\n",
    "            exc_type: Type of exception (if any)\n",
    "            exc_val: Exception instance (if any)\n",
    "            exc_tb: Traceback object (if any)\n",
    "            \n",
    "        Returns:\n",
    "            True if exception was handled, False/None to propagate\n",
    "        \"\"\"\n",
    "        if self.connection:\n",
    "            print(\"Closing connection...\")\n",
    "            self.connection = None\n",
    "            self.is_connected = False\n",
    "        \n",
    "        # Log exception if one occurred\n",
    "        if exc_type:\n",
    "            print(f\"Exception during context: {exc_val}\")\n",
    "            # Return False to propagate exception\n",
    "            return False\n",
    "        return True\n",
    "\n",
    "# Usage\n",
    "def query_database() -> None:\n",
    "    with DatabaseConnection(\"postgresql://localhost/mydb\") as conn:\n",
    "        print(f\"Executing query on {conn.connection_string}\")\n",
    "        # If exception occurs here, __exit__ still runs\n",
    "        raise RuntimeError(\"Query failed!\")  # Connection still closed\n",
    "\n",
    "# Output:\n",
    "# Connecting to postgresql://localhost/mydb...\n",
    "# Connection established\n",
    "# Executing query on postgresql://localhost/mydb\n",
    "# Closing connection...\n",
    "# Exception during context: Query failed!\n",
    "# (Exception propagates)\n",
    "```\n",
    "\n",
    "### The `contextlib` Module\n",
    "\n",
    "Writing classes for simple cleanup is verbose. The `contextlib` module provides utilities for creating context managers from functions.\n",
    "\n",
    "#### `@contextmanager` Decorator\n",
    "\n",
    "Transforms a generator function into a context manager:\n",
    "\n",
    "```python\n",
    "from contextlib import contextmanager\n",
    "from typing import Generator\n",
    "import tempfile\n",
    "import os\n",
    "\n",
    "@contextmanager\n",
    "def temporary_directory() -> Generator[str, None, None]:\n",
    "    \"\"\"\n",
    "    Create temporary directory that's cleaned up automatically.\n",
    "    \n",
    "    Yields the path, then cleanup runs after the block.\n",
    "    \"\"\"\n",
    "    temp_dir: str = tempfile.mkdtemp()\n",
    "    print(f\"Created temp directory: {temp_dir}\")\n",
    "    \n",
    "    try:\n",
    "        yield temp_dir  # Value bound to 'as' variable\n",
    "    finally:\n",
    "        # Always executes, even if exception occurred\n",
    "        import shutil\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Cleaned up: {temp_dir}\")\n",
    "\n",
    "# Usage\n",
    "with temporary_directory() as tmpdir:\n",
    "    path: str = os.path.join(tmpdir, \"data.txt\")\n",
    "    with open(path, 'w') as f:\n",
    "        f.write(\"Temporary data\")\n",
    "    # Directory and contents deleted automatically\n",
    "```\n",
    "\n",
    "#### `contextlib.closing`\n",
    "\n",
    "Ensures objects with `close()` method are properly closed:\n",
    "\n",
    "```python\n",
    "from contextlib import closing\n",
    "from urllib.request import urlopen\n",
    "\n",
    "def fetch_data(url: str) -> bytes:\n",
    "    \"\"\"Ensure connection closes even if parsing fails.\"\"\"\n",
    "    with closing(urlopen(url)) as response:\n",
    "        # response doesn't support context manager protocol directly\n",
    "        return response.read()\n",
    "```\n",
    "\n",
    "#### `contextlib.ExitStack`\n",
    "\n",
    "Manage multiple context managers dynamically, especially useful when the number of resources isn't known until runtime:\n",
    "\n",
    "```python\n",
    "from contextlib import ExitStack\n",
    "from typing import List\n",
    "\n",
    "def process_multiple_files(filenames: List[str]) -> None:\n",
    "    \"\"\"Open variable number of files safely.\"\"\"\n",
    "    with ExitStack() as stack:\n",
    "        files: List[object] = [\n",
    "            stack.enter_context(open(fname, 'r'))\n",
    "            for fname in filenames\n",
    "        ]\n",
    "        \n",
    "        # All files open; process them\n",
    "        for f in files:\n",
    "            process_line(f.readline())\n",
    "        \n",
    "        # All files closed automatically, even if exception occurs\n",
    "\n",
    "# Alternative: Nested with statements (clumsy for many files)\n",
    "# with open('a.txt') as f1:\n",
    "#     with open('b.txt') as f2:\n",
    "#         with open('c.txt') as f3:\n",
    "#             ...\n",
    "```\n",
    "\n",
    "#### `contextlib.suppress`\n",
    "\n",
    "Ignore specific exceptions cleanly:\n",
    "\n",
    "```python\n",
    "from contextlib import suppress\n",
    "import os\n",
    "\n",
    "def remove_if_exists(filepath: str) -> None:\n",
    "    \"\"\"Remove file without checking existence first.\"\"\"\n",
    "    # Instead of:\n",
    "    # if os.path.exists(filepath):\n",
    "    #     os.remove(filepath)\n",
    "    \n",
    "    # Cleaner approach:\n",
    "    with suppress(FileNotFoundError):\n",
    "        os.remove(filepath)\n",
    "```\n",
    "\n",
    "#### `contextlib.redirect_stdout/stderr`\n",
    "\n",
    "Capture or redirect output streams:\n",
    "\n",
    "```python\n",
    "from contextlib import redirect_stdout\n",
    "import io\n",
    "\n",
    "def capture_output(func) -> str:\n",
    "    \"\"\"Capture print statements from function.\"\"\"\n",
    "    buffer: io.StringIO = io.StringIO()\n",
    "    with redirect_stdout(buffer):\n",
    "        func()\n",
    "    return buffer.getvalue()\n",
    "```\n",
    "\n",
    "### Async Context Managers (Python 3.5+)\n",
    "\n",
    "For asynchronous resources, use `__aenter__` and `__aexit__`:\n",
    "\n",
    "```python\n",
    "from contextlib import asynccontextmanager\n",
    "from typing import AsyncGenerator\n",
    "\n",
    "class AsyncDatabase:\n",
    "    async def connect(self) -> None: ...\n",
    "    async def disconnect(self) -> None: ...\n",
    "\n",
    "@asynccontextmanager\n",
    "async def get_db_connection() -> AsyncGenerator[AsyncDatabase, None]:\n",
    "    \"\"\"Async context manager for database connections.\"\"\"\n",
    "    db: AsyncDatabase = AsyncDatabase()\n",
    "    await db.connect()\n",
    "    try:\n",
    "        yield db\n",
    "    finally:\n",
    "        await db.disconnect()\n",
    "\n",
    "# Usage\n",
    "async def query() -> None:\n",
    "    async with get_db_connection() as db:\n",
    "        await db.query(\"SELECT * FROM users\")\n",
    "```\n",
    "\n",
    "## 12.2 File Handling: Text and Binary Modes\n",
    "\n",
    "Python distinguishes between **text mode** (decodes bytes to strings using encoding) and **binary mode** (raw bytes). Understanding this distinction prevents common encoding errors and data corruption.\n",
    "\n",
    "### Opening Files\n",
    "\n",
    "```python\n",
    "from typing import TextIO, BinaryIO\n",
    "\n",
    "# Text mode (default) - returns str, uses encoding\n",
    "file_text: TextIO = open('data.txt', mode='r', encoding='utf-8')\n",
    "\n",
    "# Binary mode - returns bytes, no encoding\n",
    "file_binary: BinaryIO = open('image.png', mode='rb')\n",
    "\n",
    "# Context manager ensures closure\n",
    "with open('data.txt', 'r', encoding='utf-8') as f:\n",
    "    content: str = f.read()\n",
    "```\n",
    "\n",
    "**Mode Characters:**\n",
    "*   `r` - Read (default)\n",
    "*   `w` - Write (truncate if exists)\n",
    "*   `x` - Exclusive creation (fail if exists)\n",
    "*   `a` - Append (write to end)\n",
    "*   `b` - Binary mode\n",
    "*   `t` - Text mode (default)\n",
    "*   `+` - Read and write\n",
    "\n",
    "### Reading Strategies\n",
    "\n",
    "```python\n",
    "from typing import Iterator\n",
    "\n",
    "def read_strategies(filepath: str) -> None:\n",
    "    \"\"\"Demonstrate different reading approaches.\"\"\"\n",
    "    \n",
    "    # Strategy 1: Read entire file (small files only)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        content: str = f.read()\n",
    "        # Memory = size of file\n",
    "    \n",
    "    # Strategy 2: Read line by line (memory efficient)\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line in f:  # f is iterable, yields lines\n",
    "            process_line(line.strip())\n",
    "    \n",
    "    # Strategy 3: Read fixed chunks (binary processing)\n",
    "    with open(filepath, 'rb') as f:\n",
    "        while chunk := f.read(8192):  # 8KB chunks\n",
    "            process_chunk(chunk)\n",
    "    \n",
    "    # Strategy 4: Read all lines into list\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        lines: list[str] = f.readlines()  # Includes newlines\n",
    "    \n",
    "    # Strategy 5: Line iterator with enumerate\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        for line_num, line in enumerate(f, 1):\n",
    "            if 'ERROR' in line:\n",
    "                print(f\"Line {line_num}: {line.strip()}\")\n",
    "\n",
    "def process_line(line: str) -> None: ...\n",
    "def process_chunk(chunk: bytes) -> None: ...\n",
    "```\n",
    "\n",
    "**Best Practice:** Iterate over the file object directly (`for line in f`) rather than `f.readlines()`, as the former is memory-efficient (streams one line at a time) while the latter loads the entire file into memory.\n",
    "\n",
    "### Writing Files\n",
    "\n",
    "```python\n",
    "def write_data(filepath: str, data: list[dict]) -> None:\n",
    "    \"\"\"Write data with proper encoding and newline handling.\"\"\"\n",
    "    \n",
    "    # Text mode with explicit encoding (always specify encoding!)\n",
    "    with open(filepath, 'w', encoding='utf-8', newline='') as f:\n",
    "        # newline='' prevents universal newlines translation\n",
    "        # (important for CSV files)\n",
    "        \n",
    "        for item in data:\n",
    "            f.write(f\"{item['name']}: {item['value']}\\n\")\n",
    "    \n",
    "    # Binary mode (write bytes)\n",
    "    with open('output.bin', 'wb') as f:\n",
    "        f.write(b'\\x00\\x01\\x02\\x03')  # Raw bytes\n",
    "        \n",
    "        # Convert string to bytes\n",
    "        text: str = \"Hello\"\n",
    "        f.write(text.encode('utf-8'))\n",
    "\n",
    "    # Append mode\n",
    "    with open('log.txt', 'a', encoding='utf-8') as f:\n",
    "        f.write(f\"{datetime.now()}: Event occurred\\n\")\n",
    "```\n",
    "\n",
    "### File Positioning and Seeking\n",
    "\n",
    "```python\n",
    "def random_access(filepath: str) -> None:\n",
    "    \"\"\"Read specific portions of file.\"\"\"\n",
    "    with open(filepath, 'r+b') as f:  # Read + write binary\n",
    "        # Get current position\n",
    "        pos: int = f.tell()\n",
    "        \n",
    "        # Seek to specific position\n",
    "        f.seek(0)           # Beginning\n",
    "        f.seek(0, 2)        # End (0 offset from end)\n",
    "        f.seek(-10, 2)      # 10 bytes from end\n",
    "        f.seek(100)         # Absolute position\n",
    "        \n",
    "        # Read/write at position\n",
    "        f.seek(50)\n",
    "        data: bytes = f.read(10)\n",
    "        f.seek(50)\n",
    "        f.write(b'NEW_DATA')\n",
    "```\n",
    "\n",
    "### Memory-Mapped Files (Advanced)\n",
    "\n",
    "For large files requiring random access without loading entirely into memory:\n",
    "\n",
    "```python\n",
    "import mmap\n",
    "\n",
    "def process_large_file(filepath: str) -> None:\n",
    "    \"\"\"Memory-map file for efficient random access.\"\"\"\n",
    "    with open(filepath, 'r+b') as f:\n",
    "        # Memory-map entire file\n",
    "        with mmap.mmap(f.fileno(), 0, access=mmap.ACCESS_READ) as mm:\n",
    "            # mm behaves like bytes object\n",
    "            if mm.find(b'search_term') != -1:\n",
    "                print(\"Found!\")\n",
    "            \n",
    "            # Can slice without copying\n",
    "            header: bytes = mm[:100]\n",
    "```\n",
    "\n",
    "## 12.3 Modern Path Handling with `pathlib`\n",
    "\n",
    "The `pathlib` module (Python 3.4+) provides an object-oriented interface for filesystem paths, replacing the fragmented `os.path` functions with intuitive methods and proper operator overloading.\n",
    "\n",
    "### Path Types\n",
    "\n",
    "```python\n",
    "from pathlib import Path, PurePath, PurePosixPath, PureWindowsPath\n",
    "\n",
    "# Concrete path (interacts with actual filesystem)\n",
    "p: Path = Path('/usr/bin/python')  # Unix\n",
    "p: Path = Path(r'C:\\Users\\name')   # Windows (automatically handles)\n",
    "\n",
    "# Pure paths (path manipulation without filesystem access)\n",
    "pure: PurePath = PurePosixPath('/etc/hosts')\n",
    "```\n",
    "\n",
    "### Path Construction and Manipulation\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "\n",
    "# Creating paths\n",
    "current: Path = Path.cwd()          # Current working directory\n",
    "home: Path = Path.home()            # User's home directory\n",
    "file_path: Path = Path('data', 'subfolder', 'file.txt')  # Joining\n",
    "\n",
    "# Path joining with / operator (clean and intuitive)\n",
    "base: Path = Path('/home/user')\n",
    "config: Path = base / 'config' / 'app.ini'  # /home/user/config/app.ini\n",
    "\n",
    "# Path components\n",
    "path: Path = Path('/usr/local/bin/python3')\n",
    "print(path.name)        # python3 (filename)\n",
    "print(path.suffix)      # .3 (last extension)\n",
    "print(path.suffixes)    # [] if no extension, or list for multiple\n",
    "print(path.stem)        # python3 (filename without suffix)\n",
    "print(path.parent)      # /usr/local/bin (immediate parent)\n",
    "print(path.parents)     # Iterable of all ancestors\n",
    "print(path.parts)       # ('/', 'usr', 'local', 'bin', 'python3')\n",
    "print(path.anchor)      # / (root) or C:\\ on Windows\n",
    "```\n",
    "\n",
    "### File Operations\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "def path_operations() -> None:\n",
    "    \"\"\"Modern file operations with pathlib.\"\"\"\n",
    "    src: Path = Path('source.txt')\n",
    "    dst: Path = Path('backup', 'source.txt')\n",
    "    \n",
    "    # Existence and type checks\n",
    "    if src.exists():\n",
    "        print(\"File exists\")\n",
    "    if src.is_file():\n",
    "        print(\"Is a file\")\n",
    "    if src.is_dir():\n",
    "        print(\"Is directory\")\n",
    "    if src.is_symlink():\n",
    "        print(\"Is symlink\")\n",
    "    \n",
    "    # Metadata\n",
    "    size: int = src.stat().st_size      # Bytes\n",
    "    mtime: float = src.stat().st_mtime  # Modification time\n",
    "    \n",
    "    # Reading (convenient methods)\n",
    "    text: str = src.read_text(encoding='utf-8')\n",
    "    data: bytes = src.read_bytes()\n",
    "    \n",
    "    # Writing\n",
    "    dst.write_text(\"Content\", encoding='utf-8')\n",
    "    dst.write_bytes(b'\\x00\\x01')\n",
    "    \n",
    "    # Copying (using shutil with Path objects)\n",
    "    shutil.copy(src, dst)\n",
    "    \n",
    "    # Moving/Renaming\n",
    "    src.rename(dst)\n",
    "    src.replace(dst)  # Overwrites if exists\n",
    "    \n",
    "    # Deleting\n",
    "    src.unlink()      # Remove file (missing_ok=True to avoid errors)\n",
    "    src.unlink(missing_ok=True)\n",
    "    \n",
    "    # Create directory\n",
    "    dst.mkdir(parents=True, exist_ok=True)  # Like mkdir -p\n",
    "    \n",
    "    # Remove directory\n",
    "    dst.rmdir()       # Must be empty\n",
    "    shutil.rmtree(dst)  # Recursive delete\n",
    "\n",
    "def find_files(directory: Path) -> None:\n",
    "    \"\"\"Glob patterns for file discovery.\"\"\"\n",
    "    # Glob patterns\n",
    "    py_files: list[Path] = list(directory.glob('*.py'))  # Immediate children\n",
    "    all_py: list[Path] = list(directory.rglob('*.py'))   # Recursive\n",
    "    \n",
    "    # Pattern matching\n",
    "    for file in directory.iterdir():  # Like os.listdir but yields Paths\n",
    "        if file.match('test_*.py'):\n",
    "            print(file)\n",
    "    \n",
    "    # Specific glob with **\n",
    "    for log in directory.glob('**/*.log'):  # Recursive\n",
    "        process_log(log)\n",
    "```\n",
    "\n",
    "### Path Comparison and Normalization\n",
    "\n",
    "```python\n",
    "def path_equality() -> None:\n",
    "    \"\"\"Path comparison and resolution.\"\"\"\n",
    "    p1: Path = Path('/usr/bin')\n",
    "    p2: Path = Path('/usr') / 'bin'\n",
    "    \n",
    "    # Comparison\n",
    "    print(p1 == p2)  # True (compares normalized paths)\n",
    "    \n",
    "    # Absolute vs relative\n",
    "    relative: Path = Path('data/file.txt')\n",
    "    absolute: Path = relative.resolve()  # Resolve to absolute, follow symlinks\n",
    "    absolute: Path = relative.absolute() # Absolute without resolving\n",
    "    \n",
    "    # Normalization\n",
    "    messy: Path = Path('/usr//local/../bin/./python')\n",
    "    clean: Path = messy.resolve()  # /usr/bin/python\n",
    "    \n",
    "    # Relative paths\n",
    "    from_path: Path = Path('/home/user/projects')\n",
    "    to_path: Path = Path('/home/user/data/file.txt')\n",
    "    rel: Path = to_path.relative_to(from_path)  # ../data/file.txt\n",
    "```\n",
    "\n",
    "## 12.4 Serialization: JSON, CSV, and Binary Formats\n",
    "\n",
    "Serialization converts Python objects to formats storable or transmittable, with deserialization reconstructing them. Different formats suit different needs: human-readable vs. compact, schema-flexible vs. typed, secure vs. performant.\n",
    "\n",
    "### JSON (JavaScript Object Notation)\n",
    "\n",
    "JSON is the lingua franca of web APIs\u2014human-readable, language-independent, and widely supported.\n",
    "\n",
    "```python\n",
    "import json\n",
    "from typing import Any\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "class DateTimeEncoder(json.JSONEncoder):\n",
    "    \"\"\"Custom encoder for non-serializable types.\"\"\"\n",
    "    def default(self, obj: Any) -> Any:\n",
    "        if isinstance(obj, datetime):\n",
    "            return obj.isoformat()\n",
    "        if isinstance(obj, set):\n",
    "            return list(obj)\n",
    "        return super().default(obj)\n",
    "\n",
    "def json_operations() -> None:\n",
    "    \"\"\"JSON serialization and deserialization.\"\"\"\n",
    "    data: dict[str, Any] = {\n",
    "        'name': 'Alice',\n",
    "        'age': 30,\n",
    "        'scores': [85, 92, 78],\n",
    "        'active': True,\n",
    "        'created': datetime.now(),\n",
    "        'tags': {'python', 'developer'}  # Set\n",
    "    }\n",
    "    \n",
    "    # Serialize to string\n",
    "    json_str: str = json.dumps(data, cls=DateTimeEncoder, indent=2)\n",
    "    print(json_str)\n",
    "    \n",
    "    # Serialize to file\n",
    "    with open('data.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(data, f, cls=DateTimeEncoder, indent=2)\n",
    "    \n",
    "    # Deserialize from string\n",
    "    parsed: dict[str, Any] = json.loads(json_str)\n",
    "    \n",
    "    # Deserialize from file\n",
    "    with open('data.json', 'r', encoding='utf-8') as f:\n",
    "        loaded: dict[str, Any] = json.load(f)\n",
    "    \n",
    "    # Compact format (no whitespace)\n",
    "    compact: str = json.dumps(data, separators=(',', ':'), cls=DateTimeEncoder)\n",
    "\n",
    "def load_json_with_types(filepath: Path) -> dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Load JSON with type hints (requires validation in production).\n",
    "    \n",
    "    Note: json module returns basic Python types.\n",
    "    For strict typing, use pydantic or marshmallow.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "```\n",
    "\n",
    "**JSON Type Mappings:**\n",
    "| Python | JSON |\n",
    "|--------|------|\n",
    "| dict | object |\n",
    "| list, tuple | array |\n",
    "| str | string |\n",
    "| int, float | number |\n",
    "| True | true |\n",
    "| False | false |\n",
    "| None | null |\n",
    "\n",
    "**Security Note:** `json` is safe to use with untrusted data (unlike `pickle`). It doesn't execute arbitrary code during deserialization.\n",
    "\n",
    "### CSV (Comma-Separated Values)\n",
    "\n",
    "CSV remains ubiquitous for tabular data exchange despite its lack of standardization.\n",
    "\n",
    "```python\n",
    "import csv\n",
    "from pathlib import Path\n",
    "from typing import Iterator, Dict, List\n",
    "\n",
    "def csv_reading(filepath: Path) -> None:\n",
    "    \"\"\"Read CSV files efficiently.\"\"\"\n",
    "    \n",
    "    # Method 1: DictReader (access by column name)\n",
    "    with open(filepath, 'r', newline='', encoding='utf-8') as f:\n",
    "        reader: csv.DictReader = csv.DictReader(f)\n",
    "        for row in reader:  # Each row is OrderedDict or dict\n",
    "            print(f\"{row['name']}: {row['email']}\")\n",
    "    \n",
    "    # Method 2: Standard reader (access by index)\n",
    "    with open(filepath, 'r', newline='', encoding='utf-8') as f:\n",
    "        reader: csv.reader = csv.reader(f)\n",
    "        header: list[str] = next(reader)  # Skip header\n",
    "        for row in reader:\n",
    "            name, email, age = row[0], row[1], row[2]\n",
    "    \n",
    "    # Method 3: Reading with type conversion\n",
    "    def read_users(filepath: Path) -> Iterator[Dict[str, Any]]:\n",
    "        with open(filepath, 'r', newline='', encoding='utf-8') as f:\n",
    "            reader = csv.DictReader(f)\n",
    "            for row in reader:\n",
    "                yield {\n",
    "                    'name': row['name'],\n",
    "                    'age': int(row['age']),\n",
    "                    'active': row['active'].lower() == 'true'\n",
    "                }\n",
    "\n",
    "def csv_writing(data: List[Dict[str, Any]], filepath: Path) -> None:\n",
    "    \"\"\"Write data to CSV.\"\"\"\n",
    "    if not data:\n",
    "        return\n",
    "    \n",
    "    fieldnames: list[str] = list(data[0].keys())\n",
    "    \n",
    "    with open(filepath, 'w', newline='', encoding='utf-8') as f:\n",
    "        writer: csv.DictWriter = csv.DictWriter(\n",
    "            f, \n",
    "            fieldnames=fieldnames,\n",
    "            extrasaction='ignore',  # Ignore extra keys in dict\n",
    "            quoting=csv.QUOTE_MINIMAL\n",
    "        )\n",
    "        writer.writeheader()\n",
    "        writer.writerows(data)\n",
    "        \n",
    "        # Or write row by row\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "def csv_advanced() -> None:\n",
    "    \"\"\"Handle dialects and custom formatting.\"\"\"\n",
    "    # Custom dialect for weird formats\n",
    "    csv.register_dialect('unix', delimiter=' ', quoting=csv.QUOTE_NONE)\n",
    "    \n",
    "    with open('data.txt', 'r') as f:\n",
    "        reader = csv.reader(f, dialect='unix')\n",
    "```\n",
    "\n",
    "**Critical CSV Parameter:** Always use `newline=''` when opening CSV files to prevent blank line issues on Windows.\n",
    "\n",
    "### Pickle: Python-Specific Serialization\n",
    "\n",
    "**Warning:** Only unpickle data you trust. Pickle can execute arbitrary code during deserialization.\n",
    "\n",
    "```python\n",
    "import pickle\n",
    "from pathlib import Path\n",
    "from typing import Any\n",
    "\n",
    "def pickle_operations() -> None:\n",
    "    \"\"\"\n",
    "    Serialize Python objects to binary format.\n",
    "    \n",
    "    WARNING: Never unpickle untrusted data. Pickle is not secure.\n",
    "    \"\"\"\n",
    "    data: dict[str, Any] = {\n",
    "        'complex': 3 + 4j,\n",
    "        'function': lambda x: x**2,  # Can serialize lambdas!\n",
    "        'nested': {'a': [1, 2, 3]}\n",
    "    }\n",
    "    \n",
    "    # Serialize to bytes\n",
    "    pickled: bytes = pickle.dumps(data, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Serialize to file\n",
    "    with open('data.pkl', 'wb') as f:\n",
    "        pickle.dump(data, f, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "    \n",
    "    # Deserialize\n",
    "    with open('data.pkl', 'rb') as f:\n",
    "        loaded: Any = pickle.load(f)  # Security risk if file is tampered!\n",
    "    \n",
    "    # Safer alternative for simple data: use json or messagepack\n",
    "```\n",
    "\n",
    "**Pickle Protocols:**\n",
    "*   Protocol 0: ASCII, backward compatible\n",
    "*   Protocol 1: Binary, backward compatible\n",
    "*   Protocol 2: Python 2.3+\n",
    "*   Protocol 3: Python 3.0+ (explicit bytes support)\n",
    "*   Protocol 4: Python 3.4+ (large objects, framing)\n",
    "*   Protocol 5: Python 3.8+ (out-of-band buffers)\n",
    "\n",
    "**When to Use Pickle:**\n",
    "*   Caching Python objects between program runs (trusted source)\n",
    "*   Multiprocessing (sending objects between processes)\n",
    "*   Machine learning model serialization (with caution)\n",
    "\n",
    "**Never Use Pickle For:**\n",
    "*   Data from untrusted sources (network, user uploads)\n",
    "*   Long-term storage (protocol changes break compatibility)\n",
    "*   Cross-language communication\n",
    "\n",
    "### Alternative Serialization\n",
    "\n",
    "```python\n",
    "# MessagePack (binary, cross-language, faster than JSON)\n",
    "import msgpack\n",
    "\n",
    "data: bytes = msgpack.packb({'key': 'value'})\n",
    "obj: Any = msgpack.unpackb(data)\n",
    "\n",
    "# YAML (human-readable, supports comments)\n",
    "import yaml\n",
    "\n",
    "with open('config.yaml', 'r') as f:\n",
    "    config: dict = yaml.safe_load(f)  # Use safe_load, not load!\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "Persistent data management requires both technical precision and security awareness. You have mastered the **context manager protocol** (`__enter__` and `__exit__`), enabling robust resource management that guarantees cleanup even during exception handling. The `with` statement eliminates resource leaks, while `contextlib` utilities like `@contextmanager`, `ExitStack`, and `suppress` reduce boilerplate for common patterns.\n",
    "\n",
    "You understand the distinction between **text and binary modes**, the critical importance of explicit encoding declarations (UTF-8), and memory-efficient strategies for processing large files through iteration and chunking. **Pathlib** replaces archaic `os.path` string manipulation with an object-oriented interface where the `/` operator intuitively joins path components, and methods like `read_text()`, `write_bytes()`, and `glob()` streamline filesystem interactions.\n",
    "\n",
    "For data serialization, **JSON** provides universal interoperability for simple data structures, while **CSV** handles tabular exchange despite its format ambiguities. You recognize that **pickle**, while powerful for Python-specific serialization, carries severe security risks when processing untrusted data\u2014a vulnerability that has led to remote code execution exploits in production systems.\n",
    "\n",
    "However, file I/O is often the bottleneck in application performance. In the next chapter, we explore concurrency and parallelism\u2014techniques to perform multiple operations simultaneously, from threading for I/O-bound tasks to multiprocessing for CPU-bound workloads and the modern `asyncio` framework for high-performance asynchronous programming.\n",
    "\n",
    "**Next Chapter**: Chapter 13: Concurrency and Parallelism (Threading, Multiprocessing, and Asyncio)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='11. iterators_generators_and_decorators.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='13. concurrency_and_parallelism.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}