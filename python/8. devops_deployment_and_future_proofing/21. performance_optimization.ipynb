{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 21: Performance Optimization\n",
    "\n",
    "Performance optimization transforms working code into efficient code that scales under load and operates within resource constraints. However, optimization without measurement is guesswork\u2014a common trap known as \"premature optimization\" that often introduces complexity without benefit. This chapter establishes a data-driven approach to performance: first measuring precisely where bottlenecks occur, then applying targeted optimizations that yield measurable improvements.\n",
    "\n",
    "We will explore Python's profiling ecosystem to identify slow code paths, analyze algorithmic complexity using Big O notation to ensure scalability, and implement caching strategies that eliminate redundant computation. Throughout, we emphasize \"Pythonic\" patterns that leverage the language's optimized internals rather than fighting against them.\n",
    "\n",
    "## 21.1 Profiling: Identifying Bottlenecks with Precision\n",
    "\n",
    "Before optimizing, you must measure. Python provides sophisticated profiling tools that instrument your code to report where time and memory are consumed. Profiling answers the critical question: \"Where is my program actually spending its time?\"\n",
    "\n",
    "### Introduction to cProfile\n",
    "\n",
    "The `cProfile` module is the standard library's deterministic profiler\u2014it traces every function call and records timing information with minimal overhead (implemented in C). Unlike naive print-statement timing, `cProfile` captures the entire call stack and aggregates statistics across complex execution paths.\n",
    "\n",
    "```python\n",
    "import cProfile\n",
    "import pstats\n",
    "from io import StringIO\n",
    "from typing import List\n",
    "import random\n",
    "\n",
    "def slow_function(data: List[int]) -> int:\n",
    "    \"\"\"\n",
    "    A deliberately inefficient function for demonstration.\n",
    "    Uses nested loops with O(n\u00b2) complexity.\n",
    "    \"\"\"\n",
    "    result = 0\n",
    "    # Inefficient: nested loops for sum of pairs\n",
    "    for i in range(len(data)):\n",
    "        for j in range(len(data)):\n",
    "            if i != j:\n",
    "                result += data[i] * data[j]\n",
    "    return result\n",
    "\n",
    "def fast_function(data: List[int]) -> int:\n",
    "    \"\"\"\n",
    "    Optimized version using mathematical insight.\n",
    "    Sum of all pairs = (sum of all)\u00b2 - sum of squares\n",
    "    \"\"\"\n",
    "    total = sum(data)\n",
    "    sum_squares = sum(x * x for x in data)\n",
    "    return total * total - sum_squares\n",
    "\n",
    "def main():\n",
    "    \"\"\"Generate test data and process it.\"\"\"\n",
    "    # Generate 1000 random integers\n",
    "    data = [random.randint(1, 100) for _ in range(1000)]\n",
    "    \n",
    "    # Call both functions\n",
    "    result1 = slow_function(data)\n",
    "    result2 = fast_function(data)\n",
    "    \n",
    "    print(f\"Results match: {result1 == result2}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Method 1: Command-line profiling\n",
    "    # python -m cProfile -s cumulative script.py\n",
    "    \n",
    "    # Method 2: Programmatic profiling\n",
    "    profiler = cProfile.Profile()\n",
    "    profiler.enable()  # Start recording\n",
    "    \n",
    "    main()  # Run the code to profile\n",
    "    \n",
    "    profiler.disable()  # Stop recording\n",
    "    \n",
    "    # Capture and format statistics\n",
    "    stream = StringIO()\n",
    "    # Sort by cumulative time (time spent in function + subcalls)\n",
    "    stats = pstats.Stats(profiler, stream=stream)\n",
    "    stats.sort_stats(pstats.SortKey.CUMULATIVE)\n",
    "    stats.print_stats(10)  # Print top 10 functions\n",
    "    \n",
    "    print(stream.getvalue())\n",
    "```\n",
    "\n",
    "**Understanding the Output:**\n",
    "\n",
    "When you run this code, `cProfile` produces a table with these columns:\n",
    "\n",
    "```\n",
    "   ncalls  tottime  percall  cumtime  percall filename:lineno(function)\n",
    "     1000    0.123    0.000    2.456    0.002 script.py:10(slow_function)\n",
    "        1    0.001    0.001    2.500    2.500 script.py:30(main)\n",
    "```\n",
    "\n",
    "*   **ncalls**: Number of times the function was called (including recursive calls shown as `1000/1` if recursive)\n",
    "*   **tottime**: Total time spent in the function excluding subcalls (the function's own code)\n",
    "*   **percall (first)**: `tottime` divided by `ncalls`\n",
    "*   **cumtime**: Cumulative time spent in function plus all subcalls it invoked (most important for finding bottlenecks)\n",
    "*   **percall (second)**: `cumtime` divided by `ncalls`\n",
    "*   **filename:lineno(function)**: Location of the function\n",
    "\n",
    "**Analysis Strategy:**\n",
    "Focus on functions with high **cumtime** (total impact) or high **tottime** ( CPU-intensive operations). In the example above, `slow_function` dominates the cumulative time due to its O(n\u00b2) nested loops.\n",
    "\n",
    "### Line-by-Line Profiling with line_profiler\n",
    "\n",
    "While `cProfile` tells you which function is slow, `line_profiler` (third-party package: `pip install line_profiler`) tells you which specific lines within that function are slow. This is essential when optimizing complex functions where only a few lines cause the bottleneck.\n",
    "\n",
    "```python\n",
    "from line_profiler import profile\n",
    "from typing import List\n",
    "import math\n",
    "\n",
    "# Decorator marks function for line-by-line profiling\n",
    "@profile\n",
    "def process_dataset(data: List[dict]) -> dict:\n",
    "    \"\"\"\n",
    "    Process a list of dictionaries with mixed operations.\n",
    "    Line profiler will show time per line.\n",
    "    \"\"\"\n",
    "    results = {\n",
    "        'sum_values': 0,\n",
    "        'sqrt_values': [],\n",
    "        'filtered': []\n",
    "    }\n",
    "    \n",
    "    # Line 1: Loop initialization (usually fast)\n",
    "    for item in data:\n",
    "        # Line 2: Dictionary access and arithmetic\n",
    "        value = item['value'] * 2\n",
    "        \n",
    "        # Line 3: Accumulation (simple operation)\n",
    "        results['sum_values'] += value\n",
    "        \n",
    "        # Line 4: Math library call (potentially slow)\n",
    "        sqrt_val = math.sqrt(value)\n",
    "        results['sqrt_values'].append(sqrt_val)\n",
    "        \n",
    "        # Line 5: Conditional filtering\n",
    "        if value > 50:\n",
    "            # Line 6: List append (amortized O(1))\n",
    "            results['filtered'].append(value)\n",
    "    \n",
    "    # Line 7: Return statement\n",
    "    return results\n",
    "\n",
    "# Generate test data\n",
    "test_data = [{'value': i} for i in range(10000)]\n",
    "\n",
    "# Run the function\n",
    "output = process_dataset(test_data)\n",
    "\n",
    "# To run: kernprof -l -v script.py\n",
    "# -l: Use line profiler\n",
    "# -v: View results immediately after run\n",
    "```\n",
    "\n",
    "**Line Profiler Output Explanation:**\n",
    "\n",
    "```\n",
    "Total time: 0.003456 s\n",
    "File: script.py\n",
    "Function: process_dataset at line 5\n",
    "\n",
    "Line #      Time  Per Hit   % Time  Line Contents\n",
    "================================================\n",
    "     5                           @profile\n",
    "     6                           def process_dataset(data: List[dict]) -> dict:\n",
    "     7         2      2.0      0.1      results = {\n",
    "     8         1      1.0      0.0          'sum_values': 0,\n",
    "     9         1      1.0      0.0          'sqrt_values': [],\n",
    "    10         1      1.0      0.0          'filtered': []\n",
    "    11                               }\n",
    "    12                           \n",
    "    13       100    100.0      2.9      for item in data:\n",
    "    14      3000    0.3     86.8          value = item['value'] * 2\n",
    "    15       400    0.04     11.6          results['sum_values'] += value\n",
    "    16       800    0.08      0.0          sqrt_val = math.sqrt(value)\n",
    "    17       800    0.08      0.0          results['sqrt_values'].append(sqrt_val)\n",
    "    18       400    0.04      0.0          if value > 50:\n",
    "    19       200    0.02      0.0              results['filtered'].append(value)\n",
    "```\n",
    "\n",
    "**Key Insight:** Line 14 consumes 86.8% of the time. The dictionary access `item['value']` inside a tight loop is expensive. Optimization might involve extracting values into a list first using a list comprehension (which runs in C-speed internally).\n",
    "\n",
    "### Memory Profiling with memory_profiler\n",
    "\n",
    "CPU time is not the only constraint\u2014memory consumption determines whether your application can handle large datasets or if it will crash with `MemoryError`. The `memory_profiler` package tracks memory usage line-by-line.\n",
    "\n",
    "```python\n",
    "from memory_profiler import profile\n",
    "from typing import List\n",
    "\n",
    "@profile\n",
    "def inefficient_processing(n: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Demonstrates memory-intensive patterns.\n",
    "    Creates multiple intermediate lists unnecessarily.\n",
    "    \"\"\"\n",
    "    # Step 1: Create list of numbers (n integers in memory)\n",
    "    numbers = list(range(n))\n",
    "    \n",
    "    # Step 2: Create second list by doubling (another n integers)\n",
    "    doubled = [x * 2 for x in numbers]\n",
    "    \n",
    "    # Step 3: Create third list by filtering (another ~n/2 integers)\n",
    "    filtered = [x for x in doubled if x > n]\n",
    "    \n",
    "    # Step 4: Sum (creates no new list, but previous lists still exist)\n",
    "    total = sum(filtered)\n",
    "    \n",
    "    return filtered\n",
    "\n",
    "@profile\n",
    "def efficient_processing(n: int) -> List[int]:\n",
    "    \"\"\"\n",
    "    Memory-efficient version using generator expressions.\n",
    "    Values computed on-the-fly, not stored in intermediate lists.\n",
    "    \"\"\"\n",
    "    # Generator expression: computes one value at a time, constant memory\n",
    "    doubled = (x * 2 for x in range(n))\n",
    "    \n",
    "    # Filter as generator: still constant memory\n",
    "    filtered = (x for x in doubled if x > n)\n",
    "    \n",
    "    # Sum consumes generator iteratively\n",
    "    total = sum(filtered)\n",
    "    \n",
    "    # If we need the list, create only the final filtered result\n",
    "    return [x for x in range(n) if x * 2 > n]\n",
    "\n",
    "# Compare memory usage\n",
    "if __name__ == \"__main__\":\n",
    "    print(\"Inefficient version:\")\n",
    "    result1 = inefficient_processing(100000)\n",
    "    \n",
    "    print(\"\\nEfficient version:\")\n",
    "    result2 = efficient_processing(100000)\n",
    "```\n",
    "\n",
    "**Running Memory Profiler:**\n",
    "```bash\n",
    "python -m memory_profiler script.py\n",
    "```\n",
    "\n",
    "**Typical Output:**\n",
    "```\n",
    "Line #    Mem usage    Increment  Occurrences   Line Contents\n",
    "=============================================================\n",
    "     4     38.5 MiB     38.5 MiB           1   @profile\n",
    "     5                             def inefficient_processing(n: int):\n",
    "     6     42.8 MiB      4.3 MiB           1       numbers = list(range(n))\n",
    "     7     47.1 MiB      4.3 MiB           1       doubled = [x * 2 for x in numbers]\n",
    "     8     49.2 MiB      2.1 MiB           1       filtered = [x for x in doubled if x > n]\n",
    "     9     49.2 MiB      0.0 MiB           1       total = sum(filtered)\n",
    "```\n",
    "\n",
    "The inefficient version peaks at ~49 MiB (three lists). The efficient version using generators stays near baseline (~38 MiB) regardless of `n`.\n",
    "\n",
    "### Micro-benchmarking with timeit\n",
    "\n",
    "When comparing specific code snippets (e.g., \"Is `list.append()` faster than list concatenation?\"), use the `timeit` module. It runs code multiple times to eliminate background noise and provides statistically significant timing comparisons.\n",
    "\n",
    "```python\n",
    "import timeit\n",
    "from typing import List\n",
    "\n",
    "def method_append(n: int) -> List[int]:\n",
    "    \"\"\"Build list using append in loop.\"\"\"\n",
    "    result = []\n",
    "    for i in range(n):\n",
    "        result.append(i)\n",
    "    return result\n",
    "\n",
    "def method_comprehension(n: int) -> List[int]:\n",
    "    \"\"\"Build list using list comprehension.\"\"\"\n",
    "    return [i for i in range(n)]\n",
    "\n",
    "def method_extend(n: int) -> List[int]:\n",
    "    \"\"\"Build list using extend.\"\"\"\n",
    "    result = []\n",
    "    result.extend(range(n))\n",
    "    return result\n",
    "\n",
    "def benchmark():\n",
    "    \"\"\"Compare performance of list construction methods.\"\"\"\n",
    "    n = 10000\n",
    "    iterations = 1000\n",
    "    \n",
    "    # timeit.timeit(stmt, setup, number)\n",
    "    # stmt: code to execute as string\n",
    "    # setup: preparation code (imports, etc.)\n",
    "    # number: how many times to run\n",
    "    \n",
    "    # Create setup context that imports our functions\n",
    "    setup = \"\"\"\n",
    "from __main__ import method_append, method_comprehension, method_extend\n",
    "n = 10000\n",
    "\"\"\"\n",
    "    \n",
    "    # Benchmark each method\n",
    "    time_append = timeit.timeit(\n",
    "        stmt=\"method_append(n)\",\n",
    "        setup=setup,\n",
    "        number=iterations\n",
    "    )\n",
    "    \n",
    "    time_comp = timeit.timeit(\n",
    "        stmt=\"method_comprehension(n)\",\n",
    "        setup=setup,\n",
    "        number=iterations\n",
    "    )\n",
    "    \n",
    "    time_extend = timeit.timeit(\n",
    "        stmt=\"method_extend(n)\",\n",
    "        setup=setup,\n",
    "        number=iterations\n",
    "    )\n",
    "    \n",
    "    print(f\"Iterations: {iterations}, List size: {n}\")\n",
    "    print(f\"Append loop:       {time_append:.4f}s\")\n",
    "    print(f\"List comprehension: {time_comp:.4f}s\")\n",
    "    print(f\"Extend:            {time_extend:.4f}s\")\n",
    "    print(f\"\\nFastest: List comprehension ({time_append/time_comp:.1f}x faster than append)\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    benchmark()\n",
    "```\n",
    "\n",
    "**Key Principle:** List comprehensions are typically 2-3x faster than explicit `for` loops with `append()` because the iteration runs at C speed inside the Python interpreter, avoiding repeated method lookups and Python bytecode overhead.\n",
    "\n",
    "## 21.2 Algorithmic Efficiency: Big O and Pythonic Patterns\n",
    "\n",
    "Profiling identifies slow code, but Big O notation determines whether your solution will scale. Big O describes how runtime or memory grows relative to input size `n`. An O(n) algorithm doubles in time when input doubles; an O(n\u00b2) algorithm quadruples\u2014eventually becoming unusable.\n",
    "\n",
    "### Understanding Complexity Classes\n",
    "\n",
    "```python\n",
    "from typing import List, Set, Dict\n",
    "import time\n",
    "\n",
    "def demonstrate_complexity(data: List[int]):\n",
    "    \"\"\"\n",
    "    Demonstrates different Big O complexities with timing.\n",
    "    \"\"\"\n",
    "    n = len(data)\n",
    "    print(f\"Input size n = {n}\")\n",
    "    \n",
    "    # O(1) - Constant Time\n",
    "    # Operation time unchanged by input size\n",
    "    start = time.perf_counter()\n",
    "    first = data[0] if data else None\n",
    "    o1_time = time.perf_counter() - start\n",
    "    print(f\"O(1) - Access first element:     {o1_time:.6f}s\")\n",
    "    \n",
    "    # O(n) - Linear Time\n",
    "    # Time grows proportionally with input\n",
    "    start = time.perf_counter()\n",
    "    total = 0\n",
    "    for x in data:  # Single pass through data\n",
    "        total += x\n",
    "    on_time = time.perf_counter() - start\n",
    "    print(f\"O(n) - Sum all elements:         {on_time:.6f}s\")\n",
    "    \n",
    "    # O(n\u00b2) - Quadratic Time\n",
    "    # Time grows with square of input (nested loops)\n",
    "    start = time.perf_counter()\n",
    "    pairs = 0\n",
    "    for i in data:           # Outer loop: n iterations\n",
    "        for j in data:       # Inner loop: n iterations per outer\n",
    "            if i + j == 100:\n",
    "                pairs += 1\n",
    "    on2_time = time.perf_counter() - start\n",
    "    print(f\"O(n\u00b2) - Find all pairs:          {on2_time:.6f}s\")\n",
    "    print(f\"       (Expected ratio n\u00b2/n = {n}x slower, actual: {on2_time/on_time:.1f}x)\")\n",
    "    \n",
    "    # O(n log n) - Linearithmic\n",
    "    # Common in efficient sorting algorithms\n",
    "    start = time.perf_counter()\n",
    "    sorted_data = sorted(data)  # Timsort in Python\n",
    "    onlogn_time = time.perf_counter() - start\n",
    "    print(f\"O(n log n) - Sort data:          {onlogn_time:.6f}s\")\n",
    "\n",
    "# Test with different sizes to see scaling\n",
    "if __name__ == \"__main__\":\n",
    "    import random\n",
    "    for size in [1000, 2000, 4000]:\n",
    "        print(f\"\\n{'='*50}\")\n",
    "        data = [random.randint(1, 100) for _ in range(size)]\n",
    "        demonstrate_complexity(data)\n",
    "```\n",
    "\n",
    "**Common Python Complexities:**\n",
    "*   **List indexing** `lst[i]`: O(1)\n",
    "*   **Dictionary lookup** `dict[key]`: O(1) average case (hash table)\n",
    "*   **List append** `lst.append(x)`: O(1) amortized (occasionally resizes)\n",
    "*   **List insertion** `lst.insert(0, x)`: O(n) (must shift all elements)\n",
    "*   **Set operations** `x in set`: O(1) vs `x in list`: O(n)\n",
    "\n",
    "### Practical Optimization: Lookup Tables\n",
    "\n",
    "The most common optimization in Python is converting O(n) list searches into O(1) set or dictionary lookups:\n",
    "\n",
    "```python\n",
    "from typing import List, Set, Dict\n",
    "import time\n",
    "\n",
    "def find_common_slow(list1: List[int], list2: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    O(n * m) complexity - nested loop approach.\n",
    "    For lists of size 10,000, this performs 100,000,000 comparisons.\n",
    "    \"\"\"\n",
    "    common = []\n",
    "    for x in list1:           # O(n)\n",
    "        if x in list2:        # O(m) - linear search through list!\n",
    "            common.append(x)\n",
    "    return common\n",
    "\n",
    "def find_common_fast(list1: List[int], list2: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    O(n + m) complexity - set intersection.\n",
    "    Convert to sets first (O(n)), then intersection is O(min(n,m)).\n",
    "    \"\"\"\n",
    "    set1 = set(list1)         # O(n) to build hash table\n",
    "    set2 = set(list2)         # O(m) to build hash table\n",
    "    return list(set1 & set2)  # O(min(n,m)) hash lookups\n",
    "\n",
    "def find_common_memory_optimized(large_list: List[int], \n",
    "                                  small_list: List[int]) -> List[int]:\n",
    "    \"\"\"\n",
    "    If one list is much smaller, only convert the small one to set\n",
    "    to save memory while maintaining O(n) lookup time.\n",
    "    \"\"\"\n",
    "    small_set = set(small_list)  # O(m) space and time\n",
    "    return [x for x in large_list if x in small_set]  # O(n) lookups\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    size = 10000\n",
    "    list1 = list(range(size))\n",
    "    list2 = list(range(size//2, size + size//2))  # 50% overlap\n",
    "    \n",
    "    print(f\"List sizes: {size}\")\n",
    "    \n",
    "    # Slow method\n",
    "    start = time.perf_counter()\n",
    "    result_slow = find_common_slow(list1, list2)\n",
    "    slow_time = time.perf_counter() - start\n",
    "    print(f\"Nested loop (O(n\u00b2)): {slow_time:.4f}s, found {len(result_slow)} items\")\n",
    "    \n",
    "    # Fast method\n",
    "    start = time.perf_counter()\n",
    "    result_fast = find_common_fast(list1, list2)\n",
    "    fast_time = time.perf_counter() - start\n",
    "    print(f\"Set intersection (O(n)): {fast_time:.4f}s, found {len(result_fast)} items\")\n",
    "    print(f\"Speedup: {slow_time/fast_time:.0f}x faster\")\n",
    "```\n",
    "\n",
    "### String Concatenation: A Classic Pitfall\n",
    "\n",
    "Python strings are immutable. Using `+=` in a loop creates a new string object each time\u2014O(n\u00b2) behavior for building a large string.\n",
    "\n",
    "```python\n",
    "from typing import List\n",
    "import time\n",
    "from io import StringIO\n",
    "\n",
    "def concat_strings_slow(items: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    O(n\u00b2) - Quadratic time due to immutable string copying.\n",
    "    Each += creates a new string and copies all previous content.\n",
    "    \"\"\"\n",
    "    result = \"\"\n",
    "    for item in items:\n",
    "        result += item + \"\\n\"  # New allocation and copy every iteration\n",
    "    return result\n",
    "\n",
    "def concat_strings_fast(items: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    O(n) - Linear time using join().\n",
    "    Join pre-calculates total size and allocates once.\n",
    "    \"\"\"\n",
    "    return \"\\n\".join(items)  # Single allocation, C-speed loop\n",
    "\n",
    "def concat_strings_builder(items: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    O(n) using StringIO - useful when building incrementally\n",
    "    with conditional logic that makes join() awkward.\n",
    "    \"\"\"\n",
    "    buffer = StringIO()\n",
    "    for item in items:\n",
    "        buffer.write(item)\n",
    "        buffer.write(\"\\n\")\n",
    "    return buffer.getvalue()\n",
    "\n",
    "# Benchmark\n",
    "if __name__ == \"__main__\":\n",
    "    words = [f\"word_{i}\" for i in range(100000)]\n",
    "    \n",
    "    # Slow method\n",
    "    start = time.perf_counter()\n",
    "    slow_result = concat_strings_slow(words)\n",
    "    slow_time = time.perf_counter() - start\n",
    "    print(f\"String += : {slow_time:.4f}s\")\n",
    "    \n",
    "    # Fast method\n",
    "    start = time.perf_counter()\n",
    "    fast_result = concat_strings_fast(words)\n",
    "    fast_time = time.perf_counter() - start\n",
    "    print(f\"str.join(): {fast_time:.4f}s\")\n",
    "    print(f\"Speedup: {slow_time/fast_time:.0f}x\")\n",
    "```\n",
    "\n",
    "## 21.3 Caching: Eliminating Redundant Computation\n",
    "\n",
    "Caching stores the results of expensive function calls and returns the cached result when the same inputs occur again. This trades memory (storing results) for CPU time (recomputing).\n",
    "\n",
    "### Function-Level Caching with functools.lru_cache\n",
    "\n",
    "The `functools.lru_cache` decorator implements a Least Recently Used (LRU) cache with a maximum size. When the cache is full, it discards the least recently accessed items to make room for new ones.\n",
    "\n",
    "```python\n",
    "from functools import lru_cache\n",
    "from typing import Dict\n",
    "import time\n",
    "\n",
    "# Without cache - exponential time complexity for Fibonacci\n",
    "def fibonacci_slow(n: int) -> int:\n",
    "    \"\"\"\n",
    "    O(2^n) - Recalculates same values repeatedly.\n",
    "    fib(5) calls fib(3) twice, fib(2) three times, etc.\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fibonacci_slow(n - 1) + fibonacci_slow(n - 2)\n",
    "\n",
    "# With cache - linear time complexity\n",
    "@lru_cache(maxsize=128)  # Cache last 128 unique calls\n",
    "def fibonacci_fast(n: int) -> int:\n",
    "    \"\"\"\n",
    "    O(n) - Each unique n computed only once.\n",
    "    Subsequent calls return instantly from cache.\n",
    "    \"\"\"\n",
    "    if n < 2:\n",
    "        return n\n",
    "    return fibonacci_fast(n - 1) + fibonacci_fast(n - 2)\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    n = 35\n",
    "    \n",
    "    # Slow version\n",
    "    start = time.perf_counter()\n",
    "    result_slow = fibonacci_slow(n)\n",
    "    slow_time = time.perf_counter() - start\n",
    "    print(f\"Slow fibonacci({n}): {result_slow}, time: {slow_time:.4f}s\")\n",
    "    \n",
    "    # Fast version (first call populates cache)\n",
    "    start = time.perf_counter()\n",
    "    result_fast = fibonacci_fast(n)\n",
    "    fast_time = time.perf_counter() - start\n",
    "    print(f\"Fast fibonacci({n}): {result_fast}, time: {fast_time:.4f}s\")\n",
    "    \n",
    "    # Instant cached call\n",
    "    start = time.perf_counter()\n",
    "    result_cached = fibonacci_fast(n)  # Already in cache\n",
    "    cache_time = time.perf_counter() - start\n",
    "    print(f\"Cached call time: {cache_time:.6f}s\")\n",
    "    print(f\"Cache info: {fibonacci_fast.cache_info()}\")\n",
    "    \n",
    "    # Clear cache if needed (e.g., memory pressure)\n",
    "    fibonacci_fast.cache_clear()\n",
    "```\n",
    "\n",
    "**Understanding `cache_info()`:**\n",
    "The `cache_info()` method returns a named tuple showing:\n",
    "*   **hits**: How many calls were satisfied from cache (fast)\n",
    "*   **misses**: How many calls required actual computation (slow)\n",
    "*   **maxsize**: Cache capacity limit\n",
    "*   **currsize**: Current number of cached results\n",
    "\n",
    "### Advanced Caching with cachetools\n",
    "\n",
    "For production applications requiring time-based expiration, LRU with size limits, or disk-based caching, use the `cachetools` library (`pip install cachetools`).\n",
    "\n",
    "```python\n",
    "from cachetools import TTLCache, LRUCache\n",
    "from cachetools.keys import hashkey\n",
    "import time\n",
    "from typing import Dict, Any\n",
    "\n",
    "# TTL Cache - Time To Live (expiration)\n",
    "# Useful for API clients where data becomes stale\n",
    "api_cache: TTLCache = TTLCache(maxsize=100, ttl=300)  # 5 minutes\n",
    "\n",
    "def get_user_data(user_id: int) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Simulates expensive API call.\n",
    "    Cached for 5 minutes to reduce API load.\n",
    "    \"\"\"\n",
    "    # Check cache manually (or use decorator)\n",
    "    cache_key = hashkey(user_id)\n",
    "    if cache_key in api_cache:\n",
    "        print(f\"Cache hit for user {user_id}\")\n",
    "        return api_cache[cache_key]\n",
    "    \n",
    "    print(f\"Fetching fresh data for user {user_id}...\")\n",
    "    time.sleep(1)  # Simulate network delay\n",
    "    \n",
    "    data = {\n",
    "        'id': user_id,\n",
    "        'name': f'User {user_id}',\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    api_cache[cache_key] = data\n",
    "    return data\n",
    "\n",
    "# Decorator-based approach for functions\n",
    "from cachetools import cached\n",
    "\n",
    "# Cache weather data for 10 minutes (600 seconds)\n",
    "@cached(cache=TTLCache(maxsize=1024, ttl=600))\n",
    "def get_weather(city: str) -> Dict[str, float]:\n",
    "    \"\"\"Simulate weather API call.\"\"\"\n",
    "    print(f\"Fetching weather for {city}...\")\n",
    "    time.sleep(2)\n",
    "    return {\n",
    "        'city': city,\n",
    "        'temp': 22.5,\n",
    "        'humidity': 60\n",
    "    }\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Demonstrate TTL cache\n",
    "    print(\"First call:\")\n",
    "    data1 = get_user_data(42)\n",
    "    print(f\"Timestamp: {data1['timestamp']}\")\n",
    "    \n",
    "    print(\"\\nSecond call (cached):\")\n",
    "    data2 = get_user_data(42)\n",
    "    print(f\"Timestamp: {data2['timestamp']} (same as above)\")\n",
    "    \n",
    "    print(\"\\nWeather API:\")\n",
    "    print(get_weather(\"London\"))\n",
    "    print(get_weather(\"London\"))  # Instant from cache\n",
    "```\n",
    "\n",
    "### Memoization for Dynamic Programming\n",
    "\n",
    "Memoization is the fundamental technique behind dynamic programming\u2014solving complex problems by breaking them into overlapping subproblems and caching their solutions.\n",
    "\n",
    "```python\n",
    "from functools import lru_cache\n",
    "from typing import List, Tuple\n",
    "\n",
    "def knapsack_greedy(items: List[Tuple[int, int]], capacity: int) -> int:\n",
    "    \"\"\"\n",
    "    Greedy approach (not optimal) - O(n log n)\n",
    "    Sorts by value/weight ratio, fills knapsack.\n",
    "    \"\"\"\n",
    "    # Sort by value density\n",
    "    sorted_items = sorted(items, key=lambda x: x[1]/x[0], reverse=True)\n",
    "    total_value = 0\n",
    "    remaining = capacity\n",
    "    \n",
    "    for weight, value in sorted_items:\n",
    "        if weight <= remaining:\n",
    "            total_value += value\n",
    "            remaining -= weight\n",
    "    return total_value\n",
    "\n",
    "@lru_cache(maxsize=None)  # Unlimited cache for DP\n",
    "def knapsack_optimal(items: Tuple[Tuple[int, int], ...], capacity: int, index: int) -> int:\n",
    "    \"\"\"\n",
    "    Optimal 0/1 Knapsack using memoization - O(n * capacity)\n",
    "    \n",
    "    Args:\n",
    "        items: Tuple of (weight, value) - tuple required for hashing\n",
    "        capacity: Remaining capacity\n",
    "        index: Current item index being considered\n",
    "    \n",
    "    Returns:\n",
    "        Maximum value achievable\n",
    "    \"\"\"\n",
    "    # Base case: no capacity or no items left\n",
    "    if capacity <= 0 or index >= len(items):\n",
    "        return 0\n",
    "    \n",
    "    weight, value = items[index]\n",
    "    \n",
    "    # Choice 1: Don't take current item\n",
    "    skip = knapsack_optimal(items, capacity, index + 1)\n",
    "    \n",
    "    # Choice 2: Take current item (if it fits)\n",
    "    take = 0\n",
    "    if weight <= capacity:\n",
    "        take = value + knapsack_optimal(items, capacity - weight, index + 1)\n",
    "    \n",
    "    return max(skip, take)\n",
    "\n",
    "# Wrapper to convert list to tuple for caching\n",
    "def solve_knapsack(items: List[Tuple[int, int]], capacity: int) -> int:\n",
    "    \"\"\"Convert list to tuple for hashable cache keys.\"\"\"\n",
    "    return knapsack_optimal(tuple(items), capacity, 0)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # (weight, value) pairs\n",
    "    items = [(2, 3), (3, 4), (4, 5), (5, 8), (9, 10)]\n",
    "    capacity = 20\n",
    "    \n",
    "    print(\"Greedy solution:\", knapsack_greedy(items, capacity))\n",
    "    \n",
    "    optimal = solve_knapsack(items, capacity)\n",
    "    print(\"Optimal solution:\", optimal)\n",
    "    print(f\"Cache stats: {knapsack_optimal.cache_info()}\")\n",
    "```\n",
    "\n",
    "### Cache Invalidation Strategies\n",
    "\n",
    "The hardest problem in computer science is cache invalidation\u2014knowing when to clear cached data because the underlying source has changed.\n",
    "\n",
    "```python\n",
    "from functools import lru_cache\n",
    "from typing import Callable\n",
    "import time\n",
    "\n",
    "class CachedAPIClient:\n",
    "    \"\"\"\n",
    "    Demonstrates manual cache management with TTL and invalidation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._cache_timestamp = 0\n",
    "        self._cache_ttl = 60  # 60 seconds\n",
    "        self._setup_caches()\n",
    "    \n",
    "    def _setup_caches(self):\n",
    "        \"\"\"Initialize cached methods.\"\"\"\n",
    "        # We re-define cached methods to reset them\n",
    "        self.get_user = lru_cache(maxsize=128)(self._fetch_user)\n",
    "        self.get_products = lru_cache(maxsize=256)(self._fetch_products)\n",
    "    \n",
    "    def _fetch_user(self, user_id: int) -> dict:\n",
    "        \"\"\"Actual implementation.\"\"\"\n",
    "        print(f\"HTTP GET /users/{user_id}\")\n",
    "        return {\"id\": user_id, \"name\": \"John\"}\n",
    "    \n",
    "    def _fetch_products(self, category: str) -> list:\n",
    "        \"\"\"Actual implementation.\"\"\"\n",
    "        print(f\"HTTP GET /products?category={category}\")\n",
    "        return [{\"id\": 1, \"name\": \"Widget\"}]\n",
    "    \n",
    "    def invalidate_user(self, user_id: int) -> None:\n",
    "        \"\"\"Remove specific user from cache.\"\"\"\n",
    "        # lru_cache doesn't support single-key deletion directly\n",
    "        # Workaround: clear entire user cache (or use cachetools)\n",
    "        self.get_user.cache_clear()\n",
    "        print(f\"Invalidated cache for user {user_id}\")\n",
    "    \n",
    "    def refresh_all(self) -> None:\n",
    "        \"\"\"Clear all caches when data changes.\"\"\"\n",
    "        self.get_user.cache_clear()\n",
    "        self.get_products.cache_clear()\n",
    "        self._cache_timestamp = time.time()\n",
    "        print(\"All caches refreshed\")\n",
    "\n",
    "# Usage\n",
    "client = CachedAPIClient()\n",
    "client.get_user(1)  # Fetches\n",
    "client.get_user(1)  # Cached\n",
    "client.invalidate_user(1)\n",
    "client.get_user(1)  # Fetches again\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "Performance optimization is a measurement-driven discipline. You learned to identify bottlenecks using **`cProfile`** for function-level analysis, **`line_profiler`** for line-by-line inspection, and **`timeit`** for micro-benchmarks. You understand **Big O notation** as the language of scalability\u2014recognizing that O(n) set lookups outperform O(n\u00b2) nested loops, and that algorithmic improvements (like memoization) often yield greater speedups than micro-optimizations.\n",
    "\n",
    "You mastered **Pythonic patterns** that leverage the interpreter's optimizations: list comprehensions over explicit loops, generator expressions for memory efficiency, and `str.join()` for string building. You implemented **caching strategies** using `functools.lru_cache` for automatic memoization and `cachetools` for production scenarios requiring TTL expiration and fine-grained invalidation.\n",
    "\n",
    "Yet even the most optimized code requires architectural coherence to become a maintainable product. Assembling discrete functions into a production-grade application demands thoughtful project structure, configuration management, error handling, and deployment orchestration. The final chapter brings together every concept from this handbook into a cohesive capstone project\u2014a complete Python application built to professional standards.\n",
    "\n",
    "**Next Chapter**: Chapter 22: Building a Production-Grade Application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='20. containerization_and_deployment.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../9. capstone_project/22. building_a_production_grade_application.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}