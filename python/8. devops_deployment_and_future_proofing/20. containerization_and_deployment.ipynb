{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 20: Containerization and Deployment\n",
    "\n",
    "Modern Python applications rarely run on individual developer machines. They are deployed to cloud servers, auto-scaling clusters, and serverless platforms where they must handle production traffic reliably. The transition from a working Python script to a production-grade service requires containerization\u2014a technology that packages your application with its entire runtime environment into a portable unit. This chapter guides you through containerizing Python applications with Docker, orchestrating multi-service architectures, automating deployment pipelines, and deploying to modern cloud platforms.\n",
    "\n",
    "We will emphasize security-hardened container practices, efficient build caching, and GitOps workflows that enable teams to deploy confidently and roll back instantly when issues arise.\n",
    "\n",
    "## 20.1 Docker Basics: Containerizing Python Applications\n",
    "\n",
    "Docker is a platform for developing, shipping, and running applications in containers\u2014lightweight, standalone, executable packages that include everything needed to run your code: the Python interpreter, system libraries, environment variables, and your application code. Unlike virtual machines, containers share the host OS kernel, making them extremely efficient while maintaining isolation.\n",
    "\n",
    "### Writing Production-Grade Dockerfiles\n",
    "\n",
    "A Dockerfile is a text document containing instructions to assemble a Docker image. For Python applications, industry standards have evolved significantly beyond simple `FROM python` instructions.\n",
    "\n",
    "```dockerfile\n",
    "# Multi-stage Dockerfile for a Python web application\n",
    "# Stage 1: Builder (compiles dependencies)\n",
    "FROM python:3.11-slim as builder\n",
    "\n",
    "# Prevent Python from writing pyc files and buffering stdout\n",
    "ENV PYTHONDONTWRITEBYTECODE=1 \\\n",
    "    PYTHONUNBUFFERED=1 \\\n",
    "    PIP_NO_CACHE_DIR=1 \\\n",
    "    PIP_DISABLE_PIP_VERSION_CHECK=1\n",
    "\n",
    "# Install system dependencies required for compilation\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    gcc \\\n",
    "    libpq-dev \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Create virtual environment (isolation even within container)\n",
    "RUN python -m venv /opt/venv\n",
    "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
    "\n",
    "# Install Python dependencies separately for layer caching\n",
    "COPY requirements.txt .\n",
    "RUN pip install --upgrade pip && \\\n",
    "    pip install -r requirements.txt\n",
    "\n",
    "# Stage 2: Runtime (minimal attack surface)\n",
    "FROM python:3.11-slim as runtime\n",
    "\n",
    "# Security: Create non-root user (prevent container escape vulnerabilities)\n",
    "RUN groupadd -r appgroup && useradd -r -g appgroup appuser\n",
    "\n",
    "# Install runtime dependencies only (no compilers)\n",
    "RUN apt-get update && apt-get install -y --no-install-recommends \\\n",
    "    libpq5 \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Copy virtual environment from builder stage\n",
    "COPY --from=builder /opt/venv /opt/venv\n",
    "ENV PATH=\"/opt/venv/bin:$PATH\"\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy application code (after dependencies for better caching)\n",
    "COPY --chown=appuser:appgroup . .\n",
    "\n",
    "# Switch to non-root user\n",
    "USER appuser\n",
    "\n",
    "# Health check to ensure container is actually ready\n",
    "HEALTHCHECK --interval=30s --timeout=5s --start-period=5s --retries=3 \\\n",
    "    CMD python -c \"import urllib.request; urllib.request.urlopen('http://localhost:8000/health')\" || exit 1\n",
    "\n",
    "# Expose application port\n",
    "EXPOSE 8000\n",
    "\n",
    "# Run application (using exec form for proper signal handling)\n",
    "CMD [\"gunicorn\", \"--bind\", \"0.0.0.0:8000\", \"--workers\", \"4\", \"--threads\", \"2\", \"app:application\"]\n",
    "```\n",
    "\n",
    "**Key Dockerfile Best Practices Explained:**\n",
    "\n",
    "**Multi-Stage Builds:** The `builder` stage contains compilers (`gcc`) and build tools that bloat the image and create security vulnerabilities. Only the compiled virtual environment copies to the `runtime` stage, reducing the final image size by 50-70% and eliminating build-time dependencies from production.\n",
    "\n",
    "**Layer Caching Strategy:** Docker caches each instruction layer. By copying `requirements.txt` and installing dependencies *before* copying application code, pip install is skipped unless dependencies change\u2014a massive time-saver during development iterations.\n",
    "\n",
    "**Non-Root User:** Running containers as root is a critical security risk. If an attacker escapes the application, they have root access to the host. The `USER appuser` instruction ensures the application runs with minimal privileges.\n",
    "\n",
    "**`.dockerignore` File:** Prevent bloat and security leaks by excluding files from the build context:\n",
    "\n",
    "```dockerfile\n",
    "# .dockerignore\n",
    "__pycache__\n",
    "*.pyc\n",
    "*.pyo\n",
    "*.pyd\n",
    ".Python\n",
    ".git\n",
    ".gitignore\n",
    ".env\n",
    ".venv\n",
    "venv/\n",
    "ENV/\n",
    "env/\n",
    ".pytest_cache\n",
    ".coverage\n",
    "htmlcov/\n",
    "dist/\n",
    "build/\n",
    "*.egg-info/\n",
    ".DS_Store\n",
    "*.md\n",
    "!README.md  # Exception: keep README\n",
    "```\n",
    "\n",
    "### Docker Build and Run Commands\n",
    "\n",
    "```bash\n",
    "# Build the image (tag it for versioning)\n",
    "docker build -t myapp:v1.0 -t myapp:latest .\n",
    "\n",
    "# Run interactively for testing\n",
    "docker run -p 8000:8000 --env-file .env myapp:latest\n",
    "\n",
    "# Run detached (production mode)\n",
    "docker run -d --name myapp_container -p 8000:8000 myapp:latest\n",
    "\n",
    "# View logs\n",
    "docker logs -f myapp_container\n",
    "\n",
    "# Execute commands inside running container\n",
    "docker exec -it myapp_container /bin/sh\n",
    "\n",
    "# Clean up stopped containers\n",
    "docker container prune\n",
    "```\n",
    "\n",
    "## 20.2 Docker Compose: Multi-Container Orchestration\n",
    "\n",
    "Real applications rarely consist of a single container. A typical Python web application requires a PostgreSQL database, Redis cache, and possibly a Celery worker for background tasks. Docker Compose allows you to define and run multi-container Docker applications using a YAML file.\n",
    "\n",
    "### Defining Services Architecture\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yml (Development configuration)\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  # Application service\n",
    "  web:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://postgres:postgres@db:5432/myapp\n",
    "      - REDIS_URL=redis://redis:6379/0\n",
    "      - DEBUG=1\n",
    "    volumes:\n",
    "      # Mount code for hot-reload during development\n",
    "      - .:/app\n",
    "      # Anonymous volume prevents host's .venv from overwriting container's\n",
    "      - /app/.venv\n",
    "    depends_on:\n",
    "      db:\n",
    "        condition: service_healthy\n",
    "      redis:\n",
    "        condition: service_started\n",
    "    command: uvicorn app.main:app --host 0.0.0.0 --port 8000 --reload\n",
    "    networks:\n",
    "      - backend\n",
    "\n",
    "  # PostgreSQL database\n",
    "  db:\n",
    "    image: postgres:15-alpine\n",
    "    environment:\n",
    "      POSTGRES_USER: postgres\n",
    "      POSTGRES_PASSWORD: postgres\n",
    "      POSTGRES_DB: myapp\n",
    "    volumes:\n",
    "      # Named volume for data persistence\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5432:5432\"\n",
    "    healthcheck:\n",
    "      test: [\"CMD-SHELL\", \"pg_isready -U postgres\"]\n",
    "      interval: 5s\n",
    "      timeout: 5s\n",
    "      retries: 5\n",
    "    networks:\n",
    "      - backend\n",
    "\n",
    "  # Redis cache/queue\n",
    "  redis:\n",
    "    image: redis:7-alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "    networks:\n",
    "      - backend\n",
    "\n",
    "  # Background worker (Celery example)\n",
    "  worker:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "    command: celery -A app.tasks worker --loglevel=info\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://postgres:postgres@db:5432/myapp\n",
    "      - REDIS_URL=redis://redis:6379/0\n",
    "    depends_on:\n",
    "      - db\n",
    "      - redis\n",
    "    volumes:\n",
    "      - .:/app\n",
    "    networks:\n",
    "      - backend\n",
    "\n",
    "volumes:\n",
    "  postgres_data:\n",
    "  redis_data:\n",
    "\n",
    "networks:\n",
    "  backend:\n",
    "    driver: bridge\n",
    "```\n",
    "\n",
    "**Service Communication:** Containers on the same Docker network can communicate using service names as hostnames. The web container connects to `db:5432`, not `localhost:5432`, because each container has its own localhost.\n",
    "\n",
    "**Volume Management:**\n",
    "*   **Bind Mounts** (`./:/app`): Sync host directory with container (development only)\n",
    "*   **Named Volumes** (`postgres_data`): Persistent storage managed by Docker (production data)\n",
    "*   **Anonymous Volumes** (`/app/.venv`): Prevent bind mounts from overwriting container-specific directories\n",
    "\n",
    "### Production Compose Configuration\n",
    "\n",
    "Separate configurations prevent development tools (like hot-reload) from reaching production:\n",
    "\n",
    "```yaml\n",
    "# docker-compose.prod.yml (Production overrides)\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  web:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile\n",
    "      target: runtime  # Explicitly use runtime stage\n",
    "    environment:\n",
    "      - DEBUG=0\n",
    "      - SECRET_KEY=${SECRET_KEY}  # From .env file or secrets manager\n",
    "    volumes: []  # Remove bind mounts in production\n",
    "    command: gunicorn app.main:app -w 4 -k uvicorn.workers.UvicornWorker --bind 0.0.0.0:8000\n",
    "    restart: unless-stopped\n",
    "    deploy:\n",
    "      resources:\n",
    "        limits:\n",
    "          cpus: '1'\n",
    "          memory: 512M\n",
    "        reservations:\n",
    "          cpus: '0.25'\n",
    "          memory: 128M\n",
    "\n",
    "  db:\n",
    "    environment:\n",
    "      POSTGRES_PASSWORD: ${DB_PASSWORD}  # From environment/secrets\n",
    "    volumes:\n",
    "      - postgres_data:/var/lib/postgresql/data\n",
    "    restart: unless-stopped\n",
    "\n",
    "  # Add reverse proxy for SSL termination\n",
    "  nginx:\n",
    "    image: nginx:alpine\n",
    "    ports:\n",
    "      - \"80:80\"\n",
    "      - \"443:443\"\n",
    "    volumes:\n",
    "      - ./nginx.conf:/etc/nginx/nginx.conf:ro\n",
    "      - ./ssl:/etc/nginx/ssl:ro\n",
    "    depends_on:\n",
    "      - web\n",
    "    restart: unless-stopped\n",
    "\n",
    "# Use both files: docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n",
    "```\n",
    "\n",
    "## 20.3 CI/CD Pipelines: Automated Testing and Deployment\n",
    "\n",
    "Continuous Integration (CI) and Continuous Deployment (CD) automate the path from code commit to production deployment. GitHub Actions has become the industry-standard platform for Python projects hosted on GitHub, offering tight integration and a marketplace of reusable workflows.\n",
    "\n",
    "### Comprehensive GitHub Actions Workflow\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/main.yml\n",
    "name: CI/CD Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [main, develop]\n",
    "    tags: ['v*']\n",
    "  pull_request:\n",
    "    branches: [main]\n",
    "\n",
    "env:\n",
    "  REGISTRY: ghcr.io\n",
    "  IMAGE_NAME: ${{ github.repository }}\n",
    "\n",
    "jobs:\n",
    "  # Stage 1: Code Quality and Testing\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    strategy:\n",
    "      matrix:\n",
    "        python-version: [\"3.10\", \"3.11\", \"3.12\"]\n",
    "    \n",
    "    services:\n",
    "      postgres:\n",
    "        image: postgres:15\n",
    "        env:\n",
    "          POSTGRES_PASSWORD: postgres\n",
    "          POSTGRES_DB: test_db\n",
    "        options: >-\n",
    "          --health-cmd pg_isready\n",
    "          --health-interval 10s\n",
    "          --health-timeout 5s\n",
    "          --health-retries 5\n",
    "        ports:\n",
    "          - 5432:5432\n",
    "\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - name: Set up Python ${{ matrix.python-version }}\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: ${{ matrix.python-version }}\n",
    "          cache: 'pip'\n",
    "\n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          python -m pip install --upgrade pip\n",
    "          pip install -r requirements.txt\n",
    "          pip install -r requirements-dev.txt\n",
    "\n",
    "      - name: Lint with Ruff\n",
    "        run: |\n",
    "          ruff check .\n",
    "          ruff format --check .\n",
    "\n",
    "      - name: Type check with mypy\n",
    "        run: mypy app/\n",
    "\n",
    "      - name: Test with pytest\n",
    "        env:\n",
    "          DATABASE_URL: postgresql://postgres:postgres@localhost:5432/test_db\n",
    "        run: |\n",
    "          pytest --cov=app --cov-report=xml --cov-report=term\n",
    "\n",
    "      - name: Upload coverage to Codecov\n",
    "        uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          file: ./coverage.xml\n",
    "          fail_ci_if_error: true\n",
    "\n",
    "  # Stage 2: Security Scanning\n",
    "  security:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: test\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Run Trivy vulnerability scanner\n",
    "        uses: aquasecurity/trivy-action@master\n",
    "        with:\n",
    "          scan-type: 'fs'\n",
    "          scan-ref: '.'\n",
    "          format: 'sarif'\n",
    "          output: 'trivy-results.sarif'\n",
    "\n",
    "      - name: Upload Trivy scan results\n",
    "        uses: github/codeql-action/upload-sarif@v2\n",
    "        with:\n",
    "          sarif_file: 'trivy-results.sarif'\n",
    "\n",
    "  # Stage 3: Build and Push Container\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [test, security]\n",
    "    if: github.event_name != 'pull_request'\n",
    "    permissions:\n",
    "      contents: read\n",
    "      packages: write\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "\n",
    "      - name: Set up Docker Buildx\n",
    "        uses: docker/setup-buildx-action@v3\n",
    "\n",
    "      - name: Log in to Container Registry\n",
    "        uses: docker/login-action@v3\n",
    "        with:\n",
    "          registry: ${{ env.REGISTRY }}\n",
    "          username: ${{ github.actor }}\n",
    "          password: ${{ secrets.GITHUB_TOKEN }}\n",
    "\n",
    "      - name: Extract metadata\n",
    "        id: meta\n",
    "        uses: docker/metadata-action@v5\n",
    "        with:\n",
    "          images: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}\n",
    "          tags: |\n",
    "            type=ref,event=branch\n",
    "            type=semver,pattern={{version}}\n",
    "            type=sha,prefix=,suffix=,format=short\n",
    "\n",
    "      - name: Build and push Docker image\n",
    "        uses: docker/build-push-action@v5\n",
    "        with:\n",
    "          context: .\n",
    "          push: true\n",
    "          tags: ${{ steps.meta.outputs.tags }}\n",
    "          labels: ${{ steps.meta.outputs.labels }}\n",
    "          cache-from: type=gha\n",
    "          cache-to: type=gha,mode=max\n",
    "          platforms: linux/amd64,linux/arm64\n",
    "\n",
    "  # Stage 4: Deploy to Staging\n",
    "  deploy-staging:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: build\n",
    "    if: github.ref == 'refs/heads/develop'\n",
    "    environment:\n",
    "      name: staging\n",
    "      url: https://staging.example.com\n",
    "    \n",
    "    steps:\n",
    "      - name: Deploy to Staging Server\n",
    "        uses: appleboy/ssh-action@v1.0.0\n",
    "        with:\n",
    "          host: ${{ secrets.STAGING_HOST }}\n",
    "          username: ${{ secrets.STAGING_USER }}\n",
    "          key: ${{ secrets.STAGING_SSH_KEY }}\n",
    "          script: |\n",
    "            cd /opt/myapp\n",
    "            docker pull ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:develop\n",
    "            docker-compose -f docker-compose.yml -f docker-compose.prod.yml up -d\n",
    "            docker system prune -f\n",
    "\n",
    "  # Stage 5: Deploy to Production (Manual Approval)\n",
    "  deploy-production:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: build\n",
    "    if: startsWith(github.ref, 'refs/tags/v')\n",
    "    environment:\n",
    "      name: production\n",
    "      url: https://example.com\n",
    "    \n",
    "    steps:\n",
    "      - name: Deploy to Production\n",
    "        uses: some-deployment-action@v1\n",
    "        with:\n",
    "          # Specific deployment logic depends on your platform\n",
    "          image: ${{ env.REGISTRY }}/${{ env.IMAGE_NAME }}:${{ github.ref_name }}\n",
    "```\n",
    "\n",
    "**Pipeline Explanation:**\n",
    "1.  **Matrix Testing:** Tests against Python 3.10, 3.11, and 3.12 simultaneously with a real PostgreSQL service container.\n",
    "2.  **Security:** Trivy scans for known vulnerabilities in dependencies and OS packages.\n",
    "3.  **Caching:** `cache: 'pip'` and `type=gha` (GitHub Actions cache) dramatically speed up builds by reusing previous layers.\n",
    "4.  **Multi-architecture:** Builds for both AMD64 (Intel/AMD servers) and ARM64 (Apple Silicon, AWS Graviton) processors.\n",
    "5.  **GitOps:** Production deployments trigger only on version tags (`v1.0.0`), while staging auto-deploys from the develop branch.\n",
    "\n",
    "## 20.4 Cloud Deployment Strategies\n",
    "\n",
    "Containerized Python applications deploy to various platforms, each offering different trade-offs between control and convenience.\n",
    "\n",
    "### Platform-as-a-Service (PaaS)\n",
    "\n",
    "PaaS providers manage the underlying infrastructure, allowing you to focus on code. They are ideal for web applications and APIs.\n",
    "\n",
    "**Render** (Modern Heroku alternative):\n",
    "```yaml\n",
    "# render.yaml (Infrastructure as Code)\n",
    "services:\n",
    "  - type: web\n",
    "    name: my-python-api\n",
    "    runtime: python\n",
    "    plan: standard\n",
    "    buildCommand: \"pip install -r requirements.txt\"\n",
    "    startCommand: \"gunicorn app:app\"\n",
    "    envVars:\n",
    "      - key: DATABASE_URL\n",
    "        fromDatabase:\n",
    "          name: postgres-db\n",
    "          property: connectionString\n",
    "      - key: PYTHON_VERSION\n",
    "        value: 3.11.0\n",
    "\n",
    "databases:\n",
    "  - name: postgres-db\n",
    "    plan: starter\n",
    "    ipAllowList: []  # Allow access from anywhere (use with caution)\n",
    "```\n",
    "\n",
    "**Railway** (Developer-focused):\n",
    "Railway automatically detects your `Dockerfile` or `Procfile` and deploys from GitHub with automatic HTTPS, environment variables management, and managed databases.\n",
    "\n",
    "**Key PaaS Characteristics:**\n",
    "*   **Git-based deployment:** Push to GitHub triggers automatic build and deploy\n",
    "*   **Managed databases:** PostgreSQL/Redis with automatic backups\n",
    "*   **Horizontal scaling:** Increase instance count via dashboard or CLI\n",
    "*   **Zero-downtime deploys:** Health checks ensure new containers are ready before routing traffic\n",
    "\n",
    "### Serverless: AWS Lambda with Container Images\n",
    "\n",
    "For event-driven workloads (image processing, data transformation, APIs with sporadic traffic), AWS Lambda offers pay-per-invocation pricing. Modern Lambda supports container images up to 10GB, allowing complex ML dependencies.\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile for AWS Lambda\n",
    "FROM public.ecr.aws/lambda/python:3.11\n",
    "\n",
    "# Install dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt -t ${LAMBDA_TASK_ROOT}\n",
    "\n",
    "# Copy function code\n",
    "COPY app/lambda_handler.py ${LAMBDA_TASK_ROOT}\n",
    "\n",
    "# Set the CMD to your handler (file_name.function_name)\n",
    "CMD [\"lambda_handler.handler\"]\n",
    "```\n",
    "\n",
    "```python\n",
    "# app/lambda_handler.py\n",
    "import json\n",
    "import logging\n",
    "from typing import Any, Dict\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "def handler(event: Dict[str, Any], context: Any) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    AWS Lambda handler function.\n",
    "    \n",
    "    Args:\n",
    "        event: API Gateway event or direct invocation payload\n",
    "        context: Lambda runtime context (memory, time remaining, etc.)\n",
    "    \"\"\"\n",
    "    try:\n",
    "        logger.info(f\"Received event: {json.dumps(event)}\")\n",
    "        \n",
    "        # Your processing logic here\n",
    "        result = process_data(event)\n",
    "        \n",
    "        return {\n",
    "            'statusCode': 200,\n",
    "            'body': json.dumps({'result': result}),\n",
    "            'headers': {\n",
    "                'Content-Type': 'application/json'\n",
    "            }\n",
    "        }\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing request: {str(e)}\")\n",
    "        return {\n",
    "            'statusCode': 500,\n",
    "            'body': json.dumps({'error': 'Internal server error'})\n",
    "        }\n",
    "\n",
    "def process_data(event: Dict[str, Any]) -> str:\n",
    "    \"\"\"Business logic here.\"\"\"\n",
    "    return \"processed\"\n",
    "```\n",
    "\n",
    "**Deployment via AWS SAM (Serverless Application Model):**\n",
    "```yaml\n",
    "# template.yaml\n",
    "AWSTemplateFormatVersion: '2010-09-09'\n",
    "Transform: AWS::Serverless-2016-10-31\n",
    "\n",
    "Resources:\n",
    "  PythonFunction:\n",
    "    Type: AWS::Serverless::Function\n",
    "    Properties:\n",
    "      PackageType: Image\n",
    "      ImageUri: your-ecr-repo/image:latest\n",
    "      MemorySize: 1024\n",
    "      Timeout: 30\n",
    "      Events:\n",
    "        ApiEvent:\n",
    "          Type: Api\n",
    "          Properties:\n",
    "            Path: /process\n",
    "            Method: post\n",
    "```\n",
    "\n",
    "### Kubernetes (Container Orchestration)\n",
    "\n",
    "For large-scale applications requiring complex orchestration (auto-scaling, rolling updates, service mesh), Kubernetes is the industry standard. However, for most Python applications, managed services like AWS ECS (Elastic Container Service) or Google Cloud Run provide Kubernetes benefits without operational complexity.\n",
    "\n",
    "```yaml\n",
    "# kubernetes-deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: python-app\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: python-app\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: python-app\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: app\n",
    "        image: myapp:v1.0\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: DATABASE_URL\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: db-secret\n",
    "              key: url\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"256Mi\"\n",
    "            cpu: \"250m\"\n",
    "          limits:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "Containerization and deployment represent the final transformation of Python code from development artifact to running service. You have learned to build secure, optimized Docker images using **multi-stage builds** that minimize attack surfaces and image sizes. You mastered **Docker Compose** for local multi-service orchestration, including database persistence and inter-service networking.\n",
    "\n",
    "Your **CI/CD pipeline** now automatically tests code across Python versions, scans for security vulnerabilities, builds multi-architecture container images, and deploys to staging and production environments with appropriate safeguards. You understand the landscape of deployment targets\u2014from **PaaS** platforms like Render and Railway that optimize developer experience, to **serverless** Lambda functions for event-driven architectures, to **Kubernetes** for enterprise-scale orchestration.\n",
    "\n",
    "With your application now running in production, performance becomes the critical concern. Slow response times and resource exhaustion directly impact user experience and infrastructure costs. The next chapter explores profiling tools, algorithmic optimization, and caching strategies that ensure your Python applications run efficiently under production load.\n",
    "\n",
    "**Next Chapter**: Chapter 21: Performance Optimization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='19. dependency_management.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='21. performance_optimization.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}