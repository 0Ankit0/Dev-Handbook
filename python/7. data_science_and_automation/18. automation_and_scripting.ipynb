{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 18: Automation and Scripting\n",
    "\n",
    "Automation transforms Python from a programming language into a powerful tool for eliminating repetitive tasks, orchestrating system operations, and extracting valuable data from the web. Whether you're a system administrator managing hundreds of servers, a data engineer collecting market intelligence, or a developer streamlining your local workflow, Python's rich standard library and ecosystem provide robust tools for reliable automation.\n",
    "\n",
    "This chapter explores three pillars of Python automation: **system-level scripting** for file and process management, **web scraping** for data extraction, and **task scheduling** for unattended execution. We will emphasize security best practices, error resilience, and modern Python patterns that ensure your automation scripts are maintainable, portable, and production-ready.\n",
    "\n",
    "## 18.1 System Automation: Interacting with the Operating System\n",
    "\n",
    "System automation involves programmatically interacting with the file system, executing external commands, and managing environment configurations. Python provides several modules for these tasks, with modern best practices favoring high-level, path-oriented approaches over legacy string-based file manipulation.\n",
    "\n",
    "### Working with File Paths Using `pathlib`\n",
    "\n",
    "Introduced in Python 3.4 and now the industry standard, `pathlib` provides an object-oriented interface to the file system that is more intuitive and less error-prone than the older `os.path` module.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "from typing import List, Iterator\n",
    "import shutil\n",
    "\n",
    "def organize_downloads(download_dir: Path = Path.home() / \"Downloads\") -> None:\n",
    "    \"\"\"\n",
    "    Organize downloaded files by extension into subdirectories.\n",
    "    \n",
    "    Args:\n",
    "        download_dir: Path to the downloads folder (defaults to ~/Downloads)\n",
    "    \"\"\"\n",
    "    if not download_dir.exists():\n",
    "        raise FileNotFoundError(f\"Directory {download_dir} does not exist\")\n",
    "    \n",
    "    # Create mapping of extensions to folders\n",
    "    extension_map: dict[str, Path] = {\n",
    "        \".pdf\": download_dir / \"Documents\",\n",
    "        \".jpg\": download_dir / \"Images\",\n",
    "        \".png\": download_dir / \"Images\",\n",
    "        \".zip\": download_dir / \"Archives\",\n",
    "        \".mp4\": download_dir / \"Videos\",\n",
    "    }\n",
    "    \n",
    "    # Ensure destination directories exist\n",
    "    for dest_dir in set(extension_map.values()):\n",
    "        dest_dir.mkdir(exist_ok=True)\n",
    "    \n",
    "    # Move files\n",
    "    file_path: Path\n",
    "    for file_path in download_dir.iterdir():\n",
    "        if file_path.is_file():\n",
    "            dest_dir: Path | None = extension_map.get(file_path.suffix.lower())\n",
    "            if dest_dir:\n",
    "                try:\n",
    "                    shutil.move(str(file_path), str(dest_dir / file_path.name))\n",
    "                    print(f\"Moved {file_path.name} to {dest_dir.name}\")\n",
    "                except shutil.Error as e:\n",
    "                    print(f\"Error moving {file_path.name}: {e}\")\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    organize_downloads()\n",
    "```\n",
    "\n",
    "**Key `pathlib` Operations:**\n",
    "*   **Construction**: `Path(\"/usr/bin\")` or `Path.home() / \"Documents\"`\n",
    "*   **Inspection**: `.exists()`, `.is_file()`, `.is_dir()`, `.suffix`, `.stem`, `.name`\n",
    "*   **Navigation**: `.parent`, `.parents[0]`, `.joinpath()`, `/` operator\n",
    "*   **Operations**: `.mkdir()`, `.touch()`, `.unlink()`, `.rename()`, `.glob(\"*.py\")`\n",
    "\n",
    "### File I/O Operations\n",
    "\n",
    "Modern Python emphasizes context managers (`with` statements) for resource management, ensuring files are properly closed even if errors occur.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import json\n",
    "from typing import Any\n",
    "\n",
    "def process_log_file(input_path: Path, output_path: Path) -> dict[str, int]:\n",
    "    \"\"\"\n",
    "    Process a log file and generate error statistics.\n",
    "    \n",
    "    Uses context managers for safe file handling and JSON for structured output.\n",
    "    \"\"\"\n",
    "    error_counts: dict[str, int] = {}\n",
    "    \n",
    "    # Reading with context manager - ensures file closure\n",
    "    try:\n",
    "        with open(input_path, 'r', encoding='utf-8') as infile:\n",
    "            for line_num, line in enumerate(infile, 1):\n",
    "                line = line.strip()\n",
    "                if \"ERROR\" in line:\n",
    "                    error_type: str = line.split(\":\")[1].strip() if \":\" in line else \"Unknown\"\n",
    "                    error_counts[error_type] = error_counts.get(error_type, 0) + 1\n",
    "                    \n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: {input_path} not found\")\n",
    "        raise\n",
    "    except PermissionError:\n",
    "        print(f\"Error: Permission denied for {input_path}\")\n",
    "        raise\n",
    "    \n",
    "    # Writing JSON output with pretty printing\n",
    "    try:\n",
    "        with open(output_path, 'w', encoding='utf-8') as outfile:\n",
    "            json.dump(error_counts, outfile, indent=2, ensure_ascii=False)\n",
    "    except IOError as e:\n",
    "        print(f\"Failed to write output: {e}\")\n",
    "        raise\n",
    "    \n",
    "    return error_counts\n",
    "```\n",
    "\n",
    "### Executing System Commands\n",
    "\n",
    "For executing external programs, modern Python recommends `subprocess` over older modules like `os.system()` due to security and flexibility.\n",
    "\n",
    "```python\n",
    "import subprocess\n",
    "from pathlib import Path\n",
    "from typing import List, CompletedProcess\n",
    "\n",
    "def run_system_backup(source_dir: Path, backup_dir: Path) -> bool:\n",
    "    \"\"\"\n",
    "    Create a compressed backup using system tar command.\n",
    "    \n",
    "    Demonstrates secure subprocess execution with proper error handling.\n",
    "    \"\"\"\n",
    "    if not source_dir.exists():\n",
    "        raise ValueError(f\"Source directory {source_dir} does not exist\")\n",
    "    \n",
    "    backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "    archive_name: Path = backup_dir / f\"{source_dir.name}_backup.tar.gz\"\n",
    "    \n",
    "    # Secure command construction - list prevents shell injection\n",
    "    cmd: List[str] = [\n",
    "        \"tar\", \n",
    "        \"-czf\", \n",
    "        str(archive_name), \n",
    "        \"-C\", \n",
    "        str(source_dir.parent), \n",
    "        str(source_dir.name)\n",
    "    ]\n",
    "    \n",
    "    try:\n",
    "        # capture_output=True captures stdout/stderr\n",
    "        # check=True raises CalledProcessError on non-zero exit\n",
    "        result: CompletedProcess = subprocess.run(\n",
    "            cmd,\n",
    "            capture_output=True,\n",
    "            text=True,\n",
    "            check=True,\n",
    "            timeout=300  # 5 minute timeout\n",
    "        )\n",
    "        print(f\"Backup successful: {archive_name}\")\n",
    "        return True\n",
    "        \n",
    "    except subprocess.CalledProcessError as e:\n",
    "        print(f\"Backup failed with exit code {e.returncode}\")\n",
    "        print(f\"Error: {e.stderr}\")\n",
    "        return False\n",
    "        \n",
    "    except subprocess.TimeoutExpired:\n",
    "        print(\"Backup operation timed out\")\n",
    "        return False\n",
    "\n",
    "# Usage\n",
    "# run_system_backup(Path(\"/home/user/documents\"), Path(\"/backups\"))\n",
    "```\n",
    "\n",
    "**Security Warning:** Never use `shell=True` with user-provided input, as this opens shell injection vulnerabilities. Always pass commands as lists of arguments.\n",
    "\n",
    "## 18.2 Web Scraping: Extracting Data from the Web\n",
    "\n",
    "Web scraping programmatically extracts data from websites. While powerful, it requires technical skill, legal awareness, and ethical responsibility. Modern scraping emphasizes respecting robots.txt, rate limiting, and using APIs when available.\n",
    "\n",
    "### HTTP Requests with `requests`\n",
    "\n",
    "The `requests` library is the industry standard for HTTP operations, offering a clean API over Python's standard `urllib`.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from typing import Optional, Dict, Any\n",
    "import time\n",
    "\n",
    "class WebScraper:\n",
    "    \"\"\"\n",
    "    Responsible web scraper with rate limiting and error handling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delay: float = 1.0, timeout: int = 30) -> None:\n",
    "        \"\"\"\n",
    "        Initialize scraper with politeness settings.\n",
    "        \n",
    "        Args:\n",
    "            delay: Seconds between requests (rate limiting)\n",
    "            timeout: Request timeout in seconds\n",
    "        \"\"\"\n",
    "        self.delay: float = delay\n",
    "        self.timeout: int = timeout\n",
    "        self.session: requests.Session = requests.Session()\n",
    "        \n",
    "        # Set headers to identify yourself and accept JSON/HTML\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (compatible; MyBot/1.0; +http://mysite.com/bot)',\n",
    "            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,*/*;q=0.8',\n",
    "        })\n",
    "    \n",
    "    def fetch(self, url: str) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Fetch HTML content from URL with error handling and rate limiting.\n",
    "        \n",
    "        Args:\n",
    "            url: Target URL\n",
    "            \n",
    "        Returns:\n",
    "            HTML content or None if failed\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Respect rate limiting\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            response: requests.Response = self.session.get(\n",
    "                url, \n",
    "                timeout=self.timeout\n",
    "            )\n",
    "            \n",
    "            # Check for HTTP errors (4xx, 5xx)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            return response.text\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            print(f\"HTTP Error for {url}: {e}\")\n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(f\"Connection Error for {url}\")\n",
    "        except requests.exceptions.Timeout:\n",
    "            print(f\"Timeout Error for {url}\")\n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed for {url}: {e}\")\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    def fetch_json(self, url: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"Fetch and parse JSON API response.\"\"\"\n",
    "        html: Optional[str] = self.fetch(url)\n",
    "        if html:\n",
    "            try:\n",
    "                return self.session.get(url).json()\n",
    "            except ValueError:\n",
    "                print(f\"Invalid JSON from {url}\")\n",
    "        return None\n",
    "    \n",
    "    def close(self) -> None:\n",
    "        \"\"\"Clean up session resources.\"\"\"\n",
    "        self.session.close()\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    scraper: WebScraper = WebScraper(delay=2.0)  # Be polite, wait 2 seconds\n",
    "    \n",
    "    # Example: Fetch a page (replace with actual URL)\n",
    "    # content: Optional[str] = scraper.fetch(\"https://example.com\")\n",
    "    \n",
    "    scraper.close()\n",
    "```\n",
    "\n",
    "**Ethical Scraping Guidelines:**\n",
    "1.  **Check `robots.txt`**: Always verify `/robots.txt` to see which paths are allowed.\n",
    "2.  **Rate Limiting**: Never hammer servers. Use `time.sleep()` between requests.\n",
    "3.  **User-Agent**: Identify yourself so webmasters can contact you if issues arise.\n",
    "4.  **Terms of Service**: Review website ToS; some prohibit scraping entirely.\n",
    "5.  **APIs First**: Always prefer official APIs over scraping HTML.\n",
    "\n",
    "### HTML Parsing with BeautifulSoup\n",
    "\n",
    "Once you fetch HTML, you need to extract specific data. `BeautifulSoup` (from the `bs4` package) is the standard library for parsing HTML and XML documents.\n",
    "\n",
    "```python\n",
    "from bs4 import BeautifulSoup, Tag\n",
    "from typing import List, Optional, Dict\n",
    "import requests\n",
    "\n",
    "class ProductExtractor:\n",
    "    \"\"\"\n",
    "    Extract product information from e-commerce HTML.\n",
    "    Demonstrates BeautifulSoup parsing techniques.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, html_content: str) -> None:\n",
    "        \"\"\"\n",
    "        Initialize parser with HTML content.\n",
    "        \n",
    "        Args:\n",
    "            html_content: Raw HTML string to parse\n",
    "        \"\"\"\n",
    "        # Use 'lxml' parser for speed (requires: pip install lxml)\n",
    "        # Fallback to 'html.parser' (built-in) if lxml not available\n",
    "        self.soup: BeautifulSoup = BeautifulSoup(html_content, 'lxml')\n",
    "    \n",
    "    def extract_title(self) -> Optional[str]:\n",
    "        \"\"\"\n",
    "        Extract page title.\n",
    "        \n",
    "        Returns:\n",
    "            Title text or None if not found\n",
    "        \"\"\"\n",
    "        # Find returns the first match or None\n",
    "        title_tag: Optional[Tag] = self.soup.find('title')\n",
    "        return title_tag.get_text(strip=True) if title_tag else None\n",
    "    \n",
    "    def extract_products(self) -> List[Dict[str, str]]:\n",
    "        \"\"\"\n",
    "        Extract all products from a listing page.\n",
    "        Assumes HTML structure with class names.\n",
    "        \n",
    "        Returns:\n",
    "            List of product dictionaries\n",
    "        \"\"\"\n",
    "        products: List[Dict[str, str]] = []\n",
    "        \n",
    "        # find_all returns a list of all matching tags\n",
    "        # Using CSS selectors via select() is often more powerful\n",
    "        product_cards: List[Tag] = self.soup.find_all('div', class_='product-card')\n",
    "        \n",
    "        for card in product_cards:\n",
    "            try:\n",
    "                # Navigate the DOM tree\n",
    "                name_tag: Optional[Tag] = card.find('h2', class_='product-name')\n",
    "                price_tag: Optional[Tag] = card.find('span', class_='price')\n",
    "                link_tag: Optional[Tag] = card.find('a')\n",
    "                \n",
    "                if name_tag and price_tag:\n",
    "                    product: Dict[str, str] = {\n",
    "                        'name': name_tag.get_text(strip=True),\n",
    "                        'price': price_tag.get_text(strip=True),\n",
    "                        'url': link_tag['href'] if link_tag and link_tag.has_attr('href') else ''\n",
    "                    }\n",
    "                    products.append(product)\n",
    "                    \n",
    "            except Exception as e:\n",
    "                # Log error but continue processing other products\n",
    "                print(f\"Error parsing product: {e}\")\n",
    "                continue\n",
    "        \n",
    "        return products\n",
    "    \n",
    "    def extract_tables(self) -> List[List[List[str]]]:\n",
    "        \"\"\"\n",
    "        Extract all HTML tables as structured data.\n",
    "        \n",
    "        Returns:\n",
    "            List of tables, each being a list of rows (lists of cell texts)\n",
    "        \"\"\"\n",
    "        tables: List[List[List[str]]] = []\n",
    "        \n",
    "        for table in self.soup.find_all('table'):\n",
    "            table_data: List[List[str]] = []\n",
    "            \n",
    "            # Handle both <thead> and <tbody> rows\n",
    "            rows: List[Tag] = table.find_all('tr')\n",
    "            \n",
    "            for row in rows:\n",
    "                # Extract all cell types (th and td)\n",
    "                cells: List[Tag] = row.find_all(['td', 'th'])\n",
    "                row_data: List[str] = [cell.get_text(strip=True) for cell in cells]\n",
    "                \n",
    "                if row_data:  # Skip empty rows\n",
    "                    table_data.append(row_data)\n",
    "            \n",
    "            if table_data:\n",
    "                tables.append(table_data)\n",
    "        \n",
    "        return tables\n",
    "\n",
    "# Practical Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Example HTML (in real use, this comes from requests.get().text)\n",
    "    sample_html: str = \"\"\"\n",
    "    <html>\n",
    "        <head><title>Tech Store</title></head>\n",
    "        <body>\n",
    "            <div class=\"product-card\">\n",
    "                <h2 class=\"product-name\">Laptop Pro</h2>\n",
    "                <span class=\"price\">$999</span>\n",
    "                <a href=\"/products/laptop-pro\">View</a>\n",
    "            </div>\n",
    "            <div class=\"product-card\">\n",
    "                <h2 class=\"product-name\">Wireless Mouse</h2>\n",
    "                <span class=\"price\">$29</span>\n",
    "                <a href=\"/products/mouse\">View</a>\n",
    "            </div>\n",
    "        </body>\n",
    "    </html>\n",
    "    \"\"\"\n",
    "    \n",
    "    extractor: ProductExtractor = ProductExtractor(sample_html)\n",
    "    \n",
    "    # Extract title\n",
    "    title: Optional[str] = extractor.extract_title()\n",
    "    print(f\"Page Title: {title}\")\n",
    "    \n",
    "    # Extract products\n",
    "    products: List[Dict[str, str]] = extractor.extract_products()\n",
    "    for product in products:\n",
    "        print(f\"Product: {product['name']} - {product['price']}\")\n",
    "```\n",
    "\n",
    "**BeautifulSoup Best Practices:**\n",
    "1.  **Always specify a parser**: `'lxml'` is fastest, `'html.parser'` is built-in, `'html5lib'` is most lenient with broken HTML.\n",
    "2.  **Use `get_text(strip=True)`**: Removes leading/trailing whitespace and normalizes text extraction.\n",
    "3.  **Check for None**: `find()` returns `None` if no match; always check before accessing attributes.\n",
    "4.  **CSS Selectors**: Use `.select()` for complex queries (e.g., `soup.select(\"div.content > p:first-child\")`).\n",
    "\n",
    "## 18.3 Scheduling: Automating Task Execution\n",
    "\n",
    "Automation requires not just performing tasks, but performing them at specific times or intervals. Python offers several approaches from simple in-process schedulers to enterprise-grade job queues.\n",
    "\n",
    "### Simple Scheduling with `schedule`\n",
    "\n",
    "The `schedule` library (third-party, `pip install schedule`) provides a readable, human-friendly API for lightweight automation scripts that run continuously.\n",
    "\n",
    "```python\n",
    "import schedule\n",
    "import time\n",
    "import logging\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "from typing import Callable\n",
    "\n",
    "# Configure logging for production visibility\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('automation.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger: logging.Logger = logging.getLogger(__name__)\n",
    "\n",
    "class DataPipeline:\n",
    "    \"\"\"\n",
    "    Automated data processing pipeline with scheduling capabilities.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_dir: Path, backup_dir: Path) -> None:\n",
    "        self.source_dir: Path = source_dir\n",
    "        self.backup_dir: Path = backup_dir\n",
    "        self.processed_count: int = 0\n",
    "    \n",
    "    def backup_data(self) -> None:\n",
    "        \"\"\"Task 1: Backup critical data.\"\"\"\n",
    "        try:\n",
    "            timestamp: str = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            backup_path: Path = self.backup_dir / f\"backup_{timestamp}\"\n",
    "            \n",
    "            if self.source_dir.exists():\n",
    "                shutil.copytree(self.source_dir, backup_path)\n",
    "                logger.info(f\"Backup completed: {backup_path}\")\n",
    "            else:\n",
    "                logger.warning(f\"Source directory {self.source_dir} does not exist\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Backup failed: {e}\", exc_info=True)\n",
    "    \n",
    "    def generate_report(self) -> None:\n",
    "        \"\"\"Task 2: Generate daily analytics report.\"\"\"\n",
    "        try:\n",
    "            logger.info(\"Generating daily report...\")\n",
    "            # Simulate report generation\n",
    "            report_path: Path = Path(\"daily_report.txt\")\n",
    "            report_path.write_text(f\"Report generated at {datetime.now()}\")\n",
    "            self.processed_count += 1\n",
    "            logger.info(f\"Report saved to {report_path}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Report generation failed: {e}\")\n",
    "\n",
    "def setup_scheduler(pipeline: DataPipeline) -> None:\n",
    "    \"\"\"\n",
    "    Configure the job scheduler with various timing patterns.\n",
    "    \n",
    "    Args:\n",
    "        pipeline: The DataPipeline instance to schedule tasks for\n",
    "    \"\"\"\n",
    "    # Daily backup at 2:00 AM\n",
    "    schedule.every().day.at(\"02:00\").do(pipeline.backup_data)\n",
    "    \n",
    "    # Report generation every 30 minutes during business hours\n",
    "    schedule.every(30).minutes.do(pipeline.generate_report)\n",
    "    \n",
    "    # Weekly full cleanup on Sundays at 3:00 AM\n",
    "    schedule.every().sunday.at(\"03:00\").do(lambda: logger.info(\"Weekly maintenance\"))\n",
    "    \n",
    "    # Specific time with tags for identification\n",
    "    schedule.every().hour.do(\n",
    "        lambda: logger.info(\"Hourly health check\")\n",
    "    ).tag(\"monitoring\", \"health\")\n",
    "\n",
    "def run_scheduler() -> None:\n",
    "    \"\"\"Main loop to keep the scheduler running.\"\"\"\n",
    "    logger.info(\"Starting automation scheduler...\")\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Run pending jobs\n",
    "            schedule.run_pending()\n",
    "            \n",
    "            # Sleep to prevent CPU spinning (minimum 1 second)\n",
    "            time.sleep(1)\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"Scheduler stopped by user\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Scheduler error: {e}\")\n",
    "            # Continue running despite errors\n",
    "            time.sleep(5)\n",
    "\n",
    "# Production setup\n",
    "if __name__ == \"__main__\":\n",
    "    pipeline: DataPipeline = DataPipeline(\n",
    "        source_dir=Path(\"/data/source\"),\n",
    "        backup_dir=Path(\"/data/backups\")\n",
    "    )\n",
    "    \n",
    "    setup_scheduler(pipeline)\n",
    "    run_scheduler()\n",
    "```\n",
    "\n",
    "**Key Scheduling Patterns:**\n",
    "*   **Intervals**: `every(10).minutes`, `every(2).hours`, `every().monday`\n",
    "*   **Specific times**: `every().day.at(\"10:30\")` (24h format)\n",
    "*   **Tagging**: `.tag(\"maintenance\")` allows canceling specific job types: `schedule.clear('maintenance')`\n",
    "\n",
    "**Production Considerations:**\n",
    "The `schedule` library is suitable for long-running single-process scripts but lacks persistence (jobs are lost if the script crashes). For mission-critical automation, use **APScheduler** (Advanced Python Scheduler) or **Celery** with a message broker.\n",
    "\n",
    "### Enterprise Scheduling with APScheduler\n",
    "\n",
    "For production environments requiring persistence, multiple job stores, and distributed execution:\n",
    "\n",
    "```python\n",
    "from apscheduler.schedulers.background import BackgroundScheduler\n",
    "from apscheduler.triggers.cron import CronTrigger\n",
    "from apscheduler.triggers.interval import IntervalTrigger\n",
    "from apscheduler.events import EVENT_JOB_ERROR, EVENT_JOB_EXECUTED\n",
    "from datetime import datetime\n",
    "import logging\n",
    "\n",
    "logging.basicConfig()\n",
    "logging.getLogger('apscheduler').setLevel(logging.DEBUG)\n",
    "\n",
    "def job_listener(event):\n",
    "    \"\"\"Callback for job events.\"\"\"\n",
    "    if event.exception:\n",
    "        print(f\"Job crashed: {event.job_id}\")\n",
    "    else:\n",
    "        print(f\"Job executed successfully: {event.job_id}\")\n",
    "\n",
    "# Background scheduler (doesn't block main thread)\n",
    "scheduler: BackgroundScheduler = BackgroundScheduler()\n",
    "\n",
    "# Add listener\n",
    "scheduler.add_listener(job_listener, EVENT_JOB_ERROR | EVENT_JOB_EXECUTED)\n",
    "\n",
    "# Cron trigger (Unix cron-like scheduling)\n",
    "# Run at 9:30 AM on Mondays and Wednesdays\n",
    "scheduler.add_job(\n",
    "    lambda: print(\"Weekly report triggered\"),\n",
    "    trigger=CronTrigger(day_of_week='mon,wed', hour=9, minute=30),\n",
    "    id='weekly_report',\n",
    "    replace_existing=True\n",
    ")\n",
    "\n",
    "# Interval trigger with jitter to prevent thundering herd\n",
    "scheduler.add_job(\n",
    "    lambda: print(\"Health check\"),\n",
    "    trigger=IntervalTrigger(minutes=5, jitter=60),  # Â±60 seconds randomization\n",
    "    id='health_check'\n",
    ")\n",
    "\n",
    "# Start scheduler\n",
    "scheduler.start()\n",
    "\n",
    "# Keep main thread alive\n",
    "try:\n",
    "    while True:\n",
    "        time.sleep(1)\n",
    "except KeyboardInterrupt:\n",
    "    scheduler.shutdown()\n",
    "```\n",
    "\n",
    "**APScheduler Features:**\n",
    "*   **Persistence**: Store jobs in SQLAlchemy databases, Redis, or MongoDB\n",
    "*   **Distributed**: Multiple workers can pull from a shared job store\n",
    "*   **Triggers**: Cron-like, interval-based, or date-based execution\n",
    "*   **Job Management**: Add, remove, pause, and reschedule jobs dynamically\n",
    "\n",
    "## 18.3 Advanced Automation Patterns\n",
    "\n",
    "### Error Resilience and Idempotency\n",
    "\n",
    "Production automation must handle failures gracefully and be **idempotent** (running the script multiple times produces the same result as running it once).\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import hashlib\n",
    "import pickle\n",
    "from typing import Set\n",
    "from functools import wraps\n",
    "\n",
    "class IdempotentProcessor:\n",
    "    \"\"\"\n",
    "    Ensures files are processed only once using state tracking.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, state_file: Path = Path(\".processor_state\")) -> None:\n",
    "        self.state_file: Path = state_file\n",
    "        self.processed_hashes: Set[str] = self._load_state()\n",
    "    \n",
    "    def _load_state(self) -> Set[str]:\n",
    "        \"\"\"Load set of processed file hashes.\"\"\"\n",
    "        if self.state_file.exists():\n",
    "            with open(self.state_file, 'rb') as f:\n",
    "                return pickle.load(f)\n",
    "        return set()\n",
    "    \n",
    "    def _save_state(self) -> None:\n",
    "        \"\"\"Persist state to disk.\"\"\"\n",
    "        with open(self.state_file, 'wb') as f:\n",
    "            pickle.dump(self.processed_hashes, f)\n",
    "    \n",
    "    def _get_file_hash(self, filepath: Path) -> str:\n",
    "        \"\"\"Generate MD5 hash of file content.\"\"\"\n",
    "        hasher = hashlib.md5()\n",
    "        with open(filepath, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hasher.update(chunk)\n",
    "        return hasher.hexdigest()\n",
    "    \n",
    "    def process_file(self, filepath: Path) -> bool:\n",
    "        \"\"\"\n",
    "        Process file only if not previously processed.\n",
    "        \n",
    "        Returns:\n",
    "            True if processed, False if skipped (already processed)\n",
    "        \"\"\"\n",
    "        file_hash: str = self._get_file_hash(filepath)\n",
    "        \n",
    "        if file_hash in self.processed_hashes:\n",
    "            print(f\"Skipping {filepath} (already processed)\")\n",
    "            return False\n",
    "        \n",
    "        # Process the file here\n",
    "        print(f\"Processing {filepath}...\")\n",
    "        \n",
    "        # Mark as processed and save state\n",
    "        self.processed_hashes.add(file_hash)\n",
    "        self._save_state()\n",
    "        return True\n",
    "\n",
    "# Usage\n",
    "processor: IdempotentProcessor = IdempotentProcessor()\n",
    "for file in Path(\"/data/incoming\").glob(\"*.csv\"):\n",
    "    processor.process_file(file)\n",
    "```\n",
    "\n",
    "### Context Managers for Resource Safety\n",
    "\n",
    "Automation scripts often acquire resources (files, network connections, locks) that must be released. Context managers ensure cleanup happens even if errors occur.\n",
    "\n",
    "```python\n",
    "from contextlib import contextmanager\n",
    "from typing import Generator, Optional\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "@contextmanager\n",
    "def managed_temp_directory() -> Generator[Path, None, None]:\n",
    "    \"\"\"\n",
    "    Context manager that creates a temp directory and cleans it up.\n",
    "    Yields the path, then deletes it regardless of exceptions.\n",
    "    \"\"\"\n",
    "    temp_dir: Path = Path(tempfile.mkdtemp())\n",
    "    print(f\"Created temp directory: {temp_dir}\")\n",
    "    \n",
    "    try:\n",
    "        yield temp_dir\n",
    "    finally:\n",
    "        # Cleanup happens even if exception occurred\n",
    "        shutil.rmtree(temp_dir)\n",
    "        print(f\"Cleaned up temp directory: {temp_dir}\")\n",
    "\n",
    "# Usage\n",
    "with managed_temp_directory() as tmp:\n",
    "    # Work with temporary files\n",
    "    data_file: Path = tmp / \"data.txt\"\n",
    "    data_file.write_text(\"Sensitive temporary data\")\n",
    "    # Process data...\n",
    "    print(f\"File exists: {data_file.exists()}\")\n",
    "# After exiting 'with' block, directory is deleted\n",
    "```\n",
    "\n",
    "## Summary\n",
    "\n",
    "This chapter equipped you with the tools to automate the world around you using Python. You learned to navigate the file system safely using **`pathlib`**, execute system commands securely with **`subprocess`**, and build resilient automation loops with **`schedule`** and **APScheduler**.\n",
    "\n",
    "In web scraping, you mastered the **`requests`** library for HTTP communication and **BeautifulSoup** for HTML parsing, while understanding the critical importance of rate limiting, user-agent identification, and legal compliance. You explored advanced patterns including **idempotent processing** to prevent duplicate work and **context managers** for resource-safe automation.\n",
    "\n",
    "As you move from writing scripts to building distributable applications, managing dependencies becomes crucial. The next chapter explores modern Python dependency management, virtual environments, and packaging standards that ensure your automation scripts and applications run consistently across development, testing, and production environments.\n",
    "\n",
    "**Next Chapter**: Chapter 19: Dependency Management and Virtual Environments."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
