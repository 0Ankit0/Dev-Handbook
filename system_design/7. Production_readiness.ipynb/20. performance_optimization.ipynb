{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "70be1ba2",
   "metadata": {},
   "source": [
    "# **Chapter 20: Performance Optimization**\n",
    "\n",
    "Performance optimization is the art of transforming systems that \"work\" into systems that \"fly.\" Unlike premature optimization\u2014guessing at bottlenecks\u2014systematic performance engineering relies on measurement, profiling, and methodical elimination of constraints. This chapter provides the tools and strategies to diagnose and resolve performance issues at every layer of the stack.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.1 The Performance Engineering Methodology**\n",
    "\n",
    "Before writing a single line of optimized code, establish a rigorous methodology. Random optimization without measurement inevitably wastes effort on non-bottlenecks.\n",
    "\n",
    "### **The Golden Rule: Measure First**\n",
    "\n",
    "**Amdahl's Law**: The speedup of a system is limited by the fraction of time spent in the improved component.\n",
    "\n",
    "```\n",
    "If a program spends 90% of time in function A and 10% in function B:\n",
    "- Optimizing B by 100% (making it instant) yields only 10% total speedup\n",
    "- Optimizing A by 50% yields 45% total speedup\n",
    "```\n",
    "\n",
    "**The 80/20 Rule**: 80% of execution time is spent in 20% of the code. Your job is to find that 20%.\n",
    "\n",
    "**The Methodology**:\n",
    "1. **Establish Baseline**: Measure current performance (latency, throughput, resource usage)\n",
    "2. **Profile**: Identify bottlenecks (CPU, memory, I/O, network)\n",
    "3. **Hypothesize**: Form theory about root cause\n",
    "4. **Optimize**: Implement targeted fix\n",
    "5. **Verify**: Measure again to confirm improvement\n",
    "6. **Iterate**: Return to step 2 until requirements met\n",
    "\n",
    "**Anti-Patterns**:\n",
    "- **Premature Optimization**: Optimizing code that isn't measured as slow\n",
    "- **Macro-Optimization**: Focusing on micro-benchmarks while architectural flaws dominate\n",
    "- **Optimization Without Constraints**: \"Make it faster\" without defined targets (e.g., \"P99 < 100ms\")\n",
    "\n",
    "---\n",
    "\n",
    "## **20.2 Profiling and Benchmarking**\n",
    "\n",
    "### **CPU Profiling**\n",
    "\n",
    "CPU profilers sample the call stack at regular intervals to show where time is spent.\n",
    "\n",
    "**Sampling vs. Instrumentation**:\n",
    "- **Sampling** (e.g., `perf`, `async-profiler`): Low overhead (~1%), statistical accuracy, good for production\n",
    "- **Instrumentation** (e.g., code timers): High overhead, exact counts, good for specific functions\n",
    "\n",
    "**Flame Graphs** (Visualizing CPU Usage):\n",
    "```\n",
    "Interpretation:\n",
    "- Width = Time spent (wider = more time)\n",
    "- Height = Call stack depth\n",
    "- Colors = Random (or by type)\n",
    "  \n",
    "Example:\n",
    "[\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]  <- main() (100%)\n",
    "  [\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]    <- handleRequest() (80%)\n",
    "    [\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]      <- parseJSON() (40%)\n",
    "    [\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]      <- validateInput() (20%)\n",
    "  [\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591]    <- logging (20%)\n",
    "```\n",
    "\n",
    "**Action**: If `parseJSON()` is 40% of total time, optimize JSON parsing (faster library, schema validation, or binary formats).\n",
    "\n",
    "**Tools by Language**:\n",
    "- **Java**: `async-profiler` (production-safe), JProfiler, Java Flight Recorder\n",
    "- **Python**: `cProfile`, `py-spy` (sampling), `line_profiler`\n",
    "- **Go**: `pprof` (built-in), `trace`\n",
    "- **Node.js**: `clinic.js`, `0x`, Chrome DevTools\n",
    "\n",
    "**Example: Python Profiling**:\n",
    "```python\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "def slow_function():\n",
    "    result = []\n",
    "    for i in range(1000000):\n",
    "        result.append(i * 2)\n",
    "    return result\n",
    "\n",
    "# Profile\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "slow_function()\n",
    "profiler.disable()\n",
    "\n",
    "# Print stats\n",
    "stats = pstats.Stats(profiler)\n",
    "stats.sort_stats('cumulative')\n",
    "stats.print_stats(10)  # Top 10 functions\n",
    "\n",
    "# Output interpretation:\n",
    "# ncalls  tottime  percall  cumtime  filename:lineno(function)\n",
    "#      1    0.123    0.123    0.456  script.py:1(slow_function)\n",
    "#1000000    0.200    0.000    0.200  {method 'append' of 'list' objects}\n",
    "```\n",
    "\n",
    "**Optimization**: The `append` in loop is slow. Pre-allocate list or use list comprehension:\n",
    "```python\n",
    "# Optimized: 10x faster\n",
    "def fast_function():\n",
    "    return [i * 2 for i in range(1000000)]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Memory Profiling**\n",
    "\n",
    "Memory issues manifest as:\n",
    "- **High memory usage**: Costly (RAM is expensive)\n",
    "- **Memory leaks**: Growth until OOM (Out Of Memory) crash\n",
    "- **GC pressure**: Frequent garbage collection pauses\n",
    "\n",
    "**Heap Dumps** (Java):\n",
    "```bash\n",
    "# Generate heap dump\n",
    "jmap -dump:format=b,file=heap.hprof <pid>\n",
    "\n",
    "# Analyze with Eclipse MAT or VisualVM\n",
    "# Look for:\n",
    "# - Retained heap size (objects keeping others alive)\n",
    "# - Duplicate strings (interning opportunities)\n",
    "# - Collection sizes (oversized HashMaps, ArrayLists)\n",
    "```\n",
    "\n",
    "**Memory Profiling Patterns**:\n",
    "1. **Object Churn**: Creating millions of temporary objects\n",
    "   - *Fix*: Object pooling, reuse buffers\n",
    "   \n",
    "2. **Retained Memory**: Caching without eviction\n",
    "   - *Fix*: LRU caches, weak references\n",
    "   \n",
    "3. **Memory Leaks**: Unclosed resources, static collections growing forever\n",
    "   - *Fix*: Try-with-resources, bounded collections\n",
    "\n",
    "**Example: Java Memory Leak**:\n",
    "```java\n",
    "// BAD: Static collection grows unbounded\n",
    "public class Cache {\n",
    "    private static Map<String, Object> map = new HashMap<>();\n",
    "    \n",
    "    public void add(String key, Object value) {\n",
    "        map.put(key, value);  // Never removed!\n",
    "    }\n",
    "}\n",
    "\n",
    "// GOOD: Bounded, expiring cache\n",
    "public class Cache {\n",
    "    private Cache<String, Object> cache = Caffeine.newBuilder()\n",
    "        .maximumSize(1000)\n",
    "        .expireAfterWrite(10, TimeUnit.MINUTES)\n",
    "        .build();\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Load Testing**\n",
    "\n",
    "**Tools**: `k6`, `JMeter`, `Gatling`, `Locust`, `wrk2`\n",
    "\n",
    "**Methodology**:\n",
    "1. **Baseline Test**: Single user, measure ideal latency\n",
    "2. **Load Test**: Expected traffic (e.g., 1000 concurrent users)\n",
    "3. **Stress Test**: Beyond capacity until breakage (find limit)\n",
    "4. **Spike Test**: Sudden traffic surge (e.g., flash sale)\n",
    "5. **Soak Test**: Extended duration (find memory leaks)\n",
    "\n",
    "**Key Metrics**:\n",
    "- **Throughput**: Requests/second\n",
    "- **Latency Distribution**: P50, P95, P99, P99.9, Max\n",
    "- **Error Rate**: % of failed requests\n",
    "- **Resource Utilization**: CPU, Memory, Disk I/O, Network\n",
    "\n",
    "**Example: k6 Load Test**:\n",
    "```javascript\n",
    "import http from 'k6/http';\n",
    "import { check, sleep } from 'k6';\n",
    "\n",
    "export let options = {\n",
    "  stages: [\n",
    "    { duration: '2m', target: 100 },  // Ramp up\n",
    "    { duration: '5m', target: 100 },  // Steady state\n",
    "    { duration: '2m', target: 200 },  // Stress\n",
    "    { duration: '2m', target: 0 },    // Ramp down\n",
    "  ],\n",
    "  thresholds: {\n",
    "    http_req_duration: ['p(95)<200'], // 95% under 200ms\n",
    "    http_req_failed: ['rate<0.01'],   // Error rate < 1%\n",
    "  },\n",
    "};\n",
    "\n",
    "export default function() {\n",
    "  let res = http.get('https://api.example.com/users');\n",
    "  check(res, {\n",
    "    'status is 200': (r) => r.status === 200,\n",
    "    'response time < 200ms': (r) => r.timings.duration < 200,\n",
    "  });\n",
    "  sleep(1);\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **20.3 Database Query Optimization**\n",
    "\n",
    "Databases are the most common bottleneck in distributed systems. Optimizing them yields outsized returns.\n",
    "\n",
    "### **Query Analysis**\n",
    "\n",
    "**The Execution Plan** (EXPLAIN):\n",
    "```sql\n",
    "EXPLAIN ANALYZE SELECT * FROM orders \n",
    "WHERE user_id = 12345 \n",
    "AND created_at > '2024-01-01'\n",
    "ORDER BY created_at DESC \n",
    "LIMIT 10;\n",
    "```\n",
    "\n",
    "**Reading PostgreSQL EXPLAIN**:\n",
    "```\n",
    "Index Scan using idx_user_created on orders (cost=0.56..123.45 rows=50 width=200) (actual time=0.023..0.456 rows=10 loops=1)\n",
    "  Index Cond: ((user_id = 12345) AND (created_at > '2024-01-01'::date))\n",
    "  Heap Fetches: 10\n",
    "Planning Time: 0.123 ms\n",
    "Execution Time: 0.567 ms\n",
    "```\n",
    "\n",
    "**Red Flags**:\n",
    "- **Seq Scan**: Table scan (slow on large tables)\n",
    "- **High cost**: Estimated resource usage\n",
    "- **High rows**: Processing millions of rows for small result\n",
    "\n",
    "### **Indexing Strategies**\n",
    "\n",
    "**B-Tree Indexes** (Default, good for equality and range):\n",
    "```sql\n",
    "-- Composite index for the query above\n",
    "CREATE INDEX idx_user_created ON orders(user_id, created_at DESC);\n",
    "\n",
    "-- How it works:\n",
    "-- B-Tree structure: Root -> Intermediate -> Leaf nodes\n",
    "-- Leaf nodes contain (user_id, created_at, pointer_to_row)\n",
    "-- Range scan: Find first match, then traverse sequentially\n",
    "```\n",
    "\n",
    "**Covering Indexes** (Index-only scans):\n",
    "```sql\n",
    "-- If query only selects user_id and total_amount:\n",
    "CREATE INDEX idx_user_amount ON orders(user_id, total_amount);\n",
    "\n",
    "-- Index \"covers\" the query - no need to visit table heap\n",
    "-- Much faster: No random I/O to fetch rows\n",
    "```\n",
    "\n",
    "**Index Selectivity**:\n",
    "```sql\n",
    "-- BAD: Index on boolean (low selectivity - 50% of rows)\n",
    "CREATE INDEX idx_active ON users(is_active);  -- Useless if 50% active\n",
    "\n",
    "-- GOOD: Index on high-cardinality column\n",
    "CREATE INDEX idx_email ON users(email);  -- Unique, high selectivity\n",
    "```\n",
    "\n",
    "**Partial Indexes** (Index subset of table):\n",
    "```sql\n",
    "-- Only index unpaid orders (hot data)\n",
    "CREATE INDEX idx_unpaid_orders ON orders(user_id) WHERE status = 'unpaid';\n",
    "-- Smaller index, faster queries for active orders\n",
    "```\n",
    "\n",
    "### **Query Rewriting**\n",
    "\n",
    "**N+1 Problem**:\n",
    "```python\n",
    "# BAD: N+1 queries\n",
    "users = db.query(\"SELECT * FROM users LIMIT 100\")\n",
    "for user in users:\n",
    "    orders = db.query(f\"SELECT * FROM orders WHERE user_id = {user.id}\")  # 100 queries!\n",
    "\n",
    "# GOOD: Single JOIN query\n",
    "query = \"\"\"\n",
    "SELECT u.*, o.id as order_id, o.total \n",
    "FROM users u \n",
    "LEFT JOIN orders o ON u.id = o.user_id \n",
    "WHERE u.id IN (SELECT id FROM users LIMIT 100)\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Pagination Optimization**:\n",
    "```sql\n",
    "-- BAD: OFFSET is slow (scans and discards rows)\n",
    "SELECT * FROM orders ORDER BY id LIMIT 10 OFFSET 1000000;\n",
    "\n",
    "-- GOOD: Keyset pagination (cursor-based)\n",
    "SELECT * FROM orders \n",
    "WHERE id > 12345678  -- Last seen ID from previous page\n",
    "ORDER BY id \n",
    "LIMIT 10;\n",
    "-- O(log n) seek time vs O(offset) scan time\n",
    "```\n",
    "\n",
    "**Batching**:\n",
    "```sql\n",
    "-- BAD: Individual inserts (1000 round trips)\n",
    "INSERT INTO logs VALUES (...);\n",
    "INSERT INTO logs VALUES (...);\n",
    "-- ...\n",
    "\n",
    "-- GOOD: Batch insert (1 round trip)\n",
    "INSERT INTO logs VALUES (...), (...), (...);  -- 1000 rows\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **20.4 Caching Strategy Optimization**\n",
    "\n",
    "Caching is the second most effective optimization (after fixing database queries). But bad caching causes complexity and consistency nightmares.\n",
    "\n",
    "### **The Caching Hierarchy**\n",
    "\n",
    "```\n",
    "Speed:    Fastest \u25c4\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u25ba Slowest\n",
    "          L1 CPU \u25c4\u25ba L2 CPU \u25c4\u25ba RAM \u25c4\u25ba Local SSD \u25c4\u25ba Network \u25c4\u25ba Database\n",
    "Latency:  1ns     10ns      100ns   100\u03bcs        500\u03bcs      10ms\n",
    "\n",
    "Strategy: Compute \u25c4\u25ba Local Cache \u25c4\u25ba Distributed Cache \u25c4\u25ba Database\n",
    "```\n",
    "\n",
    "### **Cache Access Patterns**\n",
    "\n",
    "**Cache-Aside (Lazy Loading)**:\n",
    "```\n",
    "Read:\n",
    "  1. Check cache\n",
    "  2. If miss: Read DB, populate cache, return data\n",
    "\n",
    "Write:\n",
    "  1. Write to DB\n",
    "  2. Invalidate cache (or update if atomic)\n",
    "\n",
    "Pros: Simple, cache doesn't block on DB failure\n",
    "Cons: Cold start (empty cache), thundering herd on miss\n",
    "```\n",
    "\n",
    "**Write-Through**:\n",
    "```\n",
    "Write:\n",
    "  1. Write to cache\n",
    "  2. Synchronously write to DB\n",
    "  3. Acknowledge write\n",
    "\n",
    "Pros: Strong consistency, no stale data\n",
    "Cons: Higher write latency (2 hops), cache churn on write-heavy workloads\n",
    "```\n",
    "\n",
    "**Write-Behind (Write-Back)**:\n",
    "```\n",
    "Write:\n",
    "  1. Write to cache, acknowledge immediately\n",
    "  2. Async write to DB (queue)\n",
    "\n",
    "Pros: Low write latency, high write throughput\n",
    "Cons: Data loss risk if cache dies before DB write, eventual consistency\n",
    "```\n",
    "\n",
    "### **Cache Optimization Techniques**\n",
    "\n",
    "**Serializing with Protocol Buffers**:\n",
    "```python\n",
    "# BAD: JSON (verbose, slow parsing)\n",
    "import json\n",
    "cache.set(\"user:123\", json.dumps(user_dict))  # 500 bytes\n",
    "\n",
    "# GOOD: Protocol Buffers (compact, fast)\n",
    "from google.protobuf import json_format\n",
    "user_proto = UserProto(id=123, name=\"Alice\")\n",
    "cache.set(\"user:123\", user_proto.SerializeToString())  # 50 bytes\n",
    "# 10x smaller, 10x faster serialization\n",
    "```\n",
    "\n",
    "**Compression**:\n",
    "```python\n",
    "import zlib\n",
    "\n",
    "# For large objects (>1KB)\n",
    "data = pickle.dumps(large_object)\n",
    "compressed = zlib.compress(data, level=3)  # Balance CPU vs size\n",
    "cache.set(\"key\", compressed, raw=True)\n",
    "```\n",
    "\n",
    "**Pipeline/Batching**:\n",
    "```python\n",
    "# BAD: 100 round trips\n",
    "for key in keys:\n",
    "    cache.get(key)\n",
    "\n",
    "# GOOD: 1 round trip (Redis pipeline)\n",
    "with cache.pipeline() as pipe:\n",
    "    for key in keys:\n",
    "        pipe.get(key)\n",
    "    results = pipe.execute()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **20.5 Connection Pool Tuning**\n",
    "\n",
    "Database connections are expensive (TCP handshake + TLS + authentication + memory). Pools reuse connections.\n",
    "\n",
    "### **Pool Configuration**\n",
    "\n",
    "**Size Formula** (PostgreSQL recommended):\n",
    "```\n",
    "connections = ((core_count * 2) + effective_spindle_count)\n",
    "\n",
    "Where:\n",
    "- core_count = CPU cores on database server\n",
    "- effective_spindle_count = number of disks (1 for SSD, actual count for HDD)\n",
    "- Or: Number of application servers \u00d7 connections per app\n",
    "\n",
    "Example:\n",
    "  4-core DB server with SSD: (4 \u00d7 2) + 1 = 9 connections\n",
    "  If you have 10 app servers: 9 / 10 = ~1 connection per app (too few!)\n",
    "  \n",
    "Solution: Connection pooler (PgBouncer) in between\n",
    "```\n",
    "\n",
    "**Pool Settings**:\n",
    "```yaml\n",
    "# HikariCP (Java) - fastest connection pool\n",
    "maximumPoolSize: 20          # Max connections in pool\n",
    "minimumIdle: 5               # Minimum idle connections maintained\n",
    "connectionTimeout: 30000     # Max wait for connection from pool (ms)\n",
    "idleTimeout: 600000          # Max time connection can sit idle (ms)\n",
    "maxLifetime: 1800000         # Max connection age (rotate before DB timeout)\n",
    "leakDetectionThreshold: 60000 # Log stack trace if connection held > 60s\n",
    "```\n",
    "\n",
    "**Anti-Patterns**:\n",
    "- **Pool too small**: Threads block waiting for connections (timeouts)\n",
    "- **Pool too large**: Database overwhelmed, memory pressure, context switching\n",
    "- **Long transactions**: Holding connections while calling external APIs (use separate connection or async)\n",
    "\n",
    "**Connection Pooler** (PgBouncer):\n",
    "```\n",
    "App Servers (100 connections) \n",
    "    \u2193\n",
    "PgBouncer (Transaction pooling)\n",
    "    \u2193\n",
    "PostgreSQL (20 actual connections)\n",
    "    \n",
    "Magic: 100 apps share 20 real connections by multiplexing\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **20.6 Runtime-Specific Optimizations**\n",
    "\n",
    "### **JVM Tuning (Java/Kotlin/Scala)**\n",
    "\n",
    "**Garbage Collection** (The biggest lever):\n",
    "```\n",
    "Options:\n",
    "1. G1GC (Default, balanced): -XX:+UseG1GC\n",
    "2. ZGC (Low latency, <10ms pauses): -XX:+UseZGC\n",
    "3. Shenandoah (Concurrent): -XX:+UseShenandoahGC\n",
    "\n",
    "For microservices with heap < 4GB:\n",
    "  -Xms2g -Xmx2g (Fixed heap size prevents resize pauses)\n",
    "  -XX:+AlwaysPreTouch (Allocate memory at startup, not on demand)\n",
    "  -XX:MaxGCPauseMillis=100 (Target max pause)\n",
    "```\n",
    "\n",
    "**JIT Compilation**:\n",
    "```java\n",
    "// Warmup: JVM compiles hot methods to native code\n",
    "// Cold start: Interpreted (slow)\n",
    "// After 10,000 invocations: Compiled (fast)\n",
    "\n",
    "// Force important methods to compile early\n",
    "-XX:CompileThreshold=1000\n",
    "```\n",
    "\n",
    "**Off-Heap Memory** (DirectByteBuffer):\n",
    "```java\n",
    "// Bypass GC for large buffers (Netty, NIO)\n",
    "ByteBuffer direct = ByteBuffer.allocateDirect(1024 * 1024); // 1MB native memory\n",
    "// No GC pressure, but manual management required\n",
    "```\n",
    "\n",
    "### **Python Optimization**\n",
    "\n",
    "**GIL Limitations**:\n",
    "Python's Global Interpreter Lock means only one thread executes Python bytecode at a time. For CPU-bound work:\n",
    "\n",
    "```python\n",
    "# BAD: Threads for CPU work (no speedup due to GIL)\n",
    "from threading import Thread\n",
    "\n",
    "# GOOD: Multiprocessing for CPU work\n",
    "from multiprocessing import Pool\n",
    "with Pool(processes=4) as pool:\n",
    "    results = pool.map(cpu_intensive_function, data)\n",
    "\n",
    "# GOOD: Asyncio for I/O bound (network, disk)\n",
    "import asyncio\n",
    "async def fetch_all(urls):\n",
    "    await asyncio.gather(*[fetch(url) for url in urls])\n",
    "```\n",
    "\n",
    "**C Extensions**:\n",
    "```python\n",
    "# Use NumPy/Pandas for numerical (C-optimized)\n",
    "import numpy as np\n",
    "arr = np.array(data)  # 100x faster than Python loops\n",
    "\n",
    "# Use Cython for custom algorithms\n",
    "# mymodule.pyx -> compiled to C\n",
    "```\n",
    "\n",
    "### **Node.js Optimization**\n",
    "\n",
    "**Event Loop Lag**:\n",
    "```javascript\n",
    "// Monitor event loop health\n",
    "const lagMonitor = require('event-loop-lag');\n",
    "const lag = lagMonitor(1000); // Check every second\n",
    "\n",
    "if (lag() > 100) {  // > 100ms lag\n",
    "  console.error('Event loop blocked!');\n",
    "}\n",
    "\n",
    "// Causes of lag:\n",
    "// - Synchronous file I/O (use fs.readFile, not readFileSync)\n",
    "// - Heavy computation (offload to worker threads)\n",
    "// - JSON.parse on huge payloads (stream instead)\n",
    "```\n",
    "\n",
    "**Cluster Mode** (Utilize all CPU cores):\n",
    "```javascript\n",
    "const cluster = require('cluster');\n",
    "const os = require('os');\n",
    "\n",
    "if (cluster.isMaster) {\n",
    "  // Fork workers equal to CPU cores\n",
    "  for (let i = 0; i < os.cpus().length; i++) {\n",
    "    cluster.fork();\n",
    "  }\n",
    "} else {\n",
    "  // Worker process runs Express server\n",
    "  require('./app');\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **20.7 Network Optimization**\n",
    "\n",
    "### **TCP Tuning**\n",
    "\n",
    "**Connection Reuse** (HTTP Keep-Alive):\n",
    "```\n",
    "Without Keep-Alive:\n",
    "  Request 1: TCP Handshake (SYN/SYN-ACK/ACK) + TLS + Request + Close\n",
    "  Request 2: TCP Handshake + TLS + Request + Close\n",
    "  Latency: 2 \u00d7 (RTT + TLS overhead)\n",
    "\n",
    "With Keep-Alive:\n",
    "  Request 1: TCP Handshake + TLS + Request (connection kept open)\n",
    "  Request 2: Request (reuse connection)\n",
    "  Latency: 1 \u00d7 (RTT + TLS overhead) + 1 \u00d7 (RTT)\n",
    "```\n",
    "\n",
    "**TCP_NODELAY** (Nagle's Algorithm):\n",
    "```python\n",
    "# Disable Nagle's algorithm (buffering small packets)\n",
    "# Good for low-latency applications (gaming, trading)\n",
    "socket.setsockopt(socket.IPPROTO_TCP, socket.TCP_NODELAY, 1)\n",
    "# Trade-off: More small packets, higher bandwidth overhead\n",
    "```\n",
    "\n",
    "**TCP Fast Open** (TFO):\n",
    "```\n",
    "Eliminate one RTT from handshake by sending data in SYN packet\n",
    "Client: SYN + DATA\n",
    "Server: SYN-ACK + DATA\n",
    "Client: ACK\n",
    "\n",
    "Requirements: Both client and server OS support\n",
    "```\n",
    "\n",
    "### **Compression**\n",
    "\n",
    "**Brotli vs Gzip**:\n",
    "```\n",
    "Level  Compression Time  Size  Decompression\n",
    "Gzip-6       Fast        100%      Fast\n",
    "Brotli-4     Medium       85%      Fast\n",
    "Brotli-11    Slow         75%      Fast\n",
    "\n",
    "Strategy:\n",
    "- Static assets: Brotli-11 (pre-compressed at build time)\n",
    "- Dynamic: Brotli-4 or Gzip-6 (balance CPU vs size)\n",
    "```\n",
    "\n",
    "**HTTP/2 Server Push** (Deprecated but concept):\n",
    "Instead of client parsing HTML then requesting CSS/JS, server pushes critical resources proactively.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.8 Frontend/Client Optimization**\n",
    "\n",
    "### **Bundle Optimization**\n",
    "\n",
    "**Tree Shaking** (Dead code elimination):\n",
    "```javascript\n",
    "// BAD: Import entire library\n",
    "import _ from 'lodash';\n",
    "_.map(data, fn);  // Imports 70KB\n",
    "\n",
    "// GOOD: Import specific function\n",
    "import map from 'lodash/map';  // Imports 2KB\n",
    "// Or use lodash-es for ES modules (automatic tree shaking)\n",
    "```\n",
    "\n",
    "**Code Splitting**:\n",
    "```javascript\n",
    "// Route-based splitting\n",
    "const Dashboard = lazy(() => import('./Dashboard'));\n",
    "const Settings = lazy(() => import('./Settings'));\n",
    "\n",
    "// Load Dashboard.js only when user visits /dashboard\n",
    "```\n",
    "\n",
    "**Caching Strategies**:\n",
    "```\n",
    "Cache-Control headers:\n",
    "- HTML: no-cache (always fresh)\n",
    "- JS/CSS: max-age=31536000, immutable (versioned filenames: app.abc123.js)\n",
    "- API: max-age=60 (short cache, stale-while-revalidate)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **20.9 Chapter Summary**\n",
    "\n",
    "Performance optimization follows a hierarchy of impact:\n",
    "\n",
    "1. **Architecture** (Biggest wins): Caching, async processing, database sharding\n",
    "2. **Algorithms** (10x-1000x): Better data structures, O(n) vs O(n\u00b2)\n",
    "3. **Code** (2x-10x): Language-specific optimizations, avoiding unnecessary work\n",
    "4. **Micro-optimizations** (1.1x-1.5x): Loop unrolling, bit manipulation (rarely worth it)\n",
    "\n",
    "**The Checklist**:\n",
    "- [ ] Profile before optimizing (find actual bottlenecks)\n",
    "- [ ] Optimize database queries first (usually the bottleneck)\n",
    "- [ ] Add caching second (but handle invalidation)\n",
    "- [ ] Tune connection pools (right-size them)\n",
    "- [ ] Monitor GC/runtime behavior (adjust if pausing)\n",
    "- [ ] Compress and minimize network payloads\n",
    "- [ ] Load test to find breaking points\n",
    "\n",
    "**Remember**: \"Premature optimization is the root of all evil\" \u2014 Donald Knuth. Optimize what matters, measure everything, and stop when requirements are met.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercises**:\n",
    "\n",
    "1. **Profiling**: Given a flame graph where `json.Marshal` takes 60% of CPU time, what are three potential optimizations?\n",
    "\n",
    "2. **Database**: A query `SELECT * FROM orders WHERE user_id = ? ORDER BY created_at DESC LIMIT 20` is slow. Write the EXPLAIN output you expect to see and the index to fix it.\n",
    "\n",
    "3. **Caching**: Design a cache structure for an e-commerce product catalog that handles 1 million products with categories, prices, and inventory. What do you cache? What is the invalidation strategy?\n",
    "\n",
    "4. **JVM**: Your Java service has 4GB heap and experiences 200ms GC pauses every minute. What GC algorithm would you switch to, and what flags would you set?\n",
    "\n",
    "5. **Network**: Calculate the time to load a page with 50 resources (JS, CSS, images) over HTTP/1.1 vs HTTP/2, assuming 100ms RTT and 50ms processing per resource.\n",
    "\n",
    "---\n",
    "\n",
    "The next chapter will cover **Deployment & Infrastructure**\u2014CI/CD pipelines, infrastructure as code, and the practices that enable safe, frequent deployments in production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='19. observability_and_monitoring.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='21. deployment_and_infrastructure.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}