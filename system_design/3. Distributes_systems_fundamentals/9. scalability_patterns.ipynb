{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6022c066",
   "metadata": {},
   "source": [
    "# **Chapter 9: Data-Intensive Systems**\n",
    "\n",
    "Modern applications generate petabytes of data daily\u2014user interactions, sensor readings, logs, and transactions. Processing this data efficiently requires specialized architectures that go beyond traditional request-response models. This chapter explores batch processing, stream processing, data pipelines, and the architectural patterns that enable organizations to derive value from massive datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.1 Introduction to Data Processing Paradigms**\n",
    "\n",
    "Data processing systems are categorized by latency requirements and data volume:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    Data Processing Spectrum                          \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  Batch Processing              Micro-Batching           Stream       \u2502\n",
    "\u2502  (Hours/Days)                  (Minutes/Seconds)        Processing   \u2502\n",
    "\u2502  (Terabytes/Petabytes)                                  (Milliseconds)\u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u2502\n",
    "\u2502  \u2502  Historical  \u2502             \u2502  Near Real-  \u2502       \u2502  Real-Time \u2502 \u2502\n",
    "\u2502  \u2502  Analytics   \u2502             \u2502  Time        \u2502       \u2502  Analytics \u2502 \u2502\n",
    "\u2502  \u2502              \u2502             \u2502              \u2502       \u2502            \u2502 \u2502\n",
    "\u2502  \u2502 \u2022 Monthly    \u2502             \u2502 \u2022 5-minute   \u2502       \u2502 \u2022 Fraud    \u2502 \u2502\n",
    "\u2502  \u2502   reports    \u2502             \u2502   aggregates \u2502       \u2502   detection\u2502 \u2502\n",
    "\u2502  \u2502 \u2022 Training   \u2502             \u2502 \u2022 Lambda     \u2502       \u2502 \u2022 IoT      \u2502 \u2502\n",
    "\u2502  \u2502   ML models  \u2502             \u2502   arch       \u2502       \u2502   alerts   \u2502 \u2502\n",
    "\u2502  \u2502 \u2022 Data       \u2502             \u2502              \u2502       \u2502 \u2022 Live     \u2502 \u2502\n",
    "\u2502  \u2502   warehousing\u2502             \u2502              \u2502       \u2502   dashboards\u2502 \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2502\n",
    "\u2502         \u2502                            \u2502                      \u2502       \u2502\n",
    "\u2502         \u25bc                            \u25bc                      \u25bc       \u2502\n",
    "\u2502    Hadoop/Spark                 Spark Streaming         Flink/      \u2502\n",
    "\u2502    MapReduce                    Structured Streaming    Kafka       \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Key Trade-offs**:\n",
    "- **Throughput vs. Latency**: Batch systems optimize for throughput (data volume), stream systems optimize for latency (speed)\n",
    "- **Accuracy vs. Speed**: Batch provides exact results; streaming provides approximate results quickly, refined over time\n",
    "- **Resource Efficiency**: Batch uses resources intensively then releases; streaming requires persistent resource allocation\n",
    "\n",
    "---\n",
    "\n",
    "## **9.2 Batch Processing**\n",
    "\n",
    "Batch processing handles large volumes of data collected over time. It's optimized for throughput rather than latency.\n",
    "\n",
    "### **The MapReduce Paradigm**\n",
    "\n",
    "**Concept**: Process data in two phases\u2014Map (transform/filter) and Reduce (aggregate). Inspired by functional programming.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "Input Data (Distributed across nodes)\n",
    "    \u2502\n",
    "    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              Map Phase                       \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n",
    "\u2502  \u2502 Node 1  \u2502 \u2502 Node 2  \u2502 \u2502 Node 3  \u2502       \u2502\n",
    "\u2502  \u2502         \u2502 \u2502         \u2502 \u2502         \u2502       \u2502\n",
    "\u2502  \u2502 Input:  \u2502 \u2502 Input:  \u2502 \u2502 Input:  \u2502       \u2502\n",
    "\u2502  \u2502 \"hello  \u2502 \u2502 \"world  \u2502 \u2502 \"hello  \u2502       \u2502\n",
    "\u2502  \u2502 world\"  \u2502 \u2502 hello\"  \u2502 \u2502 hello\"  \u2502       \u2502\n",
    "\u2502  \u2502         \u2502 \u2502         \u2502 \u2502         \u2502       \u2502\n",
    "\u2502  \u2502 Output: \u2502 \u2502 Output: \u2502 \u2502 Output: \u2502       \u2502\n",
    "\u2502  \u2502 (hello,1)\u2502 \u2502 (world,1)\u2502 \u2502 (hello,1)\u2502    \u2502\n",
    "\u2502  \u2502 (world,1)\u2502 \u2502 (hello,1)\u2502 \u2502 (hello,1)\u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518       \u2502\n",
    "\u2502       \u2502           \u2502           \u2502             \u2502\n",
    "\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518             \u2502\n",
    "\u2502                   \u2502                         \u2502\n",
    "\u2502                   \u25bc                         \u2502\n",
    "\u2502           Shuffle/Sort                      \u2502\n",
    "\u2502    (Group by key across all nodes)          \u2502\n",
    "\u2502                   \u2502                         \u2502\n",
    "\u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510             \u2502\n",
    "\u2502       \u25bc           \u25bc           \u25bc             \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n",
    "\u2502  \u2502 (hello, \u2502 \u2502 (world, \u2502 \u2502         \u2502       \u2502\n",
    "\u2502  \u2502  [1,1,  \u2502 \u2502  [1,1]) \u2502 \u2502         \u2502       \u2502\n",
    "\u2502  \u2502   1,1]) \u2502 \u2502         \u2502 \u2502         \u2502       \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n",
    "\u2502       \u2502           \u2502                         \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \u2502           \u2502\n",
    "        \u25bc           \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502             Reduce Phase                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n",
    "\u2502  \u2502 Node 1  \u2502 \u2502 Node 2  \u2502                   \u2502\n",
    "\u2502  \u2502         \u2502 \u2502         \u2502                   \u2502\n",
    "\u2502  \u2502 Sum:    \u2502 \u2502 Sum:    \u2502                   \u2502\n",
    "\u2502  \u2502 1+1+1+1 \u2502 \u2502 1+1     \u2502                   \u2502\n",
    "\u2502  \u2502 = 4     \u2502 \u2502 = 2     \u2502                   \u2502\n",
    "\u2502  \u2502         \u2502 \u2502         \u2502                   \u2502\n",
    "\u2502  \u2502 Output: \u2502 \u2502 Output: \u2502                   \u2502\n",
    "\u2502  \u2502 (hello,4)\u2502 \u2502 (world,2)\u2502                  \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Final Output:\n",
    "hello: 4\n",
    "world: 2\n",
    "```\n",
    "\n",
    "**Implementation** (Hadoop MapReduce - Java):\n",
    "```java\n",
    "// Word Count Example\n",
    "public class WordCount {\n",
    "    \n",
    "    // Mapper Class\n",
    "    public static class TokenizerMapper \n",
    "        extends Mapper<Object, Text, Text, IntWritable> {\n",
    "        \n",
    "        private final static IntWritable one = new IntWritable(1);\n",
    "        private Text word = new Text();\n",
    "        \n",
    "        public void map(Object key, Text value, Context context) \n",
    "            throws IOException, InterruptedException {\n",
    "            \n",
    "            StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "            while (itr.hasMoreTokens()) {\n",
    "                word.set(itr.nextToken());\n",
    "                context.write(word, one);  // Emit (word, 1)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Reducer Class\n",
    "    public static class IntSumReducer \n",
    "        extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "        \n",
    "        private IntWritable result = new IntWritable();\n",
    "        \n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context) \n",
    "            throws IOException, InterruptedException {\n",
    "            \n",
    "            int sum = 0;\n",
    "            for (IntWritable val : values) {\n",
    "                sum += val.get();\n",
    "            }\n",
    "            result.set(sum);\n",
    "            context.write(key, result);  // Emit (word, sum)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Limitations of MapReduce**:\n",
    "- **High Latency**: Every job writes to disk between stages (slow for iterative algorithms)\n",
    "- **Complexity**: Simple operations require verbose Java code\n",
    "- **Not Real-Time**: Designed for batch, not streaming\n",
    "\n",
    "---\n",
    "\n",
    "### **Apache Spark: In-Memory Batch Processing**\n",
    "\n",
    "**Concept**: Keeps data in memory between transformations, 10-100x faster than MapReduce for iterative algorithms.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Driver Program\n",
    "    \u2502\n",
    "    \u25bc\n",
    "SparkContext\n",
    "    \u2502\n",
    "    \u251c\u2500\u2500\u2500\u25ba Cluster Manager (YARN, Mesos, Kubernetes)\n",
    "    \u2502         \u2502\n",
    "    \u2502         \u25bc\n",
    "    \u2502    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502    \u2502         Worker Nodes                \u2502\n",
    "    \u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n",
    "    \u2502    \u2502  \u2502Executor \u2502 \u2502Executor \u2502          \u2502\n",
    "    \u2502    \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u2502          \u2502\n",
    "    \u2502    \u2502  \u2502 \u2502Task \u2502 \u2502 \u2502 \u2502Task \u2502 \u2502          \u2502\n",
    "    \u2502    \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2502          \u2502\n",
    "    \u2502    \u2502  \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u2502 \u2502 \u250c\u2500\u2500\u2500\u2500\u2500\u2510 \u2502          \u2502\n",
    "    \u2502    \u2502  \u2502 \u2502Task \u2502 \u2502 \u2502 \u2502Task \u2502 \u2502          \u2502\n",
    "    \u2502    \u2502  \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2502 \u2502 \u2514\u2500\u2500\u2500\u2500\u2500\u2518 \u2502          \u2502\n",
    "    \u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n",
    "    \u2502    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "    \u2502\n",
    "    \u25bc\n",
    "RDD/DataFrame (Resilient Distributed Dataset)\n",
    "```\n",
    "\n",
    "**Implementation** (PySpark):\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, window\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessing\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data (from S3, HDFS, or local)\n",
    "df = spark.read.parquet(\"s3://data-lake/events/\")\n",
    "\n",
    "# Transformations (lazy evaluation - nothing executed yet)\n",
    "processed_df = df \\\n",
    "    .filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"purchase_count\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\")\n",
    "    ) \\\n",
    "    .filter(col(\"purchase_count\") > 5)\n",
    "\n",
    "# Action (triggers computation)\n",
    "results = processed_df.collect()\n",
    "\n",
    "# Write back to data lake\n",
    "processed_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3://data-lake/processed/high_value_users/\")\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Spark Optimization Techniques**:\n",
    "```python\n",
    "# 1. Partitioning (avoid data skew)\n",
    "df.repartition(\"user_id\")  # Hash partition by key\n",
    "\n",
    "# 2. Broadcast Join (for small tables)\n",
    "from pyspark.sql.functions import broadcast\n",
    "large_df.join(broadcast(small_df), \"key\")\n",
    "\n",
    "# 3. Caching (reuse intermediate results)\n",
    "df.cache()  # Keep in memory\n",
    "df.count()  # Materialize\n",
    "df.filter(...).show()  # Uses cached version\n",
    "\n",
    "# 4. Predicate Pushdown (filter at source)\n",
    "spark.read \\\n",
    "    .option(\"basePath\", \"s3://data-lake/\") \\\n",
    "    .parquet(\"s3://data-lake/events/\") \\\n",
    "    .filter(col(\"date\") == \"2024-01-15\")  # Pushed to Parquet reader\n",
    "\n",
    "# 5. Salting (handle skewed keys)\n",
    "from pyspark.sql.functions import rand, lit, concat\n",
    "\n",
    "# Add random salt to skewed key\n",
    "salted_df = skewed_df.withColumn(\n",
    "    \"salted_key\", \n",
    "    concat(col(\"skewed_key\"), lit(\"_\"), (rand() * 10).cast(\"int\"))\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.3 Stream Processing**\n",
    "\n",
    "Stream processing analyzes data in motion\u2014processing events as they arrive rather than waiting for batches to accumulate.\n",
    "\n",
    "### **Windowing Strategies**\n",
    "\n",
    "Since streams are unbounded, we process data in windows:\n",
    "\n",
    "**1. Tumbling Windows** (Fixed, non-overlapping):\n",
    "```\n",
    "Time: 0----10----20----30----40----50----60\n",
    "      [----Window 1----]\n",
    "                     [----Window 2----]\n",
    "                                      [----Window 3----]\n",
    "\n",
    "Events in Window 1: 0-10 seconds\n",
    "Events in Window 2: 10-20 seconds\n",
    "No overlap between windows\n",
    "```\n",
    "\n",
    "**2. Sliding Windows** (Fixed size, overlapping):\n",
    "```\n",
    "Time: 0----5----10----15----20----25----30\n",
    "      [----Window 1----]\n",
    "           [----Window 2----]\n",
    "                [----Window 3----]\n",
    "\n",
    "Window size: 10 seconds\n",
    "Slide interval: 5 seconds\n",
    "Overlap: 5 seconds\n",
    "```\n",
    "\n",
    "**3. Session Windows** (Dynamic, activity-based):\n",
    "```\n",
    "User Activity:\n",
    "Event 1: t=0\n",
    "Event 2: t=5 (gap < 10s, same session)\n",
    "Event 3: t=25 (gap > 10s, new session)\n",
    "Event 4: t=28\n",
    "\n",
    "Session 1: [0, 5] (gap of 20s closes session)\n",
    "Session 2: [25, 28]\n",
    "```\n",
    "\n",
    "**4. Global Windows** (Single window, triggered by conditions):\n",
    "```\n",
    "All events in one window\n",
    "Trigger: Every 100 events OR every 1 minute\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Apache Flink: True Stream Processing**\n",
    "\n",
    "**Concept**: Processes events one-at-a-time (not micro-batching), providing true low latency with exactly-once semantics.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Data Sources (Kafka, Kinesis, Pulsar)\n",
    "    \u2502\n",
    "    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              Job Manager                     \u2502\n",
    "\u2502  (Coordinates distributed execution)         \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "               \u2502\n",
    "       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "       \u25bc       \u25bc       \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502            Task Managers                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510       \u2502\n",
    "\u2502  \u2502 Slot 1  \u2502 \u2502 Slot 2  \u2502 \u2502 Slot 3  \u2502       \u2502\n",
    "\u2502  \u2502 Map     \u2502 \u2502 KeyBy   \u2502 \u2502 Window  \u2502       \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518       \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u2502\n",
    "\u2502  \u2502 Sink    \u2502 \u2502 Sink    \u2502                   \u2502\n",
    "\u2502  \u2502 (Kafka) \u2502 \u2502 (DB)    \u2502                   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Checkpointing: Periodic snapshots of state for fault tolerance\n",
    "```\n",
    "\n",
    "**Implementation** (Flink Python - PyFlink):\n",
    "```python\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment, EnvironmentSettings\n",
    "from pyflink.table.window import Tumble\n",
    "\n",
    "# Initialize environment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(4)\n",
    "\n",
    "# Configure checkpointing (exactly-once semantics)\n",
    "env.enable_checkpointing(5000)  # 5 seconds\n",
    "env.get_checkpoint_config().set_checkpointing_mode(\n",
    "    CheckpointingMode.EXACTLY_ONCE\n",
    ")\n",
    "\n",
    "# Create table environment\n",
    "settings = EnvironmentSettings.new_instance() \\\n",
    "    .in_streaming_mode() \\\n",
    "    .build()\n",
    "t_env = StreamTableEnvironment.create(env, settings)\n",
    "\n",
    "# Define source (Kafka)\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE user_events (\n",
    "        user_id STRING,\n",
    "        event_type STRING,\n",
    "        amount DOUBLE,\n",
    "        event_time TIMESTAMP(3),\n",
    "        WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'user-events',\n",
    "        'properties.bootstrap.servers' = 'kafka:9092',\n",
    "        'format' = 'json',\n",
    "        'scan.startup.mode' = 'latest-offset'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Define sink\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE fraud_alerts (\n",
    "        user_id STRING,\n",
    "        transaction_count BIGINT,\n",
    "        total_amount DOUBLE,\n",
    "        window_start TIMESTAMP(3),\n",
    "        window_end TIMESTAMP(3)\n",
    "    ) WITH (\n",
    "        'connector' = 'jdbc',\n",
    "        'url' = 'jdbc:postgresql://db:5432/alerts',\n",
    "        'table-name' = 'fraud_alerts',\n",
    "        'username' = 'user',\n",
    "        'password' = 'pass'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Process: Detect high-frequency transactions (potential fraud)\n",
    "result = t_env.sql_query(\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(amount) as total_amount,\n",
    "        TUMBLE_START(event_time, INTERVAL '1' MINUTE) as window_start,\n",
    "        TUMBLE_END(event_time, INTERVAL '1' MINUTE) as window_end\n",
    "    FROM user_events\n",
    "    WHERE event_type = 'purchase'\n",
    "    GROUP BY \n",
    "        user_id,\n",
    "        TUMBLE(event_time, INTERVAL '1' MINUTE)\n",
    "    HAVING COUNT(*) > 10  -- More than 10 transactions per minute\n",
    "\"\"\")\n",
    "\n",
    "# Write results\n",
    "result.execute_insert(\"fraud_alerts\")\n",
    "```\n",
    "\n",
    "**Flink State Management**:\n",
    "```python\n",
    "# Keyed State (maintains state per key)\n",
    "class CountFunction(KeyedProcessFunction):\n",
    "    def __init__(self):\n",
    "        self.state = ValueStateTypes.LONG\n",
    "    \n",
    "    def open(self, runtime_context):\n",
    "        state_descriptor = ValueStateDescriptor(\"count\", Types.LONG())\n",
    "        self.state = runtime_context.get_state(state_descriptor)\n",
    "    \n",
    "    def process_element(self, value, ctx):\n",
    "        current = self.state.value() or 0\n",
    "        current += 1\n",
    "        self.state.update(current)\n",
    "        \n",
    "        if current > 100:\n",
    "            yield value  # Alert threshold exceeded\n",
    "\n",
    "# Use in pipeline\n",
    "stream.key_by(lambda x: x.user_id) \\\n",
    "      .process(CountFunction())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka Streams: Embedded Stream Processing**\n",
    "\n",
    "**Concept**: Library for building stream processing applications on top of Kafka. Runs inside your application (no separate cluster needed).\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class KafkaStreamsApp:\n",
    "    def __init__(self):\n",
    "        self.consumer = KafkaConsumer(\n",
    "            'user-events',\n",
    "            bootstrap_servers=['kafka:9092'],\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            group_id='stream-processor',\n",
    "            auto_offset_reset='latest'\n",
    "        )\n",
    "        \n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=['kafka:9092'],\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        \n",
    "        # Local state store (in-memory)\n",
    "        self.user_activity = defaultdict(lambda: {\n",
    "            'count': 0,\n",
    "            'last_seen': None,\n",
    "            'window_start': time.time()\n",
    "        })\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Process stream with tumbling windows\"\"\"\n",
    "        window_size = 60  # 60 second windows\n",
    "        \n",
    "        for message in self.consumer:\n",
    "            event = message.value\n",
    "            user_id = event['user_id']\n",
    "            current_time = time.time()\n",
    "            \n",
    "            user_state = self.user_activity[user_id]\n",
    "            \n",
    "            # Check if window expired\n",
    "            if current_time - user_state['window_start'] > window_size:\n",
    "                # Emit windowed result\n",
    "                self.emit_result(user_id, user_state)\n",
    "                \n",
    "                # Reset window\n",
    "                user_state['count'] = 0\n",
    "                user_state['window_start'] = current_time\n",
    "            \n",
    "            # Update state\n",
    "            user_state['count'] += 1\n",
    "            user_state['last_seen'] = current_time\n",
    "            \n",
    "            # Check for anomaly (real-time)\n",
    "            if user_state['count'] > 100:\n",
    "                self.send_alert(user_id, user_state)\n",
    "    \n",
    "    def emit_result(self, user_id, state):\n",
    "        \"\"\"Send aggregated metrics to output topic\"\"\"\n",
    "        result = {\n",
    "            'user_id': user_id,\n",
    "            'event_count': state['count'],\n",
    "            'window_duration': 60,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        self.producer.send('user-metrics', result)\n",
    "    \n",
    "    def send_alert(self, user_id, state):\n",
    "        \"\"\"Send real-time alert\"\"\"\n",
    "        alert = {\n",
    "            'user_id': user_id,\n",
    "            'alert_type': 'high_frequency',\n",
    "            'count': state['count'],\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        self.producer.send('alerts', alert)\n",
    "\n",
    "# Run\n",
    "app = KafkaStreamsApp()\n",
    "app.process()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.4 Architecture Patterns: Lambda vs. Kappa**\n",
    "\n",
    "### **Lambda Architecture**\n",
    "\n",
    "**Concept**: Maintain two processing paths\u2014batch layer (accuracy) and speed layer (latency), merged at serving layer.\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                        Lambda Architecture                           \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  Raw Data                                                            \u2502\n",
    "\u2502     \u2502                                                                \u2502\n",
    "\u2502     \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n",
    "\u2502     \u2502                                      \u2502                         \u2502\n",
    "\u2502     \u25bc                                      \u25bc                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n",
    "\u2502  \u2502 Batch Layer  \u2502                   \u2502 Speed Layer  \u2502                \u2502\n",
    "\u2502  \u2502              \u2502                   \u2502              \u2502                \u2502\n",
    "\u2502  \u2502 \u2022 Hadoop     \u2502                   \u2502 \u2022 Storm      \u2502                \u2502\n",
    "\u2502  \u2502 \u2022 Spark      \u2502                   \u2502 \u2022 Flink      \u2502                \u2502\n",
    "\u2502  \u2502 \u2022 MapReduce  \u2502                   \u2502 \u2022 Kafka      \u2502                \u2502\n",
    "\u2502  \u2502              \u2502                   \u2502   Streams    \u2502                \u2502\n",
    "\u2502  \u2502              \u2502                   \u2502              \u2502                \u2502\n",
    "\u2502  \u2502 Process ALL  \u2502                   \u2502 Process      \u2502                \u2502\n",
    "\u2502  \u2502 data         \u2502                   \u2502 RECENT data  \u2502                \u2502\n",
    "\u2502  \u2502 (High        \u2502                   \u2502 (Low latency)\u2502                \u2502\n",
    "\u2502  \u2502  latency OK) \u2502                   \u2502              \u2502                \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n",
    "\u2502         \u2502                                  \u2502                         \u2502\n",
    "\u2502         \u25bc                                  \u25bc                         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n",
    "\u2502  \u2502 Batch Views  \u2502                   \u2502 Real-time    \u2502                \u2502\n",
    "\u2502  \u2502 (Accurate)   \u2502                   \u2502 Views        \u2502                \u2502\n",
    "\u2502  \u2502              \u2502                   \u2502 (Approximate)\u2502                \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                   \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n",
    "\u2502         \u2502                                  \u2502                         \u2502\n",
    "\u2502         \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n",
    "\u2502                    \u2502                                                 \u2502\n",
    "\u2502                    \u25bc                                                 \u2502\n",
    "\u2502           \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                          \u2502\n",
    "\u2502           \u2502 Serving Layer\u2502  <-- Query interface                     \u2502\n",
    "\u2502           \u2502              \u2502      (Merge batch + real-time)           \u2502\n",
    "\u2502           \u2502 \u2022 Presto     \u2502                                          \u2502\n",
    "\u2502           \u2502 \u2022 Druid      \u2502                                          \u2502\n",
    "\u2502           \u2502 \u2022 Pinot      \u2502                                          \u2502\n",
    "\u2502           \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                          \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Complexity: HIGH (maintain two codebases for same logic)\n",
    "Use case: When exact accuracy is required AND low latency needed\n",
    "```\n",
    "\n",
    "**Challenges**:\n",
    "- **Code Duplication**: Same business logic in batch and streaming code\n",
    "- **Reconciliation**: Merging batch and speed results is complex\n",
    "- **Operational Complexity**: Two separate systems to maintain\n",
    "\n",
    "---\n",
    "\n",
    "### **Kappa Architecture**\n",
    "\n",
    "**Concept**: Single processing path using stream processing for everything. Reprocess historical data by replaying from log.\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                        Kappa Architecture                            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  Raw Data                                                            \u2502\n",
    "\u2502     \u2502                                                                \u2502\n",
    "\u2502     \u25bc                                                                \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502                     Event Log (Kafka)                        \u2502    \u2502\n",
    "\u2502  \u2502  (Immutable, append-only, replayable)                        \u2502    \u2502\n",
    "\u2502  \u2502                                                              \u2502    \u2502\n",
    "\u2502  \u2502  Offset 0: Event 1                                           \u2502    \u2502\n",
    "\u2502  \u2502  Offset 1: Event 2                                           \u2502    \u2502\n",
    "\u2502  \u2502  Offset 2: Event 3                                           \u2502    \u2502\n",
    "\u2502  \u2502  ...                                                         \u2502    \u2502\n",
    "\u2502  \u2502  Offset N: Event N                                           \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502     \u2502                                                                \u2502\n",
    "\u2502     \u2502 (Stream Processing)                                            \u2502\n",
    "\u2502     \u25bc                                                                \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502              Stream Processing Layer                         \u2502    \u2502\n",
    "\u2502  \u2502                                                              \u2502    \u2502\n",
    "\u2502  \u2502  \u2022 Real-time processing (latest offset)                      \u2502    \u2502\n",
    "\u2502  \u2502  \u2022 Historical reprocessing (from offset 0)                   \u2502    \u2502\n",
    "\u2502  \u2502                                                              \u2502    \u2502\n",
    "\u2502  \u2502  Same code for both!                                         \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502     \u2502                                                                \u2502\n",
    "\u2502     \u25bc                                                                \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502                    Serving Layer                             \u2502    \u2502\n",
    "\u2502  \u2502                                                              \u2502    \u2502\n",
    "\u2502  \u2502  Materialized Views (updated by stream processor)           \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  Benefits:                                                           \u2502\n",
    "\u2502  - Single codebase                                                   \u2502\n",
    "\u2502  - Simplified operations                                             \u2502\n",
    "\u2502  - Replay capability (reprocess with new logic)                      \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Implementation** (Reprocessing with Kafka):\n",
    "```python\n",
    "# To reprocess historical data with new logic:\n",
    "# 1. Reset consumer group to earliest offset\n",
    "# 2. Deploy new version of stream processor\n",
    "# 3. Process all events from beginning\n",
    "\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'events',\n",
    "    bootstrap_servers=['kafka:9092'],\n",
    "    group_id='processor-v2',  # New consumer group\n",
    "    auto_offset_reset='earliest'  # Start from beginning\n",
    ")\n",
    "\n",
    "# Or reset existing group\n",
    "# kafka-consumer-groups.sh --bootstrap-server kafka:9092 \\\n",
    "#   --group processor-v1 --reset-offsets --to-earliest --execute --topic events\n",
    "\n",
    "for message in consumer:\n",
    "    # Process with NEW business logic\n",
    "    new_result = process_v2(message.value)\n",
    "    save_to_database(new_result)\n",
    "```\n",
    "\n",
    "**Comparison**:\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Aspect                \u2502 Lambda                 \u2502 Kappa                  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Code Complexity       \u2502 High (duplicate logic) \u2502 Low (single codebase)  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Latency               \u2502 Low (speed layer)      \u2502 Low (streaming)        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Accuracy              \u2502 Exact (batch) + Approx \u2502 Depends on windowing   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Reprocessing          \u2502 Rerun batch jobs       \u2502 Replay from log        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Use Case              \u2502 Complex analytics      \u2502 Event-driven, IoT,     \u2502\n",
    "\u2502                       \u2502 requiring exactness    \u2502 real-time monitoring   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Modern Trend**: Most organizations now prefer Kappa architecture with tools like Flink or Kafka Streams, using the event log as the single source of truth.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.5 Data Pipeline Orchestration**\n",
    "\n",
    "Complex data workflows require orchestration\u2014managing dependencies, scheduling, retries, and monitoring.\n",
    "\n",
    "### **Apache Airflow**\n",
    "\n",
    "**Concept**: Define workflows as Directed Acyclic Graphs (DAGs) in Python. Tasks are executed based on dependencies.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    Airflow Architecture                      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                             \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502              Web Server (Flask)                      \u2502   \u2502\n",
    "\u2502  \u2502         (UI, REST API, DAG management)               \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502              Scheduler                               \u2502   \u2502\n",
    "\u2502  \u2502    (Parses DAGs, schedules tasks, queues execution)  \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502              Executor                                \u2502   \u2502\n",
    "\u2502  \u2502  (Local, Celery, Kubernetes)                         \u2502   \u2502\n",
    "\u2502  \u2502                                                      \u2502   \u2502\n",
    "\u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502   \u2502\n",
    "\u2502  \u2502  \u2502 Worker  \u2502 \u2502 Worker  \u2502 \u2502 Worker  \u2502               \u2502   \u2502\n",
    "\u2502  \u2502  \u2502 (Task)  \u2502 \u2502 (Task)  \u2502 \u2502 (Task)  \u2502               \u2502   \u2502\n",
    "\u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518               \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502              Metadata Database (PostgreSQL)          \u2502   \u2502\n",
    "\u2502  \u2502         (DAG runs, task instances, logs, etc.)       \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Implementation** (Airflow DAG):\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Default arguments\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email': ['alerts@company.com'],\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "# Define DAG\n",
    "with DAG(\n",
    "    'etl_daily_sales',\n",
    "    default_args=default_args,\n",
    "    description='Daily sales ETL pipeline',\n",
    "    schedule_interval='0 2 * * *',  # Daily at 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['sales', 'etl'],\n",
    "    max_active_runs=1\n",
    ") as dag:\n",
    "\n",
    "    # Task 1: Extract from OLTP database\n",
    "    extract_task = PostgresOperator(\n",
    "        task_id='extract_sales_data',\n",
    "        postgres_conn_id='oltp_db',\n",
    "        sql=\"\"\"\n",
    "            COPY (\n",
    "                SELECT * FROM sales \n",
    "                WHERE date = '{{ ds }}'\n",
    "            ) TO STDOUT WITH CSV HEADER;\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Task 2: Transform (Python function)\n",
    "    def transform_data(**context):\n",
    "        \"\"\"Clean and aggregate sales data\"\"\"\n",
    "        execution_date = context['ds']\n",
    "        \n",
    "        # Read extracted data\n",
    "        raw_data = read_from_staging(execution_date)\n",
    "        \n",
    "        # Transformations\n",
    "        clean_data = clean_and_validate(raw_data)\n",
    "        aggregated = aggregate_by_region(clean_data)\n",
    "        \n",
    "        # Write to S3\n",
    "        write_to_s3(aggregated, f\"processed/{execution_date}/\")\n",
    "        \n",
    "        return f\"Processed {len(raw_data)} records\"\n",
    "    \n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform_data',\n",
    "        python_callable=transform_data,\n",
    "        provide_context=True\n",
    "    )\n",
    "    \n",
    "    # Task 3: Load to Data Warehouse\n",
    "    load_task = S3ToRedshiftOperator(\n",
    "        task_id='load_to_warehouse',\n",
    "        schema='analytics',\n",
    "        table='daily_sales',\n",
    "        s3_bucket='data-lake',\n",
    "        s3_key=\"processed/{{ ds }}/\",\n",
    "        redshift_conn_id='redshift_default',\n",
    "        copy_options=[\"CSV\", \"IGNOREHEADER 1\"]\n",
    "    )\n",
    "    \n",
    "    # Task 4: Data Quality Check\n",
    "    quality_check = PostgresOperator(\n",
    "        task_id='data_quality_check',\n",
    "        postgres_conn_id='redshift_default',\n",
    "        sql=\"\"\"\n",
    "            SELECT COUNT(*) FROM analytics.daily_sales\n",
    "            WHERE date = '{{ ds }}' AND amount < 0;\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define dependencies (DAG structure)\n",
    "    extract_task >> transform_task >> load_task >> quality_check\n",
    "    \n",
    "    # Alternative: Branching\n",
    "    # transform_task >> [load_task, error_handler]  # Based on condition\n",
    "```\n",
    "\n",
    "**Modern Alternatives**:\n",
    "```python\n",
    "# Prefect (Simpler, Python-native)\n",
    "from prefect import flow, task\n",
    "from prefect.tasks import task_input_hash\n",
    "\n",
    "@task(cache_key_fn=task_input_hash, retries=3)\n",
    "def extract_data(date: str):\n",
    "    return fetch_from_api(date)\n",
    "\n",
    "@task\n",
    "def transform_data(raw_data):\n",
    "    return [clean_record(r) for r in raw_data]\n",
    "\n",
    "@task\n",
    "def load_data(clean_data):\n",
    "    insert_to_warehouse(clean_data)\n",
    "\n",
    "@flow(name=\"ETL Pipeline\")\n",
    "def etl_flow(date: str):\n",
    "    raw = extract_data(date)\n",
    "    transformed = transform_data(raw)\n",
    "    load_data(transformed)\n",
    "\n",
    "# Run\n",
    "etl_flow(\"2024-01-15\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.6 Data Warehousing and OLAP**\n",
    "\n",
    "Data warehouses are optimized for analytical queries (OLAP) rather than transactional processing (OLTP).\n",
    "\n",
    "### **OLTP vs. OLAP**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    OLTP vs OLAP Comparison                           \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  OLTP (Online Transaction Processing)                                \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 \u2022 Optimized for INSERT/UPDATE/DELETE                        \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Normalized schema (3NF)                                   \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Row-oriented storage                                        \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 High concurrency, short transactions                      \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Current data only                                         \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Examples: PostgreSQL, MySQL, Oracle                       \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Use Case: Order processing, user registration             \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  OLAP (Online Analytical Processing)                                 \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u2502\n",
    "\u2502  \u2502 \u2022 Optimized for SELECT (aggregations)                       \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Denormalized schema (star/snowflake schema)               \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Column-oriented storage                                     \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Batch loads, complex queries                              \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Historical data (years)                                   \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Examples: Snowflake, BigQuery, Redshift                   \u2502    \u2502\n",
    "\u2502  \u2502 \u2022 Use Case: Sales reports, trend analysis                   \u2502    \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "### **Columnar Storage**\n",
    "\n",
    "**Why Columnar?** Analytical queries typically access few columns but many rows.\n",
    "\n",
    "```\n",
    "Row-Oriented (OLTP):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 ID \u2502 Name   \u2502 Amount \u2502 Date   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 1  \u2502 Alice  \u2502 100    \u2502 01-15  \u2502\n",
    "\u2502 2  \u2502 Bob    \u2502 200    \u2502 01-15  \u2502\n",
    "\u2502 3  \u2502 Carol  \u2502 150    \u2502 01-16  \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "Storage: [1,Alice,100,01-15,2,Bob,200,01-15,3,Carol,150,01-16]\n",
    "Query: SUM(Amount) -> Must read all data (inefficient)\n",
    "\n",
    "Column-Oriented (OLAP):\n",
    "Column \"Amount\": [100, 200, 150]\n",
    "Column \"Date\":   [01-15, 01-15, 01-16]\n",
    "\n",
    "Query: SUM(Amount) -> Read only Amount column (efficient)\n",
    "Compression: Similar values compress better (run-length encoding)\n",
    "```\n",
    "\n",
    "**Star Schema Example**:\n",
    "```sql\n",
    "-- Fact Table (measurements)\n",
    "CREATE TABLE sales_fact (\n",
    "    sale_id BIGINT,\n",
    "    date_key INT,          -- Foreign key\n",
    "    product_key INT,       -- Foreign key\n",
    "    customer_key INT,      -- Foreign key\n",
    "    store_key INT,         -- Foreign key\n",
    "    quantity INT,\n",
    "    amount DECIMAL(10,2),\n",
    "    discount DECIMAL(5,2)\n",
    ");\n",
    "\n",
    "-- Dimension Tables (descriptions)\n",
    "CREATE TABLE date_dim (\n",
    "    date_key INT PRIMARY KEY,\n",
    "    full_date DATE,\n",
    "    day_of_week VARCHAR(10),\n",
    "    month VARCHAR(10),\n",
    "    quarter INT,\n",
    "    year INT,\n",
    "    is_holiday BOOLEAN\n",
    ");\n",
    "\n",
    "CREATE TABLE product_dim (\n",
    "    product_key INT PRIMARY KEY,\n",
    "    sku VARCHAR(50),\n",
    "    name VARCHAR(200),\n",
    "    category VARCHAR(100),\n",
    "    brand VARCHAR(100),\n",
    "    cost DECIMAL(10,2)\n",
    ");\n",
    "\n",
    "-- Query: Sales by category, Q4 2023\n",
    "SELECT \n",
    "    p.category,\n",
    "    SUM(f.amount) as total_sales\n",
    "FROM sales_fact f\n",
    "JOIN date_dim d ON f.date_key = d.date_key\n",
    "JOIN product_dim p ON f.product_key = p.product_key\n",
    "WHERE d.year = 2023 AND d.quarter = 4\n",
    "GROUP BY p.category;\n",
    "```\n",
    "\n",
    "### **Modern Data Warehouses**\n",
    "\n",
    "**Snowflake** (Cloud-native, separation of compute and storage):\n",
    "```sql\n",
    "-- Snowflake architecture: Storage + Compute (Virtual Warehouses) + Services\n",
    "-- Scale compute independently of storage\n",
    "\n",
    "-- Create warehouse (compute)\n",
    "CREATE WAREHOUSE etl_wh WITH\n",
    "    WAREHOUSE_SIZE = 'X-SMALL'\n",
    "    AUTO_SUSPEND = 300  -- Suspend after 5 min idle\n",
    "    AUTO_RESUME = TRUE;\n",
    "\n",
    "-- Create database (storage)\n",
    "CREATE DATABASE analytics_db;\n",
    "\n",
    "-- Use warehouse\n",
    "USE WAREHOUSE etl_wh;\n",
    "\n",
    "-- Query (compute scales automatically)\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    AVG(order_value) as avg_order,\n",
    "    COUNT(*) as order_count\n",
    "FROM orders\n",
    "WHERE order_date >= '2024-01-01'\n",
    "GROUP BY customer_segment;\n",
    "\n",
    "-- Scale up for heavy query\n",
    "ALTER WAREHOUSE etl_wh SET WAREHOUSE_SIZE = 'LARGE';\n",
    "\n",
    "-- Zero-copy cloning (instant dev/test environments)\n",
    "CREATE DATABASE analytics_dev CLONE analytics_db;\n",
    "```\n",
    "\n",
    "**Google BigQuery** (Serverless, pay-per-query):\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Query (serverless - no infrastructure to manage)\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        COUNT(*) as session_count,\n",
    "        AVG(session_duration) as avg_duration\n",
    "    FROM `project.dataset.events`\n",
    "    WHERE event_date BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "    GROUP BY user_id\n",
    "    HAVING COUNT(*) > 10\n",
    "\"\"\"\n",
    "\n",
    "# Run query (pay for bytes processed, not uptime)\n",
    "job = client.query(query)\n",
    "results = job.result()\n",
    "\n",
    "# Partitioning and clustering for cost optimization\n",
    "\"\"\"\n",
    "CREATE TABLE project.dataset.events (\n",
    "    event_id STRING,\n",
    "    user_id STRING,\n",
    "    event_timestamp TIMESTAMP\n",
    ")\n",
    "PARTITION BY DATE(event_timestamp)\n",
    "CLUSTER BY user_id;\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.7 Real-World Data Architecture Example**\n",
    "\n",
    "**Clickstream Analytics Platform**:\n",
    "\n",
    "```\n",
    "User Events (Website/App)\n",
    "    \u2502\n",
    "    \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502           Kafka (Event Streaming)            \u2502\n",
    "\u2502  (Durable buffer, decouple producers/consumers)\u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "           \u2502                 \u2502\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u25bc             \u25bc   \u25bc             \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Flink   \u2502  \u2502 Spark   \u2502      \u2502 S3 (Data    \u2502\n",
    "\u2502 (Real-  \u2502  \u2502 (Batch  \u2502      \u2502 Lake)       \u2502\n",
    "\u2502  time)  \u2502  \u2502  ETL)   \u2502      \u2502             \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "     \u2502            \u2502                   \u2502\n",
    "     \u25bc            \u25bc                   \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Redis   \u2502  \u2502 Snowflake   \u2502    \u2502 Athena      \u2502\n",
    "\u2502 (Cache/ \u2502  \u2502 (Data       \u2502    \u2502 (Ad-hoc     \u2502\n",
    "\u2502  Hot)   \u2502  \u2502  Warehouse) \u2502    \u2502  queries)   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                    \u2502\n",
    "                    \u25bc\n",
    "            \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "            \u2502 Tableau/    \u2502\n",
    "            \u2502 Looker      \u2502\n",
    "            \u2502 (BI/Dashboards)\u2502\n",
    "            \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.8 Key Takeaways**\n",
    "\n",
    "1. **Choose batch for throughput, stream for latency**: Batch processing (Spark) handles petabytes efficiently. Stream processing (Flink) provides sub-second latency for real-time use cases.\n",
    "\n",
    "2. **Event logs are the source of truth**: Kappa architecture with Kafka provides a single source of truth. Replay capability enables reprocessing historical data with new logic.\n",
    "\n",
    "3. **Windowing is essential for streams**: Tumbling windows for fixed intervals, sliding windows for overlaps, session windows for user activity. Watermarks handle late-arriving data.\n",
    "\n",
    "4. **Columnar storage for analytics**: Data warehouses (Snowflake, BigQuery) use columnar storage and massively parallel processing for fast aggregations.\n",
    "\n",
    "5. **Orchestrate complex workflows**: Airflow/Prefect manage dependencies, retries, and scheduling for data pipelines. Treat data pipelines as code (version control, CI/CD).\n",
    "\n",
    "6. **Schema evolution matters**: Use Avro/Protobuf with schema registries for backward/forward compatibility in streaming systems.\n",
    "\n",
    "7. **Handle data skew**: Salting keys in Spark, partitioning strategies in Kafka. Skewed data causes some nodes to be overloaded while others idle.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored data-intensive systems\u2014the technologies that power modern analytics and real-time processing. We compared batch processing (MapReduce, Spark) with stream processing (Flink, Kafka Streams), understanding when each paradigm is appropriate.\n",
    "\n",
    "We examined windowing strategies for unbounded streams and the architectural patterns that unify batch and stream processing: Lambda architecture (dual paths) and Kappa architecture (single stream-based path).\n",
    "\n",
    "Data pipeline orchestration tools (Airflow, Prefect) enable complex workflow management, while modern data warehouses (Snowflake, BigQuery) provide scalable analytics through columnar storage and separation of compute and storage.\n",
    "\n",
    "The chapter concluded with practical architectural guidance for building robust data platforms that balance latency, throughput, and cost.\n",
    "\n",
    "**Coming up next**: In Chapter 10, we'll explore Reliability & Fault Tolerance\u2014strategies for building systems that survive component failures, including redundancy patterns, disaster recovery, and chaos engineering.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercises**:\n",
    "\n",
    "1. **Architecture Selection**: Design a data pipeline for a ride-sharing app that needs:\n",
    "   - Real-time driver matching (sub-second latency)\n",
    "   - Daily fare calculation and driver payments (batch)\n",
    "   - Real-time fraud detection\n",
    "   - Monthly business intelligence reports\n",
    "   \n",
    "   Which technologies would you use for each requirement? Draw the architecture diagram.\n",
    "\n",
    "2. **Windowing Strategy**: You're building a sessionization pipeline for website analytics. Users are considered \"active\" if they have events within 30 minutes of each other. Which windowing strategy would you use? Implement a simple version using your preferred stream processing framework.\n",
    "\n",
    "3. **Data Skew Handling**: You have a Spark job processing user events, but 10% of users generate 90% of events (power users). How would you handle this data skew to prevent some executors from being overwhelmed?\n",
    "\n",
    "4. **Cost Optimization**: Your BigQuery bill is unexpectedly high. The table has 1TB of data, but queries are scanning 500GB each time. What optimizations would you implement (partitioning, clustering, materialized views)?\n",
    "\n",
    "5. **Exactly-Once Semantics**: Design a system that transfers money between accounts using Kafka Streams. How would you ensure exactly-once processing (no double counting) even if the stream processor crashes and restarts?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='8. distributed_data_management.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../4. System_design_methodology/10. framework_for_system_design_interviews.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}