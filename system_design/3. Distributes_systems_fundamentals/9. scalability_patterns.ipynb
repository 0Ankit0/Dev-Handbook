{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6022c066",
   "metadata": {},
   "source": [
    "# **Chapter 9: Data-Intensive Systems**\n",
    "\n",
    "Modern applications generate petabytes of data daily—user interactions, sensor readings, logs, and transactions. Processing this data efficiently requires specialized architectures that go beyond traditional request-response models. This chapter explores batch processing, stream processing, data pipelines, and the architectural patterns that enable organizations to derive value from massive datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.1 Introduction to Data Processing Paradigms**\n",
    "\n",
    "Data processing systems are categorized by latency requirements and data volume:\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    Data Processing Spectrum                          │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  Batch Processing              Micro-Batching           Stream       │\n",
    "│  (Hours/Days)                  (Minutes/Seconds)        Processing   │\n",
    "│  (Terabytes/Petabytes)                                  (Milliseconds)│\n",
    "│                                                                      │\n",
    "│  ┌──────────────┐             ┌──────────────┐       ┌────────────┐ │\n",
    "│  │  Historical  │             │  Near Real-  │       │  Real-Time │ │\n",
    "│  │  Analytics   │             │  Time        │       │  Analytics │ │\n",
    "│  │              │             │              │       │            │ │\n",
    "│  │ • Monthly    │             │ • 5-minute   │       │ • Fraud    │ │\n",
    "│  │   reports    │             │   aggregates │       │   detection│ │\n",
    "│  │ • Training   │             │ • Lambda     │       │ • IoT      │ │\n",
    "│  │   ML models  │             │   arch       │       │   alerts   │ │\n",
    "│  │ • Data       │             │              │       │ • Live     │ │\n",
    "│  │   warehousing│             │              │       │   dashboards│ │\n",
    "│  └──────────────┘             └──────────────┘       └────────────┘ │\n",
    "│         │                            │                      │       │\n",
    "│         ▼                            ▼                      ▼       │\n",
    "│    Hadoop/Spark                 Spark Streaming         Flink/      │\n",
    "│    MapReduce                    Structured Streaming    Kafka       │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key Trade-offs**:\n",
    "- **Throughput vs. Latency**: Batch systems optimize for throughput (data volume), stream systems optimize for latency (speed)\n",
    "- **Accuracy vs. Speed**: Batch provides exact results; streaming provides approximate results quickly, refined over time\n",
    "- **Resource Efficiency**: Batch uses resources intensively then releases; streaming requires persistent resource allocation\n",
    "\n",
    "---\n",
    "\n",
    "## **9.2 Batch Processing**\n",
    "\n",
    "Batch processing handles large volumes of data collected over time. It's optimized for throughput rather than latency.\n",
    "\n",
    "### **The MapReduce Paradigm**\n",
    "\n",
    "**Concept**: Process data in two phases—Map (transform/filter) and Reduce (aggregate). Inspired by functional programming.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "Input Data (Distributed across nodes)\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              Map Phase                       │\n",
    "│  ┌─────────┐ ┌─────────┐ ┌─────────┐       │\n",
    "│  │ Node 1  │ │ Node 2  │ │ Node 3  │       │\n",
    "│  │         │ │         │ │         │       │\n",
    "│  │ Input:  │ │ Input:  │ │ Input:  │       │\n",
    "│  │ \"hello  │ │ \"world  │ │ \"hello  │       │\n",
    "│  │ world\"  │ │ hello\"  │ │ hello\"  │       │\n",
    "│  │         │ │         │ │         │       │\n",
    "│  │ Output: │ │ Output: │ │ Output: │       │\n",
    "│  │ (hello,1)│ │ (world,1)│ │ (hello,1)│    │\n",
    "│  │ (world,1)│ │ (hello,1)│ │ (hello,1)│    │\n",
    "│  └────┬────┘ └────┬────┘ └────┬────┘       │\n",
    "│       │           │           │             │\n",
    "│       └───────────┼───────────┘             │\n",
    "│                   │                         │\n",
    "│                   ▼                         │\n",
    "│           Shuffle/Sort                      │\n",
    "│    (Group by key across all nodes)          │\n",
    "│                   │                         │\n",
    "│       ┌───────────┼───────────┐             │\n",
    "│       ▼           ▼           ▼             │\n",
    "│  ┌─────────┐ ┌─────────┐ ┌─────────┐       │\n",
    "│  │ (hello, │ │ (world, │ │         │       │\n",
    "│  │  [1,1,  │ │  [1,1]) │ │         │       │\n",
    "│  │   1,1]) │ │         │ │         │       │\n",
    "│  └────┬────┘ └────┬────┘ └─────────┘       │\n",
    "│       │           │                         │\n",
    "└───────┼───────────┼─────────────────────────┘\n",
    "        │           │\n",
    "        ▼           ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│             Reduce Phase                     │\n",
    "│  ┌─────────┐ ┌─────────┐                   │\n",
    "│  │ Node 1  │ │ Node 2  │                   │\n",
    "│  │         │ │         │                   │\n",
    "│  │ Sum:    │ │ Sum:    │                   │\n",
    "│  │ 1+1+1+1 │ │ 1+1     │                   │\n",
    "│  │ = 4     │ │ = 2     │                   │\n",
    "│  │         │ │         │                   │\n",
    "│  │ Output: │ │ Output: │                   │\n",
    "│  │ (hello,4)│ │ (world,2)│                  │\n",
    "│  └─────────┘ └─────────┘                   │\n",
    "└─────────────────────────────────────────────┘\n",
    "\n",
    "Final Output:\n",
    "hello: 4\n",
    "world: 2\n",
    "```\n",
    "\n",
    "**Implementation** (Hadoop MapReduce - Java):\n",
    "```java\n",
    "// Word Count Example\n",
    "public class WordCount {\n",
    "    \n",
    "    // Mapper Class\n",
    "    public static class TokenizerMapper \n",
    "        extends Mapper<Object, Text, Text, IntWritable> {\n",
    "        \n",
    "        private final static IntWritable one = new IntWritable(1);\n",
    "        private Text word = new Text();\n",
    "        \n",
    "        public void map(Object key, Text value, Context context) \n",
    "            throws IOException, InterruptedException {\n",
    "            \n",
    "            StringTokenizer itr = new StringTokenizer(value.toString());\n",
    "            while (itr.hasMoreTokens()) {\n",
    "                word.set(itr.nextToken());\n",
    "                context.write(word, one);  // Emit (word, 1)\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    // Reducer Class\n",
    "    public static class IntSumReducer \n",
    "        extends Reducer<Text, IntWritable, Text, IntWritable> {\n",
    "        \n",
    "        private IntWritable result = new IntWritable();\n",
    "        \n",
    "        public void reduce(Text key, Iterable<IntWritable> values, Context context) \n",
    "            throws IOException, InterruptedException {\n",
    "            \n",
    "            int sum = 0;\n",
    "            for (IntWritable val : values) {\n",
    "                sum += val.get();\n",
    "            }\n",
    "            result.set(sum);\n",
    "            context.write(key, result);  // Emit (word, sum)\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Limitations of MapReduce**:\n",
    "- **High Latency**: Every job writes to disk between stages (slow for iterative algorithms)\n",
    "- **Complexity**: Simple operations require verbose Java code\n",
    "- **Not Real-Time**: Designed for batch, not streaming\n",
    "\n",
    "---\n",
    "\n",
    "### **Apache Spark: In-Memory Batch Processing**\n",
    "\n",
    "**Concept**: Keeps data in memory between transformations, 10-100x faster than MapReduce for iterative algorithms.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Driver Program\n",
    "    │\n",
    "    ▼\n",
    "SparkContext\n",
    "    │\n",
    "    ├───► Cluster Manager (YARN, Mesos, Kubernetes)\n",
    "    │         │\n",
    "    │         ▼\n",
    "    │    ┌─────────────────────────────────────┐\n",
    "    │    │         Worker Nodes                │\n",
    "    │    │  ┌─────────┐ ┌─────────┐          │\n",
    "    │    │  │Executor │ │Executor │          │\n",
    "    │    │  │ ┌─────┐ │ │ ┌─────┐ │          │\n",
    "    │    │  │ │Task │ │ │ │Task │ │          │\n",
    "    │    │  │ └─────┘ │ │ └─────┘ │          │\n",
    "    │    │  │ ┌─────┐ │ │ ┌─────┐ │          │\n",
    "    │    │  │ │Task │ │ │ │Task │ │          │\n",
    "    │    │  │ └─────┘ │ │ └─────┘ │          │\n",
    "    │    │  └─────────┘ └─────────┘          │\n",
    "    │    └─────────────────────────────────────┘\n",
    "    │\n",
    "    ▼\n",
    "RDD/DataFrame (Resilient Distributed Dataset)\n",
    "```\n",
    "\n",
    "**Implementation** (PySpark):\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, count, avg, window\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"DataProcessing\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data (from S3, HDFS, or local)\n",
    "df = spark.read.parquet(\"s3://data-lake/events/\")\n",
    "\n",
    "# Transformations (lazy evaluation - nothing executed yet)\n",
    "processed_df = df \\\n",
    "    .filter(col(\"event_type\") == \"purchase\") \\\n",
    "    .groupBy(\"user_id\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"purchase_count\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\")\n",
    "    ) \\\n",
    "    .filter(col(\"purchase_count\") > 5)\n",
    "\n",
    "# Action (triggers computation)\n",
    "results = processed_df.collect()\n",
    "\n",
    "# Write back to data lake\n",
    "processed_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .parquet(\"s3://data-lake/processed/high_value_users/\")\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "**Spark Optimization Techniques**:\n",
    "```python\n",
    "# 1. Partitioning (avoid data skew)\n",
    "df.repartition(\"user_id\")  # Hash partition by key\n",
    "\n",
    "# 2. Broadcast Join (for small tables)\n",
    "from pyspark.sql.functions import broadcast\n",
    "large_df.join(broadcast(small_df), \"key\")\n",
    "\n",
    "# 3. Caching (reuse intermediate results)\n",
    "df.cache()  # Keep in memory\n",
    "df.count()  # Materialize\n",
    "df.filter(...).show()  # Uses cached version\n",
    "\n",
    "# 4. Predicate Pushdown (filter at source)\n",
    "spark.read \\\n",
    "    .option(\"basePath\", \"s3://data-lake/\") \\\n",
    "    .parquet(\"s3://data-lake/events/\") \\\n",
    "    .filter(col(\"date\") == \"2024-01-15\")  # Pushed to Parquet reader\n",
    "\n",
    "# 5. Salting (handle skewed keys)\n",
    "from pyspark.sql.functions import rand, lit, concat\n",
    "\n",
    "# Add random salt to skewed key\n",
    "salted_df = skewed_df.withColumn(\n",
    "    \"salted_key\", \n",
    "    concat(col(\"skewed_key\"), lit(\"_\"), (rand() * 10).cast(\"int\"))\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.3 Stream Processing**\n",
    "\n",
    "Stream processing analyzes data in motion—processing events as they arrive rather than waiting for batches to accumulate.\n",
    "\n",
    "### **Windowing Strategies**\n",
    "\n",
    "Since streams are unbounded, we process data in windows:\n",
    "\n",
    "**1. Tumbling Windows** (Fixed, non-overlapping):\n",
    "```\n",
    "Time: 0----10----20----30----40----50----60\n",
    "      [----Window 1----]\n",
    "                     [----Window 2----]\n",
    "                                      [----Window 3----]\n",
    "\n",
    "Events in Window 1: 0-10 seconds\n",
    "Events in Window 2: 10-20 seconds\n",
    "No overlap between windows\n",
    "```\n",
    "\n",
    "**2. Sliding Windows** (Fixed size, overlapping):\n",
    "```\n",
    "Time: 0----5----10----15----20----25----30\n",
    "      [----Window 1----]\n",
    "           [----Window 2----]\n",
    "                [----Window 3----]\n",
    "\n",
    "Window size: 10 seconds\n",
    "Slide interval: 5 seconds\n",
    "Overlap: 5 seconds\n",
    "```\n",
    "\n",
    "**3. Session Windows** (Dynamic, activity-based):\n",
    "```\n",
    "User Activity:\n",
    "Event 1: t=0\n",
    "Event 2: t=5 (gap < 10s, same session)\n",
    "Event 3: t=25 (gap > 10s, new session)\n",
    "Event 4: t=28\n",
    "\n",
    "Session 1: [0, 5] (gap of 20s closes session)\n",
    "Session 2: [25, 28]\n",
    "```\n",
    "\n",
    "**4. Global Windows** (Single window, triggered by conditions):\n",
    "```\n",
    "All events in one window\n",
    "Trigger: Every 100 events OR every 1 minute\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Apache Flink: True Stream Processing**\n",
    "\n",
    "**Concept**: Processes events one-at-a-time (not micro-batching), providing true low latency with exactly-once semantics.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Data Sources (Kafka, Kinesis, Pulsar)\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│              Job Manager                     │\n",
    "│  (Coordinates distributed execution)         │\n",
    "└──────────────┬──────────────────────────────┘\n",
    "               │\n",
    "       ┌───────┼───────┐\n",
    "       ▼       ▼       ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│            Task Managers                     │\n",
    "│  ┌─────────┐ ┌─────────┐ ┌─────────┐       │\n",
    "│  │ Slot 1  │ │ Slot 2  │ │ Slot 3  │       │\n",
    "│  │ Map     │ │ KeyBy   │ │ Window  │       │\n",
    "│  └─────────┘ └─────────┘ └─────────┘       │\n",
    "│  ┌─────────┐ ┌─────────┐                   │\n",
    "│  │ Sink    │ │ Sink    │                   │\n",
    "│  │ (Kafka) │ │ (DB)    │                   │\n",
    "│  └─────────┘ └─────────┘                   │\n",
    "└─────────────────────────────────────────────┘\n",
    "\n",
    "Checkpointing: Periodic snapshots of state for fault tolerance\n",
    "```\n",
    "\n",
    "**Implementation** (Flink Python - PyFlink):\n",
    "```python\n",
    "from pyflink.datastream import StreamExecutionEnvironment\n",
    "from pyflink.table import StreamTableEnvironment, EnvironmentSettings\n",
    "from pyflink.table.window import Tumble\n",
    "\n",
    "# Initialize environment\n",
    "env = StreamExecutionEnvironment.get_execution_environment()\n",
    "env.set_parallelism(4)\n",
    "\n",
    "# Configure checkpointing (exactly-once semantics)\n",
    "env.enable_checkpointing(5000)  # 5 seconds\n",
    "env.get_checkpoint_config().set_checkpointing_mode(\n",
    "    CheckpointingMode.EXACTLY_ONCE\n",
    ")\n",
    "\n",
    "# Create table environment\n",
    "settings = EnvironmentSettings.new_instance() \\\n",
    "    .in_streaming_mode() \\\n",
    "    .build()\n",
    "t_env = StreamTableEnvironment.create(env, settings)\n",
    "\n",
    "# Define source (Kafka)\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE user_events (\n",
    "        user_id STRING,\n",
    "        event_type STRING,\n",
    "        amount DOUBLE,\n",
    "        event_time TIMESTAMP(3),\n",
    "        WATERMARK FOR event_time AS event_time - INTERVAL '5' SECOND\n",
    "    ) WITH (\n",
    "        'connector' = 'kafka',\n",
    "        'topic' = 'user-events',\n",
    "        'properties.bootstrap.servers' = 'kafka:9092',\n",
    "        'format' = 'json',\n",
    "        'scan.startup.mode' = 'latest-offset'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Define sink\n",
    "t_env.execute_sql(\"\"\"\n",
    "    CREATE TABLE fraud_alerts (\n",
    "        user_id STRING,\n",
    "        transaction_count BIGINT,\n",
    "        total_amount DOUBLE,\n",
    "        window_start TIMESTAMP(3),\n",
    "        window_end TIMESTAMP(3)\n",
    "    ) WITH (\n",
    "        'connector' = 'jdbc',\n",
    "        'url' = 'jdbc:postgresql://db:5432/alerts',\n",
    "        'table-name' = 'fraud_alerts',\n",
    "        'username' = 'user',\n",
    "        'password' = 'pass'\n",
    "    )\n",
    "\"\"\")\n",
    "\n",
    "# Process: Detect high-frequency transactions (potential fraud)\n",
    "result = t_env.sql_query(\"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(amount) as total_amount,\n",
    "        TUMBLE_START(event_time, INTERVAL '1' MINUTE) as window_start,\n",
    "        TUMBLE_END(event_time, INTERVAL '1' MINUTE) as window_end\n",
    "    FROM user_events\n",
    "    WHERE event_type = 'purchase'\n",
    "    GROUP BY \n",
    "        user_id,\n",
    "        TUMBLE(event_time, INTERVAL '1' MINUTE)\n",
    "    HAVING COUNT(*) > 10  -- More than 10 transactions per minute\n",
    "\"\"\")\n",
    "\n",
    "# Write results\n",
    "result.execute_insert(\"fraud_alerts\")\n",
    "```\n",
    "\n",
    "**Flink State Management**:\n",
    "```python\n",
    "# Keyed State (maintains state per key)\n",
    "class CountFunction(KeyedProcessFunction):\n",
    "    def __init__(self):\n",
    "        self.state = ValueStateTypes.LONG\n",
    "    \n",
    "    def open(self, runtime_context):\n",
    "        state_descriptor = ValueStateDescriptor(\"count\", Types.LONG())\n",
    "        self.state = runtime_context.get_state(state_descriptor)\n",
    "    \n",
    "    def process_element(self, value, ctx):\n",
    "        current = self.state.value() or 0\n",
    "        current += 1\n",
    "        self.state.update(current)\n",
    "        \n",
    "        if current > 100:\n",
    "            yield value  # Alert threshold exceeded\n",
    "\n",
    "# Use in pipeline\n",
    "stream.key_by(lambda x: x.user_id) \\\n",
    "      .process(CountFunction())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka Streams: Embedded Stream Processing**\n",
    "\n",
    "**Concept**: Library for building stream processing applications on top of Kafka. Runs inside your application (no separate cluster needed).\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "from collections import defaultdict\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class KafkaStreamsApp:\n",
    "    def __init__(self):\n",
    "        self.consumer = KafkaConsumer(\n",
    "            'user-events',\n",
    "            bootstrap_servers=['kafka:9092'],\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            group_id='stream-processor',\n",
    "            auto_offset_reset='latest'\n",
    "        )\n",
    "        \n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=['kafka:9092'],\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        \n",
    "        # Local state store (in-memory)\n",
    "        self.user_activity = defaultdict(lambda: {\n",
    "            'count': 0,\n",
    "            'last_seen': None,\n",
    "            'window_start': time.time()\n",
    "        })\n",
    "    \n",
    "    def process(self):\n",
    "        \"\"\"Process stream with tumbling windows\"\"\"\n",
    "        window_size = 60  # 60 second windows\n",
    "        \n",
    "        for message in self.consumer:\n",
    "            event = message.value\n",
    "            user_id = event['user_id']\n",
    "            current_time = time.time()\n",
    "            \n",
    "            user_state = self.user_activity[user_id]\n",
    "            \n",
    "            # Check if window expired\n",
    "            if current_time - user_state['window_start'] > window_size:\n",
    "                # Emit windowed result\n",
    "                self.emit_result(user_id, user_state)\n",
    "                \n",
    "                # Reset window\n",
    "                user_state['count'] = 0\n",
    "                user_state['window_start'] = current_time\n",
    "            \n",
    "            # Update state\n",
    "            user_state['count'] += 1\n",
    "            user_state['last_seen'] = current_time\n",
    "            \n",
    "            # Check for anomaly (real-time)\n",
    "            if user_state['count'] > 100:\n",
    "                self.send_alert(user_id, user_state)\n",
    "    \n",
    "    def emit_result(self, user_id, state):\n",
    "        \"\"\"Send aggregated metrics to output topic\"\"\"\n",
    "        result = {\n",
    "            'user_id': user_id,\n",
    "            'event_count': state['count'],\n",
    "            'window_duration': 60,\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        self.producer.send('user-metrics', result)\n",
    "    \n",
    "    def send_alert(self, user_id, state):\n",
    "        \"\"\"Send real-time alert\"\"\"\n",
    "        alert = {\n",
    "            'user_id': user_id,\n",
    "            'alert_type': 'high_frequency',\n",
    "            'count': state['count'],\n",
    "            'timestamp': time.time()\n",
    "        }\n",
    "        self.producer.send('alerts', alert)\n",
    "\n",
    "# Run\n",
    "app = KafkaStreamsApp()\n",
    "app.process()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.4 Architecture Patterns: Lambda vs. Kappa**\n",
    "\n",
    "### **Lambda Architecture**\n",
    "\n",
    "**Concept**: Maintain two processing paths—batch layer (accuracy) and speed layer (latency), merged at serving layer.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        Lambda Architecture                           │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  Raw Data                                                            │\n",
    "│     │                                                                │\n",
    "│     ├──────────────────────────────────────┐                         │\n",
    "│     │                                      │                         │\n",
    "│     ▼                                      ▼                         │\n",
    "│  ┌──────────────┐                   ┌──────────────┐                │\n",
    "│  │ Batch Layer  │                   │ Speed Layer  │                │\n",
    "│  │              │                   │              │                │\n",
    "│  │ • Hadoop     │                   │ • Storm      │                │\n",
    "│  │ • Spark      │                   │ • Flink      │                │\n",
    "│  │ • MapReduce  │                   │ • Kafka      │                │\n",
    "│  │              │                   │   Streams    │                │\n",
    "│  │              │                   │              │                │\n",
    "│  │ Process ALL  │                   │ Process      │                │\n",
    "│  │ data         │                   │ RECENT data  │                │\n",
    "│  │ (High        │                   │ (Low latency)│                │\n",
    "│  │  latency OK) │                   │              │                │\n",
    "│  └──────┬───────┘                   └──────┬───────┘                │\n",
    "│         │                                  │                         │\n",
    "│         ▼                                  ▼                         │\n",
    "│  ┌──────────────┐                   ┌──────────────┐                │\n",
    "│  │ Batch Views  │                   │ Real-time    │                │\n",
    "│  │ (Accurate)   │                   │ Views        │                │\n",
    "│  │              │                   │ (Approximate)│                │\n",
    "│  └──────┬───────┘                   └──────┬───────┘                │\n",
    "│         │                                  │                         │\n",
    "│         └──────────┬───────────────────────┘                         │\n",
    "│                    │                                                 │\n",
    "│                    ▼                                                 │\n",
    "│           ┌──────────────┐                                          │\n",
    "│           │ Serving Layer│  <-- Query interface                     │\n",
    "│           │              │      (Merge batch + real-time)           │\n",
    "│           │ • Presto     │                                          │\n",
    "│           │ • Druid      │                                          │\n",
    "│           │ • Pinot      │                                          │\n",
    "│           └──────────────┘                                          │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Complexity: HIGH (maintain two codebases for same logic)\n",
    "Use case: When exact accuracy is required AND low latency needed\n",
    "```\n",
    "\n",
    "**Challenges**:\n",
    "- **Code Duplication**: Same business logic in batch and streaming code\n",
    "- **Reconciliation**: Merging batch and speed results is complex\n",
    "- **Operational Complexity**: Two separate systems to maintain\n",
    "\n",
    "---\n",
    "\n",
    "### **Kappa Architecture**\n",
    "\n",
    "**Concept**: Single processing path using stream processing for everything. Reprocess historical data by replaying from log.\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                        Kappa Architecture                            │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  Raw Data                                                            │\n",
    "│     │                                                                │\n",
    "│     ▼                                                                │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐    │\n",
    "│  │                     Event Log (Kafka)                        │    │\n",
    "│  │  (Immutable, append-only, replayable)                        │    │\n",
    "│  │                                                              │    │\n",
    "│  │  Offset 0: Event 1                                           │    │\n",
    "│  │  Offset 1: Event 2                                           │    │\n",
    "│  │  Offset 2: Event 3                                           │    │\n",
    "│  │  ...                                                         │    │\n",
    "│  │  Offset N: Event N                                           │    │\n",
    "│  └─────────────────────────────────────────────────────────────┘    │\n",
    "│     │                                                                │\n",
    "│     │ (Stream Processing)                                            │\n",
    "│     ▼                                                                │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐    │\n",
    "│  │              Stream Processing Layer                         │    │\n",
    "│  │                                                              │    │\n",
    "│  │  • Real-time processing (latest offset)                      │    │\n",
    "│  │  • Historical reprocessing (from offset 0)                   │    │\n",
    "│  │                                                              │    │\n",
    "│  │  Same code for both!                                         │    │\n",
    "│  └─────────────────────────────────────────────────────────────┘    │\n",
    "│     │                                                                │\n",
    "│     ▼                                                                │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐    │\n",
    "│  │                    Serving Layer                             │    │\n",
    "│  │                                                              │    │\n",
    "│  │  Materialized Views (updated by stream processor)           │    │\n",
    "│  └─────────────────────────────────────────────────────────────┘    │\n",
    "│                                                                      │\n",
    "│  Benefits:                                                           │\n",
    "│  - Single codebase                                                   │\n",
    "│  - Simplified operations                                             │\n",
    "│  - Replay capability (reprocess with new logic)                      │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Implementation** (Reprocessing with Kafka):\n",
    "```python\n",
    "# To reprocess historical data with new logic:\n",
    "# 1. Reset consumer group to earliest offset\n",
    "# 2. Deploy new version of stream processor\n",
    "# 3. Process all events from beginning\n",
    "\n",
    "from kafka import KafkaConsumer, TopicPartition\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'events',\n",
    "    bootstrap_servers=['kafka:9092'],\n",
    "    group_id='processor-v2',  # New consumer group\n",
    "    auto_offset_reset='earliest'  # Start from beginning\n",
    ")\n",
    "\n",
    "# Or reset existing group\n",
    "# kafka-consumer-groups.sh --bootstrap-server kafka:9092 \\\n",
    "#   --group processor-v1 --reset-offsets --to-earliest --execute --topic events\n",
    "\n",
    "for message in consumer:\n",
    "    # Process with NEW business logic\n",
    "    new_result = process_v2(message.value)\n",
    "    save_to_database(new_result)\n",
    "```\n",
    "\n",
    "**Comparison**:\n",
    "```\n",
    "┌───────────────────────┬────────────────────────┬────────────────────────┐\n",
    "│ Aspect                │ Lambda                 │ Kappa                  │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Code Complexity       │ High (duplicate logic) │ Low (single codebase)  │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Latency               │ Low (speed layer)      │ Low (streaming)        │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Accuracy              │ Exact (batch) + Approx │ Depends on windowing   │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Reprocessing          │ Rerun batch jobs       │ Replay from log        │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Use Case              │ Complex analytics      │ Event-driven, IoT,     │\n",
    "│                       │ requiring exactness    │ real-time monitoring   │\n",
    "└───────────────────────┴────────────────────────┴────────────────────────┘\n",
    "```\n",
    "\n",
    "**Modern Trend**: Most organizations now prefer Kappa architecture with tools like Flink or Kafka Streams, using the event log as the single source of truth.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.5 Data Pipeline Orchestration**\n",
    "\n",
    "Complex data workflows require orchestration—managing dependencies, scheduling, retries, and monitoring.\n",
    "\n",
    "### **Apache Airflow**\n",
    "\n",
    "**Concept**: Define workflows as Directed Acyclic Graphs (DAGs) in Python. Tasks are executed based on dependencies.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Airflow Architecture                      │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐   │\n",
    "│  │              Web Server (Flask)                      │   │\n",
    "│  │         (UI, REST API, DAG management)               │   │\n",
    "│  └─────────────────────────────────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐   │\n",
    "│  │              Scheduler                               │   │\n",
    "│  │    (Parses DAGs, schedules tasks, queues execution)  │   │\n",
    "│  └─────────────────────────────────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐   │\n",
    "│  │              Executor                                │   │\n",
    "│  │  (Local, Celery, Kubernetes)                         │   │\n",
    "│  │                                                      │   │\n",
    "│  │  ┌─────────┐ ┌─────────┐ ┌─────────┐               │   │\n",
    "│  │  │ Worker  │ │ Worker  │ │ Worker  │               │   │\n",
    "│  │  │ (Task)  │ │ (Task)  │ │ (Task)  │               │   │\n",
    "│  │  └─────────┘ └─────────┘ └─────────┘               │   │\n",
    "│  └─────────────────────────────────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  ┌─────────────────────────────────────────────────────┐   │\n",
    "│  │              Metadata Database (PostgreSQL)          │   │\n",
    "│  │         (DAG runs, task instances, logs, etc.)       │   │\n",
    "│  └─────────────────────────────────────────────────────┘   │\n",
    "│                                                             │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Implementation** (Airflow DAG):\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Default arguments\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email': ['alerts@company.com'],\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "# Define DAG\n",
    "with DAG(\n",
    "    'etl_daily_sales',\n",
    "    default_args=default_args,\n",
    "    description='Daily sales ETL pipeline',\n",
    "    schedule_interval='0 2 * * *',  # Daily at 2 AM\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['sales', 'etl'],\n",
    "    max_active_runs=1\n",
    ") as dag:\n",
    "\n",
    "    # Task 1: Extract from OLTP database\n",
    "    extract_task = PostgresOperator(\n",
    "        task_id='extract_sales_data',\n",
    "        postgres_conn_id='oltp_db',\n",
    "        sql=\"\"\"\n",
    "            COPY (\n",
    "                SELECT * FROM sales \n",
    "                WHERE date = '{{ ds }}'\n",
    "            ) TO STDOUT WITH CSV HEADER;\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Task 2: Transform (Python function)\n",
    "    def transform_data(**context):\n",
    "        \"\"\"Clean and aggregate sales data\"\"\"\n",
    "        execution_date = context['ds']\n",
    "        \n",
    "        # Read extracted data\n",
    "        raw_data = read_from_staging(execution_date)\n",
    "        \n",
    "        # Transformations\n",
    "        clean_data = clean_and_validate(raw_data)\n",
    "        aggregated = aggregate_by_region(clean_data)\n",
    "        \n",
    "        # Write to S3\n",
    "        write_to_s3(aggregated, f\"processed/{execution_date}/\")\n",
    "        \n",
    "        return f\"Processed {len(raw_data)} records\"\n",
    "    \n",
    "    transform_task = PythonOperator(\n",
    "        task_id='transform_data',\n",
    "        python_callable=transform_data,\n",
    "        provide_context=True\n",
    "    )\n",
    "    \n",
    "    # Task 3: Load to Data Warehouse\n",
    "    load_task = S3ToRedshiftOperator(\n",
    "        task_id='load_to_warehouse',\n",
    "        schema='analytics',\n",
    "        table='daily_sales',\n",
    "        s3_bucket='data-lake',\n",
    "        s3_key=\"processed/{{ ds }}/\",\n",
    "        redshift_conn_id='redshift_default',\n",
    "        copy_options=[\"CSV\", \"IGNOREHEADER 1\"]\n",
    "    )\n",
    "    \n",
    "    # Task 4: Data Quality Check\n",
    "    quality_check = PostgresOperator(\n",
    "        task_id='data_quality_check',\n",
    "        postgres_conn_id='redshift_default',\n",
    "        sql=\"\"\"\n",
    "            SELECT COUNT(*) FROM analytics.daily_sales\n",
    "            WHERE date = '{{ ds }}' AND amount < 0;\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define dependencies (DAG structure)\n",
    "    extract_task >> transform_task >> load_task >> quality_check\n",
    "    \n",
    "    # Alternative: Branching\n",
    "    # transform_task >> [load_task, error_handler]  # Based on condition\n",
    "```\n",
    "\n",
    "**Modern Alternatives**:\n",
    "```python\n",
    "# Prefect (Simpler, Python-native)\n",
    "from prefect import flow, task\n",
    "from prefect.tasks import task_input_hash\n",
    "\n",
    "@task(cache_key_fn=task_input_hash, retries=3)\n",
    "def extract_data(date: str):\n",
    "    return fetch_from_api(date)\n",
    "\n",
    "@task\n",
    "def transform_data(raw_data):\n",
    "    return [clean_record(r) for r in raw_data]\n",
    "\n",
    "@task\n",
    "def load_data(clean_data):\n",
    "    insert_to_warehouse(clean_data)\n",
    "\n",
    "@flow(name=\"ETL Pipeline\")\n",
    "def etl_flow(date: str):\n",
    "    raw = extract_data(date)\n",
    "    transformed = transform_data(raw)\n",
    "    load_data(transformed)\n",
    "\n",
    "# Run\n",
    "etl_flow(\"2024-01-15\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.6 Data Warehousing and OLAP**\n",
    "\n",
    "Data warehouses are optimized for analytical queries (OLAP) rather than transactional processing (OLTP).\n",
    "\n",
    "### **OLTP vs. OLAP**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    OLTP vs OLAP Comparison                           │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  OLTP (Online Transaction Processing)                                │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐    │\n",
    "│  │ • Optimized for INSERT/UPDATE/DELETE                        │    │\n",
    "│  │ • Normalized schema (3NF)                                   │    │\n",
    "│  │ • Row-oriented storage                                        │    │\n",
    "│  │ • High concurrency, short transactions                      │    │\n",
    "│  │ • Current data only                                         │    │\n",
    "│  │ • Examples: PostgreSQL, MySQL, Oracle                       │    │\n",
    "│  │ • Use Case: Order processing, user registration             │    │\n",
    "│  └─────────────────────────────────────────────────────────────┘    │\n",
    "│                                                                      │\n",
    "│  OLAP (Online Analytical Processing)                                 │\n",
    "│  ┌─────────────────────────────────────────────────────────────┐    │\n",
    "│  │ • Optimized for SELECT (aggregations)                       │    │\n",
    "│  │ • Denormalized schema (star/snowflake schema)               │    │\n",
    "│  │ • Column-oriented storage                                     │    │\n",
    "│  │ • Batch loads, complex queries                              │    │\n",
    "│  │ • Historical data (years)                                   │    │\n",
    "│  │ • Examples: Snowflake, BigQuery, Redshift                   │    │\n",
    "│  │ • Use Case: Sales reports, trend analysis                   │    │\n",
    "│  └─────────────────────────────────────────────────────────────┘    │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "### **Columnar Storage**\n",
    "\n",
    "**Why Columnar?** Analytical queries typically access few columns but many rows.\n",
    "\n",
    "```\n",
    "Row-Oriented (OLTP):\n",
    "┌────────┬────────┬────────┬────────┐\n",
    "│ ID │ Name   │ Amount │ Date   │\n",
    "├────────┼────────┼────────┼────────┤\n",
    "│ 1  │ Alice  │ 100    │ 01-15  │\n",
    "│ 2  │ Bob    │ 200    │ 01-15  │\n",
    "│ 3  │ Carol  │ 150    │ 01-16  │\n",
    "└────────┴────────┴────────┴────────┘\n",
    "Storage: [1,Alice,100,01-15,2,Bob,200,01-15,3,Carol,150,01-16]\n",
    "Query: SUM(Amount) -> Must read all data (inefficient)\n",
    "\n",
    "Column-Oriented (OLAP):\n",
    "Column \"Amount\": [100, 200, 150]\n",
    "Column \"Date\":   [01-15, 01-15, 01-16]\n",
    "\n",
    "Query: SUM(Amount) -> Read only Amount column (efficient)\n",
    "Compression: Similar values compress better (run-length encoding)\n",
    "```\n",
    "\n",
    "**Star Schema Example**:\n",
    "```sql\n",
    "-- Fact Table (measurements)\n",
    "CREATE TABLE sales_fact (\n",
    "    sale_id BIGINT,\n",
    "    date_key INT,          -- Foreign key\n",
    "    product_key INT,       -- Foreign key\n",
    "    customer_key INT,      -- Foreign key\n",
    "    store_key INT,         -- Foreign key\n",
    "    quantity INT,\n",
    "    amount DECIMAL(10,2),\n",
    "    discount DECIMAL(5,2)\n",
    ");\n",
    "\n",
    "-- Dimension Tables (descriptions)\n",
    "CREATE TABLE date_dim (\n",
    "    date_key INT PRIMARY KEY,\n",
    "    full_date DATE,\n",
    "    day_of_week VARCHAR(10),\n",
    "    month VARCHAR(10),\n",
    "    quarter INT,\n",
    "    year INT,\n",
    "    is_holiday BOOLEAN\n",
    ");\n",
    "\n",
    "CREATE TABLE product_dim (\n",
    "    product_key INT PRIMARY KEY,\n",
    "    sku VARCHAR(50),\n",
    "    name VARCHAR(200),\n",
    "    category VARCHAR(100),\n",
    "    brand VARCHAR(100),\n",
    "    cost DECIMAL(10,2)\n",
    ");\n",
    "\n",
    "-- Query: Sales by category, Q4 2023\n",
    "SELECT \n",
    "    p.category,\n",
    "    SUM(f.amount) as total_sales\n",
    "FROM sales_fact f\n",
    "JOIN date_dim d ON f.date_key = d.date_key\n",
    "JOIN product_dim p ON f.product_key = p.product_key\n",
    "WHERE d.year = 2023 AND d.quarter = 4\n",
    "GROUP BY p.category;\n",
    "```\n",
    "\n",
    "### **Modern Data Warehouses**\n",
    "\n",
    "**Snowflake** (Cloud-native, separation of compute and storage):\n",
    "```sql\n",
    "-- Snowflake architecture: Storage + Compute (Virtual Warehouses) + Services\n",
    "-- Scale compute independently of storage\n",
    "\n",
    "-- Create warehouse (compute)\n",
    "CREATE WAREHOUSE etl_wh WITH\n",
    "    WAREHOUSE_SIZE = 'X-SMALL'\n",
    "    AUTO_SUSPEND = 300  -- Suspend after 5 min idle\n",
    "    AUTO_RESUME = TRUE;\n",
    "\n",
    "-- Create database (storage)\n",
    "CREATE DATABASE analytics_db;\n",
    "\n",
    "-- Use warehouse\n",
    "USE WAREHOUSE etl_wh;\n",
    "\n",
    "-- Query (compute scales automatically)\n",
    "SELECT \n",
    "    customer_segment,\n",
    "    AVG(order_value) as avg_order,\n",
    "    COUNT(*) as order_count\n",
    "FROM orders\n",
    "WHERE order_date >= '2024-01-01'\n",
    "GROUP BY customer_segment;\n",
    "\n",
    "-- Scale up for heavy query\n",
    "ALTER WAREHOUSE etl_wh SET WAREHOUSE_SIZE = 'LARGE';\n",
    "\n",
    "-- Zero-copy cloning (instant dev/test environments)\n",
    "CREATE DATABASE analytics_dev CLONE analytics_db;\n",
    "```\n",
    "\n",
    "**Google BigQuery** (Serverless, pay-per-query):\n",
    "```python\n",
    "from google.cloud import bigquery\n",
    "\n",
    "client = bigquery.Client()\n",
    "\n",
    "# Query (serverless - no infrastructure to manage)\n",
    "query = \"\"\"\n",
    "    SELECT \n",
    "        user_id,\n",
    "        COUNT(*) as session_count,\n",
    "        AVG(session_duration) as avg_duration\n",
    "    FROM `project.dataset.events`\n",
    "    WHERE event_date BETWEEN '2024-01-01' AND '2024-01-31'\n",
    "    GROUP BY user_id\n",
    "    HAVING COUNT(*) > 10\n",
    "\"\"\"\n",
    "\n",
    "# Run query (pay for bytes processed, not uptime)\n",
    "job = client.query(query)\n",
    "results = job.result()\n",
    "\n",
    "# Partitioning and clustering for cost optimization\n",
    "\"\"\"\n",
    "CREATE TABLE project.dataset.events (\n",
    "    event_id STRING,\n",
    "    user_id STRING,\n",
    "    event_timestamp TIMESTAMP\n",
    ")\n",
    "PARTITION BY DATE(event_timestamp)\n",
    "CLUSTER BY user_id;\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.7 Real-World Data Architecture Example**\n",
    "\n",
    "**Clickstream Analytics Platform**:\n",
    "\n",
    "```\n",
    "User Events (Website/App)\n",
    "    │\n",
    "    ▼\n",
    "┌─────────────────────────────────────────────┐\n",
    "│           Kafka (Event Streaming)            │\n",
    "│  (Durable buffer, decouple producers/consumers)│\n",
    "└──────────┬─────────────────┬────────────────┘\n",
    "           │                 │\n",
    "    ┌──────┴──────┐   ┌──────┴──────┐\n",
    "    ▼             ▼   ▼             ▼\n",
    "┌─────────┐  ┌─────────┐      ┌─────────────┐\n",
    "│ Flink   │  │ Spark   │      │ S3 (Data    │\n",
    "│ (Real-  │  │ (Batch  │      │ Lake)       │\n",
    "│  time)  │  │  ETL)   │      │             │\n",
    "└────┬────┘  └────┬────┘      └─────────────┘\n",
    "     │            │                   │\n",
    "     ▼            ▼                   ▼\n",
    "┌─────────┐  ┌─────────────┐    ┌─────────────┐\n",
    "│ Redis   │  │ Snowflake   │    │ Athena      │\n",
    "│ (Cache/ │  │ (Data       │    │ (Ad-hoc     │\n",
    "│  Hot)   │  │  Warehouse) │    │  queries)   │\n",
    "└─────────┘  └──────┬──────┘    └─────────────┘\n",
    "                    │\n",
    "                    ▼\n",
    "            ┌─────────────┐\n",
    "            │ Tableau/    │\n",
    "            │ Looker      │\n",
    "            │ (BI/Dashboards)│\n",
    "            └─────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.8 Key Takeaways**\n",
    "\n",
    "1. **Choose batch for throughput, stream for latency**: Batch processing (Spark) handles petabytes efficiently. Stream processing (Flink) provides sub-second latency for real-time use cases.\n",
    "\n",
    "2. **Event logs are the source of truth**: Kappa architecture with Kafka provides a single source of truth. Replay capability enables reprocessing historical data with new logic.\n",
    "\n",
    "3. **Windowing is essential for streams**: Tumbling windows for fixed intervals, sliding windows for overlaps, session windows for user activity. Watermarks handle late-arriving data.\n",
    "\n",
    "4. **Columnar storage for analytics**: Data warehouses (Snowflake, BigQuery) use columnar storage and massively parallel processing for fast aggregations.\n",
    "\n",
    "5. **Orchestrate complex workflows**: Airflow/Prefect manage dependencies, retries, and scheduling for data pipelines. Treat data pipelines as code (version control, CI/CD).\n",
    "\n",
    "6. **Schema evolution matters**: Use Avro/Protobuf with schema registries for backward/forward compatibility in streaming systems.\n",
    "\n",
    "7. **Handle data skew**: Salting keys in Spark, partitioning strategies in Kafka. Skewed data causes some nodes to be overloaded while others idle.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored data-intensive systems—the technologies that power modern analytics and real-time processing. We compared batch processing (MapReduce, Spark) with stream processing (Flink, Kafka Streams), understanding when each paradigm is appropriate.\n",
    "\n",
    "We examined windowing strategies for unbounded streams and the architectural patterns that unify batch and stream processing: Lambda architecture (dual paths) and Kappa architecture (single stream-based path).\n",
    "\n",
    "Data pipeline orchestration tools (Airflow, Prefect) enable complex workflow management, while modern data warehouses (Snowflake, BigQuery) provide scalable analytics through columnar storage and separation of compute and storage.\n",
    "\n",
    "The chapter concluded with practical architectural guidance for building robust data platforms that balance latency, throughput, and cost.\n",
    "\n",
    "**Coming up next**: In Chapter 10, we'll explore Reliability & Fault Tolerance—strategies for building systems that survive component failures, including redundancy patterns, disaster recovery, and chaos engineering.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercises**:\n",
    "\n",
    "1. **Architecture Selection**: Design a data pipeline for a ride-sharing app that needs:\n",
    "   - Real-time driver matching (sub-second latency)\n",
    "   - Daily fare calculation and driver payments (batch)\n",
    "   - Real-time fraud detection\n",
    "   - Monthly business intelligence reports\n",
    "   \n",
    "   Which technologies would you use for each requirement? Draw the architecture diagram.\n",
    "\n",
    "2. **Windowing Strategy**: You're building a sessionization pipeline for website analytics. Users are considered \"active\" if they have events within 30 minutes of each other. Which windowing strategy would you use? Implement a simple version using your preferred stream processing framework.\n",
    "\n",
    "3. **Data Skew Handling**: You have a Spark job processing user events, but 10% of users generate 90% of events (power users). How would you handle this data skew to prevent some executors from being overwhelmed?\n",
    "\n",
    "4. **Cost Optimization**: Your BigQuery bill is unexpectedly high. The table has 1TB of data, but queries are scanning 500GB each time. What optimizations would you implement (partitioning, clustering, materialized views)?\n",
    "\n",
    "5. **Exactly-Once Semantics**: Design a system that transfers money between accounts using Kafka Streams. How would you ensure exactly-once processing (no double counting) even if the stream processor crashes and restarts?\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
