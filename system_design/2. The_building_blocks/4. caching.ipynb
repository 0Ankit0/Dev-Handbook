{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2291568e",
   "metadata": {},
   "source": [
    "# **Chapter 4: Caching – Speed at Scale**\n",
    "\n",
    "In system design, caching is the most impactful optimization you can implement. A well-designed caching strategy can improve performance by 10-1000x while reducing infrastructure costs significantly. This chapter explores caching patterns, eviction policies, distributed caching, CDNs, and cache consistency strategies.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.1 Introduction to Caching**\n",
    "\n",
    "**Caching**: Storing frequently accessed data in fast storage to reduce access time and improve performance.\n",
    "\n",
    "### **Why Cache Matters**\n",
    "\n",
    "**The Speed Gap**:\n",
    "```\n",
    "Storage Type        Access Time    Cost per GB\n",
    "────────────────────────────────────────────────\n",
    "CPU Registers       0.1-1 ns       $100,000+\n",
    "CPU L1 Cache        1-4 ns         $10,000\n",
    "CPU L2 Cache        4-10 ns        $5,000\n",
    "CPU L3 Cache        10-50 ns       $500\n",
    "RAM (Main Memory)   100 ns         $10\n",
    "SSD                 100,000 ns     $0.50\n",
    "HDD                 10,000,000 ns  $0.05\n",
    "\n",
    "Insight: RAM is 1,000x faster than SSD and 100,000x faster than HDD.\n",
    "```\n",
    "\n",
    "**Real-World Example**: Amazon's Prime Day\n",
    "```\n",
    "Without caching:\n",
    "- 1 million requests per second\n",
    "- Each request queries database (100ms)\n",
    "- Total database load: 100,000 concurrent queries (database crashes)\n",
    "\n",
    "With caching (95% cache hit rate):\n",
    "- 1 million requests per second\n",
    "- 950,000 requests served from cache (0.1ms)\n",
    "- 50,000 requests query database (100ms)\n",
    "- Total database load: 5,000 concurrent queries (database happy)\n",
    "\n",
    "Result: 20x less database load, 1000x faster response time for 95% of requests\n",
    "```\n",
    "\n",
    "### **The Caching Hierarchy**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Application Layer                         │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                   Browser Cache (Client)                     │\n",
    "│  - Static assets (images, CSS, JS)                           │\n",
    "│  - HTTP cache headers (Cache-Control, ETag)                  │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                  CDN Cache (Edge)                            │\n",
    "│  - Geographically distributed edge servers                   │\n",
    "│  - Static content and API responses                          │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│              Application Server Cache (Local)                │\n",
    "│  - In-memory cache (LRU, LFU)                                │\n",
    "│  - Hot data frequently accessed                              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                Distributed Cache (Redis/Memcached)            │\n",
    "│  - Shared cache across application servers                    │\n",
    "│  - Session data, user profiles, computed results             │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                    Database Cache                            │\n",
    "│  - Query cache (MySQL query cache)                           │\n",
    "│  - Buffer pool (PostgreSQL shared buffers)                   │\n",
    "│  - Index cache (B-Tree nodes in memory)                      │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                    Disk Cache (OS)                           │\n",
    "│  - Page cache (Linux file system cache)                      │\n",
    "│  - Buffer cache (frequently accessed disk blocks)            │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                   Storage (SSD/HDD)                          │\n",
    "│  - Persistent storage (slowest layer)                        │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Each layer is faster but more expensive (or smaller).\n",
    "Optimal strategy: Keep data as high as possible in the hierarchy.\n",
    "```\n",
    "\n",
    "### **Benefits of Caching**\n",
    "\n",
    "1. **Reduced Latency**: Serving from RAM (0.1ms) vs. disk (10ms) or network (100ms)\n",
    "2. **Reduced Database Load**: Fewer queries mean less expensive database infrastructure\n",
    "3. **Improved Scalability**: Application servers can handle more requests with less backend pressure\n",
    "4. **Cost Reduction**: Cache servers (Redis) are cheaper than database servers\n",
    "5. **Better User Experience**: Faster response times mean happier users\n",
    "\n",
    "### **Caching Trade-offs**\n",
    "\n",
    "1. **Complexity**: Caching adds complexity to application code\n",
    "2. **Stale Data**: Caches may serve outdated data\n",
    "3. **Consistency**: Keeping caches synchronized with data sources is challenging\n",
    "4. **Memory Costs**: In-memory caching requires significant RAM\n",
    "5. **Cache Invalidation**: Determining when to invalidate caches is non-trivial\n",
    "\n",
    "---\n",
    "\n",
    "## **4.2 Caching Patterns**\n",
    "\n",
    "Caching patterns describe how data moves between the cache, application, and primary data store.\n",
    "\n",
    "### **Cache-Aside (Lazy Loading)**\n",
    "\n",
    "**Concept**: Application code manages the cache. On cache miss, load data from database and populate cache.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "1. Application needs data\n",
    "2. Check cache\n",
    "   ├─→ Cache hit: Return data from cache (fast!)\n",
    "   └─→ Cache miss: Query database, populate cache, return data\n",
    "3. Next request: Data is in cache (cache hit)\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import redis\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Redis connection\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Database connection (simulated)\n",
    "database = {\n",
    "    'user:123': {\n",
    "        'id': 123,\n",
    "        'name': 'Alice Johnson',\n",
    "        'email': 'alice@example.com',\n",
    "        'created_at': '2024-01-15T08:30:00Z'\n",
    "    }\n",
    "}\n",
    "\n",
    "def get_user(user_id):\n",
    "    cache_key = f'user:{user_id}'\n",
    "    \n",
    "    # Step 1: Try to get from cache\n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        print(f\"Cache HIT for {cache_key}\")\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    # Step 2: Cache miss - get from database\n",
    "    print(f\"Cache MISS for {cache_key}\")\n",
    "    user_data = database.get(f'user:{user_id}')\n",
    "    if user_data is None:\n",
    "        return None  # User doesn't exist\n",
    "    \n",
    "    # Step 3: Populate cache for next request\n",
    "    # Set expiration to 1 hour (3600 seconds)\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "    \n",
    "    return user_data\n",
    "\n",
    "# First request: Cache miss (loads from database)\n",
    "user = get_user(123)\n",
    "print(f\"User: {user['name']}\")  # Output: \"User: Alice Johnson\"\n",
    "\n",
    "# Second request: Cache hit (loads from Redis)\n",
    "user = get_user(123)\n",
    "print(f\"User: {user['name']}\")  # Output: \"User: Alice Johnson\"\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Simple**: Easy to implement and understand\n",
    "- **On-demand**: Only cache what's actually accessed\n",
    "- **Flexible**: Application controls cache population\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Thundering herd**: Many simultaneous cache misses can overwhelm database\n",
    "- **Stale data**: Cache may contain outdated data until expiration\n",
    "- **Code duplication**: Cache logic embedded in application code\n",
    "\n",
    "**When to Use**:\n",
    "- Read-heavy workloads\n",
    "- Data accessed frequently\n",
    "- When you need control over what's cached\n",
    "\n",
    "---\n",
    "\n",
    "### **Read-Through**\n",
    "\n",
    "**Concept**: Cache library manages cache population. Application only interacts with cache; cache handles cache misses.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "1. Application requests data from cache\n",
    "2. Cache checks if data exists\n",
    "   ├─→ Cache hit: Return data\n",
    "   └─→ Cache miss: Cache loads from database, populates itself, returns data\n",
    "3. Application doesn't know about database (only interacts with cache)\n",
    "```\n",
    "\n",
    "**Implementation** (using Redis with read-through):\n",
    "```python\n",
    "class ReadThroughCache:\n",
    "    def __init__(self, redis_client, database):\n",
    "        self.redis_client = redis_client\n",
    "        self.database = database\n",
    "        self.default_ttl = 3600  # 1 hour\n",
    "    \n",
    "    def get(self, key, loader=None, ttl=None):\n",
    "        # Check cache\n",
    "        cached_data = self.redis_client.get(key)\n",
    "        if cached_data:\n",
    "            print(f\"Cache HIT for {key}\")\n",
    "            return json.loads(cached_data)\n",
    "        \n",
    "        # Cache miss - load from database\n",
    "        print(f\"Cache MISS for {key}\")\n",
    "        \n",
    "        # Use provided loader or default database lookup\n",
    "        data = loader(key) if loader else self.database.get(key)\n",
    "        if data is None:\n",
    "            return None\n",
    "        \n",
    "        # Populate cache\n",
    "        self.redis_client.setex(key, ttl or self.default_ttl, json.dumps(data))\n",
    "        return data\n",
    "    \n",
    "    def set(self, key, data, ttl=None):\n",
    "        self.redis_client.setex(key, ttl or self.default_ttl, json.dumps(data))\n",
    "    \n",
    "    def delete(self, key):\n",
    "        self.redis_client.delete(key)\n",
    "\n",
    "# Usage\n",
    "cache = ReadThroughCache(redis_client, database)\n",
    "\n",
    "# Custom loader function\n",
    "def load_user(user_id):\n",
    "    # Simulate database query\n",
    "    return database.get(f'user:{user_id}')\n",
    "\n",
    "# Application only interacts with cache (doesn't know about database)\n",
    "user = cache.get(f'user:{123}', loader=load_user)\n",
    "print(f\"User: {user['name']}\")\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Simpler application code**: Application doesn't handle cache misses\n",
    "- **Consistent caching behavior**: All cache operations go through cache library\n",
    "- **Reduced code duplication**: Cache logic centralized in cache library\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Less control**: Application has less control over cache behavior\n",
    "- **Complex cache library**: Cache library must handle database interaction\n",
    "- **Potential bottlenecks**: Cache library becomes single point of failure\n",
    "\n",
    "**When to Use**:\n",
    "- When you want to simplify application code\n",
    "- When multiple parts of application access same data\n",
    "- When you need consistent caching behavior across application\n",
    "\n",
    "---\n",
    "\n",
    "### **Write-Through**\n",
    "\n",
    "**Concept**: Write to both cache and database synchronously. Cache is always consistent with database.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "1. Application updates data\n",
    "2. Application writes to cache\n",
    "3. Application writes to database\n",
    "4. Both writes complete synchronously before returning\n",
    "\n",
    "Result: Cache and database always in sync\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "class WriteThroughCache:\n",
    "    def __init__(self, redis_client, database):\n",
    "        self.redis_client = redis_client\n",
    "        self.database = database\n",
    "    \n",
    "    def update_user(self, user_id, user_data):\n",
    "        # Update cache synchronously\n",
    "        cache_key = f'user:{user_id}'\n",
    "        print(f\"Updating cache for {cache_key}\")\n",
    "        self.redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "        \n",
    "        # Update database synchronously\n",
    "        print(f\"Updating database for user:{user_id}\")\n",
    "        self.database[f'user:{user_id}'] = user_data\n",
    "        \n",
    "        # Both updates complete before returning\n",
    "        return True\n",
    "    \n",
    "    def get_user(self, user_id):\n",
    "        cache_key = f'user:{user_id}'\n",
    "        \n",
    "        # Try cache first\n",
    "        cached_data = self.redis_client.get(cache_key)\n",
    "        if cached_data:\n",
    "            print(f\"Reading from cache: {cache_key}\")\n",
    "            return json.loads(cached_data)\n",
    "        \n",
    "        # Cache miss - read from database\n",
    "        print(f\"Reading from database: user:{user_id}\")\n",
    "        user_data = self.database.get(f'user:{user_id}')\n",
    "        if user_data:\n",
    "            # Populate cache (optional, for future reads)\n",
    "            self.redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "        \n",
    "        return user_data\n",
    "\n",
    "# Usage\n",
    "cache = WriteThroughCache(redis_client, database)\n",
    "\n",
    "# Update user (writes to both cache and database)\n",
    "cache.update_user(123, {\n",
    "    'id': 123,\n",
    "    'name': 'Alice Johnson',\n",
    "    'email': 'alice@example.com',\n",
    "    'updated_at': time.time()\n",
    "})\n",
    "\n",
    "# Read user (gets from cache - always up-to-date)\n",
    "user = cache.get_user(123)\n",
    "print(f\"User: {user['name']}\")\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Data consistency**: Cache always reflects latest data\n",
    "- **Read performance**: Reads always fast (data in cache)\n",
    "- **Simple mental model**: Easy to reason about data consistency\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Slower writes**: Each write requires two operations (cache + database)\n",
    "- **Higher latency**: Write latency = cache write time + database write time\n",
    "- **Potential cache pollution**: Unneeded data cached (if data never read)\n",
    "\n",
    "**When to Use**:\n",
    "- When data consistency is critical\n",
    "- When reads significantly outnumber writes\n",
    "- When you need guaranteed up-to-date data on reads\n",
    "\n",
    "---\n",
    "\n",
    "### **Write-Behind (Write-Back)**\n",
    "\n",
    "**Concept**: Write to cache immediately, asynchronously persist to database. Database is updated later in batches.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "1. Application updates data\n",
    "2. Application writes to cache (synchronous, fast)\n",
    "3. Cache queues write to database (asynchronous, slower)\n",
    "4. Database updated later (batched)\n",
    "5. Application returns immediately (doesn't wait for database write)\n",
    "\n",
    "Result: Fast writes, but potential data loss if cache fails before database write\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "class WriteBehindCache:\n",
    "    def __init__(self, redis_client, database):\n",
    "        self.redis_client = redis_client\n",
    "        self.database = database\n",
    "        self.write_queue = queue.Queue()\n",
    "        self.worker_thread = threading.Thread(target=self._write_worker, daemon=True)\n",
    "        self.worker_thread.start()\n",
    "    \n",
    "    def _write_worker(self):\n",
    "        \"\"\"Background thread to process write queue\"\"\"\n",
    "        while True:\n",
    "            operation = self.write_queue.get()\n",
    "            if operation is None:  # Poison pill\n",
    "                break\n",
    "            \n",
    "            # Process write operation\n",
    "            op_type, key, data = operation\n",
    "            if op_type == 'set':\n",
    "                self.database[key] = data\n",
    "                print(f\"Persisted to database: {key}\")\n",
    "            \n",
    "            self.write_queue.task_done()\n",
    "    \n",
    "    def set(self, key, data):\n",
    "        \"\"\"Write to cache immediately (synchronous)\"\"\"\n",
    "        # Write to cache (synchronous)\n",
    "        self.redis_client.setex(key, 3600, json.dumps(data))\n",
    "        \n",
    "        # Queue database write (asynchronous)\n",
    "        self.write_queue.put(('set', key, data))\n",
    "        \n",
    "        # Return immediately (don't wait for database write)\n",
    "        return True\n",
    "    \n",
    "    def get(self, key):\n",
    "        \"\"\"Read from cache (should be up-to-date)\"\"\"\n",
    "        cached_data = self.redis_client.get(key)\n",
    "        if cached_data:\n",
    "            return json.loads(cached_data)\n",
    "        \n",
    "        # Cache miss - read from database\n",
    "        data = self.database.get(key)\n",
    "        if data:\n",
    "            # Populate cache (for future reads)\n",
    "            self.redis_client.setex(key, 3600, json.dumps(data))\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def shutdown(self):\n",
    "        \"\"\"Wait for pending writes to complete\"\"\"\n",
    "        self.write_queue.join()\n",
    "        self.write_queue.put(None)  # Poison pill\n",
    "        self.worker_thread.join()\n",
    "\n",
    "# Usage\n",
    "cache = WriteBehindCache(redis_client, database)\n",
    "\n",
    "# Update user (fast - doesn't wait for database write)\n",
    "cache.set('user:123', {\n",
    "    'id': 123,\n",
    "    'name': 'Alice Johnson',\n",
    "    'email': 'alice@example.com'\n",
    "})\n",
    "print(\"User updated (immediate return)\")\n",
    "\n",
    "# Read user (gets from cache)\n",
    "user = cache.get('user:123')\n",
    "print(f\"User: {user['name']}\")\n",
    "\n",
    "# Later, when shutting down, ensure all writes are persisted\n",
    "cache.shutdown()\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Very fast writes**: No database write latency (returns immediately)\n",
    "- **Batched writes**: Database writes can be batched for efficiency\n",
    "- **Reduced database load**: Fewer database operations\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Data loss risk**: If cache fails before database write, data is lost\n",
    "- **Complexity**: Requires background thread/process for writes\n",
    "- **Eventual consistency**: Data in database may be stale until write completes\n",
    "- **Write ordering**: Writes may be processed out of order (need queue ordering)\n",
    "\n",
    "**When to Use**:\n",
    "- When write latency is critical (real-time systems)\n",
    "- When you can tolerate potential data loss (non-critical data)\n",
    "- When you need to reduce database load (write-heavy workloads)\n",
    "\n",
    "---\n",
    "\n",
    "### **Refresh-Ahead**\n",
    "\n",
    "**Concept**: Proactively refresh cache entries before they expire, ensuring cache is always populated.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "1. Application reads from cache\n",
    "2. If cache entry is about to expire (e.g., within 10% of TTL), refresh it\n",
    "3. Refresh happens asynchronously in background\n",
    "4. Next request gets fresh data (no cache miss)\n",
    "\n",
    "Result: Cache always populated (no cache misses for hot data)\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "import threading\n",
    "\n",
    "class RefreshAheadCache:\n",
    "    def __init__(self, redis_client, database, refresh_threshold=0.1):\n",
    "        self.redis_client = redis_client\n",
    "        self.database = database\n",
    "        self.refresh_threshold = refresh_threshold  # Refresh when 10% of TTL remaining\n",
    "    \n",
    "    def _refresh_if_needed(self, key, ttl):\n",
    "        \"\"\"Refresh cache entry if it's about to expire\"\"\"\n",
    "        ttl_remaining = self.redis_client.ttl(key)\n",
    "        \n",
    "        # If TTL remaining is less than threshold, refresh\n",
    "        if ttl_remaining > 0 and ttl_remaining < (ttl * self.refresh_threshold):\n",
    "            print(f\"Refreshing cache for {key} (TTL remaining: {ttl_remaining}s)\")\n",
    "            \n",
    "            # Get data from database\n",
    "            user_id = key.split(':')[1]\n",
    "            data = self.database.get(f'user:{user_id}')\n",
    "            if data:\n",
    "                # Refresh cache with new TTL\n",
    "                self.redis_client.setex(key, ttl, json.dumps(data))\n",
    "    \n",
    "    def get(self, key, loader=None, ttl=3600):\n",
    "        # Try to get from cache\n",
    "        cached_data = self.redis_client.get(key)\n",
    "        if cached_data:\n",
    "            print(f\"Cache HIT for {key}\")\n",
    "            \n",
    "            # Refresh if needed (in background)\n",
    "            self._refresh_if_needed(key, ttl)\n",
    "            \n",
    "            return json.loads(cached_data)\n",
    "        \n",
    "        # Cache miss - load from database\n",
    "        print(f\"Cache MISS for {key}\")\n",
    "        \n",
    "        # Load data\n",
    "        data = loader(key) if loader else self.database.get(key)\n",
    "        if data is None:\n",
    "            return None\n",
    "        \n",
    "        # Populate cache\n",
    "        self.redis_client.setex(key, ttl, json.dumps(data))\n",
    "        return data\n",
    "\n",
    "# Usage\n",
    "cache = RefreshAheadCache(redis_client, database)\n",
    "\n",
    "def load_user(key):\n",
    "    user_id = key.split(':')[1]\n",
    "    return database.get(f'user:{user_id}')\n",
    "\n",
    "# First request: Cache miss\n",
    "user = cache.get('user:123', loader=load_user, ttl=60)  # 60 second TTL\n",
    "\n",
    "# Wait 50 seconds (50 of 60 seconds elapsed, within 10% threshold)\n",
    "time.sleep(50)\n",
    "\n",
    "# Next request: Cache hit, but refresh triggered in background\n",
    "user = cache.get('user:123', loader=load_user, ttl=60)\n",
    "print(f\"User: {user['name']}\")\n",
    "\n",
    "# Wait 10 more seconds (TTL elapsed, but cache was refreshed)\n",
    "time.sleep(10)\n",
    "\n",
    "# Next request: Cache hit (no miss because of refresh-ahead)\n",
    "user = cache.get('user:123', loader=load_user, ttl=60)\n",
    "print(f\"User: {user['name']}\")\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Eliminates cache misses**: Hot data always in cache\n",
    "- **Better user experience**: No latency spikes from cache misses\n",
    "- **Reduced database load**: Fewer cache misses mean fewer database queries\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Complexity**: Requires background refresh logic\n",
    "- **Wasted refreshes**: May refresh data that's never accessed again\n",
    "- **Prediction difficulty**: Hard to predict which data will be accessed next\n",
    "\n",
    "**When to Use**:\n",
    "- For hot data accessed frequently\n",
    "- When cache misses are expensive (slow queries)\n",
    "- When user experience is critical (no latency spikes)\n",
    "\n",
    "---\n",
    "\n",
    "### **Pattern Comparison**\n",
    "\n",
    "```\n",
    "┌──────────────────┬───────────────┬──────────────┬────────────────┐\n",
    "│     Pattern      │ Read Latency  │ Write Latency│ Consistency    │\n",
    "├──────────────────┼───────────────┼──────────────┼────────────────┤\n",
    "│ Cache-Aside      │ Miss: Slow    │ Normal       │ Eventual       │\n",
    "│                  │ Hit: Fast     │              │                │\n",
    "├──────────────────┼───────────────┼──────────────┼────────────────┤\n",
    "│ Read-Through     │ Miss: Slow    │ Normal       │ Eventual       │\n",
    "│                  │ Hit: Fast     │              │                │\n",
    "├──────────────────┼───────────────┼──────────────┼────────────────┤\n",
    "│ Write-Through    │ Fast          │ Slow         │ Strong         │\n",
    "├──────────────────┼───────────────┼──────────────┼────────────────┤\n",
    "│ Write-Behind     │ Fast          │ Very Fast    │ Eventual       │\n",
    "├──────────────────┼───────────────┼──────────────┼────────────────┤\n",
    "│ Refresh-Ahead    │ Fast          │ Normal       │ Strong (mostly)│\n",
    "└──────────────────┴───────────────┴──────────────┴────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.3 Cache Eviction Policies**\n",
    "\n",
    "Cache eviction policies determine which data to remove when cache is full. Understanding these policies is critical for maintaining cache efficiency.\n",
    "\n",
    "### **LRU (Least Recently Used)**\n",
    "\n",
    "**Concept**: Evict the item that hasn't been accessed for the longest time. Based on temporal locality—recently accessed items likely to be accessed again soon.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "Cache capacity: 3 items\n",
    "Items: A, B, C, D, E\n",
    "\n",
    "1. Access A: Cache: [A]\n",
    "2. Access B: Cache: [A, B]\n",
    "3. Access C: Cache: [A, B, C]  (cache full)\n",
    "4. Access D: Evict A (least recently used) → Cache: [B, C, D]\n",
    "5. Access B: Cache: [C, D, B]  (B moved to most recently used)\n",
    "6. Access E: Evict C (least recently used) → Cache: [D, B, E]\n",
    "\n",
    "Access order: A → B → C → D → B → E\n",
    "Eviction order: A, C, ...\n",
    "```\n",
    "\n",
    "**Implementation** (using Redis, which uses an approximation of LRU):\n",
    "```python\n",
    "import redis\n",
    "\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Set maxmemory and eviction policy\n",
    "redis_client.config_set('maxmemory', '100mb')\n",
    "redis_client.config_set('maxmemory-policy', 'allkeys-lru')  # LRU eviction\n",
    "\n",
    "# Add items to cache\n",
    "redis_client.set('user:1', '{\"name\": \"Alice\"}')\n",
    "redis_client.set('user:2', '{\"name\": \"Bob\"}')\n",
    "redis_client.set('user:3', '{\"name\": \"Charlie\"}')\n",
    "redis_client.set('user:4', '{\"name\": \"Dave\"}')  # Evicts user:1 (LRU)\n",
    "\n",
    "# Access user:2 (makes it most recently used)\n",
    "redis_client.get('user:2')\n",
    "\n",
    "# Add more items (evicts user:3 next)\n",
    "redis_client.set('user:5', '{\"name\": \"Eve\"}')\n",
    "redis_client.set('user:6', '{\"name\": \"Frank\"}')  # Evicts user:3\n",
    "```\n",
    "\n",
    "**LRU Implementation from Scratch**:\n",
    "```python\n",
    "from collections import OrderedDict\n",
    "\n",
    "class LRUCache:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.cache = OrderedDict()  # Maintains insertion order\n",
    "    \n",
    "    def get(self, key):\n",
    "        # Move to end (most recently used)\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "            return self.cache[key]\n",
    "        return None\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        # Update existing or add new\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        self.cache[key] = value\n",
    "        \n",
    "        # Evict if over capacity\n",
    "        if len(self.cache) > self.capacity:\n",
    "            # Pop from front (least recently used)\n",
    "            self.cache.popitem(last=False)\n",
    "\n",
    "# Usage\n",
    "cache = LRUCache(3)\n",
    "\n",
    "cache.set('A', 'Data A')\n",
    "cache.set('B', 'Data B')\n",
    "cache.set('C', 'Data C')\n",
    "# Cache: [A, B, C]\n",
    "\n",
    "cache.set('D', 'Data D')  # Evicts A\n",
    "# Cache: [B, C, D]\n",
    "\n",
    "print(cache.get('B'))  # Returns 'Data B', B becomes most recently used\n",
    "# Cache: [C, D, B]\n",
    "\n",
    "cache.set('E', 'Data E')  # Evicts C\n",
    "# Cache: [D, B, E]\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Intuitive**: Simple to understand—least recently used is least likely needed\n",
    "- **Good for temporal locality**: Works well when recent access predicts future access\n",
    "- **Efficient**: O(1) operations with proper implementation\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Scan resistant**: Sequential scans can evict useful data\n",
    "- **Requires access tracking**: Need to track access order\n",
    "- **Memory overhead**: Maintains access order structure\n",
    "\n",
    "**When to Use**:\n",
    "- Workloads with strong temporal locality (access patterns cluster in time)\n",
    "- General-purpose caching (good default choice)\n",
    "- When you can't predict access patterns\n",
    "\n",
    "---\n",
    "\n",
    "### **LFU (Least Frequently Used)**\n",
    "\n",
    "**Concept**: Evict the item with the lowest access frequency. Based on frequency locality—frequently accessed items are likely to be accessed again.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "Cache capacity: 3 items\n",
    "Items: A (accessed 5 times), B (accessed 3 times), C (accessed 2 times), D\n",
    "\n",
    "1. Access A: Cache: [A:1]\n",
    "2. Access A: Cache: [A:2]\n",
    "3. Access B: Cache: [A:2, B:1]\n",
    "4. Access C: Cache: [A:2, B:1, C:1]  (cache full)\n",
    "5. Access D: Evict C (least frequently used) → Cache: [A:2, B:1, D:1]\n",
    "6. Access A: Cache: [A:3, B:1, D:1]\n",
    "7. Access E: Evict B and D (tie, evict least recently among them) → Cache: [A:3, E:1]\n",
    "\n",
    "Access frequency: A (highest), B/D (tied), C (evicted)\n",
    "```\n",
    "\n",
    "**Implementation** (Redis LFU):\n",
    "```python\n",
    "import redis\n",
    "\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Configure Redis for LFU eviction\n",
    "redis_client.config_set('maxmemory', '100mb')\n",
    "redis_client.config_set('maxmemory-policy', 'allkeys-lfu')  # LFU eviction\n",
    "\n",
    "# Add items\n",
    "redis_client.set('user:1', '{\"name\": \"Alice\"}')\n",
    "redis_client.set('user:2', '{\"name\": \"Bob\"}')\n",
    "redis_client.set('user:3', '{\"name\": \"Charlie\"}')\n",
    "\n",
    "# Access user:1 multiple times (increases its LFU counter)\n",
    "for _ in range(10):\n",
    "    redis_client.get('user:1')\n",
    "\n",
    "# Access user:2 a few times\n",
    "for _ in range(3):\n",
    "    redis_client.get('user:2')\n",
    "\n",
    "# Add new items (user:3 evicted first - least frequently accessed)\n",
    "redis_client.set('user:4', '{\"name\": \"Dave\"}')\n",
    "redis_client.set('user:5', '{\"name\": \"Eve\"}')\n",
    "\n",
    "# user:1 (high frequency) still in cache\n",
    "# user:2 (medium frequency) still in cache\n",
    "# user:4 and user:5 in cache (user:3 evicted)\n",
    "```\n",
    "\n",
    "**LFU Implementation from Scratch**:\n",
    "```python\n",
    "import heapq\n",
    "import time\n",
    "\n",
    "class LFUCache:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.cache = {}  # key → (value, frequency, timestamp)\n",
    "        self.frequency_heap = []  # (frequency, timestamp, key)\n",
    "        self.timestamp = 0  # Monotonic counter for tie-breaking\n",
    "    \n",
    "    def _update_frequency(self, key):\n",
    "        \"\"\"Update frequency and reinsert into heap\"\"\"\n",
    "        value, frequency, _ = self.cache[key]\n",
    "        self.timestamp += 1\n",
    "        self.cache[key] = (value, frequency + 1, self.timestamp)\n",
    "        heapq.heappush(self.frequency_heap, (frequency + 1, self.timestamp, key))\n",
    "    \n",
    "    def get(self, key):\n",
    "        if key not in self.cache:\n",
    "            return None\n",
    "        \n",
    "        self._update_frequency(key)\n",
    "        return self.cache[key][0]\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        # Update existing\n",
    "        if key in self.cache:\n",
    "            self.cache[key] = (value, self.cache[key][1], self.cache[key][2])\n",
    "            self._update_frequency(key)\n",
    "            return\n",
    "        \n",
    "        # Add new\n",
    "        self.timestamp += 1\n",
    "        self.cache[key] = (value, 1, self.timestamp)\n",
    "        heapq.heappush(self.frequency_heap, (1, self.timestamp, key))\n",
    "        \n",
    "        # Evict if over capacity\n",
    "        if len(self.cache) > self.capacity:\n",
    "            # Find lowest frequency item\n",
    "            while self.frequency_heap:\n",
    "                freq, ts, k = self.frequency_heap[0]\n",
    "                \n",
    "                # Check if this entry is stale (frequency updated)\n",
    "                if k in self.cache and self.cache[k][1] == freq and self.cache[k][2] == ts:\n",
    "                    # Valid entry - evict it\n",
    "                    heapq.heappop(self.frequency_heap)\n",
    "                    del self.cache[k]\n",
    "                    break\n",
    "                else:\n",
    "                    # Stale entry - skip\n",
    "                    heapq.heappop(self.frequency_heap)\n",
    "\n",
    "# Usage\n",
    "cache = LFUCache(3)\n",
    "\n",
    "cache.set('A', 'Data A')\n",
    "cache.set('B', 'Data B')\n",
    "cache.set('C', 'Data C')\n",
    "\n",
    "# Access A multiple times\n",
    "for _ in range(5):\n",
    "    cache.get('A')\n",
    "\n",
    "# Access B a few times\n",
    "for _ in range(2):\n",
    "    cache.get('B')\n",
    "\n",
    "# Add D (evicts C - least frequently used)\n",
    "cache.set('D', 'Data D')\n",
    "\n",
    "# Access A (still in cache - high frequency)\n",
    "print(cache.get('A'))  # Returns 'Data A'\n",
    "\n",
    "# Add E (evicts B - second least frequently used)\n",
    "cache.set('E', 'Data E')\n",
    "\n",
    "print(cache.get('A'))  # Still 'Data A' (highest frequency)\n",
    "print(cache.get('D'))  # Still 'Data D' (tied with E, but accessed first)\n",
    "print(cache.get('E'))  # Still 'Data E'\n",
    "print(cache.get('B'))  # None (evicted)\n",
    "print(cache.get('C'))  # None (evicted)\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Frequency-aware**: Keeps popular items in cache longer\n",
    "- **Scan resistant**: Sequential scans don't evict popular items\n",
    "- **Good for long-term patterns**: Works well when access patterns are stable\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Memory overhead**: Tracks access frequency for each item\n",
    "- **Cold start problem**: New items have low frequency, get evicted quickly\n",
    "- **Frequency decay**: May keep old popular items that are no longer accessed\n",
    "\n",
    "**When to Use**:\n",
    "- Workloads with stable access patterns (some items always popular)\n",
    "- When you want to protect popular items from eviction\n",
    "- When access patterns are frequency-based (not time-based)\n",
    "\n",
    "---\n",
    "\n",
    "### **TTL (Time To Live)**\n",
    "\n",
    "**Concept**: Each cache entry has an expiration time. When TTL expires, entry is evicted automatically.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "Cache capacity: Unlimited (but each item expires)\n",
    "\n",
    "1. Set A with TTL 60 seconds: Cache: [A (expires in 60s)]\n",
    "2. Set B with TTL 120 seconds: Cache: [A (50s), B (120s)]\n",
    "3. Wait 60 seconds: Cache: [B (60s)] (A expired)\n",
    "4. Set C with TTL 30 seconds: Cache: [B (60s), C (30s)]\n",
    "5. Wait 30 seconds: Cache: [B (30s)] (C expired)\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "\n",
    "class TTLCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}  # key → (value, expiration_time)\n",
    "    \n",
    "    def set(self, key, value, ttl_seconds):\n",
    "        \"\"\"Set key with TTL (time to live)\"\"\"\n",
    "        expiration_time = time.time() + ttl_seconds\n",
    "        self.cache[key] = (value, expiration_time)\n",
    "    \n",
    "    def get(self, key):\n",
    "        \"\"\"Get key if not expired\"\"\"\n",
    "        if key not in self.cache:\n",
    "            return None\n",
    "        \n",
    "        value, expiration_time = self.cache[key]\n",
    "        \n",
    "        # Check if expired\n",
    "        if time.time() > expiration_time:\n",
    "            del self.cache[key]\n",
    "            return None\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def cleanup_expired(self):\n",
    "        \"\"\"Remove all expired entries\"\"\"\n",
    "        current_time = time.time()\n",
    "        expired_keys = [\n",
    "            key for key, (_, exp_time) in self.cache.items()\n",
    "            if current_time > exp_time\n",
    "        ]\n",
    "        for key in expired_keys:\n",
    "            del self.cache[key]\n",
    "        return len(expired_keys)\n",
    "\n",
    "# Usage\n",
    "cache = TTLCache()\n",
    "\n",
    "# Set items with different TTLs\n",
    "cache.set('user:1', '{\"name\": \"Alice\"}', ttl_seconds=60)  # 1 minute\n",
    "cache.set('user:2', '{\"name\": \"Bob\"}', ttl_seconds=120)   # 2 minutes\n",
    "\n",
    "# Get item before expiration\n",
    "print(cache.get('user:1'))  # Returns '{\"name\": \"Alice\"}'\n",
    "\n",
    "# Wait 70 seconds\n",
    "time.sleep(70)\n",
    "\n",
    "# user:1 expired, user:2 still valid\n",
    "print(cache.get('user:1'))  # Returns None (expired)\n",
    "print(cache.get('user:2'))  # Returns '{\"name\": \"Bob\"}'\n",
    "\n",
    "# Cleanup expired entries\n",
    "expired_count = cache.cleanup_expired()\n",
    "print(f\"Cleaned up {expired_count} expired entries\")\n",
    "```\n",
    "\n",
    "**Redis TTL Example**:\n",
    "```python\n",
    "import redis\n",
    "import time\n",
    "\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "# Set key with TTL\n",
    "redis_client.setex('session:123', 'user_data', 3600)  # Expires in 1 hour\n",
    "\n",
    "# Check TTL\n",
    "ttl = redis_client.ttl('session:123')\n",
    "print(f\"Session expires in {ttl} seconds\")\n",
    "\n",
    "# Update TTL (refresh session)\n",
    "redis_client.expire('session:123', 7200)  # Extend to 2 hours\n",
    "\n",
    "# Wait and check again\n",
    "time.sleep(60)\n",
    "ttl = redis_client.ttl('session:123')\n",
    "print(f\"Session expires in {ttl} seconds\")  # Should be ~7140 seconds\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Automatic expiration**: No manual eviction needed\n",
    "- **Freshness guarantee**: Data never older than TTL\n",
    "- **Simple**: Easy to understand and implement\n",
    "\n",
    "**Disadvantages**:\n",
    "- **TTL selection**: Choosing optimal TTL is difficult\n",
    "- **Cache stampede**: Many items expiring simultaneously causes load spikes\n",
    "- **No reuse**: Items evicted even if they would be accessed again\n",
    "\n",
    "**When to Use**:\n",
    "- Data with natural expiration (sessions, tokens)\n",
    "- Time-sensitive data (stock prices, weather data)\n",
    "- When you want guaranteed freshness\n",
    "\n",
    "---\n",
    "\n",
    "### **Random Replacement**\n",
    "\n",
    "**Concept**: Randomly select an item to evict when cache is full. Simple but surprisingly effective in some scenarios.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "Cache capacity: 3 items\n",
    "Items: A, B, C, D, E\n",
    "\n",
    "1. Add A: Cache: [A]\n",
    "2. Add B: Cache: [A, B]\n",
    "3. Add C: Cache: [A, B, C]  (cache full)\n",
    "4. Add D: Randomly evict C → Cache: [A, B, D]\n",
    "5. Add E: Randomly evict B → Cache: [A, D, E]\n",
    "\n",
    "Random eviction: C, B, ...\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import random\n",
    "\n",
    "class RandomReplacementCache:\n",
    "    def __init__(self, capacity):\n",
    "        self.capacity = capacity\n",
    "        self.cache = {}  # key → value\n",
    "    \n",
    "    def get(self, key):\n",
    "        return self.cache.get(key)\n",
    "    \n",
    "    def set(self, key, value):\n",
    "        # Update existing or add new\n",
    "        if key in self.cache:\n",
    "            self.cache[key] = value\n",
    "            return\n",
    "        \n",
    "        self.cache[key] = value\n",
    "        \n",
    "        # Evict random item if over capacity\n",
    "        if len(self.cache) > self.capacity:\n",
    "            # Choose random key to evict\n",
    "            key_to_evict = random.choice(list(self.cache.keys()))\n",
    "            del self.cache[key_to_evict]\n",
    "\n",
    "# Usage\n",
    "cache = RandomReplacementCache(3)\n",
    "\n",
    "cache.set('A', 'Data A')\n",
    "cache.set('B', 'Data B')\n",
    "cache.set('C', 'Data C')\n",
    "\n",
    "cache.set('D', 'Data D')  # Randomly evicts one of A, B, or C\n",
    "\n",
    "print(cache.get('A'))  # Might be None if A was evicted\n",
    "print(cache.get('B'))  # Might be None if B was evicted\n",
    "print(cache.get('C'))  # Might be None if C was evicted\n",
    "print(cache.get('D'))  # Returns 'Data D'\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Simplest to implement**: No tracking needed\n",
    "- **Fast**: O(1) operations with minimal overhead\n",
    "- **Fair**: All items have equal chance of staying in cache\n",
    "\n",
    "**Disadvantages**:\n",
    "- **No intelligence**: Evicts potentially useful items\n",
    "- **Poor performance**: Usually performs worse than LRU/LFU\n",
    "- **Unpredictable**: Hard to reason about cache contents\n",
    "\n",
    "**When to Use**:\n",
    "- When simplicity is more important than performance\n",
    "- When access patterns are truly random (no patterns)\n",
    "- As a baseline for comparison with smarter policies\n",
    "\n",
    "---\n",
    "\n",
    "### **Eviction Policy Comparison**\n",
    "\n",
    "```\n",
    "┌───────────────────┬──────────────┬──────────────┬────────────────┐\n",
    "│     Policy        │ Hit Rate     │ Complexity   │ Best For       │\n",
    "├───────────────────┼──────────────┼──────────────┼────────────────┤\n",
    "│ LRU               │ Good         │ Medium       │ General-purpose │\n",
    "│ LFU               │ Very Good    │ High         │ Stable patterns │\n",
    "│ TTL               │ Variable     │ Low          │ Time-sensitive  │\n",
    "│ Random            │ Poor         │ Very Low     │ Random access   │\n",
    "│ FIFO              │ Poor         │ Low          │ Simple use cases│\n",
    "└───────────────────┴──────────────┴──────────────┴────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.4 Distributed Caching**\n",
    "\n",
    "**Distributed Cache**: A cache that spans multiple machines, sharing the cache load and providing fault tolerance.\n",
    "\n",
    "### **Why Distributed Caching?**\n",
    "\n",
    "**Problem**: Single-machine cache has limits.\n",
    "```\n",
    "Single Cache Server:\n",
    "- Memory: 64 GB (max affordable RAM)\n",
    "- QPS: 100,000 (single machine limit)\n",
    "- SPOF: Single point of failure (if it crashes, all cached data lost)\n",
    "\n",
    "Distributed Cache:\n",
    "- Memory: 64 GB × 10 servers = 640 GB (10x more)\n",
    "- QPS: 100,000 × 10 servers = 1,000,000 (10x more)\n",
    "- High availability: If one server fails, others continue serving\n",
    "```\n",
    "\n",
    "### **Redis Cluster: Distributed In-Memory Cache**\n",
    "\n",
    "**Redis Cluster**: Distributed Redis implementation that automatically shards data across multiple nodes.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Application Servers\n",
    "    │\n",
    "    ├───► Redis Cluster\n",
    "    │       │\n",
    "    │       ├─── Node 1 (Shards 0-5460)\n",
    "    │       ├─── Node 2 (Shards 5461-10922)\n",
    "    │       └─── Node 3 (Shards 10923-16383)\n",
    "    │\n",
    "    └───► Application Servers (each knows which node has which shard)\n",
    "\n",
    "Sharding: 16384 hash slots (shards)\n",
    "- Each key is hashed to determine its slot\n",
    "- Each node manages a range of slots\n",
    "- Automatic failover if node fails\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import redis\n",
    "from redis.cluster import RedisCluster\n",
    "\n",
    "# Connect to Redis Cluster\n",
    "redis_cluster = RedisCluster(\n",
    "    startup_nodes=[\n",
    "        {\"host\": \"redis-node1\", \"port\": 6379},\n",
    "        {\"host\": \"redis-node2\", \"port\": 6379},\n",
    "        {\"host\": \"redis-node3\", \"port\": 6379}\n",
    "    ],\n",
    "    decode_responses=True\n",
    ")\n",
    "\n",
    "# Set value (automatically routed to correct node)\n",
    "redis_cluster.set('user:123', '{\"name\": \"Alice\"}')\n",
    "\n",
    "# Get value (automatically routed to correct node)\n",
    "user_data = redis_cluster.get('user:123')\n",
    "\n",
    "# Pipeline (multiple operations, distributed across cluster)\n",
    "pipe = redis_cluster.pipeline()\n",
    "pipe.set('user:1', '{\"name\": \"Alice\"}')\n",
    "pipe.set('user:2', '{\"name\": \"Bob\"}')\n",
    "pipe.set('user:3', '{\"name\": \"Charlie\"}')\n",
    "results = pipe.execute()\n",
    "\n",
    "# Transaction (multi-key operation, must be on same node)\n",
    "pipe = redis_cluster.pipeline(transaction=True)\n",
    "pipe.set('counter:views', 0)\n",
    "pipe.incr('counter:views')\n",
    "pipe.incr('counter:views')\n",
    "results = pipe.execute()\n",
    "\n",
    "# Check which node has a key\n",
    "slot = redis_cluster.keyslot('user:123')\n",
    "node_info = redis_cluster.cluster_nodes()\n",
    "print(f\"user:123 is on slot {slot}\")\n",
    "```\n",
    "\n",
    "**Redis Cluster Features**:\n",
    "- **Automatic sharding**: Data distributed across nodes using hash slots\n",
    "- **Automatic failover**: If master fails, replica promoted to master\n",
    "- **Horizontal scaling**: Add nodes to increase capacity\n",
    "- **Redis compatibility**: Same Redis commands and data structures\n",
    "- **High availability**: Multiple replicas for each master\n",
    "\n",
    "**Redis Cluster Limitations**:\n",
    "- **Multi-key operations**: Keys must be in same slot (use hash tags)\n",
    "- **Cross-slot transactions**: Not supported\n",
    "- **Smaller keys**: Recommended to keep keys under 512 KB\n",
    "- **Network partitions**: Requires majority of masters to be available\n",
    "\n",
    "**Hash Tags**: Ensuring related keys are on same node.\n",
    "```python\n",
    "# Use hash tags {} to ensure keys are on same node\n",
    "# Hash tags are only part of key used for hashing\n",
    "\n",
    "# These keys will be on different nodes (no hash tag)\n",
    "redis_cluster.set('user:123:name', 'Alice')\n",
    "redis_cluster.set('user:123:email', 'alice@example.com')\n",
    "# Problem: Can't use transaction (different nodes)\n",
    "\n",
    "# These keys will be on same node (using hash tag)\n",
    "redis_cluster.set('{user:123}:name', 'Alice')\n",
    "redis_cluster.set('{user:123}:email', 'alice@example.com')\n",
    "# Only {user:123} is used for hashing → same node → transaction works\n",
    "\n",
    "# Transaction (multi-key operation on same node)\n",
    "pipe = redis_cluster.pipeline(transaction=True)\n",
    "pipe.set('{user:123}:name', 'Alice')\n",
    "pipe.set('{user:123}:email', 'alice@example.com')\n",
    "results = pipe.execute()  # Works!\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Memcached: Simple Distributed Cache**\n",
    "\n",
    "**Memcached**: High-performance, distributed memory object caching system. Simpler than Redis but less feature-rich.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Application Servers\n",
    "    │\n",
    "    ├───► Memcached Client\n",
    "    │       │\n",
    "    │       ├───► Consistent Hashing\n",
    "    │       │       │\n",
    "    │       │       ├─── Node 1\n",
    "    │       │       ├─── Node 2\n",
    "    │       │       └─── Node 3\n",
    "    │       │\n",
    "    │       └───► Routes key to correct node\n",
    "    │\n",
    "    └───► Application Servers (each has client library)\n",
    "\n",
    "Key difference from Redis Cluster:\n",
    "- No built-in clustering (client-side sharding)\n",
    "- No persistence (purely in-memory)\n",
    "- Simpler data structures (only key-value)\n",
    "- Faster for simple use cases\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import memcache\n",
    "\n",
    "# Connect to Memcached cluster\n",
    "mc = memcache.Client(['memcached-node1:11211', 'memcached-node2:11211', 'memcached-node3:11211'])\n",
    "\n",
    "# Set value\n",
    "mc.set('user:123', '{\"name\": \"Alice\"}', time=3600)  # 1 hour TTL\n",
    "\n",
    "# Get value\n",
    "user_data = mc.get('user:123')\n",
    "\n",
    "# Set multiple values\n",
    "mc.set_multi({\n",
    "    'user:1': '{\"name\": \"Alice\"}',\n",
    "    'user:2': '{\"name\": \"Bob\"}',\n",
    "    'user:3': '{\"name\": \"Charlie\"}'\n",
    "}, time=3600)\n",
    "\n",
    "# Get multiple values\n",
    "users = mc.get_multi(['user:1', 'user:2', 'user:3'])\n",
    "\n",
    "# Increment (atomic operation)\n",
    "mc.set('counter:views', 0)\n",
    "mc.incr('counter:views')\n",
    "mc.incr('counter:views')\n",
    "views = mc.get('counter:views')  # Returns 2\n",
    "\n",
    "# Add (only sets if key doesn't exist)\n",
    "mc.add('lock:resource:123', 'locked', time=10)  # 10 second lock\n",
    "\n",
    "# Delete\n",
    "mc.delete('user:123')\n",
    "```\n",
    "\n",
    "**Memcached vs. Redis**:\n",
    "```\n",
    "┌───────────────────────┬──────────────────┬─────────────────────┐\n",
    "│ Feature               │ Memcached        │ Redis               │\n",
    "├───────────────────────┼──────────────────┼─────────────────────┤\n",
    "│ Data Structures       │ Key-value only   │ Rich (lists, sets,  │\n",
    "│                       │                  │ hashes, sorted sets) │\n",
    "├───────────────────────┼──────────────────┼─────────────────────┤\n",
    "│ Persistence           │ None             │ RDB, AOF            │\n",
    "├───────────────────────┼──────────────────┼─────────────────────┤\n",
    "│ Replication           │ None             │ Master-slave        │\n",
    "├───────────────────────┼──────────────────┼─────────────────────┤\n",
    "│ Clustering            │ Client-side      │ Built-in            │\n",
    "├───────────────────────┼──────────────────┼─────────────────────┤\n",
    "│ Memory Usage          │ Lower            │ Higher              │\n",
    "├───────────────────────┼──────────────────┼─────────────────────┤\n",
    "│ Performance           │ Very Fast        │ Fast                │\n",
    "├───────────────────────┼──────────────────┼─────────────────────┤\n",
    "│ Complexity            │ Simple           │ More complex        │\n",
    "└───────────────────────┴──────────────────┴─────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Consistent Hashing in Distributed Caches**\n",
    "\n",
    "**Problem**: When adding/removing cache nodes, most keys need to be remapped.\n",
    "\n",
    "**Solution**: Consistent hashing—minimizes key remapping when nodes change.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "Ring Visualization:\n",
    "\n",
    "                    Key: user:123\n",
    "                          ↓\n",
    "    ┌────────────────────────────────────────┐\n",
    "    │            Ring                         │\n",
    "    │                                        │\n",
    "    │  Key: user:456      Node A            │\n",
    "    │       ↓               ↑                │\n",
    "    │  Node B ───► Node C ───► Node D       │\n",
    "    │      ↑                   ↓             │\n",
    "    │  Key: user:789   Key: user:000       │\n",
    "    │                                        │\n",
    "    └────────────────────────────────────────┘\n",
    "\n",
    "Key assignment rule: Each key assigned to next node clockwise\n",
    "\n",
    "Adding Node E:\n",
    "- Node E added between C and D\n",
    "- Only keys between C and E move to E\n",
    "- Other keys stay on same nodes (minimal remapping)\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import hashlib\n",
    "from bisect import bisect_left\n",
    "\n",
    "class ConsistentHash:\n",
    "    def __init__(self, nodes=None, replicas=100):\n",
    "        \"\"\"Initialize consistent hash ring\n",
    "        \n",
    "        Args:\n",
    "            nodes: List of node identifiers\n",
    "            replicas: Number of virtual nodes per physical node\n",
    "        \"\"\"\n",
    "        self.replicas = replicas\n",
    "        self.ring = []\n",
    "        self.nodes = {}  # hash → node\n",
    "        \n",
    "        if nodes:\n",
    "            for node in nodes:\n",
    "                self.add_node(node)\n",
    "    \n",
    "    def _hash(self, key):\n",
    "        \"\"\"Hash key to integer\"\"\"\n",
    "        return int(hashlib.md5(key.encode('utf-8')).hexdigest(), 16)\n",
    "    \n",
    "    def add_node(self, node):\n",
    "        \"\"\"Add node to ring\"\"\"\n",
    "        for i in range(self.replicas):\n",
    "            # Create virtual node: \"node:0\", \"node:1\", etc.\n",
    "            virtual_node = f\"{node}:{i}\"\n",
    "            hash_value = self._hash(virtual_node)\n",
    "            self.nodes[hash_value] = node\n",
    "            self.ring.append(hash_value)\n",
    "        \n",
    "        self.ring.sort()  # Keep ring sorted\n",
    "    \n",
    "    def remove_node(self, node):\n",
    "        \"\"\"Remove node from ring\"\"\"\n",
    "        for i in range(self.replicas):\n",
    "            virtual_node = f\"{node}:{i}\"\n",
    "            hash_value = self._hash(virtual_node)\n",
    "            if hash_value in self.nodes:\n",
    "                del self.nodes[hash_value]\n",
    "                self.ring.remove(hash_value)\n",
    "    \n",
    "    def get_node(self, key):\n",
    "        \"\"\"Get node for key\"\"\"\n",
    "        if not self.ring:\n",
    "            return None\n",
    "        \n",
    "        hash_value = self._hash(key)\n",
    "        \n",
    "        # Find first node clockwise from hash_value\n",
    "        index = bisect_left(self.ring, hash_value)\n",
    "        if index == len(self.ring):\n",
    "            # Wrap around to first node\n",
    "            index = 0\n",
    "        \n",
    "        hash_value = self.ring[index]\n",
    "        return self.nodes[hash_value]\n",
    "\n",
    "# Usage\n",
    "cache_ring = ConsistentHash()\n",
    "\n",
    "# Add nodes\n",
    "cache_ring.add_node('cache-node-1')\n",
    "cache_ring.add_node('cache-node-2')\n",
    "cache_ring.add_node('cache-node-3')\n",
    "\n",
    "# Get node for key\n",
    "node = cache_ring.get_node('user:123')\n",
    "print(f\"user:123 should be cached on {node}\")\n",
    "\n",
    "node = cache_ring.get_node('user:456')\n",
    "print(f\"user:456 should be cached on {node}\")\n",
    "\n",
    "# Add new node (minimal key remapping)\n",
    "cache_ring.add_node('cache-node-4')\n",
    "\n",
    "# Most keys still on same nodes\n",
    "node = cache_ring.get_node('user:123')\n",
    "print(f\"user:123 still on {node}\")\n",
    "\n",
    "# Remove node (minimal key remapping)\n",
    "cache_ring.remove_node('cache-node-2')\n",
    "\n",
    "# Most keys still on same nodes (except those on cache-node-2)\n",
    "node = cache_ring.get_node('user:123')\n",
    "print(f\"user:123 now on {node}\")\n",
    "```\n",
    "\n",
    "**Benefits of Consistent Hashing**:\n",
    "- **Minimal remapping**: Adding/removing nodes only affects nearby keys\n",
    "- **Balanced distribution**: Virtual nodes ensure even distribution\n",
    "- **Scalability**: Easy to add/remove nodes dynamically\n",
    "- **Fault tolerance**: If node fails, its keys are redistributed to neighbors\n",
    "\n",
    "---\n",
    "\n",
    "### **Client-Side vs. Server-Side Sharding**\n",
    "\n",
    "**Client-Side Sharding**: Application determines which cache node to use.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "class ClientSideShardedCache:\n",
    "    def __init__(self, nodes):\n",
    "        \"\"\"Initialize with list of cache nodes\n",
    "        \n",
    "        Args:\n",
    "            nodes: List of (host, port) tuples\n",
    "        \"\"\"\n",
    "        self.nodes = nodes\n",
    "        self.ring = ConsistentHash([f\"{host}:{port}\" for host, port in nodes])\n",
    "        self.connections = {f\"{host}:{port}\": redis.Redis(host=host, port=port)\n",
    "                           for host, port in nodes}\n",
    "    \n",
    "    def _get_connection(self, key):\n",
    "        \"\"\"Get Redis connection for key\"\"\"\n",
    "        node = self.ring.get_node(key)\n",
    "        return self.connections[node]\n",
    "    \n",
    "    def set(self, key, value, ttl=3600):\n",
    "        \"\"\"Set key-value pair (routes to correct node)\"\"\"\n",
    "        conn = self._get_connection(key)\n",
    "        conn.setex(key, ttl, value)\n",
    "    \n",
    "    def get(self, key):\n",
    "        \"\"\"Get value for key (routes to correct node)\"\"\"\n",
    "        conn = self._get_connection(key)\n",
    "        return conn.get(key)\n",
    "\n",
    "# Usage\n",
    "cache = ClientSideShardedCache([\n",
    "    ('cache-node-1', 6379),\n",
    "    ('cache-node-2', 6379),\n",
    "    ('cache-node-3', 6379)\n",
    "])\n",
    "\n",
    "# These operations are routed to correct nodes automatically\n",
    "cache.set('user:123', '{\"name\": \"Alice\"}')\n",
    "user = cache.get('user:123')\n",
    "```\n",
    "\n",
    "**Server-Side Sharding**: Cache infrastructure determines node placement (Redis Cluster, Memcached).\n",
    "\n",
    "**Comparison**:\n",
    "```\n",
    "┌────────────────────────┬─────────────────────┬──────────────────────┐\n",
    "│ Feature                │ Client-Side         │ Server-Side          │\n",
    "├────────────────────────┼─────────────────────┼──────────────────────┤\n",
    "│ Complexity             │ Application         │ Cache infrastructure │\n",
    "│                        │ manages sharding    │ manages sharding     │\n",
    "├────────────────────────┼─────────────────────┼──────────────────────┤\n",
    "│ Flexibility            │ High (custom        │ Low (fixed           │\n",
    "│                        │ sharding logic)     │ sharding logic)      │\n",
    "├────────────────────────┼─────────────────────┼──────────────────────┤\n",
    "│ Transparency           │ Low (app knows      │ High (app sees       │\n",
    "│                        │ about nodes)        │ single cache)        │\n",
    "├────────────────────────┼─────────────────────┼──────────────────────┤\n",
    "│ Failover               │ Manual (app must    │ Automatic (cache     │\n",
    "│                        │ handle)             │ handles)             │\n",
    "├────────────────────────┼─────────────────────┼──────────────────────┤\n",
    "│ Multi-key operations   │ Difficult (keys     │ Possible if keys     │\n",
    "│                        │ on different nodes) │ on same node         │\n",
    "├────────────────────────┼─────────────────────┼──────────────────────┤\n",
    "│ Example                │ Custom client       │ Redis Cluster,       │\n",
    "│                        │ sharding            │ Memcached            │\n",
    "└────────────────────────┴─────────────────────┴──────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.5 CDN Architecture**\n",
    "\n",
    "**CDN (Content Delivery Network)**: Geographically distributed network of servers that deliver content to users based on their geographic location, reducing latency and improving performance.\n",
    "\n",
    "### **What is a CDN?**\n",
    "\n",
    "**Concept**: Store copies of content on edge servers closer to users. When user requests content, it's served from the nearest edge server, not the origin server.\n",
    "\n",
    "**How It Works**:\n",
    "```\n",
    "User in Tokyo requests video\n",
    "    │\n",
    "    ▼\n",
    "DNS resolves to Tokyo CDN edge (not origin in US)\n",
    "    │\n",
    "    ▼\n",
    "CDN edge checks if content cached\n",
    "    ├─→ Cache hit: Serve from Tokyo edge (10ms)\n",
    "    └─→ Cache miss: Fetch from origin, cache, serve (200ms)\n",
    "\n",
    "Next request from Tokyo user: Served from Tokyo edge (10ms)\n",
    "User in New York requests same video\n",
    "    │\n",
    "    ▼\n",
    "DNS resolves to New York CDN edge\n",
    "    │\n",
    "    ▼\n",
    "CDN edge checks if content cached\n",
    "    ├─→ Cache hit: Serve from New York edge (20ms)\n",
    "    └─→ Cache miss: Fetch from origin, cache, serve (150ms)\n",
    "```\n",
    "\n",
    "**Without CDN**:\n",
    "```\n",
    "User in Tokyo → Origin server in US (200ms)\n",
    "User in London → Origin server in US (100ms)\n",
    "User in New York → Origin server in US (30ms)\n",
    "Average latency: 110ms\n",
    "```\n",
    "\n",
    "**With CDN**:\n",
    "```\n",
    "User in Tokyo → Tokyo CDN edge (10ms)\n",
    "User in London → London CDN edge (15ms)\n",
    "User in New York → New York CDN edge (20ms)\n",
    "Average latency: 15ms (7x improvement!)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **CDN Caching Strategies**\n",
    "\n",
    "**1. Static Content Caching**\n",
    "\n",
    "**Static Content**: Content that doesn't change frequently (images, CSS, JavaScript, videos).\n",
    "\n",
    "**Implementation**:\n",
    "```html\n",
    "<!-- HTML file -->\n",
    "<!DOCTYPE html>\n",
    "<html>\n",
    "<head>\n",
    "    <title>My Website</title>\n",
    "    \n",
    "    <!-- CSS - cached for 1 year -->\n",
    "    <link rel=\"stylesheet\" href=\"https://cdn.example.com/styles.css\">\n",
    "    \n",
    "    <!-- JavaScript - cached for 1 year with cache busting -->\n",
    "    <script src=\"https://cdn.example.com/app.v1.2.3.js\"></script>\n",
    "</head>\n",
    "<body>\n",
    "    <!-- Images - cached for 1 year -->\n",
    "    <img src=\"https://cdn.example.com/logo.png\" alt=\"Logo\">\n",
    "    \n",
    "    <!-- Content -->\n",
    "    <h1>Hello, World!</h1>\n",
    "</body>\n",
    "</html>\n",
    "```\n",
    "\n",
    "**HTTP Headers for Static Content**:\n",
    "```http\n",
    "HTTP/1.1 200 OK\n",
    "Content-Type: text/css\n",
    "Cache-Control: public, max-age=31536000, immutable\n",
    "ETag: \"abc123\"\n",
    "Last-Modified: Wed, 01 Jan 2024 00:00:00 GMT\n",
    "\n",
    "Explanation:\n",
    "- Cache-Control: public (can be cached by CDNs), max-age=31536000 (1 year)\n",
    "- immutable (content never changes, browsers won't revalidate)\n",
    "- ETag and Last-Modified: For revalidation (if needed)\n",
    "```\n",
    "\n",
    "**2. Dynamic Content Caching**\n",
    "\n",
    "**Dynamic Content**: Content that changes frequently (API responses, personalized content).\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from flask import Flask, jsonify, request\n",
    "import redis\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "app = Flask(__name__)\n",
    "redis_client = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "@app.route('/api/user/<int:user_id>')\n",
    "def get_user(user_id):\n",
    "    # Generate cache key based on request parameters\n",
    "    cache_key = f\"user:{user_id}\"\n",
    "    \n",
    "    # Check cache\n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        # Return cached response with CDN-friendly headers\n",
    "        response = jsonify(json.loads(cached_data))\n",
    "        response.headers['Cache-Control'] = 'public, max-age=60, s-maxage=300'\n",
    "        response.headers['Vary'] = 'Accept-Encoding'\n",
    "        return response\n",
    "    \n",
    "    # Cache miss - generate response\n",
    "    user_data = {\n",
    "        'id': user_id,\n",
    "        'name': 'Alice Johnson',\n",
    "        'email': 'alice@example.com'\n",
    "    }\n",
    "    \n",
    "    # Cache for 60 seconds (browser), 300 seconds (CDN)\n",
    "    redis_client.setex(cache_key, 300, json.dumps(user_data))\n",
    "    \n",
    "    # Return response with CDN-friendly headers\n",
    "    response = jsonify(user_data)\n",
    "    response.headers['Cache-Control'] = 'public, max-age=60, s-maxage=300'\n",
    "    response.headers['Vary'] = 'Accept-Encoding'\n",
    "    return response\n",
    "\n",
    "# Explanation of headers:\n",
    "# - Cache-Control: public (can be cached by CDNs)\n",
    "# - max-age=60 (browser cache for 60 seconds)\n",
    "# - s-maxage=300 (CDN cache for 300 seconds)\n",
    "# - Vary: Accept-Encoding (cache based on Accept-Encoding header)\n",
    "```\n",
    "\n",
    "**3. Cache Busting**\n",
    "\n",
    "**Problem**: How to invalidate cached content when it changes?\n",
    "\n",
    "**Solution 1: URL Versioning**\n",
    "```html\n",
    "<!-- Version in filename -->\n",
    "<script src=\"https://cdn.example.com/app.v1.2.3.js\"></script>\n",
    "\n",
    "<!-- When updating, change version -->\n",
    "<script src=\"https://cdn.example.com/app.v1.2.4.js\"></script>\n",
    "<!-- New URL = new cached version (CDN treats as different file) -->\n",
    "```\n",
    "\n",
    "**Solution 2: Query Parameter Versioning**\n",
    "```html\n",
    "<!-- Version in query parameter -->\n",
    "<script src=\"https://cdn.example.com/app.js?v=1.2.3\"></script>\n",
    "\n",
    "<!-- When updating, change version -->\n",
    "<script src=\"https://cdn.example.com/app.js?v=1.2.4\"></script>\n",
    "<!-- New URL = new cached version -->\n",
    "```\n",
    "\n",
    "**Solution 3: Content Hash (Automatic Cache Busting)**\n",
    "```html\n",
    "<!-- Hash based on content -->\n",
    "<link rel=\"stylesheet\" href=\"https://cdn.example.com/styles.abc123def456.css\">\n",
    "\n",
    "<!-- When content changes, hash changes -->\n",
    "<link rel=\"stylesheet\" href=\"https://cdn.example.com/styles.xyz789ghi012.css\">\n",
    "<!-- Different hash = different URL = new cache entry -->\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **CDN Cache Invalidation**\n",
    "\n",
    "**Problem**: How to remove outdated content from CDN edge servers?\n",
    "\n",
    "**1. Time-Based Expiration**\n",
    "\n",
    "**Concept**: Content automatically expires after configured TTL.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# Set cache headers with expiration\n",
    "response.headers['Cache-Control'] = 'public, max-age=3600'  # Expire after 1 hour\n",
    "\n",
    "# Content automatically removed from CDN cache after 1 hour\n",
    "```\n",
    "\n",
    "**2. Manual Invalidation**\n",
    "\n",
    "**Concept**: Explicitly invalidate cached content.\n",
    "\n",
    "**Implementation** (AWS CloudFront):\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "# Create CloudFront client\n",
    "cloudfront = boto3.client('cloudfront')\n",
    "\n",
    "# Invalidate specific paths\n",
    "response = cloudfront.create_invalidation(\n",
    "    DistributionId='E1234567890AB',  # Your CloudFront distribution ID\n",
    "    InvalidationBatch={\n",
    "        'CallerReference': 'invalidate-styles-2024-01-15',  # Unique ID\n",
    "        'Paths': {\n",
    "            'Quantity': 2,\n",
    "            'Items': [\n",
    "                '/styles.css',  # Specific file\n",
    "                '/images/*'     # Wildcard pattern\n",
    "            ]\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "# Get invalidation ID\n",
    "invalidation_id = response['Invalidation']['Id']\n",
    "print(f\"Invalidation created: {invalidation_id}\")\n",
    "\n",
    "# Check invalidation status\n",
    "response = cloudfront.get_invalidation(\n",
    "    DistributionId='E1234567890AB',\n",
    "    Id=invalidation_id\n",
    ")\n",
    "\n",
    "status = response['Invalidation']['Status']\n",
    "print(f\"Invalidation status: {status}\")  # InProgress or Completed\n",
    "```\n",
    "\n",
    "**3. Tag-Based Invalidation**\n",
    "\n",
    "**Concept**: Tag content, invalidate by tag.\n",
    "\n",
    "**Implementation** (Cloudflare):\n",
    "```python\n",
    "# Set cache tags on response\n",
    "response.headers['Cache-Tag'] = 'user-profile,user:123'\n",
    "\n",
    "# Later, invalidate all content with tag 'user:123'\n",
    "# (via Cloudflare API or dashboard)\n",
    "\n",
    "# When user data changes:\n",
    "# 1. Update user data in database\n",
    "# 2. Invalidate all cached content with tag 'user:123'\n",
    "# 3. Next request fetches fresh data and re-caches\n",
    "```\n",
    "\n",
    "**4. Purge All**\n",
    "\n",
    "**Concept**: Invalidate all cached content (nuclear option).\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# Invalidate all content in CDN (use carefully!)\n",
    "response = cloudfront.create_invalidation(\n",
    "    DistributionId='E1234567890AB',\n",
    "    InvalidationBatch={\n",
    "        'CallerReference': 'purge-all-2024-01-15',\n",
    "        'Paths': {\n",
    "            'Quantity': 1,\n",
    "            'Items': ['/*']  # Wildcard matches all paths\n",
    "        }\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "**Warning**: Purging all content removes all cached items, causing massive load on origin server. Use sparingly.\n",
    "\n",
    "---\n",
    "\n",
    "### **Popular CDN Providers**\n",
    "\n",
    "**1. Cloudflare**\n",
    "\n",
    "**Features**:\n",
    "- Free tier available\n",
    "- Global network (200+ locations)\n",
    "- DDoS protection\n",
    "- DNS services\n",
    "- Edge functions (Cloudflare Workers)\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# Cloudflare CDN via API\n",
    "import requests\n",
    "\n",
    "# Purge cache by URL\n",
    "response = requests.post(\n",
    "    'https://api.cloudflare.com/client/v4/zones/ZONE_ID/purge_cache',\n",
    "    headers={\n",
    "        'Authorization': 'Bearer API_TOKEN',\n",
    "        'Content-Type': 'application/json'\n",
    "    },\n",
    "    json={\n",
    "        'files': [\n",
    "            'https://example.com/styles.css',\n",
    "            'https://example.com/app.js'\n",
    "        ]\n",
    "    }\n",
    ")\n",
    "\n",
    "print(response.json())\n",
    "```\n",
    "\n",
    "**2. AWS CloudFront**\n",
    "\n",
    "**Features**:\n",
    "- Integrated with AWS (S3, EC2, Lambda@Edge)\n",
    "- Global network (400+ locations)\n",
    "- Custom SSL certificates\n",
    "- Lambda@Edge (run code at edge)\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# CloudFront signed URLs (for private content)\n",
    "from botocore.signers import CloudFrontSigner\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# CloudFront key pair ID and private key\n",
    "key_pair_id = 'APKAEXAMPLEKEY'\n",
    "private_key = open('private_key.pem', 'rb').read()\n",
    "\n",
    "# Create signer\n",
    "signer = CloudFrontSigner(key_pair_id, lambda **kwargs: private_key)\n",
    "\n",
    "# Generate signed URL\n",
    "url = 'https://d1234567890.cloudfront.net/private/video.mp4'\n",
    "expire_time = datetime.utcnow() + timedelta(hours=1)\n",
    "signed_url = signer.generate_presigned_url(url, date_less_than=expire_time)\n",
    "\n",
    "print(f\"Signed URL (expires in 1 hour): {signed_url}\")\n",
    "```\n",
    "\n",
    "**3. Fastly**\n",
    "\n",
    "**Features**:\n",
    "- High-performance edge cloud\n",
    "- Edge dictionary (key-value store at edge)\n",
    "- VCL (Varnish Configuration Language) for advanced caching\n",
    "- Real-time logging\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "# Fastly API\n",
    "import requests\n",
    "\n",
    "# Purge specific URL\n",
    "response = requests.request(\n",
    "    'PURGE',\n",
    "    'https://example.com/api/user/123',\n",
    "    headers={\n",
    "        'Fastly-Key': 'API_KEY',\n",
    "        'Accept': 'application/json'\n",
    "    }\n",
    ")\n",
    "\n",
    "print(f\"Status: {response.status_code}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Origin Shield: Protecting Your Origin Server**\n",
    "\n",
    "**Problem**: CDN edge servers requesting content from origin simultaneously can overwhelm it.\n",
    "\n",
    "**Solution**: Origin Shield—regional cache that protects origin server from thundering herd.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "User Request\n",
    "    │\n",
    "    ▼\n",
    "CDN Edge (Tokyo)\n",
    "    │\n",
    "    ▼    Cache Miss\n",
    "Origin Shield (Asia Pacific)\n",
    "    │\n",
    "    ▼    Cache Miss\n",
    "Origin Server (US)\n",
    "\n",
    "Benefits:\n",
    "- CDN edges don't all hit origin simultaneously\n",
    "- Origin Shield caches content regionally\n",
    "- Origin sees fewer requests (reduced load)\n",
    "```\n",
    "\n",
    "**Implementation** (AWS CloudFront Origin Shield):\n",
    "```python\n",
    "# Create distribution with Origin Shield\n",
    "response = cloudfront.create_distribution(\n",
    "    DistributionConfig={\n",
    "        'CallerReference': 'distribution-with-origin-shield',\n",
    "        'Origins': {\n",
    "            'Quantity': 1,\n",
    "            'Items': [{\n",
    "                'Id': 'my-origin',\n",
    "                'DomainName': 'origin.example.com',\n",
    "                'OriginShield': {\n",
    "                    'Enabled': True,\n",
    "                    'OriginShieldRegion': 'us-east-1'  # Origin Shield region\n",
    "                }\n",
    "            }]\n",
    "        },\n",
    "        # ... other configuration\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.6 Cache Consistency**\n",
    "\n",
    "Cache consistency ensures cached data matches the source of truth. Inconsistencies arise when data changes but cache isn't updated.\n",
    "\n",
    "### **Cache Invalidation Strategies**\n",
    "\n",
    "**Strategy 1: Time-Based Expiration**\n",
    "\n",
    "**Concept**: Cache entries expire automatically after TTL.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "def get_user(user_id):\n",
    "    cache_key = f'user:{user_id}'\n",
    "    \n",
    "    # Check cache\n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    # Cache miss - load from database\n",
    "    user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "    \n",
    "    # Cache with 1 hour TTL\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "    \n",
    "    return user_data\n",
    "\n",
    "def update_user(user_id, user_data):\n",
    "    # Update database\n",
    "    database.update(user_id, user_data)\n",
    "    \n",
    "    # No cache invalidation - let it expire naturally\n",
    "    # Next cache miss (within 1 hour) loads fresh data\n",
    "```\n",
    "\n",
    "**Pros**: Simple, no manual invalidation\n",
    "**Cons**: Stale data until expiration, unpredictable inconsistency window\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 2: Write-Through Invalidation**\n",
    "\n",
    "**Concept**: Update cache when data changes (write-through).\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "def update_user(user_id, user_data):\n",
    "    # Update database\n",
    "    database.update(user_id, user_data)\n",
    "    \n",
    "    # Update cache synchronously\n",
    "    cache_key = f'user:{user_id}'\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "    \n",
    "    # Cache always up-to-date\n",
    "```\n",
    "\n",
    "**Pros**: Strong consistency, cache always fresh\n",
    "**Cons**: Slower writes (must update cache), complex invalidation logic\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 3: Write-Back (Delayed) Invalidation**\n",
    "\n",
    "**Concept**: Invalidate cache asynchronously after data changes.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import queue\n",
    "import threading\n",
    "\n",
    "class WriteBackCache:\n",
    "    def __init__(self):\n",
    "        self.invalidate_queue = queue.Queue()\n",
    "        self.worker_thread = threading.Thread(target=self._invalidate_worker, daemon=True)\n",
    "        self.worker_thread.start()\n",
    "    \n",
    "    def _invalidate_worker(self):\n",
    "        \"\"\"Background thread to process invalidations\"\"\"\n",
    "        while True:\n",
    "            cache_key = self.invalidate_queue.get()\n",
    "            redis_client.delete(cache_key)\n",
    "            print(f\"Invalidated {cache_key}\")\n",
    "            self.invalidate_queue.task_done()\n",
    "    \n",
    "    def update_user(self, user_id, user_data):\n",
    "        # Update database (synchronous)\n",
    "        database.update(user_id, user_data)\n",
    "        \n",
    "        # Queue cache invalidation (asynchronous)\n",
    "        cache_key = f'user:{user_id}'\n",
    "        self.invalidate_queue.put(cache_key)\n",
    "        \n",
    "        # Return immediately (cache invalidated in background)\n",
    "\n",
    "# Usage\n",
    "cache = WriteBackCache()\n",
    "\n",
    "# Update user (fast - doesn't wait for cache invalidation)\n",
    "cache.update_user(123, {'name': 'Alice Updated'})\n",
    "\n",
    "# User might see stale data for a few milliseconds (until invalidation completes)\n",
    "```\n",
    "\n",
    "**Pros**: Fast writes, reduced contention\n",
    "**Cons**: Brief inconsistency window, complexity\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 4: Cache Tagging**\n",
    "\n",
    "**Concept**: Tag cache entries, invalidate by tag.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "def get_user(user_id):\n",
    "    cache_key = f'user:{user_id}'\n",
    "    \n",
    "    # Check cache\n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    # Cache miss - load from database\n",
    "    user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "    \n",
    "    # Cache with tags\n",
    "    redis_client.hset('cache_tags', cache_key, 'user')  # Tag this entry\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "    \n",
    "    return user_data\n",
    "\n",
    "def invalidate_all_users():\n",
    "    # Invalidate all entries tagged with 'user'\n",
    "    cache_keys = redis_client.hgetall('cache_tags')\n",
    "    for key, tag in cache_keys.items():\n",
    "        if tag == b'user':\n",
    "            redis_client.delete(key.decode())\n",
    "            redis_client.hdel('cache_tags', key.decode())\n",
    "\n",
    "# When user schema changes (affects all users)\n",
    "invalidate_all_users()  # Invalidate all user caches\n",
    "```\n",
    "\n",
    "**Pros**: Batch invalidation, flexible\n",
    "**Cons**: Additional metadata storage, complexity\n",
    "\n",
    "---\n",
    "\n",
    "### **Thundering Herd Problem**\n",
    "\n",
    "**Problem**: Many simultaneous cache misses cause overwhelming load on backend.\n",
    "\n",
    "**Scenario**:\n",
    "```\n",
    "Time 0: Popular item (user:123) expires from cache\n",
    "Time 1: 1000 users request user:123 simultaneously\n",
    "Time 2: All 1000 requests check cache → miss\n",
    "Time 3: All 1000 requests query database simultaneously\n",
    "Time 4: Database overwhelmed (crashes)\n",
    "\n",
    "Result: Database failure due to thundering herd\n",
    "```\n",
    "\n",
    "**Solution 1: Cache Locking**\n",
    "\n",
    "**Concept**: Only one request populates cache; others wait.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "def get_user_with_lock(user_id):\n",
    "    cache_key = f'user:{user_id}'\n",
    "    lock_key = f'lock:{cache_key}'\n",
    "    \n",
    "    # Check cache\n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    # Try to acquire lock\n",
    "    lock_acquired = redis_client.set(lock_key, '1', nx=True, ex=10)  # 10 second lock\n",
    "    if lock_acquired:\n",
    "        try:\n",
    "            # We have the lock - load from database\n",
    "            user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "            \n",
    "            # Populate cache\n",
    "            redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "            \n",
    "            return user_data\n",
    "        finally:\n",
    "            # Release lock\n",
    "            redis_client.delete(lock_key)\n",
    "    else:\n",
    "        # Someone else has the lock - wait and retry\n",
    "        time.sleep(0.1)  # Wait 100ms\n",
    "        \n",
    "        # Try cache again (might be populated by now)\n",
    "        return get_user_with_lock(user_id)  # Retry\n",
    "\n",
    "# When user:123 expires:\n",
    "# Request 1: Acquires lock, loads from database\n",
    "# Requests 2-1000: Wait for lock, then get from cache\n",
    "# Result: Only 1 database query (not 1000)\n",
    "```\n",
    "\n",
    "**Solution 2: Probabilistic Early Expiration**\n",
    "\n",
    "**Concept**: Some requests refresh cache early to prevent mass expiration.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import random\n",
    "\n",
    "def get_user_with_early_expiration(user_id):\n",
    "    cache_key = f'user:{user_id}'\n",
    "    \n",
    "    # Check cache\n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        # Check TTL\n",
    "        ttl = redis_client.ttl(cache_key)\n",
    "        \n",
    "        # 10% chance to refresh early if TTL < 60 seconds\n",
    "        if ttl < 60 and random.random() < 0.1:\n",
    "            print(f\"Early refresh for {cache_key}\")\n",
    "            # Refresh cache asynchronously\n",
    "            refresh_user_async(user_id)\n",
    "        \n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    # Cache miss - load from database\n",
    "    user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "    return user_data\n",
    "\n",
    "def refresh_user_async(user_id):\n",
    "    \"\"\"Refresh user cache asynchronously\"\"\"\n",
    "    user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "    redis_client.setex(f'user:{user_id}', 3600, json.dumps(user_data))\n",
    "\n",
    "# Result: Cache entries refreshed before mass expiration\n",
    "# Fewer simultaneous cache misses\n",
    "```\n",
    "\n",
    "**Solution 3: Request Coalescing**\n",
    "\n",
    "**Concept**: Combine multiple requests for same data into single backend request.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "class RequestCoalescer:\n",
    "    def __init__(self):\n",
    "        self.pending_requests = {}\n",
    "        self.executor = ThreadPoolExecutor(max_workers=10)\n",
    "    \n",
    "    def get_user(self, user_id):\n",
    "        cache_key = f'user:{user_id}'\n",
    "        \n",
    "        # Check cache\n",
    "        cached_data = redis_client.get(cache_key)\n",
    "        if cached_data:\n",
    "            return json.loads(cached_data)\n",
    "        \n",
    "        # Check if request is already pending\n",
    "        if cache_key in self.pending_requests:\n",
    "            print(f\"Coalescing request for {cache_key}\")\n",
    "            # Wait for existing request to complete\n",
    "            return self.pending_requests[cache_key].result()\n",
    "        \n",
    "        # Create new request\n",
    "        future = self.executor.submit(self._load_user, user_id)\n",
    "        self.pending_requests[cache_key] = future\n",
    "        \n",
    "        try:\n",
    "            return future.result()\n",
    "        finally:\n",
    "            # Clean up\n",
    "            del self.pending_requests[cache_key]\n",
    "    \n",
    "    def _load_user(self, user_id):\n",
    "        \"\"\"Load user from database\"\"\"\n",
    "        user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "        cache_key = f'user:{user_id}'\n",
    "        redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "        return user_data\n",
    "\n",
    "# Usage\n",
    "coalescer = RequestCoalescer()\n",
    "\n",
    "# When 1000 users request user:123 simultaneously:\n",
    "# Request 1: Loads from database\n",
    "# Requests 2-1000: Wait for request 1's result\n",
    "# Result: Only 1 database query\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cache Stampede Problem**\n",
    "\n",
    "**Problem**: Similar to thundering herd, but caused by malicious or erroneous mass invalidation.\n",
    "\n",
    "**Scenario**:\n",
    "```\n",
    "Normal operation:\n",
    "- 1000 requests/second for popular content\n",
    "- 95% cache hit rate\n",
    "- 50 requests/second to database (cache misses)\n",
    "\n",
    "Cache stampede:\n",
    "- Bug or invalidation causes all cache entries to be invalidated\n",
    "- 1000 requests/second become cache misses\n",
    "- Database overwhelmed with 1000 requests/second\n",
    "- System crashes\n",
    "```\n",
    "\n",
    "**Solution: Rate Limiting Cache Misses**\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "def get_user_with_rate_limit(user_id):\n",
    "    cache_key = f'user:{user_id}'\n",
    "    miss_key = f'miss:{cache_key}'\n",
    "    \n",
    "    # Check cache\n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    # Check if too many recent cache misses\n",
    "    miss_count = redis_client.incr(miss_key)\n",
    "    if miss_count == 1:\n",
    "        redis_client.expire(miss_key, 60)  # Count misses in last 60 seconds\n",
    "    \n",
    "    # If more than 10 cache misses in last 60 seconds, block\n",
    "    if miss_count > 10:\n",
    "        print(f\"Rate limiting cache misses for {cache_key}\")\n",
    "        # Return stale data or error\n",
    "        return {'error': 'Too many requests, please retry'}\n",
    "    \n",
    "    # Load from database\n",
    "    user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "    return user_data\n",
    "\n",
    "# Result: Limits cache misses, prevents database overload\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cache Penetration Problem**\n",
    "\n",
    "**Problem**: Repeated requests for non-existent data bypass cache and hit database.\n",
    "\n",
    "**Scenario**:\n",
    "```\n",
    "Attacker requests:\n",
    "GET /api/user/999999 (doesn't exist)\n",
    "GET /api/user/999998 (doesn't exist)\n",
    "GET /api/user/999997 (doesn't exist)\n",
    "...\n",
    "\n",
    "Each request:\n",
    "1. Check cache → miss\n",
    "2. Query database → miss (user doesn't exist)\n",
    "3. Return 404\n",
    "\n",
    "Problem:\n",
    "- All requests bypass cache (nothing to cache for non-existent data)\n",
    "- Database overwhelmed with queries for non-existent data\n",
    "```\n",
    "\n",
    "**Solution 1: Cache Null Values**\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "def get_user_with_null_caching(user_id):\n",
    "    cache_key = f'user:{user_id}'\n",
    "    \n",
    "    # Check cache\n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        if cached_data == b'NULL':\n",
    "            # Cached null value - user doesn't exist\n",
    "            return None\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    # Query database\n",
    "    user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "    \n",
    "    if user_data is None:\n",
    "        # Cache null value (short TTL)\n",
    "        redis_client.setex(cache_key, 60, 'NULL')  # Cache for 1 minute\n",
    "        return None\n",
    "    \n",
    "    # Cache user data\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "    return user_data\n",
    "\n",
    "# Result: Non-existent users cached (shorter TTL)\n",
    "# Fewer database queries for non-existent data\n",
    "```\n",
    "\n",
    "**Solution 2: Bloom Filter**\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from pybloom_live import ScalableBloomFilter\n",
    "\n",
    "# Create bloom filter\n",
    "user_bloom_filter = ScalableBloomFilter(initial_capacity=1000000, error_rate=0.001)\n",
    "\n",
    "# Populate bloom filter with existing user IDs\n",
    "for user_id in get_all_user_ids():\n",
    "    user_bloom_filter.add(user_id)\n",
    "\n",
    "def get_user_with_bloom_filter(user_id):\n",
    "    # Check bloom filter\n",
    "    if user_id not in user_bloom_filter:\n",
    "        # Definitely not in database (bloom filter guarantees no false negatives)\n",
    "        return None\n",
    "    \n",
    "    # Might be in database (bloom filter has false positives)\n",
    "    # Query database to confirm\n",
    "    cache_key = f'user:{user_id}'\n",
    "    \n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    user_data = database.query(f'SELECT * FROM users WHERE id = {user_id}')\n",
    "    if user_data:\n",
    "        redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "        return user_data\n",
    "    \n",
    "    # User doesn't exist (false positive from bloom filter)\n",
    "    return None\n",
    "\n",
    "# Result:\n",
    "# Non-existent users filtered out by bloom filter\n",
    "- No database queries for guaranteed non-existent users\n",
    "- Minimal database queries for false positives (rare)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.7 Application-Level Caching**\n",
    "\n",
    "### **Browser Caching**\n",
    "\n",
    "**Concept**: Store resources in user's browser to reduce server requests.\n",
    "\n",
    "**HTTP Cache Headers**:\n",
    "```http\n",
    "# Strong caching (no revalidation)\n",
    "Cache-Control: public, max-age=31536000, immutable\n",
    "ETag: \"abc123\"\n",
    "\n",
    "# Revalidation (check if changed)\n",
    "Cache-Control: public, max-age=3600\n",
    "ETag: \"abc123\"\n",
    "Last-Modified: Wed, 01 Jan 2024 00:00:00 GMT\n",
    "\n",
    "# No caching\n",
    "Cache-Control: no-store, no-cache, must-revalidate\n",
    "Pragma: no-cache\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from flask import Flask, jsonify, request, make_response\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "@app.route('/api/user/<int:user_id>')\n",
    "def get_user(user_id):\n",
    "    user_data = database.get_user(user_id)\n",
    "    \n",
    "    # Generate ETag (hash of response)\n",
    "    import hashlib\n",
    "    etag = hashlib.md5(json.dumps(user_data).encode()).hexdigest()\n",
    "    \n",
    "    # Check if client has cached version\n",
    "    if request.headers.get('If-None-Match') == etag:\n",
    "        # Client has latest version - return 304 (Not Modified)\n",
    "        response = make_response('', 304)\n",
    "        response.headers['ETag'] = etag\n",
    "        return response\n",
    "    \n",
    "    # Return user data with cache headers\n",
    "    response = jsonify(user_data)\n",
    "    response.headers['Cache-Control'] = 'public, max-age=60'\n",
    "    response.headers['ETag'] = etag\n",
    "    return response\n",
    "\n",
    "# Flow:\n",
    "# 1. Client requests user data\n",
    "# 2. Server returns data with ETag and Cache-Control headers\n",
    "# 3. Client caches data locally\n",
    "# 4. Next request: Client sends If-None-Match header with ETag\n",
    "# 5. Server checks if data changed (by comparing ETags)\n",
    "# 6. If unchanged: Returns 304 (client uses cached data)\n",
    "# 7. If changed: Returns new data with new ETag\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Local Application Cache**\n",
    "\n",
    "**Concept**: Cache data in application process memory (fastest, but not shared between processes).\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from functools import lru_cache\n",
    "import time\n",
    "\n",
    "# Python LRU cache (in-memory)\n",
    "@lru_cache(maxsize=1000)\n",
    "def get_user_local(user_id):\n",
    "    \"\"\"Cache user data in local process memory\"\"\"\n",
    "    print(f\"Loading user {user_id} from database\")\n",
    "    return database.get_user(user_id)\n",
    "\n",
    "# Usage\n",
    "# First call: Loads from database\n",
    "user = get_user_local(123)\n",
    "\n",
    "# Second call: Returns from cache (no database query)\n",
    "user = get_user_local(123)\n",
    "\n",
    "# Clear cache (if needed)\n",
    "get_user_local.cache_clear()\n",
    "\n",
    "# Note: This cache is per-process (not shared across multiple application servers)\n",
    "# Use for data that doesn't change often and is accessed frequently\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Distributed Cache Hierarchy**\n",
    "\n",
    "**Concept**: Layered caching—try faster caches first, fall back to slower ones.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Request\n",
    "    │\n",
    "    ▼\n",
    "Browser Cache (Client)\n",
    "    │\n",
    "    ▼    Cache Miss\n",
    "CDN Cache (Edge)\n",
    "    │\n",
    "    ▼    Cache Miss\n",
    "Local Application Cache (Process Memory)\n",
    "    │\n",
    "    ▼    Cache Miss\n",
    "Distributed Cache (Redis Cluster)\n",
    "    │\n",
    "    ▼    Cache Miss\n",
    "Database (PostgreSQL)\n",
    "\n",
    "Each layer is slower but larger.\n",
    "Cache hit rate improves as data moves up hierarchy.\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "class HierarchicalCache:\n",
    "    def __init__(self, redis_client):\n",
    "        self.local_cache = {}  # In-memory cache\n",
    "        self.redis_client = redis_client\n",
    "    \n",
    "    def get(self, key):\n",
    "        # Level 1: Local cache\n",
    "        if key in self.local_cache:\n",
    "            print(f\"Local cache HIT for {key}\")\n",
    "            return self.local_cache[key]\n",
    "        \n",
    "        # Level 2: Distributed cache\n",
    "        cached_data = self.redis_client.get(key)\n",
    "        if cached_data:\n",
    "            print(f\"Distributed cache HIT for {key}\")\n",
    "            data = json.loads(cached_data)\n",
    "            # Populate local cache\n",
    "            self.local_cache[key] = data\n",
    "            return data\n",
    "        \n",
    "        # Level 3: Database\n",
    "        print(f\"Cache MISS for {key} - loading from database\")\n",
    "        data = self.load_from_database(key)\n",
    "        \n",
    "        # Populate distributed cache\n",
    "        self.redis_client.setex(key, 3600, json.dumps(data))\n",
    "        \n",
    "        # Populate local cache\n",
    "        self.local_cache[key] = data\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    def set(self, key, data, ttl=3600):\n",
    "        # Update all cache levels\n",
    "        self.local_cache[key] = data\n",
    "        self.redis_client.setex(key, ttl, json.dumps(data))\n",
    "    \n",
    "    def invalidate(self, key):\n",
    "        # Invalidate all cache levels\n",
    "        if key in self.local_cache:\n",
    "            del self.local_cache[key]\n",
    "        self.redis_client.delete(key)\n",
    "\n",
    "# Usage\n",
    "cache = HierarchicalCache(redis_client)\n",
    "\n",
    "# First request: Loads from database, populates all cache levels\n",
    "data = cache.get('user:123')\n",
    "\n",
    "# Subsequent requests: Returns from local cache (fastest)\n",
    "data = cache.get('user:123')\n",
    "\n",
    "# Invalidates all cache levels\n",
    "cache.invalidate('user:123')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.8 Caching Best Practices**\n",
    "\n",
    "### **Cache Key Design**\n",
    "\n",
    "**Principle**: Design cache keys that are descriptive, consistent, and support invalidation.\n",
    "\n",
    "**Good Cache Keys**:\n",
    "```python\n",
    "# Hierarchical keys (namespace organization)\n",
    "cache_key = f'user:{user_id}:profile'  # User profile\n",
    "cache_key = f'user:{user_id}:settings'  # User settings\n",
    "cache_key = f'product:{product_id}:details'  # Product details\n",
    "\n",
    "# Keys with version (for schema changes)\n",
    "cache_key = f'user:{user_id}:profile:v2'  # Version 2 of user profile\n",
    "\n",
    "# Keys with parameters (for query result caching)\n",
    "cache_key = f'users:page:{page}:limit:{limit}:sort:{sort}'\n",
    "\n",
    "# Keys with hash tags (for distributed caching)\n",
    "cache_key = f'{user_id}:profile'  # Ensures related keys on same node\n",
    "```\n",
    "\n",
    "**Bad Cache Keys**:\n",
    "```python\n",
    "# Too generic (hard to invalidate)\n",
    "cache_key = 'data'  # What data? When to invalidate?\n",
    "\n",
    "# Inconsistent naming\n",
    "cache_key = f'userProfile{user_id}'  # Mixes camelCase and snake_case\n",
    "\n",
    "# Too long (inefficient)\n",
    "cache_key = f'user:{user_id}:profile:including:all:details:and:metadata:which:is:very:long'\n",
    "```\n",
    "\n",
    "**Cache Key Versioning**:\n",
    "```python\n",
    "def get_cache_key(user_id, version=2):\n",
    "    \"\"\"Generate cache key with version\"\"\"\n",
    "    return f'user:{user_id}:profile:v{version}'\n",
    "\n",
    "# When schema changes, increment version\n",
    "# Old version cached data ignored (treated as different key)\n",
    "cache_key_v2 = get_cache_key(123, version=2)\n",
    "cache_key_v3 = get_cache_key(123, version=3)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Cache Warming**\n",
    "\n",
    "**Concept**: Pre-populate cache with expected data before users request it.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "def warm_user_cache():\n",
    "    \"\"\"Pre-populate cache with popular user data\"\"\"\n",
    "    # Get most frequently accessed user IDs\n",
    "    popular_user_ids = database.query(\"\"\"\n",
    "        SELECT user_id \n",
    "        FROM access_logs \n",
    "        WHERE access_time > NOW() - INTERVAL '1 day'\n",
    "        GROUP BY user_id \n",
    "        ORDER BY COUNT(*) DESC \n",
    "        LIMIT 1000\n",
    "    \"\"\")\n",
    "    \n",
    "    # Load and cache each user\n",
    "    for user_id in popular_user_ids:\n",
    "        user_data = database.get_user(user_id)\n",
    "        cache_key = f'user:{user_id}'\n",
    "        redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "        print(f\"Warmed cache for user {user_id}\")\n",
    "\n",
    "# Run cache warming during off-peak hours\n",
    "# (e.g., 3 AM when traffic is low)\n",
    "warm_user_cache()\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- Reduced cache misses during peak hours\n",
    "- Better user experience (faster response times)\n",
    "- Lower database load during peak traffic\n",
    "\n",
    "---\n",
    "\n",
    "### **Cache Monitoring**\n",
    "\n",
    "**Metrics to Monitor**:\n",
    "```python\n",
    "import time\n",
    "from functools import wraps\n",
    "\n",
    "def monitor_cache(func):\n",
    "    \"\"\"Decorator to monitor cache performance\"\"\"\n",
    "    @wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Call original function\n",
    "        result = func(*args, **kwargs)\n",
    "        \n",
    "        # Calculate duration\n",
    "        duration = time.time() - start_time\n",
    "        \n",
    "        # Log metrics\n",
    "        print(f\"Cache operation: {func.__name__}, Duration: {duration:.3f}s\")\n",
    "        \n",
    "        # Send to monitoring system\n",
    "        # monitoring_system.gauge('cache.operation.duration', duration)\n",
    "        # monitoring_system.increment('cache.operation.count')\n",
    "        \n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "@monitor_cache\n",
    "def get_user(user_id):\n",
    "    cache_key = f'user:{user_id}'\n",
    "    \n",
    "    cached_data = redis_client.get(cache_key)\n",
    "    if cached_data:\n",
    "        # Cache hit\n",
    "        monitoring_system.increment('cache.hits')\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    # Cache miss\n",
    "    monitoring_system.increment('cache.misses')\n",
    "    user_data = database.get_user(user_id)\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "    return user_data\n",
    "\n",
    "# Important metrics:\n",
    "# - Cache hit rate: hits / (hits + misses)\n",
    "# - Cache latency: Time to get data from cache\n",
    "# - Cache size: Memory usage of cache\n",
    "# - Eviction rate: How often items are evicted\n",
    "# - TTL distribution: How long items stay in cache\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Common Caching Pitfalls**\n",
    "\n",
    "**Pitfall 1: Caching Everything**\n",
    "\n",
    "**Problem**: Caching too much data wastes memory and increases complexity.\n",
    "\n",
    "**Solution**: Cache judiciously based on access patterns.\n",
    "```python\n",
    "# Bad: Cache all data regardless of access frequency\n",
    "def get_any_data(key):\n",
    "    cached_data = redis_client.get(key)\n",
    "    if cached_data:\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    data = load_from_source(key)\n",
    "    redis_client.setex(key, 3600, json.dumps(data))\n",
    "    return data\n",
    "\n",
    "# Good: Cache only frequently accessed data\n",
    "def get_frequently_accessed_data(key):\n",
    "    # Check if key is in \"frequently accessed\" set\n",
    "    if not redis_client.sismember('frequently_accessed', key):\n",
    "        # Not frequently accessed - don't cache\n",
    "        return load_from_source(key)\n",
    "    \n",
    "    # Frequently accessed - cache it\n",
    "    cached_data = redis_client.get(key)\n",
    "    if cached_data:\n",
    "        return json.loads(cached_data)\n",
    "    \n",
    "    data = load_from_source(key)\n",
    "    redis_client.setex(key, 3600, json.dumps(data))\n",
    "    return data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Pitfall 2: Incorrect TTL**\n",
    "\n",
    "**Problem**: TTL too short (cache ineffective) or too long (stale data).\n",
    "\n",
    "**Solution**: Choose TTL based on data freshness requirements.\n",
    "```python\n",
    "# Bad: Fixed TTL for all data\n",
    "def cache_data(key, data):\n",
    "    redis_client.setex(key, 3600, json.dumps(data))  # 1 hour for everything\n",
    "\n",
    "# Good: TTL based on data characteristics\n",
    "def cache_data(key, data, data_type):\n",
    "    ttl_by_type = {\n",
    "        'static': 86400,      # 24 hours (static assets)\n",
    "        'session': 3600,      # 1 hour (user sessions)\n",
    "        'dynamic': 60,        # 1 minute (dynamic data)\n",
    "        'realtime': 5         # 5 seconds (real-time data)\n",
    "    }\n",
    "    \n",
    "    ttl = ttl_by_type.get(data_type, 3600)\n",
    "    redis_client.setex(key, ttl, json.dumps(data))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Pitfall 3: Cache Inconsistency**\n",
    "\n",
    "**Problem**: Cache and database out of sync.\n",
    "\n",
    "**Solution**: Implement cache invalidation strategy.\n",
    "```python\n",
    "# Bad: No cache invalidation\n",
    "def update_user(user_id, user_data):\n",
    "    database.update(user_id, user_data)\n",
    "    # Cache not updated - serves stale data!\n",
    "\n",
    "# Good: Write-through cache invalidation\n",
    "def update_user(user_id, user_data):\n",
    "    database.update(user_id, user_data)\n",
    "    \n",
    "    # Invalidate cache\n",
    "    cache_key = f'user:{user_id}'\n",
    "    redis_client.delete(cache_key)\n",
    "    \n",
    "    # Optionally, repopulate cache with new data\n",
    "    redis_client.setex(cache_key, 3600, json.dumps(user_data))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.9 Real-World Caching Examples**\n",
    "\n",
    "### **Instagram's Caching Architecture**\n",
    "\n",
    "**Challenge**: Instagram serves billions of photos and videos per day.\n",
    "\n",
    "**Solution**: Multi-layer caching strategy.\n",
    "```\n",
    "1. CDN: Static assets (images, videos) cached globally\n",
    "2. Edge Cache: API responses cached at edge locations\n",
    "3. Application Cache: User profiles, feed data cached in Redis\n",
    "4. Database Cache: Query results cached in PostgreSQL buffer pool\n",
    "\n",
    "Result:\n",
    "- 95%+ cache hit rate for static assets\n",
    "- 80%+ cache hit rate for API responses\n",
    "- Reduced database load by 90%\n",
    "- Sub-second response times for most requests\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Twitter's Timeline Caching**\n",
    "\n",
    "**Challenge**: Generate personalized timelines for 300+ million users.\n",
    "\n",
    "**Solution**: Pre-compute and cache timelines.\n",
    "```python\n",
    "# User timeline generation\n",
    "def generate_timeline(user_id):\n",
    "    timeline_key = f'timeline:{user_id}'\n",
    "    \n",
    "    # Check cache\n",
    "    cached_timeline = redis_client.get(timeline_key)\n",
    "    if cached_timeline:\n",
    "        return json.loads(cached_timeline)\n",
    "    \n",
    "    # Generate timeline (expensive operation)\n",
    "    followed_users = get_followed_users(user_id)\n",
    "    tweets = []\n",
    "    for followed_user in followed_users:\n",
    "        user_tweets = get_recent_tweets(followed_user, limit=10)\n",
    "        tweets.extend(user_tweets)\n",
    "    \n",
    "    # Sort by timestamp (most recent first)\n",
    "    tweets.sort(key=lambda x: x['timestamp'], reverse=True)\n",
    "    \n",
    "    # Cache timeline (5 minute TTL)\n",
    "    redis_client.setex(timeline_key, 300, json.dumps(tweets))\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "# When a user tweets:\n",
    "def post_tweet(user_id, tweet_content):\n",
    "    # Save tweet\n",
    "    tweet_id = save_tweet(user_id, tweet_content)\n",
    "    \n",
    "    # Invalidate followers' timelines\n",
    "    followers = get_followers(user_id)\n",
    "    for follower_id in followers:\n",
    "        timeline_key = f'timeline:{follower_id}'\n",
    "        redis_client.delete(timeline_key)\n",
    "    \n",
    "    # Alternatively, update timelines directly (faster for followers)\n",
    "    # (but more complex)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Netflix's Content Delivery Caching**\n",
    "\n",
    "**Challenge**: Stream high-quality video to millions of users simultaneously.\n",
    "\n",
    "**Solution**: Hierarchical CDN caching.\n",
    "```\n",
    "1. Open Connect: Netflix's own CDN (deployed in ISP networks)\n",
    "2. Regional Caches: Cache popular content regionally\n",
    "3. Edge Caches: Cache content at the edge (close to users)\n",
    "4. Client Caching: Buffer content in user's device\n",
    "\n",
    "Optimizations:\n",
    "- Adaptive bitrate streaming (adjust quality based on bandwidth)\n",
    "- Pre-fetching (download next segment before user watches)\n",
    "- Predictive caching (predict what user will watch next)\n",
    "- P2P caching (share cached content between nearby users)\n",
    "\n",
    "Result:\n",
    "- 99.9%+ uptime\n",
    "- Sub-second start times\n",
    "- High-quality streaming (4K, HDR)\n",
    "- Reduced bandwidth costs (90%+ cached)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.10 Key Takeaways**\n",
    "\n",
    "1. **Caching is impactful**: Well-implemented caching can improve performance by 10-1000x while reducing costs.\n",
    "\n",
    "2. **Choose the right pattern**: Cache-aside for simplicity, read-through for reduced application complexity, write-through for consistency, write-behind for performance.\n",
    "\n",
    "3. **Eviction policies matter**: LRU for general use, LFU for stable patterns, TTL for time-sensitive data.\n",
    "\n",
    "4. **Distributed caching scales**: Use Redis Cluster or Memcached for large-scale deployments.\n",
    "\n",
    "5. **CDNs reduce latency**: Edge caching significantly improves user experience globally.\n",
    "\n",
    "6. **Cache consistency is complex**: Implement proper invalidation strategies (time-based, write-through, tagging).\n",
    "\n",
    "7. **Avoid caching pitfalls**: Don't cache everything, choose appropriate TTLs, implement invalidation, monitor cache performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored caching—a critical optimization for system design. We covered caching patterns (cache-aside, read-through, write-through, write-behind, refresh-ahead), eviction policies (LRU, LFU, TTL, random), distributed caching (Redis Cluster, Memcached), and CDN architecture.\n",
    "\n",
    "We understood cache consistency challenges and solutions (invalidation strategies, thundering herd, cache stampede, cache penetration). We explored application-level caching (browser, local, distributed cache hierarchy) and caching best practices (cache key design, warming, monitoring).\n",
    "\n",
    "Finally, we examined real-world caching examples from Instagram, Twitter, and Netflix, understanding how top companies implement caching at scale.\n",
    "\n",
    "**Coming up next**: In Chapter 5, we'll explore Message Queues & Event-Driven Architecture, covering synchronous vs. asynchronous communication, message queue patterns, Apache Kafka, RabbitMQ, event sourcing, and backpressure handling.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercises**:\n",
    "\n",
    "1. **Caching Pattern Selection**: For each scenario, which caching pattern would you use and why?\n",
    "   - A banking application where account balances must always be accurate\n",
    "   - A social media news feed where speed is more important than perfect consistency\n",
    "   - An analytics system processing large datasets (write-heavy)\n",
    "   - A real-time multiplayer game where low latency is critical\n",
    "\n",
    "2. **Cache Eviction Policy**: You're building a music streaming service with 10 million songs. Users frequently listen to popular songs (top 1000) but also explore new songs. Which eviction policy would you use? Why?\n",
    "\n",
    "3. **Distributed Cache Design**: You need to design a distributed cache for a global e-commerce platform with 100 million products. Requirements:\n",
    "   - Products accessed from all regions\n",
    "   - Some products are very popular (hot spots)\n",
    "   - System must remain available if one cache node fails\n",
    "   How would you design this? Which distributed caching solution would you use?\n",
    "\n",
    "4. **Cache Invalidation Strategy**: You're building a collaborative document editor (like Google Docs). Multiple users can edit the same document simultaneously. How would you handle cache invalidation? What strategy would you use to ensure all users see consistent document state?\n",
    "\n",
    "5. **CDN Caching Strategy**: You're launching a video streaming service. Requirements:\n",
    "   - Videos are large (1-10 GB each)\n",
    "   - Some videos are very popular (millions of views)\n",
    "   - Videos are updated occasionally (new versions released)\n",
    "   How would you design CDN caching for this service? What cache invalidation strategy would you use?\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
