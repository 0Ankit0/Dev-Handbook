{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8fc4d7",
   "metadata": {},
   "source": [
    "# **Chapter 5: Message Queues & Event-Driven Architecture**\n",
    "\n",
    "In modern distributed systems, synchronous communication (direct HTTP calls between services) creates tight coupling and reduces resilience. Message queues and event-driven architectures decouple services, improve scalability, and enable fault-tolerant systems. This chapter explores asynchronous communication patterns, message queue implementations, and event-driven architectural patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **5.1 Synchronous vs. Asynchronous Communication**\n",
    "\n",
    "Understanding the distinction between synchronous and asynchronous communication is fundamental to designing resilient distributed systems.\n",
    "\n",
    "### **Synchronous Communication**\n",
    "\n",
    "**Concept**: The caller waits for the callee to respond before proceeding. Like making a phone call—you speak, wait for the other person to respond, then continue the conversation.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Service A                  Service B                  Service C\n",
    "    │                          │                          │\n",
    "    │── HTTP Request ──────────>│                          │\n",
    "    │                          │── HTTP Request ──────────>│\n",
    "    │                          │                          │\n",
    "    │<── Response (200ms) ─────┤                          │\n",
    "    │                          │<── Response (100ms) ─────┤\n",
    "    │                          │                          │\n",
    "    │── Response to client ────┤                          │\n",
    "    │                          │                          │\n",
    "\n",
    "Total latency: 300ms\n",
    "Service A blocked for 300ms waiting for responses\n",
    "```\n",
    "\n",
    "**Code Example**:\n",
    "```python\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def process_order(order_id):\n",
    "    \"\"\"Synchronous order processing\"\"\"\n",
    "    print(f\"Processing order {order_id}\")\n",
    "    \n",
    "    # Step 1: Validate order (synchronous call to Order Service)\n",
    "    print(\"Validating order...\")\n",
    "    validation_response = requests.post(\n",
    "        'http://order-service/validate',\n",
    "        json={'order_id': order_id}\n",
    "    )\n",
    "    if not validation_response.json()['valid']:\n",
    "        return \"Order validation failed\"\n",
    "    \n",
    "    # Step 2: Reserve inventory (synchronous call to Inventory Service)\n",
    "    print(\"Reserving inventory...\")\n",
    "    inventory_response = requests.post(\n",
    "        'http://inventory-service/reserve',\n",
    "        json={'order_id': order_id, 'items': [...]},\n",
    "        timeout=5.0  # 5 second timeout\n",
    "    )\n",
    "    if not inventory_response.json()['reserved']:\n",
    "        return \"Inventory reservation failed\"\n",
    "    \n",
    "    # Step 3: Process payment (synchronous call to Payment Service)\n",
    "    print(\"Processing payment...\")\n",
    "    payment_response = requests.post(\n",
    "        'http://payment-service/process',\n",
    "        json={'order_id': order_id, 'amount': 99.99},\n",
    "        timeout=10.0  # 10 second timeout\n",
    "    )\n",
    "    if not payment_response.json()['success']:\n",
    "        return \"Payment processing failed\"\n",
    "    \n",
    "    # Step 4: Send confirmation email (synchronous call to Email Service)\n",
    "    print(\"Sending confirmation email...\")\n",
    "    email_response = requests.post(\n",
    "        'http://email-service/send',\n",
    "        json={'order_id': order_id, 'email': 'customer@example.com'},\n",
    "        timeout=3.0  # 3 second timeout\n",
    "    )\n",
    "    \n",
    "    return \"Order processed successfully\"\n",
    "\n",
    "# Problem: Total time = sum of all service calls\n",
    "# If any service is slow or down, entire operation fails\n",
    "# Service A is blocked waiting for all other services\n",
    "```\n",
    "\n",
    "**Disadvantages**:\n",
    "1. **Tight coupling**: Services depend on each other's availability\n",
    "2. **Blocking**: Calling service waits for response (can't handle other requests)\n",
    "3. **Cascading failures**: Failure in one service causes failure in calling service\n",
    "4. **Poor resilience**: System脆弱性 increases with more synchronous dependencies\n",
    "5. **Limited scalability**: Bottlenecks at slowest service\n",
    "\n",
    "**When to Use**:\n",
    "- When immediate response is required (user-facing operations)\n",
    "- When subsequent operations depend on the result of the call\n",
    "- When data consistency is critical and must be confirmed immediately\n",
    "\n",
    "---\n",
    "\n",
    "### **Asynchronous Communication**\n",
    "\n",
    "**Concept**: The caller sends a message and continues without waiting for the callee to respond. Like sending an email—you send it and continue with your day; the recipient reads it later and responds.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Service A         Message Queue         Service B         Service C\n",
    "    │                   │                   │                 │\n",
    "    │── Publish ───────>│                   │                 │\n",
    "    │   \"Order Created\" │                   │                 │\n",
    "    │                   │── Consume ───────>│                 │\n",
    "    │                   │                   │── Publish ────>│\n",
    "    │                   │                   │   \"Inventory    │\n",
    "    │                   │                   │    Reserved\"    │\n",
    "    │                   │<──────────────────┤                 │\n",
    "    │                   │                   │                 │\n",
    "    │<──────────────────┤                   │                 │\n",
    "    │   \"Order Processed\"                   │                 │\n",
    "    │                   │                   │                 │\n",
    "\n",
    "Service A publishes message and immediately continues\n",
    "Service B and C process independently\n",
    "No blocking, no waiting\n",
    "```\n",
    "\n",
    "**Code Example**:\n",
    "```python\n",
    "import pika  # RabbitMQ client\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Connection to message broker\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "# Declare queue\n",
    "channel.queue_declare(queue='orders')\n",
    "\n",
    "def publish_order_created(order_id):\n",
    "    \"\"\"Publish order created event (asynchronous)\"\"\"\n",
    "    message = {\n",
    "        'event_type': 'order_created',\n",
    "        'order_id': order_id,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='orders',\n",
    "        body=json.dumps(message),\n",
    "        properties=pika.BasicProperties(\n",
    "            delivery_mode=2,  # Make message persistent\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\" [x] Sent 'Order Created: {order_id}'\")\n",
    "    # Function returns immediately - no waiting!\n",
    "\n",
    "def process_order(order_id):\n",
    "    \"\"\"Process order (publishes events, doesn't wait for responses)\"\"\"\n",
    "    print(f\"Processing order {order_id}\")\n",
    "    \n",
    "    # Step 1: Validate order (synchronous to Order Service)\n",
    "    print(\"Validating order...\")\n",
    "    validation_response = requests.post(\n",
    "        'http://order-service/validate',\n",
    "        json={'order_id': order_id}\n",
    "    )\n",
    "    \n",
    "    if validation_response.json()['valid']:\n",
    "        # Publish \"Order Validated\" event (asynchronous)\n",
    "        publish_event('order_validated', {'order_id': order_id})\n",
    "    else:\n",
    "        # Publish \"Order Validation Failed\" event (asynchronous)\n",
    "        publish_event('order_validation_failed', {'order_id': order_id})\n",
    "        return \"Order validation failed\"\n",
    "    \n",
    "    # Step 2: Reserve inventory (publish event, don't wait)\n",
    "    print(\"Publishing inventory reservation request...\")\n",
    "    publish_event('inventory_reservation_requested', {\n",
    "        'order_id': order_id,\n",
    "        'items': [...]\n",
    "    })\n",
    "    # Don't wait for inventory service!\n",
    "    # Continue to next step\n",
    "    \n",
    "    # Step 3: Process payment (publish event, don't wait)\n",
    "    print(\"Publishing payment processing request...\")\n",
    "    publish_event('payment_processing_requested', {\n",
    "        'order_id': order_id,\n",
    "        'amount': 99.99\n",
    "    })\n",
    "    # Don't wait for payment service!\n",
    "    # Continue to next step\n",
    "    \n",
    "    # Step 4: Return immediately (order processing initiated)\n",
    "    return \"Order processing initiated\"\n",
    "\n",
    "def publish_event(event_type, data):\n",
    "    \"\"\"Publish event to message queue\"\"\"\n",
    "    message = {\n",
    "        'event_type': event_type,\n",
    "        'data': data,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='events',\n",
    "        body=json.dumps(message),\n",
    "        properties=pika.BasicProperties(\n",
    "            delivery_mode=2,  # Make message persistent\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Benefits:\n",
    "# 1. Service A publishes events and immediately returns\n",
    "# 2. Other services consume events independently\n",
    "# 3. No blocking, no waiting\n",
    "# 4. Failure in one service doesn't affect others\n",
    "# 5. Each service can scale independently\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "1. **Loose coupling**: Services don't depend on each other's availability\n",
    "2. **Non-blocking**: Calling service can handle other requests immediately\n",
    "3. **Fault tolerance**: Failure in one service doesn't affect others\n",
    "4. **Scalability**: Each service can scale independently based on load\n",
    "5. **Resilience**: System can tolerate temporary failures (messages queued)\n",
    "\n",
    "**Disadvantages**:\n",
    "1. **Complexity**: Harder to reason about system behavior (asynchronous)\n",
    "2. **Eventual consistency**: Data not immediately consistent across services\n",
    "3. **Error handling**: Need mechanisms for failed messages (retry, dead letter queues)\n",
    "4. **Monitoring**: Harder to track end-to-end request flow\n",
    "5. **Debugging**: Difficult to trace message flow through multiple services\n",
    "\n",
    "**When to Use**:\n",
    "- When immediate response is not required (background processing)\n",
    "- When operations are independent and can be processed separately\n",
    "- When you need fault tolerance and resilience\n",
    "- When services have varying performance characteristics\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison: Synchronous vs. Asynchronous**\n",
    "\n",
    "```\n",
    "┌───────────────────────┬────────────────────────┬────────────────────────┐\n",
    "│ Characteristic         │ Synchronous             │ Asynchronous            │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Coupling               │ Tight                  │ Loose                   │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Blocking               │ Yes (caller waits)     │ No (caller continues)   │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Fault Tolerance        │ Low (cascading         │ High (isolated          │\n",
    "│                       │ failures)              │ failures)               │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Scalability            │ Limited (bottlenecks)  │ High (independent       │\n",
    "│                       │                        │ scaling)                │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Complexity             │ Simple (direct calls)  │ Complex (event-driven)  │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Consistency            │ Strong                 │ Eventual                │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Response Time          │ Slower (sum of all     │ Faster (immediate       │\n",
    "│                       │ service times)         │ return)                 │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Error Handling         │ Caller handles         │ Retry/Dead Letter       │\n",
    "│                       │ immediately            │ Queues                  │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Use Cases              │ User requests,         │ Background processing,  │\n",
    "│                       │ critical operations    │ notifications,         │\n",
    "│                       │                        │ analytics               │\n",
    "└───────────────────────┴────────────────────────┴────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.2 Message Queue Patterns**\n",
    "\n",
    "Message queues implement different communication patterns for different use cases. Understanding these patterns is essential for designing event-driven systems.\n",
    "\n",
    "### **Point-to-Point Pattern**\n",
    "\n",
    "**Concept**: Each message is consumed by exactly one consumer. Like a task queue—each task is processed by one worker.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Producer                    Queue                    Consumers\n",
    "    │                          │                           │\n",
    "    │── Publish Message 1 ────>│── Consume Message 1 ─────>│ Consumer 1\n",
    "    │                          │                           │\n",
    "    │── Publish Message 2 ────>│── Consume Message 2 ─────>│ Consumer 2\n",
    "    │                          │                           │\n",
    "    │── Publish Message 3 ────>│── Consume Message 3 ─────>│ Consumer 3\n",
    "    │                          │                           │\n",
    "    │                          │                           │ Consumer 4 (idle)\n",
    "    │                          │                           │\n",
    "\n",
    "Each message consumed by exactly one consumer\n",
    "Workload distributed among available consumers\n",
    "```\n",
    "\n",
    "**Use Cases**:\n",
    "- **Task queues**: Background job processing\n",
    "- **Work distribution**: Load balancing across workers\n",
    "- **Sequential processing**: Messages processed in order\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Producer: Publish tasks to queue\n",
    "def publish_task(task_data):\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare queue\n",
    "    channel.queue_declare(queue='tasks', durable=True)\n",
    "    \n",
    "    # Publish task\n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='tasks',\n",
    "        body=json.dumps(task_data),\n",
    "        properties=pika.BasicProperties(\n",
    "            delivery_mode=2,  # Make message persistent\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\" [x] Published task: {task_data['task_id']}\")\n",
    "    connection.close()\n",
    "\n",
    "# Publish 10 tasks\n",
    "for i in range(10):\n",
    "    task_data = {\n",
    "        'task_id': f'task_{i}',\n",
    "        'type': 'image_processing',\n",
    "        'data': {'image_url': f'https://example.com/image_{i}.jpg'}\n",
    "    }\n",
    "    publish_task(task_data)\n",
    "\n",
    "# Consumer: Process tasks from queue\n",
    "def consume_tasks(worker_id):\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare queue\n",
    "    channel.queue_declare(queue='tasks', durable=True)\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        task_data = json.loads(body)\n",
    "        print(f\" [Worker {worker_id}] Processing task: {task_data['task_id']}\")\n",
    "        \n",
    "        # Simulate processing\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Acknowledge message (removes from queue)\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "        print(f\" [Worker {worker_id}] Completed task: {task_data['task_id']}\")\n",
    "    \n",
    "    # Set fair dispatch (don't give new messages to worker until current task acknowledged)\n",
    "    channel.basic_qos(prefetch_count=1)\n",
    "    \n",
    "    # Consume messages\n",
    "    channel.basic_consume(queue='tasks', on_message_callback=callback)\n",
    "    \n",
    "    print(f' [Worker {worker_id}] Waiting for tasks...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Start multiple workers (in separate processes/threads)\n",
    "# Worker 1: Processes task_0, task_4, task_8\n",
    "# Worker 2: Processes task_1, task_5, task_9\n",
    "# Worker 3: Processes task_2, task_6\n",
    "# Worker 4: Processes task_3, task_7\n",
    "\n",
    "# Each task processed by exactly one worker\n",
    "# Workload evenly distributed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Publish-Subscribe Pattern**\n",
    "\n",
    "**Concept**: Each message is consumed by multiple consumers. Like a radio broadcast—multiple listeners receive the same signal.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Producer                    Exchange                    Subscribers\n",
    "    │                          │                              │\n",
    "    │── Publish Message ──────>│── Fanout ──────────────────>│ Subscriber 1\n",
    "    │   \"Order Created\"        │                              │\n",
    "    │                          │── Fanout ──────────────────>│ Subscriber 2\n",
    "    │                          │                              │\n",
    "    │                          │── Fanout ──────────────────>│ Subscriber 3\n",
    "    │                          │                              │\n",
    "    │                          │                              │ Subscriber 4\n",
    "    │                          │                              │\n",
    "\n",
    "Each message consumed by all subscribers\n",
    "All subscribers receive copy of message\n",
    "```\n",
    "\n",
    "**Use Cases**:\n",
    "- **Notifications**: Multiple services notified of events\n",
    "- **Event broadcasting**: Inform multiple systems of state changes\n",
    "- **Fan-out processing**: Same data processed by multiple consumers\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "\n",
    "# Producer: Publish events\n",
    "def publish_event(event_data):\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange (fanout = broadcast to all queues)\n",
    "    channel.exchange_declare(exchange='events', exchange_type='fanout')\n",
    "    \n",
    "    # Publish event (no routing key for fanout)\n",
    "    channel.basic_publish(\n",
    "        exchange='events',\n",
    "        routing_key='',  # Ignored for fanout exchange\n",
    "        body=json.dumps(event_data)\n",
    "    )\n",
    "    \n",
    "    print(f\" [x] Published event: {event_data['event_type']}\")\n",
    "    connection.close()\n",
    "\n",
    "# Publish order created event\n",
    "event_data = {\n",
    "    'event_type': 'order_created',\n",
    "    'order_id': 'ORDER_123',\n",
    "    'customer_id': 'CUSTOMER_456',\n",
    "    'total': 99.99,\n",
    "    'timestamp': '2024-01-15T10:30:00Z'\n",
    "}\n",
    "publish_event(event_data)\n",
    "\n",
    "# Consumer 1: Inventory Service\n",
    "def consume_inventory_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='fanout')\n",
    "    \n",
    "    # Declare exclusive queue (temporary queue for this consumer)\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange\n",
    "    channel.queue_bind(exchange='events', queue=queue_name)\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Inventory Service] Received event: {event_data['event_type']}\")\n",
    "        \n",
    "        if event_data['event_type'] == 'order_created':\n",
    "            # Reserve inventory\n",
    "            print(f\" [Inventory Service] Reserving inventory for order {event_data['order_id']}\")\n",
    "        \n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Inventory Service] Waiting for events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer 2: Email Service\n",
    "def consume_email_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='fanout')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange\n",
    "    channel.queue_bind(exchange='events', queue=queue_name)\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Email Service] Received event: {event_data['event_type']}\")\n",
    "        \n",
    "        if event_data['event_type'] == 'order_created':\n",
    "            # Send confirmation email\n",
    "            print(f\" [Email Service] Sending confirmation email for order {event_data['order_id']}\")\n",
    "        \n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Email Service] Waiting for events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer 3: Analytics Service\n",
    "def consume_analytics_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='fanout')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange\n",
    "    channel.queue_bind(exchange='events', queue=queue_name)\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Analytics Service] Received event: {event_data['event_type']}\")\n",
    "        \n",
    "        # Log to analytics database\n",
    "        print(f\" [Analytics Service] Logging event to analytics database\")\n",
    "        \n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Analytics Service] Waiting for events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# All three consumers receive the same event\n",
    "# Each processes independently\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Topic Pattern (Routing)**\n",
    "\n",
    "**Concept**: Messages routed to consumers based on routing patterns (wildcards). Like subscribing to specific topics—consumers receive messages matching their interests.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Producer                    Exchange                    Subscribers\n",
    "    │                          │                              │\n",
    "    │── Publish ─────────────>│── Routing: \"orders.*\" ─────>│ Subscriber 1\n",
    "    │   \"orders.created\"      │   (matches orders.created)  │   (receives)\n",
    "    │                          │                              │\n",
    "    │── Publish ─────────────>│                              │\n",
    "    │   \"orders.shipped\"      │── Routing: \"orders.shipped\" >│ Subscriber 2\n",
    "    │                          │   (matches orders.shipped)   │   (receives)\n",
    "    │                          │                              │\n",
    "    │── Publish ─────────────>│── Routing: \"orders.*\" ─────>│ Subscriber 1\n",
    "    │   \"orders.cancelled\"    │   (matches orders.cancelled) │   (receives)\n",
    "    │                          │                              │\n",
    "    │                          │                              │ Subscriber 3\n",
    "    │                          │   (doesn't match)            │   (doesn't receive)\n",
    "\n",
    "Messages routed based on routing key patterns\n",
    "* = single word wildcard\n",
    "# = multi-word wildcard\n",
    "```\n",
    "\n",
    "**Use Cases**:\n",
    "- **Selective routing**: Consumers receive only relevant messages\n",
    "- **Topic-based filtering**: Multiple topics, selective subscriptions\n",
    "- **Complex routing**: Pattern-based message distribution\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "\n",
    "# Producer: Publish events with routing keys\n",
    "def publish_event(routing_key, event_data):\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange (topic = pattern-based routing)\n",
    "    channel.exchange_declare(exchange='events', exchange_type='topic')\n",
    "    \n",
    "    # Publish event with routing key\n",
    "    channel.basic_publish(\n",
    "        exchange='events',\n",
    "        routing_key=routing_key,\n",
    "        body=json.dumps(event_data)\n",
    "    )\n",
    "    \n",
    "    print(f\" [x] Published event: {routing_key}\")\n",
    "    connection.close()\n",
    "\n",
    "# Publish various events\n",
    "publish_event('orders.created', {\n",
    "    'event_type': 'order_created',\n",
    "    'order_id': 'ORDER_123',\n",
    "    'total': 99.99\n",
    "})\n",
    "\n",
    "publish_event('orders.shipped', {\n",
    "    'event_type': 'order_shipped',\n",
    "    'order_id': 'ORDER_123',\n",
    "    'tracking_number': 'TRACK_456'\n",
    "})\n",
    "\n",
    "publish_event('orders.cancelled', {\n",
    "    'event_type': 'order_cancelled',\n",
    "    'order_id': 'ORDER_124',\n",
    "    'reason': 'customer_request'\n",
    "})\n",
    "\n",
    "publish_event('payments.processed', {\n",
    "    'event_type': 'payment_processed',\n",
    "    'payment_id': 'PAYMENT_789',\n",
    "    'amount': 99.99\n",
    "})\n",
    "\n",
    "# Consumer 1: Subscribe to all order events\n",
    "def consume_all_order_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='topic')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange with routing pattern\n",
    "    channel.queue_bind(exchange='events', queue=queue_name, routing_key='orders.*')\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        routing_key = method.routing_key\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Order Service] Received: {routing_key}\")\n",
    "        print(f\" [Order Service] Event: {event_data['event_type']}\")\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Order Service] Waiting for order events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer 2: Subscribe to shipped events only\n",
    "def consume_shipped_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='topic')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange with specific routing key\n",
    "    channel.queue_bind(exchange='events', queue=queue_name, routing_key='orders.shipped')\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        routing_key = method.routing_key\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Shipping Service] Received: {routing_key}\")\n",
    "        print(f\" [Shipping Service] Tracking: {event_data['tracking_number']}\")\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Shipping Service] Waiting for shipped events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer 3: Subscribe to all events\n",
    "def consume_all_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='topic')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange with wildcard (all events)\n",
    "    channel.queue_bind(exchange='events', queue=queue_name, routing_key='#')\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        routing_key = method.routing_key\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Analytics Service] Received: {routing_key}\")\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Analytics Service] Waiting for all events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Routing patterns:\n",
    "# * = matches one word (orders.* matches orders.created, orders.shipped)\n",
    "# # = matches zero or more words (# matches everything)\n",
    "\n",
    "# Results:\n",
    "# - Order Service receives: orders.created, orders.shipped, orders.cancelled\n",
    "# - Shipping Service receives: orders.shipped only\n",
    "# - Analytics Service receives: all events (orders.created, orders.shipped, orders.cancelled, payments.processed)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Exchange Types in RabbitMQ**\n",
    "\n",
    "RabbitMQ supports different exchange types for different routing patterns:\n",
    "\n",
    "```\n",
    "┌───────────────────┬────────────────────────┬────────────────────────┐\n",
    "│ Exchange Type      │ Routing Behavior       │ Use Case               │\n",
    "├───────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Direct             │ Exact match on         │ Point-to-point         │\n",
    "│                    │ routing key            │ communication          │\n",
    "├───────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Fanout             │ Broadcast to all       │ Pub/sub notifications  │\n",
    "│                    │ bound queues           │                        │\n",
    "├───────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Topic              │ Pattern-based routing  │ Selective routing      │\n",
    "│                    │ (wildcards: *, #)      │                        │\n",
    "├───────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Headers            │ Match based on message │ Complex routing        │\n",
    "│                    │ headers                │ criteria               │\n",
    "└───────────────────┴────────────────────────┴────────────────────────┘\n",
    "```\n",
    "\n",
    "**Direct Exchange Example**:\n",
    "```python\n",
    "# Direct exchange: messages routed to queues with exact routing key match\n",
    "channel.exchange_declare(exchange='direct_logs', exchange_type='direct')\n",
    "\n",
    "# Bind queue with routing key \"error\"\n",
    "channel.queue_bind(exchange='direct_logs', queue='error_logs', routing_key='error')\n",
    "\n",
    "# Bind queue with routing key \"warning\"\n",
    "channel.queue_bind(exchange='direct_logs', queue='warning_logs', routing_key='warning')\n",
    "\n",
    "# Publish with routing key \"error\"\n",
    "channel.basic_publish(exchange='direct_logs', routing_key='error', body='Error message')\n",
    "# Received by error_logs queue only\n",
    "\n",
    "# Publish with routing key \"warning\"\n",
    "channel.basic_publish(exchange='direct_logs', routing_key='warning', body='Warning message')\n",
    "# Received by warning_logs queue only\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.3 Apache Kafka: Distributed Streaming Platform**\n",
    "\n",
    "Apache Kafka is a distributed streaming platform designed for high-throughput, fault-tolerant, scalable event streaming. Unlike traditional message queues (like RabbitMQ), Kafka is designed for streaming large volumes of data in real-time.\n",
    "\n",
    "### **Kafka Architecture**\n",
    "\n",
    "**Components**:\n",
    "```\n",
    "                    ┌─────────────────┐\n",
    "                    │   Producers     │\n",
    "                    │   (Apps sending │\n",
    "                    │    events)      │\n",
    "                    └────────┬────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "        ┌──────────────────────────────────────┐\n",
    "        │         Kafka Cluster                │\n",
    "        │                                      │\n",
    "        │  ┌────────────┐    ┌────────────┐   │\n",
    "        │  │   Broker   │    │   Broker   │   │\n",
    "        │  │    (K1)    │    │    (K2)    │   │\n",
    "        │  │            │    │            │   │\n",
    "        │  │  ┌──────┐  │    │  ┌──────┐  │   │\n",
    "        │  │  │Topic │  │    │  │Topic │  │   │\n",
    "        │  │  │ A    │  │    │  │ B    │  │   │\n",
    "        │  │  │(0,1) │◄─┼────┼──│(2,3) │  │   │\n",
    "        │  │  └──────┘  │    │  └──────┘  │   │\n",
    "        │  │  ┌──────┐  │    │  ┌──────┐  │   │\n",
    "        │  │  │Topic │  │    │  │Topic │  │   │\n",
    "        │  │  │ C    │  │    │  │ A    │  │   │\n",
    "        │  │  │(4,5) │  │    │  │(1,2) │  │   │\n",
    "        │  │  └──────┘  │    │  └──────┘  │   │\n",
    "        │  └────────────┘    └────────────┘   │\n",
    "        └──────────────────────────────────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                    ┌─────────────────┐\n",
    "                    │   Consumers     │\n",
    "                    │   (Apps reading │\n",
    "                    │    events)      │\n",
    "                    └─────────────────┘\n",
    "\n",
    "Key Concepts:\n",
    "- Topic: Category/feed name to which records are published\n",
    "- Partition: Ordered, immutable sequence of messages within a topic\n",
    "- Broker: Kafka server that stores topics and partitions\n",
    "- Producer: App that publishes events to Kafka topics\n",
    "- Consumer: App that subscribes to topics and processes events\n",
    "- Consumer Group: Group of consumers that cooperate to consume a topic\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka Topics and Partitions**\n",
    "\n",
    "**Topic**: Named channel to which records are published. Like a table in a database, but streaming.\n",
    "\n",
    "**Partition**: Ordered, immutable sequence of records within a topic. Each partition is an ordered, immutable commit log.\n",
    "\n",
    "**Partitioning**: Distributes data across multiple partitions for parallel processing.\n",
    "\n",
    "**Visualization**:\n",
    "```\n",
    "Topic: \"orders\"\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                    Topic: orders                              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Partition 0                  Partition 1                  Partition 2\n",
    "│  ┌──────────┐                 ┌──────────┐                 ┌──────────┐\n",
    "│  │Offset 0  │                 │Offset 0  │                 │Offset 0  │\n",
    "│  │Order A   │                 │Order D   │                 │Order G   │\n",
    "│  ├──────────┤                 ├──────────┤                 ├──────────┤\n",
    "│  │Offset 1  │                 │Offset 1  │                 │Offset 1  │\n",
    "│  │Order B   │                 │Order E   │                 │Order H   │\n",
    "│  ├──────────┤                 ├──────────┤                 ├──────────┤\n",
    "│  │Offset 2  │                 │Offset 2  │                 │Offset 2  │\n",
    "│  │Order C   │                 │Order F   │                 │Order I   │\n",
    "│  ├──────────┤                 ├──────────┤                 ├──────────┤\n",
    "│  │Offset 3  │                 │Offset 3  │                 │Offset 3  │\n",
    "│  │Order J   │                 │Order K   │                 │Order L   │\n",
    "│  └──────────┘                 └──────────┘                 └──────────┘\n",
    "│       │                           │                           │\n",
    "│       │                           │                           │\n",
    "│  Consumer Group 1         Consumer Group 2           Consumer Group 3\n",
    "│  (Consumes P0)           (Consumes P1)            (Consumes P2)\n",
    "\n",
    "Partitioning benefits:\n",
    "- Parallelism: Multiple partitions can be consumed in parallel\n",
    "- Scalability: Add more partitions to increase throughput\n",
    "- Ordering: Messages within partition are ordered\n",
    "- Load balancing: Distribute load across consumers\n",
    "```\n",
    "\n",
    "**Partition Key**: Determines which partition a message goes to.\n",
    "\n",
    "**Partitioning Strategy**:\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "# Producer configuration\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Option 1: No key (messages distributed round-robin)\n",
    "producer.send('orders', {\n",
    "    'order_id': 'ORDER_1',\n",
    "    'customer_id': 'CUSTOMER_1',\n",
    "    'total': 99.99\n",
    "})\n",
    "# Goes to random partition (load balancing)\n",
    "\n",
    "# Option 2: Partition key (messages with same key go to same partition)\n",
    "producer.send('orders', \n",
    "    value={\n",
    "        'order_id': 'ORDER_2',\n",
    "        'customer_id': 'CUSTOMER_1',\n",
    "        'total': 199.99\n",
    "    },\n",
    "    key='CUSTOMER_1'  # Partition key\n",
    ")\n",
    "# All orders for CUSTOMER_1 go to same partition (ordered processing)\n",
    "\n",
    "# Option 3: Custom partitioner (hash-based partitioning)\n",
    "def custom_partitioner(key, all_partitions, available_partitions):\n",
    "    \"\"\"Custom partitioner: hash key and modulo number of partitions\"\"\"\n",
    "    if key is None:\n",
    "        # No key: random partition\n",
    "        import random\n",
    "        return random.choice(available_partitions)\n",
    "    \n",
    "    # Hash key and modulo\n",
    "    hash_value = hashlib.md5(str(key).encode()).hexdigest()\n",
    "    partition = int(hash_value, 16) % len(all_partitions)\n",
    "    return partition\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "    partitioner=custom_partitioner  # Custom partitioner\n",
    ")\n",
    "\n",
    "# Use custom partitioner\n",
    "producer.send('orders', \n",
    "    value={'order_id': 'ORDER_3', 'customer_id': 'CUSTOMER_2'},\n",
    "    key='CUSTOMER_2'\n",
    ")\n",
    "# Goes to partition determined by custom partitioner\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka Consumer Groups**\n",
    "\n",
    "**Consumer Group**: Group of consumers that cooperate to consume a topic. Each partition is consumed by exactly one consumer within the group.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Topic: \"orders\" (3 partitions)\n",
    "\n",
    "Consumer Group A (3 consumers):\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  Consumer Group A                                            │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Partition 0    Partition 1    Partition 2                  │\n",
    "│       │              │              │                       │\n",
    "│       │              │              │                       │\n",
    "│  Consumer A1    Consumer A2    Consumer A3                  │\n",
    "│  (Consumes P0)  (Consumes P1)  (Consumes P2)                │\n",
    "│                                                             │\n",
    "│  Each partition consumed by exactly one consumer            │\n",
    "│  Load balanced across consumers                              │\n",
    "│  Parallel processing: 3x throughput                          │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Consumer Group B (1 consumer):\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│  Consumer Group B                                            │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Partition 0    Partition 1    Partition 2                  │\n",
    "│       │              │              │                       │\n",
    "│       └──────────────┴──────────────┘                       │\n",
    "│                      │                                       │\n",
    "│                 Consumer B1                                  │\n",
    "│              (Consumes all partitions)                       │\n",
    "│                                                             │\n",
    "│  Single consumer consumes all partitions                    │\n",
    "│  Sequential processing (lower throughput)                   │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Key Points:\n",
    "- Each consumer group independently consumes topic\n",
    "- Each partition consumed by exactly one consumer within group\n",
    "- Consumers within group share partitions (load balancing)\n",
    "- Multiple consumer groups can consume same topic independently\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Consumer configuration\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',  # Consumer group ID\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',  # Start from earliest if no offset committed\n",
    "    enable_auto_commit=True,  # Automatically commit offsets\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print('Consumer started...')\n",
    "\n",
    "for message in consumer:\n",
    "    # Process message\n",
    "    order_data = message.value\n",
    "    partition = message.partition\n",
    "    offset = message.offset\n",
    "    \n",
    "    print(f\"Received message: Partition {partition}, Offset {offset}\")\n",
    "    print(f\"Order ID: {order_data['order_id']}\")\n",
    "    print(f\"Customer ID: {order_data['consumer_id']}\")\n",
    "    \n",
    "    # Process order (business logic)\n",
    "    process_order(order_data)\n",
    "    \n",
    "    # Offset automatically committed (if enable_auto_commit=True)\n",
    "    # Manual commit (if enable_auto_commit=False):\n",
    "    # consumer.commit()\n",
    "\n",
    "# Consumer Group Behavior:\n",
    "# - If 3 consumers in group, each gets 1 partition (3 partitions total)\n",
    "# - If 5 consumers in group, 3 get 1 partition each, 2 idle (over-provisioned)\n",
    "# - If 1 consumer in group, it gets all 3 partitions (under-provisioned)\n",
    "\n",
    "# Rebalancing:\n",
    "# - When consumer joins/leaves group, partitions rebalanced\n",
    "# - Kafka automatically reassigns partitions\n",
    "# - Consumers must handle rebalance events\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka Message Semantics**\n",
    "\n",
    "Kafka provides different message delivery guarantees:\n",
    "\n",
    "**1. At-Most-Once Semantics**\n",
    "\n",
    "**Concept**: Messages may be lost but never redelivered.\n",
    "\n",
    "**Configuration**:\n",
    "```python\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    enable_auto_commit=True,  # Auto-commit offsets immediately\n",
    "    auto_commit_interval_ms=1000  # Commit every 1 second\n",
    ")\n",
    "\n",
    "# Problem: If consumer crashes before processing completes,\n",
    "# messages are lost (offset committed, but not processed)\n",
    "```\n",
    "\n",
    "**Use Cases**: Non-critical data where loss is acceptable (analytics, logs)\n",
    "\n",
    "---\n",
    "\n",
    "**2. At-Least-Once Semantics**\n",
    "\n",
    "**Concept**: Messages never lost but may be redelivered (duplicate processing possible).\n",
    "\n",
    "**Configuration**:\n",
    "```python\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    enable_auto_commit=False  # Manual commit (after processing)\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    try:\n",
    "        # Process message\n",
    "        order_data = message.value\n",
    "        process_order(order_data)\n",
    "        \n",
    "        # Commit offset after successful processing\n",
    "        consumer.commit({\n",
    "            topic: message.topic,\n",
    "            partition: message.partition,\n",
    "            offset: message.offset + 1  # Commit next offset\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Processing failed: don't commit offset\n",
    "        # Message will be redelivered on next poll\n",
    "        print(f\"Error processing message: {e}\")\n",
    "\n",
    "# Benefit: Messages never lost (offset committed only after successful processing)\n",
    "# Drawback: Messages may be processed multiple times (if committed offset fails)\n",
    "```\n",
    "\n",
    "**Use Cases**: Critical data where loss is unacceptable (orders, payments)\n",
    "\n",
    "---\n",
    "\n",
    "**3. Exactly-Once Semantics**\n",
    "\n",
    "**Concept**: Each message processed exactly once (no loss, no duplicates).\n",
    "\n",
    "**Implementation** (using Kafka Transactions):\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Producer with exactly-once semantics\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "    transactional_id='order_processing_producer'  # Unique transactional ID\n",
    ")\n",
    "\n",
    "# Initialize transactions\n",
    "producer.init_transaction()\n",
    "\n",
    "# Consumer with exactly-once semantics\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    enable_auto_commit=False,\n",
    "    isolation_level='read_committed'  # Only read committed transactions\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    try:\n",
    "        # Begin transaction\n",
    "        producer.begin_transaction()\n",
    "        \n",
    "        # Process message\n",
    "        order_data = message.value\n",
    "        process_order(order_data)\n",
    "        \n",
    "        # Send result to output topic\n",
    "        producer.send(\n",
    "            'processed_orders',\n",
    "            value={\n",
    "                'order_id': order_data['order_id'],\n",
    "                'status': 'processed',\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Send offsets to transaction\n",
    "        producer.send_offsets_to_transaction(\n",
    "            {\n",
    "                TopicPartition(message.topic, message.partition): OffsetAndMetadata(message.offset + 1, None)\n",
    "            },\n",
    "            consumer.consumer_group()\n",
    "        )\n",
    "        \n",
    "        # Commit transaction (atomic: both message send and offset commit)\n",
    "        producer.commit_transaction()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Transaction failed: abort\n",
    "        producer.abort_transaction()\n",
    "        print(f\"Error processing message: {e}\")\n",
    "\n",
    "# Exactly-once semantics guarantees:\n",
    "# - Messages processed exactly once\n",
    "# - No data loss\n",
    "# - No duplicates\n",
    "# - Atomic operations (message send + offset commit)\n",
    "```\n",
    "\n",
    "**Use Cases**: Critical data where both loss and duplicates are unacceptable (financial transactions)\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka vs. RabbitMQ: When to Use Which**\n",
    "\n",
    "```\n",
    "┌───────────────────────┬────────────────────────┬────────────────────────┐\n",
    "│ Characteristic         │ Kafka                  │ RabbitMQ               │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Architecture           │ Distributed log        │ Message queue          │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Message Ordering       │ Per-partition          │ Per-queue              │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Throughput             │ Very high              │ High                   │\n",
    "│                       │ (millions/sec)         │ (hundreds of thousands)│\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Latency                │ Low (ms)               │ Very low (µs-ms)       │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Persistence            │ Built-in (log-based)   │ Optional               │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Message Retention      │ Configurable           │ Until consumed         │\n",
    "                       │ (hours to days)         │                        │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Scaling                │ Horizontal             │ Vertical                │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Consumer Groups        │ Native support         │ Not native             │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Backpressure           │ Consumer-controlled    │ Publisher-controlled    │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Complex Routing        │ Limited                │ Rich (exchanges,       │\n",
    "│                       │ (topic-based)          │ routing keys)           │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Message Replay         │ Yes (offset rewind)    │ No                      │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Management             │ More complex           │ Simpler                │\n",
    "├───────────────────────┼────────────────────────┼────────────────────────┤\n",
    "│ Use Cases              │ Stream processing,     │ Task queues,           │\n",
    "│                       │ event sourcing,        │ routing,               │\n",
    "│                       │ log aggregation,       │ RPC patterns,          │\n",
    "│                       │ analytics              │ pub/sub                │\n",
    "└───────────────────────┴────────────────────────┴────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.4 Event Sourcing and CQRS**\n",
    "\n",
    "Event Sourcing and CQRS (Command Query Responsibility Segregation) are architectural patterns that leverage message queues and event-driven design to build scalable, auditable systems.\n",
    "\n",
    "### **Event Sourcing**\n",
    "\n",
    "**Concept**: Store state as a sequence of events rather than current state. To reconstruct state, replay all events.\n",
    "\n",
    "**Traditional Architecture (State-Based)**:\n",
    "```\n",
    "User Table:\n",
    "┌─────┬──────────┬─────────────────┬───────────────┐\n",
    "│ ID  │ Name     │ Email           │ Balance       │\n",
    "├─────┼──────────┼─────────────────┼───────────────┤\n",
    "│ 123 │ Alice    │ alice@...       │ 100.00        │\n",
    "└─────┴──────────┴─────────────────┴───────────────┘\n",
    "\n",
    "Problem:\n",
    "- Current state only (no history)\n",
    "- Can't audit changes\n",
    "- Can't replay events\n",
    "- Can't reconstruct past states\n",
    "```\n",
    "\n",
    "**Event Sourcing Architecture (Event-Based)**:\n",
    "```\n",
    "User Events Table:\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│ Event ID │ Event Type      │ User ID │ Data                │ Timestamp │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ EVT_001  │ USER_CREATED    │ 123     │ {\"name\": \"Alice\",   │ 2024-01-01│\n",
    "│          │                │         │  \"email\": \"alice@..\"}│ 10:00:00 │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ EVT_002  │ BALANCE_ADDED   │ 123     │ {\"amount\": 100.00} │ 2024-01-02│\n",
    "│          │                │         │                      │ 09:30:00 │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ EVT_003  │ BALANCE_ADDED   │ 123     │ {\"amount\": 50.00}  │ 2024-01-03│\n",
    "│          │                │         │                      │ 14:15:00 │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│ EVT_004  │ BALANCE_DEDUCTED│ 123     │ {\"amount\": 25.00}  │ 2024-01-04│\n",
    "│          │                │         │                      │ 11:45:00 │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Benefits:\n",
    "- Complete audit trail (all changes recorded)\n",
    "- Can reconstruct any past state (replay events up to that point)\n",
    "- Can replay events (for debugging, testing)\n",
    "- Temporal queries (what was state at time X?)\n",
    "- Event replay (reprocess events with new business logic)\n",
    "```\n",
    "\n",
    "**Event Sourcing Implementation**:\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Event Store (Kafka)\n",
    "event_producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "class UserAggregate:\n",
    "    \"\"\"User aggregate root (reconstructs state from events)\"\"\"\n",
    "    \n",
    "    def __init__(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.name = None\n",
    "        self.email = None\n",
    "        self.balance = 0.0\n",
    "        self.version = 0  # Event version\n",
    "    \n",
    "    def apply_event(self, event):\n",
    "        \"\"\"Apply event to aggregate (updates state)\"\"\"\n",
    "        event_type = event['event_type']\n",
    "        event_data = event['data']\n",
    "        \n",
    "        if event_type == 'USER_CREATED':\n",
    "            self.name = event_data['name']\n",
    "            self.email = event_data['email']\n",
    "            self.version += 1\n",
    "            \n",
    "        elif event_type == 'BALANCE_ADDED':\n",
    "            self.balance += event_data['amount']\n",
    "            self.version += 1\n",
    "            \n",
    "        elif event_type == 'BALANCE_DEDUCTED':\n",
    "            self.balance -= event_data['amount']\n",
    "            self.version += 1\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Serialize aggregate to dict\"\"\"\n",
    "        return {\n",
    "            'user_id': self.user_id,\n",
    "            'name': self.name,\n",
    "            'email': self.email,\n",
    "            'balance': self.balance,\n",
    "            'version': self.version\n",
    "        }\n",
    "\n",
    "# Command: Create User\n",
    "def create_user(user_id, name, email):\n",
    "    \"\"\"Create user command (produces USER_CREATED event)\"\"\"\n",
    "    event = {\n",
    "        'event_id': generate_event_id(),\n",
    "        'event_type': 'USER_CREATED',\n",
    "        'aggregate_id': user_id,\n",
    "        'aggregate_type': 'USER',\n",
    "        'data': {\n",
    "            'name': name,\n",
    "            'email': email\n",
    "        },\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Publish event to event store (Kafka)\n",
    "    event_producer.send('user_events', value=event)\n",
    "    \n",
    "    print(f\"Published event: {event['event_type']}\")\n",
    "    return event\n",
    "\n",
    "# Command: Add Balance\n",
    "def add_balance(user_id, amount):\n",
    "    \"\"\"Add balance command (produces BALANCE_ADDED event)\"\"\"\n",
    "    event = {\n",
    "        'event_id': generate_event_id(),\n",
    "        'event_type': 'BALANCE_ADDED',\n",
    "        'aggregate_id': user_id,\n",
    "        'aggregate_type': 'USER',\n",
    "        'data': {\n",
    "            'amount': amount\n",
    "        },\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Publish event to event store (Kafka)\n",
    "    event_producer.send('user_events', value=event)\n",
    "    \n",
    "    print(f\"Published event: {event['event_type']}\")\n",
    "    return event\n",
    "\n",
    "# Query: Get User State (reconstruct from events)\n",
    "def get_user_state(user_id):\n",
    "    \"\"\"Get user state by replaying events\"\"\"\n",
    "    from kafka import KafkaConsumer\n",
    "    \n",
    "    # Create consumer for user events\n",
    "    consumer = KafkaConsumer(\n",
    "        'user_events',\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='earliest'  # Start from beginning\n",
    "    )\n",
    "    \n",
    "    # Create aggregate\n",
    "    user_aggregate = UserAggregate(user_id)\n",
    "    \n",
    "    # Replay events for this user\n",
    "    for message in consumer:\n",
    "        event = message.value\n",
    "        \n",
    "        # Only process events for this user\n",
    "        if event['aggregate_id'] == user_id:\n",
    "            user_aggregate.apply_event(event)\n",
    "    \n",
    "    # Return reconstructed state\n",
    "    return user_aggregate.to_dict()\n",
    "\n",
    "# Usage:\n",
    "# 1. Create user (produces event)\n",
    "create_user('USER_123', 'Alice', 'alice@example.com')\n",
    "\n",
    "# 2. Add balance (produces event)\n",
    "add_balance('USER_123', 100.00)\n",
    "\n",
    "# 3. Add more balance (produces event)\n",
    "add_balance('USER_123', 50.00)\n",
    "\n",
    "# 4. Deduct balance (produces event)\n",
    "event = {\n",
    "    'event_id': generate_event_id(),\n",
    "    'event_type': 'BALANCE_DEDUCTED',\n",
    "    'aggregate_id': 'USER_123',\n",
    "    'aggregate_type': 'USER',\n",
    "    'data': {'amount': 25.00},\n",
    "    'timestamp': datetime.utcnow().isoformat()\n",
    "}\n",
    "event_producer.send('user_events', value=event)\n",
    "\n",
    "# 5. Get user state (reconstructs from events)\n",
    "user_state = get_user_state('USER_123')\n",
    "print(f\"User State: {user_state}\")\n",
    "# Output: {'user_id': 'USER_123', 'name': 'Alice', 'email': 'alice@example.com', 'balance': 125.0, 'version': 4}\n",
    "```\n",
    "\n",
    "**Event Sourcing Benefits**:\n",
    "1. **Audit Trail**: Complete history of all changes\n",
    "2. **Temporal Queries**: Query state at any point in time\n",
    "3. **Event Replay**: Reprocess events with new business logic\n",
    "4. **Debugging**: Replay events to debug issues\n",
    "5. **Scalability**: Events are immutable (easier to distribute)\n",
    "\n",
    "**Event Sourcing Challenges**:\n",
    "1. **Complexity**: More complex than traditional CRUD\n",
    "2. **Event Schema**: Event schema evolution is challenging\n",
    "3. **Query Performance**: Replaying events is slow (need read models)\n",
    "4. **Storage**: More storage (events vs. current state)\n",
    "\n",
    "---\n",
    "\n",
    "### **CQRS (Command Query Responsibility Segregation)**\n",
    "\n",
    "**Concept**: Separate models for updating (commands) and reading (queries) data. Commands modify state, queries read from optimized read models.\n",
    "\n",
    "**Traditional Architecture (Single Model)**:\n",
    "```\n",
    "Single Database Model:\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                     Users Table                              │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  ┌─────┬──────────┬─────────────────┬───────────────┐      │\n",
    "│  │ ID  │ Name     │ Email           │ Balance       │      │\n",
    "│  ├─────┼──────────┼─────────────────┼───────────────┤      │\n",
    "│  │ 123 │ Alice    │ alice@...       │ 100.00        │      │\n",
    "│  │ 456 │ Bob      │ bob@...         │ 250.00        │      │\n",
    "│  │ 789 │ Charlie │ charlie@...     │ 75.00         │      │\n",
    "│  └─────┴──────────┴─────────────────┴───────────────┘      │\n",
    "│                                                             │\n",
    "│  Same model for:                                           │\n",
    "│  - Writing (commands)                                       │\n",
    "│  - Reading (queries)                                        │\n",
    "│                                                             │\n",
    "│  Problems:                                                  │\n",
    "│  - Optimized for neither                                    │\n",
    "│  - Complex queries slow                                     │\n",
    "│  - Read-heavy vs. write-heavy trade-offs                    │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**CQRS Architecture (Separate Models)**:\n",
    "```\n",
    "Write Model (Command Side):\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                  Write Database (Event Store)               │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  User Events (append-only log):                             │\n",
    "│  ┌──────────────────────────────────────────────────────┐   │\n",
    "│  │ USER_CREATED    │ USER_123 │ {\"name\": \"Alice\", ...} │   │\n",
    "│  │ BALANCE_ADDED   │ USER_123 │ {\"amount\": 100.00}     │   │\n",
    "│  │ BALANCE_DEDUCTED│ USER_123 │ {\"amount\": 25.00}      │   │\n",
    "│  └──────────────────────────────────────────────────────┘   │\n",
    "│                                                             │\n",
    "│  Optimized for:                                             │\n",
    "│  - Appending events (write performance)                      │\n",
    "│  - Transactional consistency                                │\n",
    "│  - Audit trail                                               │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "                │\n",
    "                │ Event Stream\n",
    "                │ (Kafka)\n",
    "                ▼\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                  Read Database (Read Models)                 │\n",
    "├─────────────────────────────────────────────────────────────┤\n",
    "│                                                             │\n",
    "│  Read Model 1: User Summary (optimized for listing)         │\n",
    "│  ┌─────┬──────────┬───────────────┐                         │\n",
    "│  │ ID  │ Name     │ Balance       │                         │\n",
    "│  ├─────┼──────────┼───────────────┤                         │\n",
    "│  │ 123 │ Alice    │ 100.00        │                         │\n",
    "│  │ 456 │ Bob      │ 250.00        │                         │\n",
    "│  └─────┴──────────┴───────────────┘                         │\n",
    "│                                                             │\n",
    "│  Read Model 2: User Transactions (optimized for history)    │\n",
    "│  ┌─────┬──────────────┬────────┬───────────┐                │\n",
    "│  │ ID  │ Type         │ Amount │ Timestamp │                │\n",
    "│  ├─────┼──────────────┼────────┼───────────┤                │\n",
    "│  │ 123 │ BALANCE_ADD  │ 100.00 │ 2024-01-02│                │\n",
    "│  │ 123 │ BALANCE_SUB  │ 25.00  │ 2024-01-04│                │\n",
    "│  └─────┴──────────────┴────────┴───────────┘                │\n",
    "│                                                             │\n",
    "│  Optimized for:                                             │\n",
    "│  - Query performance (read-optimized indexes)               │\n",
    "│  - Complex queries (joins, aggregations)                    │\n",
    "│  - Reporting (analytics)                                    │\n",
    "└─────────────────────────────────────────────────────────────┘\n",
    "\n",
    "Process:\n",
    "1. Command: Update user balance\n",
    "   → Write to event store (append event)\n",
    "   \n",
    "2. Event Stream: New event published\n",
    "   → Event consumed by read model projector\n",
    "   \n",
    "3. Projection: Update read models\n",
    "   → Read models updated asynchronously\n",
    "   \n",
    "4. Query: Read user summary\n",
    "   → Query from read model (fast!)\n",
    "```\n",
    "\n",
    "**CQRS Implementation**:\n",
    "```python\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Event Producer (Command Side)\n",
    "event_producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Command: Add Balance\n",
    "def add_balance_command(user_id, amount):\n",
    "    \"\"\"Command to add balance (produces event)\"\"\"\n",
    "    # Validate command\n",
    "    if amount <= 0:\n",
    "        raise ValueError(\"Amount must be positive\")\n",
    "    \n",
    "    # Produce event\n",
    "    event = {\n",
    "        'event_id': generate_event_id(),\n",
    "        'event_type': 'BALANCE_ADDED',\n",
    "        'aggregate_id': user_id,\n",
    "        'data': {'amount': amount},\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    event_producer.send('user_events', value=event)\n",
    "    return event\n",
    "\n",
    "# Projector: Updates Read Models from Events\n",
    "def user_summary_projector():\n",
    "    \"\"\"Projects events to user summary read model\"\"\"\n",
    "    consumer = KafkaConsumer(\n",
    "        'user_events',\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='earliest',\n",
    "        group_id='user_summary_projector'\n",
    "    )\n",
    "    \n",
    "    for message in consumer:\n",
    "        event = message.value\n",
    "        user_id = event['aggregate_id']\n",
    "        event_type = event['event_type']\n",
    "        \n",
    "        # Update read model based on event\n",
    "        if event_type == 'USER_CREATED':\n",
    "            # Insert user into summary table\n",
    "            db.insert('user_summary', {\n",
    "                'user_id': user_id,\n",
    "                'name': event['data']['name'],\n",
    "                'balance': 0.0\n",
    "            })\n",
    "            \n",
    "        elif event_type == 'BALANCE_ADDED':\n",
    "            # Update balance in summary table\n",
    "            db.update(\n",
    "                'user_summary',\n",
    "                {'balance': db.raw('balance + ?')},\n",
    "                {'user_id': user_id},\n",
    "                params=[event['data']['amount']]\n",
    "            )\n",
    "            \n",
    "        elif event_type == 'BALANCE_DEDUCTED':\n",
    "            # Update balance in summary table\n",
    "            db.update(\n",
    "                'user_summary',\n",
    "                {'balance': db.raw('balance - ?')},\n",
    "                {'user_id': user_id},\n",
    "                params=[event['data']['amount']]\n",
    "            )\n",
    "        \n",
    "        print(f\"Projected event: {event_type} for user {user_id}\")\n",
    "\n",
    "def user_transactions_projector():\n",
    "    \"\"\"Projects events to user transactions read model\"\"\"\n",
    "    consumer = KafkaConsumer(\n",
    "        'user_events',\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='earliest',\n",
    "        group_id='user_transactions_projector'\n",
    "    )\n",
    "    \n",
    "    for message in consumer:\n",
    "        event = message.value\n",
    "        user_id = event['aggregate_id']\n",
    "        event_type = event['event_type']\n",
    "        \n",
    "        # Update transactions read model\n",
    "        if event_type in ['BALANCE_ADDED', 'BALANCE_DEDUCTED']:\n",
    "            transaction_type = 'CREDIT' if event_type == 'BALANCE_ADDED' else 'DEBIT'\n",
    "            \n",
    "            db.insert('user_transactions', {\n",
    "                'transaction_id': generate_transaction_id(),\n",
    "                'user_id': user_id,\n",
    "                'transaction_type': transaction_type,\n",
    "                'amount': event['data']['amount'],\n",
    "                'timestamp': event['timestamp']\n",
    "            })\n",
    "        \n",
    "        print(f\"Projected event: {event_type} for user {user_id}\")\n",
    "\n",
    "# Query: Get User Summary (from Read Model)\n",
    "def get_user_summary_query(user_id):\n",
    "    \"\"\"Query user summary (from read model)\"\"\"\n",
    "    # Query from optimized read model (fast!)\n",
    "    user_summary = db.query_one(\n",
    "        'SELECT * FROM user_summary WHERE user_id = ?',\n",
    "        params=[user_id]\n",
    "    )\n",
    "    return user_summary\n",
    "\n",
    "# Query: Get User Transactions (from Read Model)\n",
    "def get_user_transactions_query(user_id, limit=10):\n",
    "    \"\"\"Query user transactions (from read model)\"\"\"\n",
    "    # Query from optimized read model (fast!)\n",
    "    transactions = db.query(\n",
    "        '''SELECT * FROM user_transactions \n",
    "           WHERE user_id = ? \n",
    "           ORDER BY timestamp DESC \n",
    "           LIMIT ?''',\n",
    "        params=[user_id, limit]\n",
    "    )\n",
    "    return transactions\n",
    "\n",
    "# Usage:\n",
    "# 1. Command: Add balance (writes to event store)\n",
    "add_balance_command('USER_123', 100.00)\n",
    "\n",
    "# 2. Projector (running in background): Updates read models\n",
    "# (user_summary_projector and user_transactions_projector)\n",
    "\n",
    "# 3. Query: Get user summary (from read model - fast!)\n",
    "user_summary = get_user_summary_query('USER_123')\n",
    "print(f\"User Summary: {user_summary}\")\n",
    "\n",
    "# 4. Query: Get user transactions (from read model - fast!)\n",
    "transactions = get_user_transactions_query('USER_123', limit=10)\n",
    "print(f\"User Transactions: {transactions}\")\n",
    "```\n",
    "\n",
    "**CQRS Benefits**:\n",
    "1. **Performance**: Optimized read and write models\n",
    "2. **Scalability**: Read and write sides can scale independently\n",
    "3. **Flexibility**: Multiple read models for different query patterns\n",
    "4. **Complex Queries**: Complex queries don't affect write performance\n",
    "5. **Event Sourcing**: Natural fit with event sourcing\n",
    "\n",
    "**CQRS Challenges**:\n",
    "1. **Complexity**: More complex than traditional CRUD\n",
    "2. **Eventual Consistency**: Read models eventually consistent (not immediate)\n",
    "3. **Duplicate Code**: Separate code for command and query sides\n",
    "4. **Debugging**: Harder to trace command → query flow\n",
    "\n",
    "---\n",
    "\n",
    "## **5.5 Backpressure Handling and Rate Limiting**\n",
    "\n",
    "In event-driven systems, producers can produce messages faster than consumers can process them. This leads to backpressure—the system's inability to process messages quickly enough, causing message accumulation and potential system failure.\n",
    "\n",
    "### **Backpressure Problem**\n",
    "\n",
    "**Scenario**: Producer produces 10,000 messages/second, but consumer only processes 1,000 messages/second.\n",
    "\n",
    "**Consequences**:\n",
    "```\n",
    "Producer (10,000 msg/s)\n",
    "    │\n",
    "    ▼\n",
    "Message Queue (Unbounded)\n",
    "    │\n",
    "    ▼\n",
    "Consumer (1,000 msg/s)\n",
    "\n",
    "Result:\n",
    "- Queue grows unbounded (9,000 messages/second accumulation)\n",
    "- Memory exhaustion (queue stores unprocessed messages)\n",
    "- Increased latency (messages wait longer in queue)\n",
    "- System failure (out of memory, disk full)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Backpressure Strategies**\n",
    "\n",
    "**Strategy 1: Throttling (Rate Limiting)**\n",
    "\n",
    "**Concept**: Limit producer rate to match consumer capacity.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Producer with rate limiting\n",
    "class RateLimitedProducer:\n",
    "    def __init__(self, max_messages_per_second):\n",
    "        self.max_messages_per_second = max_messages_per_second\n",
    "        self.min_interval = 1.0 / max_messages_per_second\n",
    "        self.last_send_time = 0\n",
    "        \n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=['localhost:9092'],\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "    \n",
    "    def send(self, topic, message):\n",
    "        \"\"\"Send message with rate limiting\"\"\"\n",
    "        # Calculate time to wait\n",
    "        current_time = time.time()\n",
    "        time_since_last_send = current_time - self.last_send_time\n",
    "        \n",
    "        if time_since_last_send < self.min_interval:\n",
    "            # Wait to respect rate limit\n",
    "            time.sleep(self.min_interval - time_since_last_send)\n",
    "        \n",
    "        # Send message\n",
    "        self.producer.send(topic, value=message)\n",
    "        self.last_send_time = time.time()\n",
    "\n",
    "# Usage: Limit producer to 1,000 messages/second\n",
    "producer = RateLimitedProducer(max_messages_per_second=1000)\n",
    "\n",
    "# Produce messages (rate limited to 1,000 msg/s)\n",
    "for i in range(10000):\n",
    "    message = {'order_id': f'ORDER_{i}'}\n",
    "    producer.send('orders', message)\n",
    "    # Won't exceed 1,000 messages/second\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 2: Bounded Queues**\n",
    "\n",
    "**Concept**: Limit queue size. Drop oldest messages when queue is full.\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "\n",
    "# Producer with bounded queue\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "# Declare queue with maximum length (10,000 messages)\n",
    "channel.queue_declare(\n",
    "    queue='orders',\n",
    "    durable=True,\n",
    "    arguments={\n",
    "        'x-max-length': 10000,  # Maximum 10,000 messages\n",
    "        'x-overflow': 'drop-head'  # Drop oldest messages when queue is full\n",
    "    }\n",
    ")\n",
    "\n",
    "# Publish messages\n",
    "for i in range(20000):\n",
    "    message = {'order_id': f'ORDER_{i}'}\n",
    "    \n",
    "    try:\n",
    "        channel.basic_publish(\n",
    "            exchange='',\n",
    "            routing_key='orders',\n",
    "            body=json.dumps(message)\n",
    "        )\n",
    "        print(f\"Published message {i}\")\n",
    "    except pika.exceptions.ChannelClosed as e:\n",
    "        # Queue full (message dropped)\n",
    "        print(f\"Queue full, message {i} dropped\")\n",
    "        break\n",
    "\n",
    "# Result:\n",
    "# First 10,000 messages accepted\n",
    "# Next 10,000 messages dropped (queue full)\n",
    "# Queue size stays at 10,000 messages\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 3: Consumer Scaling**\n",
    "\n",
    "**Concept**: Add more consumers to increase processing capacity.\n",
    "\n",
    "**Implementation** (Kafka Consumer Group):\n",
    "```python\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Consumer group with multiple consumers\n",
    "# Start multiple instances of this script (each adds a consumer to group)\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',  # Same group ID for all consumers\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print('Consumer started...')\n",
    "\n",
    "for message in consumer:\n",
    "    # Process message\n",
    "    order_data = message.value\n",
    "    process_order(order_data)\n",
    "    print(f\"Processed order: {order_data['order_id']}\")\n",
    "\n",
    "# Scaling:\n",
    "# - Start with 1 consumer: Processes all partitions\n",
    "# - Add 2nd consumer: Partitions rebalanced (each gets half)\n",
    "# - Add 3rd consumer: Partitions rebalanced (each gets third)\n",
    "# - Continue adding consumers until each partition has its own consumer\n",
    "\n",
    "# Limitation: Maximum consumers = number of partitions\n",
    "# (need more partitions to add more consumers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 4: Reactive Backpressure (Flow Control)**\n",
    "\n",
    "**Concept**: Consumer signals producer to slow down when overwhelmed.\n",
    "\n",
    "**Implementation** (RxPY - Reactive Extensions for Python):\n",
    "```python\n",
    "from rx import operators as ops\n",
    "from rx.subject import Subject\n",
    "import time\n",
    "\n",
    "# Producer (observable)\n",
    "message_producer = Subject()\n",
    "\n",
    "# Consumer (subscriber with backpressure)\n",
    "def consume_message(message):\n",
    "    \"\"\"Simulate message processing\"\"\"\n",
    "    print(f\"Processing message: {message}\")\n",
    "    time.sleep(0.01)  # Simulate processing (10ms per message)\n",
    "\n",
    "# Subscribe with backpressure\n",
    "message_producer.pipe(\n",
    "    # Buffer up to 100 messages\n",
    "    ops.buffer_with_time_or_count(timespan=1.0, count=100),\n",
    "    # Drop oldest messages when buffer is full\n",
    "    ops.map(lambda buffer: buffer[-1] if buffer else None),\n",
    "    # Filter out None (dropped messages)\n",
    "    ops.filter(lambda x: x is not None)\n",
    ").subscribe(\n",
    "    on_next=consume_message,\n",
    "    on_error=lambda e: print(f\"Error: {e}\")\n",
    ")\n",
    "\n",
    "# Producer produces messages (backpressure handled automatically)\n",
    "for i in range(1000):\n",
    "    message_producer.on_next(f'Message_{i}')\n",
    "    time.sleep(0.001)  # Produce 1,000 messages/second\n",
    "\n",
    "# Result:\n",
    "# - Producer produces 1,000 messages/second\n",
    "# - Consumer processes 100 messages/second\n",
    "# - Backpressure handled: Buffer fills, oldest messages dropped\n",
    "# - Consumer never overwhelmed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Rate Limiting Strategies**\n",
    "\n",
    "**Strategy 1: Token Bucket Algorithm**\n",
    "\n",
    "**Concept**: Tokens added to bucket at fixed rate. Each message consumes a token. If no tokens available, message is rejected.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "\n",
    "class TokenBucket:\n",
    "    def __init__(self, rate, capacity):\n",
    "        \"\"\"Initialize token bucket\n",
    "        \n",
    "        Args:\n",
    "            rate: Tokens added per second\n",
    "            capacity: Maximum tokens in bucket\n",
    "        \"\"\"\n",
    "        self.rate = rate\n",
    "        self.capacity = capacity\n",
    "        self.tokens = capacity\n",
    "        self.last_update = time.time()\n",
    "    \n",
    "    def consume(self, tokens=1):\n",
    "        \"\"\"Consume tokens from bucket\n",
    "        \n",
    "        Returns:\n",
    "            True if tokens consumed successfully\n",
    "            False if not enough tokens available\n",
    "        \"\"\"\n",
    "        # Add tokens based on time elapsed\n",
    "        current_time = time.time()\n",
    "        time_elapsed = current_time - self.last_update\n",
    "        self.tokens = min(self.capacity, self.tokens + time_elapsed * self.rate)\n",
    "        self.last_update = current_time\n",
    "        \n",
    "        # Check if enough tokens available\n",
    "        if self.tokens >= tokens:\n",
    "            self.tokens -= tokens\n",
    "            return True\n",
    "        else:\n",
    "            # Not enough tokens\n",
    "            return False\n",
    "\n",
    "# Usage: Rate limit to 1,000 messages/second, burst capacity 100\n",
    "rate_limiter = TokenBucket(rate=1000, capacity=100)\n",
    "\n",
    "# Try to send messages\n",
    "for i in range(200):\n",
    "    if rate_limiter.consume(tokens=1):\n",
    "        print(f\"Message {i} accepted\")\n",
    "        # Send message...\n",
    "    else:\n",
    "        print(f\"Message {i} rejected (rate limit exceeded)\")\n",
    "        time.sleep(0.01)  # Wait and retry\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 2: Sliding Window Algorithm**\n",
    "\n",
    "**Concept**: Track requests within sliding time window. Reject if exceeds limit.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class SlidingWindowRateLimiter:\n",
    "    def __init__(self, window_size, max_requests):\n",
    "        \"\"\"Initialize sliding window rate limiter\n",
    "        \n",
    "        Args:\n",
    "            window_size: Window size in seconds\n",
    "            max_requests: Maximum requests allowed within window\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.max_requests = max_requests\n",
    "        self.requests = deque()  # Store request timestamps\n",
    "    \n",
    "    def allow_request(self):\n",
    "        \"\"\"Check if request is allowed\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Remove requests outside window\n",
    "        while self.requests and current_time - self.requests[0] > self.window_size:\n",
    "            self.requests.popleft()\n",
    "        \n",
    "        # Check if under limit\n",
    "        if len(self.requests) < self.max_requests:\n",
    "            self.requests.append(current_time)\n",
    "            return True\n",
    "        else:\n",
    "            # Rate limit exceeded\n",
    "            return False\n",
    "\n",
    "# Usage: Limit to 100 requests per minute\n",
    "rate_limiter = SlidingWindowRateLimiter(window_size=60, max_requests=100)\n",
    "\n",
    "# Try to send requests\n",
    "for i in range(150):\n",
    "    if rate_limiter.allow_request():\n",
    "        print(f\"Request {i} allowed\")\n",
    "        # Send request...\n",
    "    else:\n",
    "        print(f\"Request {i} rejected (rate limit exceeded)\")\n",
    "        time.sleep(1)  # Wait and retry\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.6 Dead Letter Queues (DLQ) and Message Retry Strategies**\n",
    "\n",
    "In event-driven systems, messages can fail to process (transient errors, consumer crashes, bugs). Dead Letter Queues (DLQ) store failed messages for analysis and retry. Retry strategies determine how and when failed messages are retried.\n",
    "\n",
    "### **Dead Letter Queues**\n",
    "\n",
    "**Concept**: Queue that stores messages that failed to process after multiple retry attempts.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Producer\n",
    "    │\n",
    "    ▼\n",
    "Main Queue\n",
    "    │\n",
    "    ├─→ Process successfully (acknowledge, remove from queue)\n",
    "    │\n",
    "    └─→ Process failed (retry)\n",
    "           │\n",
    "           ├─→ Retry 1: Process successfully (acknowledge, remove from queue)\n",
    "           │\n",
    "           └─→ Retry 2: Process failed (retry)\n",
    "                  │\n",
    "                  ├─→ Retry 3: Process successfully (acknowledge, remove from queue)\n",
    "                  │\n",
    "                  └─→ Max retries exceeded (move to DLQ)\n",
    "                         │\n",
    "                         ▼\n",
    "                  Dead Letter Queue\n",
    "                         │\n",
    "                         ├─→ Analyze (debugging)\n",
    "                         │\n",
    "                         ├─→ Fix (bug fix, data fix)\n",
    "                         │\n",
    "                         └─→ Reprocess (move back to main queue)\n",
    "```\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Connection\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "# Declare main queue with DLQ arguments\n",
    "channel.queue_declare(\n",
    "    queue='orders',\n",
    "    durable=True,\n",
    "    arguments={\n",
    "        'x-dead-letter-exchange': 'dlx',  # DLQ exchange\n",
    "        'x-dead-letter-routing-key': 'orders_dlq'  # DLQ routing key\n",
    "    }\n",
    ")\n",
    "\n",
    "# Declare DLQ exchange\n",
    "channel.exchange_declare(exchange='dlx', exchange_type='direct')\n",
    "\n",
    "# Declare DLQ queue\n",
    "channel.queue_declare(queue='orders_dlq', durable=True)\n",
    "\n",
    "# Bind DLQ queue to DLQ exchange\n",
    "channel.queue_bind(exchange='dlx', queue='orders_dlq', routing_key='orders_dlq')\n",
    "\n",
    "# Producer: Publish messages to main queue\n",
    "def publish_order(order_data):\n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='orders',\n",
    "        body=json.dumps(order_data),\n",
    "        properties=pika.BasicProperties(\n",
    "            delivery_mode=2  # Make message persistent\n",
    "        )\n",
    "    )\n",
    "    print(f\"Published order: {order_data['order_id']}\")\n",
    "\n",
    "# Consumer: Process messages from main queue\n",
    "def consume_orders():\n",
    "    def callback(ch, method, properties, body):\n",
    "        order_data = json.loads(body)\n",
    "        print(f\"Processing order: {order_data['order_id']}\")\n",
    "        \n",
    "        try:\n",
    "            # Process order (may fail)\n",
    "            process_order(order_data)\n",
    "            \n",
    "            # Acknowledge message (removes from queue)\n",
    "            ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "            print(f\"Order processed successfully: {order_data['order_id']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Processing failed\n",
    "            print(f\"Error processing order: {order_data['order_id']}, Error: {e}\")\n",
    "            \n",
    "            # Reject message (requeue for retry)\n",
    "            # Or: Nack message with requeue=False (moves to DLQ after max retries)\n",
    "            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)\n",
    "    \n",
    "    # Set prefetch (consume one message at a time)\n",
    "    channel.basic_qos(prefetch_count=1)\n",
    "    \n",
    "    # Consume messages\n",
    "    channel.basic_consume(queue='orders', on_message_callback=callback)\n",
    "    \n",
    "    print('Waiting for orders...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer: Process messages from DLQ (for analysis)\n",
    "def consume_dead_letters():\n",
    "    def callback(ch, method, properties, body):\n",
    "        order_data = json.loads(body)\n",
    "        print(f\"Dead letter: {order_data['order_id']}\")\n",
    "        \n",
    "        # Analyze failure (logs, debugging)\n",
    "        analyze_failure(order_data, properties)\n",
    "        \n",
    "        # Optionally: Fix and reprocess (move back to main queue)\n",
    "        # reprocess_order(order_data)\n",
    "        \n",
    "        # Acknowledge DLQ message\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    # Consume DLQ messages\n",
    "    channel.basic_consume(queue='orders_dlq', on_message_callback=callback)\n",
    "    \n",
    "    print('Waiting for dead letters...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Result:\n",
    "# - Failed messages retried automatically (with requeue=True)\n",
    "# - After max retries, messages moved to DLQ (if configured)\n",
    "# - DLQ messages analyzed and potentially reprocessed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Message Retry Strategies**\n",
    "\n",
    "**Strategy 1: Fixed Delay Retry**\n",
    "\n",
    "**Concept**: Retry after fixed delay between attempts.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "\n",
    "def process_with_fixed_delay_retry(message, max_retries=3, delay_seconds=5):\n",
    "    \"\"\"Process message with fixed delay retry\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Process message\n",
    "            result = process_message(message)\n",
    "            return result  # Success!\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                # Retry after delay\n",
    "                print(f\"Retrying in {delay_seconds} seconds...\")\n",
    "                time.sleep(delay_seconds)\n",
    "            else:\n",
    "                # Max retries exceeded\n",
    "                print(\"Max retries exceeded, giving up\")\n",
    "                raise e\n",
    "\n",
    "# Usage: Retry up to 3 times, 5 seconds between retries\n",
    "try:\n",
    "    process_with_fixed_delay_retry(message, max_retries=3, delay_seconds=5)\n",
    "except Exception as e:\n",
    "    # Move to DLQ\n",
    "    send_to_dlq(message)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 2: Exponential Backoff Retry**\n",
    "\n",
    "**Concept**: Retry with exponentially increasing delay between attempts.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "import random\n",
    "\n",
    "def process_with_exponential_backoff_retry(message, max_retries=5, base_delay=1, max_delay=60):\n",
    "    \"\"\"Process message with exponential backoff retry\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Process message\n",
    "            result = process_message(message)\n",
    "            return result  # Success!\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                # Calculate delay with exponential backoff\n",
    "                delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)\n",
    "                print(f\"Retrying in {delay:.2f} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                # Max retries exceeded\n",
    "                print(\"Max retries exceeded, giving up\")\n",
    "                raise e\n",
    "\n",
    "# Usage: Retry up to 5 times, exponential backoff (1s, 2s, 4s, 8s, 16s)\n",
    "try:\n",
    "    process_with_exponential_backoff_retry(message, max_retries=5, base_delay=1, max_delay=60)\n",
    "except Exception as e:\n",
    "    # Move to DLQ\n",
    "    send_to_dlq(message)\n",
    "\n",
    "# Benefits:\n",
    "# - First retry quick (1 second)\n",
    "# - Later retries slower (exponential backoff)\n",
    "# - Prevents overwhelming system with retries\n",
    "# - Random jitter prevents thundering herd\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 3: Circuit Breaker Retry**\n",
    "\n",
    "**Concept**: Stop retrying after consecutive failures (circuit opens). Retry after cooldown period.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, failure_threshold=5, cooldown_seconds=60):\n",
    "        \"\"\"Initialize circuit breaker\n",
    "        \n",
    "        Args:\n",
    "            failure_threshold: Consecutive failures before opening circuit\n",
    "            cooldown_seconds: Cooldown period before retrying\n",
    "        \"\"\"\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.cooldown_seconds = cooldown_seconds\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.circuit_open = False\n",
    "    \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        \"\"\"Call function with circuit breaker\"\"\"\n",
    "        # Check if circuit is open\n",
    "        if self.circuit_open:\n",
    "            # Check if cooldown period elapsed\n",
    "            if time.time() - self.last_failure_time > self.cooldown_seconds:\n",
    "                # Cooldown elapsed, close circuit (allow one attempt)\n",
    "                print(\"Cooldown elapsed, closing circuit for one attempt\")\n",
    "                self.circuit_open = False\n",
    "                self.failure_count = 0\n",
    "            else:\n",
    "                # Circuit still open (in cooldown)\n",
    "                raise Exception(\"Circuit breaker open (in cooldown)\")\n",
    "        \n",
    "        try:\n",
    "            # Call function\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # Success: Reset failure count, close circuit\n",
    "            self.failure_count = 0\n",
    "            self.circuit_open = False\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Failure: Increment failure count\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = time.time()\n",
    "            \n",
    "            # Check if threshold exceeded\n",
    "            if self.failure_count >= self.failure_threshold:\n",
    "                # Open circuit (stop retrying)\n",
    "                self.circuit_open = True\n",
    "                print(f\"Circuit breaker opened after {self.failure_threshold} failures\")\n",
    "            \n",
    "            raise e\n",
    "\n",
    "# Usage: Process message with circuit breaker\n",
    "circuit_breaker = CircuitBreaker(failure_threshold=5, cooldown_seconds=60)\n",
    "\n",
    "def process_with_circuit_breaker(message):\n",
    "    \"\"\"Process message with circuit breaker retry\"\"\"\n",
    "    try:\n",
    "        result = circuit_breaker.call(process_message, message)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Circuit breaker open (or message processing failed)\n",
    "        print(f\"Error processing message: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Benefits:\n",
    "# - Prevents cascading failures (stops retrying after threshold)\n",
    "# - System recovers automatically (circuit closes after cooldown)\n",
    "# - Reduces load on failing system (circuit open = no retries)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.7 Message Queue Comparison**\n",
    "\n",
    "**Comparison of Popular Message Queue Systems**:\n",
    "\n",
    "```\n",
    "┌───────────────────────┬──────────────┬──────────────┬──────────────┬─────────────┐\n",
    "│ Feature                │ RabbitMQ     │ Kafka        │ AWS SQS      │ Google Pub  │\n",
    "│                       │              │              │              │ /Sub        │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Type                  │ Message      │ Distributed  │ Message      │ Messaging   │\n",
    "│                       │ Queue        │ Streaming    │ Queue        │ Service     │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Architecture          │ Broker-based │ Distributed  │ Fully        │ Fully       │\n",
    "│                       │              │ Log          │ Managed      │ Managed     │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Message Ordering      │ Per-queue    │ Per-         │ Per-queue    │ Per-topic   │\n",
    "│                       │              │ partition    │              │             │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Throughput            │ High         │ Very High    │ High         │ High        │\n",
    "│                       │ (100K msg/s) │ (Millions/   │ (Unlimited)  │ (1M msg/s)  │\n",
    "│                       │              │ s)           │              │             │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Latency               │ Low (ms)     │ Low (ms)     │ Low (ms)     │ Low (ms)    │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Persistence           │ Optional     │ Built-in     │ Optional     │ Built-in    │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Message Retention      │ Until        │ Configurable │ 4 days       │ 7 days      │\n",
    "│                       │ consumed     │ (hours-      │ (extendable) │ (extendable)│\n",
    "│                       │              │ days)        │              │             │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Scaling               │ Vertical     │ Horizontal   │ Horizontal   │ Horizontal  │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Consumer Groups       │ Manual       │ Native       │ Native       │ Native      │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Backpressure          │ TTL-based    │ Consumer-    │ Manual       │ Consumer-   │\n",
    "│                       │              │ controlled   │              │ controlled  │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Routing               │ Rich (4      │ Limited      │ Manual       │ Rich (topic │\n",
    "│                       │ exchange     │ (topic-      │ (filter)     │ based)     │\n",
    "│                       │ types)       │ based)       │              │             │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Message Replay        │ No           │ Yes          │ No           │ Yes         │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Dead Letter Queues    │ Native       │ Manual       │ Native       │ Native      │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Transaction Support   │ Native       │ Native       │ No           │ Native      │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Management            │ Moderate     │ High         │ Low (managed)│ Low         │\n",
    "│                       │              │              │              │ (managed)   │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Cost                  │ Open source  │ Open source  │ Pay per      │ Pay per     │\n",
    "│                       │ (self-host) │ (self-host) │ request      │ usage       │\n",
    "├───────────────────────┼──────────────┼──────────────┼──────────────┼─────────────┤\n",
    "│ Use Cases             │ Task queues, │ Stream       │ Decoupled    │ Event-      │\n",
    "│                       │ routing,     │ processing,  │ services,    │ driven,     │\n",
    "│                       │ pub/sub      │ event        │ task queues  │ analytics   │\n",
    "│                       │              │ sourcing     │              │             │\n",
    "└───────────────────────┴──────────────┴──────────────┴──────────────┴─────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.8 Key Takeaways**\n",
    "\n",
    "1. **Asynchronous communication decouples services**: Message queues enable loose coupling, fault tolerance, and independent scaling.\n",
    "\n",
    "2. **Choose the right pattern**: Point-to-point for task distribution, publish-subscribe for notifications, topic for selective routing.\n",
    "\n",
    "3. **Kafka for streaming, RabbitMQ for messaging**: Kafka excels at high-throughput streaming and event sourcing. RabbitMQ excels at complex routing and traditional messaging.\n",
    "\n",
    "4. **Event sourcing and CQRS enable auditability**: Event sourcing stores complete event history. CQRS separates read and write models for performance.\n",
    "\n",
    "5. **Handle backpressure proactively**: Implement throttling, bounded queues, consumer scaling, and reactive backpressure to prevent system overload.\n",
    "\n",
    "6. **Implement DLQs and retry strategies**: Dead Letter Queues store failed messages for analysis. Retry strategies (fixed delay, exponential backoff, circuit breaker) determine how and when to retry.\n",
    "\n",
    "7. **Monitor message queue performance**: Track metrics (message rate, queue size, consumer lag, error rates) to ensure system health.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored message queues and event-driven architecture—essential patterns for building scalable, fault-tolerant distributed systems. We covered synchronous vs. asynchronous communication, understanding when each is appropriate.\n",
    "\n",
    "We examined message queue patterns (point-to-point, publish-subscribe, topic), implementing each with RabbitMQ. We explored Apache Kafka in detail, understanding topics, partitions, consumer groups, and message semantics (at-most-once, at-least-once, exactly-once).\n",
    "\n",
    "We introduced Event Sourcing and CQRS—architectural patterns that leverage events for auditability and performance. We covered backpressure handling and rate limiting, understanding how to prevent system overload when producers outpace consumers.\n",
    "\n",
    "Finally, we examined Dead Letter Queues and message retry strategies, understanding how to handle failed messages gracefully.\n",
    "\n",
    "**Coming up next**: In Chapter 6, we'll explore Load Balancing & Traffic Management, covering Layer 4 vs. Layer 7 load balancing, load balancing algorithms, health checks, circuit breakers, global load balancing, API Gateway patterns, and service mesh introduction.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercises**:\n",
    "\n",
    "1. **Communication Pattern Selection**: For each scenario, would you use synchronous or asynchronous communication? Why?\n",
    "   - A user placing an order (immediate confirmation required)\n",
    "   - Sending order confirmation email (background task)\n",
    "   - Updating inventory levels (must be consistent)\n",
    "   - Generating monthly sales report (background analytics)\n",
    "   - Processing payment (must be consistent and confirmed)\n",
    "\n",
    "2. **Message Queue Pattern Design**: You're building a microservices architecture for an e-commerce platform. Design the event flow for:\n",
    "   - Order creation (notifications to inventory, payment, email services)\n",
    "   - Payment processing (notifications to order, inventory, email services)\n",
    "   - Order shipping (notifications to order, email services)\n",
    "   Which message queue pattern would you use for each? How would you structure the topics/exchanges?\n",
    "\n",
    "3. **Kafka Consumer Group Scaling**: You have a Kafka topic with 6 partitions processing 100,000 messages/second. Each consumer can process 10,000 messages/second. How many consumers do you need in the consumer group? What happens if you add more consumers than partitions?\n",
    "\n",
    "4. **Event Sourcing Implementation**: You're building a banking application using Event Sourcing. Design the events for:\n",
    "   - Account creation\n",
    "   - Deposits\n",
    "   - Withdrawals\n",
    "   - Transfers (between accounts)\n",
    "   How would you reconstruct the account balance from events?\n",
    "\n",
    "5. **Backpressure Strategy Selection**: You're building a real-time analytics pipeline processing sensor data from 1 million devices (10 messages/second per device = 10 million messages/second). Your consumers can only process 5 million messages/second. Which backpressure strategy would you use? How would you design the system to handle this load?\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
