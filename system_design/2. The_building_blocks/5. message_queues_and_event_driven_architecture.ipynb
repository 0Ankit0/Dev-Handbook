{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b8fc4d7",
   "metadata": {},
   "source": [
    "# **Chapter 5: Message Queues & Event-Driven Architecture**\n",
    "\n",
    "In modern distributed systems, synchronous communication (direct HTTP calls between services) creates tight coupling and reduces resilience. Message queues and event-driven architectures decouple services, improve scalability, and enable fault-tolerant systems. This chapter explores asynchronous communication patterns, message queue implementations, and event-driven architectural patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **5.1 Synchronous vs. Asynchronous Communication**\n",
    "\n",
    "Understanding the distinction between synchronous and asynchronous communication is fundamental to designing resilient distributed systems.\n",
    "\n",
    "### **Synchronous Communication**\n",
    "\n",
    "**Concept**: The caller waits for the callee to respond before proceeding. Like making a phone call\u2014you speak, wait for the other person to respond, then continue the conversation.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Service A                  Service B                  Service C\n",
    "    \u2502                          \u2502                          \u2502\n",
    "    \u2502\u2500\u2500 HTTP Request \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502                          \u2502\n",
    "    \u2502                          \u2502\u2500\u2500 HTTP Request \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502\n",
    "    \u2502                          \u2502                          \u2502\n",
    "    \u2502<\u2500\u2500 Response (200ms) \u2500\u2500\u2500\u2500\u2500\u2524                          \u2502\n",
    "    \u2502                          \u2502<\u2500\u2500 Response (100ms) \u2500\u2500\u2500\u2500\u2500\u2524\n",
    "    \u2502                          \u2502                          \u2502\n",
    "    \u2502\u2500\u2500 Response to client \u2500\u2500\u2500\u2500\u2524                          \u2502\n",
    "    \u2502                          \u2502                          \u2502\n",
    "\n",
    "Total latency: 300ms\n",
    "Service A blocked for 300ms waiting for responses\n",
    "```\n",
    "\n",
    "**Code Example**:\n",
    "```python\n",
    "import requests\n",
    "import time\n",
    "\n",
    "def process_order(order_id):\n",
    "    \"\"\"Synchronous order processing\"\"\"\n",
    "    print(f\"Processing order {order_id}\")\n",
    "    \n",
    "    # Step 1: Validate order (synchronous call to Order Service)\n",
    "    print(\"Validating order...\")\n",
    "    validation_response = requests.post(\n",
    "        'http://order-service/validate',\n",
    "        json={'order_id': order_id}\n",
    "    )\n",
    "    if not validation_response.json()['valid']:\n",
    "        return \"Order validation failed\"\n",
    "    \n",
    "    # Step 2: Reserve inventory (synchronous call to Inventory Service)\n",
    "    print(\"Reserving inventory...\")\n",
    "    inventory_response = requests.post(\n",
    "        'http://inventory-service/reserve',\n",
    "        json={'order_id': order_id, 'items': [...]},\n",
    "        timeout=5.0  # 5 second timeout\n",
    "    )\n",
    "    if not inventory_response.json()['reserved']:\n",
    "        return \"Inventory reservation failed\"\n",
    "    \n",
    "    # Step 3: Process payment (synchronous call to Payment Service)\n",
    "    print(\"Processing payment...\")\n",
    "    payment_response = requests.post(\n",
    "        'http://payment-service/process',\n",
    "        json={'order_id': order_id, 'amount': 99.99},\n",
    "        timeout=10.0  # 10 second timeout\n",
    "    )\n",
    "    if not payment_response.json()['success']:\n",
    "        return \"Payment processing failed\"\n",
    "    \n",
    "    # Step 4: Send confirmation email (synchronous call to Email Service)\n",
    "    print(\"Sending confirmation email...\")\n",
    "    email_response = requests.post(\n",
    "        'http://email-service/send',\n",
    "        json={'order_id': order_id, 'email': 'customer@example.com'},\n",
    "        timeout=3.0  # 3 second timeout\n",
    "    )\n",
    "    \n",
    "    return \"Order processed successfully\"\n",
    "\n",
    "# Problem: Total time = sum of all service calls\n",
    "# If any service is slow or down, entire operation fails\n",
    "# Service A is blocked waiting for all other services\n",
    "```\n",
    "\n",
    "**Disadvantages**:\n",
    "1. **Tight coupling**: Services depend on each other's availability\n",
    "2. **Blocking**: Calling service waits for response (can't handle other requests)\n",
    "3. **Cascading failures**: Failure in one service causes failure in calling service\n",
    "4. **Poor resilience**: System\u8106\u5f31\u6027 increases with more synchronous dependencies\n",
    "5. **Limited scalability**: Bottlenecks at slowest service\n",
    "\n",
    "**When to Use**:\n",
    "- When immediate response is required (user-facing operations)\n",
    "- When subsequent operations depend on the result of the call\n",
    "- When data consistency is critical and must be confirmed immediately\n",
    "\n",
    "---\n",
    "\n",
    "### **Asynchronous Communication**\n",
    "\n",
    "**Concept**: The caller sends a message and continues without waiting for the callee to respond. Like sending an email\u2014you send it and continue with your day; the recipient reads it later and responds.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Service A         Message Queue         Service B         Service C\n",
    "    \u2502                   \u2502                   \u2502                 \u2502\n",
    "    \u2502\u2500\u2500 Publish \u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502                   \u2502                 \u2502\n",
    "    \u2502   \"Order Created\" \u2502                   \u2502                 \u2502\n",
    "    \u2502                   \u2502\u2500\u2500 Consume \u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502                 \u2502\n",
    "    \u2502                   \u2502                   \u2502\u2500\u2500 Publish \u2500\u2500\u2500\u2500>\u2502\n",
    "    \u2502                   \u2502                   \u2502   \"Inventory    \u2502\n",
    "    \u2502                   \u2502                   \u2502    Reserved\"    \u2502\n",
    "    \u2502                   \u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u2502\n",
    "    \u2502                   \u2502                   \u2502                 \u2502\n",
    "    \u2502<\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                   \u2502                 \u2502\n",
    "    \u2502   \"Order Processed\"                   \u2502                 \u2502\n",
    "    \u2502                   \u2502                   \u2502                 \u2502\n",
    "\n",
    "Service A publishes message and immediately continues\n",
    "Service B and C process independently\n",
    "No blocking, no waiting\n",
    "```\n",
    "\n",
    "**Code Example**:\n",
    "```python\n",
    "import pika  # RabbitMQ client\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Connection to message broker\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "# Declare queue\n",
    "channel.queue_declare(queue='orders')\n",
    "\n",
    "def publish_order_created(order_id):\n",
    "    \"\"\"Publish order created event (asynchronous)\"\"\"\n",
    "    message = {\n",
    "        'event_type': 'order_created',\n",
    "        'order_id': order_id,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='orders',\n",
    "        body=json.dumps(message),\n",
    "        properties=pika.BasicProperties(\n",
    "            delivery_mode=2,  # Make message persistent\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\" [x] Sent 'Order Created: {order_id}'\")\n",
    "    # Function returns immediately - no waiting!\n",
    "\n",
    "def process_order(order_id):\n",
    "    \"\"\"Process order (publishes events, doesn't wait for responses)\"\"\"\n",
    "    print(f\"Processing order {order_id}\")\n",
    "    \n",
    "    # Step 1: Validate order (synchronous to Order Service)\n",
    "    print(\"Validating order...\")\n",
    "    validation_response = requests.post(\n",
    "        'http://order-service/validate',\n",
    "        json={'order_id': order_id}\n",
    "    )\n",
    "    \n",
    "    if validation_response.json()['valid']:\n",
    "        # Publish \"Order Validated\" event (asynchronous)\n",
    "        publish_event('order_validated', {'order_id': order_id})\n",
    "    else:\n",
    "        # Publish \"Order Validation Failed\" event (asynchronous)\n",
    "        publish_event('order_validation_failed', {'order_id': order_id})\n",
    "        return \"Order validation failed\"\n",
    "    \n",
    "    # Step 2: Reserve inventory (publish event, don't wait)\n",
    "    print(\"Publishing inventory reservation request...\")\n",
    "    publish_event('inventory_reservation_requested', {\n",
    "        'order_id': order_id,\n",
    "        'items': [...]\n",
    "    })\n",
    "    # Don't wait for inventory service!\n",
    "    # Continue to next step\n",
    "    \n",
    "    # Step 3: Process payment (publish event, don't wait)\n",
    "    print(\"Publishing payment processing request...\")\n",
    "    publish_event('payment_processing_requested', {\n",
    "        'order_id': order_id,\n",
    "        'amount': 99.99\n",
    "    })\n",
    "    # Don't wait for payment service!\n",
    "    # Continue to next step\n",
    "    \n",
    "    # Step 4: Return immediately (order processing initiated)\n",
    "    return \"Order processing initiated\"\n",
    "\n",
    "def publish_event(event_type, data):\n",
    "    \"\"\"Publish event to message queue\"\"\"\n",
    "    message = {\n",
    "        'event_type': event_type,\n",
    "        'data': data,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    \n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='events',\n",
    "        body=json.dumps(message),\n",
    "        properties=pika.BasicProperties(\n",
    "            delivery_mode=2,  # Make message persistent\n",
    "        )\n",
    "    )\n",
    "\n",
    "# Benefits:\n",
    "# 1. Service A publishes events and immediately returns\n",
    "# 2. Other services consume events independently\n",
    "# 3. No blocking, no waiting\n",
    "# 4. Failure in one service doesn't affect others\n",
    "# 5. Each service can scale independently\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "1. **Loose coupling**: Services don't depend on each other's availability\n",
    "2. **Non-blocking**: Calling service can handle other requests immediately\n",
    "3. **Fault tolerance**: Failure in one service doesn't affect others\n",
    "4. **Scalability**: Each service can scale independently based on load\n",
    "5. **Resilience**: System can tolerate temporary failures (messages queued)\n",
    "\n",
    "**Disadvantages**:\n",
    "1. **Complexity**: Harder to reason about system behavior (asynchronous)\n",
    "2. **Eventual consistency**: Data not immediately consistent across services\n",
    "3. **Error handling**: Need mechanisms for failed messages (retry, dead letter queues)\n",
    "4. **Monitoring**: Harder to track end-to-end request flow\n",
    "5. **Debugging**: Difficult to trace message flow through multiple services\n",
    "\n",
    "**When to Use**:\n",
    "- When immediate response is not required (background processing)\n",
    "- When operations are independent and can be processed separately\n",
    "- When you need fault tolerance and resilience\n",
    "- When services have varying performance characteristics\n",
    "\n",
    "---\n",
    "\n",
    "### **Comparison: Synchronous vs. Asynchronous**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Characteristic         \u2502 Synchronous             \u2502 Asynchronous            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Coupling               \u2502 Tight                  \u2502 Loose                   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Blocking               \u2502 Yes (caller waits)     \u2502 No (caller continues)   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Fault Tolerance        \u2502 Low (cascading         \u2502 High (isolated          \u2502\n",
    "\u2502                       \u2502 failures)              \u2502 failures)               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Scalability            \u2502 Limited (bottlenecks)  \u2502 High (independent       \u2502\n",
    "\u2502                       \u2502                        \u2502 scaling)                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Complexity             \u2502 Simple (direct calls)  \u2502 Complex (event-driven)  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Consistency            \u2502 Strong                 \u2502 Eventual                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Response Time          \u2502 Slower (sum of all     \u2502 Faster (immediate       \u2502\n",
    "\u2502                       \u2502 service times)         \u2502 return)                 \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Error Handling         \u2502 Caller handles         \u2502 Retry/Dead Letter       \u2502\n",
    "\u2502                       \u2502 immediately            \u2502 Queues                  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Use Cases              \u2502 User requests,         \u2502 Background processing,  \u2502\n",
    "\u2502                       \u2502 critical operations    \u2502 notifications,         \u2502\n",
    "\u2502                       \u2502                        \u2502 analytics               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.2 Message Queue Patterns**\n",
    "\n",
    "Message queues implement different communication patterns for different use cases. Understanding these patterns is essential for designing event-driven systems.\n",
    "\n",
    "### **Point-to-Point Pattern**\n",
    "\n",
    "**Concept**: Each message is consumed by exactly one consumer. Like a task queue\u2014each task is processed by one worker.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Producer                    Queue                    Consumers\n",
    "    \u2502                          \u2502                           \u2502\n",
    "    \u2502\u2500\u2500 Publish Message 1 \u2500\u2500\u2500\u2500>\u2502\u2500\u2500 Consume Message 1 \u2500\u2500\u2500\u2500\u2500>\u2502 Consumer 1\n",
    "    \u2502                          \u2502                           \u2502\n",
    "    \u2502\u2500\u2500 Publish Message 2 \u2500\u2500\u2500\u2500>\u2502\u2500\u2500 Consume Message 2 \u2500\u2500\u2500\u2500\u2500>\u2502 Consumer 2\n",
    "    \u2502                          \u2502                           \u2502\n",
    "    \u2502\u2500\u2500 Publish Message 3 \u2500\u2500\u2500\u2500>\u2502\u2500\u2500 Consume Message 3 \u2500\u2500\u2500\u2500\u2500>\u2502 Consumer 3\n",
    "    \u2502                          \u2502                           \u2502\n",
    "    \u2502                          \u2502                           \u2502 Consumer 4 (idle)\n",
    "    \u2502                          \u2502                           \u2502\n",
    "\n",
    "Each message consumed by exactly one consumer\n",
    "Workload distributed among available consumers\n",
    "```\n",
    "\n",
    "**Use Cases**:\n",
    "- **Task queues**: Background job processing\n",
    "- **Work distribution**: Load balancing across workers\n",
    "- **Sequential processing**: Messages processed in order\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Producer: Publish tasks to queue\n",
    "def publish_task(task_data):\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare queue\n",
    "    channel.queue_declare(queue='tasks', durable=True)\n",
    "    \n",
    "    # Publish task\n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='tasks',\n",
    "        body=json.dumps(task_data),\n",
    "        properties=pika.BasicProperties(\n",
    "            delivery_mode=2,  # Make message persistent\n",
    "        )\n",
    "    )\n",
    "    \n",
    "    print(f\" [x] Published task: {task_data['task_id']}\")\n",
    "    connection.close()\n",
    "\n",
    "# Publish 10 tasks\n",
    "for i in range(10):\n",
    "    task_data = {\n",
    "        'task_id': f'task_{i}',\n",
    "        'type': 'image_processing',\n",
    "        'data': {'image_url': f'https://example.com/image_{i}.jpg'}\n",
    "    }\n",
    "    publish_task(task_data)\n",
    "\n",
    "# Consumer: Process tasks from queue\n",
    "def consume_tasks(worker_id):\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare queue\n",
    "    channel.queue_declare(queue='tasks', durable=True)\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        task_data = json.loads(body)\n",
    "        print(f\" [Worker {worker_id}] Processing task: {task_data['task_id']}\")\n",
    "        \n",
    "        # Simulate processing\n",
    "        time.sleep(2)\n",
    "        \n",
    "        # Acknowledge message (removes from queue)\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "        print(f\" [Worker {worker_id}] Completed task: {task_data['task_id']}\")\n",
    "    \n",
    "    # Set fair dispatch (don't give new messages to worker until current task acknowledged)\n",
    "    channel.basic_qos(prefetch_count=1)\n",
    "    \n",
    "    # Consume messages\n",
    "    channel.basic_consume(queue='tasks', on_message_callback=callback)\n",
    "    \n",
    "    print(f' [Worker {worker_id}] Waiting for tasks...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Start multiple workers (in separate processes/threads)\n",
    "# Worker 1: Processes task_0, task_4, task_8\n",
    "# Worker 2: Processes task_1, task_5, task_9\n",
    "# Worker 3: Processes task_2, task_6\n",
    "# Worker 4: Processes task_3, task_7\n",
    "\n",
    "# Each task processed by exactly one worker\n",
    "# Workload evenly distributed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Publish-Subscribe Pattern**\n",
    "\n",
    "**Concept**: Each message is consumed by multiple consumers. Like a radio broadcast\u2014multiple listeners receive the same signal.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Producer                    Exchange                    Subscribers\n",
    "    \u2502                          \u2502                              \u2502\n",
    "    \u2502\u2500\u2500 Publish Message \u2500\u2500\u2500\u2500\u2500\u2500>\u2502\u2500\u2500 Fanout \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 Subscriber 1\n",
    "    \u2502   \"Order Created\"        \u2502                              \u2502\n",
    "    \u2502                          \u2502\u2500\u2500 Fanout \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 Subscriber 2\n",
    "    \u2502                          \u2502                              \u2502\n",
    "    \u2502                          \u2502\u2500\u2500 Fanout \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502 Subscriber 3\n",
    "    \u2502                          \u2502                              \u2502\n",
    "    \u2502                          \u2502                              \u2502 Subscriber 4\n",
    "    \u2502                          \u2502                              \u2502\n",
    "\n",
    "Each message consumed by all subscribers\n",
    "All subscribers receive copy of message\n",
    "```\n",
    "\n",
    "**Use Cases**:\n",
    "- **Notifications**: Multiple services notified of events\n",
    "- **Event broadcasting**: Inform multiple systems of state changes\n",
    "- **Fan-out processing**: Same data processed by multiple consumers\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "\n",
    "# Producer: Publish events\n",
    "def publish_event(event_data):\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange (fanout = broadcast to all queues)\n",
    "    channel.exchange_declare(exchange='events', exchange_type='fanout')\n",
    "    \n",
    "    # Publish event (no routing key for fanout)\n",
    "    channel.basic_publish(\n",
    "        exchange='events',\n",
    "        routing_key='',  # Ignored for fanout exchange\n",
    "        body=json.dumps(event_data)\n",
    "    )\n",
    "    \n",
    "    print(f\" [x] Published event: {event_data['event_type']}\")\n",
    "    connection.close()\n",
    "\n",
    "# Publish order created event\n",
    "event_data = {\n",
    "    'event_type': 'order_created',\n",
    "    'order_id': 'ORDER_123',\n",
    "    'customer_id': 'CUSTOMER_456',\n",
    "    'total': 99.99,\n",
    "    'timestamp': '2024-01-15T10:30:00Z'\n",
    "}\n",
    "publish_event(event_data)\n",
    "\n",
    "# Consumer 1: Inventory Service\n",
    "def consume_inventory_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='fanout')\n",
    "    \n",
    "    # Declare exclusive queue (temporary queue for this consumer)\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange\n",
    "    channel.queue_bind(exchange='events', queue=queue_name)\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Inventory Service] Received event: {event_data['event_type']}\")\n",
    "        \n",
    "        if event_data['event_type'] == 'order_created':\n",
    "            # Reserve inventory\n",
    "            print(f\" [Inventory Service] Reserving inventory for order {event_data['order_id']}\")\n",
    "        \n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Inventory Service] Waiting for events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer 2: Email Service\n",
    "def consume_email_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='fanout')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange\n",
    "    channel.queue_bind(exchange='events', queue=queue_name)\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Email Service] Received event: {event_data['event_type']}\")\n",
    "        \n",
    "        if event_data['event_type'] == 'order_created':\n",
    "            # Send confirmation email\n",
    "            print(f\" [Email Service] Sending confirmation email for order {event_data['order_id']}\")\n",
    "        \n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Email Service] Waiting for events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer 3: Analytics Service\n",
    "def consume_analytics_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='fanout')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange\n",
    "    channel.queue_bind(exchange='events', queue=queue_name)\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Analytics Service] Received event: {event_data['event_type']}\")\n",
    "        \n",
    "        # Log to analytics database\n",
    "        print(f\" [Analytics Service] Logging event to analytics database\")\n",
    "        \n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Analytics Service] Waiting for events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# All three consumers receive the same event\n",
    "# Each processes independently\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Topic Pattern (Routing)**\n",
    "\n",
    "**Concept**: Messages routed to consumers based on routing patterns (wildcards). Like subscribing to specific topics\u2014consumers receive messages matching their interests.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Producer                    Exchange                    Subscribers\n",
    "    \u2502                          \u2502                              \u2502\n",
    "    \u2502\u2500\u2500 Publish \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502\u2500\u2500 Routing: \"orders.*\" \u2500\u2500\u2500\u2500\u2500>\u2502 Subscriber 1\n",
    "    \u2502   \"orders.created\"      \u2502   (matches orders.created)  \u2502   (receives)\n",
    "    \u2502                          \u2502                              \u2502\n",
    "    \u2502\u2500\u2500 Publish \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502                              \u2502\n",
    "    \u2502   \"orders.shipped\"      \u2502\u2500\u2500 Routing: \"orders.shipped\" >\u2502 Subscriber 2\n",
    "    \u2502                          \u2502   (matches orders.shipped)   \u2502   (receives)\n",
    "    \u2502                          \u2502                              \u2502\n",
    "    \u2502\u2500\u2500 Publish \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500>\u2502\u2500\u2500 Routing: \"orders.*\" \u2500\u2500\u2500\u2500\u2500>\u2502 Subscriber 1\n",
    "    \u2502   \"orders.cancelled\"    \u2502   (matches orders.cancelled) \u2502   (receives)\n",
    "    \u2502                          \u2502                              \u2502\n",
    "    \u2502                          \u2502                              \u2502 Subscriber 3\n",
    "    \u2502                          \u2502   (doesn't match)            \u2502   (doesn't receive)\n",
    "\n",
    "Messages routed based on routing key patterns\n",
    "* = single word wildcard\n",
    "# = multi-word wildcard\n",
    "```\n",
    "\n",
    "**Use Cases**:\n",
    "- **Selective routing**: Consumers receive only relevant messages\n",
    "- **Topic-based filtering**: Multiple topics, selective subscriptions\n",
    "- **Complex routing**: Pattern-based message distribution\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "\n",
    "# Producer: Publish events with routing keys\n",
    "def publish_event(routing_key, event_data):\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange (topic = pattern-based routing)\n",
    "    channel.exchange_declare(exchange='events', exchange_type='topic')\n",
    "    \n",
    "    # Publish event with routing key\n",
    "    channel.basic_publish(\n",
    "        exchange='events',\n",
    "        routing_key=routing_key,\n",
    "        body=json.dumps(event_data)\n",
    "    )\n",
    "    \n",
    "    print(f\" [x] Published event: {routing_key}\")\n",
    "    connection.close()\n",
    "\n",
    "# Publish various events\n",
    "publish_event('orders.created', {\n",
    "    'event_type': 'order_created',\n",
    "    'order_id': 'ORDER_123',\n",
    "    'total': 99.99\n",
    "})\n",
    "\n",
    "publish_event('orders.shipped', {\n",
    "    'event_type': 'order_shipped',\n",
    "    'order_id': 'ORDER_123',\n",
    "    'tracking_number': 'TRACK_456'\n",
    "})\n",
    "\n",
    "publish_event('orders.cancelled', {\n",
    "    'event_type': 'order_cancelled',\n",
    "    'order_id': 'ORDER_124',\n",
    "    'reason': 'customer_request'\n",
    "})\n",
    "\n",
    "publish_event('payments.processed', {\n",
    "    'event_type': 'payment_processed',\n",
    "    'payment_id': 'PAYMENT_789',\n",
    "    'amount': 99.99\n",
    "})\n",
    "\n",
    "# Consumer 1: Subscribe to all order events\n",
    "def consume_all_order_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='topic')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange with routing pattern\n",
    "    channel.queue_bind(exchange='events', queue=queue_name, routing_key='orders.*')\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        routing_key = method.routing_key\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Order Service] Received: {routing_key}\")\n",
    "        print(f\" [Order Service] Event: {event_data['event_type']}\")\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Order Service] Waiting for order events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer 2: Subscribe to shipped events only\n",
    "def consume_shipped_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='topic')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange with specific routing key\n",
    "    channel.queue_bind(exchange='events', queue=queue_name, routing_key='orders.shipped')\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        routing_key = method.routing_key\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Shipping Service] Received: {routing_key}\")\n",
    "        print(f\" [Shipping Service] Tracking: {event_data['tracking_number']}\")\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Shipping Service] Waiting for shipped events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer 3: Subscribe to all events\n",
    "def consume_all_events():\n",
    "    connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "    channel = connection.channel()\n",
    "    \n",
    "    # Declare exchange\n",
    "    channel.exchange_declare(exchange='events', exchange_type='topic')\n",
    "    \n",
    "    # Declare exclusive queue\n",
    "    result = channel.queue_declare(queue='', exclusive=True)\n",
    "    queue_name = result.method.queue\n",
    "    \n",
    "    # Bind queue to exchange with wildcard (all events)\n",
    "    channel.queue_bind(exchange='events', queue=queue_name, routing_key='#')\n",
    "    \n",
    "    def callback(ch, method, properties, body):\n",
    "        routing_key = method.routing_key\n",
    "        event_data = json.loads(body)\n",
    "        print(f\" [Analytics Service] Received: {routing_key}\")\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    channel.basic_consume(queue=queue_name, on_message_callback=callback)\n",
    "    print(' [Analytics Service] Waiting for all events...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Routing patterns:\n",
    "# * = matches one word (orders.* matches orders.created, orders.shipped)\n",
    "# # = matches zero or more words (# matches everything)\n",
    "\n",
    "# Results:\n",
    "# - Order Service receives: orders.created, orders.shipped, orders.cancelled\n",
    "# - Shipping Service receives: orders.shipped only\n",
    "# - Analytics Service receives: all events (orders.created, orders.shipped, orders.cancelled, payments.processed)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Exchange Types in RabbitMQ**\n",
    "\n",
    "RabbitMQ supports different exchange types for different routing patterns:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Exchange Type      \u2502 Routing Behavior       \u2502 Use Case               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Direct             \u2502 Exact match on         \u2502 Point-to-point         \u2502\n",
    "\u2502                    \u2502 routing key            \u2502 communication          \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Fanout             \u2502 Broadcast to all       \u2502 Pub/sub notifications  \u2502\n",
    "\u2502                    \u2502 bound queues           \u2502                        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Topic              \u2502 Pattern-based routing  \u2502 Selective routing      \u2502\n",
    "\u2502                    \u2502 (wildcards: *, #)      \u2502                        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Headers            \u2502 Match based on message \u2502 Complex routing        \u2502\n",
    "\u2502                    \u2502 headers                \u2502 criteria               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Direct Exchange Example**:\n",
    "```python\n",
    "# Direct exchange: messages routed to queues with exact routing key match\n",
    "channel.exchange_declare(exchange='direct_logs', exchange_type='direct')\n",
    "\n",
    "# Bind queue with routing key \"error\"\n",
    "channel.queue_bind(exchange='direct_logs', queue='error_logs', routing_key='error')\n",
    "\n",
    "# Bind queue with routing key \"warning\"\n",
    "channel.queue_bind(exchange='direct_logs', queue='warning_logs', routing_key='warning')\n",
    "\n",
    "# Publish with routing key \"error\"\n",
    "channel.basic_publish(exchange='direct_logs', routing_key='error', body='Error message')\n",
    "# Received by error_logs queue only\n",
    "\n",
    "# Publish with routing key \"warning\"\n",
    "channel.basic_publish(exchange='direct_logs', routing_key='warning', body='Warning message')\n",
    "# Received by warning_logs queue only\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.3 Apache Kafka: Distributed Streaming Platform**\n",
    "\n",
    "Apache Kafka is a distributed streaming platform designed for high-throughput, fault-tolerant, scalable event streaming. Unlike traditional message queues (like RabbitMQ), Kafka is designed for streaming large volumes of data in real-time.\n",
    "\n",
    "### **Kafka Architecture**\n",
    "\n",
    "**Components**:\n",
    "```\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502   Producers     \u2502\n",
    "                    \u2502   (Apps sending \u2502\n",
    "                    \u2502    events)      \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                             \u2502\n",
    "                             \u25bc\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502         Kafka Cluster                \u2502\n",
    "        \u2502                                      \u2502\n",
    "        \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "        \u2502  \u2502   Broker   \u2502    \u2502   Broker   \u2502   \u2502\n",
    "        \u2502  \u2502    (K1)    \u2502    \u2502    (K2)    \u2502   \u2502\n",
    "        \u2502  \u2502            \u2502    \u2502            \u2502   \u2502\n",
    "        \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502   \u2502\n",
    "        \u2502  \u2502  \u2502Topic \u2502  \u2502    \u2502  \u2502Topic \u2502  \u2502   \u2502\n",
    "        \u2502  \u2502  \u2502 A    \u2502  \u2502    \u2502  \u2502 B    \u2502  \u2502   \u2502\n",
    "        \u2502  \u2502  \u2502(0,1) \u2502\u25c4\u2500\u253c\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2502(2,3) \u2502  \u2502   \u2502\n",
    "        \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502\n",
    "        \u2502  \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502    \u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u2502   \u2502\n",
    "        \u2502  \u2502  \u2502Topic \u2502  \u2502    \u2502  \u2502Topic \u2502  \u2502   \u2502\n",
    "        \u2502  \u2502  \u2502 C    \u2502  \u2502    \u2502  \u2502 A    \u2502  \u2502   \u2502\n",
    "        \u2502  \u2502  \u2502(4,5) \u2502  \u2502    \u2502  \u2502(1,2) \u2502  \u2502   \u2502\n",
    "        \u2502  \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502    \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2502   \u2502\n",
    "        \u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                             \u2502\n",
    "                             \u25bc\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2502   Consumers     \u2502\n",
    "                    \u2502   (Apps reading \u2502\n",
    "                    \u2502    events)      \u2502\n",
    "                    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Key Concepts:\n",
    "- Topic: Category/feed name to which records are published\n",
    "- Partition: Ordered, immutable sequence of messages within a topic\n",
    "- Broker: Kafka server that stores topics and partitions\n",
    "- Producer: App that publishes events to Kafka topics\n",
    "- Consumer: App that subscribes to topics and processes events\n",
    "- Consumer Group: Group of consumers that cooperate to consume a topic\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka Topics and Partitions**\n",
    "\n",
    "**Topic**: Named channel to which records are published. Like a table in a database, but streaming.\n",
    "\n",
    "**Partition**: Ordered, immutable sequence of records within a topic. Each partition is an ordered, immutable commit log.\n",
    "\n",
    "**Partitioning**: Distributes data across multiple partitions for parallel processing.\n",
    "\n",
    "**Visualization**:\n",
    "```\n",
    "Topic: \"orders\"\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    Topic: orders                              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Partition 0                  Partition 1                  Partition 2\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  \u2502Offset 0  \u2502                 \u2502Offset 0  \u2502                 \u2502Offset 0  \u2502\n",
    "\u2502  \u2502Order A   \u2502                 \u2502Order D   \u2502                 \u2502Order G   \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  \u2502Offset 1  \u2502                 \u2502Offset 1  \u2502                 \u2502Offset 1  \u2502\n",
    "\u2502  \u2502Order B   \u2502                 \u2502Order E   \u2502                 \u2502Order H   \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  \u2502Offset 2  \u2502                 \u2502Offset 2  \u2502                 \u2502Offset 2  \u2502\n",
    "\u2502  \u2502Order C   \u2502                 \u2502Order F   \u2502                 \u2502Order I   \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                 \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502  \u2502Offset 3  \u2502                 \u2502Offset 3  \u2502                 \u2502Offset 3  \u2502\n",
    "\u2502  \u2502Order J   \u2502                 \u2502Order K   \u2502                 \u2502Order L   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\u2502       \u2502                           \u2502                           \u2502\n",
    "\u2502       \u2502                           \u2502                           \u2502\n",
    "\u2502  Consumer Group 1         Consumer Group 2           Consumer Group 3\n",
    "\u2502  (Consumes P0)           (Consumes P1)            (Consumes P2)\n",
    "\n",
    "Partitioning benefits:\n",
    "- Parallelism: Multiple partitions can be consumed in parallel\n",
    "- Scalability: Add more partitions to increase throughput\n",
    "- Ordering: Messages within partition are ordered\n",
    "- Load balancing: Distribute load across consumers\n",
    "```\n",
    "\n",
    "**Partition Key**: Determines which partition a message goes to.\n",
    "\n",
    "**Partitioning Strategy**:\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import hashlib\n",
    "\n",
    "# Producer configuration\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Option 1: No key (messages distributed round-robin)\n",
    "producer.send('orders', {\n",
    "    'order_id': 'ORDER_1',\n",
    "    'customer_id': 'CUSTOMER_1',\n",
    "    'total': 99.99\n",
    "})\n",
    "# Goes to random partition (load balancing)\n",
    "\n",
    "# Option 2: Partition key (messages with same key go to same partition)\n",
    "producer.send('orders', \n",
    "    value={\n",
    "        'order_id': 'ORDER_2',\n",
    "        'customer_id': 'CUSTOMER_1',\n",
    "        'total': 199.99\n",
    "    },\n",
    "    key='CUSTOMER_1'  # Partition key\n",
    ")\n",
    "# All orders for CUSTOMER_1 go to same partition (ordered processing)\n",
    "\n",
    "# Option 3: Custom partitioner (hash-based partitioning)\n",
    "def custom_partitioner(key, all_partitions, available_partitions):\n",
    "    \"\"\"Custom partitioner: hash key and modulo number of partitions\"\"\"\n",
    "    if key is None:\n",
    "        # No key: random partition\n",
    "        import random\n",
    "        return random.choice(available_partitions)\n",
    "    \n",
    "    # Hash key and modulo\n",
    "    hash_value = hashlib.md5(str(key).encode()).hexdigest()\n",
    "    partition = int(hash_value, 16) % len(all_partitions)\n",
    "    return partition\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "    partitioner=custom_partitioner  # Custom partitioner\n",
    ")\n",
    "\n",
    "# Use custom partitioner\n",
    "producer.send('orders', \n",
    "    value={'order_id': 'ORDER_3', 'customer_id': 'CUSTOMER_2'},\n",
    "    key='CUSTOMER_2'\n",
    ")\n",
    "# Goes to partition determined by custom partitioner\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka Consumer Groups**\n",
    "\n",
    "**Consumer Group**: Group of consumers that cooperate to consume a topic. Each partition is consumed by exactly one consumer within the group.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Topic: \"orders\" (3 partitions)\n",
    "\n",
    "Consumer Group A (3 consumers):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Consumer Group A                                            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Partition 0    Partition 1    Partition 2                  \u2502\n",
    "\u2502       \u2502              \u2502              \u2502                       \u2502\n",
    "\u2502       \u2502              \u2502              \u2502                       \u2502\n",
    "\u2502  Consumer A1    Consumer A2    Consumer A3                  \u2502\n",
    "\u2502  (Consumes P0)  (Consumes P1)  (Consumes P2)                \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Each partition consumed by exactly one consumer            \u2502\n",
    "\u2502  Load balanced across consumers                              \u2502\n",
    "\u2502  Parallel processing: 3x throughput                          \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Consumer Group B (1 consumer):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502  Consumer Group B                                            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Partition 0    Partition 1    Partition 2                  \u2502\n",
    "\u2502       \u2502              \u2502              \u2502                       \u2502\n",
    "\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                       \u2502\n",
    "\u2502                      \u2502                                       \u2502\n",
    "\u2502                 Consumer B1                                  \u2502\n",
    "\u2502              (Consumes all partitions)                       \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Single consumer consumes all partitions                    \u2502\n",
    "\u2502  Sequential processing (lower throughput)                   \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Key Points:\n",
    "- Each consumer group independently consumes topic\n",
    "- Each partition consumed by exactly one consumer within group\n",
    "- Consumers within group share partitions (load balancing)\n",
    "- Multiple consumer groups can consume same topic independently\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Consumer configuration\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',  # Consumer group ID\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    auto_offset_reset='earliest',  # Start from earliest if no offset committed\n",
    "    enable_auto_commit=True,  # Automatically commit offsets\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print('Consumer started...')\n",
    "\n",
    "for message in consumer:\n",
    "    # Process message\n",
    "    order_data = message.value\n",
    "    partition = message.partition\n",
    "    offset = message.offset\n",
    "    \n",
    "    print(f\"Received message: Partition {partition}, Offset {offset}\")\n",
    "    print(f\"Order ID: {order_data['order_id']}\")\n",
    "    print(f\"Customer ID: {order_data['consumer_id']}\")\n",
    "    \n",
    "    # Process order (business logic)\n",
    "    process_order(order_data)\n",
    "    \n",
    "    # Offset automatically committed (if enable_auto_commit=True)\n",
    "    # Manual commit (if enable_auto_commit=False):\n",
    "    # consumer.commit()\n",
    "\n",
    "# Consumer Group Behavior:\n",
    "# - If 3 consumers in group, each gets 1 partition (3 partitions total)\n",
    "# - If 5 consumers in group, 3 get 1 partition each, 2 idle (over-provisioned)\n",
    "# - If 1 consumer in group, it gets all 3 partitions (under-provisioned)\n",
    "\n",
    "# Rebalancing:\n",
    "# - When consumer joins/leaves group, partitions rebalanced\n",
    "# - Kafka automatically reassigns partitions\n",
    "# - Consumers must handle rebalance events\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka Message Semantics**\n",
    "\n",
    "Kafka provides different message delivery guarantees:\n",
    "\n",
    "**1. At-Most-Once Semantics**\n",
    "\n",
    "**Concept**: Messages may be lost but never redelivered.\n",
    "\n",
    "**Configuration**:\n",
    "```python\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    enable_auto_commit=True,  # Auto-commit offsets immediately\n",
    "    auto_commit_interval_ms=1000  # Commit every 1 second\n",
    ")\n",
    "\n",
    "# Problem: If consumer crashes before processing completes,\n",
    "# messages are lost (offset committed, but not processed)\n",
    "```\n",
    "\n",
    "**Use Cases**: Non-critical data where loss is acceptable (analytics, logs)\n",
    "\n",
    "---\n",
    "\n",
    "**2. At-Least-Once Semantics**\n",
    "\n",
    "**Concept**: Messages never lost but may be redelivered (duplicate processing possible).\n",
    "\n",
    "**Configuration**:\n",
    "```python\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    enable_auto_commit=False  # Manual commit (after processing)\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    try:\n",
    "        # Process message\n",
    "        order_data = message.value\n",
    "        process_order(order_data)\n",
    "        \n",
    "        # Commit offset after successful processing\n",
    "        consumer.commit({\n",
    "            topic: message.topic,\n",
    "            partition: message.partition,\n",
    "            offset: message.offset + 1  # Commit next offset\n",
    "        })\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Processing failed: don't commit offset\n",
    "        # Message will be redelivered on next poll\n",
    "        print(f\"Error processing message: {e}\")\n",
    "\n",
    "# Benefit: Messages never lost (offset committed only after successful processing)\n",
    "# Drawback: Messages may be processed multiple times (if committed offset fails)\n",
    "```\n",
    "\n",
    "**Use Cases**: Critical data where loss is unacceptable (orders, payments)\n",
    "\n",
    "---\n",
    "\n",
    "**3. Exactly-Once Semantics**\n",
    "\n",
    "**Concept**: Each message processed exactly once (no loss, no duplicates).\n",
    "\n",
    "**Implementation** (using Kafka Transactions):\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Producer with exactly-once semantics\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "    transactional_id='order_processing_producer'  # Unique transactional ID\n",
    ")\n",
    "\n",
    "# Initialize transactions\n",
    "producer.init_transaction()\n",
    "\n",
    "# Consumer with exactly-once semantics\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    enable_auto_commit=False,\n",
    "    isolation_level='read_committed'  # Only read committed transactions\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    try:\n",
    "        # Begin transaction\n",
    "        producer.begin_transaction()\n",
    "        \n",
    "        # Process message\n",
    "        order_data = message.value\n",
    "        process_order(order_data)\n",
    "        \n",
    "        # Send result to output topic\n",
    "        producer.send(\n",
    "            'processed_orders',\n",
    "            value={\n",
    "                'order_id': order_data['order_id'],\n",
    "                'status': 'processed',\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Send offsets to transaction\n",
    "        producer.send_offsets_to_transaction(\n",
    "            {\n",
    "                TopicPartition(message.topic, message.partition): OffsetAndMetadata(message.offset + 1, None)\n",
    "            },\n",
    "            consumer.consumer_group()\n",
    "        )\n",
    "        \n",
    "        # Commit transaction (atomic: both message send and offset commit)\n",
    "        producer.commit_transaction()\n",
    "        \n",
    "    except Exception as e:\n",
    "        # Transaction failed: abort\n",
    "        producer.abort_transaction()\n",
    "        print(f\"Error processing message: {e}\")\n",
    "\n",
    "# Exactly-once semantics guarantees:\n",
    "# - Messages processed exactly once\n",
    "# - No data loss\n",
    "# - No duplicates\n",
    "# - Atomic operations (message send + offset commit)\n",
    "```\n",
    "\n",
    "**Use Cases**: Critical data where both loss and duplicates are unacceptable (financial transactions)\n",
    "\n",
    "---\n",
    "\n",
    "### **Kafka vs. RabbitMQ: When to Use Which**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Characteristic         \u2502 Kafka                  \u2502 RabbitMQ               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Architecture           \u2502 Distributed log        \u2502 Message queue          \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Message Ordering       \u2502 Per-partition          \u2502 Per-queue              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Throughput             \u2502 Very high              \u2502 High                   \u2502\n",
    "\u2502                       \u2502 (millions/sec)         \u2502 (hundreds of thousands)\u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Latency                \u2502 Low (ms)               \u2502 Very low (\u00b5s-ms)       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Persistence            \u2502 Built-in (log-based)   \u2502 Optional               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Message Retention      \u2502 Configurable           \u2502 Until consumed         \u2502\n",
    "                       \u2502 (hours to days)         \u2502                        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Scaling                \u2502 Horizontal             \u2502 Vertical                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Consumer Groups        \u2502 Native support         \u2502 Not native             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Backpressure           \u2502 Consumer-controlled    \u2502 Publisher-controlled    \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Complex Routing        \u2502 Limited                \u2502 Rich (exchanges,       \u2502\n",
    "\u2502                       \u2502 (topic-based)          \u2502 routing keys)           \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Message Replay         \u2502 Yes (offset rewind)    \u2502 No                      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Management             \u2502 More complex           \u2502 Simpler                \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Use Cases              \u2502 Stream processing,     \u2502 Task queues,           \u2502\n",
    "\u2502                       \u2502 event sourcing,        \u2502 routing,               \u2502\n",
    "\u2502                       \u2502 log aggregation,       \u2502 RPC patterns,          \u2502\n",
    "\u2502                       \u2502 analytics              \u2502 pub/sub                \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.4 Event Sourcing and CQRS**\n",
    "\n",
    "Event Sourcing and CQRS (Command Query Responsibility Segregation) are architectural patterns that leverage message queues and event-driven design to build scalable, auditable systems.\n",
    "\n",
    "### **Event Sourcing**\n",
    "\n",
    "**Concept**: Store state as a sequence of events rather than current state. To reconstruct state, replay all events.\n",
    "\n",
    "**Traditional Architecture (State-Based)**:\n",
    "```\n",
    "User Table:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 ID  \u2502 Name     \u2502 Email           \u2502 Balance       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 123 \u2502 Alice    \u2502 alice@...       \u2502 100.00        \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Problem:\n",
    "- Current state only (no history)\n",
    "- Can't audit changes\n",
    "- Can't replay events\n",
    "- Can't reconstruct past states\n",
    "```\n",
    "\n",
    "**Event Sourcing Architecture (Event-Based)**:\n",
    "```\n",
    "User Events Table:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Event ID \u2502 Event Type      \u2502 User ID \u2502 Data                \u2502 Timestamp \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 EVT_001  \u2502 USER_CREATED    \u2502 123     \u2502 {\"name\": \"Alice\",   \u2502 2024-01-01\u2502\n",
    "\u2502          \u2502                \u2502         \u2502  \"email\": \"alice@..\"}\u2502 10:00:00 \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 EVT_002  \u2502 BALANCE_ADDED   \u2502 123     \u2502 {\"amount\": 100.00} \u2502 2024-01-02\u2502\n",
    "\u2502          \u2502                \u2502         \u2502                      \u2502 09:30:00 \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 EVT_003  \u2502 BALANCE_ADDED   \u2502 123     \u2502 {\"amount\": 50.00}  \u2502 2024-01-03\u2502\n",
    "\u2502          \u2502                \u2502         \u2502                      \u2502 14:15:00 \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 EVT_004  \u2502 BALANCE_DEDUCTED\u2502 123     \u2502 {\"amount\": 25.00}  \u2502 2024-01-04\u2502\n",
    "\u2502          \u2502                \u2502         \u2502                      \u2502 11:45:00 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Benefits:\n",
    "- Complete audit trail (all changes recorded)\n",
    "- Can reconstruct any past state (replay events up to that point)\n",
    "- Can replay events (for debugging, testing)\n",
    "- Temporal queries (what was state at time X?)\n",
    "- Event replay (reprocess events with new business logic)\n",
    "```\n",
    "\n",
    "**Event Sourcing Implementation**:\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Event Store (Kafka)\n",
    "event_producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "class UserAggregate:\n",
    "    \"\"\"User aggregate root (reconstructs state from events)\"\"\"\n",
    "    \n",
    "    def __init__(self, user_id):\n",
    "        self.user_id = user_id\n",
    "        self.name = None\n",
    "        self.email = None\n",
    "        self.balance = 0.0\n",
    "        self.version = 0  # Event version\n",
    "    \n",
    "    def apply_event(self, event):\n",
    "        \"\"\"Apply event to aggregate (updates state)\"\"\"\n",
    "        event_type = event['event_type']\n",
    "        event_data = event['data']\n",
    "        \n",
    "        if event_type == 'USER_CREATED':\n",
    "            self.name = event_data['name']\n",
    "            self.email = event_data['email']\n",
    "            self.version += 1\n",
    "            \n",
    "        elif event_type == 'BALANCE_ADDED':\n",
    "            self.balance += event_data['amount']\n",
    "            self.version += 1\n",
    "            \n",
    "        elif event_type == 'BALANCE_DEDUCTED':\n",
    "            self.balance -= event_data['amount']\n",
    "            self.version += 1\n",
    "    \n",
    "    def to_dict(self):\n",
    "        \"\"\"Serialize aggregate to dict\"\"\"\n",
    "        return {\n",
    "            'user_id': self.user_id,\n",
    "            'name': self.name,\n",
    "            'email': self.email,\n",
    "            'balance': self.balance,\n",
    "            'version': self.version\n",
    "        }\n",
    "\n",
    "# Command: Create User\n",
    "def create_user(user_id, name, email):\n",
    "    \"\"\"Create user command (produces USER_CREATED event)\"\"\"\n",
    "    event = {\n",
    "        'event_id': generate_event_id(),\n",
    "        'event_type': 'USER_CREATED',\n",
    "        'aggregate_id': user_id,\n",
    "        'aggregate_type': 'USER',\n",
    "        'data': {\n",
    "            'name': name,\n",
    "            'email': email\n",
    "        },\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Publish event to event store (Kafka)\n",
    "    event_producer.send('user_events', value=event)\n",
    "    \n",
    "    print(f\"Published event: {event['event_type']}\")\n",
    "    return event\n",
    "\n",
    "# Command: Add Balance\n",
    "def add_balance(user_id, amount):\n",
    "    \"\"\"Add balance command (produces BALANCE_ADDED event)\"\"\"\n",
    "    event = {\n",
    "        'event_id': generate_event_id(),\n",
    "        'event_type': 'BALANCE_ADDED',\n",
    "        'aggregate_id': user_id,\n",
    "        'aggregate_type': 'USER',\n",
    "        'data': {\n",
    "            'amount': amount\n",
    "        },\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    # Publish event to event store (Kafka)\n",
    "    event_producer.send('user_events', value=event)\n",
    "    \n",
    "    print(f\"Published event: {event['event_type']}\")\n",
    "    return event\n",
    "\n",
    "# Query: Get User State (reconstruct from events)\n",
    "def get_user_state(user_id):\n",
    "    \"\"\"Get user state by replaying events\"\"\"\n",
    "    from kafka import KafkaConsumer\n",
    "    \n",
    "    # Create consumer for user events\n",
    "    consumer = KafkaConsumer(\n",
    "        'user_events',\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='earliest'  # Start from beginning\n",
    "    )\n",
    "    \n",
    "    # Create aggregate\n",
    "    user_aggregate = UserAggregate(user_id)\n",
    "    \n",
    "    # Replay events for this user\n",
    "    for message in consumer:\n",
    "        event = message.value\n",
    "        \n",
    "        # Only process events for this user\n",
    "        if event['aggregate_id'] == user_id:\n",
    "            user_aggregate.apply_event(event)\n",
    "    \n",
    "    # Return reconstructed state\n",
    "    return user_aggregate.to_dict()\n",
    "\n",
    "# Usage:\n",
    "# 1. Create user (produces event)\n",
    "create_user('USER_123', 'Alice', 'alice@example.com')\n",
    "\n",
    "# 2. Add balance (produces event)\n",
    "add_balance('USER_123', 100.00)\n",
    "\n",
    "# 3. Add more balance (produces event)\n",
    "add_balance('USER_123', 50.00)\n",
    "\n",
    "# 4. Deduct balance (produces event)\n",
    "event = {\n",
    "    'event_id': generate_event_id(),\n",
    "    'event_type': 'BALANCE_DEDUCTED',\n",
    "    'aggregate_id': 'USER_123',\n",
    "    'aggregate_type': 'USER',\n",
    "    'data': {'amount': 25.00},\n",
    "    'timestamp': datetime.utcnow().isoformat()\n",
    "}\n",
    "event_producer.send('user_events', value=event)\n",
    "\n",
    "# 5. Get user state (reconstructs from events)\n",
    "user_state = get_user_state('USER_123')\n",
    "print(f\"User State: {user_state}\")\n",
    "# Output: {'user_id': 'USER_123', 'name': 'Alice', 'email': 'alice@example.com', 'balance': 125.0, 'version': 4}\n",
    "```\n",
    "\n",
    "**Event Sourcing Benefits**:\n",
    "1. **Audit Trail**: Complete history of all changes\n",
    "2. **Temporal Queries**: Query state at any point in time\n",
    "3. **Event Replay**: Reprocess events with new business logic\n",
    "4. **Debugging**: Replay events to debug issues\n",
    "5. **Scalability**: Events are immutable (easier to distribute)\n",
    "\n",
    "**Event Sourcing Challenges**:\n",
    "1. **Complexity**: More complex than traditional CRUD\n",
    "2. **Event Schema**: Event schema evolution is challenging\n",
    "3. **Query Performance**: Replaying events is slow (need read models)\n",
    "4. **Storage**: More storage (events vs. current state)\n",
    "\n",
    "---\n",
    "\n",
    "### **CQRS (Command Query Responsibility Segregation)**\n",
    "\n",
    "**Concept**: Separate models for updating (commands) and reading (queries) data. Commands modify state, queries read from optimized read models.\n",
    "\n",
    "**Traditional Architecture (Single Model)**:\n",
    "```\n",
    "Single Database Model:\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                     Users Table                              \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                             \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u2502\n",
    "\u2502  \u2502 ID  \u2502 Name     \u2502 Email           \u2502 Balance       \u2502      \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524      \u2502\n",
    "\u2502  \u2502 123 \u2502 Alice    \u2502 alice@...       \u2502 100.00        \u2502      \u2502\n",
    "\u2502  \u2502 456 \u2502 Bob      \u2502 bob@...         \u2502 250.00        \u2502      \u2502\n",
    "\u2502  \u2502 789 \u2502 Charlie \u2502 charlie@...     \u2502 75.00         \u2502      \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518      \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Same model for:                                           \u2502\n",
    "\u2502  - Writing (commands)                                       \u2502\n",
    "\u2502  - Reading (queries)                                        \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Problems:                                                  \u2502\n",
    "\u2502  - Optimized for neither                                    \u2502\n",
    "\u2502  - Complex queries slow                                     \u2502\n",
    "\u2502  - Read-heavy vs. write-heavy trade-offs                    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**CQRS Architecture (Separate Models)**:\n",
    "```\n",
    "Write Model (Command Side):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                  Write Database (Event Store)               \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                             \u2502\n",
    "\u2502  User Events (append-only log):                             \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502 USER_CREATED    \u2502 USER_123 \u2502 {\"name\": \"Alice\", ...} \u2502   \u2502\n",
    "\u2502  \u2502 BALANCE_ADDED   \u2502 USER_123 \u2502 {\"amount\": 100.00}     \u2502   \u2502\n",
    "\u2502  \u2502 BALANCE_DEDUCTED\u2502 USER_123 \u2502 {\"amount\": 25.00}      \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Optimized for:                                             \u2502\n",
    "\u2502  - Appending events (write performance)                      \u2502\n",
    "\u2502  - Transactional consistency                                \u2502\n",
    "\u2502  - Audit trail                                               \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "                \u2502\n",
    "                \u2502 Event Stream\n",
    "                \u2502 (Kafka)\n",
    "                \u25bc\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                  Read Database (Read Models)                 \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Read Model 1: User Summary (optimized for listing)         \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                         \u2502\n",
    "\u2502  \u2502 ID  \u2502 Name     \u2502 Balance       \u2502                         \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                         \u2502\n",
    "\u2502  \u2502 123 \u2502 Alice    \u2502 100.00        \u2502                         \u2502\n",
    "\u2502  \u2502 456 \u2502 Bob      \u2502 250.00        \u2502                         \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                         \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Read Model 2: User Transactions (optimized for history)    \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                \u2502\n",
    "\u2502  \u2502 ID  \u2502 Type         \u2502 Amount \u2502 Timestamp \u2502                \u2502\n",
    "\u2502  \u251c\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524                \u2502\n",
    "\u2502  \u2502 123 \u2502 BALANCE_ADD  \u2502 100.00 \u2502 2024-01-02\u2502                \u2502\n",
    "\u2502  \u2502 123 \u2502 BALANCE_SUB  \u2502 25.00  \u2502 2024-01-04\u2502                \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                \u2502\n",
    "\u2502                                                             \u2502\n",
    "\u2502  Optimized for:                                             \u2502\n",
    "\u2502  - Query performance (read-optimized indexes)               \u2502\n",
    "\u2502  - Complex queries (joins, aggregations)                    \u2502\n",
    "\u2502  - Reporting (analytics)                                    \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Process:\n",
    "1. Command: Update user balance\n",
    "   \u2192 Write to event store (append event)\n",
    "   \n",
    "2. Event Stream: New event published\n",
    "   \u2192 Event consumed by read model projector\n",
    "   \n",
    "3. Projection: Update read models\n",
    "   \u2192 Read models updated asynchronously\n",
    "   \n",
    "4. Query: Read user summary\n",
    "   \u2192 Query from read model (fast!)\n",
    "```\n",
    "\n",
    "**CQRS Implementation**:\n",
    "```python\n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Event Producer (Command Side)\n",
    "event_producer = KafkaProducer(\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Command: Add Balance\n",
    "def add_balance_command(user_id, amount):\n",
    "    \"\"\"Command to add balance (produces event)\"\"\"\n",
    "    # Validate command\n",
    "    if amount <= 0:\n",
    "        raise ValueError(\"Amount must be positive\")\n",
    "    \n",
    "    # Produce event\n",
    "    event = {\n",
    "        'event_id': generate_event_id(),\n",
    "        'event_type': 'BALANCE_ADDED',\n",
    "        'aggregate_id': user_id,\n",
    "        'data': {'amount': amount},\n",
    "        'timestamp': datetime.utcnow().isoformat()\n",
    "    }\n",
    "    \n",
    "    event_producer.send('user_events', value=event)\n",
    "    return event\n",
    "\n",
    "# Projector: Updates Read Models from Events\n",
    "def user_summary_projector():\n",
    "    \"\"\"Projects events to user summary read model\"\"\"\n",
    "    consumer = KafkaConsumer(\n",
    "        'user_events',\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='earliest',\n",
    "        group_id='user_summary_projector'\n",
    "    )\n",
    "    \n",
    "    for message in consumer:\n",
    "        event = message.value\n",
    "        user_id = event['aggregate_id']\n",
    "        event_type = event['event_type']\n",
    "        \n",
    "        # Update read model based on event\n",
    "        if event_type == 'USER_CREATED':\n",
    "            # Insert user into summary table\n",
    "            db.insert('user_summary', {\n",
    "                'user_id': user_id,\n",
    "                'name': event['data']['name'],\n",
    "                'balance': 0.0\n",
    "            })\n",
    "            \n",
    "        elif event_type == 'BALANCE_ADDED':\n",
    "            # Update balance in summary table\n",
    "            db.update(\n",
    "                'user_summary',\n",
    "                {'balance': db.raw('balance + ?')},\n",
    "                {'user_id': user_id},\n",
    "                params=[event['data']['amount']]\n",
    "            )\n",
    "            \n",
    "        elif event_type == 'BALANCE_DEDUCTED':\n",
    "            # Update balance in summary table\n",
    "            db.update(\n",
    "                'user_summary',\n",
    "                {'balance': db.raw('balance - ?')},\n",
    "                {'user_id': user_id},\n",
    "                params=[event['data']['amount']]\n",
    "            )\n",
    "        \n",
    "        print(f\"Projected event: {event_type} for user {user_id}\")\n",
    "\n",
    "def user_transactions_projector():\n",
    "    \"\"\"Projects events to user transactions read model\"\"\"\n",
    "    consumer = KafkaConsumer(\n",
    "        'user_events',\n",
    "        bootstrap_servers=['localhost:9092'],\n",
    "        value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "        auto_offset_reset='earliest',\n",
    "        group_id='user_transactions_projector'\n",
    "    )\n",
    "    \n",
    "    for message in consumer:\n",
    "        event = message.value\n",
    "        user_id = event['aggregate_id']\n",
    "        event_type = event['event_type']\n",
    "        \n",
    "        # Update transactions read model\n",
    "        if event_type in ['BALANCE_ADDED', 'BALANCE_DEDUCTED']:\n",
    "            transaction_type = 'CREDIT' if event_type == 'BALANCE_ADDED' else 'DEBIT'\n",
    "            \n",
    "            db.insert('user_transactions', {\n",
    "                'transaction_id': generate_transaction_id(),\n",
    "                'user_id': user_id,\n",
    "                'transaction_type': transaction_type,\n",
    "                'amount': event['data']['amount'],\n",
    "                'timestamp': event['timestamp']\n",
    "            })\n",
    "        \n",
    "        print(f\"Projected event: {event_type} for user {user_id}\")\n",
    "\n",
    "# Query: Get User Summary (from Read Model)\n",
    "def get_user_summary_query(user_id):\n",
    "    \"\"\"Query user summary (from read model)\"\"\"\n",
    "    # Query from optimized read model (fast!)\n",
    "    user_summary = db.query_one(\n",
    "        'SELECT * FROM user_summary WHERE user_id = ?',\n",
    "        params=[user_id]\n",
    "    )\n",
    "    return user_summary\n",
    "\n",
    "# Query: Get User Transactions (from Read Model)\n",
    "def get_user_transactions_query(user_id, limit=10):\n",
    "    \"\"\"Query user transactions (from read model)\"\"\"\n",
    "    # Query from optimized read model (fast!)\n",
    "    transactions = db.query(\n",
    "        '''SELECT * FROM user_transactions \n",
    "           WHERE user_id = ? \n",
    "           ORDER BY timestamp DESC \n",
    "           LIMIT ?''',\n",
    "        params=[user_id, limit]\n",
    "    )\n",
    "    return transactions\n",
    "\n",
    "# Usage:\n",
    "# 1. Command: Add balance (writes to event store)\n",
    "add_balance_command('USER_123', 100.00)\n",
    "\n",
    "# 2. Projector (running in background): Updates read models\n",
    "# (user_summary_projector and user_transactions_projector)\n",
    "\n",
    "# 3. Query: Get user summary (from read model - fast!)\n",
    "user_summary = get_user_summary_query('USER_123')\n",
    "print(f\"User Summary: {user_summary}\")\n",
    "\n",
    "# 4. Query: Get user transactions (from read model - fast!)\n",
    "transactions = get_user_transactions_query('USER_123', limit=10)\n",
    "print(f\"User Transactions: {transactions}\")\n",
    "```\n",
    "\n",
    "**CQRS Benefits**:\n",
    "1. **Performance**: Optimized read and write models\n",
    "2. **Scalability**: Read and write sides can scale independently\n",
    "3. **Flexibility**: Multiple read models for different query patterns\n",
    "4. **Complex Queries**: Complex queries don't affect write performance\n",
    "5. **Event Sourcing**: Natural fit with event sourcing\n",
    "\n",
    "**CQRS Challenges**:\n",
    "1. **Complexity**: More complex than traditional CRUD\n",
    "2. **Eventual Consistency**: Read models eventually consistent (not immediate)\n",
    "3. **Duplicate Code**: Separate code for command and query sides\n",
    "4. **Debugging**: Harder to trace command \u2192 query flow\n",
    "\n",
    "---\n",
    "\n",
    "## **5.5 Backpressure Handling and Rate Limiting**\n",
    "\n",
    "In event-driven systems, producers can produce messages faster than consumers can process them. This leads to backpressure\u2014the system's inability to process messages quickly enough, causing message accumulation and potential system failure.\n",
    "\n",
    "### **Backpressure Problem**\n",
    "\n",
    "**Scenario**: Producer produces 10,000 messages/second, but consumer only processes 1,000 messages/second.\n",
    "\n",
    "**Consequences**:\n",
    "```\n",
    "Producer (10,000 msg/s)\n",
    "    \u2502\n",
    "    \u25bc\n",
    "Message Queue (Unbounded)\n",
    "    \u2502\n",
    "    \u25bc\n",
    "Consumer (1,000 msg/s)\n",
    "\n",
    "Result:\n",
    "- Queue grows unbounded (9,000 messages/second accumulation)\n",
    "- Memory exhaustion (queue stores unprocessed messages)\n",
    "- Increased latency (messages wait longer in queue)\n",
    "- System failure (out of memory, disk full)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Backpressure Strategies**\n",
    "\n",
    "**Strategy 1: Throttling (Rate Limiting)**\n",
    "\n",
    "**Concept**: Limit producer rate to match consumer capacity.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "# Producer with rate limiting\n",
    "class RateLimitedProducer:\n",
    "    def __init__(self, max_messages_per_second):\n",
    "        self.max_messages_per_second = max_messages_per_second\n",
    "        self.min_interval = 1.0 / max_messages_per_second\n",
    "        self.last_send_time = 0\n",
    "        \n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=['localhost:9092'],\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "    \n",
    "    def send(self, topic, message):\n",
    "        \"\"\"Send message with rate limiting\"\"\"\n",
    "        # Calculate time to wait\n",
    "        current_time = time.time()\n",
    "        time_since_last_send = current_time - self.last_send_time\n",
    "        \n",
    "        if time_since_last_send < self.min_interval:\n",
    "            # Wait to respect rate limit\n",
    "            time.sleep(self.min_interval - time_since_last_send)\n",
    "        \n",
    "        # Send message\n",
    "        self.producer.send(topic, value=message)\n",
    "        self.last_send_time = time.time()\n",
    "\n",
    "# Usage: Limit producer to 1,000 messages/second\n",
    "producer = RateLimitedProducer(max_messages_per_second=1000)\n",
    "\n",
    "# Produce messages (rate limited to 1,000 msg/s)\n",
    "for i in range(10000):\n",
    "    message = {'order_id': f'ORDER_{i}'}\n",
    "    producer.send('orders', message)\n",
    "    # Won't exceed 1,000 messages/second\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 2: Bounded Queues**\n",
    "\n",
    "**Concept**: Limit queue size. Drop oldest messages when queue is full.\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "\n",
    "# Producer with bounded queue\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "# Declare queue with maximum length (10,000 messages)\n",
    "channel.queue_declare(\n",
    "    queue='orders',\n",
    "    durable=True,\n",
    "    arguments={\n",
    "        'x-max-length': 10000,  # Maximum 10,000 messages\n",
    "        'x-overflow': 'drop-head'  # Drop oldest messages when queue is full\n",
    "    }\n",
    ")\n",
    "\n",
    "# Publish messages\n",
    "for i in range(20000):\n",
    "    message = {'order_id': f'ORDER_{i}'}\n",
    "    \n",
    "    try:\n",
    "        channel.basic_publish(\n",
    "            exchange='',\n",
    "            routing_key='orders',\n",
    "            body=json.dumps(message)\n",
    "        )\n",
    "        print(f\"Published message {i}\")\n",
    "    except pika.exceptions.ChannelClosed as e:\n",
    "        # Queue full (message dropped)\n",
    "        print(f\"Queue full, message {i} dropped\")\n",
    "        break\n",
    "\n",
    "# Result:\n",
    "# First 10,000 messages accepted\n",
    "# Next 10,000 messages dropped (queue full)\n",
    "# Queue size stays at 10,000 messages\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 3: Consumer Scaling**\n",
    "\n",
    "**Concept**: Add more consumers to increase processing capacity.\n",
    "\n",
    "**Implementation** (Kafka Consumer Group):\n",
    "```python\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "# Consumer group with multiple consumers\n",
    "# Start multiple instances of this script (each adds a consumer to group)\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'orders',\n",
    "    group_id='order_processing_group',  # Same group ID for all consumers\n",
    "    bootstrap_servers=['localhost:9092'],\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "print('Consumer started...')\n",
    "\n",
    "for message in consumer:\n",
    "    # Process message\n",
    "    order_data = message.value\n",
    "    process_order(order_data)\n",
    "    print(f\"Processed order: {order_data['order_id']}\")\n",
    "\n",
    "# Scaling:\n",
    "# - Start with 1 consumer: Processes all partitions\n",
    "# - Add 2nd consumer: Partitions rebalanced (each gets half)\n",
    "# - Add 3rd consumer: Partitions rebalanced (each gets third)\n",
    "# - Continue adding consumers until each partition has its own consumer\n",
    "\n",
    "# Limitation: Maximum consumers = number of partitions\n",
    "# (need more partitions to add more consumers)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 4: Reactive Backpressure (Flow Control)**\n",
    "\n",
    "**Concept**: Consumer signals producer to slow down when overwhelmed.\n",
    "\n",
    "**Implementation** (RxPY - Reactive Extensions for Python):\n",
    "```python\n",
    "from rx import operators as ops\n",
    "from rx.subject import Subject\n",
    "import time\n",
    "\n",
    "# Producer (observable)\n",
    "message_producer = Subject()\n",
    "\n",
    "# Consumer (subscriber with backpressure)\n",
    "def consume_message(message):\n",
    "    \"\"\"Simulate message processing\"\"\"\n",
    "    print(f\"Processing message: {message}\")\n",
    "    time.sleep(0.01)  # Simulate processing (10ms per message)\n",
    "\n",
    "# Subscribe with backpressure\n",
    "message_producer.pipe(\n",
    "    # Buffer up to 100 messages\n",
    "    ops.buffer_with_time_or_count(timespan=1.0, count=100),\n",
    "    # Drop oldest messages when buffer is full\n",
    "    ops.map(lambda buffer: buffer[-1] if buffer else None),\n",
    "    # Filter out None (dropped messages)\n",
    "    ops.filter(lambda x: x is not None)\n",
    ").subscribe(\n",
    "    on_next=consume_message,\n",
    "    on_error=lambda e: print(f\"Error: {e}\")\n",
    ")\n",
    "\n",
    "# Producer produces messages (backpressure handled automatically)\n",
    "for i in range(1000):\n",
    "    message_producer.on_next(f'Message_{i}')\n",
    "    time.sleep(0.001)  # Produce 1,000 messages/second\n",
    "\n",
    "# Result:\n",
    "# - Producer produces 1,000 messages/second\n",
    "# - Consumer processes 100 messages/second\n",
    "# - Backpressure handled: Buffer fills, oldest messages dropped\n",
    "# - Consumer never overwhelmed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Rate Limiting Strategies**\n",
    "\n",
    "**Strategy 1: Token Bucket Algorithm**\n",
    "\n",
    "**Concept**: Tokens added to bucket at fixed rate. Each message consumes a token. If no tokens available, message is rejected.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "\n",
    "class TokenBucket:\n",
    "    def __init__(self, rate, capacity):\n",
    "        \"\"\"Initialize token bucket\n",
    "        \n",
    "        Args:\n",
    "            rate: Tokens added per second\n",
    "            capacity: Maximum tokens in bucket\n",
    "        \"\"\"\n",
    "        self.rate = rate\n",
    "        self.capacity = capacity\n",
    "        self.tokens = capacity\n",
    "        self.last_update = time.time()\n",
    "    \n",
    "    def consume(self, tokens=1):\n",
    "        \"\"\"Consume tokens from bucket\n",
    "        \n",
    "        Returns:\n",
    "            True if tokens consumed successfully\n",
    "            False if not enough tokens available\n",
    "        \"\"\"\n",
    "        # Add tokens based on time elapsed\n",
    "        current_time = time.time()\n",
    "        time_elapsed = current_time - self.last_update\n",
    "        self.tokens = min(self.capacity, self.tokens + time_elapsed * self.rate)\n",
    "        self.last_update = current_time\n",
    "        \n",
    "        # Check if enough tokens available\n",
    "        if self.tokens >= tokens:\n",
    "            self.tokens -= tokens\n",
    "            return True\n",
    "        else:\n",
    "            # Not enough tokens\n",
    "            return False\n",
    "\n",
    "# Usage: Rate limit to 1,000 messages/second, burst capacity 100\n",
    "rate_limiter = TokenBucket(rate=1000, capacity=100)\n",
    "\n",
    "# Try to send messages\n",
    "for i in range(200):\n",
    "    if rate_limiter.consume(tokens=1):\n",
    "        print(f\"Message {i} accepted\")\n",
    "        # Send message...\n",
    "    else:\n",
    "        print(f\"Message {i} rejected (rate limit exceeded)\")\n",
    "        time.sleep(0.01)  # Wait and retry\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 2: Sliding Window Algorithm**\n",
    "\n",
    "**Concept**: Track requests within sliding time window. Reject if exceeds limit.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "from collections import deque\n",
    "\n",
    "class SlidingWindowRateLimiter:\n",
    "    def __init__(self, window_size, max_requests):\n",
    "        \"\"\"Initialize sliding window rate limiter\n",
    "        \n",
    "        Args:\n",
    "            window_size: Window size in seconds\n",
    "            max_requests: Maximum requests allowed within window\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.max_requests = max_requests\n",
    "        self.requests = deque()  # Store request timestamps\n",
    "    \n",
    "    def allow_request(self):\n",
    "        \"\"\"Check if request is allowed\"\"\"\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # Remove requests outside window\n",
    "        while self.requests and current_time - self.requests[0] > self.window_size:\n",
    "            self.requests.popleft()\n",
    "        \n",
    "        # Check if under limit\n",
    "        if len(self.requests) < self.max_requests:\n",
    "            self.requests.append(current_time)\n",
    "            return True\n",
    "        else:\n",
    "            # Rate limit exceeded\n",
    "            return False\n",
    "\n",
    "# Usage: Limit to 100 requests per minute\n",
    "rate_limiter = SlidingWindowRateLimiter(window_size=60, max_requests=100)\n",
    "\n",
    "# Try to send requests\n",
    "for i in range(150):\n",
    "    if rate_limiter.allow_request():\n",
    "        print(f\"Request {i} allowed\")\n",
    "        # Send request...\n",
    "    else:\n",
    "        print(f\"Request {i} rejected (rate limit exceeded)\")\n",
    "        time.sleep(1)  # Wait and retry\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.6 Dead Letter Queues (DLQ) and Message Retry Strategies**\n",
    "\n",
    "In event-driven systems, messages can fail to process (transient errors, consumer crashes, bugs). Dead Letter Queues (DLQ) store failed messages for analysis and retry. Retry strategies determine how and when failed messages are retried.\n",
    "\n",
    "### **Dead Letter Queues**\n",
    "\n",
    "**Concept**: Queue that stores messages that failed to process after multiple retry attempts.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Producer\n",
    "    \u2502\n",
    "    \u25bc\n",
    "Main Queue\n",
    "    \u2502\n",
    "    \u251c\u2500\u2192 Process successfully (acknowledge, remove from queue)\n",
    "    \u2502\n",
    "    \u2514\u2500\u2192 Process failed (retry)\n",
    "           \u2502\n",
    "           \u251c\u2500\u2192 Retry 1: Process successfully (acknowledge, remove from queue)\n",
    "           \u2502\n",
    "           \u2514\u2500\u2192 Retry 2: Process failed (retry)\n",
    "                  \u2502\n",
    "                  \u251c\u2500\u2192 Retry 3: Process successfully (acknowledge, remove from queue)\n",
    "                  \u2502\n",
    "                  \u2514\u2500\u2192 Max retries exceeded (move to DLQ)\n",
    "                         \u2502\n",
    "                         \u25bc\n",
    "                  Dead Letter Queue\n",
    "                         \u2502\n",
    "                         \u251c\u2500\u2192 Analyze (debugging)\n",
    "                         \u2502\n",
    "                         \u251c\u2500\u2192 Fix (bug fix, data fix)\n",
    "                         \u2502\n",
    "                         \u2514\u2500\u2192 Reprocess (move back to main queue)\n",
    "```\n",
    "\n",
    "**Implementation** (RabbitMQ):\n",
    "```python\n",
    "import pika\n",
    "import json\n",
    "import time\n",
    "\n",
    "# Connection\n",
    "connection = pika.BlockingConnection(pika.ConnectionParameters('localhost'))\n",
    "channel = connection.channel()\n",
    "\n",
    "# Declare main queue with DLQ arguments\n",
    "channel.queue_declare(\n",
    "    queue='orders',\n",
    "    durable=True,\n",
    "    arguments={\n",
    "        'x-dead-letter-exchange': 'dlx',  # DLQ exchange\n",
    "        'x-dead-letter-routing-key': 'orders_dlq'  # DLQ routing key\n",
    "    }\n",
    ")\n",
    "\n",
    "# Declare DLQ exchange\n",
    "channel.exchange_declare(exchange='dlx', exchange_type='direct')\n",
    "\n",
    "# Declare DLQ queue\n",
    "channel.queue_declare(queue='orders_dlq', durable=True)\n",
    "\n",
    "# Bind DLQ queue to DLQ exchange\n",
    "channel.queue_bind(exchange='dlx', queue='orders_dlq', routing_key='orders_dlq')\n",
    "\n",
    "# Producer: Publish messages to main queue\n",
    "def publish_order(order_data):\n",
    "    channel.basic_publish(\n",
    "        exchange='',\n",
    "        routing_key='orders',\n",
    "        body=json.dumps(order_data),\n",
    "        properties=pika.BasicProperties(\n",
    "            delivery_mode=2  # Make message persistent\n",
    "        )\n",
    "    )\n",
    "    print(f\"Published order: {order_data['order_id']}\")\n",
    "\n",
    "# Consumer: Process messages from main queue\n",
    "def consume_orders():\n",
    "    def callback(ch, method, properties, body):\n",
    "        order_data = json.loads(body)\n",
    "        print(f\"Processing order: {order_data['order_id']}\")\n",
    "        \n",
    "        try:\n",
    "            # Process order (may fail)\n",
    "            process_order(order_data)\n",
    "            \n",
    "            # Acknowledge message (removes from queue)\n",
    "            ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "            print(f\"Order processed successfully: {order_data['order_id']}\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Processing failed\n",
    "            print(f\"Error processing order: {order_data['order_id']}, Error: {e}\")\n",
    "            \n",
    "            # Reject message (requeue for retry)\n",
    "            # Or: Nack message with requeue=False (moves to DLQ after max retries)\n",
    "            ch.basic_nack(delivery_tag=method.delivery_tag, requeue=True)\n",
    "    \n",
    "    # Set prefetch (consume one message at a time)\n",
    "    channel.basic_qos(prefetch_count=1)\n",
    "    \n",
    "    # Consume messages\n",
    "    channel.basic_consume(queue='orders', on_message_callback=callback)\n",
    "    \n",
    "    print('Waiting for orders...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Consumer: Process messages from DLQ (for analysis)\n",
    "def consume_dead_letters():\n",
    "    def callback(ch, method, properties, body):\n",
    "        order_data = json.loads(body)\n",
    "        print(f\"Dead letter: {order_data['order_id']}\")\n",
    "        \n",
    "        # Analyze failure (logs, debugging)\n",
    "        analyze_failure(order_data, properties)\n",
    "        \n",
    "        # Optionally: Fix and reprocess (move back to main queue)\n",
    "        # reprocess_order(order_data)\n",
    "        \n",
    "        # Acknowledge DLQ message\n",
    "        ch.basic_ack(delivery_tag=method.delivery_tag)\n",
    "    \n",
    "    # Consume DLQ messages\n",
    "    channel.basic_consume(queue='orders_dlq', on_message_callback=callback)\n",
    "    \n",
    "    print('Waiting for dead letters...')\n",
    "    channel.start_consuming()\n",
    "\n",
    "# Result:\n",
    "# - Failed messages retried automatically (with requeue=True)\n",
    "# - After max retries, messages moved to DLQ (if configured)\n",
    "# - DLQ messages analyzed and potentially reprocessed\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Message Retry Strategies**\n",
    "\n",
    "**Strategy 1: Fixed Delay Retry**\n",
    "\n",
    "**Concept**: Retry after fixed delay between attempts.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "\n",
    "def process_with_fixed_delay_retry(message, max_retries=3, delay_seconds=5):\n",
    "    \"\"\"Process message with fixed delay retry\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Process message\n",
    "            result = process_message(message)\n",
    "            return result  # Success!\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                # Retry after delay\n",
    "                print(f\"Retrying in {delay_seconds} seconds...\")\n",
    "                time.sleep(delay_seconds)\n",
    "            else:\n",
    "                # Max retries exceeded\n",
    "                print(\"Max retries exceeded, giving up\")\n",
    "                raise e\n",
    "\n",
    "# Usage: Retry up to 3 times, 5 seconds between retries\n",
    "try:\n",
    "    process_with_fixed_delay_retry(message, max_retries=3, delay_seconds=5)\n",
    "except Exception as e:\n",
    "    # Move to DLQ\n",
    "    send_to_dlq(message)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 2: Exponential Backoff Retry**\n",
    "\n",
    "**Concept**: Retry with exponentially increasing delay between attempts.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "import random\n",
    "\n",
    "def process_with_exponential_backoff_retry(message, max_retries=5, base_delay=1, max_delay=60):\n",
    "    \"\"\"Process message with exponential backoff retry\"\"\"\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            # Process message\n",
    "            result = process_message(message)\n",
    "            return result  # Success!\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Attempt {attempt + 1} failed: {e}\")\n",
    "            \n",
    "            if attempt < max_retries - 1:\n",
    "                # Calculate delay with exponential backoff\n",
    "                delay = min(base_delay * (2 ** attempt) + random.uniform(0, 1), max_delay)\n",
    "                print(f\"Retrying in {delay:.2f} seconds...\")\n",
    "                time.sleep(delay)\n",
    "            else:\n",
    "                # Max retries exceeded\n",
    "                print(\"Max retries exceeded, giving up\")\n",
    "                raise e\n",
    "\n",
    "# Usage: Retry up to 5 times, exponential backoff (1s, 2s, 4s, 8s, 16s)\n",
    "try:\n",
    "    process_with_exponential_backoff_retry(message, max_retries=5, base_delay=1, max_delay=60)\n",
    "except Exception as e:\n",
    "    # Move to DLQ\n",
    "    send_to_dlq(message)\n",
    "\n",
    "# Benefits:\n",
    "# - First retry quick (1 second)\n",
    "# - Later retries slower (exponential backoff)\n",
    "# - Prevents overwhelming system with retries\n",
    "# - Random jitter prevents thundering herd\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**Strategy 3: Circuit Breaker Retry**\n",
    "\n",
    "**Concept**: Stop retrying after consecutive failures (circuit opens). Retry after cooldown period.\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "import time\n",
    "\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, failure_threshold=5, cooldown_seconds=60):\n",
    "        \"\"\"Initialize circuit breaker\n",
    "        \n",
    "        Args:\n",
    "            failure_threshold: Consecutive failures before opening circuit\n",
    "            cooldown_seconds: Cooldown period before retrying\n",
    "        \"\"\"\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.cooldown_seconds = cooldown_seconds\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.circuit_open = False\n",
    "    \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        \"\"\"Call function with circuit breaker\"\"\"\n",
    "        # Check if circuit is open\n",
    "        if self.circuit_open:\n",
    "            # Check if cooldown period elapsed\n",
    "            if time.time() - self.last_failure_time > self.cooldown_seconds:\n",
    "                # Cooldown elapsed, close circuit (allow one attempt)\n",
    "                print(\"Cooldown elapsed, closing circuit for one attempt\")\n",
    "                self.circuit_open = False\n",
    "                self.failure_count = 0\n",
    "            else:\n",
    "                # Circuit still open (in cooldown)\n",
    "                raise Exception(\"Circuit breaker open (in cooldown)\")\n",
    "        \n",
    "        try:\n",
    "            # Call function\n",
    "            result = func(*args, **kwargs)\n",
    "            \n",
    "            # Success: Reset failure count, close circuit\n",
    "            self.failure_count = 0\n",
    "            self.circuit_open = False\n",
    "            return result\n",
    "            \n",
    "        except Exception as e:\n",
    "            # Failure: Increment failure count\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = time.time()\n",
    "            \n",
    "            # Check if threshold exceeded\n",
    "            if self.failure_count >= self.failure_threshold:\n",
    "                # Open circuit (stop retrying)\n",
    "                self.circuit_open = True\n",
    "                print(f\"Circuit breaker opened after {self.failure_threshold} failures\")\n",
    "            \n",
    "            raise e\n",
    "\n",
    "# Usage: Process message with circuit breaker\n",
    "circuit_breaker = CircuitBreaker(failure_threshold=5, cooldown_seconds=60)\n",
    "\n",
    "def process_with_circuit_breaker(message):\n",
    "    \"\"\"Process message with circuit breaker retry\"\"\"\n",
    "    try:\n",
    "        result = circuit_breaker.call(process_message, message)\n",
    "        return result\n",
    "    except Exception as e:\n",
    "        # Circuit breaker open (or message processing failed)\n",
    "        print(f\"Error processing message: {e}\")\n",
    "        raise e\n",
    "\n",
    "# Benefits:\n",
    "# - Prevents cascading failures (stops retrying after threshold)\n",
    "# - System recovers automatically (circuit closes after cooldown)\n",
    "# - Reduces load on failing system (circuit open = no retries)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.7 Message Queue Comparison**\n",
    "\n",
    "**Comparison of Popular Message Queue Systems**:\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 Feature                \u2502 RabbitMQ     \u2502 Kafka        \u2502 AWS SQS      \u2502 Google Pub  \u2502\n",
    "\u2502                       \u2502              \u2502              \u2502              \u2502 /Sub        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Type                  \u2502 Message      \u2502 Distributed  \u2502 Message      \u2502 Messaging   \u2502\n",
    "\u2502                       \u2502 Queue        \u2502 Streaming    \u2502 Queue        \u2502 Service     \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Architecture          \u2502 Broker-based \u2502 Distributed  \u2502 Fully        \u2502 Fully       \u2502\n",
    "\u2502                       \u2502              \u2502 Log          \u2502 Managed      \u2502 Managed     \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Message Ordering      \u2502 Per-queue    \u2502 Per-         \u2502 Per-queue    \u2502 Per-topic   \u2502\n",
    "\u2502                       \u2502              \u2502 partition    \u2502              \u2502             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Throughput            \u2502 High         \u2502 Very High    \u2502 High         \u2502 High        \u2502\n",
    "\u2502                       \u2502 (100K msg/s) \u2502 (Millions/   \u2502 (Unlimited)  \u2502 (1M msg/s)  \u2502\n",
    "\u2502                       \u2502              \u2502 s)           \u2502              \u2502             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Latency               \u2502 Low (ms)     \u2502 Low (ms)     \u2502 Low (ms)     \u2502 Low (ms)    \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Persistence           \u2502 Optional     \u2502 Built-in     \u2502 Optional     \u2502 Built-in    \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Message Retention      \u2502 Until        \u2502 Configurable \u2502 4 days       \u2502 7 days      \u2502\n",
    "\u2502                       \u2502 consumed     \u2502 (hours-      \u2502 (extendable) \u2502 (extendable)\u2502\n",
    "\u2502                       \u2502              \u2502 days)        \u2502              \u2502             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Scaling               \u2502 Vertical     \u2502 Horizontal   \u2502 Horizontal   \u2502 Horizontal  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Consumer Groups       \u2502 Manual       \u2502 Native       \u2502 Native       \u2502 Native      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Backpressure          \u2502 TTL-based    \u2502 Consumer-    \u2502 Manual       \u2502 Consumer-   \u2502\n",
    "\u2502                       \u2502              \u2502 controlled   \u2502              \u2502 controlled  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Routing               \u2502 Rich (4      \u2502 Limited      \u2502 Manual       \u2502 Rich (topic \u2502\n",
    "\u2502                       \u2502 exchange     \u2502 (topic-      \u2502 (filter)     \u2502 based)     \u2502\n",
    "\u2502                       \u2502 types)       \u2502 based)       \u2502              \u2502             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Message Replay        \u2502 No           \u2502 Yes          \u2502 No           \u2502 Yes         \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Dead Letter Queues    \u2502 Native       \u2502 Manual       \u2502 Native       \u2502 Native      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Transaction Support   \u2502 Native       \u2502 Native       \u2502 No           \u2502 Native      \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Management            \u2502 Moderate     \u2502 High         \u2502 Low (managed)\u2502 Low         \u2502\n",
    "\u2502                       \u2502              \u2502              \u2502              \u2502 (managed)   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Cost                  \u2502 Open source  \u2502 Open source  \u2502 Pay per      \u2502 Pay per     \u2502\n",
    "\u2502                       \u2502 (self-host) \u2502 (self-host) \u2502 request      \u2502 usage       \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 Use Cases             \u2502 Task queues, \u2502 Stream       \u2502 Decoupled    \u2502 Event-      \u2502\n",
    "\u2502                       \u2502 routing,     \u2502 processing,  \u2502 services,    \u2502 driven,     \u2502\n",
    "\u2502                       \u2502 pub/sub      \u2502 event        \u2502 task queues  \u2502 analytics   \u2502\n",
    "\u2502                       \u2502              \u2502 sourcing     \u2502              \u2502             \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **5.8 Key Takeaways**\n",
    "\n",
    "1. **Asynchronous communication decouples services**: Message queues enable loose coupling, fault tolerance, and independent scaling.\n",
    "\n",
    "2. **Choose the right pattern**: Point-to-point for task distribution, publish-subscribe for notifications, topic for selective routing.\n",
    "\n",
    "3. **Kafka for streaming, RabbitMQ for messaging**: Kafka excels at high-throughput streaming and event sourcing. RabbitMQ excels at complex routing and traditional messaging.\n",
    "\n",
    "4. **Event sourcing and CQRS enable auditability**: Event sourcing stores complete event history. CQRS separates read and write models for performance.\n",
    "\n",
    "5. **Handle backpressure proactively**: Implement throttling, bounded queues, consumer scaling, and reactive backpressure to prevent system overload.\n",
    "\n",
    "6. **Implement DLQs and retry strategies**: Dead Letter Queues store failed messages for analysis. Retry strategies (fixed delay, exponential backoff, circuit breaker) determine how and when to retry.\n",
    "\n",
    "7. **Monitor message queue performance**: Track metrics (message rate, queue size, consumer lag, error rates) to ensure system health.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored message queues and event-driven architecture\u2014essential patterns for building scalable, fault-tolerant distributed systems. We covered synchronous vs. asynchronous communication, understanding when each is appropriate.\n",
    "\n",
    "We examined message queue patterns (point-to-point, publish-subscribe, topic), implementing each with RabbitMQ. We explored Apache Kafka in detail, understanding topics, partitions, consumer groups, and message semantics (at-most-once, at-least-once, exactly-once).\n",
    "\n",
    "We introduced Event Sourcing and CQRS\u2014architectural patterns that leverage events for auditability and performance. We covered backpressure handling and rate limiting, understanding how to prevent system overload when producers outpace consumers.\n",
    "\n",
    "Finally, we examined Dead Letter Queues and message retry strategies, understanding how to handle failed messages gracefully.\n",
    "\n",
    "**Coming up next**: In Chapter 6, we'll explore Load Balancing & Traffic Management, covering Layer 4 vs. Layer 7 load balancing, load balancing algorithms, health checks, circuit breakers, global load balancing, API Gateway patterns, and service mesh introduction.\n",
    "\n",
    "---\n",
    "\n",
    "**Exercises**:\n",
    "\n",
    "1. **Communication Pattern Selection**: For each scenario, would you use synchronous or asynchronous communication? Why?\n",
    "   - A user placing an order (immediate confirmation required)\n",
    "   - Sending order confirmation email (background task)\n",
    "   - Updating inventory levels (must be consistent)\n",
    "   - Generating monthly sales report (background analytics)\n",
    "   - Processing payment (must be consistent and confirmed)\n",
    "\n",
    "2. **Message Queue Pattern Design**: You're building a microservices architecture for an e-commerce platform. Design the event flow for:\n",
    "   - Order creation (notifications to inventory, payment, email services)\n",
    "   - Payment processing (notifications to order, inventory, email services)\n",
    "   - Order shipping (notifications to order, email services)\n",
    "   Which message queue pattern would you use for each? How would you structure the topics/exchanges?\n",
    "\n",
    "3. **Kafka Consumer Group Scaling**: You have a Kafka topic with 6 partitions processing 100,000 messages/second. Each consumer can process 10,000 messages/second. How many consumers do you need in the consumer group? What happens if you add more consumers than partitions?\n",
    "\n",
    "4. **Event Sourcing Implementation**: You're building a banking application using Event Sourcing. Design the events for:\n",
    "   - Account creation\n",
    "   - Deposits\n",
    "   - Withdrawals\n",
    "   - Transfers (between accounts)\n",
    "   How would you reconstruct the account balance from events?\n",
    "\n",
    "5. **Backpressure Strategy Selection**: You're building a real-time analytics pipeline processing sensor data from 1 million devices (10 messages/second per device = 10 million messages/second). Your consumers can only process 5 million messages/second. Which backpressure strategy would you use? How would you design the system to handle this load?\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='4. caching.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='6. load_balancing_and_traffic_management.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}