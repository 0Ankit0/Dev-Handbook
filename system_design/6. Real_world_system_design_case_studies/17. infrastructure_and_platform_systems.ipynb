{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4f7a08f",
   "metadata": {},
   "source": [
    "# **Chapter 17: Infrastructure & Platform Systems**\n",
    "\n",
    "While user-facing applications deliver direct value to customers, infrastructure systems are the invisible engines that power them. These systems handle data storage, caching, rate limiting, and background processing at massive scale. They require extreme reliability—if your cache goes down, every service that depends on it fails.\n",
    "\n",
    "This chapter covers seven foundational systems that appear in almost every modern architecture. We progress from data collection (web crawler) to storage (key-value store, cache) to traffic management (rate limiter, ID generation) to search (autocomplete) and communication (notifications).\n",
    "\n",
    "---\n",
    "\n",
    "## **17.1 Design a Web Crawler**\n",
    "\n",
    "A web crawler systematically browses the internet to index web pages for search engines, archive content, or extract data. Googlebot, Bingbot, and site-specific crawlers (like those monitoring prices or news) all follow similar patterns.\n",
    "\n",
    "### **Step 1: Scope (Requirements)**\n",
    "\n",
    "**Functional Requirements**:\n",
    "1. **Crawl the web**: Start from a seed URL, follow links, download pages\n",
    "2. **Parse content**: Extract links, index text, handle different formats (HTML, PDF, images)\n",
    "3. **Politeness**: Respect robots.txt and crawl rate limits (don't overwhelm websites)\n",
    "4. **Deduplication**: Don't crawl the same page twice\n",
    "5. **Storage**: Store crawled content for processing\n",
    "\n",
    "**Non-Functional Requirements**:\n",
    "1. **Scalability**: Crawl 1 billion pages per day\n",
    "2. **Politeness**: Wait at least 1 second between requests to same domain\n",
    "3. **Extensibility**: Easy to add new content types\n",
    "4. **Fault tolerance**: Handle network failures, server errors gracefully\n",
    "\n",
    "**Constraints**:\n",
    "- Average page size: 500KB HTML\n",
    "- Average links per page: 100\n",
    "- Must respect `robots.txt` rules\n",
    "- Must handle JavaScript-rendered content (modern SPAs)\n",
    "\n",
    "### **Step 2: Sketch (Back-of-the-Envelope)**\n",
    "\n",
    "**Traffic**:\n",
    "```\n",
    "1 billion pages/day = 11,574 pages/second (average)\n",
    "Peak: 100,000 pages/second\n",
    "\n",
    "Storage:\n",
    "1B pages × 500KB = 500 TB/day\n",
    "Yearly: 182 PB (raw, before compression)\n",
    "\n",
    "Network:\n",
    "500 TB/day = 46 Gbps sustained download\n",
    "```\n",
    "\n",
    "**URL Frontier Size**:\n",
    "```\n",
    "If each page has 100 links, but 70% are duplicates:\n",
    "New URLs discovered per page: 30\n",
    "URL queue growth: 30B URLs/day\n",
    "\n",
    "Storage for URL queue:\n",
    "30B URLs × 100 chars × 2 bytes = 6 TB of URLs to process\n",
    "```\n",
    "\n",
    "### **Step 3: Solidify (Data Model)**\n",
    "\n",
    "**URL Frontier** (Message Queue - Kafka/RabbitMQ):\n",
    "```\n",
    "Topic: url_queue\n",
    "Message: {\n",
    "    \"url\": \"https://example.com/page\",\n",
    "    \"priority\": 1,           // 0 = high (news sites), 5 = low (archives)\n",
    "    \"depth\": 2,              // BFS depth from seed\n",
    "    \"discovered_at\": \"2024-01-15T10:30:00Z\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Visited URLs** (Bloom Filter + Database):\n",
    "- **Bloom Filter**: In-memory check (100GB RAM for 10B URLs, 1% false positive)\n",
    "- **Database**: Cassandra for permanent storage of visited URLs with timestamp\n",
    "\n",
    "```sql\n",
    "CREATE TABLE crawled_pages (\n",
    "    url_hash VARCHAR(64) PRIMARY KEY,  -- SHA-256 of URL\n",
    "    url TEXT,\n",
    "    crawled_at TIMESTAMP,\n",
    "    content_hash VARCHAR(64),  -- To detect changes\n",
    "    status_code INT,\n",
    "    page_size INT,\n",
    "    server_domain VARCHAR(255)\n",
    ");\n",
    "```\n",
    "\n",
    "**Content Storage** (Object Storage):\n",
    "```\n",
    "s3://crawler-bucket/raw/{domain}/{year}/{month}/{day}/{url_hash}.html.gz\n",
    "```\n",
    "\n",
    "### **Step 4: Scale (Architecture)**\n",
    "\n",
    "**High-Level Design**:\n",
    "```\n",
    "┌──────────────┐\n",
    "│   Seed URLs  │ (Initial list: top 1000 websites)\n",
    "└──────┬───────┘\n",
    "       │\n",
    "       ▼\n",
    "┌──────────────┐\n",
    "│   URL        │\n",
    "│   Frontier   │ (Priority Queue - Kafka)\n",
    "└──────┬───────┘\n",
    "       │\n",
    "       ▼\n",
    "┌──────────────┐      ┌──────────────┐\n",
    "│   Crawler    │─────>│   Politeness │\n",
    "│   Workers    │      │   Checker    │\n",
    "│   (1000s)    │<─────│   (Redis)    │\n",
    "└──────┬───────┘      └──────────────┘\n",
    "       │\n",
    "       ├─► Download page\n",
    "       ├─► Store in S3\n",
    "       ├─► Extract links\n",
    "       └─► Add new URLs to Frontier\n",
    "```\n",
    "\n",
    "**The Politeness Constraint (Critical Design Challenge)**:\n",
    "\n",
    "We cannot hit the same server with 1000 requests simultaneously. We must rate-limit per domain.\n",
    "\n",
    "**Solution - Token Bucket per Domain**:\n",
    "```\n",
    "Redis key: \"domain:example.com:tokens\"\n",
    "Value: Integer (remaining tokens)\n",
    "Refill rate: 1 token/second (configurable per robots.txt)\n",
    "Capacity: 5 tokens (burst allowance)\n",
    "\n",
    "Before crawling example.com/page2:\n",
    "    INCR domain:example.com:tokens\n",
    "    If result > 0: Crawl allowed\n",
    "    Else: Re-queue URL with delay\n",
    "```\n",
    "\n",
    "**Distributed Crawler Coordination**:\n",
    "```\n",
    "Hash ring of crawler nodes:\n",
    "    Domain A → Node 1 (only Node 1 crawls example.com)\n",
    "    Domain B → Node 2\n",
    "    Domain C → Node 3\n",
    "\n",
    "This ensures only one crawler hits a specific domain at a time,\n",
    "naturally enforcing politeness without complex coordination.\n",
    "```\n",
    "\n",
    "**Content Processing Pipeline**:\n",
    "```\n",
    "Raw HTML downloaded\n",
    "    │\n",
    "    ▼\n",
    "┌──────────────┐\n",
    "│   Parser     │──┐\n",
    "│   Service    │  │\n",
    "└──────┬───────┘  │\n",
    "       │          │\n",
    "       ▼          ▼\n",
    "┌──────────┐  ┌──────────┐\n",
    "│ Link     │  │ Content  │\n",
    "│ Extractor│  │ Indexer  │\n",
    "└────┬─────┘  └────┬─────┘\n",
    "     │             │\n",
    "     ▼             ▼\n",
    "┌──────────┐  ┌──────────┐\n",
    "│ URL      │  │ Search   │\n",
    "│ Frontier │  │ Index    │\n",
    "│ (New     │  │ (Elastic)│\n",
    "│  URLs)   │  │          │\n",
    "└──────────┘  └──────────┘\n",
    "```\n",
    "\n",
    "**Handling JavaScript (Modern Web)**:\n",
    "Many sites (React, Vue, Angular) require JavaScript execution to render content.\n",
    "\n",
    "**Solution**:\n",
    "- Use headless Chrome (Puppeteer/Playwright) for JavaScript-heavy sites\n",
    "- Trade-off: 10x slower, 10x more CPU/memory\n",
    "- Detection: Try static crawl first; if no content detected, escalate to headless\n",
    "\n",
    "**Deduplication Strategy**:\n",
    "1. **URL-level**: Bloom filter prevents crawling same URL twice\n",
    "2. **Content-level**: SHA-256 hash of content; if same as previous crawl, skip re-indexing\n",
    "3. **Near-duplicate**: SimHash to detect pages with minor changes (ads, timestamps)\n",
    "\n",
    "**Fault Tolerance**:\n",
    "- **Retry with backoff**: 5xx errors → retry in 1 min, 5 min, 25 min (exponential)\n",
    "- **Dead Letter Queue**: After 3 failures, move to DLQ for manual inspection\n",
    "- **Circuit Breaker**: If example.com is down, stop trying for 10 minutes\n",
    "\n",
    "### **Deep Dive: Distributed politeness**\n",
    "\n",
    "**Problem**: With 1000 crawler nodes, how do we ensure we don't exceed 1 req/sec per domain across all nodes?\n",
    "\n",
    "**Solution 1: Domain Sharding**\n",
    "```\n",
    "Consistent hash of domain name → determines which crawler node handles it\n",
    "example.com always goes to Node 5\n",
    "Node 5 maintains local rate limiter for its assigned domains\n",
    "```\n",
    "\n",
    "**Solution 2: Centralized Redis (Simpler, but bottleneck)**\n",
    "```\n",
    "Each crawler asks Redis: \"Can I crawl example.com now?\"\n",
    "Redis manages token bucket per domain\n",
    "Trade-off: Network round-trip for every check, Redis hotspot\n",
    "```\n",
    "\n",
    "**Hybrid Approach**:\n",
    "- Assign domains to crawler nodes (sharding)\n",
    "- If Node 5 fails, redistribute to other nodes (consistent hashing)\n",
    "- Each node maintains local token buckets for its domains\n",
    "\n",
    "### **Deep Dive: Freshness vs. Coverage**\n",
    "\n",
    "**Freshness**: How often to re-crawl?\n",
    "- News sites (cnn.com): Every 5 minutes\n",
    "- Blogs: Every day\n",
    "- Static documentation: Every month\n",
    "\n",
    "**Adaptive Re-crawling**:\n",
    "```\n",
    "If page changes frequently (detected by content hash comparison):\n",
    "    Decrease interval (crawl more often)\n",
    "If page rarely changes:\n",
    "    Increase interval up to max (30 days)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **17.2 Design a Distributed Key-Value Store (DynamoDB-style)**\n",
    "\n",
    "Amazon DynamoDB revolutionized NoSQL databases by offering consistent single-digit millisecond latency at any scale. Let's design a similar system.\n",
    "\n",
    "### **Step 1: Scope**\n",
    "\n",
    "**Functional Requirements**:\n",
    "1. **Put/Get/Delete**: Basic CRUD operations by key\n",
    "2. **Range queries**: Query by key prefix (if using composite keys)\n",
    "3. **TTL**: Automatic expiration of keys\n",
    "\n",
    "**Non-Functional Requirements**:\n",
    "1. **High availability**: 99.999% uptime (5 nines)\n",
    "2. **Partition tolerance**: System works despite network partitions\n",
    "3. **Scalability**: Unlimited storage and throughput\n",
    "4. **Low latency**: P99 < 10ms for reads/writes\n",
    "\n",
    "**CAP Trade-off**: Choose AP (Availability + Partition tolerance) with eventual consistency, tunable to strong consistency per request.\n",
    "\n",
    "### **Step 2: Sketch**\n",
    "\n",
    "**Scale Target**:\n",
    "```\n",
    "10 trillion key-value pairs\n",
    "Average value size: 10KB\n",
    "Total storage: 100 PB\n",
    "\n",
    "Request rate: 100 million operations/second\n",
    "Read:Write ratio: 80:20\n",
    "```\n",
    "\n",
    "**Partition Calculation**:\n",
    "```\n",
    "Each node handles 10,000 QPS comfortably\n",
    "100M QPS / 10K = 10,000 nodes needed\n",
    "```\n",
    "\n",
    "### **Step 3: Data Model**\n",
    "\n",
    "**Logical Model**:\n",
    "```\n",
    "Key: \"user:123:profile\"\n",
    "Value: {name: \"Alice\", age: 30, city: \"NYC\"}\n",
    "Metadata: Version vector, timestamp, TTL\n",
    "```\n",
    "\n",
    "**Physical Storage**:\n",
    "- **MemTable**: In-memory balanced tree (skip list or red-black tree) for recent writes\n",
    "- **SSTable**: Immutable sorted string tables on disk\n",
    "- **Commit Log**: Append-only log for durability (WAL - Write Ahead Log)\n",
    "\n",
    "### **Step 4: Scale (Architecture)**\n",
    "\n",
    "**Consistent Hashing Ring**:\n",
    "```\n",
    "┌─────────────────────────────────────────┐\n",
    "│           Consistent Hash Ring          │\n",
    "│                                         │\n",
    "│  ┌─────┐  ┌─────┐  ┌─────┐  ┌─────┐   │\n",
    "│  │NodeA│  │NodeB│  │NodeC│  │NodeD│   │\n",
    "│  │ 0-25│  │25-50│  │50-75│  │75-100│  │\n",
    "│  └──┬──┘  └──┬──┘  └──┬──┘  └──┬──┘   │\n",
    "│     └─────────┴────────┴────────┘       │\n",
    "│           Replication                   │\n",
    "└─────────────────────────────────────────┘\n",
    "\n",
    "Key \"user:123\" hashes to 42 → stored on NodeB\n",
    "Also replicated to NodeC and NodeD (next 2 nodes in ring)\n",
    "```\n",
    "\n",
    "**Write Path**:\n",
    "```\n",
    "Client → Load Balancer → Coordinator Node\n",
    "    │\n",
    "    ├─► Hash key to determine partition (Node B, C, D)\n",
    "    │\n",
    "    ├─► Write to Commit Log (WAL) on all 3 nodes (blocking)\n",
    "    │\n",
    "    ├─► Update MemTable (in-memory)\n",
    "    │\n",
    "    └─► Acknowledge write to client\n",
    "        (Data not yet on disk, but durable in logs)\n",
    "\n",
    "Async: MemTable flushed to SSTable when threshold reached\n",
    "```\n",
    "\n",
    "**Read Path**:\n",
    "```\n",
    "Client → Coordinator\n",
    "    │\n",
    "    ├─► Query all 3 replicas in parallel\n",
    "    │\n",
    "    ├─► If versions differ:\n",
    "    │     - Return latest version to client\n",
    "    │     - Initiate read repair (update stale replicas)\n",
    "    │\n",
    "    └─► Return result\n",
    "```\n",
    "\n",
    "**Storage Engine Details**:\n",
    "\n",
    "**MemTable** (In-Memory):\n",
    "```\n",
    "Data structure: Concurrent skip list\n",
    "Insertion: O(log n)\n",
    "Lookup: O(log n)\n",
    "Flush trigger: 128MB\n",
    "When flushed: Written to SSTable file, MemTable cleared\n",
    "```\n",
    "\n",
    "**SSTable** (On-Disk):\n",
    "```\n",
    "Immutable, sorted file format:\n",
    "[data block 1][data block 2]...[index block][footer]\n",
    "\n",
    "Index block: Sparse index of keys to file offsets\n",
    "Compression: Snappy or LZ4 per block\n",
    "Bloom filter: To check if key might exist in this SSTable\n",
    "```\n",
    "\n",
    "**Compaction** (Background Process):\n",
    "```\n",
    "As writes continue, SSTables accumulate:\n",
    "sstable_1: [a-c], sstable_2: [d-f], sstable_3: [a-b] (newer)\n",
    "\n",
    "Problem: Multiple versions of same key across files\n",
    "Solution: Periodically merge SSTables (like merge sort)\n",
    "         Keep only latest version of each key\n",
    "         Remove deleted keys (tombstones) after retention period\n",
    "```\n",
    "\n",
    "### **Deep Dive: Conflict Resolution (Vector Clocks)**\n",
    "\n",
    "When a network partition occurs, different replicas may receive different updates.\n",
    "\n",
    "**Scenario**:\n",
    "```\n",
    "Partition occurs: Node A isolated from Node B\n",
    "\n",
    "Client 1 writes to Node A: {name: \"Alice\"}\n",
    "Client 2 writes to Node B: {name: \"Bob\"}\n",
    "\n",
    "Network heals. Node A and B reconcile.\n",
    "\n",
    "Options:\n",
    "1. Last-Write-Wins (LWW): Use timestamp. Risk: Clock skew causes data loss\n",
    "2. Vector Clocks: Track which node wrote which version\n",
    "   Result: Both versions kept, client must resolve (siblings)\n",
    "```\n",
    "\n",
    "**Vector Clock Example**:\n",
    "```\n",
    "Initial: {[A:1], value: \"Alice\"}\n",
    "\n",
    "Update at A: {[A:2], value: \"Alicia\"}\n",
    "Update at B (during partition): {[A:1, B:1], value: \"Bob\"}\n",
    "\n",
    "On reconciliation:\n",
    "- [A:2] and [A:1,B:1] are concurrent (neither descends from other)\n",
    "- Keep both: [\"Alicia\", \"Bob\"] → return to client for resolution\n",
    "```\n",
    "\n",
    "### **Deep Dive: Gossip Protocol**\n",
    "\n",
    "How do nodes know about each other and detect failures?\n",
    "\n",
    "**Gossip Protocol**:\n",
    "```\n",
    "Every 1 second, each node picks random peer and exchanges state:\n",
    "    - \"I know about keys 0-1000\"\n",
    "    - \"Node C failed 30 seconds ago\"\n",
    "    - \"New node D joined at position 75\"\n",
    "\n",
    "Properties:\n",
    "- Information spreads like epidemic (exponential)\n",
    "- Eventually consistent membership\n",
    "- Scales to thousands of nodes (O(log n) rounds to propagate)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **17.3 Design a Distributed Cache (Redis-style)**\n",
    "\n",
    "Caching reduces database load and improves latency by storing hot data in memory. A distributed cache scales this across multiple servers.\n",
    "\n",
    "### **Step 1: Scope**\n",
    "\n",
    "**Requirements**:\n",
    "1. **Key-Value operations**: Get, Set, Delete, Expire (TTL)\n",
    "2. **Data structures**: Strings, Lists, Sets, Hashes, Sorted Sets\n",
    "3. **Pub/Sub**: Real-time messaging between clients\n",
    "4. **Persistence**: Optional snapshotting to disk\n",
    "5. **High availability**: Replication and automatic failover\n",
    "\n",
    "### **Step 2: Sketch**\n",
    "\n",
    "**Scale**:\n",
    "```\n",
    "100 million keys\n",
    "Average value size: 5KB\n",
    "Total data: 500 GB RAM per node\n",
    "Cluster size: 100 nodes = 50 TB cache capacity\n",
    "\n",
    "Throughput: 10 million ops/sec cluster-wide\n",
    "```\n",
    "\n",
    "### **Step 3: Architecture**\n",
    "\n",
    "**Sharding Strategy** (Redis Cluster):\n",
    "```\n",
    "Hash Slot Algorithm:\n",
    "- 16,384 hash slots total\n",
    "- Key hashed to slot: slot = CRC16(key) % 16384\n",
    "- Slots assigned to nodes: Node A (0-5500), Node B (5501-11000), etc.\n",
    "\n",
    "Client caches slot-to-node mapping for efficiency\n",
    "```\n",
    "\n",
    "**Replication** (Master-Replica):\n",
    "```\n",
    "Each master has 1-2 replicas\n",
    "Write to master, replicate async to replicas\n",
    "If master fails: Replica promoted via consensus (Raft)\n",
    "```\n",
    "\n",
    "**Eviction Policies** (When memory full):\n",
    "```\n",
    "LRU (Least Recently Used): Remove least accessed keys\n",
    "LFU (Least Frequently Used): Remove rarely accessed keys\n",
    "TTL: Remove expired keys first\n",
    "Random: Fast but suboptimal\n",
    "No eviction: Return errors on write\n",
    "```\n",
    "\n",
    "### **Deep Dive: Cache Consistency**\n",
    "\n",
    "**Cache-Aside Pattern** (Most Common):\n",
    "```\n",
    "Read:\n",
    "    1. Check cache\n",
    "    2. If miss: Read DB, write to cache, return value\n",
    "\n",
    "Write:\n",
    "    1. Update DB\n",
    "    2. Delete cache (not update! Why? Race conditions)\n",
    "       or Use distributed transaction ( Saga pattern )\n",
    "```\n",
    "\n",
    "**Thundering Herd Prevention**:\n",
    "```\n",
    "Scenario: Cache expires, 1000 clients request same key simultaneously\n",
    "Result: 1000 DB queries (database dies)\n",
    "\n",
    "Solutions:\n",
    "1. Lease mechanism: Only one client allowed to regenerate cache\n",
    "2. Probabilistic early expiration: Refresh at 90% of TTL\n",
    "3. Lock + Retry: First client acquires lock, others wait\n",
    "```\n",
    "\n",
    "**Code Example** (Cache-Aside with Lease):\n",
    "```python\n",
    "def get_with_lease(key):\n",
    "    value, lease_token = cache.get(key)\n",
    "    \n",
    "    if value is None:\n",
    "        if cache.try_acquire_lease(key, ttl=10):\n",
    "            # Only this process refreshes\n",
    "            value = db.query(key)\n",
    "            cache.set(key, value, lease_token=lease_token)\n",
    "        else:\n",
    "            # Wait and retry\n",
    "            time.sleep(0.1)\n",
    "            return get_with_lease(key)\n",
    "    \n",
    "    return value\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **17.4 Design a Rate Limiter**\n",
    "\n",
    "Rate limiting prevents abuse by restricting how many requests a client can make in a time window (e.g., 100 requests/minute).\n",
    "\n",
    "### **Step 1: Scope**\n",
    "\n",
    "**Requirements**:\n",
    "1. **Limit by**: IP address, User ID, API key, or combination\n",
    "2. **Algorithms**: Token bucket, Sliding window, Fixed window\n",
    "3. **Response**: HTTP 429 (Too Many Requests) when limit exceeded\n",
    "4. **Headers**: Return remaining quota (X-RateLimit-Remaining)\n",
    "\n",
    "### **Step 2: Algorithms**\n",
    "\n",
    "**Token Bucket**:\n",
    "```\n",
    "Bucket capacity: 10 tokens\n",
    "Refill rate: 1 token/second\n",
    "\n",
    "Request arrives:\n",
    "    If tokens > 0:\n",
    "        tokens -= 1\n",
    "        Allow request\n",
    "    Else:\n",
    "        Reject request\n",
    "\n",
    "Pros: Allows bursts up to bucket size\n",
    "Cons: Requires storing token count per client\n",
    "```\n",
    "\n",
    "**Sliding Window Log**:\n",
    "```\n",
    "Store timestamp of each request in sorted set\n",
    "When new request arrives:\n",
    "    Remove entries older than window (e.g., 1 minute ago)\n",
    "    Count remaining entries\n",
    "    If count < limit: Allow and add timestamp\n",
    "    Else: Reject\n",
    "\n",
    "Pros: Precise\n",
    "Cons: O(log n) memory per request, high storage cost\n",
    "```\n",
    "\n",
    "**Sliding Window Counter** (Hybrid):\n",
    "```\n",
    "Divide time into buckets (e.g., 1 minute buckets)\n",
    "Store count per bucket in Redis Hash\n",
    "\n",
    "Current window = (Current bucket × weight) + Previous bucket × (1 - weight)\n",
    "Weight = time elapsed in current bucket / bucket size\n",
    "\n",
    "Approximate but memory efficient\n",
    "```\n",
    "\n",
    "### **Step 3: Architecture**\n",
    "\n",
    "**Distributed Rate Limiter**:\n",
    "```\n",
    "API Gateway / Load Balancer\n",
    "    │\n",
    "    ▼\n",
    "┌──────────────┐\n",
    "│   Rate       │────> Redis (Centralized counter)\n",
    "│   Limiter    │      Key: \"ratelimit:{user_id}:{minute}\"\n",
    "│   Service    │      Value: Counter\n",
    "└──────┬───────┘      TTL: 2 minutes\n",
    "       │\n",
    "       ▼\n",
    "   Application\n",
    "```\n",
    "\n",
    "**Edge Rate Limiting** (CloudFlare style):\n",
    "```\n",
    "Rate limit at CDN edge (PoP) to block attacks before they reach origin\n",
    "Synchronize counters between edge nodes periodically (eventual consistency)\n",
    "```\n",
    "\n",
    "### **Deep Dive: Burst Handling**\n",
    "\n",
    "**Token Bucket vs. Leaky Bucket**:\n",
    "```\n",
    "Token Bucket (Traffic Shaping):\n",
    "    - Bursts allowed up to bucket size\n",
    "    - Then limited to refill rate\n",
    "    \n",
    "Leaky Bucket (Traffic Policing):\n",
    "    - Requests enter queue at any rate\n",
    "    - Processed at constant rate (leak rate)\n",
    "    - Queue full → drops requests\n",
    "    - Smooths traffic but adds latency\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **17.5 Design a Unique ID Generator**\n",
    "\n",
    "Distributed systems need unique IDs without central coordination (to avoid single point of failure). Twitter's Snowflake is the industry standard.\n",
    "\n",
    "### **Requirements**\n",
    "- 64-bit integers (fits in database BIGINT)\n",
    "- Roughly sortable by time (newer IDs > older IDs)\n",
    "- Unique across distributed nodes without coordination\n",
    "- High throughput (10,000+ IDs/second per node)\n",
    "\n",
    "### **Snowflake Structure** (64 bits):\n",
    "```\n",
    "0 | 0000000000 0000000000 0000000000 0000000000 0 | 00000 | 00000 | 000000000000\n",
    "^ |                    41 bits                     | 5bits | 5bits |   12 bits\n",
    "| |                    (Timestamp)                 |(DC ID)|(Node) |  Sequence\n",
    "Sign\n",
    "```\n",
    "\n",
    "**Breakdown**:\n",
    "1. **1 bit**: Sign (always 0 for positive)\n",
    "2. **41 bits**: Milliseconds since epoch (69 years)\n",
    "3. **5 bits**: Data center ID (32 data centers)\n",
    "4. **5 bits**: Machine ID (32 machines per DC)\n",
    "5. **12 bits**: Sequence number (4096 IDs per millisecond per machine)\n",
    "\n",
    "**Total capacity**: 32 DCs × 32 machines × 4096 IDs/ms = 4 million IDs/second per DC\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "class Snowflake:\n",
    "    def __init__(self, datacenter_id, machine_id):\n",
    "        self.datacenter_id = datacenter_id\n",
    "        self.machine_id = machine_id\n",
    "        self.sequence = 0\n",
    "        self.last_timestamp = -1\n",
    "        \n",
    "    def generate(self):\n",
    "        timestamp = self.current_time_millis()\n",
    "        \n",
    "        if timestamp < self.last_timestamp:\n",
    "            raise Exception(\"Clock moved backwards!\")\n",
    "            \n",
    "        if timestamp == self.last_timestamp:\n",
    "            self.sequence = (self.sequence + 1) & 0xFFF  # 12 bits\n",
    "            if self.sequence == 0:  # Overflow, wait next millisecond\n",
    "                timestamp = self.wait_next_millis()\n",
    "        else:\n",
    "            self.sequence = 0\n",
    "            \n",
    "        self.last_timestamp = timestamp\n",
    "        \n",
    "        # Build ID\n",
    "        id = ((timestamp - EPOCH) << 22) | \\\n",
    "             (self.datacenter_id << 17) | \\\n",
    "             (self.machine_id << 12) | \\\n",
    "             self.sequence\n",
    "             \n",
    "        return id\n",
    "```\n",
    "\n",
    "**Alternative: UUID v4**\n",
    "- 128 bits (too large for DB indexes)\n",
    "- Not sortable (random)\n",
    "- No coordination needed (random)\n",
    "- Use when you don't need sortability\n",
    "\n",
    "---\n",
    "\n",
    "## **17.6 Design a Search Autocomplete System (Typeahead)**\n",
    "\n",
    "Autocomplete suggests completions as users type (e.g., \"sys\" → \"system design\", \"system architecture\").\n",
    "\n",
    "### **Step 1: Scope**\n",
    "\n",
    "**Requirements**:\n",
    "1. **Fast**: Suggestions in < 50ms\n",
    "2. **Relevant**: Sorted by popularity/frequency\n",
    "3. **Fresh**: Reflect recent trends (breaking news)\n",
    "4. **Scale**: Support 10 million queries/day\n",
    "\n",
    "### **Step 2: Data Structure - Trie (Prefix Tree)**\n",
    "\n",
    "```\n",
    "                root\n",
    "                 |\n",
    "                 s\n",
    "                 |\n",
    "                 y\n",
    "                 |\n",
    "           ┌─────┴─────┐\n",
    "           s           m\n",
    "           |           |\n",
    "      ┌────┴────┐      p\n",
    "      t         t      |\n",
    "      |         |      t\n",
    "      e         e      |\n",
    "      m         m      o\n",
    "                |      |\n",
    "                d      m\n",
    "```\n",
    "\n",
    "Each node stores:\n",
    "- Character\n",
    "- Children pointers\n",
    "- Top K completions (cached at node)\n",
    "- Frequency count\n",
    "\n",
    "**Optimization**: Each node stores top 5 suggestions, so traversal stops early.\n",
    "\n",
    "### **Step 3: Architecture**\n",
    "\n",
    "```\n",
    "User types \"sys\"\n",
    "    │\n",
    "    ▼\n",
    "CDN / Edge Cache (Frequently requested prefixes cached)\n",
    "    │\n",
    "    ▼\n",
    "Autocomplete Service\n",
    "    │\n",
    "    ├─► Check Redis (prefix \"sys\" → [\"system design\", \"system architecture\"])\n",
    "    │\n",
    "    └─► Cache miss? Query Trie Service\n",
    "            │\n",
    "            ▼\n",
    "        Shard by first 2 characters (\"sy\")\n",
    "        Trie stored in memory (100GB RAM for 10M terms)\n",
    "```\n",
    "\n",
    "**Data Collection**:\n",
    "```\n",
    "Search Analytics (Kafka) → Aggregator (Spark/Flink) → Update Trie nightly\n",
    "Recent trends (last hour): Separate hot-trie updated in real-time\n",
    "```\n",
    "\n",
    "### **Deep Dive: Personalization**\n",
    "\n",
    "Generic: \"sys\" → \"system design\"\n",
    "Personalized: \"sys\" → \"system design book\" (if user bought books before)\n",
    "\n",
    "**Approach**:\n",
    "- Maintain user-specific suffixes in separate index\n",
    "- Merge generic + personalized results at serving time\n",
    "- Weight: 70% global popularity, 30% personal history\n",
    "\n",
    "---\n",
    "\n",
    "## **17.7 Design a Notification System (Push, Email, SMS)**\n",
    "\n",
    "A notification system delivers messages across multiple channels (iOS Push, Android Push, Email, SMS) with prioritization and batching.\n",
    "\n",
    "### **Step 1: Scope**\n",
    "\n",
    "**Requirements**:\n",
    "1. **Multi-channel**: iOS (APNs), Android (FCM), Email (SMTP), SMS (Twilio)\n",
    "2. **Prioritization**: Critical (2FA) vs. Marketing (batch at night)\n",
    "3. **Rate limiting**: Don't overwhelm user (max 10 push/hour)\n",
    "4. **Retry**: Exponential backoff for failures\n",
    "5. **Tracking**: Delivered, Opened, Clicked metrics\n",
    "\n",
    "### **Step 2: Architecture**\n",
    "\n",
    "```\n",
    "API Request (Send notification to user 123)\n",
    "    │\n",
    "    ▼\n",
    "┌──────────────┐\n",
    "│  Notification│──┐\n",
    "│  Service     │  │\n",
    "└──────┬───────┘  │\n",
    "       │          │\n",
    "       ▼          ▼\n",
    "┌──────────┐  ┌──────────┐\n",
    "│ Priority │  │ Template │\n",
    "│ Queue    │  │ Engine   │\n",
    "│(RabbitMQ)│  │(i18n)    │\n",
    "└────┬─────┘  └────┬─────┘\n",
    "     │             │\n",
    "     └──────┬──────┘\n",
    "            ▼\n",
    "     ┌──────────────┐\n",
    "     │   Router     │──┐──┬──┐\n",
    "     │(Channel sel)│  │  │  │\n",
    "     └──────┬───────┘  │  │  │\n",
    "            │          │  │  │\n",
    "            ▼          ▼  ▼  ▼\n",
    "        ┌───────┐  ┌────┐┌───┐┌────┐\n",
    "        │ iOS   │  │Andr││Email││SMS │\n",
    "        │Push   │  │oid ││    ││    │\n",
    "        └───────┘  └────┘└───┘└────┘\n",
    "```\n",
    "\n",
    "**Priority Queues**:\n",
    "```\n",
    "Critical: 2FA, password reset → Immediate, dedicated workers\n",
    "High: Direct messages → < 1 second delay\n",
    "Normal: Likes, comments → Batched (5 min window)\n",
    "Low: Marketing, digests → Night time, rate limited\n",
    "```\n",
    "\n",
    "**Batching Strategy**:\n",
    "```\n",
    "User receives 50 likes in 5 minutes:\n",
    "    Don't: Send 50 push notifications\n",
    "    Do: Send \"Alice and 49 others liked your post\"\n",
    "    \n",
    "Implementation:\n",
    "    Aggregate in Redis for 5 minutes\n",
    "    Key: \"pending:user_123\"\n",
    "    Value: List of notifications\n",
    "    On flush: Render template with count\n",
    "```\n",
    "\n",
    "**Delivery Guarantees**:\n",
    "```\n",
    "At-least-once delivery:\n",
    "    1. Write to database (notification status: PENDING)\n",
    "    2. Push to Queue\n",
    "    3. Worker processes, sends to APNs\n",
    "    4. APNs acknowledges → Mark DELIVERED\n",
    "    5. If no ACK in 30s → Retry (max 3 times)\n",
    "    6. After 3 failures → Mark FAILED, alert admin\n",
    "```\n",
    "\n",
    "**Idempotency**:\n",
    "```\n",
    "Notification ID generated by client or service\n",
    "If retry occurs, APNs deduplicates by ID\n",
    "Prevents duplicate notifications on network timeout retry\n",
    "```\n",
    "\n",
    "### **Deep Dive: Third-Party Integration**\n",
    "\n",
    "**APNs (Apple Push Notification service)**:\n",
    "- HTTP/2 connection maintained\n",
    "- JWT token authentication (expires every hour)\n",
    "- Feedback service: Reports invalid tokens (app uninstalled)\n",
    "\n",
    "**FCM (Firebase Cloud Messaging)**:\n",
    "- Similar to APNs but Google's implementation\n",
    "- Topic subscriptions (pub/sub model for broadcast)\n",
    "\n",
    "**Circuit Breakers**:\n",
    "```\n",
    "If APNs is down:\n",
    "    Circuit breaker opens after 10 failures\n",
    "    Queue notifications in \"delayed\" queue\n",
    "    Retry every 5 minutes\n",
    "    When APNs recovers, drain queue\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **17.8 Chapter Summary**\n",
    "\n",
    "Infrastructure systems share common patterns:\n",
    "\n",
    "1. **Sharding**: Consistent hashing for data distribution (DynamoDB, Cache)\n",
    "2. **Replication**: Leader-follower for durability, leaderless for availability\n",
    "3. **Backpressure**: When overwhelmed, shed load (rate limiting) or buffer (queues)\n",
    "4. **Idempotency**: Design for retries (notifications, ID generation)\n",
    "5. **Eventual Consistency**: Accept temporary inconsistency for availability\n",
    "\n",
    "**Key Trade-offs**:\n",
    "- **Consistency vs. Availability**: Dynamo chooses availability; traditional SQL chooses consistency\n",
    "- **Memory vs. Disk**: Redis chooses speed (memory); Cassandra chooses cost (disk)\n",
    "- **Precision vs. Performance**: Sliding window is precise but slow; token bucket is approximate but fast\n",
    "\n",
    "**Interview Tips for Infrastructure**:\n",
    "- Always mention CAP theorem implications\n",
    "- Discuss failure modes: What happens when a node dies? Network partitions?\n",
    "- Calculate capacity: Memory per node, QPS per shard, replication factor\n",
    "- Know the math: 2^10 ≈ 1000, 2^20 ≈ 1 million, 2^32 ≈ 4 billion\n",
    "\n",
    "---\n",
    "\n",
    "**Exercises**:\n",
    "\n",
    "1. **Web Crawler**: How would you detect and avoid crawler traps (infinite URL spaces like calendars)?\n",
    "\n",
    "2. **Key-Value Store**: Design a consistent read operation that guarantees reading the latest write (R+W > N quorum).\n",
    "\n",
    "3. **Cache**: How would you implement a distributed cache that maintains strong consistency (all nodes see same value) versus eventual consistency?\n",
    "\n",
    "4. **Rate Limiter**: Design a rate limiter that supports \"100 requests per minute per user, but max 1000 per hour per IP\" (multi-dimensional limits).\n",
    "\n",
    "5. **ID Generator**: What happens if the Snowflake machine's clock goes backwards by 1 second? How do you handle it?\n",
    "\n",
    "6. **Autocomplete**: How would you implement fuzzy matching (typo tolerance) in the autocomplete system?\n",
    "\n",
    "7. **Notification System**: Design a \"quiet hours\" feature where users don't receive non-critical notifications from 10 PM to 8 AM, but queue them for delivery at 8 AM.\n",
    "\n",
    "---\n",
    "\n",
    "The next chapter will cover **Enterprise-Grade Systems**—the heavy-duty infrastructure that powers the world's largest companies, including distributed message queues, payment systems, and collaborative editors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
