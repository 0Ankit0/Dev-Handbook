{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17601a8",
   "metadata": {},
   "source": [
    "# **Chapter 22: High-Scalability Challenges**\n",
    "\n",
    "When your system grows from thousands to millions to billions of users, the challenges change fundamentally. It's no longer about making a single server faster—it's about architectural patterns that can absorb massive scale, handle unpredictable viral traffic, and maintain performance across the globe. This chapter covers the advanced techniques used by companies like Google, Meta, and Netflix to operate at planetary scale.\n",
    "\n",
    "---\n",
    "\n",
    "## **22.1 Handling Flash Traffic and Viral Growth**\n",
    "\n",
    "Imagine waking up to discover your app is #1 on the App Store, or a celebrity just tweeted about your service. Normal traffic patterns go out the window. Your system must handle 100x or 1000x normal load within minutes—not hours.\n",
    "\n",
    "### **The Thundering Herd Problem**\n",
    "\n",
    "When a popular cache entry expires, thousands of requests simultaneously hit your database, potentially crashing it.\n",
    "\n",
    "**The Scenario:**\n",
    "```\n",
    "Time 08:00:00 - Cache hit for \"trending_post_123\"\n",
    "  ↓ 10,000 requests/second served from cache (fast)\n",
    "\n",
    "Time 08:00:01 - Cache entry expires (TTL reached)\n",
    "  ↓ All 10,000 requests miss cache simultaneously\n",
    "  ↓ 10,000 database queries execute at once\n",
    "  ↓ Database crashes under load\n",
    "```\n",
    "\n",
    "**Solution 1: Cache Warming with Staggered TTL**\n",
    "Instead of letting entries expire all at once, add randomness to expiration times.\n",
    "\n",
    "```python\n",
    "import random\n",
    "import redis\n",
    "\n",
    "def set_with_jitter(cache, key, value, base_ttl=300):\n",
    "    \"\"\"\n",
    "    Add randomness to prevent simultaneous expiration\n",
    "    base_ttl = 300 seconds (5 minutes)\n",
    "    jitter = +/- 10% random variation\n",
    "    \"\"\"\n",
    "    jitter = random.randint(-30, 30)  # +/- 30 seconds\n",
    "    actual_ttl = base_ttl + jitter\n",
    "    cache.setex(key, actual_ttl, value)\n",
    "    \n",
    "# Without jitter: 1000 keys expire at exactly 08:00:00\n",
    "# With jitter: Keys expire between 07:59:30 and 08:00:30\n",
    "# Database load is spread over 60 seconds instead of 1 second\n",
    "```\n",
    "\n",
    "**Solution 2: Lease-Based Cache (Thundering Herd Protection)**\n",
    "\n",
    "Only allow one request to regenerate the cache value; others wait or serve stale data.\n",
    "\n",
    "```python\n",
    "import threading\n",
    "import time\n",
    "\n",
    "class LeaseBasedCache:\n",
    "    def __init__(self):\n",
    "        self.cache = {}\n",
    "        self.locks = {}\n",
    "        self.lock = threading.Lock()\n",
    "    \n",
    "    def get(self, key, compute_func, ttl=300):\n",
    "        # Check cache first\n",
    "        if key in self.cache:\n",
    "            value, expiry = self.cache[key]\n",
    "            if time.time() < expiry:\n",
    "                return value  # Cache hit\n",
    "        \n",
    "        # Try to acquire lease for regeneration\n",
    "        with self.lock:\n",
    "            if key not in self.locks:\n",
    "                self.locks[key] = threading.Lock()\n",
    "        \n",
    "        # Only one thread can acquire the lease\n",
    "        if self.locks[key].acquire(blocking=False):\n",
    "            try:\n",
    "                # This thread regenerates the value\n",
    "                value = compute_func()\n",
    "                self.cache[key] = (value, time.time() + ttl)\n",
    "                return value\n",
    "            finally:\n",
    "                self.locks[key].release()\n",
    "        else:\n",
    "            # Another thread is regenerating\n",
    "            # Option A: Wait briefly and retry\n",
    "            time.sleep(0.1)\n",
    "            return self.get(key, compute_func, ttl)\n",
    "            \n",
    "            # Option B: Return stale data (if available)\n",
    "            # return self.cache.get(key, (None, 0))[0]\n",
    "```\n",
    "\n",
    "**Solution 3: Circuit Breakers with Graceful Degradation**\n",
    "\n",
    "When overload is detected, fail fast and serve fallback content.\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "import time\n",
    "\n",
    "class CircuitState(Enum):\n",
    "    CLOSED = \"closed\"      # Normal operation\n",
    "    OPEN = \"open\"          # Failing fast\n",
    "    HALF_OPEN = \"half_open\"  # Testing if recovered\n",
    "\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, failure_threshold=5, timeout=60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = CircuitState.CLOSED\n",
    "    \n",
    "    def call(self, func, fallback_func, *args, **kwargs):\n",
    "        if self.state == CircuitState.OPEN:\n",
    "            if time.time() - self.last_failure_time > self.timeout:\n",
    "                self.state = CircuitState.HALF_OPEN\n",
    "            else:\n",
    "                return fallback_func(*args, **kwargs)\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            if self.state == CircuitState.HALF_OPEN:\n",
    "                self.state = CircuitState.CLOSED\n",
    "                self.failure_count = 0\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self.failure_count += 1\n",
    "            self.last_failure_time = time.time()\n",
    "            \n",
    "            if self.failure_count >= self.failure_threshold:\n",
    "                self.state = CircuitState.OPEN\n",
    "            \n",
    "            return fallback_func(*args, **kwargs)\n",
    "\n",
    "# Usage example\n",
    "breaker = CircuitBreaker(failure_threshold=10, timeout=30)\n",
    "\n",
    "def get_user_profile(user_id):\n",
    "    # Database query that might fail under load\n",
    "    return db.query(\"SELECT * FROM users WHERE id = ?\", user_id)\n",
    "\n",
    "def get_cached_profile(user_id):\n",
    "    # Fallback: Return cached version or default\n",
    "    return cache.get(f\"user:{user_id}\") or {\"error\": \"Service temporarily unavailable\"}\n",
    "\n",
    "# Under normal load: Returns fresh data\n",
    "# Under flash traffic: Returns cached data, protecting database\n",
    "profile = breaker.call(get_user_profile, get_cached_profile, user_id=123)\n",
    "```\n",
    "\n",
    "### **Autoscaling Strategies for Viral Events**\n",
    "\n",
    "**Reactive Scaling (Traditional)**\n",
    "```\n",
    "Traffic increases → CloudWatch alarm triggers → Launch new instances → 5 minutes later → New instances handle load\n",
    "```\n",
    "\n",
    "**Problem**: 5-minute lag is too slow for viral traffic. By the time new servers are ready, the database is already down.\n",
    "\n",
    "**Predictive Scaling**\n",
    "Use machine learning to predict traffic spikes based on:\n",
    "- Time of day patterns\n",
    "- Social media trends (Twitter API monitoring)\n",
    "- Marketing campaign schedules\n",
    "- Historical viral events\n",
    "\n",
    "```python\n",
    "# Pseudo-code for predictive scaling\n",
    "def predict_traffic(current_time):\n",
    "    base_load = get_historical_average(current_time)\n",
    "    \n",
    "    # Check for viral indicators\n",
    "    twitter_mentions = get_twitter_mentions_count(\"our_app\")\n",
    "    if twitter_mentions > threshold:\n",
    "        viral_multiplier = min(twitter_mentions / threshold, 10)  # Cap at 10x\n",
    "        return base_load * viral_multiplier\n",
    "    \n",
    "    return base_load\n",
    "\n",
    "# Pre-scale before traffic hits\n",
    "predicted_load = predict_traffic(datetime.now())\n",
    "if predicted_load > current_capacity * 0.8:\n",
    "    scale_up(predicted_load * 1.5)  # 50% buffer\n",
    "```\n",
    "\n",
    "**Scheduled Scaling**\n",
    "For known events (product launches, Black Friday):\n",
    "```yaml\n",
    "# AWS Auto Scaling configuration\n",
    "ScheduledActions:\n",
    "  - ScheduledActionName: BlackFridayPrep\n",
    "    StartTime: 2024-11-28T00:00:00Z\n",
    "    EndTime: 2024-11-30T23:59:59Z\n",
    "    MinSize: 100  # Normal: 10\n",
    "    MaxSize: 1000\n",
    "    DesiredCapacity: 500\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.2 Geographical Distribution**\n",
    "\n",
    "When users are global, latency is physics. Light takes 67ms to travel from New York to London—and that's in a vacuum. Real networks take 80-150ms. If your data is in Virginia but your user is in Tokyo, every request pays a 200ms penalty.\n",
    "\n",
    "### **Content Delivery Networks (CDNs)**\n",
    "\n",
    "**How CDNs Work**\n",
    "```\n",
    "User in Tokyo requests image.jpg\n",
    "    ↓\n",
    "Local DNS resolves to nearest CDN edge (Tokyo)\n",
    "    ↓\n",
    "Tokyo edge checks cache:\n",
    "    ├─ Cache Hit: Serve immediately (5-10ms)\n",
    "    └─ Cache Miss: Fetch from Origin (Virginia), cache locally, serve (200ms + 5ms)\n",
    "```\n",
    "\n",
    "**CDN Caching Strategies**\n",
    "\n",
    "**1. Static Asset Caching (Images, CSS, JS)**\n",
    "```http\n",
    "# Response headers for long-term caching\n",
    "Cache-Control: public, max-age=31536000, immutable\n",
    "ETag: \"33a64df5\"\n",
    "# immutable = file never changes (versioned filenames: app.v2.js)\n",
    "```\n",
    "\n",
    "**2. Dynamic Content Caching (API Responses)**\n",
    "```http\n",
    "# Short-term caching for semi-dynamic content\n",
    "Cache-Control: public, max-age=60, stale-while-revalidate=300\n",
    "# Serve from cache for 60 seconds\n",
    "# If expired, serve stale version while fetching fresh in background\n",
    "```\n",
    "\n",
    "**3. Cache Invalidation Strategies**\n",
    "\n",
    "**Purge API** (Immediate but expensive):\n",
    "```bash\n",
    "# Invalidate specific URL across all edge locations\n",
    "curl -X POST \"https://api.cdn.com/purge\" \\\n",
    "  -d '{\"url\": \"https://example.com/prices\"}'\n",
    "# Takes 2-5 minutes to propagate globally\n",
    "```\n",
    "\n",
    "**Versioned URLs** (Preferred for static assets):\n",
    "```html\n",
    "<!-- Instead of -->\n",
    "<script src=\"/app.js\"></script>\n",
    "\n",
    "<!-- Use -->\n",
    "<script src=\"/app.v2.js\"></script>\n",
    "<!-- Change HTML to v3 when deploying new version -->\n",
    "<!-- Old version stays cached forever, new version is fresh -->\n",
    "```\n",
    "\n",
    "**4. Edge Computing: Logic at the Edge**\n",
    "\n",
    "Modern CDNs (Cloudflare Workers, AWS Lambda@Edge) allow you to run code at edge locations, not just serve static files.\n",
    "\n",
    "```javascript\n",
    "// Cloudflare Worker: A/B testing at the edge\n",
    "addEventListener('fetch', event => {\n",
    "  event.respondWith(handleRequest(event.request))\n",
    "})\n",
    "\n",
    "async function handleRequest(request) {\n",
    "  const url = new URL(request.url)\n",
    "  \n",
    "  // Check cookie for existing assignment\n",
    "  let group = request.headers.get('Cookie')?.match(/ab_group=(\\w)/)?.[1]\n",
    "  \n",
    "  if (!group) {\n",
    "    // Assign to group A or B (50/50 split)\n",
    "    group = Math.random() < 0.5 ? 'A' : 'B'\n",
    "    \n",
    "    // Modify request to route to different origin\n",
    "    if (group === 'B') {\n",
    "      url.hostname = 'origin-b.example.com'\n",
    "    }\n",
    "    \n",
    "    // Fetch from appropriate origin\n",
    "    const response = await fetch(url, request)\n",
    "    \n",
    "    // Add cookie to response\n",
    "    const newResponse = new Response(response.body, response)\n",
    "    newResponse.headers.append('Set-Cookie', `ab_group=${group}; Path=/`)\n",
    "    return newResponse\n",
    "  }\n",
    "  \n",
    "  return fetch(url, request)\n",
    "}\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "- A/B testing without latency penalty (decision made at edge)\n",
    "- Authentication at edge (block bad requests before they hit origin)\n",
    "- Geolocation-based routing (serve different content to EU vs. US users for GDPR compliance)\n",
    "\n",
    "### **Global Databases: Spanner and CockroachDB**\n",
    "\n",
    "**The Problem with Traditional Replication**\n",
    "```\n",
    "Master in Virginia, Replica in Tokyo\n",
    "├─ Read from Tokyo: 5ms (fast, but might be stale)\n",
    "└─ Write to Tokyo: Must go to Virginia (200ms round-trip)\n",
    "```\n",
    "\n",
    "**Google Spanner: True Global Consistency**\n",
    "\n",
    "Spanner uses **TrueTime API** (atomic clocks + GPS) to provide globally consistent reads without locking.\n",
    "\n",
    "**How TrueTime Works**\n",
    "```python\n",
    "# Traditional database timestamp: 2024-01-15 10:30:00.000\n",
    "# Problem: Clocks on different servers differ by 10-100ms\n",
    "\n",
    "# Spanner TrueTime: [earliest, latest]\n",
    "# Example: [10:30:00.100, 10:30:00.200]\n",
    "# Uncertainty interval: 100ms\n",
    "\n",
    "# Spanner waits out the uncertainty interval before committing\n",
    "# Guarantees: If transaction A commits before B starts, A's timestamp < B's\n",
    "```\n",
    "\n",
    "**Spanner Architecture**\n",
    "```\n",
    "Global Layer (Location Tracking):\n",
    "  ├─ US-Central: Leader for Users 1-1000000\n",
    "  ├─ Europe-West: Leader for Users 1000001-2000000\n",
    "  └─ Asia-East: Leader for Users 2000001-3000000\n",
    "\n",
    "Local Layer (Within each region):\n",
    "  ├─ Paxos group (3-5 replicas for consensus)\n",
    "  └─ Data split into chunks (splits), moved for load balancing\n",
    "```\n",
    "\n",
    "**Code Example** (CockroachDB, open-source Spanner alternative):\n",
    "```sql\n",
    "-- CockroachDB automatically distributes data geographically\n",
    "-- Table partitioned by region for data locality\n",
    "\n",
    "CREATE TABLE orders (\n",
    "    id UUID DEFAULT gen_random_uuid(),\n",
    "    region STRING,\n",
    "    amount DECIMAL,\n",
    "    PRIMARY KEY (region, id)\n",
    ") PARTITION BY LIST (region) (\n",
    "    PARTITION us_west VALUES IN ('us-west'),\n",
    "    PARTITION eu_west VALUES IN ('eu-west'),\n",
    "    PARTITION asia_east VALUES IN ('asia-east')\n",
    ");\n",
    "\n",
    "-- Pin partitions to specific regions for compliance/latency\n",
    "ALTER PARTITION us_west CONFIGURE ZONE USING \n",
    "    constraints = '[+region=us-west1]';\n",
    "\n",
    "-- Queries automatically routed to nearest replica\n",
    "-- Writes go to regional leader, then replicate asynchronously\n",
    "```\n",
    "\n",
    "**Trade-offs**:\n",
    "- **Spanner**: Strong consistency globally, but writes are slower (consensus required)\n",
    "- **DynamoDB Global Tables**: Eventually consistent, but writes are fast (local)\n",
    "- **Choice depends on**: Can your app tolerate temporary inconsistency for better performance?\n",
    "\n",
    "---\n",
    "\n",
    "## **22.3 Federated and Cell-Based Architecture**\n",
    "\n",
    "When a single monolithic database can't scale further (even with sharding), you need **federation**—breaking the system into independent, self-contained units.\n",
    "\n",
    "### **Cell-Based Architecture**\n",
    "\n",
    "**Concept**: Instead of one giant system, create many small copies (cells), each handling a subset of users.\n",
    "\n",
    "```\n",
    "Traditional Monolith:\n",
    "┌─────────────────────────────┐\n",
    "│  Load Balancer              │\n",
    "│     ↓                       │\n",
    "│  App Servers (1000s)        │\n",
    "│     ↓                       │\n",
    "│  Database Cluster (Petabytes) │\n",
    "└─────────────────────────────┘\n",
    "     Single point of failure\n",
    "\n",
    "Cell-Based:\n",
    "┌─────────┐ ┌─────────┐ ┌─────────┐\n",
    "│ Cell A  │ │ Cell B  │ │ Cell C  │\n",
    "│ Users   │ │ Users   │ │ Users   │\n",
    "│ 0-1M    │ │ 1M-2M   │ │ 2M-3M   │\n",
    "│ ┌─────┐ │ │ ┌─────┐ │ │ ┌─────┐ │\n",
    "│ │App  │ │ │ │App  │ │ │ │App  │ │\n",
    "│ │DB   │ │ │ │DB   │ │ │ │DB   │ │\n",
    "│ └─────┘ │ │ └─────┘ │ │ └─────┘ │\n",
    "└─────────┘ └─────────┘ └─────────┘\n",
    "     ↓           ↓           ↓\n",
    "  Isolated    Isolated    Isolated\n",
    "  Failures    Failures    Failures\n",
    "```\n",
    "\n",
    "**Benefits**:\n",
    "1. **Fault Isolation**: If Cell A fails, only 1M users affected, not 100M\n",
    "2. **Incremental Deployment**: Deploy to Cell A first, monitor, then roll out to others\n",
    "3. **Geographic Distribution**: Cell A in US, Cell B in EU (data sovereignty)\n",
    "4. **Scalability**: Add new cells indefinitely; no single database grows too large\n",
    "\n",
    "**Implementation: User Assignment**\n",
    "```python\n",
    "class CellRouter:\n",
    "    def __init__(self):\n",
    "        self.cells = {\n",
    "            'cell-us-1': {'range': (0, 1000000), 'endpoint': 'us1.example.com'},\n",
    "            'cell-us-2': {'range': (1000000, 2000000), 'endpoint': 'us2.example.com'},\n",
    "            'cell-eu-1': {'range': (2000000, 3000000), 'endpoint': 'eu1.example.com'},\n",
    "        }\n",
    "    \n",
    "    def get_cell_for_user(self, user_id):\n",
    "        for cell_id, config in self.cells.items():\n",
    "            if config['range'][0] <= user_id < config['range'][1]:\n",
    "                return config['endpoint']\n",
    "        raise ValueError(f\"No cell found for user {user_id}\")\n",
    "    \n",
    "    def move_user(self, user_id, target_cell):\n",
    "        # Migration logic for rebalancing cells\n",
    "        pass\n",
    "\n",
    "# Usage\n",
    "router = CellRouter()\n",
    "endpoint = router.get_cell_for_user(user_id=123456)\n",
    "# Route request to us1.example.com\n",
    "```\n",
    "\n",
    "**Cross-Cell Communication**\n",
    "When User A (Cell 1) needs to message User B (Cell 2):\n",
    "\n",
    "```python\n",
    "class CrossCellMessenger:\n",
    "    def send_message(self, from_user, to_user, message):\n",
    "        target_cell = self.router.get_cell_for_user(to_user)\n",
    "        \n",
    "        # Async message to avoid blocking\n",
    "        self.message_queue.publish(\n",
    "            topic=f\"cell-{target_cell}-messages\",\n",
    "            data={\n",
    "                'from': from_user,\n",
    "                'to': to_user,\n",
    "                'content': message,\n",
    "                'timestamp': time.time()\n",
    "            }\n",
    "        )\n",
    "        \n",
    "        # Local cell stores \"sent\" copy\n",
    "        self.local_db.store_outbox(from_user, to_user, message)\n",
    "```\n",
    "\n",
    "### **Federated Architecture**\n",
    "\n",
    "**Difference from Cells**: Cells are identical copies handling different users. Federation is when different services own different data types, with autonomy.\n",
    "\n",
    "**Example: Federated Social Media**\n",
    "```\n",
    "User Service (Team A): Owns profiles, authentication\n",
    "Post Service (Team B): Owns content, feeds\n",
    "Media Service (Team C): Owns images, videos\n",
    "\n",
    "Each service has its own:\n",
    "- Database\n",
    "- Deployment pipeline\n",
    "- Scaling policies\n",
    "- Team ownership\n",
    "\n",
    "Communication via:\n",
    "- Async events (Kafka) for loose coupling\n",
    "- gRPC for synchronous queries (with circuit breakers)\n",
    "```\n",
    "\n",
    "**Federation Gateway**\n",
    "```python\n",
    "# GraphQL Federation: Single entry point, distributed backends\n",
    "class FederationGateway:\n",
    "    def resolve_query(self, query):\n",
    "        # Parse query to find required services\n",
    "        services_needed = self.analyze_query(query)\n",
    "        \n",
    "        # Parallel fetch from multiple services\n",
    "        with ThreadPoolExecutor() as executor:\n",
    "            futures = {\n",
    "                executor.submit(self.call_service, svc): svc \n",
    "                for svc in services_needed\n",
    "            }\n",
    "            \n",
    "            results = {}\n",
    "            for future in as_completed(futures):\n",
    "                service = futures[future]\n",
    "                results[service] = future.result()\n",
    "        \n",
    "        # Stitch results together\n",
    "        return self.stitch_results(query, results)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.4 Petabyte-Scale Data Processing**\n",
    "\n",
    "When your data no longer fits on one disk (or even one rack), you need distributed data processing frameworks.\n",
    "\n",
    "### **The MapReduce Paradigm**\n",
    "\n",
    "**Concept**: Split big task into small chunks (Map), process in parallel, combine results (Reduce).\n",
    "\n",
    "**Word Count Example** (The \"Hello World\" of Big Data):\n",
    "```\n",
    "Input: 10TB of text files across 1000 servers\n",
    "\n",
    "Map Phase (Parallel on each server):\n",
    "  Server 1: \"the quick brown fox\" → (the,1), (quick,1), (brown,1), (fox,1)\n",
    "  Server 2: \"the lazy dog\" → (the,1), (lazy,1), (dog,1)\n",
    "  Server 3: \"the quick dog\" → (the,1), (quick,1), (dog,1)\n",
    "\n",
    "Shuffle Phase (Sort by key):\n",
    "  (the, [1,1,1])\n",
    "  (quick, [1,1])\n",
    "  (brown, [1])\n",
    "  ...\n",
    "\n",
    "Reduce Phase (Aggregate):\n",
    "  (the, 3)\n",
    "  (quick, 2)\n",
    "  (brown, 1)\n",
    "  ...\n",
    "```\n",
    "\n",
    "**Apache Spark** (Modern MapReduce):\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize cluster connection\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"LargeScaleAnalytics\") \\\n",
    "    .config(\"spark.executor.memory\", \"64g\") \\\n",
    "    .config(\"spark.executor.cores\", \"16\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Load petabyte-scale dataset from S3/HDFS\n",
    "df = spark.read.parquet(\"s3://data-lake/events/\")\n",
    "\n",
    "# Transformations (lazy evaluation - nothing executed yet)\n",
    "processed = df.filter(df.timestamp > \"2024-01-01\") \\\n",
    "              .groupBy(\"user_id\") \\\n",
    "              .agg({\"amount\": \"sum\"}) \\\n",
    "              .filter(col(\"sum(amount)\") > 10000)\n",
    "\n",
    "# Action (triggers distributed computation)\n",
    "high_value_users = processed.collect()\n",
    "\n",
    "# Spark automatically:\n",
    "# 1. Splits data into partitions across cluster\n",
    "# 2. Schedules tasks to nodes with local data (data locality)\n",
    "# 3. Handles node failures by recomputing lost partitions\n",
    "# 4. Optimizes query plan (predicate pushdown, etc.)\n",
    "```\n",
    "\n",
    "**Data Locality Optimization**\n",
    "```\n",
    "Without locality: Data in S3 → Download to Node → Process → Upload result\n",
    "With locality: Data already on Node's local disk → Process immediately\n",
    "Speed difference: 10-100x faster\n",
    "```\n",
    "\n",
    "### **Stream Processing at Scale**\n",
    "\n",
    "**Lambda Architecture** (Batch + Speed layers):\n",
    "```\n",
    "Raw Data → ┌──────────────┐\n",
    "           │ Batch Layer  │ → Precompute views (hourly, daily)\n",
    "           │ (Hadoop/Spark)│   Accurate but slow\n",
    "           └──────────────┘\n",
    "           ↓\n",
    "        Serving Layer (Merge batch + real-time views)\n",
    "           ↑\n",
    "           └──────────────┐\n",
    "             Speed Layer  │ → Real-time approximations\n",
    "             (Storm/Flink)│   Fast but approximate\n",
    "             └──────────────┘\n",
    "```\n",
    "\n",
    "**Kappa Architecture** (Streaming only, simpler):\n",
    "```\n",
    "Raw Data → Kafka → Stream Processor (Flink) → Serving Database\n",
    "                    ↓\n",
    "              Real-time aggregations\n",
    "              Exactly-once processing\n",
    "```\n",
    "\n",
    "**Apache Flink Example** (Real-time analytics):\n",
    "```java\n",
    "// Process 1 million events/second with exactly-once semantics\n",
    "StreamExecutionEnvironment env = \n",
    "    StreamExecutionEnvironment.getExecutionEnvironment();\n",
    "\n",
    "env.enableCheckpointing(60000); // Snapshot state every 60s for fault tolerance\n",
    "\n",
    "DataStream<Event> stream = env\n",
    "    .addSource(new KafkaSource<>(\"events-topic\"))\n",
    "    .map(event -> parseJson(event))\n",
    "    .keyBy(event -> event.userId)\n",
    "    .window(TumblingEventTimeWindows.of(Time.minutes(5)))\n",
    "    .aggregate(new CountAggregate());\n",
    "\n",
    "// Results written to Redis for real-time dashboards\n",
    "stream.addSink(new RedisSink<>());\n",
    "\n",
    "env.execute();\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.5 Real-Time Systems and Low-Latency Optimization**\n",
    "\n",
    "When milliseconds matter (high-frequency trading, gaming, ad bidding), architecture changes fundamentally.\n",
    "\n",
    "### **The Latency Hierarchy**\n",
    "\n",
    "```\n",
    "Latency      System Type                    Techniques\n",
    "─────────────────────────────────────────────────────────\n",
    "< 1ms        In-memory, single machine      Lock-free queues, CPU pinning\n",
    "1-10ms       Local network                  Kernel bypass (DPDK), RDMA\n",
    "10-100ms     Same datacenter                Async I/O, connection pooling\n",
    "100ms+       Cross-region                   Caching, CDNs, eventual consistency\n",
    "```\n",
    "\n",
    "### **Kernel Bypass: DPDK**\n",
    "\n",
    "**Problem**: Traditional networking:\n",
    "```\n",
    "Network Card → Kernel TCP stack → User Application\n",
    "     ↓              ↓                    ↓\n",
    "  10µs          100µs                Your code\n",
    "```\n",
    "\n",
    "**DPDK (Data Plane Development Kit)**:\n",
    "```\n",
    "Network Card → DPDK (User space) → Application\n",
    "     ↓              ↓                    ↓\n",
    "  10µs           1µs                  Your code\n",
    "```\n",
    "\n",
    "**How it works**: Application takes direct control of network card, bypassing kernel entirely.\n",
    "\n",
    "```c\n",
    "// DPDK allows processing packets in userspace\n",
    "// Used by Cloudflare, AWS, high-frequency trading firms\n",
    "\n",
    "// Traditional: recv() system call (context switch to kernel)\n",
    "// DPDK: Poll network card directly from userspace (no context switch)\n",
    "\n",
    "// Trade-off: \n",
    "// - Pros: 10x lower latency, 10x higher throughput\n",
    "// - Cons: Application must implement TCP stack, harder to debug\n",
    "```\n",
    "\n",
    "### **Lock-Free Data Structures**\n",
    "\n",
    "**Problem**: Locks cause contention and cache coherence traffic between CPU cores.\n",
    "\n",
    "**Solution**: Atomic operations and memory ordering.\n",
    "\n",
    "```java\n",
    "// Java: Lock-free queue (Disruptor pattern)\n",
    "// Used by LMAX exchange for 6 million transactions/second\n",
    "\n",
    "class RingBuffer<T> {\n",
    "    private final long[] sequence;\n",
    "    private final Object[] entries;\n",
    "    private final AtomicLong writeSequence = new AtomicLong(-1);\n",
    "    private final AtomicLong readSequence = new AtomicLong(-1);\n",
    "    \n",
    "    public void publish(T event) {\n",
    "        long sequence = writeSequence.incrementAndGet();\n",
    "        entries[(int)(sequence % entries.length)] = event;\n",
    "        // Memory barrier ensures visibility across cores\n",
    "        writeSequence.lazySet(sequence);\n",
    "    }\n",
    "    \n",
    "    public T poll() {\n",
    "        long nextSequence = readSequence.get() + 1;\n",
    "        if (nextSequence <= writeSequence.get()) {\n",
    "            T result = (T) entries[(int)(nextSequence % entries.length)];\n",
    "            readSequence.set(nextSequence);\n",
    "            return result;\n",
    "        }\n",
    "        return null;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### **Pre-computation and Materialized Views**\n",
    "\n",
    "For read-heavy workloads, compute answers before questions are asked.\n",
    "\n",
    "```sql\n",
    "-- Traditional: Compute on read (slow for complex aggregations)\n",
    "SELECT user_id, COUNT(*), SUM(amount) \n",
    "FROM transactions \n",
    "WHERE date > NOW() - INTERVAL 30 DAY\n",
    "GROUP BY user_id;\n",
    "\n",
    "-- Optimized: Materialized view (pre-computed)\n",
    "CREATE MATERIALIZED VIEW user_monthly_stats AS\n",
    "SELECT user_id, COUNT(*) as txn_count, SUM(amount) as total_amount\n",
    "FROM transactions\n",
    "WHERE date > NOW() - INTERVAL 30 DAY\n",
    "GROUP BY user_id;\n",
    "\n",
    "-- Refresh every 5 minutes (acceptable staleness for dashboard)\n",
    "REFRESH MATERIALIZED VIEW user_monthly_stats;\n",
    "\n",
    "-- Read is now O(1) instead of O(n)\n",
    "SELECT * FROM user_monthly_stats WHERE user_id = 123;\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.6 Key Takeaways**\n",
    "\n",
    "1. **Flash traffic requires defense in depth**: Cache warming, circuit breakers, and autoscaling work together to prevent cascades.\n",
    "\n",
    "2. **Geography is physics**: You can't beat the speed of light. Use CDNs, edge computing, and global databases to put data close to users.\n",
    "\n",
    "3. **Cells provide blast radius containment**: When scaling beyond millions of users, cellular architecture limits the impact of failures.\n",
    "\n",
    "4. **Batch for throughput, stream for latency**: Use Spark/Hadoop for petabyte-scale batch processing, Flink/Kafka for real-time streams.\n",
    "\n",
    "5. **Last-mile optimization**: When every microsecond counts, kernel bypass (DPDK) and lock-free structures provide 10x improvements.\n",
    "\n",
    "6. **Pre-computation trades space for time**: Materialized views and edge caching make reads instant at the cost of staleness.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "This chapter explored the challenges of planetary-scale systems. We learned to handle viral traffic through cache warming and circuit breakers, distribute data globally using CDNs and databases like Spanner, and architect cellular systems for fault isolation. We covered processing petabytes of data with MapReduce and Spark, and achieving microsecond latencies through kernel bypass and lock-free programming.\n",
    "\n",
    "The key insight: At massive scale, architecture matters more than code optimization. A well-designed cellular system with eventual consistency will outperform a tightly-coupled strongly-consistent monolith every time.\n",
    "\n",
    "**Coming up next**: In Chapter 23, we'll explore AI/ML System Design—how to serve machine learning models at scale, implement feature stores, and build RAG (Retrieval-Augmented Generation) architectures for LLMs.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercises**\n",
    "\n",
    "1. **Thundering Herd Calculation**: Your cache expires 1000 keys simultaneously. Each cache miss triggers a database query taking 50ms. Your database can handle 100 concurrent connections. How long does it take to serve all 1000 requests? How would staggered TTL with 30-second jitter improve this?\n",
    "\n",
    "2. **Cell-Based Routing**: Design a routing function that assigns users to cells based on user ID hash, ensuring even distribution across 8 cells. Write the code to determine which cell handles user 12345, and how to migrate that user to cell 3 with zero downtime.\n",
    "\n",
    "3. **Global Latency Math**: A user in Sydney (Australia) requests data from a database in Dublin (Ireland). Calculate:\n",
    "   - Minimum theoretical latency (speed of light, fiber optic refractive index 1.44)\n",
    "   - Realistic latency with network hops (add 30% overhead)\n",
    "   - How much faster would the request be if served from a Singapore edge node (distance: Sydney to Singapore)?\n",
    "\n",
    "4. **Spark Optimization**: You have 1TB of data in S3 and a Spark cluster with 10 nodes (each 32 cores, 128GB RAM). Your job is taking 2 hours. Identify three optimizations to reduce this to 10 minutes (hint: consider data locality, partition size, and serialization).\n",
    "\n",
    "5. **Circuit Breaker Design**: Implement a circuit breaker that transitions from CLOSED → OPEN after 5 failures, stays open for 60 seconds, then enters HALF_OPEN state where it allows 1 test request before deciding to close or reopen.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
