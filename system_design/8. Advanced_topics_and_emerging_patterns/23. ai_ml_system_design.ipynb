{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "970ec7fb",
   "metadata": {},
   "source": [
    "# **Chapter 23: AI/ML System Design**\n",
    "\n",
    "Artificial Intelligence and Machine Learning have moved from research labs to production systems serving billions of requests daily. Designing ML systems presents unique challenges: models are compute-intensive, require specialized hardware (GPUs/TPUs), need constant retraining as data drifts, and must serve predictions with strict latency requirements. This chapter covers the architecture patterns that power recommendation engines, fraud detection systems, and Large Language Models (LLMs) at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.1 Model Serving Architectures**\n",
    "\n",
    "When deploying machine learning models, the serving strategy determines your latency, throughput, and cost. The choice between batch and real-time inference depends on your use case's tolerance for delay.\n",
    "\n",
    "### **Batch Inference: Process Everything at Once**\n",
    "\n",
    "**Concept**: Collect data over a time window (hourly, daily), run inference on all data simultaneously, store results for later retrieval.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "Data Lake (S3/HDFS) → Spark/Flink Job → Model Inference → Feature Store/Database\n",
    "     ↑                                                      ↓\n",
    "   Raw Events                                         Pre-computed Predictions\n",
    "                                                        (User recommendations,\n",
    "                                                         fraud scores, etc.)\n",
    "```\n",
    "\n",
    "**When to Use**:\n",
    "- Recommendations that update hourly (not instant)\n",
    "- Overnight fraud risk scoring\n",
    "- Customer churn prediction (doesn't need to be real-time)\n",
    "- Training data generation\n",
    "\n",
    "**Example: Netflix Movie Recommendations**\n",
    "```\n",
    "Every 4 hours:\n",
    "1. Collect viewing history for all 230M users\n",
    "2. Run collaborative filtering model on GPU cluster\n",
    "3. Generate \"Top 10 for You\" list for each user\n",
    "4. Store in Redis (serves in <1ms when user opens app)\n",
    "\n",
    "Latency: 4 hours stale, but retrieval is instant\n",
    "```\n",
    "\n",
    "**Code Example** (Apache Spark with MLlib):\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.recommendation import ALSModel\n",
    "\n",
    "# Load pre-trained model\n",
    "model = ALSModel.load(\"s3://models/collaborative-filtering/v2.1/\")\n",
    "\n",
    "# Batch process all users\n",
    "user_features = spark.read.parquet(\"s3://features/user-profiles/\")\n",
    "predictions = model.recommendForUserSubset(user_features, 10)\n",
    "\n",
    "# Write results to serving layer\n",
    "predictions.write \\\n",
    "    .format(\"redis\") \\\n",
    "    .option(\"table\", \"recommendations\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save()\n",
    "\n",
    "# 230M users processed in 30 minutes on 100 GPU instances\n",
    "```\n",
    "\n",
    "**Advantages**:\n",
    "- **Cost efficient**: GPUs utilized at 100% (no idle time)\n",
    "- **High throughput**: Process millions of records in parallel\n",
    "- **Complex models**: Can use larger models that would be too slow for real-time\n",
    "\n",
    "**Disadvantages**:\n",
    "- **Stale predictions**: Results are hours old\n",
    "- **Cold start**: New users wait for next batch cycle\n",
    "- **Storage cost**: Must store predictions for all users\n",
    "\n",
    "### **Real-Time (Online) Inference: Predict on Demand**\n",
    "\n",
    "**Concept**: Receive request → Extract features → Run model → Return prediction in milliseconds.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "User Request → API Gateway → Feature Store (fetch) → Model Server (GPU) → Response\n",
    "                   ↓                ↓                      ↓\n",
    "            Rate Limiting      Pre-computed         Model Inference\n",
    "                               features            (TensorFlow/PyTorch)\n",
    "```\n",
    "\n",
    "**When to Use**:\n",
    "- Fraud detection (must check transaction now)\n",
    "- Search ranking (results needed immediately)\n",
    "- Autocomplete (every keystroke)\n",
    "- Self-driving cars (real-time object detection)\n",
    "\n",
    "**Example: Credit Card Fraud Detection**\n",
    "```\n",
    "User swipes card → Transaction sent to API → \n",
    "    ├─ Fetch user features (avg transaction amount, location history)\n",
    "    ├─ Run XGBoost model on GPU (2ms inference time)\n",
    "    ├─ Return \"approve\" or \"decline\"\n",
    "Total latency: 50ms (user doesn't notice delay)\n",
    "```\n",
    "\n",
    "**Model Serving Frameworks**:\n",
    "\n",
    "**TensorFlow Serving**:\n",
    "```python\n",
    "# Model server configuration\n",
    "model_config {\n",
    "  name: 'fraud_detection'\n",
    "  base_path: '/models/fraud'\n",
    "  model_version_policy {\n",
    "    specific {\n",
    "      versions: 1\n",
    "      versions: 2  # Keep both versions for A/B testing\n",
    "    }\n",
    "  }\n",
    "  version_labels {\n",
    "    key: 'stable'\n",
    "    value: 1\n",
    "  }\n",
    "  version_labels {\n",
    "    key: 'canary'\n",
    "    value: 2\n",
    "  }\n",
    "}\n",
    "\n",
    "# Client request\n",
    "import grpc\n",
    "from tensorflow_serving.apis import predict_pb2\n",
    "from tensorflow_serving.apis import prediction_service_pb2_grpc\n",
    "\n",
    "channel = grpc.insecure_channel('localhost:8500')\n",
    "stub = prediction_service_pb2_grpc.PredictionServiceStub(channel)\n",
    "\n",
    "request = predict_pb2.PredictRequest()\n",
    "request.model_spec.name = 'fraud_detection'\n",
    "request.model_spec.signature_name = 'serving_default'\n",
    "request.inputs['transaction'].CopyFrom(tf.make_tensor_proto(transaction_data))\n",
    "\n",
    "result = stub.Predict(request, 10.0)  # 10 second timeout\n",
    "fraud_score = result.outputs['score'].float_val[0]\n",
    "```\n",
    "\n",
    "**TorchServe** (PyTorch):\n",
    "```python\n",
    "# handler.py\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "class FraudHandler(BaseHandler):\n",
    "    def preprocess(self, data):\n",
    "        # Extract features from request\n",
    "        return torch.tensor([data[0]['body']])\n",
    "    \n",
    "    def inference(self, inputs):\n",
    "        # Run on GPU if available\n",
    "        with torch.no_grad():\n",
    "            return self.model(inputs).sigmoid()\n",
    "    \n",
    "    def postprocess(self, inference_output):\n",
    "        return [{\"fraud_probability\": float(inference_output[0][0])}]\n",
    "\n",
    "# Deployment\n",
    "# torch-model-archiver --model-name fraud --version 1.0 --handler handler.py\n",
    "# torchserve --start --model-store model_store --models fraud=fraud.mar\n",
    "```\n",
    "\n",
    "### **Hybrid Architecture: The Lambda Pattern for ML**\n",
    "\n",
    "Most production systems use both approaches simultaneously:\n",
    "\n",
    "```\n",
    "Real-time Path (Latency-critical):\n",
    "User Search Query → Feature Store → Small Model (BERT-tiny) → Initial Ranking (100ms)\n",
    "                                                          ↓\n",
    "Batch Path (Quality-critical):                          Merge\n",
    "Overnight Spark Job → Large Model (GPT-4) → Re-ranking Scores (stored in cache)\n",
    "                                                          ↓\n",
    "                                                    Final Results\n",
    "```\n",
    "\n",
    "**Example: Amazon Search**\n",
    "1. **Real-time**: User types \"wireless headphones\" → Quick retrieval of 1000 candidates using inverted index → Light model scores them in 50ms\n",
    "2. **Batch**: Every 6 hours, deep learning model re-ranks all products based on inventory, seasonality, profit margins\n",
    "3. **Merge**: Combine real-time relevance with batch quality scores\n",
    "\n",
    "---\n",
    "\n",
    "## **23.2 Feature Stores: The Feature Management Layer**\n",
    "\n",
    "A feature store is a centralized repository for storing, serving, and managing machine learning features. It solves the \"training-serving skew\" problem where models perform differently in training versus production due to inconsistent feature computation.\n",
    "\n",
    "### **The Problem: Training-Serving Skew**\n",
    "\n",
    "```python\n",
    "# Training code (Python/Pandas)\n",
    "def compute_user_features(df):\n",
    "    df['avg_purchase_7d'] = df.groupby('user_id')['amount'].rolling(7).mean()\n",
    "    return df\n",
    "\n",
    "# Serving code (Java/Production)\n",
    "public double getAvgPurchase(User user) {\n",
    "    // Different implementation!\n",
    "    return database.average(user.getPurchases(), Days.SEVEN);\n",
    "}\n",
    "\n",
    "# Result: Model expects normalized values, serving code returns raw values\n",
    "# Model performance drops from 95% to 70% accuracy in production!\n",
    "```\n",
    "\n",
    "### **Feature Store Architecture**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                      Feature Store                           │\n",
    "├─────────────────────────┬───────────────────────────────────┤\n",
    "│   Offline Store         │      Online Store                 │\n",
    "│   (Training Data)       │      (Real-time Serving)          │\n",
    "│                         │                                   │\n",
    "│   ┌──────────────┐     │      ┌──────────────┐             │\n",
    "│   │ Data Lake    │     │      │ Redis/Low    │             │\n",
    "│   │ (Parquet/    │     │      │ Latency DB   │             │\n",
    "│   │  Delta Lake) │     │      │              │             │\n",
    "│   └──────────────┘     │      └──────────────┘             │\n",
    "│          ↑             │             ↑                      │\n",
    "│   Feature Pipeline     │      Feature Retrieval API         │\n",
    "│   (Spark/Flink)        │      (GRPC/REST)                   │\n",
    "└─────────────────────────┴───────────────────────────────────┘\n",
    "                    ↓\n",
    "            Feature Registry (Metadata, Lineage)\n",
    "```\n",
    "\n",
    "### **Feast (Open Source Feature Store)**\n",
    "\n",
    "**Defining Features**:\n",
    "```python\n",
    "from feast import Entity, Feature, FeatureView, ValueType\n",
    "from feast.types import Float32, Int64\n",
    "from feast import FileSource, RedisSource\n",
    "\n",
    "# Define the entity (what you're predicting for)\n",
    "user = Entity(\n",
    "    name=\"user_id\",\n",
    "    value_type=ValueType.INT64,\n",
    "    description=\"User identifier\"\n",
    ")\n",
    "\n",
    "# Define data sources\n",
    "user_transactions = FileSource(\n",
    "    path=\"s3://features/user_transactions.parquet\",\n",
    "    event_timestamp_column=\"timestamp\"\n",
    ")\n",
    "\n",
    "# Define feature view (how to compute features)\n",
    "user_stats_view = FeatureView(\n",
    "    name=\"user_transaction_stats\",\n",
    "    entities=[\"user_id\"],\n",
    "    ttl=timedelta(hours=24),  # Feature freshness\n",
    "    features=[\n",
    "        Feature(name=\"avg_transaction_7d\", dtype=Float32),\n",
    "        Feature(name=\"transaction_count_30d\", dtype=Int64),\n",
    "        Feature(name=\"preferred_category\", dtype=String)\n",
    "    ],\n",
    "    online=True,  # Store in Redis for real-time serving\n",
    "    source=user_transactions\n",
    ")\n",
    "```\n",
    "\n",
    "**Training (Offline)**:\n",
    "```python\n",
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "\n",
    "# Get features for training (batch retrieval from S3)\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=user_ids_with_labels,\n",
    "    features=[\n",
    "        \"user_transaction_stats:avg_transaction_7d\",\n",
    "        \"user_transaction_stats:transaction_count_30d\"\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "# train_model(training_df) - Features match production exactly\n",
    "```\n",
    "\n",
    "**Serving (Online)**:\n",
    "```python\n",
    "# Real-time feature retrieval (from Redis, <5ms)\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"user_transaction_stats:avg_transaction_7d\",\n",
    "        \"user_transaction_stats:preferred_category\"\n",
    "    ],\n",
    "    entity_rows=[{\"user_id\": 12345}]\n",
    ").to_dict()\n",
    "\n",
    "# Pass to model for inference\n",
    "prediction = model.predict(features)\n",
    "```\n",
    "\n",
    "### **Feature Engineering at Scale**\n",
    "\n",
    "**Stream Processing for Real-Time Features**:\n",
    "```python\n",
    "# Apache Flink: Compute real-time features as events arrive\n",
    "class TransactionFeatureJob:\n",
    "    def flat_map(self, transaction):\n",
    "        user_id = transaction.user_id\n",
    "        amount = transaction.amount\n",
    "        \n",
    "        # Maintain running average using stateful operators\n",
    "        current_avg = self.state.value()\n",
    "        new_avg = (current_avg * self.state.count + amount) / (self.state.count + 1)\n",
    "        self.state.update(new_avg)\n",
    "        \n",
    "        # Emit feature update\n",
    "        yield FeatureRow(\n",
    "            entity_key=user_id,\n",
    "            feature_name=\"avg_transaction_realtime\",\n",
    "            value=new_avg,\n",
    "            timestamp=transaction.timestamp\n",
    "        )\n",
    "\n",
    "# Updates feature store in real-time as transactions happen\n",
    "```\n",
    "\n",
    "**Feature Validation**:\n",
    "```python\n",
    "# Ensure feature quality before serving\n",
    "from great_expectations import validate\n",
    "\n",
    "expectations = {\n",
    "    \"avg_transaction_7d\": {\n",
    "        \"min_value\": 0,\n",
    "        \"max_value\": 100000,\n",
    "        \"null_rate\": \"< 0.01\"\n",
    "    }\n",
    "}\n",
    "\n",
    "# Validate features before writing to store\n",
    "if not validate(computed_features, expectations):\n",
    "    alert_data_team(\"Feature validation failed!\")\n",
    "    # Don't pollute feature store with bad data\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.3 Model Versioning and A/B Testing**\n",
    "\n",
    "Machine learning models are software artifacts that need versioning, testing, and gradual rollouts—just like regular code, but with additional complexity around data dependencies and performance drift.\n",
    "\n",
    "### **Model Registry (MLflow Example)**\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"accuracy\", 0.95)\n",
    "    mlflow.log_metric(\"f1_score\", 0.94)\n",
    "    mlflow.log_metric(\"inference_latency_ms\", 12)\n",
    "    \n",
    "    # Log model artifact\n",
    "    mlflow.sklearn.log_model(\n",
    "        sk_model=model,\n",
    "        artifact_path=\"model\",\n",
    "        registered_model_name=\"fraud-detection-model\"\n",
    "    )\n",
    "    \n",
    "    # Log feature dependencies\n",
    "    mlflow.log_artifact(\"features.yaml\")  # Schema of expected inputs\n",
    "\n",
    "# Model versions: v1.0 (stable), v1.1 (candidate), v2.0 (experimental)\n",
    "```\n",
    "\n",
    "### **Shadow Deployment: Testing Without Risk**\n",
    "\n",
    "Before serving a new model to users, run it in \"shadow mode\"—process real requests but don't return results to users. Compare shadow model predictions against production model.\n",
    "\n",
    "```\n",
    "User Request → Load Balancer → Production Model (v1.0) → Response to User\n",
    "                    ↓\n",
    "               Shadow Model (v2.0) → Results logged to analytics\n",
    "                    ↓\n",
    "               Compare: v1.0 predicted 0.9 fraud, v2.0 predicted 0.3\n",
    "               If disagreement > threshold, alert data scientists\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "class ShadowModelMiddleware:\n",
    "    def __init__(self):\n",
    "        self.production_model = load_model(\"v1.0\")\n",
    "        self.shadow_model = load_model(\"v2.0\")\n",
    "        self.comparison_queue = []\n",
    "    \n",
    "    def predict(self, request):\n",
    "        # Production path (blocking, must be fast)\n",
    "        prod_result = self.production_model.predict(request)\n",
    "        \n",
    "        # Shadow path (async, non-blocking)\n",
    "        def shadow_predict():\n",
    "            shadow_result = self.shadow_model.predict(request)\n",
    "            self.log_comparison(request, prod_result, shadow_result)\n",
    "        \n",
    "        threading.Thread(target=shadow_predict).start()\n",
    "        \n",
    "        return prod_result  # User only sees production result\n",
    "```\n",
    "\n",
    "### **A/B Testing for ML Models**\n",
    "\n",
    "**Traffic Splitting Strategies**:\n",
    "\n",
    "**1. Random Split (User-level)**:\n",
    "```python\n",
    "def route_request(user_id):\n",
    "    # Consistent hashing ensures same user always hits same model\n",
    "    bucket = hash(user_id) % 100\n",
    "    \n",
    "    if bucket < 50:\n",
    "        return model_v1  # 50% traffic\n",
    "    else:\n",
    "        return model_v2  # 50% traffic\n",
    "```\n",
    "\n",
    "**2. Canary Deployment**:\n",
    "```python\n",
    "def route_request(user_id, context):\n",
    "    # Start with 1% of traffic\n",
    "    if hash(user_id) % 100 == 0:\n",
    "        return model_v2\n",
    "    \n",
    "    # Monitor error rates, latency for 1 hour\n",
    "    # If healthy, increase to 10%, then 50%, then 100%\n",
    "```\n",
    "\n",
    "**3. Multi-Armed Bandit (Automatic Optimization)**:\n",
    "```python\n",
    "# Epsilon-greedy strategy: 90% exploit best model, 10% explore alternatives\n",
    "if random.random() < 0.1:\n",
    "    # Exploration: Try random model to gather data\n",
    "    model = random.choice([model_a, model_b, model_c])\n",
    "else:\n",
    "    # Exploitation: Use model with highest conversion rate\n",
    "    model = get_best_performing_model()\n",
    "\n",
    "# Automatically shifts traffic to best model based on real-time metrics\n",
    "```\n",
    "\n",
    "**Monitoring Model Performance**:\n",
    "```python\n",
    "# Track model drift over time\n",
    "class ModelMonitor:\n",
    "    def check_drift(self, recent_predictions, baseline_distribution):\n",
    "        # Statistical test (Kolmogorov-Smirnov) for distribution shift\n",
    "        drift_score = ks_test(recent_predictions, baseline_distribution)\n",
    "        \n",
    "        if drift_score > 0.1:\n",
    "            alert(\"Model drift detected! Retraining required.\")\n",
    "            # Trigger automated retraining pipeline\n",
    "            self.trigger_retraining()\n",
    "    \n",
    "    def check_latency(self, response_times):\n",
    "        p99_latency = np.percentile(response_times, 99)\n",
    "        if p99_latency > 100:  # SLA: 100ms\n",
    "            alert(\"Latency degradation detected\")\n",
    "            # Roll back to previous version or scale up infrastructure\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.4 Vector Databases: Semantic Search at Scale**\n",
    "\n",
    "Traditional databases search by exact match (SQL `WHERE` clauses) or inverted indexes (Elasticsearch). Vector databases enable \"semantic search\"—finding similar items based on meaning rather than keywords.\n",
    "\n",
    "### **Embeddings: Converting Data to Vectors**\n",
    "\n",
    "An embedding is a numerical representation (vector) of data (text, images, audio) where \"similar\" items are close together in vector space.\n",
    "\n",
    "```\n",
    "Text: \"king\" → [0.2, -0.5, 0.8, ..., 0.1] (768 dimensions)\n",
    "Text: \"queen\" → [0.22, -0.48, 0.79, ..., 0.12] (close to king)\n",
    "Text: \"apple\" → [-0.9, 0.3, -0.2, ..., 0.8] (far from king)\n",
    "```\n",
    "\n",
    "**Generating Embeddings** (OpenAI API):\n",
    "```python\n",
    "import openai\n",
    "\n",
    "def get_embedding(text):\n",
    "    response = openai.Embedding.create(\n",
    "        input=text,\n",
    "        model=\"text-embedding-3-large\"\n",
    "    )\n",
    "    return response['data'][0]['embedding']  # 3072-dimensional vector\n",
    "\n",
    "# Example\n",
    "embedding = get_embedding(\"machine learning system design\")\n",
    "# Returns: [0.012, -0.034, 0.256, ...] (3072 numbers)\n",
    "```\n",
    "\n",
    "### **Approximate Nearest Neighbor (ANN) Search**\n",
    "\n",
    "Finding the exact closest vectors in high-dimensional space is slow (O(n)). ANN algorithms trade 1-2% accuracy for 1000x speedup.\n",
    "\n",
    "**HNSW (Hierarchical Navigable Small World)** - Used by Pinecone, Weaviate:\n",
    "```\n",
    "Layer 2 (Sparse):        A ────────→ D\n",
    "                              ↘\n",
    "Layer 1 (Medium):      A → B → C → D → E\n",
    "                          ↘   ↗\n",
    "Layer 0 (Dense):     A-B-C-D-E-F-G-H-I-J (all connections)\n",
    "\n",
    "Search: Start at top layer, greedily navigate toward query\n",
    "       When no closer nodes, drop to next layer\n",
    "       Repeat until layer 0 (exact local search)\n",
    "       \n",
    "Complexity: O(log n) instead of O(n)\n",
    "```\n",
    "\n",
    "### **Pinecone (Managed Vector DB)**\n",
    "\n",
    "```python\n",
    "import pinecone\n",
    "\n",
    "# Initialize\n",
    "pinecone.init(api_key=\"key\", environment=\"us-west1-gcp\")\n",
    "index = pinecone.Index(\"product-catalog\")\n",
    "\n",
    "# Upsert vectors (product descriptions → embeddings)\n",
    "vectors = [\n",
    "    (\"prod_1\", [0.1, -0.2, ...], {\"category\": \"electronics\", \"price\": 299}),\n",
    "    (\"prod_2\", [0.15, -0.18, ...], {\"category\": \"electronics\", \"price\": 499}),\n",
    "    # ... millions of products\n",
    "]\n",
    "index.upsert(vectors=vectors)\n",
    "\n",
    "# Query: Find similar products\n",
    "query_embedding = get_embedding(\"wireless noise canceling headphones\")\n",
    "results = index.query(\n",
    "    vector=query_embedding,\n",
    "    top_k=10,\n",
    "    filter={\"category\": {\"$eq\": \"electronics\"}, \"price\": {\"$lt\": 400}}\n",
    ")\n",
    "\n",
    "# Returns: prod_1 (score: 0.95), prod_5 (score: 0.92), etc.\n",
    "```\n",
    "\n",
    "### **pgvector (PostgreSQL Extension)**\n",
    "\n",
    "For applications already using PostgreSQL, pgvector adds vector capabilities without new infrastructure.\n",
    "\n",
    "```sql\n",
    "-- Enable extension\n",
    "CREATE EXTENSION vector;\n",
    "\n",
    "-- Create table with vector column\n",
    "CREATE TABLE products (\n",
    "    id SERIAL PRIMARY KEY,\n",
    "    name TEXT,\n",
    "    embedding vector(768)  -- 768 dimensions\n",
    ");\n",
    "\n",
    "-- Create IVFFlat index for fast ANN search\n",
    "CREATE INDEX ON products USING ivfflat (embedding vector_cosine_ops)\n",
    "WITH (lists = 100);  -- Number of clusters (tune based on data size)\n",
    "\n",
    "-- Insert data\n",
    "INSERT INTO products (name, embedding) \n",
    "VALUES ('iPhone 15', '[0.1, -0.2, 0.3, ...]');\n",
    "\n",
    "-- Semantic search (find 5 most similar products)\n",
    "SELECT name, 1 - (embedding <=> '[0.1, -0.15, ...]') AS similarity\n",
    "FROM products\n",
    "WHERE 1 - (embedding <=> '[0.1, -0.15, ...]') > 0.8  -- Similarity threshold\n",
    "ORDER BY embedding <=> '[0.1, -0.15, ...]'\n",
    "LIMIT 5;\n",
    "```\n",
    "\n",
    "### **Vector Database Comparison**\n",
    "\n",
    "```\n",
    "Database     Best For                Latency      Scale\n",
    "─────────────────────────────────────────────────────────\n",
    "Pinecone     Production ML apps      <10ms        Billions\n",
    "Weaviate     Semantic search         <20ms        Millions\n",
    "Milvus       Open-source large scale <50ms        10B+\n",
    "pgvector     Existing Postgres users <100ms       Millions\n",
    "Redis        Real-time caching       <5ms         Millions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.5 LLM Integration: RAG Architecture and Prompt Engineering at Scale**\n",
    "\n",
    "Large Language Models (GPT-4, Claude, Llama) require special architectural patterns due to their size (hundreds of gigabytes), cost (per-token pricing), and latency (1-10 seconds).\n",
    "\n",
    "### **Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "LLMs have knowledge cutoffs and hallucinate facts. RAG grounds LLM responses in your private data.\n",
    "\n",
    "**Architecture**:\n",
    "```\n",
    "User Query → Embedding Model → Vector DB (your docs) → Top-K Chunks → \n",
    "                                                      ↓\n",
    "Prompt: \"Context: [Chunks] Question: [Query]\" → LLM API → Response\n",
    "```\n",
    "\n",
    "**Example: Customer Support Chatbot**\n",
    "```\n",
    "User: \"What's your return policy for electronics?\"\n",
    "\n",
    "1. Convert query to vector: \"return policy electronics\" → [0.2, -0.1, ...]\n",
    "2. Search vector DB: Find return_policy.pdf chunks 3, 7, 12\n",
    "3. Retrieve text chunks:\n",
    "   - \"Electronics can be returned within 30 days...\"\n",
    "   - \"Items must be in original packaging...\"\n",
    "   - \"Refunds processed in 5-7 business days...\"\n",
    "4. Construct prompt:\n",
    "   \"Context: Electronics can be returned within 30 days...\n",
    "    Question: What's your return policy for electronics?\n",
    "    Answer:\"\n",
    "5. Send to LLM (GPT-4)\n",
    "6. Return: \"You can return electronics within 30 days in original packaging...\"\n",
    "```\n",
    "\n",
    "**Implementation**:\n",
    "```python\n",
    "class RAGPipeline:\n",
    "    def __init__(self):\n",
    "        self.vector_store = pinecone.Index(\"company-docs\")\n",
    "        self.llm = OpenAIClient(model=\"gpt-4\")\n",
    "    \n",
    "    def answer_query(self, user_query):\n",
    "        # Step 1: Retrieve relevant documents\n",
    "        query_embedding = self.embed(user_query)\n",
    "        matches = self.vector_store.query(query_embedding, top_k=5)\n",
    "        \n",
    "        # Step 2: Build context\n",
    "        context = \"\\n\\n\".join([match.text for match in matches])\n",
    "        \n",
    "        # Step 3: Construct prompt with context\n",
    "        prompt = f\"\"\"Answer the question based on the following context.\n",
    "If the answer is not in the context, say \"I don't know\".\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {user_query}\n",
    "\n",
    "Answer:\"\"\"\n",
    "\n",
    "        # Step 4: Generate response\n",
    "        response = self.llm.complete(prompt, max_tokens=500)\n",
    "        \n",
    "        return response\n",
    "```\n",
    "\n",
    "### **Prompt Engineering at Scale**\n",
    "\n",
    "**Prompt Versioning and Management**:\n",
    "```python\n",
    "# Don't hardcode prompts in code!\n",
    "# Use a prompt registry (like PromptLayer or simple config)\n",
    "\n",
    "# prompts.yaml\n",
    "prompts:\n",
    "  customer_support:\n",
    "    version: \"2.3\"\n",
    "    template: |\n",
    "      You are a helpful support agent for {company_name}.\n",
    "      Context: {context}\n",
    "      Customer question: {question}\n",
    "      Provide a concise, friendly answer.\n",
    "    parameters:\n",
    "      temperature: 0.7\n",
    "      max_tokens: 300\n",
    "\n",
    "# Code\n",
    "from jinja2 import Template\n",
    "\n",
    "def load_prompt(prompt_id, variables):\n",
    "    prompt_config = registry.get(prompt_id)\n",
    "    template = Template(prompt_config.template)\n",
    "    return template.render(**variables)\n",
    "\n",
    "# A/B test different prompts\n",
    "if user_bucket == \"A\":\n",
    "    prompt = load_prompt(\"customer_support_v2.3\", vars)\n",
    "else:\n",
    "    prompt = load_prompt(\"customer_support_v2.4_experimental\", vars)\n",
    "```\n",
    "\n",
    "**Caching LLM Responses**:\n",
    "```python\n",
    "class LLMCache:\n",
    "    def __init__(self):\n",
    "        self.cache = RedisCache()\n",
    "        self.semantic_cache = VectorStore()  # For similar queries\n",
    "    \n",
    "    def get(self, query):\n",
    "        # Exact match cache (cheap, fast)\n",
    "        if cached := self.cache.get(hash(query)):\n",
    "            return cached\n",
    "        \n",
    "        # Semantic cache (expensive, but catches paraphrases)\n",
    "        query_embedding = embed(query)\n",
    "        similar = self.semantic_cache.similarity_search(query_embedding, threshold=0.95)\n",
    "        if similar:\n",
    "            return similar[0].response  # Return cached response for similar query\n",
    "        \n",
    "        return None\n",
    "\n",
    "# Cache hit saves $0.01-0.10 per request (important at scale!)\n",
    "# Also reduces latency from 2s to 10ms\n",
    "```\n",
    "\n",
    "### **Handling LLM Constraints**\n",
    "\n",
    "**Rate Limiting and Token Management**:\n",
    "```python\n",
    "class LLMRateLimiter:\n",
    "    def __init__(self):\n",
    "        # OpenAI limits: 10,000 requests/minute, 2M tokens/minute\n",
    "        self.request_bucket = TokenBucket(capacity=10000, refill_rate=10000/60)\n",
    "        self.token_bucket = TokenBucket(capacity=2000000, refill_rate=2000000/60)\n",
    "    \n",
    "    def call(self, prompt):\n",
    "        tokens = count_tokens(prompt)\n",
    "        \n",
    "        if not self.request_bucket.consume(1):\n",
    "            raise RateLimitError(\"Too many requests\")\n",
    "        \n",
    "        if not self.token_bucket.consume(tokens):\n",
    "            raise RateLimitError(\"Token quota exceeded\")\n",
    "        \n",
    "        return openai.Completion.create(prompt=prompt)\n",
    "\n",
    "# Queue-based architecture for burst handling\n",
    "class LLMQueue:\n",
    "    def __init__(self):\n",
    "        self.queue = PriorityQueue()\n",
    "        self.workers = [Thread(target=self.process) for _ in range(10)]\n",
    "    \n",
    "    def enqueue(self, request, priority=1):\n",
    "        # Priority: 0 = urgent, 1 = normal, 2 = batch\n",
    "        self.queue.put((priority, time.time(), request))\n",
    "    \n",
    "    def process(self):\n",
    "        while True:\n",
    "            priority, timestamp, request = self.queue.get()\n",
    "            \n",
    "            # Exponential backoff for rate limits\n",
    "            for attempt in range(5):\n",
    "                try:\n",
    "                    response = llm.call(request.prompt)\n",
    "                    request.callback(response)\n",
    "                    break\n",
    "                except RateLimitError:\n",
    "                    time.sleep(2 ** attempt)  # 1s, 2s, 4s, 8s, 16s\n",
    "```\n",
    "\n",
    "**Model Routing (Route to Cheapest/Fastest Model)**:\n",
    "```python\n",
    "def route_query(user_query):\n",
    "    complexity = classify_complexity(user_query)\n",
    "    \n",
    "    if complexity == \"simple\":\n",
    "        # Use GPT-3.5 ($0.002/1K tokens, 100ms latency)\n",
    "        return \"gpt-3.5-turbo\"\n",
    "    elif complexity == \"complex\":\n",
    "        # Use GPT-4 ($0.06/1K tokens, 2s latency)\n",
    "        return \"gpt-4\"\n",
    "    elif complexity == \"coding\":\n",
    "        # Use Claude for code (better at programming)\n",
    "        return \"claude-3-opus\"\n",
    "    \n",
    "    # Default to middle tier\n",
    "    return \"gpt-4-turbo-preview\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.6 Key Takeaways**\n",
    "\n",
    "1. **Choose batch for throughput, real-time for latency**: Batch inference is 10x cheaper but hours stale; online inference provides instant results at higher cost.\n",
    "\n",
    "2. **Feature stores eliminate training-serving skew**: Centralized feature computation ensures models see consistent data in training and production.\n",
    "\n",
    "3. **Shadow deployment validates models safely**: Test new models on real traffic without user impact before A/B testing.\n",
    "\n",
    "4. **Vector databases enable semantic search**: ANN algorithms (HNSW) make billion-scale similarity search feasible in milliseconds.\n",
    "\n",
    "5. **RAG grounds LLMs in facts**: Retrieve relevant context from vector DB to prevent hallucinations and provide up-to-date answers.\n",
    "\n",
    "6. **LLMs require special infrastructure**: Rate limiting, semantic caching, and prompt versioning are essential for production LLM applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "This chapter covered the unique challenges of ML system design. We explored batch versus real-time inference architectures, implemented feature stores with Feast to ensure consistency, and learned to version and A/B test models safely. We examined vector databases for semantic search and built RAG pipelines to ground LLMs in proprietary data.\n",
    "\n",
    "The key insight: ML systems are data systems first. The sophistication of your model matters less than the quality, freshness, and consistency of your features. Invest in data infrastructure (feature stores, monitoring, validation) before chasing marginal model improvements.\n",
    "\n",
    "**Coming up next**: In Chapter 24, we'll explore Edge Computing and IoT—processing data on devices, handling intermittent connectivity, and architecting for the billions of smart devices at the network edge.\n",
    "\n",
    "---\n",
    "\n",
    "## **Exercises**\n",
    "\n",
    "1. **Model Serving Cost Analysis**: Compare costs for serving 10 million predictions daily:\n",
    "   - Option A: Batch processing on GPU spot instances ($0.50/hour, process nightly)\n",
    "   - Option B: Real-time inference on dedicated GPUs ($2.00/hour, always on)\n",
    "   - Option C: Serverless (AWS SageMaker, $0.0001 per inference)\n",
    "   Calculate monthly costs and identify when each option makes sense.\n",
    "\n",
    "2. **Feature Store Design**: Design a feature store schema for a ride-sharing app. Identify 5 real-time features (update within seconds) and 5 batch features (update hourly). Write the Feast feature definitions.\n",
    "\n",
    "3. **Vector Search Implementation**: Implement a semantic search for customer support tickets using pgvector:\n",
    "   - Create table with vector column (384 dimensions using all-MiniLM-L6-v2 model)\n",
    "   - Insert 1000 sample tickets with embeddings\n",
    "   - Write query to find top 5 similar tickets for a new incoming ticket\n",
    "   - Add metadata filtering (by product category)\n",
    "\n",
    "4. **RAG Pipeline**: Build a RAG system that:\n",
    "   - Loads a PDF document, chunks it into 500-token segments\n",
    "   - Stores chunks in Pinecone with embeddings\n",
    "   - Answers user questions using GPT-4 with retrieved context\n",
    "   - Implements semantic caching to avoid repeated LLM calls for similar questions\n",
    "\n",
    "5. **LLM Rate Limiting**: Design a token bucket algorithm for OpenAI API limits (10,000 req/min, 2M tokens/min). Handle burst traffic gracefully using a queue with exponential backoff on rate limit errors.\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
