{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8ac1fe1",
   "metadata": {},
   "source": [
    "# **Chapter 26: Mock Interviews & Solutions**\n",
    "\n",
    "This chapter applies the 4S framework to the most common system design interview questions. Each walkthrough demonstrates exactly how to structure your response, what details to include, and how to navigate trade-offs. Study these patterns, but remember: the goal isn't memorization\u2014it's understanding how to reason through ambiguity.\n",
    "\n",
    "---\n",
    "\n",
    "## **Problem 1: Design a URL Shortener (TinyURL)**\n",
    "\n",
    "**Difficulty**: Easy to Medium  \n",
    "**Key Concepts**: Hashing, Database Sharding, Caching, Analytics  \n",
    "**Estimated Time**: 45 minutes\n",
    "\n",
    "### **SCOPE: Requirements Gathering**\n",
    "\n",
    "> **Candidate**: \"Before I begin, let me clarify the requirements. For functional requirements, I'm assuming:\n",
    "> 1. Users submit a long URL and receive a short alias (e.g., `short.io/abc123`)\n",
    "> 2. Visiting the short URL redirects to the original (301 permanent redirect)\n",
    "> 3. Optional custom aliases (`short.io/my-brand`)\n",
    "> 4. Links expire after a configurable time (default 1 year)\n",
    "> 5. Basic analytics: click count, referrer, geographic distribution\n",
    ">\n",
    "> For non-functional requirements, what scale should I target?\"\n",
    ">\n",
    "> **Interviewer**: \"Design for 100 million new URLs per month, 10 billion redirects per month. Read-heavy.\"\n",
    ">\n",
    "> **Candidate**: \"Got it. And for latency\u2014I'm assuming sub-100ms for redirects is acceptable? Should we support global distribution or start single-region?\"\n",
    ">\n",
    "> **Interviewer**: \"Global distribution for latency, 99.9% availability.\"\n",
    "\n",
    "### **SKETCH: Back-of-the-Envelope Estimation**\n",
    "\n",
    "> \"Let me calculate the scale:\n",
    ">\n",
    "> **Write throughput**: 100M URLs/month \u00f7 2.6M seconds \u2248 **40 URLs/second**. Peak 5\u00d7 = **200/s**.\n",
    ">\n",
    "> **Read throughput**: 10B redirects/month \u00f7 2.6M seconds \u2248 **4,000/s**. Peak 10\u00d7 = **40,000/s**.\n",
    ">\n",
    "> **Storage calculations**:\n",
    "> - Average URL: 500 bytes (long) + 50 bytes (short) + 100 bytes (metadata) = **650 bytes/URL**\n",
    "> - 100M/month \u00d7 12 months \u00d7 5 years = 6B URLs\n",
    "> - 6B \u00d7 650 bytes = **3.9 TB** (plus 30% overhead for indexes = **~5 TB**)\n",
    ">\n",
    "> **Bandwidth**: 40,000 redirects/s \u00d7 500 bytes = **20 MB/s** (160 Mbps)\u2014well within 1 Gbps.\n",
    ">\n",
    "> **Analytics storage**: If we log every click with IP, timestamp, user-agent (~200 bytes):\n",
    "> - 10B clicks/month \u00d7 200 bytes = **2 TB/month** of clickstream data.\n",
    ">\n",
    "> These numbers tell me: single database can handle writes, but we need caching for reads and separate analytics pipeline.\"\n",
    "\n",
    "### **SOLIDIFY: API and Data Model**\n",
    "\n",
    "**API Design**:\n",
    "\n",
    "```http\n",
    "POST /api/v1/shorten\n",
    "Content-Type: application/json\n",
    "\n",
    "{\n",
    "  \"long_url\": \"https://example.com/very/long/path?query=params\",\n",
    "  \"custom_alias\": \"mylink\",        // optional\n",
    "  \"expires_in_days\": 30            // optional, default 365\n",
    "}\n",
    "\n",
    "Response 201 Created:\n",
    "{\n",
    "  \"short_code\": \"mylink\",          // or auto-generated \"a7x9k2\"\n",
    "  \"short_url\": \"https://short.io/mylink\",\n",
    "  \"long_url\": \"https://example.com/...\",\n",
    "  \"created_at\": \"2024-01-15T10:30:00Z\",\n",
    "  \"expires_at\": \"2024-02-14T10:30:00Z\"\n",
    "}\n",
    "\n",
    "Response 409 Conflict:\n",
    "{\n",
    "  \"error\": \"Custom alias already taken\",\n",
    "  \"suggested\": [\"mylink123\", \"mylink2024\"]\n",
    "}\n",
    "```\n",
    "\n",
    "```http\n",
    "GET /{short_code}\n",
    "Response 302 Found:\n",
    "Location: https://example.com/very/long/path\n",
    "[Sets tracking cookie for analytics]\n",
    "\n",
    "Response 410 Gone:\n",
    "{\n",
    "  \"error\": \"Link expired or deleted\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Database Schema**:\n",
    "\n",
    "```sql\n",
    "-- Main URL table (sharded by short_code hash)\n",
    "CREATE TABLE url_mappings (\n",
    "    short_code VARCHAR(10) PRIMARY KEY,\n",
    "    long_url TEXT NOT NULL,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    expires_at TIMESTAMP,\n",
    "    user_id UUID,\n",
    "    click_count BIGINT DEFAULT 0,\n",
    "    is_custom BOOLEAN DEFAULT FALSE\n",
    ");\n",
    "\n",
    "-- Indexes\n",
    "CREATE INDEX idx_expires_at ON url_mappings(expires_at) \n",
    "WHERE expires_at IS NOT NULL;\n",
    "\n",
    "-- Analytics table (time-series, separate database)\n",
    "CREATE TABLE click_events (\n",
    "    event_id BIGSERIAL,\n",
    "    short_code VARCHAR(10),\n",
    "    clicked_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    ip_address INET,\n",
    "    country_code CHAR(2),\n",
    "    referrer TEXT,\n",
    "    user_agent TEXT\n",
    ") PARTITION BY RANGE (clicked_at);\n",
    "```\n",
    "\n",
    "**URL Generation Strategy**:\n",
    "\n",
    "> \"For generating short codes, I have two options:\n",
    ">\n",
    "> **Option A: Hash-based (MD5/SHA)**\n",
    "> - Hash the long URL, take first 7 characters\n",
    "> - Check for collision, if exists append counter\n",
    "> - Pros: Deterministic (same URL \u2192 same short code)\n",
    "> - Cons: Collisions increase with scale, complex collision handling\n",
    ">\n",
    "> **Option B: Base62 Counter**\n",
    "> - Use auto-increment ID (1, 2, 3...) \u2192 convert to Base62 (a-z, A-Z, 0-9)\n",
    "> - 7 characters = 62^7 \u2248 3.5 trillion unique URLs\n",
    "> - Pros: No collisions, sequential (good for B-tree inserts), simple\n",
    "> - Cons: Predictable (sequential IDs expose creation rate), need distributed counter\n",
    ">\n",
    "> **Decision**: I'll use Base62 with a distributed ID generator (Snowflake-style) to avoid single-point-of-failure in the counter. For custom aliases, we reserve those in a separate namespace.\"\n",
    "\n",
    "### **SCALE: Architecture and Deep Dives**\n",
    "\n",
    "**High-Level Architecture**:\n",
    "\n",
    "```\n",
    "[User] \u2192 [DNS/GeoDNS] \u2192 [CDN] \u2192 [Load Balancer]\n",
    "                              \u2193\n",
    "                   [API Servers] (Stateless, Auto-scaling)\n",
    "                    /          \\\n",
    "   [Write Path]  /              \\  [Read Path]\n",
    "                \u2193                \u2193\n",
    "        [ID Generator]      [Redis Cluster]\n",
    "        (Snowflake)         (URL Cache)\n",
    "                \u2193                \u2193\n",
    "        [Database] \u2190\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "     (PostgreSQL/MySQL)   (Cache miss fetch)\n",
    "                \u2193\n",
    "        [Kafka] \u2192 [ClickHouse] (Analytics)\n",
    "```\n",
    "\n",
    "**Deep Dive 1: Database Sharding**\n",
    "\n",
    "> \"With 5TB over 5 years, we might need to shard. I'll use **hash-based sharding** on `short_code` because:\n",
    "> 1. **Uniform distribution**: Random short codes spread evenly\n",
    "> 2. **Direct lookup**: No routing table needed\u2014`shard = hash(code) % N`\n",
    "> 3. **No hot spots**: Unlike time-based sharding where recent data is hot\n",
    ">\n",
    "> **Rebalancing strategy**: Use consistent hashing (virtual nodes) so adding a shard only moves 1/N of data.\"\n",
    "\n",
    "**Deep Dive 2: Caching Strategy**\n",
    "\n",
    "> \"For 40,000 reads/second with 100:1 read ratio:\n",
    "> - **Cache tier**: Redis Cluster with 100GB RAM (holds hot 20% of URLs = 1.2B URLs, likely serving 95% of traffic)\n",
    "> - **TTL strategy**: 24 hours for popular URLs, 1 hour for others\n",
    "> - **Cache warming**: On miss, populate cache asynchronously so next request hits cache\n",
    "> - **Thundering herd**: Use lease tokens\u2014only one thread fetches from DB, others wait or serve stale data temporarily\"\n",
    "\n",
    "**Deep Dive 3: Analytics Pipeline**\n",
    "\n",
    "> \"10B clicks/month = 3,800 clicks/second average, 38,000 peak.\n",
    ">\n",
    "> **Problem**: Writing every click to SQL database would kill performance.\n",
    ">\n",
    "> **Solution**: \n",
    "> 1. **Click logs**: Append-only Kafka topic (high throughput, retention 7 days)\n",
    "> 2. **Stream processing**: Flink jobs aggregate real-time metrics (clicks per URL per hour)\n",
    "> 3. **Data warehouse**: Copy to ClickHouse for ad-hoc analytics (fast columnar queries)\n",
    "> 4. **Pre-aggregation**: Update counter cache in Redis for 'click_count' displayed on dashboard\"\n",
    "\n",
    "**Deep Dive 4: Global Distribution**\n",
    "\n",
    "> \"For global low latency:\n",
    "> 1. **Multi-region deployment**: US-East, US-West, EU, Asia-Pacific\n",
    "> 2. **Database**: Single primary for writes (US-East), read replicas in other regions\n",
    "> 3. **Cache**: Redis in each region, evicted on write via invalidation messages\n",
    "> 4. **CDN**: CloudFront/Cloudflare caches 301 redirects (cacheable indefinitely unless expired)\n",
    ">\n",
    "> **Write latency**: Acceptable\u2014writes are rare (200/s) and don't need to be instant.\n",
    "> **Read latency**: <50ms via CDN + regional cache.\"\n",
    "\n",
    "**Trade-offs Discussed**:\n",
    "\n",
    "> \"I chose 301 (permanent) over 302 (temporary) redirects because:\n",
    "> - **Pros**: Browsers cache 301s forever (faster subsequent visits, less server load)\n",
    "> - **Cons**: Cannot change destination once issued; if we delete a URL, cached 301s still work in browsers\n",
    ">\n",
    "> I chose eventual consistency for analytics (real-time not required) over strong consistency to achieve higher write throughput.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Problem 2: Design Twitter News Feed**\n",
    "\n",
    "**Difficulty**: Hard  \n",
    "**Key Concepts**: Fan-out Problem, Timeline Generation, Push vs. Pull  \n",
    "**Estimated Time**: 45-50 minutes\n",
    "\n",
    "### **SCOPE: Requirements Gathering**\n",
    "\n",
    "> \"For functional requirements:\n",
    "> - Users post tweets (text, images, video)\n",
    "> - Users follow other users\n",
    "> - Home timeline shows tweets from followed users (reverse chronological)\n",
    "> - User timeline shows user's own tweets\n",
    "> - Like, retweet, reply functionality\n",
    ">\n",
    "> Scale targets?\"\n",
    ">\n",
    "> **Interviewer**: \"10 million DAU, average 200 followers per user, 100 million tweets per day.\"\n",
    ">\n",
    "> \"Non-functional: Timeline generation <200ms, post tweet <500ms, available 99.9%.\"\n",
    "\n",
    "### **SKETCH: Estimations**\n",
    "\n",
    "> **Tweet volume**: 100M/day \u00f7 86,400s = **1,150 tweets/second** (peak 5\u00d7 = 5,750/s)\n",
    ">\n",
    "> **Timeline reads**: 10M DAU \u00d7 10 timeline views/day = 100M views/day = **1,150 reads/second** (peak 10\u00d7 = 11,500/s)\n",
    ">\n",
    "> **Storage**:\n",
    "> - Tweet: 140 chars = 280 bytes + metadata = 500 bytes\n",
    "> - Media: Average 200KB per tweet (20% have media)\n",
    "> - 100M tweets \u00d7 (500 bytes + 0.2\u00d7200KB) = **4 TB/day** (1.4 PB/year)\n",
    ">\n",
    "> **Fan-out**: 10M users \u00d7 200 followers = 2 billion potential fan-out edges.\"\n",
    "\n",
    "### **SOLIDIFY: Data Model**\n",
    "\n",
    "**Relational Schema**:\n",
    "\n",
    "```sql\n",
    "-- Users table\n",
    "CREATE TABLE users (\n",
    "    user_id BIGINT PRIMARY KEY,\n",
    "    username VARCHAR(15) UNIQUE NOT NULL,\n",
    "    follower_count INT DEFAULT 0,\n",
    "    following_count INT DEFAULT 0,\n",
    "    created_at TIMESTAMP\n",
    ");\n",
    "\n",
    "-- Follows relationship (graph edge)\n",
    "CREATE TABLE follows (\n",
    "    follower_id BIGINT REFERENCES users(user_id),\n",
    "    following_id BIGINT REFERENCES users(user_id),\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    PRIMARY KEY (follower_id, following_id)\n",
    ");\n",
    "\n",
    "-- Tweets\n",
    "CREATE TABLE tweets (\n",
    "    tweet_id BIGINT PRIMARY KEY,\n",
    "    user_id BIGINT REFERENCES users(user_id),\n",
    "    content TEXT,\n",
    "    media_urls TEXT[], -- Array of URLs\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "    like_count INT DEFAULT 0,\n",
    "    retweet_count INT DEFAULT 0\n",
    ");\n",
    "\n",
    "-- Indexes for timeline queries\n",
    "CREATE INDEX idx_tweets_user_time ON tweets(user_id, created_at DESC);\n",
    "CREATE INDEX idx_follows_follower ON follows(follower_id);\n",
    "```\n",
    "\n",
    "**NoSQL Alternative (Cassandra)**:\n",
    "\n",
    "> \"For the timeline itself, I'll use Cassandra with wide rows:\n",
    "> ```sql\n",
    "> CREATE TABLE user_timeline (\n",
    ">     user_id BIGINT,\n",
    ">     tweet_id BIGINT,\n",
    ">     author_id BIGINT,\n",
    ">     content TEXT,\n",
    ">     created_at TIMESTAMP,\n",
    ">     PRIMARY KEY (user_id, created_at, tweet_id)\n",
    "> ) WITH CLUSTERING ORDER BY (created_at DESC);\n",
    "> ```\n",
    "> This gives us O(1) lookup for a user's timeline by time range.\"\n",
    "\n",
    "### **SCALE: The Fan-Out Problem**\n",
    "\n",
    "**The Core Challenge**:\n",
    "\n",
    "> \"When Kim Kardashian (100M followers) tweets, do we:\n",
    "> 1. **Push model**: Write the tweet to all 100M followers' timelines immediately (write-heavy)\n",
    "> 2. **Pull model**: Fetch tweets from all followed users when timeline is requested (read-heavy)\n",
    "> 3. **Hybrid**: Push to normal users, pull for celebrities?\"\n",
    "\n",
    "**Hybrid Approach (The Solution)**:\n",
    "\n",
    "```\n",
    "Celebrity Threshold: 1 million followers\n",
    "\n",
    "Normal User (500 followers) tweets:\n",
    "  \u2192 Push to 500 timelines immediately (fast, cheap)\n",
    "  \u2192 Read timeline: O(1) fetch from Redis/cache\n",
    "\n",
    "Celebrity (Kim K) tweets:\n",
    "  \u2192 Don't push to anyone\n",
    "  \u2192 When user requests timeline:\n",
    "     1. Fetch tweets from normal follows (from cache)\n",
    "     2. Fetch recent celebrity tweets separately (merge on read)\n",
    "     3. Merge and sort by time\n",
    "```\n",
    "\n",
    "**Architecture Diagram**:\n",
    "\n",
    "```\n",
    "[Post Tweet] \u2192 [Load Balancer] \u2192 [Tweet Service]\n",
    "                     \u2193\n",
    "              [Fan-out Service]\n",
    "               /            \\\n",
    "   [Normal User]              [Celebrity]\n",
    "         \u2193                         \u2193\n",
    "   [Write to Redis]          [Skip push]\n",
    "   (User timelines)           \n",
    "         \u2193\n",
    "   [Async to DB]\n",
    "   \n",
    "[Read Timeline] \u2192 [Timeline Service]\n",
    "       \u2193\n",
    "   [Redis: User timeline]\n",
    "       \u2193\n",
    "   [Merge with celeb tweets]\n",
    "       \u2193\n",
    "   [Return sorted]\n",
    "```\n",
    "\n",
    "**Deep Dive 1: Timeline Generation Algorithm**\n",
    "\n",
    "> **For normal users (Push model)**:\n",
    "> ```python\n",
    "> def post_tweet(user_id, content):\n",
    ">     # 1. Store tweet\n",
    ">     tweet_id = save_to_db(user_id, content)\n",
    ">     \n",
    ">     # 2. Get followers (cached)\n",
    ">     followers = get_followers(user_id)  # 500 users\n",
    ">     \n",
    ">     # 3. Fan out to their timelines (Redis LPUSH)\n",
    ">     for follower in followers:\n",
    ">         redis.lpush(f\"timeline:{follower}\", tweet_id)\n",
    ">         redis.ltrim(f\"timeline:{follower}\", 0, 1000)  # Keep last 1000\n",
    ">     \n",
    ">     return tweet_id\n",
    "> ```\n",
    ">\n",
    "> **For celebrities**:\n",
    "> ```python\n",
    "> def get_timeline(user_id, cursor=None):\n",
    ">     # 1. Get normal follows timeline from Redis (pre-computed)\n",
    ">     normal_tweets = redis.lrange(f\"timeline:{user_id}\", 0, 100)\n",
    ">     \n",
    ">     # 2. Get followed celebrities\n",
    ">     celebs = get_followed_celebrities(user_id)\n",
    ">     \n",
    ">     # 3. Fetch their recent tweets (last 24h) from DB\n",
    ">     celeb_tweets = []\n",
    ">     for celeb in celebs:\n",
    ">         celeb_tweets.extend(\n",
    ">             db.query(\"SELECT * FROM tweets WHERE user_id = ? AND created_at > ? LIMIT 10\",\n",
    ">                     celeb, now() - 24h)\n",
    ">         )\n",
    ">     \n",
    ">     # 4. Merge and sort (heap merge for efficiency)\n",
    ">     all_tweets = merge_sorted([normal_tweets, celeb_tweets], key='created_at')\n",
    ">     \n",
    ">     return all_tweets[:100]  # Return top 100\n",
    "> ```\n",
    "\n",
    "**Deep Dive 2: Media Storage**\n",
    "\n",
    "> \"4TB/day of media requires object storage:\n",
    "> - **Storage**: S3 with lifecycle policies (move to Glacier after 1 year)\n",
    "> - **CDN**: CloudFront for global distribution\n",
    "> - **Processing**: Lambda@Edge to resize images (thumbnail vs full)\n",
    "> - **URL structure**: `https://media.twitter.com/{user_id}/{tweet_id}/{size}.jpg`\n",
    ">   - Sizes: thumb (150\u00d7150), medium (600\u00d7400), large (1200\u00d7800)\"\n",
    "\n",
    "**Deep Dive 3: Counter Consistency**\n",
    "\n",
    "> \"Like counts and retweet counts need to be accurate but don't need real-time precision.\n",
    ">\n",
    "> **Strategy**:\n",
    "> 1. **Write path**: Increment counter in Redis (fast)\n",
    "> 2. **Sync**: Every 10 seconds, flush Redis counters to persistent DB\n",
    "> 3. **Read path**: Read from Redis (approximate) or DB (exact) based on use case\n",
    ">\n",
    "> **Conflict resolution**: If Redis restarts and loses count, recalculate from event log (Kafka) or accept minor inaccuracy temporarily.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Problem 3: Design a Chat System (WhatsApp/Slack)**\n",
    "\n",
    "**Difficulty**: Hard  \n",
    "**Key Concepts**: WebSockets, Message Ordering, Presence, Read Receipts  \n",
    "**Estimated Time**: 50 minutes\n",
    "\n",
    "### **SCOPE**\n",
    "\n",
    "> \"Requirements clarification:\n",
    "> - 1-on-1 messaging and group chats (up to 500 people)\n",
    "> - Online status (last seen)\n",
    "> - Read receipts (double checkmarks)\n",
    "> - Message history (searchable)\n",
    "> - Media sharing (images, files)\n",
    "> - End-to-end encryption (optional but mentioned)\n",
    ">\n",
    "> Scale: 1 billion daily active users, 100 billion messages/day.\"\n",
    "\n",
    "### **SKETCH**\n",
    "\n",
    "> **Messages**: 100B/day \u00f7 86,400 = **1.16 million messages/second** (peak 5M/s)\n",
    ">\n",
    "> **Storage**: Average message 100 bytes + metadata = 200 bytes\n",
    "> - 100B \u00d7 200 bytes = **20 TB/day** (7.3 PB/year)\n",
    ">\n",
    "> **Connections**: 1B users, 20% online simultaneously = **200 million concurrent connections**\"\n",
    "\n",
    "### **SOLIDIFY: Data Model**\n",
    "\n",
    "**Message Table (Cassandra)**:\n",
    "\n",
    "```sql\n",
    "CREATE TABLE messages (\n",
    "    chat_id TEXT,           -- user1_user2 (sorted) or group_id\n",
    "    message_id BIGINT,      -- Snowflake ID (time-based)\n",
    "    sender_id BIGINT,\n",
    "    content TEXT,\n",
    "    media_url TEXT,\n",
    "    created_at TIMESTAMP,\n",
    "    status TINYINT,         -- 0:sent, 1:delivered, 2:read\n",
    "    PRIMARY KEY (chat_id, message_id)\n",
    ") WITH CLUSTERING ORDER BY (message_id DESC);\n",
    "```\n",
    "\n",
    "**User Sessions (Redis)**:\n",
    "\n",
    "```python\n",
    "# Online status\n",
    "SET user:{user_id}:status \"online\" EX 60  # Expires in 60s unless refreshed\n",
    "\n",
    "# Active connections (for routing)\n",
    "SADD user:{user_id}:connections \"ws_server_42\" \"ws_server_15\"\n",
    "\n",
    "# Last seen\n",
    "SET user:{user_id}:last_seen \"2024-01-15T10:30:00Z\"\n",
    "```\n",
    "\n",
    "### **SCALE: Real-Time Architecture**\n",
    "\n",
    "**Connection Handling**:\n",
    "\n",
    "> \"200M concurrent WebSocket connections cannot fit on one server.\n",
    ">\n",
    "> **Architecture**:\n",
    "> - **Gateway Layer**: HAProxy with sticky sessions (IP hash) routes to WebSocket servers\n",
    "> - **WebSocket Servers**: 10,000 servers \u00d7 20,000 connections each = 200M capacity\n",
    "> - **Stateless**: WebSocket servers store only in-memory state (connection mapping). If server crashes, clients reconnect to different server and re-fetch missed messages.\n",
    ">\n",
    "> **Connection Management**:\n",
    "> ```python\n",
    "> class ChatServer:\n",
    ">     def __init__(self):\n",
    ">         self.connections = {}  # user_id -> WebSocket connection\n",
    ">     \n",
    ">     async def handle_connection(self, ws, user_id):\n",
    ">         # Register connection\n",
    ">         self.connections[user_id] = ws\n",
    ">         await redis.sadd(f\"user:{user_id}:connections\", self.server_id)\n",
    ">         \n",
    ">         # Send undelivered messages (offline messages)\n",
    ">         pending = await db.get_pending_messages(user_id)\n",
    ">         for msg in pending:\n",
    ">             await ws.send(msg)\n",
    ">         \n",
    ">         try:\n",
    ">             while True:\n",
    ">                 msg = await ws.recv()\n",
    ">                 await handle_message(user_id, msg)\n",
    ">         except ConnectionClosed:\n",
    ">             await redis.srem(f\"user:{user_id}:connections\", self.server_id)\n",
    ">             del self.connections[user_id]\n",
    "> ```\n",
    "\n",
    "**Message Flow**:\n",
    "\n",
    "```\n",
    "User A (Server 1) \u2192 \"Hello\" \n",
    "    \u2193\n",
    "Message Service validates, stores in DB (Cassandra)\n",
    "    \u2193\n",
    "Pub/Sub (Redis/Kafka): \"new_message:{chat_id}\"\n",
    "    \u2193\n",
    "User B Online? \n",
    "    \u251c\u2500 Yes (Server 5): Push via WebSocket immediately\n",
    "    \u2514\u2500 No: Store in \"pending:{user_id}\" queue, send push notification\n",
    "```\n",
    "\n",
    "**Deep Dive 1: Message Ordering**\n",
    "\n",
    "> \"Problem: Network latency causes messages to arrive out of order.\n",
    ">\n",
    "> **Solution**:\n",
    "> 1. **Server-assigned sequence numbers**: Not client timestamp (clocks drift)\n",
    "> 2. **Snowflake IDs**: Embed timestamp + sequence for ordering\n",
    ">    - 41 bits: timestamp (ms since epoch)\n",
    ">    - 10 bits: machine ID\n",
    ">    - 12 bits: sequence number (4096 messages/ms per machine)\n",
    "> 3. **Causality tracking**: Vector clocks for 'User is typing...' indicators (causal consistency)\"\n",
    "\n",
    "**Deep Dive 2: Group Chats**\n",
    "\n",
    "> \"500 people in a group sending 1 msg/sec = 500 messages/sec to distribute.\n",
    ">\n",
    "> **Optimization**:\n",
    "> - **Fan-out on read vs write**: For small groups (<100), fan-out on write (push to all). For large groups, fan-out on read (pull when user opens app).\n",
    "> - **Presence optimization**: Only send 'typing' indicators to users who have app open (check Redis presence set)\n",
    "> - **Message synchronization**: Use 'sync tokens'\u2014client sends last received message ID, server sends only newer messages (efficient for reconnection)\"\n",
    "\n",
    "**Deep Dive 3: Read Receipts**\n",
    "\n",
    "> \"Double checkmark logic:\n",
    "> 1. **Sent**: Message stored in sender's outbox\n",
    "> 2. **Delivered**: Recipient's server ACK received (store delivery receipt in DB)\n",
    "> 3. **Read**: Recipient opened chat and fetched messages (update status)\n",
    ">\n",
    "> **Scaling read receipts**:\n",
    "> - Don't update DB for every read (write amplification)\n",
    "> - Batch updates: Update 'read' status in Redis, flush to DB every 5 seconds or when user leaves chat\n",
    "> - For group chats: Store 'read_by' as bitmap or separate table (message_id, user_id, read_at)\"\n",
    "\n",
    "---\n",
    "\n",
    "## **Problem 4: Design Uber (Ride Sharing)**\n",
    "\n",
    "**Difficulty**: Hard  \n",
    "**Key Concepts**: Geo-Spatial Indexing, Matching Algorithm, Supply-Demand  \n",
    "**Estimated Time**: 50 minutes\n",
    "\n",
    "### **SCOPE**\n",
    "\n",
    "> \"Functional:\n",
    "> - Riders request rides (real-time tracking)\n",
    "> - Drivers accept/decline requests\n",
    "> - ETA calculation and route optimization\n",
    "> - Surge pricing\n",
    "> - Payment processing (out of scope, just mention)\n",
    ">\n",
    "> Scale: 100 million rides per month, 5 million drivers.\"\n",
    "\n",
    "### **SKETCH**\n",
    "\n",
    "> **Rides**: 100M/month \u00f7 2.6M seconds = **38 rides/second** (peak 200/s)\n",
    ">\n",
    "> **Location updates**: 5M drivers \u00d7 every 4 seconds = **1.25M updates/second**\n",
    ">\n",
    "> **Storage**: GPS points (lat, long, timestamp) = 24 bytes\n",
    "> - 1.25M \u00d7 24 bytes \u00d7 86400 seconds = **2.5 TB/day of location data**\"\n",
    "\n",
    "### **SCALE: Geo-Spatial Architecture**\n",
    "\n",
    "**The Dispatcher Service**:\n",
    "\n",
    "```\n",
    "[Rider Request] \u2192 [Load Balancer] \u2192 [API Gateway]\n",
    "                                          \u2193\n",
    "                                    [Dispatch Service]\n",
    "                                          \u2193\n",
    "                    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "                    \u2193                     \u2193                     \u2193\n",
    "            [Geospatial DB]      [Matching Engine]      [Notification]\n",
    "            (Driver locations)   (Find nearest driver)  (APNS/FCM)\n",
    "```\n",
    "\n",
    "**Geospatial Indexing**:\n",
    "\n",
    "> \"Problem: Find nearest 10 drivers to rider (lat, lng) efficiently.\n",
    ">\n",
    "> **Solution 1: Geohash** (simpler)\n",
    "> - Encode lat/lng to 8-character string (precision ~20 meters)\n",
    "> - Use as Redis key: `drivers:geohash:{hash}`\n",
    "> - Query adjacent 9 cells (center + 8 neighbors) to ensure coverage\n",
    ">\n",
    "> **Solution 2: S2 Geometry** (Google's library, more accurate)\n",
    "> - Hilbert curve space-filling curve\n",
    "> - Cells at multiple levels (level 10 = ~10km, level 16 = ~150m)\n",
    "> - Range queries on cell IDs\n",
    ">\n",
    "> **Implementation**:\n",
    "> ```python\n",
    "> def find_nearest_drivers(rider_lat, rider_lng, radius_m=5000):\n",
    ">     # Get geohash precision based on radius\n",
    ">     precision = geohash_precision(radius_m)  # e.g., 6 chars\n",
    ">     center_hash = geohash.encode(rider_lat, rider_lng, precision)\n",
    ">     \n",
    ">     # Get 8 neighboring cells\n",
    ">     neighbors = geohash.neighbors(center_hash)\n",
    ">     all_hashes = [center_hash] + neighbors\n",
    ">     \n",
    ">     drivers = []\n",
    ">     for hash in all_hashes:\n",
    ">         # Fetch from Redis (drivers in this cell)\n",
    ">         drivers.extend(redis.georadius(f\"cell:{hash}\", rider_lat, rider_lng, radius_m))\n",
    ">     \n",
    ">     # Sort by actual distance, return top 10\n",
    ">     return sorted(drivers, key=lambda d: distance(rider_lat, rider_lng, d.lat, d.lng))[:10]\n",
    "> ```\n",
    "\n",
    "**Matching Algorithm**:\n",
    "\n",
    "> \"Simple: Nearest driver. But reality is complex:\n",
    "> - Driver rating\n",
    "> - Driver preferences (UberX vs Pool)\n",
    "> - Direction (don't match driver going opposite way)\n",
    "> - ETA vs distance (highway vs city streets)\n",
    ">\n",
    "> **Scoring function**:\n",
    "> ```python\n",
    "> def score_driver(driver, rider):\n",
    ">     eta = calculate_eta(driver.location, rider.location)\n",
    ">     distance = haversine(driver.location, rider.location)\n",
    ">     rating_bonus = (driver.rating - 4.5) * 100  # seconds reduction\n",
    ">     \n",
    ">     # Weighted score (lower is better)\n",
    ">     return (eta * 0.6) + (distance * 0.2) - rating_bonus\n",
    "> ```\n",
    "\n",
    "**Location Update Optimization**:\n",
    "\n",
    "> \"1.25M updates/second is massive. Optimizations:\n",
    ">\n",
    "> 1. **Batching**: Drivers send location every 4 seconds, but we batch 10 updates before writing to database (reduce DB writes 10x)\n",
    "> 2. **In-memory storage**: Keep recent locations in Redis (TTL 1 minute), archive to Cassandra for history\n",
    "> 3. **Delta updates**: Only update if moved >50 meters (reduce noise)\n",
    "> 4. **Regional sharding**: US-West drivers don't need to update servers in Asia\"\n",
    "\n",
    "**Surge Pricing**:\n",
    "\n",
    "> \"When demand > supply in a geo-fence (hexagonal grid):\n",
    "> - Calculate supply/demand ratio per cell\n",
    "> - If ratio < 0.5 (2 riders per driver), apply multiplier (1.2x, 1.5x, 2.0x)\n",
    "> - Update every 2 minutes\n",
    "> - Use Kafka to stream price updates to clients\"\n",
    "\n",
    "---\n",
    "\n",
    "## **The Interviewer's Perspective**\n",
    "\n",
    "**What Interviewers Actually Look For**:\n",
    "\n",
    "1. **Structured Thinking**: Did you follow a framework or jump randomly? Candidates who say \"First, let me understand the requirements\" score higher than those who immediately draw boxes.\n",
    "\n",
    "2. **Trade-off Awareness**: The best candidates say \"I could do X or Y. X gives us consistency but higher latency, Y gives speed but potential staleness. Given our requirements for financial transactions, I choose X.\"\n",
    "\n",
    "3. **Practicality**: Junior candidates design for 1 billion users when asked for a startup MVP. Senior candidates say \"Start with PostgreSQL on a single server, shard when we exceed 10k QPS.\"\n",
    "\n",
    "4. **Deep Knowledge**: When you mention Kafka, be ready to explain partition replication, ISR (In-Sync Replicas), and exactly-once semantics. Surface-level buzzwords hurt more than help.\n",
    "\n",
    "5. **Communication**: Do you check if the interviewer is following? Do you adapt when they hint (\"Would that handle the write amplification?\")?\n",
    "\n",
    "**Red Flags**:\n",
    "- Ignoring scale constraints (\"The database will handle it\")\n",
    "- No discussion of failure modes\n",
    "- Inability to calculate basic throughput/storage\n",
    "- Over-complicating simple problems (microservices for a URL shortener)\n",
    "\n",
    "**Green Flags**:\n",
    "- Asking clarifying questions before designing\n",
    "- Back-of-the-envelope math within 2x of correct answer\n",
    "- Discussing monitoring and observability\n",
    "- Admitting uncertainty but reasoning from first principles\n",
    "\n",
    "---\n",
    "\n",
    "## **System Design Checklist in Practice**\n",
    "\n",
    "Apply this checklist to any problem:\n",
    "\n",
    "**Before Drawing**:\n",
    "- [ ] Clarified functional requirements (core features)\n",
    "- [ ] Clarified non-functional (scale, latency, availability)\n",
    "- [ ] Estimated QPS, storage, bandwidth\n",
    "- [ ] Identified read-heavy vs write-heavy\n",
    "\n",
    "**During Design**:\n",
    "- [ ] Defined API contracts (REST/gRPC)\n",
    "- [ ] Designed data schema (tables, NoSQL structures)\n",
    "- [ ] Explained sharding strategy (if needed)\n",
    "- [ ] Justified technology choices (Redis vs Memcached, SQL vs NoSQL)\n",
    "- [ ] Addressed single points of failure\n",
    "- [ ] Discussed caching strategy (what, where, eviction)\n",
    "\n",
    "**Deep Dives**:\n",
    "- [ ] Database: Indexing, replication, partitioning\n",
    "- [ ] Caching: Consistency, thundering herd, cache warming\n",
    "- [ ] Scalability: Horizontal scaling, load balancing, auto-scaling triggers\n",
    "- [ ] Reliability: Circuit breakers, retries, dead letter queues, graceful degradation\n",
    "\n",
    "**Production Readiness**:\n",
    "- [ ] Monitoring: Metrics (latency, errors, saturation), alerting thresholds\n",
    "- [ ] Security: Authentication, authorization, data encryption\n",
    "- [ ] Deployment: Rolling updates, feature flags, rollback strategy\n",
    "\n",
    "---\n",
    "\n",
    "## **Final Advice**\n",
    "\n",
    "**The Day Before**:\n",
    "- Review latency numbers (L1 cache, SSD, network)\n",
    "- Practice one estimation problem (Twitter, Uber, or YouTube)\n",
    "- Sleep >8 hours (cognitive function drops 30% with poor sleep)\n",
    "\n",
    "**During the Interview**:\n",
    "- Bring a notebook (write down requirements so you don't forget)\n",
    "- Speak slowly (nervousness makes people rush)\n",
    "- Validate with interviewer (\"Does this approach make sense for the scale we discussed?\")\n",
    "\n",
    "**Remember**: You're not building the perfect system. You're demonstrating that you can think systematically about complex problems, make justified trade-offs, and communicate technical concepts clearly.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "We walked through four distinct problems\u2014URL shortener (hashing/caching), Twitter (fan-out), WhatsApp (real-time messaging), and Uber (geo-spatial)\u2014demonstrating how to apply the 4S framework to diverse domains. Each solution emphasized different architectural patterns while maintaining the same structured approach.\n",
    "\n",
    "The system design interview is a conversation about constraints, not a test of memorization. Master the fundamentals, practice the framework, and approach each problem with curiosity rather than anxiety.\n",
    "\n",
    "**Congratulations**: You've completed the System Design Handbook. You now possess the architectural knowledge, communication frameworks, and analytical tools to design systems that power the modern internet.\n",
    "\n",
    "---\n",
    "\n",
    "**Final Exercise**: Pick one problem from this chapter. Record yourself explaining it in 45 minutes. Watch the recording and ask: \"Would I hire this person?\" Iterate until the answer is yes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='25. interview_strategy_and_communication.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <span style='color:gray; font-size:1.05em;'>Next</span>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}