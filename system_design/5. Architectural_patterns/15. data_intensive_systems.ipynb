{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c4d0409",
   "metadata": {},
   "source": [
    "# Chapter 15: Data-Intensive Systems\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Design batch processing pipelines using MapReduce and modern frameworks\n",
    "- Build real-time stream processing applications with Apache Flink and Kafka Streams\n",
    "- Choose between Lambda and Kappa architectures for big data systems\n",
    "- Orchestrate complex data pipelines using Apache Airflow\n",
    "- Differentiate between OLTP and OLAP systems and select appropriate technologies\n",
    "- Implement data warehousing solutions with columnar storage\n",
    "- Handle the challenges of processing petabyte-scale data\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction: The Data Explosion**\n",
    "\n",
    "We live in the age of data. Every click, swipe, purchase, and sensor reading generates data. Modern systems must process terabytes to petabytes of data efficiently, whether analyzing historical trends or reacting to events in real-time.\n",
    "\n",
    "### **The Two Worlds of Data Processing**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              BATCH VS STREAM PROCESSING                          \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  BATCH PROCESSING                                               \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Characteristics:                                               \u2502\n",
    "\u2502  \u2022 Process large volumes of historical data                     \u2502\n",
    "\u2502  \u2022 High latency acceptable (minutes to hours)                   \u2502\n",
    "\u2502  \u2022 High throughput                                              \u2502\n",
    "\u2502  \u2022 Complex aggregations and joins                               \u2502\n",
    "\u2502  \u2022 Scheduled or triggered                                       \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Examples:                                                      \u2502\n",
    "\u2502  \u2022 Daily sales reports                                          \u2502\n",
    "\u2502  \u2022 Monthly billing calculations                                 \u2502\n",
    "\u2502  \u2022 Machine learning model training                              \u2502\n",
    "\u2502  \u2022 Data warehouse ETL                                           \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Tools: Hadoop, Spark, MapReduce, Airflow                       \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  STREAM PROCESSING                                              \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Characteristics:                                               \u2502\n",
    "\u2502  \u2022 Process data in real-time as it arrives                      \u2502\n",
    "\u2502  \u2022 Low latency required (milliseconds to seconds)               \u2502\n",
    "\u2502  \u2022 Continuous processing                                          \u2502\n",
    "\u2502  \u2022 Event-by-event or micro-batches                              \u2502\n",
    "\u2502  \u2022 Stateful operations (windows, aggregations)                  \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Examples:                                                      \u2502\n",
    "\u2502  \u2022 Real-time fraud detection                                    \u2502\n",
    "\u2502  \u2022 Stock price monitoring                                       \u2502\n",
    "\u2502  \u2022 IoT sensor processing                                        \u2502\n",
    "\u2502  \u2022 Live dashboard updates                                       \u2502\n",
    "\u2502  \u2022 Clickstream analysis                                         \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Tools: Flink, Kafka Streams, Spark Streaming, Storm            \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Batch Processing with MapReduce**\n",
    "\n",
    "MapReduce is the foundational paradigm for distributed batch processing, introduced by Google and popularized by Hadoop.\n",
    "\n",
    "### **The MapReduce Paradigm**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    MAPREDUCE WORKFLOW                            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Input Data (Distributed across cluster)                        \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "\u2502  \u2502 Split 1 \u2502 \u2502 Split 2 \u2502 \u2502 Split 3 \u2502 \u2502 Split 4 \u2502               \u2502\n",
    "\u2502  \u2502 (64MB)  \u2502 \u2502 (64MB)  \u2502 \u2502 (64MB)  \u2502 \u2502 (64MB)  \u2502               \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "\u2502       \u2502           \u2502           \u2502           \u2502                      \u2502\n",
    "\u2502       \u2193           \u2193           \u2193           \u2193                      \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510               \u2502\n",
    "\u2502  \u2502  Map    \u2502 \u2502  Map    \u2502 \u2502  Map    \u2502 \u2502  Map    \u2502               \u2502\n",
    "\u2502  \u2502 Task 1  \u2502 \u2502 Task 2  \u2502 \u2502 Task 3  \u2502 \u2502 Task 4  \u2502               \u2502\n",
    "\u2502  \u2502         \u2502 \u2502         \u2502 \u2502         \u2502 \u2502         \u2502               \u2502\n",
    "\u2502  \u2502 Input:  \u2502 \u2502 Input:  \u2502 \u2502 Input:  \u2502 \u2502 Input:  \u2502               \u2502\n",
    "\u2502  \u2502 K1,V1   \u2502 \u2502 K2,V2   \u2502 \u2502 K3,V3   \u2502 \u2502 K4,V4   \u2502               \u2502\n",
    "\u2502  \u2502         \u2502 \u2502         \u2502 \u2502         \u2502 \u2502         \u2502               \u2502\n",
    "\u2502  \u2502 Output: \u2502 \u2502 Output: \u2502 \u2502 Output: \u2502 \u2502 Output: \u2502               \u2502\n",
    "\u2502  \u2502 K1,[V1] \u2502 \u2502 K2,[V2] \u2502 \u2502 K1,[V3] \u2502 \u2502 K3,[V4] \u2502               \u2502\n",
    "\u2502  \u2502         \u2502 \u2502         \u2502 \u2502 K2,[V5] \u2502 \u2502         \u2502               \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518               \u2502\n",
    "\u2502       \u2502           \u2502           \u2502           \u2502                      \u2502\n",
    "\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                      \u2502\n",
    "\u2502                         \u2502                                        \u2502\n",
    "\u2502                         \u2193                                        \u2502\n",
    "\u2502                  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                 \u2502\n",
    "\u2502                  \u2502   Shuffle   \u2502 \u2190 Sort and group by key         \u2502\n",
    "\u2502                  \u2502   & Sort    \u2502                                 \u2502\n",
    "\u2502                  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n",
    "\u2502                         \u2502                                        \u2502\n",
    "\u2502       \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                    \u2502\n",
    "\u2502       \u2193                 \u2193                 \u2193                    \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510      \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                 \u2502\n",
    "\u2502  \u2502 Reduce  \u2502      \u2502 Reduce  \u2502      \u2502 Reduce  \u2502                 \u2502\n",
    "\u2502  \u2502 Task 1  \u2502      \u2502 Task 2  \u2502      \u2502 Task 3  \u2502                 \u2502\n",
    "\u2502  \u2502         \u2502      \u2502         \u2502      \u2502         \u2502                 \u2502\n",
    "\u2502  \u2502 Input:  \u2502      \u2502 Input:  \u2502      \u2502 Input:  \u2502                 \u2502\n",
    "\u2502  \u2502 K1,[V1,\u2502      \u2502 K2,[V2, \u2502      \u2502 K3,[V4] \u2502                 \u2502\n",
    "\u2502  \u2502    V3]  \u2502      \u2502    V5]  \u2502      \u2502         \u2502                 \u2502\n",
    "\u2502  \u2502         \u2502      \u2502         \u2502      \u2502         \u2502                 \u2502\n",
    "\u2502  \u2502 Output: \u2502      \u2502 Output: \u2502      \u2502 Output: \u2502                 \u2502\n",
    "\u2502  \u2502 K1,Sum  \u2502      \u2502 K2,Sum  \u2502      \u2502 K3,Sum  \u2502                 \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518      \u2514\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2518                 \u2502\n",
    "\u2502       \u2502                \u2502                \u2502                        \u2502\n",
    "\u2502       \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                        \u2502\n",
    "\u2502                        \u2193                                        \u2502\n",
    "\u2502                 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                                 \u2502\n",
    "\u2502                 \u2502   Output    \u2502                                 \u2502\n",
    "\u2502                 \u2502   Files     \u2502                                 \u2502\n",
    "\u2502                 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518                                 \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  MapReduce Word Count Example:                                  \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Map:   Input  \u2192 (word, 1) for each word                        \u2502\n",
    "\u2502  Shuffle: Group by word                                          \u2502\n",
    "\u2502  Reduce: (word, [1,1,1,...]) \u2192 (word, sum)                       \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Modern Alternative: Apache Spark**\n",
    "\n",
    "Spark is the modern successor to Hadoop MapReduce, offering better performance and easier APIs.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum as spark_sum, count\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OrderAnalytics\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read data (from S3, HDFS, etc.)\n",
    "orders_df = spark.read.parquet(\"s3://data-lake/orders/\")\n",
    "\n",
    "# Process with DataFrame API (optimized)\n",
    "result = orders_df \\\n",
    "    .groupBy(\"customer_id\") \\\n",
    "    .agg(\n",
    "        spark_sum(\"total\").alias(\"lifetime_value\"),\n",
    "        count(\"*\").alias(\"order_count\")\n",
    "    ) \\\n",
    "    .filter(col(\"lifetime_value\") > 1000)\n",
    "\n",
    "# Write results\n",
    "result.write.mode(\"overwrite\").parquet(\"s3://analytics/high-value-customers/\")\n",
    "\n",
    "# Stop Spark\n",
    "spark.stop()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Stream Processing**\n",
    "\n",
    "Real-time processing requires different tools than batch.\n",
    "\n",
    "### **Apache Flink**\n",
    "\n",
    "Flink is the leading stream processing framework, offering true streaming (not micro-batching).\n",
    "\n",
    "```java\n",
    "// Flink Java API example\n",
    "StreamExecutionEnvironment env = \n",
    "    StreamExecutionEnvironment.getExecutionEnvironment();\n",
    "\n",
    "// Source: Kafka\n",
    "DataStream<OrderEvent> orders = env\n",
    "    .addSource(new FlinkKafkaConsumer<>(\n",
    "        \"orders-topic\",\n",
    "        new OrderDeserializationSchema(),\n",
    "        properties\n",
    "    ));\n",
    "\n",
    "// Processing: Windowed aggregation\n",
    "DataStream<OrderStats> stats = orders\n",
    "    .keyBy(OrderEvent::getProductId)\n",
    "    .window(TumblingEventTimeWindows.of(Time.minutes(5)))\n",
    "    .aggregate(new OrderAggregator());\n",
    "\n",
    "// Sink: DynamoDB\n",
    "stats.addSink(new DynamoDBSink<>(\n",
    "    \"order-stats-table\",\n",
    "    new StatsSerializer()\n",
    "));\n",
    "\n",
    "env.execute(\"Order Analytics Job\");\n",
    "```\n",
    "\n",
    "### **Kafka Streams**\n",
    "\n",
    "For simpler stream processing, Kafka Streams provides a lightweight library.\n",
    "\n",
    "```java\n",
    "// Kafka Streams DSL\n",
    "StreamsBuilder builder = new StreamsBuilder();\n",
    "\n",
    "KStream<String, Order> orders = builder.stream(\"orders\");\n",
    "\n",
    "// Enrich with customer data\n",
    "KTable<String, Customer> customers = builder.table(\"customers\");\n",
    "\n",
    "KStream<String, EnrichedOrder> enriched = orders\n",
    "    .leftJoin(customers, (order, customer) -> \n",
    "        new EnrichedOrder(order, customer)\n",
    "    );\n",
    "\n",
    "// Filter and aggregate\n",
    "enriched\n",
    "    .filter((key, value) -> value.getAmount() > 100)\n",
    "    .groupBy((key, value) -> value.getCategory())\n",
    "    .windowedBy(TimeWindows.of(Duration.ofMinutes(5)))\n",
    "    .aggregate(\n",
    "        () -> 0.0,\n",
    "        (key, value, aggregate) -> aggregate + value.getAmount(),\n",
    "        Materialized.as(\"category-revenue\")\n",
    "    );\n",
    "\n",
    "KafkaStreams streams = new KafkaStreams(builder.build(), props);\n",
    "streams.start();\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Lambda vs Kappa Architecture**\n",
    "\n",
    "Two competing paradigms for big data systems.\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502              LAMBDA VS KAPPA ARCHITECTURE                        \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  LAMBDA ARCHITECTURE                                            \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502                    Data Source                           \u2502   \u2502\n",
    "\u2502  \u2502                   (Kafka, Kinesis, etc.)                  \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                          \u2502                                      \u2502\n",
    "\u2502          \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510                     \u2502\n",
    "\u2502          \u2193               \u2193               \u2193                     \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510 \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510          \u2502\n",
    "\u2502  \u2502 Speed Layer  \u2502 \u2502 Batch Layer  \u2502 \u2502 Serving Layer \u2502          \u2502\n",
    "\u2502  \u2502 (Stream)     \u2502 \u2502 (Batch)      \u2502 \u2502 (Query)      \u2502          \u2502\n",
    "\u2502  \u2502              \u2502 \u2502              \u2502 \u2502              \u2502          \u2502\n",
    "\u2502  \u2502 \u2022 Real-time  \u2502 \u2502 \u2022 Hadoop/    \u2502 \u2502 \u2022 Presto/    \u2502          \u2502\n",
    "\u2502  \u2502   views      \u2502 \u2502   Spark      \u2502 \u2502   Druid      \u2502          \u2502\n",
    "\u2502  \u2502 \u2022 Low        \u2502 \u2502 \u2022 Accurate   \u2502 \u2502 \u2022 Merged     \u2502          \u2502\n",
    "\u2502  \u2502   latency    \u2502 \u2502   historical \u2502 \u2502   results    \u2502          \u2502\n",
    "\u2502  \u2502 \u2022 Approximate\u2502 \u2502   data       \u2502 \u2502              \u2502          \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518 \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518          \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Problems:                                                      \u2502\n",
    "\u2502  \u2022 Complex: Two codebases (batch + stream)                    \u2502\n",
    "\u2502  \u2022 Expensive: Duplicate processing                              \u2502\n",
    "\u2502  \u2022 Inconsistent: Different results from batch vs speed        \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  KAPPA ARCHITECTURE                                             \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502                    Data Source                           \u2502   \u2502\n",
    "\u2502  \u2502                   (Kafka, Kinesis)                        \u2502   \u2502\n",
    "\u2502  \u2502                   (Immutable Log)                        \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                          \u2502                                      \u2502\n",
    "\u2502                          \u2193                                      \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502              Stream Processing Layer                     \u2502   \u2502\n",
    "\u2502  \u2502                                                          \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Single codebase for all processing                   \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Reprocess entire log when needed (rebuild state)     \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Kafka Streams, Flink, Spark Streaming                \u2502   \u2502\n",
    "\u2502  \u2502                                                          \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                          \u2502                                      \u2502\n",
    "\u2502                          \u2193                                      \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502              Serving Layer (Materialized Views)          \u2502   \u2502\n",
    "\u2502  \u2502                                                          \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Queryable state stores                                \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Real-time and historical from same pipeline          \u2502   \u2502\n",
    "\u2502  \u2502                                                          \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Advantages:                                                    \u2502\n",
    "\u2502  \u2022 Simpler: Single processing pipeline                          \u2502\n",
    "\u2502  \u2022 Consistent: Same code for real-time and batch                \u2502\n",
    "\u2502  \u2022 Replayable: Can reprocess history by rewinding log            \u2502\n",
    "\u2502  \u2022 Scalable: Log-centric architecture scales well               \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  When to Use Kappa:                                             \u2502\n",
    "\u2502  \u2022 Event-sourced systems                                          \u2502\n",
    "\u2502  \u2022 Real-time analytics with historical replay needs               \u2502\n",
    "\u2502  \u2022 Audit trails and compliance                                    \u2502\n",
    "\u2502  \u2022 When you want to avoid Lambda complexity                       \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Kappa Architecture Implementation:**\n",
    "\n",
    "```python\n",
    "# Kafka Streams - Kappa Architecture\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "\n",
    "class KappaProcessor:\n",
    "    \"\"\"\n",
    "    Kappa architecture processor using Kafka.\n",
    "    Single pipeline for both real-time and historical processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, bootstrap_servers):\n",
    "        self.consumer = KafkaConsumer(\n",
    "            'events-topic',\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            auto_offset_reset='earliest',  # Can rewind to beginning\n",
    "            enable_auto_commit=False,       # Manual commit for exactly-once\n",
    "            group_id='kappa-processor'\n",
    "        )\n",
    "        \n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "        )\n",
    "        \n",
    "        # Materialized view (state store)\n",
    "        self.state_store = {}\n",
    "    \n",
    "    def process_event(self, event):\n",
    "        \"\"\"\n",
    "        Process single event and update materialized view.\n",
    "        Same code runs for both real-time and replay.\n",
    "        \"\"\"\n",
    "        event_type = event.get('type')\n",
    "        user_id = event.get('user_id')\n",
    "        \n",
    "        if event_type == 'page_view':\n",
    "            # Update view count\n",
    "            if user_id not in self.state_store:\n",
    "                self.state_store[user_id] = {'views': 0, 'purchases': 0}\n",
    "            self.state_store[user_id]['views'] += 1\n",
    "            \n",
    "        elif event_type == 'purchase':\n",
    "            # Update purchase count\n",
    "            if user_id not in self.state_store:\n",
    "                self.state_store[user_id] = {'views': 0, 'purchases': 0}\n",
    "            self.state_store[user_id]['purchases'] += 1\n",
    "            \n",
    "            # Emit derived event\n",
    "            self.producer.send('user-metrics', {\n",
    "                'user_id': user_id,\n",
    "                'conversion_rate': self.calculate_conversion(user_id)\n",
    "            })\n",
    "    \n",
    "    def calculate_conversion(self, user_id):\n",
    "        \"\"\"Calculate conversion rate for user.\"\"\"\n",
    "        data = self.state_store.get(user_id, {})\n",
    "        views = data.get('views', 0)\n",
    "        purchases = data.get('purchases', 0)\n",
    "        return purchases / views if views > 0 else 0\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Main processing loop.\"\"\"\n",
    "        print(\"Starting Kappa processor...\")\n",
    "        print(\"Can replay from beginning by resetting offset\")\n",
    "        \n",
    "        for message in self.consumer:\n",
    "            try:\n",
    "                event = json.loads(message.value.decode('utf-8'))\n",
    "                self.process_event(event)\n",
    "                \n",
    "                # Commit offset (mark as processed)\n",
    "                self.consumer.commit()\n",
    "                \n",
    "            except Exception as e:\n",
    "                print(f\"Error processing event: {e}\")\n",
    "    \n",
    "    def replay_from_beginning(self):\n",
    "        \"\"\"Replay all events from beginning (Kappa advantage).\"\"\"\n",
    "        print(\"Replaying all events from beginning...\")\n",
    "        self.consumer.seek_to_beginning()\n",
    "        self.state_store.clear()  # Reset state\n",
    "        self.run()\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    processor = KappaProcessor(['localhost:9092'])\n",
    "    \n",
    "    # Normal real-time processing\n",
    "    # processor.run()\n",
    "    \n",
    "    # Replay historical data (Kappa advantage)\n",
    "    # processor.replay_from_beginning()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Data Pipeline Orchestration**\n",
    "\n",
    "Apache Airflow is the industry standard for orchestrating data pipelines.\n",
    "\n",
    "### **Airflow Architecture**\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    APACHE AIRFLOW ARCHITECTURE                   \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502                    Web Server (Flask)                    \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 UI for monitoring and manual triggers                \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 REST API                                             \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                              \u2502                                   \u2502\n",
    "\u2502                              \u2193                                   \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502                    Scheduler                              \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Parses DAGs (Directed Acyclic Graphs)                  \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Determines task execution order                        \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Queues tasks to Executor                               \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                              \u2502                                   \u2502\n",
    "\u2502                              \u2193                                   \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502                    Executor                             \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Executes tasks                                       \u2502   \u2502\n",
    "\u2502  \u2502  Types:                                                 \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 SequentialExecutor (single process, dev only)        \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 LocalExecutor (parallel on single machine)          \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 CeleryExecutor (distributed, production)             \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 KubernetesExecutor (K8s native)                     \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                              \u2502                                   \u2502\n",
    "\u2502                              \u2193                                   \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502                    Workers                              \u2502   \u2502\n",
    "\u2502  \u2502  (Celery workers or Kubernetes pods)                    \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Execute actual tasks                                 \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Report status back to metadata DB                    \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510   \u2502\n",
    "\u2502  \u2502                    Metadata Database                    \u2502   \u2502\n",
    "\u2502  \u2502  (PostgreSQL, MySQL)                                    \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 DAG definitions                                      \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Task execution history                               \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 Task states (queued, running, success, failed)       \u2502   \u2502\n",
    "\u2502  \u2502  \u2022 XComs (cross-communication between tasks)            \u2502   \u2502\n",
    "\u2502  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Airflow DAG Example:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.operators.s3 import S3FileTransformOperator\n",
    "from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\n",
    "from airflow.utils.dates import days_ago\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Default arguments for all tasks\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'execution_timeout': timedelta(hours=2),\n",
    "}\n",
    "\n",
    "# Define the DAG\n",
    "dag = DAG(\n",
    "    'daily_sales_etl',\n",
    "    default_args=default_args,\n",
    "    description='Daily sales data ETL pipeline',\n",
    "    schedule_interval='0 2 * * *',  # Run at 2 AM daily\n",
    "    start_date=days_ago(1),\n",
    "    catchup=False,\n",
    "    tags=['sales', 'etl', 'daily'],\n",
    "    max_active_runs=1,  # Don't run multiple instances simultaneously\n",
    ")\n",
    "\n",
    "# Task 1: Extract from source database\n",
    "def extract_sales_data(ds, **kwargs):\n",
    "    \"\"\"\n",
    "    Extract yesterday's sales data.\n",
    "    ds is the execution date (YYYY-MM-DD).\n",
    "    \"\"\"\n",
    "    from sqlalchemy import create_engine\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Connect to source database\n",
    "    engine = create_engine('postgresql://user:pass@source-db:5432/sales')\n",
    "    \n",
    "    # Extract data for execution date\n",
    "    query = f\"\"\"\n",
    "    SELECT \n",
    "        order_id,\n",
    "        customer_id,\n",
    "        product_id,\n",
    "        quantity,\n",
    "        price,\n",
    "        order_date,\n",
    "        status\n",
    "    FROM orders\n",
    "    WHERE DATE(order_date) = '{ds}'\n",
    "    \"\"\"\n",
    "    \n",
    "    df = pd.read_sql(query, engine)\n",
    "    \n",
    "    # Save to temporary location for next task\n",
    "    output_path = f'/tmp/sales_raw_{ds}.csv'\n",
    "    df.to_csv(output_path, index=False)\n",
    "    \n",
    "    # Push to XCom for downstream tasks\n",
    "    kwargs['ti'].xcom_push(key='raw_data_path', value=output_path)\n",
    "    \n",
    "    return f\"Extracted {len(df)} records for {ds}\"\n",
    "\n",
    "extract_task = PythonOperator(\n",
    "    task_id='extract_sales',\n",
    "    python_callable=extract_sales_data,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 2: Transform data\n",
    "def transform_sales_data(ds, **kwargs):\n",
    "    \"\"\"Clean and transform sales data.\"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get input from previous task\n",
    "    ti = kwargs['ti']\n",
    "    input_path = ti.xcom_pull(task_ids='extract_sales', key='raw_data_path')\n",
    "    \n",
    "    # Read data\n",
    "    df = pd.read_csv(input_path)\n",
    "    \n",
    "    # Transformations\n",
    "    df['total_amount'] = df['quantity'] * df['price']\n",
    "    df['discount'] = df.apply(calculate_discount, axis=1)\n",
    "    df['final_amount'] = df['total_amount'] - df['discount']\n",
    "    \n",
    "    # Data quality checks\n",
    "    if df['final_amount'].min() < 0:\n",
    "        raise ValueError(\"Negative amounts detected!\")\n",
    "    \n",
    "    # Save transformed data\n",
    "    output_path = f'/tmp/sales_transformed_{ds}.parquet'\n",
    "    df.to_parquet(output_path, index=False)\n",
    "    \n",
    "    # Push to XCom\n",
    "    ti.xcom_push(key='transformed_path', value=output_path)\n",
    "    \n",
    "    return f\"Transformed {len(df)} records\"\n",
    "\n",
    "def calculate_discount(row):\n",
    "    \"\"\"Calculate discount based on rules.\"\"\"\n",
    "    if row['quantity'] > 10:\n",
    "        return row['total_amount'] * 0.1\n",
    "    elif row['customer_id'].startswith('VIP'):\n",
    "        return row['total_amount'] * 0.05\n",
    "    return 0\n",
    "\n",
    "transform_task = PythonOperator(\n",
    "    task_id='transform_sales',\n",
    "    python_callable=transform_sales_data,\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 3: Load to data warehouse\n",
    "load_task = PythonOperator(\n",
    "    task_id='load_to_warehouse',\n",
    "    python_callable=lambda ds, **kwargs: print(f\"Loading data for {ds}\"),\n",
    "    provide_context=True,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Task 4: Send notification\n",
    "def send_success_notification(ds, **kwargs):\n",
    "    \"\"\"Send Slack notification on success.\"\"\"\n",
    "    import requests\n",
    "    \n",
    "    webhook_url = \"https://hooks.slack.com/services/...\"\n",
    "    message = {\n",
    "        \"text\": f\"\u2705 Daily sales ETL completed successfully for {ds}\"\n",
    "    }\n",
    "    \n",
    "    requests.post(webhook_url, json=message)\n",
    "    return \"Notification sent\"\n",
    "\n",
    "notify_task = PythonOperator(\n",
    "    task_id='send_notification',\n",
    "    python_callable=send_success_notification,\n",
    "    provide_context=True,\n",
    "    trigger_rule='all_success',  # Only run if all upstream succeed\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Define dependencies\n",
    "extract_task >> transform_task >> load_task >> notify_task\n",
    "\n",
    "# Alternative: Parallel tasks after extract\n",
    "# extract_task >> [transform_task, another_task] >> load_task\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **OLTP vs OLAP**\n",
    "\n",
    "Understanding the difference between transactional and analytical processing.\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    OLTP VS OLAP                                  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  OLTP (Online Transaction Processing)                           \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Purpose: Handle day-to-day transactions                        \u2502\n",
    "\u2502  Examples: Order processing, banking transactions, user updates   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Characteristics:                                               \u2502\n",
    "\u2502  \u2022 High write volume                                            \u2502\n",
    "\u2502  \u2022 Simple queries (single row lookups)                          \u2502\n",
    "\u2502  \u2022 ACID compliance required                                     \u2502\n",
    "\u2502  \u2022 Low latency for individual operations                        \u2502\n",
    "\u2502  \u2022 Row-oriented storage                                         \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Technologies:                                                  \u2502\n",
    "\u2502  \u2022 PostgreSQL, MySQL (relational)                               \u2502\n",
    "\u2502  \u2022 DynamoDB, Cassandra (NoSQL)                                  \u2502\n",
    "\u2502  \u2022 Spanner, CockroachDB (distributed SQL)                       \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  OLAP (Online Analytical Processing)                            \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Purpose: Analyze historical data for business intelligence     \u2502\n",
    "\u2502  Examples: Sales trends, user behavior analysis, reporting        \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Characteristics:                                               \u2502\n",
    "\u2502  \u2022 High read volume                                             \u2502\n",
    "\u2502  \u2022 Complex queries (aggregations, joins across tables)          \u2502\n",
    "\u2502  \u2022 Eventual consistency acceptable                              \u2502\n",
    "\u2502  \u2022 Higher latency acceptable (seconds to minutes)               \u2502\n",
    "\u2502  \u2022 Column-oriented storage (efficient for aggregations)         \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Technologies:                                                  \u2502\n",
    "\u2502  \u2022 Snowflake, BigQuery, Redshift (cloud warehouses)             \u2502\n",
    "\u2502  \u2022 Apache Druid, ClickHouse (real-time analytics)             \u2502\n",
    "\u2502  \u2022 Presto/Trino (federated query engine)                        \u2502\n",
    "\u2502  \u2022 Apache Pinot (low-latency OLAP)                                \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500   \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  HYBRID APPROACHES:                                             \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  HTAP (Hybrid Transactional/Analytical Processing):             \u2502\n",
    "\u2502  \u2022 Single system for both OLTP and OLAP                         \u2502\n",
    "\u2502  \u2022 Examples: TiDB, SingleStore, AlloyDB                           \u2502\n",
    "\u2502  \u2022 Trade-off: Neither optimized as pure OLTP or OLAP              \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2502  Lambda Architecture (from earlier):                              \u2502\n",
    "\u2502  \u2022 Speed layer (stream) + Batch layer (historical)              \u2502\n",
    "\u2502  \u2022 Separate systems optimized for each                          \u2502\n",
    "\u2502                                                                 \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "**Columnar Storage Example (Why OLAP is Fast):**\n",
    "\n",
    "```\n",
    "Row-Oriented (OLTP - PostgreSQL):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 ID \u2502 Name  \u2502 Age \u2502 City     \u2502 Salary \u2502 Dept            \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502 1  \u2502 Alice \u2502 30  \u2502 NYC      \u2502 100000 \u2502 Engineering     \u2502\n",
    "\u2502 2  \u2502 Bob   \u2502 25  \u2502 LA       \u2502 80000  \u2502 Sales           \u2502\n",
    "\u2502 3  \u2502 Carol \u2502 35  \u2502 Chicago  \u2502 120000 \u2502 Engineering     \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Query: SELECT AVG(salary) FROM employees WHERE dept = 'Engineering'\n",
    "Must read: All rows, then filter, then aggregate\n",
    "I/O: Read entire table (inefficient for analytics)\n",
    "\n",
    "Column-Oriented (OLAP - Parquet/ClickHouse):\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 ID       \u2502  \u2502 Name     \u2502  \u2502 Age      \u2502  \u2502 Salary   \u2502\n",
    "\u2502 [1,2,3]  \u2502  \u2502[A,B,C]   \u2502  \u2502[30,25,35]\u2502  \u2502[100,80,120]\u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510  \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502 City     \u2502  \u2502 Dept     \u2502\n",
    "\u2502[NYC,LA,Chi]\u2502  \u2502[Eng,Sal,Eng]\u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518  \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "\n",
    "Query: SELECT AVG(salary) FROM employees WHERE dept = 'Engineering'\n",
    "1. Read only 'Dept' column (compressed, fast)\n",
    "2. Find positions where Dept = 'Engineering' (positions 0, 2)\n",
    "3. Read only 'Salary' column at positions 0, 2\n",
    "4. Calculate average\n",
    "\n",
    "I/O: Read 2 columns only (much faster!)\n",
    "Compression: Columnar data compresses better (similar values together)\n",
    "Vectorization: CPU can process column data in parallel\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "### **Key Takeaways**\n",
    "\n",
    "| Concept | Summary |\n",
    "|---------|---------|\n",
    "| **Batch Processing** | MapReduce is foundational; Spark is modern standard; process historical data |\n",
    "| **Stream Processing** | Flink for complex event processing; Kafka Streams for simpler cases; real-time |\n",
    "| **Lambda Architecture** | Separate speed (stream) and batch layers; complex but accurate |\n",
    "| **Kappa Architecture** | Single stream pipeline; simpler; replay log for reprocessing |\n",
    "| **Data Orchestration** | Airflow for workflow management; DAGs define dependencies |\n",
    "| **OLTP vs OLAP** | OLTP for transactions (row-oriented); OLAP for analytics (column-oriented) |\n",
    "| **Data Warehouses** | Snowflake, BigQuery, Redshift for large-scale analytics |\n",
    "| **Cold Start Mitigation** | Provisioned concurrency, keep-alive, lazy loading, connection pooling |\n",
    "\n",
    "### **Technology Selection Guide**\n",
    "\n",
    "| Use Case | Recommended Technology |\n",
    "|----------|------------------------|\n",
    "| Batch ETL | Apache Spark, AWS Glue |\n",
    "| Real-time streaming | Apache Flink, Kafka Streams |\n",
    "| Simple workflows | AWS Step Functions |\n",
    "| Complex DAGs | Apache Airflow, Prefect |\n",
    "| Ad-hoc analytics | Presto/Trino, BigQuery |\n",
    "| Real-time OLAP | Apache Druid, ClickHouse |\n",
    "| Data warehouse | Snowflake, Redshift, BigQuery |\n",
    "| Event storage | Kafka, AWS Kinesis |\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** In Chapter 16, we'll explore **Real-World System Design Case Studies**, applying everything we've learned to design systems like a URL Shortener, Twitter News Feed, Chat Application, Video Streaming Service, and more.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='14. serverless_and_cloud_native_architecture.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../6. Real_world_system_design_case_studies/16. user_facing_applications.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}