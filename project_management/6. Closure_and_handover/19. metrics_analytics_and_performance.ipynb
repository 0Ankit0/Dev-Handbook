{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8dc636d",
   "metadata": {},
   "source": [
    "# **Chapter 19: Metrics, Analytics, and Performance**\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Implement the four DORA metrics to measure and improve software delivery performance\n",
    "- Calculate and interpret flow metrics (Cycle Time, Throughput, WIP Age) to identify bottlenecks\n",
    "- Distinguish between vanity metrics and actionable metrics that drive improvement\n",
    "- Build predictive models to forecast project completion dates using Monte Carlo simulations\n",
    "- Create automated metric collection pipelines using Python and Prometheus\n",
    "- Design balanced scorecards that combine delivery speed with quality and team health\n",
    "- Use metrics to facilitate data-driven conversations rather than blame-oriented evaluations\n",
    "\n",
    "---\n",
    "\n",
    "## **Real-World Case Study: The Velocity Trap**\n",
    "\n",
    "In 2022, **CodeStream Inc.** was struggling. Their engineering team of 40 developers had adopted Agile with zeal—they were measuring velocity, burning down charts, and holding daily standups. But despite \"high velocity\" (75 story points per sprint), they had a problem:\n",
    "\n",
    "- **Customers were furious**: Features took 6 months to reach production despite 2-week sprints\n",
    "- **Developers were burned out**: Constant overtime to \"make the numbers look good\"\n",
    "- **Quality was abysmal**: Production incidents increased 300% year-over-year\n",
    "- **The CEO was confused**: \"We have high velocity, why can't we ship?\"\n",
    "\n",
    "The issue was their metrics. They were optimizing for **story points completed** (a vanity metric) while ignoring **lead time** (how long ideas took to reach customers) and **change failure rate** (how often deployments broke things).\n",
    "\n",
    "When they switched to DORA metrics and flow analytics, the truth emerged:\n",
    "- **Lead Time**: 142 days (industry elite is <1 hour)\n",
    "- **Change Failure Rate**: 45% (industry elite is <5%)\n",
    "- **Deployment Frequency**: Once every 3 weeks (elite is on-demand/multiple per day)\n",
    "\n",
    "They weren't high-performing—they were high-chaos.\n",
    "\n",
    "Over the next 12 months, by focusing on the right metrics and eliminating bottlenecks, they achieved:\n",
    "- Lead time: 142 days → 3 days\n",
    "- Deployment frequency: 3 weeks → 4 times daily\n",
    "- Change failure rate: 45% → 8%\n",
    "- And ironically, actual velocity (business value delivered) increased 400%\n",
    "\n",
    "This chapter teaches you how to measure what actually matters.\n",
    "\n",
    "---\n",
    "\n",
    "## **19.1 DORA Metrics: The Four Keys**\n",
    "\n",
    "The DevOps Research and Assessment (DORA) team identified four metrics that predict software delivery performance. These are the gold standard for measuring engineering effectiveness.\n",
    "\n",
    "### **The Four DORA Metrics**\n",
    "\n",
    "| Metric | Definition | Elite | High | Medium | Low |\n",
    "|--------|-----------|-------|------|--------|-----|\n",
    "| **Deployment Frequency** | How often code is deployed | On demand (multiple/day) | 1/day - 1/week | 1/week - 1/month | <1/month |\n",
    "| **Lead Time for Changes** | Time from commit to production | <1 hour | 1 day - 1 week | 1 week - 1 month | >1 month |\n",
    "| **Mean Time to Recovery (MTTR)** | Time to recover from failure | <1 hour | <1 day | <1 day | >1 week |\n",
    "| **Change Failure Rate** | Percentage of deployments causing failures | <5% | 5-15% | 16-30% | >30% |\n",
    "\n",
    "**Why These Four?**\n",
    "They balance speed (frequency, lead time) against stability (MTTR, failure rate). Optimizing for one without the other creates the \"velocity trap\" CodeStream fell into.\n",
    "\n",
    "---\n",
    "\n",
    "### **Metric 1: Deployment Frequency**\n",
    "\n",
    "**Definition**: The number of times code is deployed to production in a given time period.\n",
    "\n",
    "**Why It Matters**: High frequency means smaller batches, less risk, faster feedback.\n",
    "\n",
    "**How to Calculate**:\n",
    "```python\n",
    "# Simple calculation\n",
    "deployment_frequency = number_of_deployments / number_of_days\n",
    "\n",
    "# Example: 20 deployments in 30 days = 0.67 deployments/day\n",
    "# Or: 1 deployment every 1.5 days\n",
    "```\n",
    "\n",
    "**Industry Context**:\n",
    "- **Elite**: Multiple deployments per day\n",
    "- **High**: Weekly deployments\n",
    "- **Medium**: Monthly deployments\n",
    "- **Low**: Quarterly or less\n",
    "\n",
    "**Code Snippet: Deployment Frequency Tracker**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from collections import defaultdict\n",
    "from typing import List, Dict\n",
    "import requests\n",
    "\n",
    "class DeploymentFrequencyCalculator:\n",
    "    def __init__(self, deployments: List[datetime]):\n",
    "        \"\"\"\n",
    "        deployments: List of deployment timestamps\n",
    "        \"\"\"\n",
    "        self.deployments = sorted(deployments)\n",
    "    \n",
    "    def calculate_daily_rate(self, days: int = 30) -> float:\n",
    "        \"\"\"Calculate average deployments per day over last N days\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        recent = [d for d in self.deployments if d > cutoff]\n",
    "        return len(recent) / days\n",
    "    \n",
    "    def calculate_lead_time_between_deployments(self) -> timedelta:\n",
    "        \"\"\"Calculate average time between deployments\"\"\"\n",
    "        if len(self.deployments) < 2:\n",
    "            return timedelta(0)\n",
    "        \n",
    "        intervals = []\n",
    "        for i in range(1, len(self.deployments)):\n",
    "            diff = self.deployments[i] - self.deployments[i-1]\n",
    "            intervals.append(diff)\n",
    "        \n",
    "        avg_interval = sum(intervals, timedelta(0)) / len(intervals)\n",
    "        return avg_interval\n",
    "    \n",
    "    def get_trend(self, window_days: int = 7) -> Dict:\n",
    "        \"\"\"Calculate trend (improving, declining, stable)\"\"\"\n",
    "        now = datetime.now()\n",
    "        periods = 4\n",
    "        \n",
    "        rates = []\n",
    "        for i in range(periods):\n",
    "            start = now - timedelta(days=(i+1)*window_days)\n",
    "            end = now - timedelta(days=i*window_days)\n",
    "            period_deploys = [d for d in self.deployments if start <= d < end]\n",
    "            rates.append(len(period_deploys))\n",
    "        \n",
    "        # Simple trend analysis\n",
    "        if rates[0] > rates[-1] * 1.2:\n",
    "            trend = \"improving\"\n",
    "        elif rates[0] < rates[-1] * 0.8:\n",
    "            trend = \"declining\"\n",
    "        else:\n",
    "            trend = \"stable\"\n",
    "        \n",
    "        return {\n",
    "            \"current_rate\": rates[0] / window_days,\n",
    "            \"previous_rate\": rates[-1] / window_days,\n",
    "            \"trend\": trend,\n",
    "            \"weekly_counts\": rates\n",
    "        }\n",
    "    \n",
    "    def classify_performance(self) -> str:\n",
    "        \"\"\"Classify according to DORA standards\"\"\"\n",
    "        daily_rate = self.calculate_daily_rate(30)\n",
    "        weekly_rate = daily_rate * 7\n",
    "        \n",
    "        if daily_rate >= 1:  # At least daily\n",
    "            return \"elite\"\n",
    "        elif weekly_rate >= 1:  # At least weekly\n",
    "            return \"high\"\n",
    "        elif weekly_rate >= 0.25:  # At least monthly\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "\n",
    "# Example usage\n",
    "deployments = [\n",
    "    datetime(2025, 3, 1, 10, 0),\n",
    "    datetime(2025, 3, 1, 14, 30),\n",
    "    datetime(2025, 3, 2, 9, 15),\n",
    "    datetime(2025, 3, 2, 16, 0),\n",
    "    datetime(2025, 3, 3, 11, 0),\n",
    "    # ... more deployments\n",
    "]\n",
    "\n",
    "calc = DeploymentFrequencyCalculator(deployments)\n",
    "print(f\"Daily Rate: {calc.calculate_daily_rate():.2f}\")\n",
    "print(f\"Performance Level: {calc.classify_performance()}\")\n",
    "print(f\"Trend: {calc.get_trend()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Metric 2: Lead Time for Changes**\n",
    "\n",
    "**Definition**: The time it takes for a code change to reach production from the moment the developer commits the code.\n",
    "\n",
    "**Why It Matters**: Shorter lead times mean faster feedback, faster learning, and less work-in-progress inventory.\n",
    "\n",
    "**How to Calculate**:\n",
    "```python\n",
    "lead_time = deployment_timestamp - first_commit_timestamp\n",
    "\n",
    "# For multiple changes in one deployment:\n",
    "# Either average them, or track separately\n",
    "```\n",
    "\n",
    "**The Components of Lead Time**:\n",
    "```\n",
    "Total Lead Time = \n",
    "    Code Review Time + \n",
    "    Build Time + \n",
    "    Test Time + \n",
    "    Deployment Time + \n",
    "    Queue Time (waiting)\n",
    "```\n",
    "\n",
    "**Code Snippet: Lead Time Calculator**\n",
    "\n",
    "```python\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "@dataclass\n",
    "class Change:\n",
    "    commit_id: str\n",
    "    commit_timestamp: datetime\n",
    "    deployment_timestamp: Optional[datetime]\n",
    "    pull_request_merged: Optional[datetime] = None\n",
    "    \n",
    "    def calculate_lead_time(self) -> float:\n",
    "        \"\"\"Calculate lead time in hours\"\"\"\n",
    "        if not self.deployment_timestamp:\n",
    "            return 0.0\n",
    "        \n",
    "        delta = self.deployment_timestamp - self.commit_timestamp\n",
    "        return delta.total_seconds() / 3600\n",
    "    \n",
    "    def calculate_queue_time(self) -> float:\n",
    "        \"\"\"Time spent waiting (not being worked on)\"\"\"\n",
    "        if not self.pull_request_merged or not self.deployment_timestamp:\n",
    "            return 0.0\n",
    "        \n",
    "        # Time between merge and deployment is often queue time\n",
    "        delta = self.deployment_timestamp - self.pull_request_merged\n",
    "        return delta.total_seconds() / 3600\n",
    "\n",
    "class LeadTimeAnalyzer:\n",
    "    def __init__(self, changes: List[Change]):\n",
    "        self.changes = changes\n",
    "    \n",
    "    def get_average_lead_time(self, days: int = 30) -> float:\n",
    "        \"\"\"Calculate average lead time in hours\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        recent = [c for c in self.changes \n",
    "                  if c.deployment_timestamp and c.deployment_timestamp > cutoff]\n",
    "        \n",
    "        if not recent:\n",
    "            return 0.0\n",
    "        \n",
    "        times = [c.calculate_lead_time() for c in recent]\n",
    "        return sum(times) / len(times)\n",
    "    \n",
    "    def get_percentile_lead_time(self, percentile: float = 0.85) -> float:\n",
    "        \"\"\"Calculate Nth percentile lead time (e.g., 85th percentile)\"\"\"\n",
    "        times = [c.calculate_lead_time() for c in self.changes \n",
    "                if c.deployment_timestamp]\n",
    "        times.sort()\n",
    "        \n",
    "        index = int(len(times) * percentile)\n",
    "        return times[index] if index < len(times) else 0.0\n",
    "    \n",
    "    def classify_performance(self) -> str:\n",
    "        \"\"\"Classify according to DORA\"\"\"\n",
    "        avg_hours = self.get_average_lead_time()\n",
    "        \n",
    "        if avg_hours < 1:\n",
    "            return \"elite\"\n",
    "        elif avg_hours < 24 * 7:  # Less than a week\n",
    "            return \"high\"\n",
    "        elif avg_hours < 24 * 30:  # Less than a month\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def identify_bottlenecks(self) -> Dict[str, float]:\n",
    "        \"\"\"Identify where time is being spent\"\"\"\n",
    "        total_lead = self.get_average_lead_time()\n",
    "        avg_queue = sum(c.calculate_queue_time() for c in self.changes) / len(self.changes)\n",
    "        \n",
    "        queue_percentage = (avg_queue / total_lead * 100) if total_lead > 0 else 0\n",
    "        \n",
    "        return {\n",
    "            \"average_lead_time_hours\": total_lead,\n",
    "            \"queue_time_hours\": avg_queue,\n",
    "            \"queue_percentage\": queue_percentage,\n",
    "            \"bottleneck\": \"queue\" if queue_percentage > 50 else \"process\"\n",
    "        }\n",
    "\n",
    "# Example\n",
    "changes = [\n",
    "    Change(\n",
    "        commit_id=\"abc123\",\n",
    "        commit_timestamp=datetime(2025, 3, 1, 9, 0),\n",
    "        pull_request_merged=datetime(2025, 3, 1, 11, 0),\n",
    "        deployment_timestamp=datetime(2025, 3, 1, 14, 0)\n",
    "    ),\n",
    "    # Lead time: 5 hours\n",
    "]\n",
    "\n",
    "analyzer = LeadTimeAnalyzer(changes)\n",
    "print(f\"Average Lead Time: {analyzer.get_average_lead_time():.1f} hours\")\n",
    "print(f\"85th Percentile: {analyzer.get_percentile_lead_time(0.85):.1f} hours\")\n",
    "print(f\"Bottleneck Analysis: {analyzer.identify_bottlenecks()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Metric 3: Mean Time to Recovery (MTTR)**\n",
    "\n",
    "**Definition**: The average time it takes to restore service after a failure or incident.\n",
    "\n",
    "**Why It Matters**: Stuff breaks. What matters is how fast you fix it. Low MTTR enables risk-taking and innovation.\n",
    "\n",
    "**Calculation**:\n",
    "```python\n",
    "MTTR = sum(recovery_times) / number_of_incidents\n",
    "\n",
    "# Or for time-weighted:\n",
    "# Total downtime minutes / number of incidents\n",
    "```\n",
    "\n",
    "**Code Snippet: Incident Response Tracker**\n",
    "\n",
    "```python\n",
    "from datetime import datetime\n",
    "from dataclasses import dataclass\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class Incident:\n",
    "    id: str\n",
    "    start_time: datetime\n",
    "    detection_time: datetime  # When monitoring alerted\n",
    "    resolution_time: datetime  # When service restored\n",
    "    severity: str  # P0, P1, P2, P3\n",
    "    \n",
    "    def time_to_detect(self) -> float:\n",
    "        \"\"\"Time to detect in minutes (MTTD)\"\"\"\n",
    "        delta = self.detection_time - self.start_time\n",
    "        return delta.total_seconds() / 60\n",
    "    \n",
    "    def time_to_recover(self) -> float:\n",
    "        \"\"\"Time to recover in minutes (MTTR)\"\"\"\n",
    "        delta = self.resolution_time - self.start_time\n",
    "        return delta.total_seconds() / 60\n",
    "    \n",
    "    def time_to_resolve(self) -> float:\n",
    "        \"\"\"Time to fully resolve (not just mitigate)\"\"\"\n",
    "        # If you track full resolution separately\n",
    "        return self.time_to_recover()\n",
    "\n",
    "class IncidentAnalyzer:\n",
    "    def __init__(self, incidents: List[Incident]):\n",
    "        self.incidents = incidents\n",
    "    \n",
    "    def calculate_mttr(self, days: int = 30, severity: str = None) -> float:\n",
    "        \"\"\"Calculate MTTR in minutes\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        \n",
    "        filtered = [i for i in self.incidents \n",
    "                   if i.resolution_time > cutoff]\n",
    "        \n",
    "        if severity:\n",
    "            filtered = [i for i in filtered if i.severity == severity]\n",
    "        \n",
    "        if not filtered:\n",
    "            return 0.0\n",
    "        \n",
    "        times = [i.time_to_recover() for i in filtered]\n",
    "        return sum(times) / len(times)\n",
    "    \n",
    "    def calculate_mttd(self, days: int = 30) -> float:\n",
    "        \"\"\"Calculate Mean Time To Detect\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        recent = [i for i in self.incidents if i.start_time > cutoff]\n",
    "        \n",
    "        if not recent:\n",
    "            return 0.0\n",
    "        \n",
    "        times = [i.time_to_detect() for i in recent]\n",
    "        return sum(times) / len(times)\n",
    "    \n",
    "    def classify_performance(self) -> str:\n",
    "        \"\"\"DORA classification\"\"\"\n",
    "        mttr_minutes = self.calculate_mttr()\n",
    "        \n",
    "        if mttr_minutes < 60:  # Less than 1 hour\n",
    "            return \"elite\"\n",
    "        elif mttr_minutes < 60 * 24:  # Less than 1 day\n",
    "            return \"high\"\n",
    "        elif mttr_minutes < 60 * 24 * 7:  # Less than 1 week\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def trend_analysis(self) -> Dict:\n",
    "        \"\"\"Analyze if MTTR is improving or worsening\"\"\"\n",
    "        # Compare last 30 days vs previous 30 days\n",
    "        current = self.calculate_mttr(30)\n",
    "        previous = self.calculate_mttr(60)  # This is simplified\n",
    "        \n",
    "        if current < previous * 0.8:\n",
    "            return {\"trend\": \"improving\", \"change\": f\"{(1-current/previous)*100:.1f}%\"}\n",
    "        elif current > previous * 1.2:\n",
    "            return {\"trend\": \"worsening\", \"change\": f\"{(current/previous-1)*100:.1f}%\"}\n",
    "        else:\n",
    "            return {\"trend\": \"stable\", \"change\": \"0%\"}\n",
    "\n",
    "# Example usage\n",
    "incidents = [\n",
    "    Incident(\n",
    "        id=\"INC-001\",\n",
    "        start_time=datetime(2025, 3, 1, 14, 0),\n",
    "        detection_time=datetime(2025, 3, 1, 14, 5),  # 5 min detection\n",
    "        resolution_time=datetime(2025, 3, 1, 14, 45),  # 45 min recovery\n",
    "        severity=\"P1\"\n",
    "    )\n",
    "]\n",
    "\n",
    "analyzer = IncidentAnalyzer(incidents)\n",
    "print(f\"MTTR: {analyzer.calculate_mttr():.0f} minutes\")\n",
    "print(f\"MTTD: {analyzer.calculate_mttd():.0f} minutes\")\n",
    "print(f\"Performance: {analyzer.classify_performance()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **Metric 4: Change Failure Rate**\n",
    "\n",
    "**Definition**: The percentage of deployments that result in a failure (rollback, hotfix, service degradation) in production.\n",
    "\n",
    "**Why It Matters**: Measures quality of releases. High rates mean unstable systems and firefighting instead of building.\n",
    "\n",
    "**Calculation**:\n",
    "```python\n",
    "change_failure_rate = (failed_deployments / total_deployments) * 100\n",
    "\n",
    "# What counts as \"failure\":\n",
    "# - Rollback\n",
    "# - Hotfix required\n",
    "# - Service degradation/incident\n",
    "# - Failed deployment (didn't reach traffic)\n",
    "```\n",
    "\n",
    "**Code Snippet: Change Failure Rate Calculator**\n",
    "\n",
    "```python\n",
    "from enum import Enum\n",
    "from typing import List\n",
    "from dataclasses import dataclass\n",
    "\n",
    "class DeploymentStatus(Enum):\n",
    "    SUCCESS = \"success\"\n",
    "    FAILED = \"failed\"\n",
    "    ROLLED_BACK = \"rolled_back\"\n",
    "    HOTFIX_REQUIRED = \"hotfix_required\"\n",
    "\n",
    "@dataclass\n",
    "class Deployment:\n",
    "    id: str\n",
    "    timestamp: datetime\n",
    "    status: DeploymentStatus\n",
    "    commits: List[str]\n",
    "    \n",
    "    def is_failure(self) -> bool:\n",
    "        \"\"\"Determine if this deployment counts as a failure\"\"\"\n",
    "        return self.status in [\n",
    "            DeploymentStatus.FAILED,\n",
    "            DeploymentStatus.ROLLED_BACK,\n",
    "            DeploymentStatus.HOTFIX_REQUIRED\n",
    "        ]\n",
    "\n",
    "class ChangeFailureRateCalculator:\n",
    "    def __init__(self, deployments: List[Deployment]):\n",
    "        self.deployments = deployments\n",
    "    \n",
    "    def calculate_cfr(self, days: int = 30) -> float:\n",
    "        \"\"\"Calculate Change Failure Rate as percentage\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        recent = [d for d in self.deployments if d.timestamp > cutoff]\n",
    "        \n",
    "        if not recent:\n",
    "            return 0.0\n",
    "        \n",
    "        failures = len([d for d in recent if d.is_failure()])\n",
    "        return (failures / len(recent)) * 100\n",
    "    \n",
    "    def classify_performance(self) -> str:\n",
    "        \"\"\"DORA classification\"\"\"\n",
    "        cfr = self.calculate_cfr()\n",
    "        \n",
    "        if cfr < 5:\n",
    "            return \"elite\"\n",
    "        elif cfr < 15:\n",
    "            return \"high\"\n",
    "        elif cfr < 30:\n",
    "            return \"medium\"\n",
    "        else:\n",
    "            return \"low\"\n",
    "    \n",
    "    def failure_analysis(self) -> Dict:\n",
    "        \"\"\"Analyze patterns in failures\"\"\"\n",
    "        failures = [d for d in self.deployments if d.is_failure()]\n",
    "        \n",
    "        # Group by day of week\n",
    "        dow_failures = defaultdict(int)\n",
    "        for f in failures:\n",
    "            dow_failures[f.timestamp.strftime(\"%A\")] += 1\n",
    "        \n",
    "        # Group by hour\n",
    "        hour_failures = defaultdict(int)\n",
    "        for f in failures:\n",
    "            hour_failures[f.timestamp.hour] += 1\n",
    "        \n",
    "        return {\n",
    "            \"total_failures\": len(failures),\n",
    "            \"most_risky_day\": max(dow_failures.items(), key=lambda x: x[1])[0] if dow_failures else \"N/A\",\n",
    "            \"most_risky_hour\": max(hour_failures.items(), key=lambda x: x[1])[0] if hour_failures else \"N/A\",\n",
    "            \"failure_rate_trend\": \"stable\"  # Would calculate from historical data\n",
    "        }\n",
    "\n",
    "# Example\n",
    "deployments = [\n",
    "    Deployment(\"dep-1\", datetime(2025, 3, 1), DeploymentStatus.SUCCESS, [\"abc\"]),\n",
    "    Deployment(\"dep-2\", datetime(2025, 3, 2), DeploymentStatus.ROLLED_BACK, [\"def\"]),\n",
    "    Deployment(\"dep-3\", datetime(2025, 3, 3), DeploymentStatus.SUCCESS, [\"ghi\"]),\n",
    "]\n",
    "\n",
    "calc = ChangeFailureRateCalculator(deployments)\n",
    "print(f\"Change Failure Rate: {calc.calculate_cfr():.1f}%\")\n",
    "print(f\"Performance: {calc.classify_performance()}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **19.2 Flow Metrics**\n",
    "\n",
    "While DORA measures outcomes, Flow Metrics (from Lean/Kanban) measure the movement of work through your system.\n",
    "\n",
    "### **The Four Flow Metrics**\n",
    "\n",
    "**1. Work in Progress (WIP)**: Count of items started but not finished\n",
    "**2. Cycle Time**: Time from \"In Progress\" to \"Done\"\n",
    "**3. Throughput**: Number of items completed per time unit\n",
    "**4. Work Item Age**: How long current WIP has been in progress\n",
    "\n",
    "**Little's Law**: The fundamental relationship between these metrics:\n",
    "```\n",
    "Average Cycle Time = Average WIP / Average Throughput\n",
    "```\n",
    "\n",
    "This means: To reduce cycle time, either reduce WIP or increase throughput.\n",
    "\n",
    "### **Code Snippet: Flow Metrics Dashboard**\n",
    "\n",
    "```python\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict\n",
    "from dataclasses import dataclass\n",
    "import statistics\n",
    "\n",
    "@dataclass\n",
    "class WorkItem:\n",
    "    id: str\n",
    "    created_at: datetime\n",
    "    started_at: datetime\n",
    "    completed_at: datetime\n",
    "    type: str  # feature, bug, task\n",
    "    \n",
    "    def cycle_time(self) -> float:\n",
    "        \"\"\"Hours from start to completion\"\"\"\n",
    "        if not self.completed_at or not self.started_at:\n",
    "            return 0.0\n",
    "        delta = self.completed_at - self.started_at\n",
    "        return delta.total_seconds() / 3600\n",
    "    \n",
    "    def age(self, now: datetime = None) -> float:\n",
    "        \"\"\"Current age if not completed\"\"\"\n",
    "        if self.completed_at:\n",
    "            return 0.0\n",
    "        now = now or datetime.now()\n",
    "        delta = now - self.started_at\n",
    "        return delta.total_seconds() / 3600\n",
    "\n",
    "class FlowMetricsCalculator:\n",
    "    def __init__(self, items: List[WorkItem]):\n",
    "        self.items = items\n",
    "    \n",
    "    def calculate_wip(self, status_check: datetime = None) -> int:\n",
    "        \"\"\"Count of items in progress at given time\"\"\"\n",
    "        status_check = status_check or datetime.now()\n",
    "        return len([i for i in self.items \n",
    "                   if i.started_at and \n",
    "                   (not i.completed_at or i.completed_at > status_check) and\n",
    "                   i.started_at <= status_check])\n",
    "    \n",
    "    def calculate_cycle_time(self, days: int = 30) -> Dict:\n",
    "        \"\"\"Calculate cycle time statistics\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        completed = [i for i in self.items \n",
    "                    if i.completed_at and i.completed_at > cutoff]\n",
    "        \n",
    "        if not completed:\n",
    "            return {\"average\": 0, \"median\": 0, \"p85\": 0, \"p95\": 0}\n",
    "        \n",
    "        times = [i.cycle_time() for i in completed]\n",
    "        times.sort()\n",
    "        \n",
    "        return {\n",
    "            \"average\": statistics.mean(times),\n",
    "            \"median\": statistics.median(times),\n",
    "            \"p85\": times[int(len(times)*0.85)],\n",
    "            \"p95\": times[int(len(times)*0.95)],\n",
    "            \"count\": len(times)\n",
    "        }\n",
    "    \n",
    "    def calculate_throughput(self, days: int = 30) -> float:\n",
    "        \"\"\"Items per day\"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=days)\n",
    "        completed = [i for i in self.items if i.completed_at and i.completed_at > cutoff]\n",
    "        return len(completed) / days\n",
    "    \n",
    "    def calculate_aging_wip(self) -> List[Dict]:\n",
    "        \"\"\"Current WIP items sorted by age\"\"\"\n",
    "        in_progress = [i for i in self.items if i.started_at and not i.completed_at]\n",
    "        in_progress.sort(key=lambda x: x.started_at)\n",
    "        \n",
    "        return [{\n",
    "            \"id\": i.id,\n",
    "            \"type\": i.type,\n",
    "            \"age_hours\": i.age(),\n",
    "            \"age_days\": i.age() / 24,\n",
    "            \"risk\": \"high\" if i.age() > 24 * 7 else \"medium\" if i.age() > 24 * 3 else \"normal\"\n",
    "        } for i in in_progress]\n",
    "    \n",
    "    def predict_completion(self, backlog_size: int) -> Dict:\n",
    "        \"\"\"Monte Carlo simulation for completion date\"\"\"\n",
    "        # Get historical throughput distribution\n",
    "        daily_throughputs = []\n",
    "        for i in range(30):\n",
    "            day_start = datetime.now() - timedelta(days=i+1)\n",
    "            day_end = datetime.now() - timedelta(days=i)\n",
    "            count = len([item for item in self.items \n",
    "                        if item.completed_at and day_start <= item.completed_at < day_end])\n",
    "            daily_throughputs.append(count)\n",
    "        \n",
    "        # Run simulation\n",
    "        simulations = 1000\n",
    "        results = []\n",
    "        \n",
    "        import random\n",
    "        for _ in range(simulations):\n",
    "            days = 0\n",
    "            remaining = backlog_size\n",
    "            while remaining > 0:\n",
    "                daily = random.choice(daily_throughputs)\n",
    "                remaining -= daily\n",
    "                days += 1\n",
    "            results.append(days)\n",
    "        \n",
    "        results.sort()\n",
    "        \n",
    "        return {\n",
    "            \"50_percent_likely\": results[500],  # Median\n",
    "            \"85_percent_likely\": results[850],  # 85th percentile\n",
    "            \"95_percent_likely\": results[950],  # 95th percentile\n",
    "            \"average\": statistics.mean(results)\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "items = [\n",
    "    WorkItem(\"FEAT-1\", datetime(2025, 3, 1), datetime(2025, 3, 2), datetime(2025, 3, 5), \"feature\"),\n",
    "    WorkItem(\"FEAT-2\", datetime(2025, 3, 2), datetime(2025, 3, 3), None, \"feature\"),  # Still in progress\n",
    "]\n",
    "\n",
    "flow = FlowMetricsCalculator(items)\n",
    "print(f\"Current WIP: {flow.calculate_wip()}\")\n",
    "print(f\"Cycle Time: {flow.calculate_cycle_time()}\")\n",
    "print(f\"Throughput: {flow.calculate_throughput():.2f} items/day\")\n",
    "print(f\"Aging WIP: {flow.calculate_aging_wip()}\")\n",
    "print(f\"Prediction (10 items): {flow.predict_completion(10)}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **19.3 Team Health and Velocity Trends**\n",
    "\n",
    "### **Warning Signs in Metrics**\n",
    "\n",
    "Metrics can indicate team health issues:\n",
    "\n",
    "| Metric Pattern | Possible Issue | Investigation |\n",
    "|----------------|----------------|---------------|\n",
    "| Velocity increasing but quality decreasing | Cutting corners | Check code review depth, test coverage |\n",
    "| Cycle time increasing, WIP constant | Bottlenecks | Check code review queues, external dependencies |\n",
    "| Deployment frequency dropping | Technical debt | Check build times, test flakiness |\n",
    "| MTTR increasing | Alert fatigue | Check on-call rotation, incident processes |\n",
    "\n",
    "**Code Snippet: Team Health Dashboard**\n",
    "\n",
    "```python\n",
    "class TeamHealthChecker:\n",
    "    def __init__(self, flow_metrics, dora_metrics):\n",
    "        self.flow = flow_metrics\n",
    "        self.dora = dora_metrics\n",
    "    \n",
    "    def assess_health(self) -> Dict:\n",
    "        issues = []\n",
    "        score = 100\n",
    "        \n",
    "        # Check for overloaded team (high WIP)\n",
    "        wip = self.flow.calculate_wip()\n",
    "        if wip > 10:  # Arbitrary threshold\n",
    "            issues.append(\"High WIP - team may be overloaded\")\n",
    "            score -= 20\n",
    "        \n",
    "        # Check for quality issues\n",
    "        if self.dora.calculate_cfr() > 15:\n",
    "            issues.append(\"High change failure rate - quality concerns\")\n",
    "            score -= 25\n",
    "        \n",
    "        # Check for burnout indicators (increasing cycle time)\n",
    "        current_ct = self.flow.calculate_cycle_time(7)[\"average\"]\n",
    "        previous_ct = self.flow.calculate_cycle_time(14)[\"average\"]\n",
    "        if current_ct > previous_ct * 1.5:\n",
    "            issues.append(\"Cycle time increasing - possible burnout or complexity\")\n",
    "            score -= 15\n",
    "        \n",
    "        return {\n",
    "            \"health_score\": max(0, score),\n",
    "            \"status\": \"healthy\" if score > 80 else \"at-risk\" if score > 60 else \"critical\",\n",
    "            \"issues\": issues\n",
    "        }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **19.4 Predictive Analytics and Forecasting**\n",
    "\n",
    "Instead of guessing completion dates, use historical data to generate probabilistic forecasts.\n",
    "\n",
    "### **Monte Carlo Simulation**\n",
    "\n",
    "Rather than saying \"We'll be done in 3 weeks,\" say \"We have an 85% probability of finishing in 3 weeks or less.\"\n",
    "\n",
    "**The Method**:\n",
    "1. Collect historical throughput (items completed per day) for last 30-90 days\n",
    "2. Run 1,000+ simulations of the remaining work\n",
    "3. Each day in simulation, randomly pick a throughput from historical data\n",
    "4. Count how many days to finish remaining items\n",
    "5. Sort results to get percentiles (50%, 85%, 95%)\n",
    "\n",
    "**Code Snippet: Monte Carlo Forecast (from Flow Metrics section above, expanded)**\n",
    "\n",
    "```python\n",
    "import random\n",
    "from typing import List\n",
    "import statistics\n",
    "\n",
    "class MonteCarloForecast:\n",
    "    def __init__(self, historical_throughputs: List[int]):\n",
    "        \"\"\"\n",
    "        historical_throughputs: List of daily throughput counts\n",
    "        Example: [3, 2, 4, 3, 0, 5, ...] for each day\n",
    "        \"\"\"\n",
    "        self.throughputs = historical_throughputs\n",
    "    \n",
    "    def forecast(self, remaining_items: int, simulations: int = 10000) -> Dict:\n",
    "        \"\"\"\n",
    "        Run Monte Carlo simulation\n",
    "        Returns completion dates with different confidence levels\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for _ in range(simulations):\n",
    "            days = 0\n",
    "            remaining = remaining_items\n",
    "            \n",
    "            while remaining > 0:\n",
    "                # Randomly select a daily throughput from history\n",
    "                daily = random.choice(self.throughputs)\n",
    "                remaining -= daily\n",
    "                days += 1\n",
    "            \n",
    "            results.append(days)\n",
    "        \n",
    "        results.sort()\n",
    "        \n",
    "        return {\n",
    "            \"50_percent\": results[int(simulations * 0.5)],\n",
    "            \"70_percent\": results[int(simulations * 0.7)],\n",
    "            \"85_percent\": results[int(simulations * 0.85)],\n",
    "            \"95_percent\": results[int(simulations * 0.95)],\n",
    "            \"99_percent\": results[int(simulations * 0.99)],\n",
    "            \"average\": statistics.mean(results),\n",
    "            \"standard_deviation\": statistics.stdev(results)\n",
    "        }\n",
    "    \n",
    "    def forecast_date(self, remaining_items: int, start_date: datetime = None) -> Dict:\n",
    "        \"\"\"Convert day counts to actual dates\"\"\"\n",
    "        start = start_date or datetime.now()\n",
    "        forecast = self.forecast(remaining_items)\n",
    "        \n",
    "        return {\n",
    "            \"50_percent_date\": start + timedelta(days=forecast[\"50_percent\"]),\n",
    "            \"85_percent_date\": start + timedelta(days=forecast[\"85_percent\"]),\n",
    "            \"95_percent_date\": start + timedelta(days=forecast[\"95_percent\"]),\n",
    "            \"forecast\": forecast\n",
    "        }\n",
    "\n",
    "# Example\n",
    "historical = [2, 3, 1, 4, 2, 3, 2, 1, 3, 2, 4, 3, 2, 1, 2]  # Daily completions\n",
    "mcf = MonteCarloForecast(historical)\n",
    "\n",
    "forecast = mcf.forecast_date(remaining_items=20)\n",
    "print(f\"20 items remaining:\")\n",
    "print(f\"50% confident: {forecast['50_percent_date'].strftime('%Y-%m-%d')}\")\n",
    "print(f\"85% confident: {forecast['85_percent_date'].strftime('%Y-%m-%d')}\")\n",
    "print(f\"95% confident: {forecast['95_percent_date'].strftime('%Y-%m-%d')}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **19.5 DORA Metrics Collector (Python/Prometheus)**\n",
    "\n",
    "For production use, you need to collect these metrics automatically and expose them to monitoring systems like Prometheus.\n",
    "\n",
    "**Code Snippet: Complete Metrics Collection System**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, Gauge, start_http_server\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Optional\n",
    "\n",
    "class DORAMetricsCollector:\n",
    "    def __init__(self):\n",
    "        # Deployment Frequency\n",
    "        self.deployment_counter = Counter(\n",
    "            'software_deployments_total',\n",
    "            'Total number of deployments',\n",
    "            ['environment', 'status']  # status: success, failed, rollback\n",
    "        )\n",
    "        \n",
    "        # Lead Time\n",
    "        self.lead_time_histogram = Histogram(\n",
    "            'change_lead_time_hours',\n",
    "            'Lead time for changes in hours',\n",
    "            buckets=[1, 4, 24, 72, 168, 336, 720]  # 1h, 4h, 1d, 3d, 7d, 14d, 30d\n",
    "        )\n",
    "        \n",
    "        # MTTR\n",
    "        self.mttr_histogram = Histogram(\n",
    "            'incident_recovery_time_minutes',\n",
    "            'Time to recover from incident',\n",
    "            buckets=[5, 15, 60, 240, 1440]  # 5min, 15min, 1h, 4h, 1d\n",
    "        )\n",
    "        \n",
    "        # Change Failure Rate (tracked via deployment status)\n",
    "        self.failed_deployment_counter = Counter(\n",
    "            'deployments_failed_total',\n",
    "            'Total failed deployments'\n",
    "        )\n",
    "        \n",
    "        # Flow Metrics\n",
    "        self.wip_gauge = Gauge(\n",
    "            'work_in_progress_items',\n",
    "            'Current WIP count',\n",
    "            ['type']\n",
    "        )\n",
    "        \n",
    "        self.cycle_time_histogram = Histogram(\n",
    "            'work_item_cycle_time_hours',\n",
    "            'Cycle time by item type',\n",
    "            ['type'],\n",
    "            buckets=[4, 8, 24, 48, 168]\n",
    "        )\n",
    "    \n",
    "    def record_deployment(self, environment: str, status: str, lead_time_hours: Optional[float] = None):\n",
    "        \"\"\"Record a deployment event\"\"\"\n",
    "        self.deployment_counter.labels(\n",
    "            environment=environment, \n",
    "            status=status\n",
    "        ).inc()\n",
    "        \n",
    "        if status in ['failed', 'rollback']:\n",
    "            self.failed_deployment_counter.inc()\n",
    "        \n",
    "        if lead_time_hours and status == 'success':\n",
    "            self.lead_time_histogram.observe(lead_time_hours)\n",
    "    \n",
    "    def record_incident(self, detection_to_recovery_minutes: float):\n",
    "        \"\"\"Record incident recovery\"\"\"\n",
    "        self.mttr_histogram.observe(detection_to_recovery_minutes)\n",
    "    \n",
    "    def update_wip(self, count: int, item_type: str = 'feature'):\n",
    "        \"\"\"Update current WIP gauge\"\"\"\n",
    "        self.wip_gauge.labels(type=item_type).set(count)\n",
    "    \n",
    "    def record_completion(self, cycle_time_hours: float, item_type: str = 'feature'):\n",
    "        \"\"\"Record item completion\"\"\"\n",
    "        self.cycle_time_histogram.labels(type=item_type).observe(cycle_time_hours)\n",
    "\n",
    "# Usage in CI/CD pipeline\n",
    "collector = DORAMetricsCollector()\n",
    "\n",
    "# Start Prometheus metrics server\n",
    "start_http_server(8000)\n",
    "\n",
    "# Example: Record deployment\n",
    "collector.record_deployment(\n",
    "    environment='production',\n",
    "    status='success',\n",
    "    lead_time_hours=4.5\n",
    ")\n",
    "\n",
    "# Example: Record incident\n",
    "collector.record_incident(detection_to_recovery_minutes=25)\n",
    "\n",
    "# Example: Update WIP (would be called regularly)\n",
    "collector.update_wip(count=8, item_type='feature')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "**Key Takeaways:**\n",
    "\n",
    "1. **DORA Metrics** are the industry standard for software delivery performance:\n",
    "   - Deployment Frequency (speed)\n",
    "   - Lead Time for Changes (speed)\n",
    "   - Mean Time to Recovery (stability)\n",
    "   - Change Failure Rate (stability)\n",
    "\n",
    "2. **Flow Metrics** help you understand system bottlenecks:\n",
    "   - WIP, Cycle Time, Throughput, and Work Item Age\n",
    "   - Little's Law connects them: Cycle Time = WIP / Throughput\n",
    "\n",
    "3. **Vanity vs. Actionable Metrics**:\n",
    "   - Vanity: Lines of code, story points (can be gamed)\n",
    "   - Actionable: Cycle time, failure rate (drive real improvement)\n",
    "\n",
    "4. **Predictive Analytics**: Use Monte Carlo simulations based on historical throughput to give probabilistic forecasts (85% confidence) rather than single-date guesses.\n",
    "\n",
    "5. **Automation**: Instrument your CI/CD pipeline to collect metrics automatically via Prometheus or similar systems.\n",
    "\n",
    "**The Metrics Manifesto**:\n",
    "- Measure outcomes, not outputs\n",
    "- Optimize for flow, not utilization\n",
    "- Improve predictability, not just speed\n",
    "- Use metrics to guide, not punish\n",
    "\n",
    "---\n",
    "\n",
    "## **Review Questions**\n",
    "\n",
    "1. **Your team has high velocity (story points) but low deployment frequency (monthly). What does this indicate, and which metric would you focus on improving first?**\n",
    "\n",
    "2. **Calculate the Change Failure Rate for this scenario: 12 deployments this month, 2 required hotfixes, 1 was rolled back completely.**\n",
    "\n",
    "3. **Your Monte Carlo simulation shows 50% probability of finishing in 2 weeks, but 85% probability in 4 weeks. How do you communicate this to stakeholders who want a guaranteed date?**\n",
    "\n",
    "4. **Why is \"Lines of Code per Developer\" a vanity metric? What would you measure instead to assess productivity?**\n",
    "\n",
    "5. **Design a balanced scorecard for an engineering team that includes DORA metrics, flow metrics, and quality metrics. What weights would you assign and why?**\n",
    "\n",
    "---\n",
    "\n",
    "## **Practical Exercise: Metrics Implementation Plan**\n",
    "\n",
    "**Scenario**: Your organization currently tracks only \"hours worked\" and \"bugs fixed.\" You want to implement DORA and Flow metrics.\n",
    "\n",
    "**Task**:\n",
    "1. Identify what data sources you need (CI/CD tool, issue tracker, incident management)\n",
    "2. Design the data collection pipeline (what to extract, transform, load)\n",
    "3. Create a dashboard mockup showing the four DORA metrics plus WIP and Cycle Time\n",
    "4. Define targets for each metric based on current state assessment\n",
    "5. Write a rollout plan (pilot team first, then expand)\n",
    "\n",
    "**Deliverable**: A 3-page implementation proposal with technical architecture and timeline.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 19**\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
