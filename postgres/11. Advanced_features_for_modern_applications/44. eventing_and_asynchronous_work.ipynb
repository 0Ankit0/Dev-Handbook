{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 44: Eventing and Asynchronous Work\n",
    "\n",
    "Synchronous request-response cycles cannot handle long-running operations, external API calls, or distributed transaction coordination without destroying user experience. PostgreSQL provides mechanisms for asynchronous processing\u2014from lightweight pub/sub notifications to durable job queues\u2014allowing applications to defer work, communicate between services, and coordinate distributed systems. This chapter covers the architectural patterns, reliability guarantees, and operational constraints of event-driven architectures built on PostgreSQL.\n",
    "\n",
    "## 44.1 The Landscape of Asynchronous Patterns\n",
    "\n",
    "PostgreSQL supports three distinct asynchronous communication paradigms, each with different durability and delivery guarantees:\n",
    "\n",
    "1. **LISTEN/NOTIFY**: Ephemeral pub/sub for real-time signaling (at-most-once delivery)\n",
    "2. **Outbox Pattern**: Transactional message publishing with guaranteed delivery (at-least-once)\n",
    "3. **Job Queues**: Durable work distribution with competitive consumption (exactly-once processing)\n",
    "\n",
    "Understanding the trade-offs between latency, durability, and complexity determines pattern selection.\n",
    "\n",
    "## 44.2 LISTEN/NOTIFY (Pub/Sub)\n",
    "\n",
    "PostgreSQL's notification system provides real-time, connection-oriented messaging between database sessions.\n",
    "\n",
    "### 44.2.1 Mechanism and Payload Limits\n",
    "\n",
    "The NOTIFY command sends a message to all connected sessions listening on a specific channel:\n",
    "\n",
    "```sql\n",
    "-- Session A: Subscribe to channel\n",
    "LISTEN user_updates;\n",
    "\n",
    "-- Session B: Publish message\n",
    "NOTIFY user_updates, '{\"user_id\": 123, \"action\": \"profile_updated\"}';\n",
    "\n",
    "-- Session A receives: Asynchronous notification \"user_updates\" \n",
    "-- with payload \"{\"user_id\": 123, \"action\": \"profile_updated\"}\" from server process PID 12345\n",
    "```\n",
    "\n",
    "**Critical Constraints:**\n",
    "- **Payload limit**: 8000 bytes (uncompressed). Larger payloads truncate silently in older versions, error in PostgreSQL 14+.\n",
    "- **Connection-bound**: Notifications queue per session; if disconnected, messages are lost permanently.\n",
    "- **No persistence**: Notifications exist only in server memory; crash or restart clears all undelivered messages.\n",
    "- **Single delivery**: Each notification delivers to each connected listener exactly once (if connected).\n",
    "\n",
    "**Payload Size Management:**\n",
    "```sql\n",
    "-- Anti-pattern: Large JSON payloads\n",
    "NOTIFY order_events, (SELECT row_to_json(orders)::text FROM orders WHERE order_id = 123);\n",
    "-- Risk: Exceeds 8KB, truncates or fails\n",
    "\n",
    "-- Pattern: Reference pattern (durable, small payload)\n",
    "NOTIFY order_events, json_build_object('order_id', 123, 'event_type', 'created')::text;\n",
    "-- Client receives notification, queries database for full details if needed\n",
    "```\n",
    "\n",
    "### 44.2.2 Client Implementation Patterns\n",
    "\n",
    "Database drivers handle notifications via separate connections or async callbacks:\n",
    "\n",
    "**Node.js (pg driver):**\n",
    "```javascript\n",
    "const { Client } = require('pg');\n",
    "\n",
    "const client = new Client();\n",
    "await client.connect();\n",
    "\n",
    "// Dedicated connection for notifications (cannot be used for queries)\n",
    "client.on('notification', (msg) => {\n",
    "    console.log('Channel:', msg.channel);\n",
    "    console.log('Payload:', msg.payload);\n",
    "    console.log('PID:', msg.processId);\n",
    "    \n",
    "    // Acknowledge by processing (no explicit ACK needed)\n",
    "    processEvent(JSON.parse(msg.payload));\n",
    "});\n",
    "\n",
    "await client.query('LISTEN user_updates');\n",
    "\n",
    "// Keep connection alive (heartbeats or periodic queries)\n",
    "setInterval(async () => {\n",
    "    await client.query('SELECT 1');\n",
    "}, 30000);\n",
    "```\n",
    "\n",
    "**Python (asyncpg):**\n",
    "```python\n",
    "import asyncpg\n",
    "import asyncio\n",
    "\n",
    "async def listener():\n",
    "    conn = await asyncpg.connect(dsn='postgresql://...')\n",
    "    await conn.add_listener('inventory_changes', handle_notification)\n",
    "    \n",
    "    # Keep running\n",
    "    while True:\n",
    "        await asyncio.sleep(3600)\n",
    "\n",
    "def handle_notification(connection, pid, channel, payload):\n",
    "    print(f'Received on {channel}: {payload}')\n",
    "    # Process asynchronously\n",
    "\n",
    "# Critical: Use dedicated connection; cannot share with transaction pool\n",
    "```\n",
    "\n",
    "**Go (pgx):**\n",
    "```go\n",
    "conn, _ := pgx.Connect(context.Background(), os.Getenv(\"DATABASE_URL\"))\n",
    "defer conn.Close(context.Background())\n",
    "\n",
    "_, err := conn.Exec(context.Background(), \"LISTEN inventory_updates\")\n",
    "if err != nil { log.Fatal(err) }\n",
    "\n",
    "for {\n",
    "    notification, err := conn.WaitForNotification(context.Background())\n",
    "    if err != nil { log.Fatal(err) }\n",
    "    \n",
    "    fmt.Printf(\"Notification: %s\\n\", notification.Payload)\n",
    "}\n",
    "```\n",
    "\n",
    "### 44.2.3 Reliability Limitations and Mitigations\n",
    "\n",
    "**The Missed Notification Problem:**\n",
    "If a listener disconnects (network blip, application restart) between NOTIFY and LISTEN reconnection, messages are lost forever.\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "\n",
    "1. **Complementary Polling Pattern** (Eventual Consistency):\n",
    "```sql\n",
    "-- Fallback polling for missed notifications\n",
    "SELECT MAX(event_id) as last_processed FROM processed_events;\n",
    "\n",
    "-- Poll every 30 seconds for events > last_processed\n",
    "SELECT * FROM events \n",
    "WHERE event_id > $1 \n",
    "  AND created_at > now() - interval '5 minutes'\n",
    "ORDER BY event_id;\n",
    "```\n",
    "\n",
    "2. **Logical Decoding** (Reliable Streaming):\n",
    "For guaranteed delivery, use logical replication slots instead of NOTIFY:\n",
    "```sql\n",
    "-- Create replication slot (pg_recvlogical or driver equivalent)\n",
    "SELECT pg_create_logical_replication_slot('events_slot', 'pgoutput');\n",
    "\n",
    "-- Stream changes via protocol (not SQL) for guaranteed delivery\n",
    "-- Requires separate logical replication connection\n",
    "```\n",
    "\n",
    "3. **Application-Level Deduplication:**\n",
    "```sql\n",
    "-- Track processed notification IDs (for idempotent processing)\n",
    "CREATE TABLE notification_log (\n",
    "    notification_id uuid PRIMARY KEY,\n",
    "    channel text,\n",
    "    processed_at timestamptz DEFAULT now()\n",
    ");\n",
    "\n",
    "-- On receive, insert with ON CONFLICT DO NOTHING\n",
    "INSERT INTO notification_log (notification_id, channel) \n",
    "VALUES ($1, 'user_updates')\n",
    "ON CONFLICT DO NOTHING;\n",
    "\n",
    "-- If rows affected = 0, duplicate notification (ignore)\n",
    "```\n",
    "\n",
    "### 44.2.4 Use Cases and Anti-Patterns\n",
    "\n",
    "**Appropriate Uses:**\n",
    "- **Cache invalidation**: Notify edge caches to purge keys when database updates occur\n",
    "- **Real-time UI updates**: WebSocket broadcasts when underlying data changes\n",
    "- **Configuration reload**: Signal applications to refresh in-memory settings\n",
    "- **Monitoring alerts**: Low-frequency critical events (error rate spikes)\n",
    "\n",
    "**Anti-Patterns:**\n",
    "- **High-frequency events** (>100/sec): Exhausts network buffers, causes latency\n",
    "- **Critical financial events**: No durability guarantee; use Outbox instead\n",
    "- **Distributed locking**: No fencing tokens; use advisory locks instead\n",
    "\n",
    "## 44.3 The Outbox Pattern (Reliable Messaging)\n",
    "\n",
    "Microservices architectures face the \"dual write problem\": updating the database and sending a message to a broker cannot both be atomic in separate systems.\n",
    "\n",
    "### 44.3.1 The Problem Statement\n",
    "\n",
    "```go\n",
    "// Anti-pattern: Non-atomic dual write\n",
    "func CreateOrder(ctx context.Context, order Order) error {\n",
    "    tx, _ := db.Begin(ctx)\n",
    "    defer tx.Rollback(ctx)\n",
    "    \n",
    "    // Write to database\n",
    "    err := tx.Exec(ctx, \"INSERT INTO orders ...\", order)\n",
    "    if err != nil { return err }\n",
    "    \n",
    "    tx.Commit(ctx)  // Database committed\n",
    "    \n",
    "    // Send to Kafka (can fail independently)\n",
    "    err = kafkaProducer.Send(order)  // If this fails, order exists but no event emitted\n",
    "    // Or worse: if process crashes here, inconsistency\n",
    "    \n",
    "    return err\n",
    "}\n",
    "```\n",
    "\n",
    "### 44.3.2 Outbox Table Implementation\n",
    "\n",
    "The Outbox Pattern ensures atomicity by writing events to a PostgreSQL table (the outbox) in the same transaction as business data changes:\n",
    "\n",
    "```sql\n",
    "-- Outbox table design\n",
    "CREATE TABLE outbox_events (\n",
    "    event_id bigserial PRIMARY KEY,\n",
    "    aggregate_type text NOT NULL,      -- 'order', 'user', etc.\n",
    "    aggregate_id text NOT NULL,        -- 'order-123'\n",
    "    event_type text NOT NULL,          -- 'OrderCreated', 'PaymentReceived'\n",
    "    payload jsonb NOT NULL,            -- Event data\n",
    "    metadata jsonb,                    -- Correlation IDs, timestamps\n",
    "    created_at timestamptz DEFAULT now(),\n",
    "    published_at timestamptz,         -- NULL until relay confirms\n",
    "    retry_count int DEFAULT 0,\n",
    "    \n",
    "    CONSTRAINT valid_event CHECK (payload <> '{}')\n",
    ");\n",
    "\n",
    "-- Index for efficient polling\n",
    "CREATE INDEX idx_outbox_unpublished \n",
    "ON outbox_events (created_at) \n",
    "WHERE published_at IS NULL;\n",
    "\n",
    "-- Partition by time if high volume (monthly partitions)\n",
    "-- CREATE TABLE outbox_events_2024_01 PARTITION OF outbox_events ...\n",
    "```\n",
    "\n",
    "**Transactional Event Publishing:**\n",
    "```sql\n",
    "-- Within application transaction\n",
    "BEGIN;\n",
    "\n",
    "-- 1. Business logic\n",
    "INSERT INTO orders (order_id, customer_id, total) \n",
    "VALUES ('ord-456', 'cust-789', 10000);\n",
    "\n",
    "-- 2. Atomic event write (same transaction)\n",
    "INSERT INTO outbox_events (\n",
    "    aggregate_type, \n",
    "    aggregate_id, \n",
    "    event_type, \n",
    "    payload,\n",
    "    metadata\n",
    ") VALUES (\n",
    "    'order',\n",
    "    'ord-456',\n",
    "    'OrderCreated',\n",
    "    jsonb_build_object(\n",
    "        'order_id', 'ord-456',\n",
    "        'customer_id', 'cust-789',\n",
    "        'total_cents', 10000,\n",
    "        'line_items', (SELECT jsonb_agg(item) FROM order_items WHERE order_id = 'ord-456')\n",
    "    ),\n",
    "    jsonb_build_object(\n",
    "        'correlation_id', current_setting('app.correlation_id', true),\n",
    "        'timestamp', extract(epoch from now())\n",
    "    )\n",
    ");\n",
    "\n",
    "COMMIT;\n",
    "-- Both order and event committed atomically\n",
    "-- If commit fails, neither exists (consistency maintained)\n",
    "```\n",
    "\n",
    "### 44.3.3 The Relay Process (Message Delivery)\n",
    "\n",
    "A separate process polls the outbox and publishes to the message broker:\n",
    "\n",
    "```python\n",
    "# Relay implementation (Python example)\n",
    "import asyncio\n",
    "import asyncpg\n",
    "\n",
    "async def relay_loop():\n",
    "    conn = await asyncpg.connect(dsn='postgresql://...')\n",
    "    \n",
    "    while True:\n",
    "        # Fetch batch of unpublished events\n",
    "        rows = await conn.fetch(\"\"\"\n",
    "            SELECT event_id, aggregate_type, event_type, payload, metadata\n",
    "            FROM outbox_events\n",
    "            WHERE published_at IS NULL\n",
    "            ORDER BY event_id\n",
    "            LIMIT 100\n",
    "            FOR UPDATE SKIP LOCKED\n",
    "        \"\"\")\n",
    "        \n",
    "        for row in rows:\n",
    "            try:\n",
    "                # Publish to Kafka/RabbitMQ/SNS\n",
    "                await message_bus.publish(\n",
    "                    topic=f\"{row['aggregate_type']}-{row['event_type']}\",\n",
    "                    message=row['payload'],\n",
    "                    headers=row['metadata']\n",
    "                )\n",
    "                \n",
    "                # Mark as published (or delete)\n",
    "                await conn.execute(\"\"\"\n",
    "                    UPDATE outbox_events \n",
    "                    SET published_at = now()\n",
    "                    WHERE event_id = $1\n",
    "                \"\"\", row['event_id'])\n",
    "                \n",
    "            except Exception as e:\n",
    "                # Increment retry, will be picked up later\n",
    "                await conn.execute(\"\"\"\n",
    "                    UPDATE outbox_events \n",
    "                    SET retry_count = retry_count + 1,\n",
    "                        last_error = $2\n",
    "                    WHERE event_id = $1\n",
    "                \"\"\", row['event_id'], str(e))\n",
    "                logger.error(f\"Failed to publish event {row['event_id']}: {e}\")\n",
    "        \n",
    "        await asyncio.sleep(0.1)  # 100ms polling interval\n",
    "```\n",
    "\n",
    "**WAL-Based Relay (PostgreSQL 10+):**\n",
    "For lower latency without polling, use logical replication to stream outbox changes:\n",
    "\n",
    "```sql\n",
    "-- Create publication for outbox only\n",
    "CREATE PUBLICATION outbox_pub FOR TABLE outbox_events;\n",
    "\n",
    "-- Relay subscribes via logical replication protocol\n",
    "-- Receives events immediately on commit, no polling delay\n",
    "```\n",
    "\n",
    "### 44.3.4 Ordering and Idempotency Guarantees\n",
    "\n",
    "**Ordering:**\n",
    "Outbox preserves event order within an aggregate (same `aggregate_id`) due to serial `event_id` and ordered polling:\n",
    "\n",
    "```sql\n",
    "-- Ensure aggregate-level ordering\n",
    "SELECT * FROM outbox_events \n",
    "WHERE published_at IS NULL\n",
    "ORDER BY aggregate_id, event_id  -- Process in order per aggregate\n",
    "```\n",
    "\n",
    "**Idempotency:**\n",
    "Consumers must handle duplicate deliveries (at-least-once semantics):\n",
    "\n",
    "```sql\n",
    "-- Consumer deduplication\n",
    "INSERT INTO processed_events (event_id, processed_at)\n",
    "VALUES ('evt-uuid-from-message', now())\n",
    "ON CONFLICT (event_id) DO NOTHING;\n",
    "\n",
    "-- If rows affected = 0, duplicate detected, skip processing\n",
    "```\n",
    "\n",
    "### 44.3.5 Cleanup Strategies\n",
    "\n",
    "Outbox tables grow indefinitely without cleanup:\n",
    "\n",
    "```sql\n",
    "-- Strategy 1: Delete after publishing (if no audit requirement)\n",
    "DELETE FROM outbox_events \n",
    "WHERE published_at < now() - interval '1 hour';\n",
    "\n",
    "-- Strategy 2: Archive then delete\n",
    "INSERT INTO outbox_archive \n",
    "SELECT * FROM outbox_events \n",
    "WHERE published_at < now() - interval '7 days';\n",
    "\n",
    "DELETE FROM outbox_events \n",
    "WHERE published_at < now() - interval '7 days';\n",
    "\n",
    "-- Strategy 3: Partitioning with DROP\n",
    "-- Monthly partitions, drop old partitions after archival to S3\n",
    "DROP TABLE outbox_events_2024_01;  -- Instant, no vacuum bloat\n",
    "```\n",
    "\n",
    "## 44.4 Job Queues (SKIP LOCKED Pattern)\n",
    "\n",
    "For background work processing (image resizing, email sending, report generation), PostgreSQL implements durable job queues using the `SKIP LOCKED` concurrency feature.\n",
    "\n",
    "### 44.4.1 Queue Table Design\n",
    "\n",
    "```sql\n",
    "-- Job queue table\n",
    "CREATE TABLE job_queue (\n",
    "    job_id bigserial PRIMARY KEY,\n",
    "    queue_name text NOT NULL DEFAULT 'default',\n",
    "    job_type text NOT NULL,           -- 'send_email', 'resize_image'\n",
    "    payload jsonb NOT NULL,\n",
    "    priority int DEFAULT 0,           -- Higher = more important\n",
    "    attempts int DEFAULT 0,\n",
    "    max_attempts int DEFAULT 3,\n",
    "    status job_status DEFAULT 'pending',  -- pending, processing, failed, completed\n",
    "    scheduled_for timestamptz DEFAULT now(),\n",
    "    created_at timestamptz DEFAULT now(),\n",
    "    processed_at timestamptz,\n",
    "    processed_by text,                -- Worker identifier\n",
    "    error_message text\n",
    ");\n",
    "\n",
    "-- Critical index for efficient dequeuing\n",
    "CREATE INDEX idx_job_queue_fetch \n",
    "ON job_queue (queue_name, scheduled_for, priority DESC, job_id)\n",
    "WHERE status = 'pending';\n",
    "\n",
    "-- Partial index for failed job analysis\n",
    "CREATE INDEX idx_job_queue_failed \n",
    "ON job_queue (queue_name, created_at) \n",
    "WHERE status = 'failed';\n",
    "```\n",
    "\n",
    "**Status Enum:**\n",
    "```sql\n",
    "CREATE TYPE job_status AS ENUM ('pending', 'processing', 'failed', 'completed');\n",
    "```\n",
    "\n",
    "### 44.4.2 The SKIP LOCKED Pattern\n",
    "\n",
    "`SELECT ... FOR UPDATE SKIP LOCKED` allows concurrent workers to dequeue without blocking:\n",
    "\n",
    "```sql\n",
    "-- Worker dequeuing (competitive consumption)\n",
    "WITH next_job AS (\n",
    "    SELECT job_id, job_type, payload, attempts\n",
    "    FROM job_queue\n",
    "    WHERE queue_name = 'emails'\n",
    "      AND status = 'pending'\n",
    "      AND scheduled_for <= now()\n",
    "    ORDER BY priority DESC, job_id  -- Fair ordering\n",
    "    LIMIT 1\n",
    "    FOR UPDATE SKIP LOCKED  -- Skip jobs locked by other workers\n",
    ")\n",
    "UPDATE job_queue \n",
    "SET \n",
    "    status = 'processing',\n",
    "    attempts = attempts + 1,\n",
    "    processed_by = current_setting('application.worker_id', true),\n",
    "    processed_at = now()\n",
    "FROM next_job\n",
    "WHERE job_queue.job_id = next_job.job_id\n",
    "RETURNING job_queue.*;\n",
    "-- Returns exactly one job, atomically reserved for this worker\n",
    "```\n",
    "\n",
    "**Why SKIP LOCKED Matters:**\n",
    "Without `SKIP LOCKED`, concurrent workers would block on the first row, creating serialization bottlenecks. `SKIP LOCKED` allows workers to skip rows already locked, grabbing the next available job immediately.\n",
    "\n",
    "### 44.4.3 Worker Implementation\n",
    "\n",
    "**Ruby (Sidekiq-style):**\n",
    "```ruby\n",
    "class Worker\n",
    "  def run\n",
    "    loop do\n",
    "      job = fetch_job\n",
    "      break unless job\n",
    "      \n",
    "      begin\n",
    "        process(job)\n",
    "        complete_job(job)\n",
    "      rescue => e\n",
    "        fail_job(job, e)\n",
    "      end\n",
    "    end\n",
    "  end\n",
    "  \n",
    "  private\n",
    "  \n",
    "  def fetch_job\n",
    "    # Raw SQL with SKIP LOCKED\n",
    "    ActiveRecord::Base.connection.execute(<<-SQL).first\n",
    "      WITH next_job AS (\n",
    "        SELECT job_id FROM job_queue \n",
    "        WHERE status = 'pending' AND scheduled_for <= now()\n",
    "        ORDER BY priority DESC, job_id\n",
    "        LIMIT 1\n",
    "        FOR UPDATE SKIP LOCKED\n",
    "      )\n",
    "      UPDATE job_queue \n",
    "      SET status = 'processing', attempts = attempts + 1\n",
    "      FROM next_job\n",
    "      WHERE job_queue.job_id = next_job.job_id\n",
    "      RETURNING *\n",
    "    SQL\n",
    "  end\n",
    "  \n",
    "  def complete_job(job)\n",
    "    ActiveRecord::Base.connection.execute(\n",
    "      \"DELETE FROM job_queue WHERE job_id = #{job['job_id']}\"\n",
    "      # Or UPDATE SET status = 'completed' if audit trail needed\n",
    "    )\n",
    "  end\n",
    "  \n",
    "  def fail_job(job, error)\n",
    "    ActiveRecord::Base.connection.execute(<<-SQL)\n",
    "      UPDATE job_queue \n",
    "      SET \n",
    "        status = CASE \n",
    "          WHEN attempts >= max_attempts THEN 'failed'::job_status \n",
    "          ELSE 'pending'::job_status \n",
    "        END,\n",
    "        error_message = '#{escape_string(error.message)}',\n",
    "        scheduled_for = now() + (attempts || 1) * interval '5 minutes'  -- Exponential backoff\n",
    "      WHERE job_id = #{job['job_id']}\n",
    "    SQL\n",
    "  end\n",
    "end\n",
    "```\n",
    "\n",
    "### 44.4.4 Priority and Delayed Jobs\n",
    "\n",
    "**Priority Levels:**\n",
    "```sql\n",
    "-- Insert with priority (higher = more urgent)\n",
    "INSERT INTO job_queue (job_type, payload, priority)\n",
    "VALUES ('send_email', '{\"to\": \"user@example.com\"}', 10);\n",
    "\n",
    "-- Urgent jobs (priority 100) processed before default (0)\n",
    "ORDER BY priority DESC, job_id\n",
    "```\n",
    "\n",
    "**Delayed Execution:**\n",
    "```sql\n",
    "-- Schedule for future processing\n",
    "INSERT INTO job_queue (job_type, payload, scheduled_for)\n",
    "VALUES (\n",
    "    'reminder', \n",
    "    '{\"user_id\": 123}', \n",
    "    now() + interval '1 day'\n",
    ");\n",
    "\n",
    "-- Worker query includes scheduled_for check\n",
    "WHERE scheduled_for <= now()\n",
    "```\n",
    "\n",
    "### 44.4.5 Dead Letter Queues\n",
    "\n",
    "Failed jobs exceeding retry limits require separate handling:\n",
    "\n",
    "```sql\n",
    "-- Move to dead letter queue (separate table)\n",
    "WITH failed_job AS (\n",
    "    UPDATE job_queue \n",
    "    SET status = 'failed'\n",
    "    WHERE job_id = $1 \n",
    "      AND attempts >= max_attempts\n",
    "    RETURNING *\n",
    ")\n",
    "INSERT INTO dead_letter_queue \n",
    "SELECT *, now() as failed_at FROM failed_job;\n",
    "\n",
    "-- Alert on dead letter queue growth\n",
    "SELECT count(*) FROM dead_letter_queue \n",
    "WHERE failed_at > now() - interval '1 hour';\n",
    "```\n",
    "\n",
    "## 44.5 Limitations and External Alternatives\n",
    "\n",
    "While PostgreSQL queues work for moderate loads, specific thresholds demand dedicated message brokers.\n",
    "\n",
    "### 44.5.1 PostgreSQL Queue Limitations\n",
    "\n",
    "**Throughput Ceiling:**\n",
    "- Practical limit: ~1,000-2,000 jobs/second dequeue rate across all workers\n",
    "- Beyond this, `SKIP LOCKED` contention and vacuum overhead dominate\n",
    "\n",
    "**Connection Exhaustion:**\n",
    "Each worker requires a dedicated connection (long-running transaction during job processing):\n",
    "```sql\n",
    "-- If 100 workers processing 10-second jobs\n",
    "-- Requires 100 concurrent connections\n",
    "-- At 1000 workers, max_connections exceeded or pool exhausted\n",
    "```\n",
    "\n",
    "**Vacuum Bloat:**\n",
    "High-churn queue tables (millions of jobs/day) generate massive dead tuple bloat:\n",
    "```sql\n",
    "-- Queue table with 1M jobs/hour processed\n",
    "-- Creates 1M dead tuples/hour\n",
    "-- Requires aggressive autovacuum or constant bloat\n",
    "```\n",
    "\n",
    "**No Priority Queueing Efficiency:**\n",
    "PostgreSQL cannot efficiently implement strict priority queues (skip low-priority jobs to find high-priority). The index scan must traverse all pending jobs.\n",
    "\n",
    "### 44.5.2 Migration Triggers\n",
    "\n",
    "Move to Redis/RabbitMQ/SQS when:\n",
    "- Dequeue rate > 2,000/second sustained\n",
    "- Job latency requirements < 10ms (PostgreSQL polling minimum ~5-10ms)\n",
    "- Worker count > 200 (connection pool pressure)\n",
    "- Complex routing required (topic exchanges, dead letter routing)\n",
    "- Need for visibility timeout extensions (SQS feature)\n",
    "\n",
    "**Hybrid Architecture:**\n",
    "```sql\n",
    "-- Use PostgreSQL for durability-critical events (Outbox)\n",
    "-- Use Redis for high-volume, ephemeral jobs (cache warming)\n",
    "\n",
    "-- Or: Tiered queue\n",
    "-- Fast path: Redis for non-critical jobs\n",
    "-- Slow path: PostgreSQL for must-not-lose jobs (payments, webhooks)\n",
    "```\n",
    "\n",
    "## 44.6 Operational Considerations\n",
    "\n",
    "### 44.6.1 Monitoring Queue Health\n",
    "\n",
    "```sql\n",
    "-- Queue depth alerting (backlog detection)\n",
    "SELECT \n",
    "    queue_name,\n",
    "    count(*) FILTER (WHERE status = 'pending') as pending,\n",
    "    count(*) FILTER (WHERE status = 'processing') as processing,\n",
    "    count(*) FILTER (WHERE status = 'failed') as failed,\n",
    "    max(created_at) FILTER (WHERE status = 'pending') as oldest_pending,\n",
    "    avg(extract(epoch from (now() - created_at))) \n",
    "        FILTER (WHERE status = 'pending') as avg_wait_seconds\n",
    "FROM job_queue\n",
    "GROUP BY queue_name;\n",
    "\n",
    "-- Alert if pending > 10000 or oldest_pending > now() - interval '1 hour'\n",
    "```\n",
    "\n",
    "### 44.6.2 Vacuum and Partitioning for Queues\n",
    "\n",
    "High-churn queue tables require special handling:\n",
    "\n",
    "```sql\n",
    "-- Aggressive autovacuum for queue tables\n",
    "ALTER TABLE job_queue SET (\n",
    "    autovacuum_vacuum_scale_factor = 0,\n",
    "    autovacuum_vacuum_threshold = 1000,\n",
    "    autovacuum_vacuum_cost_limit = 2000,\n",
    "    fillfactor = 50  -- Leave room for HOT updates (status changes)\n",
    ");\n",
    "\n",
    "-- Or partition by time for easy rotation\n",
    "CREATE TABLE job_queue_2024_01 PARTITION OF job_queue \n",
    "FOR VALUES FROM ('2024-01-01') TO ('2024-02-01');\n",
    "\n",
    "-- Drop old completed partitions instead of deleting rows\n",
    "DROP TABLE job_queue_2023_12;  -- Instant, no vacuum needed\n",
    "```\n",
    "\n",
    "### 44.6.3 Connection Pool Integration\n",
    "\n",
    "Queue workers must use separate connection pools from application servers:\n",
    "\n",
    "```yaml\n",
    "# Application config\n",
    "databases:\n",
    "  app:\n",
    "    pool_size: 20          # Web application queries\n",
    "  queue:\n",
    "    pool_size: 100         # Dedicated to workers\n",
    "    max_lifetime: 3600     # Longer-lived connections acceptable\n",
    "    checkout_timeout: 5    # Fail fast if no connections\n",
    "```\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, you learned:\n",
    "\n",
    "1. **LISTEN/NOTIFY**: Use for ephemeral, real-time signaling (cache invalidation, UI updates) where missed messages are acceptable. Respect the 8KB payload limit and implement complementary polling for reliability. Never use for critical financial events or high-frequency (>100/sec) messaging.\n",
    "\n",
    "2. **Outbox Pattern**: Implement transactional outbox tables to solve the dual-write problem in microservices. Write events atomically with business data, use a relay process to publish to message buses, and ensure consumers handle at-least-once delivery with idempotency checks. Clean up published events to prevent table bloat.\n",
    "\n",
    "3. **Job Queues**: Use `SELECT ... FOR UPDATE SKIP LOCKED` for high-performance concurrent job dequeuing. Design queue tables with partial indexes on pending status, implement exponential backoff for failures, and move exhausted jobs to dead letter queues. Respect PostgreSQL's practical limit of ~1,000-2,000 jobs/second dequeue rate.\n",
    "\n",
    "4. **Limitations**: Migrate to dedicated message brokers (Redis, RabbitMQ, SQS) when dequeue rates exceed 2,000/sec, latency requirements drop below 10ms, or worker counts exceed 200 connections. PostgreSQL queues excel at durability and exactly-once semantics but suffer from vacuum bloat and connection overhead at high scale.\n",
    "\n",
    "5. **Operational Management**: Monitor queue depth, oldest pending job age, and failed job rates. Configure aggressive autovacuum for queue tables or use time-based partitioning for easy data rotation. Maintain separate connection pools for queue workers to prevent application connection starvation.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** In Chapter 45, we will explore Testing Strategies\u2014covering unit tests for database functions, integration tests with ephemeral databases, property-based testing for SQL correctness, and deterministic test data management that ensures reproducible results across environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='43. geospatial.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../12. Testing_delivery_and_team_practices/45. local_development_workflows.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}