{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 47: CI/CD for Databases\n",
    "\n",
    "Continuous Integration and Continuous Deployment for databases requires stricter safety guarantees than application code due to the stateful nature of data. While application deployments can be rolled back by reverting to a previous container image, database changes are permanent once committed. This chapter establishes industry-standard patterns for validating, testing, and deploying database changes with zero-downtime guarantees and deterministic rollback procedures.\n",
    "\n",
    "## 47.1 Migration Validation and Testing in CI\n",
    "\n",
    "### 47.1.1 Pre-deployment Validation Pipeline\n",
    "\n",
    "Before any migration reaches production, it must pass through a multi-stage validation pipeline that checks syntax, conflicts, and compatibility.\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/database-ci.yml\n",
    "name: Database CI Validation\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "    paths:\n",
    "      - 'migrations/**'\n",
    "      - 'schema/**'\n",
    "      - '.github/workflows/database-ci.yml'\n",
    "\n",
    "jobs:\n",
    "  validate-migrations:\n",
    "    runs-on: ubuntu-latest\n",
    "    services:\n",
    "      postgres:\n",
    "        image: postgres:16-alpine\n",
    "        env:\n",
    "          POSTGRES_PASSWORD: postgres\n",
    "          POSTGRES_DB: migration_test\n",
    "        options: >-\n",
    "          --health-cmd pg_isready\n",
    "          --health-interval 10s\n",
    "          --health-timeout 5s\n",
    "          --health-retries 5\n",
    "        ports:\n",
    "          - 5432:5432\n",
    "\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "        with:\n",
    "          fetch-depth: 0  # Full history for conflict detection\n",
    "\n",
    "      - name: Setup PostgreSQL Client\n",
    "        run: |\n",
    "          sudo apt-get update\n",
    "          sudo apt-get install -y postgresql-client\n",
    "\n",
    "      - name: Check Migration Naming Convention\n",
    "        run: |\n",
    "          # Enforce timestamp prefixes to prevent ordering conflicts\n",
    "          for file in migrations/*.sql; do\n",
    "            if [[ ! \"$(basename \"$file\")\" =~ ^[0-9]{14}_.*\\.sql$ ]]; then\n",
    "              echo \"ERROR: $file does not follow naming convention YYYYMMDDHHMMSS_description.sql\"\n",
    "              exit 1\n",
    "            fi\n",
    "          done\n",
    "          echo \"\u2713 Migration naming convention validated\"\n",
    "\n",
    "      - name: Detect Migration Conflicts\n",
    "        run: |\n",
    "          # Check if two branches added migrations with same timestamp prefix\n",
    "          git fetch origin main\n",
    "          MAIN_MIGRATIONS=$(git ls-tree -r --name-only origin/main migrations/ | sort)\n",
    "          PR_MIGRATIONS=$(ls -1 migrations/*.sql 2>/dev/null | sort)\n",
    "          \n",
    "          # Check for duplicate timestamps\n",
    "          echo \"$PR_MIGRATIONS\" | while read file; do\n",
    "            timestamp=$(basename \"$file\" | cut -d'_' -f1)\n",
    "            matches=$(echo \"$MAIN_MIGRATIONS\" | grep \"^migrations/${timestamp}\" || true)\n",
    "            if [ ! -z \"$matches\" ]; then\n",
    "              echo \"ERROR: Migration timestamp collision detected\"\n",
    "              echo \"Main branch has: $matches\"\n",
    "              echo \"PR branch has: $file\"\n",
    "              exit 1\n",
    "            fi\n",
    "          done\n",
    "\n",
    "      - name: Syntax Validation (Dry Run)\n",
    "        env:\n",
    "          PGPASSWORD: postgres\n",
    "        run: |\n",
    "          # Test all migrations in transaction that rolls back\n",
    "          psql -h localhost -U postgres -d migration_test << 'EOF'\n",
    "            BEGIN;\n",
    "            -- Set exit on error\n",
    "            \\set ON_ERROR_STOP on\n",
    "            \n",
    "            -- Apply all migrations\n",
    "            \\i migrations/001_initial_schema.sql\n",
    "            \\i migrations/002_add_user_indexes.sql\n",
    "            -- ... etc\n",
    "            \n",
    "            -- Verify no syntax errors occurred\n",
    "            SELECT 'All migrations valid' as status;\n",
    "            \n",
    "            -- Rollback everything (dry run)\n",
    "            ROLLBACK;\n",
    "          EOF\n",
    "```\n",
    "\n",
    "**Validation Checklist:**\n",
    "\n",
    "1. **Naming Convention**: Enforce `YYYYMMDDHHMMSS_description.sql` to ensure ordering and prevent merge conflicts\n",
    "2. **Idempotency Check**: Verify `IF NOT EXISTS` or `CREATE OR REPLACE` where applicable\n",
    "3. **Transaction Safety**: Ensure DDL statements are transactional (PostgreSQL supports transactional DDL)\n",
    "4. **Conflict Detection**: Prevent two developers from creating migrations with identical timestamps\n",
    "5. **Syntax Validation**: Parse SQL without executing permanently (dry run in transaction)\n",
    "\n",
    "### 47.1.2 Shadow Database Testing\n",
    "\n",
    "Shadow databases validate migrations against production-like data volumes and schemas without affecting production.\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# scripts/shadow-migration-test.sh\n",
    "\n",
    "set -euo pipefail\n",
    "\n",
    "# Configuration\n",
    "PROD_DUMP_S3=\"s3://backups/production/anonymized-latest.dump\"\n",
    "SHADOW_DB=\"shadow_test_$(date +%s)\"\n",
    "SHADOW_HOST=\"${SHADOW_HOST:-localhost}\"\n",
    "SHADOW_USER=\"postgres\"\n",
    "\n",
    "echo \"\ud83d\udd04 Creating shadow database: $SHADOW_DB\"\n",
    "\n",
    "# Create temporary database\n",
    "psql -h $SHADOW_HOST -U $SHADOW_USER -c \"CREATE DATABASE $SHADOW_DB;\"\n",
    "\n",
    "cleanup() {\n",
    "    echo \"\ud83e\uddf9 Cleaning up shadow database...\"\n",
    "    psql -h $SHADOW_HOST -U $SHADOW_USER -c \"DROP DATABASE IF EXISTS $SHADOW_DB;\"\n",
    "}\n",
    "trap cleanup EXIT\n",
    "\n",
    "# Restore production schema (anonymized)\n",
    "echo \"\ud83d\udce5 Restoring production schema...\"\n",
    "pg_restore -h $SHADOW_HOST -U $SHADOW_USER -d $SHADOW_DB --schema-only \\\n",
    "    <(aws s3 cp $PROD_DUMP_S3 -) 2>/dev/null || true\n",
    "\n",
    "# Run migrations and capture timing\n",
    "echo \"\ud83d\ude80 Running migrations...\"\n",
    "START_TIME=$(date +%s)\n",
    "\n",
    "if ! psql -h $SHADOW_HOST -U $SHADOW_USER -d $SHADOW_DB \\\n",
    "    --single-transaction \\\n",
    "    --file=migrations/all_migrations.sql; then\n",
    "    \n",
    "    echo \"\u274c Migration failed on shadow database\"\n",
    "    exit 1\n",
    "fi\n",
    "\n",
    "END_TIME=$(date +%s)\n",
    "DURATION=$((END_TIME - START_TIME))\n",
    "\n",
    "echo \"\u2705 Migrations completed in ${DURATION}s\"\n",
    "\n",
    "# Performance assertions\n",
    "if [ $DURATION -gt 300 ]; then\n",
    "    echo \"\u26a0\ufe0f WARNING: Migrations took >5 minutes. Consider breaking into smaller transactions.\"\n",
    "fi\n",
    "\n",
    "# Verify constraints are valid (not deferred failures)\n",
    "echo \"\ud83d\udd0d Validating constraints...\"\n",
    "psql -h $SHADOW_HOST -U $SHADOW_USER -d $SHADOW_DB -c \"\n",
    "    SELECT conname, contype \n",
    "    FROM pg_constraint \n",
    "    WHERE convalidated = false;\n",
    "\" | grep -q \"0 rows\" || {\n",
    "    echo \"\u274c Unvalidated constraints detected\"\n",
    "    exit 1\n",
    "}\n",
    "```\n",
    "\n",
    "### 47.1.3 Backward Compatibility Enforcement\n",
    "\n",
    "Database changes must maintain backward compatibility during deployment to support rolling application updates (where old and new code run simultaneously).\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/backward-compat-check.yml\n",
    "name: Backward Compatibility\n",
    "\n",
    "on: [pull_request]\n",
    "\n",
    "jobs:\n",
    "  check-breaking-changes:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Check for Breaking Changes\n",
    "        run: |\n",
    "          # List of breaking change patterns\n",
    "          BREAKING_PATTERNS=(\n",
    "            \"DROP TABLE\"\n",
    "            \"DROP COLUMN\"\n",
    "            \"ALTER TABLE.*DROP\"\n",
    "            \"RENAME COLUMN\"\n",
    "            \"RENAME TABLE\"\n",
    "            \"ALTER.*TYPE.*USING\"  # Type changes that fail on old data\n",
    "            \"NOT NULL.*ADD\"       # Adding NOT NULL without default\n",
    "          )\n",
    "          \n",
    "          EXIT_CODE=0\n",
    "          \n",
    "          for file in $(git diff --name-only origin/main | grep '\\.sql$'); do\n",
    "            echo \"Checking $file...\"\n",
    "            \n",
    "            for pattern in \"${BREAKING_PATTERNS[@]}\"; do\n",
    "              if grep -iE \"$pattern\" \"$file\" > /dev/null 2>&1; then\n",
    "                echo \"\u274c BREAKING CHANGE detected in $file:\"\n",
    "                grep -n -iE \"$pattern\" \"$file\"\n",
    "                echo \"\"\n",
    "                echo \"Remediation:\"\n",
    "                echo \"- Use 'Expand and Contract' pattern for column drops\"\n",
    "                echo \"- Add new columns as NULL first, populate, then add constraint\"\n",
    "                echo \"- Create new table/column, migrate data, drop old in separate release\"\n",
    "                EXIT_CODE=1\n",
    "              fi\n",
    "            done\n",
    "          done\n",
    "          \n",
    "          exit $EXIT_CODE\n",
    "\n",
    "      - name: Verify Migration Order Safety\n",
    "        run: |\n",
    "          # Ensure new migrations don't reference objects created in same PR\n",
    "          # unless they are in the same file\n",
    "          echo \"Checking cross-migration dependencies...\"\n",
    "```\n",
    "\n",
    "**Backward Compatibility Patterns:**\n",
    "\n",
    "| Change Type | Breaking? | Safe Approach |\n",
    "|-------------|-----------|---------------|\n",
    "| Add column | No | Add as nullable or with default |\n",
    "| Add `NOT NULL` | Yes | Add nullable \u2192 Backfill \u2192 Add constraint in next PR |\n",
    "| Drop column | Yes | Stop using in app \u2192 Deploy \u2192 Drop column in next PR |\n",
    "| Rename column | Yes | Add new column \u2192 Dual write \u2192 Migrate \u2192 Drop old |\n",
    "| Change type | Yes | Add new column \u2192 Migrate \u2192 Update app \u2192 Drop old |\n",
    "| Drop table | Yes | Stop writes \u2192 Archive data \u2192 Drop in next PR |\n",
    "| Add index | No (mostly) | Use `CONCURRENTLY` to avoid locking |\n",
    "| Add FK constraint | Yes (if existing data invalid) | Validate data first, add `NOT VALID` then validate separately |\n",
    "\n",
    "## 47.2 SQL Linting and Formatting\n",
    "\n",
    "### 47.2.1 SQLFluff Configuration\n",
    "\n",
    "SQLFluff is the industry-standard linter for SQL, supporting PostgreSQL dialect and custom rule configurations.\n",
    "\n",
    "```ini\n",
    "# .sqlfluff\n",
    "[sqlfluff]\n",
    "dialect = postgres\n",
    "templater = jinja\n",
    "runaway_limit = 10\n",
    "max_line_length = 88\n",
    "\n",
    "[sqlfluff:indentation]\n",
    "tab_width = 4\n",
    "indented_joins = false\n",
    "indented_using_on = true\n",
    "indented_ctes = false\n",
    "\n",
    "[sqlfluff:layout:type:comma]\n",
    "line_position = trailing\n",
    "\n",
    "[sqlfluff:rules]\n",
    "exclude_rules = L016,L031,L034  # Exclude specific rules if needed\n",
    "\n",
    "[sqlfluff:rules:aliasing.table]\n",
    "aliasing = explicit  # Require 'AS' keyword\n",
    "\n",
    "[sqlfluff:rules:aliasing.column]\n",
    "aliasing = explicit\n",
    "\n",
    "[sqlfluff:rules:capitalisation.keywords]\n",
    "capitalisation_policy = upper\n",
    "\n",
    "[sqlfluff:rules:capitalisation.identifiers]\n",
    "capitalisation_policy = lower\n",
    "\n",
    "[sqlfluff:rules:capitalisation.functions]\n",
    "extended_capitalisation_policy = upper\n",
    "\n",
    "[sqlfluff:rules:convention.select_trailing_comma]\n",
    "select_clause_trailing_comma = forbid\n",
    "\n",
    "[sqlfluff:rules:convention.quoted_literals]\n",
    "preferred_quoted_literal_style = single_quotes\n",
    "\n",
    "[sqlfluff:rules:convention.casting_style]\n",
    "preferred_type_casting_style = cast\n",
    "\n",
    "[sqlfluff:rules:structure.subquery]\n",
    "forbid_subquery_in = join\n",
    "```\n",
    "\n",
    "**GitHub Actions Integration:**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/sql-lint.yml\n",
    "name: SQL Lint\n",
    "\n",
    "on: [pull_request]\n",
    "\n",
    "jobs:\n",
    "  lint-sql:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Setup Python\n",
    "        uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "          \n",
    "      - name: Install SQLFluff\n",
    "        run: pip install sqlfluff==2.3.0\n",
    "        \n",
    "      - name: Lint SQL Files\n",
    "        run: |\n",
    "          sqlfluff lint migrations/ schema/ --format github-annotation \\\n",
    "            --annotation-level failure\n",
    "          \n",
    "      - name: Check Formatting (diff)\n",
    "        run: |\n",
    "          sqlfluff fix migrations/ schema/ --dry-run --diff\n",
    "          if [ $? -ne 0 ]; then\n",
    "            echo \"\u274c SQL files need formatting. Run: sqlfluff fix migrations/\"\n",
    "            exit 1\n",
    "          fi\n",
    "```\n",
    "\n",
    "### 47.2.2 Naming Convention Enforcement\n",
    "\n",
    "Consistent naming prevents confusion and enables automated tooling.\n",
    "\n",
    "```yaml\n",
    "# .sqlfluff - Naming convention rules\n",
    "[sqlfluff:rules:convention.casing_columns]\n",
    "# Custom rule (requires plugin or custom check)\n",
    "# Tables: plural, snake_case\n",
    "# Columns: singular, snake_case\n",
    "# Indexes: idx_{table}_{columns}\n",
    "# Constraints: {type}_{table}_{columns} (pk_, fk_, chk_, uq_)\n",
    "\n",
    "[sqlfluff:rules:convention.naming.table]\n",
    "pattern = ^[a-z][a-z0-9_]*s$  # Plural, starts with lowercase\n",
    "\n",
    "[sqlfluff:rules:convention.naming.columns]\n",
    "pattern = ^[a-z][a-z0-9_]*$  # Singular implied by context\n",
    "```\n",
    "\n",
    "**Custom Naming Check Script:**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# scripts/check-naming-conventions.sh\n",
    "\n",
    "check_naming() {\n",
    "    local file=$1\n",
    "    local errors=0\n",
    "    \n",
    "    # Check table names (should be plural)\n",
    "    while IFS= read -r line; do\n",
    "        if [[ $line =~ CREATE[[:space:]]+TABLE[[:space:]]+([a-z_]+) ]]; then\n",
    "            table=\"${BASH_REMATCH[1]}\"\n",
    "            # Check if ends with 's' (simple plural check)\n",
    "            if [[ ! $table =~ s$ ]]; then\n",
    "                echo \"ERROR: Table '$table' should be plural (e.g., ${table}s)\"\n",
    "                ((errors++))\n",
    "            fi\n",
    "        fi\n",
    "    done < \"$file\"\n",
    "    \n",
    "    # Check index naming\n",
    "    while IFS= read -r line; do\n",
    "        if [[ $line =~ CREATE[[:space:]]+INDEX[[:space:]]+([a-z_]+) ]]; then\n",
    "            idx=\"${BASH_REMATCH[1]}\"\n",
    "            if [[ ! $idx =~ ^idx_ ]]; then\n",
    "                echo \"ERROR: Index '$idx' should start with 'idx_'\"\n",
    "                ((errors++))\n",
    "            fi\n",
    "        fi\n",
    "    done < \"$file\"\n",
    "    \n",
    "    # Check foreign key naming\n",
    "    while IFS= read -r line; do\n",
    "        if [[ $line =~ CONSTRAINT[[:space:]]+([a-z_]+)[[:space:]]+FOREIGN[[:space:]]+KEY ]]; then\n",
    "            fk=\"${BASH_REMATCH[1]}\"\n",
    "            if [[ ! $fk =~ ^fk_ ]]; then\n",
    "                echo \"ERROR: FK constraint '$fk' should start with 'fk_'\"\n",
    "                ((errors++))\n",
    "            fi\n",
    "        fi\n",
    "    done < \"$file\"\n",
    "    \n",
    "    return $errors\n",
    "}\n",
    "\n",
    "# Run on all SQL files\n",
    "total_errors=0\n",
    "for file in migrations/*.sql; do\n",
    "    if ! check_naming \"$file\"; then\n",
    "        ((total_errors++))\n",
    "    fi\n",
    "done\n",
    "\n",
    "exit $total_errors\n",
    "```\n",
    "\n",
    "## 47.3 Schema Drift Detection\n",
    "\n",
    "### 47.3.1 Automated Drift Monitoring\n",
    "\n",
    "Schema drift occurs when manual changes (hotfixes, emergency DBA interventions) modify production schema outside the migration pipeline.\n",
    "\n",
    "```python\n",
    "# scripts/drift_detector.py\n",
    "import subprocess\n",
    "import json\n",
    "import sys\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import psycopg2\n",
    "\n",
    "@dataclass\n",
    "class SchemaObject:\n",
    "    type: str\n",
    "    name: str\n",
    "    definition: str\n",
    "\n",
    "def get_expected_schema(migration_files: List[str]) -> List[SchemaObject]:\n",
    "    \"\"\"Apply migrations to temporary database and extract schema\"\"\"\n",
    "    # This would use a temporary Docker container or ephemeral DB\n",
    "    # For brevity, showing conceptual implementation\n",
    "    pass\n",
    "\n",
    "def get_actual_schema(conn_string: str) -> List[SchemaObject]:\n",
    "    \"\"\"Extract current schema from production\"\"\"\n",
    "    conn = psycopg2.connect(conn_string)\n",
    "    cur = conn.cursor()\n",
    "    \n",
    "    objects = []\n",
    "    \n",
    "    # Get tables\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT schemaname, tablename, \n",
    "               pg_catalog.pg_get_ddl(schemaname, tablename)\n",
    "        FROM pg_tables \n",
    "        WHERE schemaname = 'public'\n",
    "    \"\"\")\n",
    "    for row in cur.fetchall():\n",
    "        objects.append(SchemaObject('table', row[1], row[2]))\n",
    "    \n",
    "    # Get indexes\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT schemaname, indexname, indexdef\n",
    "        FROM pg_indexes\n",
    "        WHERE schemaname = 'public'\n",
    "    \"\"\")\n",
    "    for row in cur.fetchall():\n",
    "        objects.append(SchemaObject('index', row[1], row[2]))\n",
    "    \n",
    "    # Get constraints\n",
    "    cur.execute(\"\"\"\n",
    "        SELECT conname, pg_get_constraintdef(oid)\n",
    "        FROM pg_constraint\n",
    "        WHERE connamespace = 'public'::regnamespace\n",
    "    \"\"\")\n",
    "    for row in cur.fetchall():\n",
    "        objects.append(SchemaObject('constraint', row[0], row[1]))\n",
    "    \n",
    "    return objects\n",
    "\n",
    "def detect_drift(expected: List[SchemaObject], actual: List[SchemaObject]) -> dict:\n",
    "    drift = {\n",
    "        'missing_in_prod': [],  # In migrations but not production\n",
    "        'extra_in_prod': [],    # In production but not migrations\n",
    "        'modified': []          # Definition differs\n",
    "    }\n",
    "    \n",
    "    expected_dict = {f\"{o.type}:{o.name}\": o for o in expected}\n",
    "    actual_dict = {f\"{o.type}:{o.name}\": o for o in actual}\n",
    "    \n",
    "    # Check for missing objects\n",
    "    for key, obj in expected_dict.items():\n",
    "        if key not in actual_dict:\n",
    "            drift['missing_in_prod'].append(obj)\n",
    "        elif actual_dict[key].definition != obj.definition:\n",
    "            drift['modified'].append({\n",
    "                'name': key,\n",
    "                'expected': obj.definition,\n",
    "                'actual': actual_dict[key].definition\n",
    "            })\n",
    "    \n",
    "    # Check for extra objects\n",
    "    for key, obj in actual_dict.items():\n",
    "        if key not in expected_dict:\n",
    "            drift['extra_in_prod'].append(obj)\n",
    "    \n",
    "    return drift\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Run in CI against staging/production\n",
    "    drift = detect_drift(\n",
    "        get_expected_schema([\"migrations/\"]),\n",
    "        get_actual_schema(os.environ['DATABASE_URL'])\n",
    "    )\n",
    "    \n",
    "    if any(drift.values()):\n",
    "        print(\"\u274c Schema drift detected!\")\n",
    "        print(json.dumps(drift, indent=2))\n",
    "        sys.exit(1)\n",
    "    else:\n",
    "        print(\"\u2705 Schema matches migrations\")\n",
    "```\n",
    "\n",
    "### 47.3.2 Remediation Strategies\n",
    "\n",
    "When drift is detected, remediation must be careful to avoid data loss.\n",
    "\n",
    "```bash\n",
    "# scripts/reconcile-drift.sh\n",
    "# Emergency script to bring production back to expected state\n",
    "\n",
    "set -e\n",
    "\n",
    "DRIFT_REPORT=\"drift-report.json\"\n",
    "BACKUP_DIR=\"backups/$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "echo \"Creating safety backup...\"\n",
    "mkdir -p $BACKUP_DIR\n",
    "pg_dump $DATABASE_URL --schema-only > $BACKUP_DIR/schema_before.sql\n",
    "pg_dump $DATABASE_URL --data-only --format=custom > $BACKUP_DIR/data.dump\n",
    "\n",
    "# Strategy 1: Extra objects in production (safe to drop if confirmed unused)\n",
    "echo \"Checking for extra objects...\"\n",
    "# Manual review required - never auto-drop in production\n",
    "\n",
    "# Strategy 2: Missing objects (apply missing migrations only)\n",
    "echo \"Applying missing migrations...\"\n",
    "# Run only specific migrations that are missing, not all\n",
    "\n",
    "# Strategy 3: Modified definitions (most dangerous)\n",
    "echo \"Modified objects detected:\"\n",
    "# Require manual ALTER statements to reconcile\n",
    "# Example: If column type differs, create migration to alter\n",
    "```\n",
    "\n",
    "## 47.4 Deployment Strategies and Release Playbooks\n",
    "\n",
    "### 47.4.1 Blue/Green Database Deployments\n",
    "\n",
    "Blue/green deployment minimizes downtime by running two identical environments and switching traffic atomically.\n",
    "\n",
    "```yaml\n",
    "# Deployment architecture\n",
    "# Blue (Current): Active production\n",
    "# Green (New): New version being prepared\n",
    "# Migration Strategy:\n",
    "# 1. Clone Blue to Green\n",
    "# 2. Apply migrations to Green\n",
    "# 3. Sync data changes from Blue to Green (using logical replication)\n",
    "# 4. Switch traffic to Green\n",
    "# 5. Keep Blue as instant rollback\n",
    "\n",
    "# docker-compose.blue-green.yml\n",
    "version: \"3.8\"\n",
    "\n",
    "services:\n",
    "  postgres-blue:\n",
    "    image: postgres:15-alpine  # Current version\n",
    "    volumes:\n",
    "      - blue_data:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5432:5432\"  # Current production\n",
    "  \n",
    "  postgres-green:\n",
    "    image: postgres:16-alpine  # New version (if major upgrade)\n",
    "    volumes:\n",
    "      - green_data:/var/lib/postgresql/data\n",
    "    ports:\n",
    "      - \"5433:5432\"  # New instance, different port\n",
    "  \n",
    "  # Logical replication for zero-downtime cutover\n",
    "  pglogical:\n",
    "    image: pglogical/pglogical:latest\n",
    "    environment:\n",
    "      SOURCE_DB: postgres://user:pass@postgres-blue:5432/app\n",
    "      TARGET_DB: postgres://user:pass@postgres-green:5432/app\n",
    "```\n",
    "\n",
    "**Cutover Procedure:**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# scripts/blue-green-cutover.sh\n",
    "\n",
    "set -e\n",
    "\n",
    "BLUE_HOST=\"postgres-blue\"\n",
    "GREEN_HOST=\"postgres-green\"\n",
    "DATABASE=\"app\"\n",
    "\n",
    "echo \"Phase 1: Preparing Green environment\"\n",
    "# Apply migrations to Green (offline, no traffic)\n",
    "psql -h $GREEN_HOST -d $DATABASE -f migrations/pending.sql\n",
    "\n",
    "echo \"Phase 2: Setting up replication\"\n",
    "# Using pglogical or native logical replication\n",
    "psql -h $BLUE_HOST -d $DATABASE -c \"\n",
    "    SELECT pglogical.create_subscription(\n",
    "        subscription_name := 'sync_to_green',\n",
    "        provider_dsn := 'host=$GREEN_HOST dbname=$DATABASE'\n",
    "    );\n",
    "\"\n",
    "\n",
    "echo \"Phase 3: Waiting for replication lag...\"\n",
    "until [ \"$(psql -h $BLUE_HOST -d $DATABASE -t -c \"SELECT pglogical.wait_for_subscription_sync_complete('sync_to_green');\" | xargs)\" = \"true\" ]; do\n",
    "    echo \"Waiting for sync...\"\n",
    "    sleep 5\n",
    "done\n",
    "\n",
    "echo \"Phase 4: Switching traffic (The Cutover)\"\n",
    "# 1. Set Blue to read-only (optional, for absolute safety)\n",
    "psql -h $BLUE_HOST -d $DATABASE -c \"ALTER DATABASE $DATABASE SET default_transaction_read_only = on;\"\n",
    "\n",
    "# 2. Final sync check\n",
    "psql -h $BLUE_HOST -d $DATABASE -c \"SELECT pglogical.wait_for_subscription_sync_complete('sync_to_green');\"\n",
    "\n",
    "# 3. Update connection pooler (PgBouncer) to point to Green\n",
    "#    Or update DNS/Service Discovery\n",
    "./update-connection-target.sh $GREEN_HOST\n",
    "\n",
    "# 4. Verify Green is receiving traffic\n",
    "sleep 5\n",
    "if ./health-check.sh $GREEN_HOST; then\n",
    "    echo \"\u2705 Cutover successful\"\n",
    "    \n",
    "    # 5. Keep Blue running but read-only for safety period (1 hour)\n",
    "    (sleep 3600 && docker-compose stop postgres-blue) &\n",
    "else\n",
    "    echo \"\u274c Cutover failed, rolling back...\"\n",
    "    ./update-connection-target.sh $BLUE_HOST\n",
    "    exit 1\n",
    "fi\n",
    "```\n",
    "\n",
    "### 47.4.2 Expand and Contract Pattern\n",
    "\n",
    "The only safe way to modify schema without downtime is the expand-contract pattern: expand (add new), migrate (dual write), contract (remove old).\n",
    "\n",
    "```sql\n",
    "-- Example: Rename column 'email' to 'email_address'\n",
    "\n",
    "-- Step 1: EXPAND - Add new column (Deploy 1)\n",
    "ALTER TABLE users ADD COLUMN email_address TEXT;\n",
    "CREATE INDEX CONCURRENTLY idx_users_email_address ON users(email_address);\n",
    "\n",
    "-- Step 2: DUAL WRITE - Application writes to both (Deploy 2)\n",
    "-- Application code:\n",
    "-- INSERT INTO users (email, email_address) VALUES (?, ?)\n",
    "-- UPDATE users SET email = ?, email_address = ? WHERE id = ?\n",
    "\n",
    "-- Step 3: BACKFILL - Migrate existing data (Background job)\n",
    "UPDATE users \n",
    "SET email_address = email \n",
    "WHERE email_address IS NULL \n",
    "  AND id > $last_processed_id;  -- Batch processing\n",
    "\n",
    "-- Step 4: SWITCH READS - Start reading from new column (Deploy 3)\n",
    "-- Application code:\n",
    "-- SELECT email_address as email FROM users\n",
    "\n",
    "-- Step 5: CONTRACT - Remove old column (Deploy 4)\n",
    "-- After confirming no old code references 'email'\n",
    "ALTER TABLE users DROP COLUMN email;\n",
    "```\n",
    "\n",
    "### 47.4.3 Feature Flags with Database Schema\n",
    "\n",
    "Schema changes can be hidden behind feature flags to enable gradual rollout.\n",
    "\n",
    "```sql\n",
    "-- Add new feature schema behind flag\n",
    "CREATE TABLE IF NOT EXISTS new_feature_data (\n",
    "    id UUID PRIMARY KEY,\n",
    "    user_id UUID REFERENCES users(user_id),\n",
    "    data JSONB\n",
    ");\n",
    "\n",
    "-- Application checks flag before using new table\n",
    "-- if (featureFlags.isEnabled('new-billing')) {\n",
    "--     use new_feature_data table\n",
    "-- } else {\n",
    "--     use old billing table\n",
    "-- }\n",
    "\n",
    "-- Migration safety: New tables don't affect old code\n",
    "-- Can be dropped if feature is cancelled\n",
    "```\n",
    "\n",
    "## 47.5 Rollback and Disaster Recovery\n",
    "\n",
    "### 47.5.1 Roll-Forward vs Rollback Decision Matrix\n",
    "\n",
    "| Scenario | Strategy | Implementation |\n",
    "|----------|----------|----------------|\n",
    "| Migration fails mid-way | Rollback | Transaction rolls back automatically (if single transaction) |\n",
    "| Migration succeeds but app fails | Rollback | Run `down` migration or restore from backup |\n",
    "| Data corruption detected | Roll-forward | Fix data with new migration, don't rollback (data loss) |\n",
    "| Performance regression | Roll-forward | Add indexes, optimize queries in hotfix |\n",
    "| Schema incompatible with old app | Roll-forward | Deploy app fix, schema stays |\n",
    "\n",
    "**Golden Rule**: Never rollback a migration that has been running in production for >X minutes (where X is your backup RPO), as you may lose data created since deployment.\n",
    "\n",
    "### 47.5.2 Migration Downgrade Procedures\n",
    "\n",
    "Always maintain `down` migrations, but test them thoroughly.\n",
    "\n",
    "```python\n",
    "# Alembic downgrade example (Python)\n",
    "def downgrade():\n",
    "    # Dangerous: This loses data added to new column\n",
    "    op.drop_column('users', 'new_column')\n",
    "    \n",
    "    # Safer: Keep column, just stop using it (soft delete)\n",
    "    # Then remove in later release after data archived\n",
    "\n",
    "# Better approach: Reversible migrations\n",
    "def upgrade():\n",
    "    op.add_column('users', sa.Column('tier', sa.String(), nullable=True))\n",
    "    op.execute(\"UPDATE users SET tier = 'free'\")\n",
    "    op.alter_column('users', 'tier', nullable=False)\n",
    "\n",
    "def downgrade():\n",
    "    # Restore previous state\n",
    "    op.add_column('users', sa.Column('plan_type', sa.String(), nullable=True))\n",
    "    op.execute(\"UPDATE users SET plan_type = tier\")  # Migrate data back\n",
    "    op.drop_column('users', 'tier')\n",
    "```\n",
    "\n",
    "### 47.5.3 Emergency Runbooks\n",
    "\n",
    "```markdown\n",
    "# Database Emergency Runbook\n",
    "\n",
    "## Scenario 1: Migration Stuck/Locking Tables\n",
    "\n",
    "1. **Identify blocking queries**:\n",
    "   ```sql\n",
    "   SELECT * FROM pg_stat_activity \n",
    "   WHERE state = 'active' \n",
    "     AND query LIKE '%ALTER TABLE%';\n",
    "   ```\n",
    "\n",
    "2. **If safe, cancel migration**:\n",
    "   ```sql\n",
    "   SELECT pg_cancel_backend(pid);\n",
    "   -- If that fails:\n",
    "   SELECT pg_terminate_backend(pid);\n",
    "   ```\n",
    "\n",
    "3. **Check lock status**:\n",
    "   ```sql\n",
    "   SELECT * FROM pg_locks WHERE NOT granted;\n",
    "   ```\n",
    "\n",
    "4. **Decision**:\n",
    "   - If transaction rolled back: Retry with `LOCK TIMEOUT` set\n",
    "   - If transaction committed partially: **Do not rollback**, assess data manually\n",
    "\n",
    "## Scenario 2: Accidental Data Loss\n",
    "\n",
    "1. **Immediate**: Stop all writes to table\n",
    "   ```sql\n",
    "   ALTER TABLE critical_table DISABLE TRIGGER ALL;\n",
    "   ```\n",
    "\n",
    "2. **Assess**: Determine time of incident\n",
    "   ```sql\n",
    "   -- Check Point-in-Time Recovery capability\n",
    "   SHOW archive_mode;\n",
    "   ```\n",
    "\n",
    "3. **Restore**: Create clone from PITR\n",
    "   ```bash\n",
    "   # Create new instance from backup to specific time\n",
    "   pg_restore --target-time \"2024-01-15 14:30:00\" ...\n",
    "   ```\n",
    "\n",
    "4. **Reconcile**: Compare and merge data, don't just overwrite\n",
    "```\n",
    "\n",
    "## 47.6 Pipeline Security and Governance\n",
    "\n",
    "### 47.6.1 Database Access in CI\n",
    "\n",
    "Never use long-lived credentials in CI pipelines.\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/deploy.yml\n",
    "jobs:\n",
    "  deploy-database:\n",
    "    runs-on: ubuntu-latest\n",
    "    permissions:\n",
    "      id-token: write  # For OIDC\n",
    "      contents: read\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Configure AWS Credentials\n",
    "        uses: aws-actions/configure-aws-credentials@v4\n",
    "        with:\n",
    "          role-to-assume: arn:aws:iam::ACCOUNT:role/DatabaseDeployRole\n",
    "          aws-region: us-east-1\n",
    "      \n",
    "      - name: Get Temporary Database Credentials\n",
    "        run: |\n",
    "          # Generate temporary credentials via IAM auth or Secrets Manager\n",
    "          CREDS=$(aws secretsmanager get-secret-value \\\n",
    "            --secret-id prod/db/deploy-credentials \\\n",
    "            --query SecretString --output text)\n",
    "          \n",
    "          echo \"::add-mask::$CREDS\"  # Mask in logs\n",
    "          echo \"DB_CREDS=$CREDS\" >> $GITHUB_ENV\n",
    "      \n",
    "      - name: Run Migrations\n",
    "        run: |\n",
    "          # Credentials auto-expire after 15 minutes\n",
    "          echo \"$DB_CREDS\" | jq -r '.password' | \\\n",
    "            psql -h $DB_HOST -U $(echo $DB_CREDS | jq -r '.username') -d app -f migrations/deploy.sql\n",
    "```\n",
    "\n",
    "### 47.6.2 Approval Gates\n",
    "\n",
    "Database changes to production should require human approval.\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/deploy.yml\n",
    "jobs:\n",
    "  preview-changes:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Generate Migration Preview\n",
    "        run: |\n",
    "          sqitch deploy --to-target HEAD --verify-only --log-only > migration-preview.sql\n",
    "          echo \"### Migration Preview\" >> $GITHUB_STEP_SUMMARY\n",
    "          echo '```sql' >> $GITHUB_STEP_SUMMARY\n",
    "          cat migration-preview.sql >> $GITHUB_STEP_SUMMARY\n",
    "          echo '```' >> $GITHUB_STEP_SUMMARY\n",
    "  \n",
    "  deploy:\n",
    "    needs: preview-changes\n",
    "    environment: production  # Requires manual approval in GitHub\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - name: Deploy\n",
    "        run: sqitch deploy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, you learned:\n",
    "\n",
    "1. **Migration Validation**: Implement multi-stage CI pipelines that check naming conventions (timestamps), detect merge conflicts, validate syntax via dry runs, and test against shadow databases with production-like data volumes; never allow migrations with duplicate timestamps or breaking changes without explicit \"expand-contract\" documentation.\n",
    "\n",
    "2. **SQL Linting**: Configure SQLFluff with PostgreSQL dialect to enforce consistent casing (UPPER keywords, lowercase identifiers), trailing commas, and explicit aliasing; integrate linting into pre-commit hooks and CI checks to prevent style violations from reaching main branch.\n",
    "\n",
    "3. **Drift Detection**: Deploy automated monitors that compare actual production schema against migration-defined expected state; detect manual hotfixes (extra objects, modified definitions) and alert immediately; maintain reconciliation runbooks that prioritize data preservation over schema purity.\n",
    "\n",
    "4. **Deployment Strategies**: Implement blue/green deployments using logical replication for zero-downtime major version upgrades; use expand-contract patterns for all breaking changes (add new column \u2192 dual write \u2192 switch reads \u2192 drop old); hide schema changes behind feature flags to enable gradual rollouts and instant rollback of application logic without reverting schema.\n",
    "\n",
    "5. **Rollback Procedures**: Prefer roll-forward (fixing data with new migrations) over rollback for any change live >15 minutes to prevent data loss; maintain tested `down` migrations but treat them as emergency-only; create explicit runbooks for stuck migrations (cancel backends, set lock timeouts) and data recovery (PITR clones, table disabling).\n",
    "\n",
    "6. **Pipeline Security**: Use OIDC and temporary credentials (15-minute expiry) for database access in CI rather than static passwords; implement approval gates for production deployments that require human review of migration previews; maintain audit trails of who deployed what and when via structured logging in migration tools.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** In Chapter 48, we will explore Documentation and Standards\u2014the \"handbook within the handbook\"\u2014covering SQL style guides, schema review checklists, query review procedures, and operational runbook templates that ensure organizational consistency and knowledge retention."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='46. database_testing_strategies.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='48. documentation_and_standards.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}