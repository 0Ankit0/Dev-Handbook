{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 36: Scaling Strategies (What Actually Works)\n",
    "\n",
    "Scaling PostgreSQL is fundamentally about choosing the right tradeoffs between consistency, availability, and operational complexity. This chapter provides a decision framework for scaling decisions\u2014emphasizing that vertical scaling and query optimization handle 80% of use cases, while horizontal partitioning (sharding) should be the option of last resort. We cover practical patterns for read scaling, sharding when unavoidable, and caching strategies that protect the database without compromising consistency.\n",
    "\n",
    "---\n",
    "\n",
    "## 36.1 Vertical Scaling (The First 80%)\n",
    "\n",
    "Before considering complex distributed architectures, exhaust vertical scaling. Modern hardware can support PostgreSQL handling tens of thousands of transactions per second and terabytes of data.\n",
    "\n",
    "### 36.1.1 Hardware Sizing Priorities\n",
    "\n",
    "**The Hierarchy of Needs**:\n",
    "\n",
    "1. **Storage IOPS** (Most Critical)\n",
    "   - Random read/write performance determines transaction throughput\n",
    "   - NVMe SSDs: 100K+ IOPS per device\n",
    "   - Cloud: Provisioned IOPS (AWS io2, Azure Premium SSD v2)\n",
    "   - RAID 10 for critical workloads (striping + mirroring)\n",
    "\n",
    "2. **Memory** (Second Critical)\n",
    "   - Rule: `shared_buffers` + OS cache = Working set size\n",
    "   - For OLTP: 64-256 GB RAM typical\n",
    "   - For analytics: 512 GB+ to cache hot partitions\n",
    "\n",
    "3. **CPU** (Third)\n",
    "   - PostgreSQL scales well to 32-64 cores\n",
    "   - Beyond 64 cores: Diminishing returns due to lock contention\n",
    "   - Prefer faster cores over more cores for OLTP\n",
    "\n",
    "4. **Network** (Often Overlooked)\n",
    "   - 10 Gbps minimum for replicas\n",
    "   - 25-100 Gbps for high-throughput OLAP\n",
    "\n",
    "```sql\n",
    "-- Determine if you need more memory (cache hit ratio)\n",
    "SELECT \n",
    "    sum(heap_blks_read) as disk_read,\n",
    "    sum(heap_blks_hit) as cache_hit,\n",
    "    sum(heap_blks_hit) / (sum(heap_blks_hit) + sum(heap_blks_read))::float as ratio\n",
    "FROM pg_statio_user_tables;\n",
    "\n",
    "-- Target: > 99% cache hit ratio for OLTP\n",
    "-- If < 95%, you need more RAM or query optimization\n",
    "```\n",
    "\n",
    "### 36.1.2 Configuration for Vertical Scale\n",
    "\n",
    "```ini\n",
    "# postgresql.conf for high-end hardware (64+ cores, 256GB+ RAM)\n",
    "\n",
    "# Memory\n",
    "shared_buffers = 64GB                  # 25% of RAM (max ~32GB effective due to double buffering)\n",
    "effective_cache_size = 192GB           # 75% of RAM (planner hint)\n",
    "work_mem = 256MB                       # Per-operation (sorts, hashes)\n",
    "                                       # 256MB \u00d7 max_connections = danger zone\n",
    "                                       # Better: 32MB with moderate connections + pooling\n",
    "maintenance_work_mem = 2GB             # VACUUM, CREATE INDEX operations\n",
    "\n",
    "# Parallelism\n",
    "max_parallel_workers_per_gather = 8    # Per query parallel workers\n",
    "max_parallel_workers = 32              # Total parallel workers cluster-wide\n",
    "max_parallel_maintenance_workers = 8   # CREATE INDEX, VACUUM parallel workers\n",
    "\n",
    "# WAL and Checkpointing (high write throughput)\n",
    "max_wal_size = 16GB                    # Allow more WAL before forced checkpoint\n",
    "checkpoint_completion_target = 0.9     # Spread writes over 90% of checkpoint interval\n",
    "wal_buffers = 16MB                     # Increase with high write volume\n",
    "\n",
    "# Connection limits (with pooling!)\n",
    "max_connections = 500                  # Hard limit\n",
    "superuser_reserved_connections = 5     # Emergency access\n",
    "```\n",
    "\n",
    "### 36.1.3 When to Stop Vertical Scaling\n",
    "\n",
    "**Hard Limits**:\n",
    "- Single-server PostgreSQL practical limit: ~100K TPS, 10-50 TB data\n",
    "- Network bandwidth saturation on replicas\n",
    "- Cost-efficiency: Cloud instances beyond 64 vCPUs / 512 GB RAM have diminishing price/performance\n",
    "\n",
    "**Decision Point**:\n",
    "If CPU > 80% sustained AND queries are already optimized AND you cannot partition by tenant/date, consider horizontal scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## 36.2 Read Scaling with Replicas\n",
    "\n",
    "Read replicas are the industry standard for scaling read-heavy workloads (typical web apps have 80% reads, 20% writes).\n",
    "\n",
    "### 36.2.1 Replication Topologies\n",
    "\n",
    "**Single Primary, Multiple Standbys**:\n",
    "```text\n",
    "Primary (Write)\n",
    "    \u251c\u2500 Hot Standby 1 (Read)\n",
    "    \u251c\u2500 Hot Standby 2 (Read)\n",
    "    \u2514\u2500 Hot Standby 3 (Read + Reporting)\n",
    "```\n",
    "\n",
    "**Cascading Replication** (for many replicas):\n",
    "```text\n",
    "Primary\n",
    "    \u251c\u2500 Standby 1\n",
    "    \u2502     \u251c\u2500 Standby 1a\n",
    "    \u2502     \u2514\u2500 Standby 1b\n",
    "    \u2514\u2500 Standby 2\n",
    "```\n",
    "- Reduces load on primary (only replicates to 2 standbys, they replicate to others)\n",
    "- Increases lag for cascading nodes\n",
    "\n",
    "### 36.2.2 Load Balancing Strategies\n",
    "\n",
    "**Application-Level Routing** (Recommended for precision):\n",
    "\n",
    "```python\n",
    "# Python with SQLAlchemy\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Write engine (primary)\n",
    "write_engine = create_engine(\n",
    "    \"postgresql://user:pass@primary.internal:5432/production\",\n",
    "    pool_size=20,\n",
    "    max_overflow=10\n",
    ")\n",
    "\n",
    "# Read engines (standbys with health checks)\n",
    "read_engines = [\n",
    "    create_engine(\"postgresql://user:pass@replica1.internal:5432/production\", pool_size=10),\n",
    "    create_engine(\"postgresql://user:pass@replica2.internal:5432/production\", pool_size=10)\n",
    "]\n",
    "\n",
    "def get_session(read_only=False):\n",
    "    if read_only:\n",
    "        # Round-robin or least-connections selection\n",
    "        return random.choice(read_engines)\n",
    "    return write_engine\n",
    "\n",
    "# Usage\n",
    "with get_session(read_only=True).connect() as conn:\n",
    "    result = conn.execute(\"SELECT * FROM products WHERE category = %s\", (category,))\n",
    "```\n",
    "\n",
    "**PgPool-II or HAProxy for Transparent Routing**:\n",
    "\n",
    "```ini\n",
    "# HAProxy configuration for read scaling\n",
    "listen postgres_read\n",
    "    bind *:5433\n",
    "    mode tcp\n",
    "    option tcp-check\n",
    "    tcp-check expect string 5\n",
    "    \n",
    "    # Health check: connect and expect PostgreSQL protocol version 5\n",
    "    server replica1 10.0.2.10:5432 check inter 2s rise 2 fall 3\n",
    "    server replica2 10.0.2.11:5432 check inter 2s rise 2 fall 3\n",
    "    server replica3 10.0.2.12:5432 check inter 2s rise 2 fall 3 backup  # Backup only\n",
    "```\n",
    "\n",
    "### 36.2.3 Handling Replication Lag\n",
    "\n",
    "**The Stale Read Problem**:\n",
    "User writes comment \u2192 Primary accepts \u2192 Page refresh hits replica \u2192 Comment missing (lag = 200ms)\n",
    "\n",
    "**Solutions**:\n",
    "\n",
    "1. **Session Stickiness** (Post-Write Redirect):\n",
    "```python\n",
    "# After write, force subsequent reads to primary for a few seconds\n",
    "def create_comment(user_id, content):\n",
    "    with write_engine.connect() as conn:\n",
    "        comment_id = conn.execute(\n",
    "            \"INSERT INTO comments (user_id, content) VALUES (%s, %s) RETURNING id\",\n",
    "            (user_id, content)\n",
    "        ).scalar()\n",
    "        \n",
    "        # Flag session: next 5 seconds read from primary\n",
    "        session['read_from_primary_until'] = time.time() + 5\n",
    "        return comment_id\n",
    "\n",
    "def get_comments(user_id):\n",
    "    if session.get('read_from_primary_until', 0) > time.time():\n",
    "        engine = write_engine  # Primary\n",
    "    else:\n",
    "        engine = random.choice(read_engines)  # Replicas\n",
    "```\n",
    "\n",
    "2. **Logical Replication Slot Lag Monitoring**:\n",
    "```sql\n",
    "-- Don't route to replicas with > 5 seconds lag\n",
    "SELECT \n",
    "    client_addr,\n",
    "    pg_wal_lsn_diff(sent_lsn, replay_lsn) as lag_bytes,\n",
    "    extract(epoch from (now() - backend_start)) as connected_seconds\n",
    "FROM pg_stat_replication\n",
    "WHERE application_name = 'replica_1';\n",
    "\n",
    "# Application logic: If lag_bytes > 10MB, remove from rotation\n",
    "```\n",
    "\n",
    "3. **Synchronous Commit for Critical Reads**:\n",
    "```sql\n",
    "-- Force write to wait for replica sync (only for critical operations)\n",
    "SET synchronous_commit = remote_apply;\n",
    "INSERT INTO payments ...;\n",
    "-- Now safe to read from replica\n",
    "```\n",
    "\n",
    "### 36.2.4 Read Replica Anti-Patterns\n",
    "\n",
    "**Anti-Pattern**: Treating replicas as identical to primary for all queries.\n",
    "- Long-running analytics queries on replicas can cause replication lag\n",
    "- Vacuum conflicts on hot standbys\n",
    "\n",
    "**Solution**: Dedicated reporting replica with `hot_standby_feedback = on` and `max_standby_streaming_delay` tuned, or use logical replication to data warehouse.\n",
    "\n",
    "---\n",
    "\n",
    "## 36.3 Sharding (The Nuclear Option)\n",
    "\n",
    "Sharding splits data across multiple PostgreSQL instances (nodes). It solves write scalability limits but introduces massive operational complexity.\n",
    "\n",
    "### 36.3.1 When to Shard\n",
    "\n",
    "**Valid Reasons**:\n",
    "- Single-node write throughput exceeded (~50K-100K writes/sec)\n",
    "- Data size exceeds single-node storage (10+ TB with growth)\n",
    "- Geographic latency requirements (EU data in EU, US data in US)\n",
    "- Compliance (data residency requirements)\n",
    "\n",
    "**Invalid Reasons** (Don't Shard Yet):\n",
    "- \"We might need it later\" (Premature optimization)\n",
    "- CPU at 20% (Optimize queries first)\n",
    "- Just learned about microservices (Don't use DB sharding for service boundaries)\n",
    "\n",
    "### 36.3.2 Sharding Strategies\n",
    "\n",
    "**1. Hash Sharding (Distributed evenly)**\n",
    "\n",
    "```sql\n",
    "-- Shard key: user_id % 4\n",
    "-- Shard 0: user_id % 4 = 0\n",
    "-- Shard 1: user_id % 4 = 1\n",
    "-- etc.\n",
    "\n",
    "-- Application logic\n",
    "def get_shard(user_id):\n",
    "    shard_id = user_id % 4\n",
    "    return shards[shard_id]  # Array of 4 connection pools\n",
    "\n",
    "def get_user(user_id):\n",
    "    conn = get_shard(user_id).getconn()\n",
    "    return conn.execute(\"SELECT * FROM users WHERE user_id = %s\", (user_id,))\n",
    "```\n",
    "\n",
    "**Pros**: Even distribution\n",
    "**Cons**: Range queries scan all shards; resharding required when adding nodes\n",
    "\n",
    "**2. Range Sharding (Time or ID ranges)**\n",
    "\n",
    "```sql\n",
    "-- Shard 0: user_id 1 - 1,000,000\n",
    "-- Shard 1: user_id 1,000,001 - 2,000,000\n",
    "-- Or time-based:\n",
    "-- Shard 0: orders_2023\n",
    "-- Shard 1: orders_2024\n",
    "\n",
    "-- Application router\n",
    "def get_shard(user_id):\n",
    "    if user_id <= 1000000:\n",
    "        return shard_0\n",
    "    elif user_id <= 2000000:\n",
    "        return shard_1\n",
    "    # etc.\n",
    "```\n",
    "\n",
    "**Pros**: Efficient range queries; easy to archive old shards\n",
    "**Cons**: Hot spots (newest shard gets all writes); uneven growth\n",
    "\n",
    "**3. Directory-Based Sharding (Lookup table)**\n",
    "\n",
    "```sql\n",
    "-- Central lookup table (small, cached)\n",
    "CREATE TABLE shard_directory (\n",
    "    tenant_id BIGINT PRIMARY KEY,\n",
    "    shard_node VARCHAR(50),  -- 'shard_001', 'shard_002'\n",
    "    created_at TIMESTAMPTZ\n",
    ");\n",
    "\n",
    "-- Application checks directory before each query (cached in Redis/memcached)\n",
    "def get_shard(tenant_id):\n",
    "    shard_node = redis.get(f\"shard:{tenant_id}\")\n",
    "    if not shard_node:\n",
    "        shard_node = query_directory_table(tenant_id)\n",
    "        redis.setex(f\"shard:{tenant_id}\", 3600, shard_node)\n",
    "    return connections[shard_node]\n",
    "```\n",
    "\n",
    "**Pros**: Flexible; can move tenants between shards\n",
    "**Cons**: Directory is single point of failure; extra lookup latency\n",
    "\n",
    "### 36.3.3 Cross-Shard Operations (The Hard Part)\n",
    "\n",
    "**JOINs across shards don't work**.\n",
    "\n",
    "```sql\n",
    "-- If users in shard_1 and orders in shard_2, this fails:\n",
    "SELECT u.*, o.* \n",
    "FROM users u \n",
    "JOIN orders o ON u.user_id = o.user_id;\n",
    "-- Must fetch users from shard_1, orders from shard_2, join in application\n",
    "```\n",
    "\n",
    "**Application-Level Join**:\n",
    "```python\n",
    "def get_user_with_orders(user_id):\n",
    "    # Get user from shard\n",
    "    user = shard.execute(\"SELECT * FROM users WHERE user_id = %s\", (user_id,))\n",
    "    \n",
    "    # Get orders (hopefully same shard if co-located)\n",
    "    orders = shard.execute(\"SELECT * FROM orders WHERE user_id = %s\", (user_id,))\n",
    "    \n",
    "    # Manual join\n",
    "    return {**user, 'orders': orders}\n",
    "```\n",
    "\n",
    "**Global Tables (Replicated to all shards)**:\n",
    "Small lookup tables (countries, currencies) replicated to every shard to avoid cross-shard joins.\n",
    "\n",
    "**Aggregation across shards**:\n",
    "Map-reduce pattern required.\n",
    "```python\n",
    "# Count total users across all shards\n",
    "total = 0\n",
    "for shard in shards:\n",
    "    count = shard.execute(\"SELECT COUNT(*) FROM users\").scalar()\n",
    "    total += count\n",
    "```\n",
    "\n",
    "### 36.3.4 Sharding Tools (PostgreSQL-Specific)\n",
    "\n",
    "**1. Citus (Extension)**\n",
    "- Transforms PostgreSQL into a distributed database\n",
    "- Coordinator node routes queries to worker shards\n",
    "- Supports distributed joins, aggregates\n",
    "\n",
    "```sql\n",
    "-- Citus setup\n",
    "CREATE EXTENSION citus;\n",
    "\n",
    "-- Add worker nodes\n",
    "SELECT * FROM citus_add_node('worker-1', 5432);\n",
    "SELECT * FROM citus_add_node('worker-2', 5432);\n",
    "\n",
    "-- Distribute table\n",
    "SELECT create_distributed_table('users', 'user_id');\n",
    "\n",
    "-- Queries automatically routed; aggregates handled by coordinator\n",
    "```\n",
    "\n",
    "**2. PostgreSQL FDW (Foreign Data Wrappers)**\n",
    "- Manual sharding with postgres_fdw\n",
    "- Joins possible but slow (pulls all data to coordinator)\n",
    "\n",
    "```sql\n",
    "-- On coordinator\n",
    "CREATE SERVER shard_1 FOREIGN DATA WRAPPER postgres_fdw \n",
    "OPTIONS (host 'shard1.internal', dbname 'production');\n",
    "\n",
    "CREATE USER MAPPING FOR coordinator SERVER shard_1 \n",
    "OPTIONS (user 'shard_user', password 'secret');\n",
    "\n",
    "CREATE FOREIGN TABLE users_shard_1 (...) SERVER shard_1 OPTIONS (table_name 'users');\n",
    "\n",
    "-- Create view unioning all shards\n",
    "CREATE VIEW all_users AS \n",
    "SELECT * FROM users_shard_1\n",
    "UNION ALL\n",
    "SELECT * FROM users_shard_2;\n",
    "```\n",
    "\n",
    "**3. Application Sharding (DIY)**\n",
    "- Most common for SaaS (tenant ID-based)\n",
    "- No magic, just routing logic in app\n",
    "\n",
    "---\n",
    "\n",
    "## 36.4 Caching Strategies\n",
    "\n",
    "Caching reduces database load but introduces consistency challenges. Use caching for read-heavy, low-mutation data.\n",
    "\n",
    "### 36.4.1 Cache Layers\n",
    "\n",
    "**L1: Application Memory** (Fastest, smallest)\n",
    "```python\n",
    "# LRU cache in Python\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=10000)\n",
    "def get_user_by_id(user_id):\n",
    "    return db.execute(\"SELECT * FROM users WHERE id = %s\", (user_id,))\n",
    "```\n",
    "\n",
    "**L2: Redis/Memcached** (Fast, network round-trip)\n",
    "```python\n",
    "def get_user(user_id):\n",
    "    # Check cache\n",
    "    cached = redis.get(f\"user:{user_id}\")\n",
    "    if cached:\n",
    "        return json.loads(cached)\n",
    "    \n",
    "    # Cache miss: query DB\n",
    "    user = db.execute(\"SELECT * FROM users WHERE id = %s\", (user_id,)).fetchone()\n",
    "    \n",
    "    # Write to cache (TTL = 5 minutes)\n",
    "    redis.setex(f\"user:{user_id}\", 300, json.dumps(user))\n",
    "    return user\n",
    "```\n",
    "\n",
    "**L3: CDN** (Static assets, rarely changing API responses)\n",
    "\n",
    "### 36.4.2 Cache Invalidation Strategies\n",
    "\n",
    "**Cache-Aside (Lazy Loading)**:\n",
    "- Read: Check cache \u2192 miss \u2192 read DB \u2192 write cache\n",
    "- Write: Write DB \u2192 delete cache (not update, to avoid race conditions)\n",
    "\n",
    "```python\n",
    "def update_user(user_id, new_data):\n",
    "    # Update database first (source of truth)\n",
    "    db.execute(\"UPDATE users SET ... WHERE id = %s\", (user_id, ...))\n",
    "    \n",
    "    # Invalidate cache (delete, don't update)\n",
    "    redis.delete(f\"user:{user_id}\")\n",
    "    # Next read will refresh from DB\n",
    "```\n",
    "\n",
    "**Why Delete Instead of Update?**\n",
    "```\n",
    "Thread A: Read user (gets v1)\n",
    "Thread B: Update user to v2, updates cache to v2\n",
    "Thread A: Writes stale v1 back to cache (race condition)\n",
    "```\n",
    "\n",
    "**Write-Through**:\n",
    "- Write: Update DB and cache synchronously\n",
    "- Slower writes, consistent reads\n",
    "- Risk: Cache write fails \u2192 inconsistency\n",
    "\n",
    "**Write-Behind (Async)**:\n",
    "- Write: Update cache immediately, queue DB write\n",
    "- Fastest but risk of data loss on crash\n",
    "\n",
    "### 36.4.3 PostgreSQL LISTEN/NOTIFY for Cache Invalidation\n",
    "\n",
    "Use database triggers to notify cache layer of changes.\n",
    "\n",
    "```sql\n",
    "-- Function to broadcast changes\n",
    "CREATE OR REPLACE FUNCTION cache_invalidation() RETURNS TRIGGER AS $$\n",
    "BEGIN\n",
    "    PERFORM pg_notify('cache_invalidation', \n",
    "        json_build_object(\n",
    "            'table', TG_TABLE_NAME,\n",
    "            'id', COALESCE(NEW.id, OLD.id),\n",
    "            'operation', TG_OP\n",
    "        )::text\n",
    "    );\n",
    "    RETURN COALESCE(NEW, OLD);\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "-- Trigger on users table\n",
    "CREATE TRIGGER user_cache_invalidation\n",
    "AFTER INSERT OR UPDATE OR DELETE ON users\n",
    "FOR EACH ROW EXECUTE FUNCTION cache_invalidation();\n",
    "```\n",
    "\n",
    "**Application Listener**:\n",
    "```python\n",
    "# Python async listener\n",
    "import asyncio\n",
    "import asyncpg\n",
    "\n",
    "async def cache_invalidation_listener():\n",
    "    conn = await asyncpg.connect(database='production')\n",
    "    await conn.add_listener('cache_invalidation', handle_notification)\n",
    "    \n",
    "    while True:\n",
    "        await asyncio.sleep(3600)\n",
    "\n",
    "def handle_notification(connection, pid, channel, payload):\n",
    "    data = json.loads(payload)\n",
    "    cache_key = f\"{data['table']}:{data['id']}\"\n",
    "    redis.delete(cache_key)\n",
    "    print(f\"Invalidated {cache_key}\")\n",
    "```\n",
    "\n",
    "### 36.4.4 Materialized Views as Caches\n",
    "\n",
    "For complex aggregations that are expensive to compute:\n",
    "\n",
    "```sql\n",
    "-- Materialized view (snapshot)\n",
    "CREATE MATERIALIZED VIEW daily_sales_summary AS\n",
    "SELECT \n",
    "    date_trunc('day', created_at) as sale_date,\n",
    "    region,\n",
    "    sum(amount) as total_sales,\n",
    "    count(*) as transaction_count\n",
    "FROM orders\n",
    "GROUP BY 1, 2;\n",
    "\n",
    "-- Index for fast refresh\n",
    "CREATE INDEX idx_daily_sales_date ON daily_sales_summary(sale_date);\n",
    "\n",
    "-- Refresh strategy (concurrent = no lock)\n",
    "REFRESH MATERIALIZED VIEW CONCURRENTLY daily_sales_summary;\n",
    "\n",
    "-- Automation: Run every 5 minutes via pg_cron or external scheduler\n",
    "```\n",
    "\n",
    "**Tradeoff**: Stale data (max 5 minutes old), but O(1) query time vs O(n) table scan.\n",
    "\n",
    "---\n",
    "\n",
    "## 36.5 The Scaling Decision Tree\n",
    "\n",
    "```\n",
    "Is CPU/Memory saturated?\n",
    "\u251c\u2500\u2500 No \u2192 Optimize queries (indexes, EXPLAIN ANALYZE)\n",
    "\u2514\u2500\u2500 Yes \u2192 Is it read-heavy (>80% reads)?\n",
    "    \u251c\u2500\u2500 Yes \u2192 Add read replicas (Chapter 33)\n",
    "    \u2514\u2500\u2500 No \u2192 Is it write-heavy?\n",
    "        \u251c\u2500\u2500 Yes \u2192 Can you partition by time/tenant?\n",
    "        \u2502   \u251c\u2500\u2500 Yes \u2192 Partitioning (Declarative, Chapter 10)\n",
    "        \u2502   \u2514\u2500\u2500 No \u2192 Consider sharding (Citus or DIY)\n",
    "        \u2514\u2500\u2500 No \u2192 Vertical scaling (bigger instance)\n",
    "            \u2514\u2500\u2500 Maxed out vertical?\n",
    "                \u2514\u2500\u2500 Sharding (last resort)\n",
    "```\n",
    "\n",
    "**Golden Rule**: Sharding is a business decision (data residency, compliance) as much as a technical one. If you shard for performance alone, you will likely regret the operational burden.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, you learned:\n",
    "\n",
    "1. **Vertical Scaling**: Maximize single-node performance before distributing. Prioritize NVMe storage IOPS, then RAM (for cache hit ratios >99%), then CPU cores. Modern cloud instances can handle 100K+ TPS before requiring horizontal scaling.\n",
    "\n",
    "2. **Read Scaling**: Use streaming replicas for read-heavy workloads (80/20 read/write split). Implement application-level routing to direct queries to replicas, with lag monitoring to prevent stale reads. Use session stickiness or synchronous commit for critical read-after-write consistency.\n",
    "\n",
    "3. **Sharding**: Only shard when single-node write throughput (<100K TPS) or storage (>10TB) is exhausted, or for data residency compliance. Use hash sharding for even distribution, range sharding for time-series archival, or directory-based routing for tenant isolation. Avoid cross-shard joins\u2014they require application-level coordination.\n",
    "\n",
    "4. **Caching**: Implement multi-layer caching (application LRU, Redis) using cache-aside pattern (invalidate on write, lazy load on read). Use PostgreSQL `LISTEN/NOTIFY` for cache invalidation triggers. Materialized views provide database-native caching for expensive aggregations with controlled staleness.\n",
    "\n",
    "5. **Operational Reality**: Sharding multiplies operational complexity (backups, monitoring, schema changes) by the number of shards. Exhaust query optimization (indexes, partitioning), connection pooling, and read replicas before sharding. When sharding is unavoidable, use extensions like Citus or managed services (Google Spanner, CockroachDB, Yugabyte) rather than DIY unless absolutely necessary.\n",
    "\n",
    "**Next**: In Chapter 37, we will explore Local Development Workflows\u2014covering reproducible development environments with Docker Compose, database seeding strategies, managing multiple PostgreSQL versions locally, and testing patterns that ensure production parity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='35. connection_management_and_pooling.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../10. operations_and_observability_dev_sre_friendly/37. configuration_basics_practical_not_mystical.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}