{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0cef497d",
   "metadata": {},
   "source": [
    "# Chapter 17: EXPLAIN Like You Mean It\n",
    "\n",
    "Reading execution plans is the definitive skill for PostgreSQL performance tuning. This chapter transforms EXPLAIN output from cryptic text into actionable intelligence, covering interpretation methodologies, buffer analysis, anti-pattern detection, and statistical rigor when validating optimizations.\n",
    "\n",
    "## 17.1 EXPLAIN Fundamentals\n",
    "\n",
    "PostgreSQL provides multiple EXPLAIN modes that serve different diagnostic purposes. Understanding when to use each format prevents misinterpretation of performance characteristics.\n",
    "\n",
    "### 17.1.1 EXPLAIN vs EXPLAIN ANALYZE\n",
    "\n",
    "```sql\n",
    "-- EXPLAIN (estimates only): Shows planner's cost model predictions\n",
    "EXPLAIN SELECT * FROM users WHERE user_id = 123;\n",
    "\n",
    "-- Output:\n",
    "-- Index Scan using users_pkey on users  (cost=0.29..8.30 rows=1 width=72)\n",
    "--   Index Cond: (user_id = 123)\n",
    "\n",
    "-- Key characteristics:\n",
    "-- - No query execution (fast, safe on production)\n",
    "-- - Shows estimated costs (arbitrary units) and row counts\n",
    "-- - Reveals chosen plan without runtime overhead\n",
    "-- - Cannot detect runtime issues (memory spills, lock contention)\n",
    "\n",
    "-- EXPLAIN ANALYZE (actual execution): Runs query and compares estimates to reality\n",
    "EXPLAIN (ANALYZE) SELECT * FROM users WHERE user_id = 123;\n",
    "\n",
    "-- Output adds:\n",
    "-- (actual time=0.012..0.013 rows=1 loops=1)\n",
    "-- Planning Time: 0.150 ms\n",
    "-- Execution Time: 0.025 ms\n",
    "\n",
    "-- Critical differences:\n",
    "-- - Actually executes the query (writes happen, locks acquired)\n",
    "-- - Shows actual time (milliseconds) and actual row counts\n",
    "-- - Planning Time: Parser + Rewriter + Planner duration\n",
    "-- - Execution Time: Executor runtime (excluding planning)\n",
    "-- - loops: How many times node executed (crucial for nested loops)\n",
    "\n",
    "-- DANGER: EXPLAIN ANALYZE executes writes!\n",
    "EXPLAIN (ANALYZE) DELETE FROM orders WHERE status = 'pending';\n",
    "-- Actually deletes rows! Use transactions for safety:\n",
    "BEGIN;\n",
    "EXPLAIN (ANALYZE) DELETE FROM orders WHERE status = 'pending';\n",
    "ROLLBACK;\n",
    "\n",
    "-- EXPLAIN (ANALYZE, BUFFERS): Adds I/O statistics (essential for performance)\n",
    "EXPLAIN (ANALYZE, BUFFERS) SELECT * FROM users WHERE user_id = 123;\n",
    "\n",
    "-- Additional output:\n",
    "-- Buffers: shared hit=3\n",
    "-- - shared hit: Block found in shared buffer cache (fast)\n",
    "-- - shared read: Block read from disk (slow)\n",
    "-- - shared dirtied: Block modified in cache\n",
    "-- - shared written: Block written to disk by checkpoint/bgwriter\n",
    "-- - local hits/read: For temporary tables\n",
    "-- - temp read/write: For work_mem spills to disk\n",
    "```\n",
    "\n",
    "### 17.1.2 Output Formats\n",
    "\n",
    "```sql\n",
    "-- TEXT (default): Human-readable, compact\n",
    "EXPLAIN (FORMAT TEXT) SELECT * FROM users;\n",
    "\n",
    "-- JSON (machine-readable, programmatic analysis):\n",
    "EXPLAIN (FORMAT JSON) SELECT * FROM users;\n",
    "-- Useful for: Automated plan analysis tools, diffing plans, storage\n",
    "\n",
    "-- XML (verbose, tool integration):\n",
    "EXPLAIN (FORMAT XML) SELECT * FROM users;\n",
    "\n",
    "-- YAML (structured, moderately readable):\n",
    "EXPLAIN (FORMAT YAML) SELECT * FROM users;\n",
    "\n",
    "-- Recommended combination for deep analysis:\n",
    "EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON, COSTS, VERBOSE, TIMING)\n",
    "SELECT * FROM users WHERE email = 'test@example.com';\n",
    "\n",
    "-- COSTS: Show cost calculations (can disable for cleaner output)\n",
    "-- VERBOSE: Show additional info (schema names, column names, partition pruning)\n",
    "-- TIMING: Include actual time (can disable if timing overhead concerns)\n",
    "-- SETTINGS: Include modified configuration parameters (PostgreSQL 12+)\n",
    "```\n",
    "\n",
    "## 17.2 Reading Plans from the Bottom Up\n",
    "\n",
    "Execution plans are trees where data flows from leaf nodes (scans) through intermediate nodes (joins, sorts) to the root. Reading bottom-up reveals the actual execution flow.\n",
    "\n",
    "### 17.2.1 Node Structure and Indentation\n",
    "\n",
    "```sql\n",
    "EXPLAIN (ANALYZE, BUFFERS)\n",
    "SELECT u.email, COUNT(o.order_id)\n",
    "FROM users u\n",
    "JOIN orders o ON u.user_id = o.user_id\n",
    "WHERE u.status = 'active'\n",
    "  AND o.created_at > '2024-01-01'\n",
    "GROUP BY u.email\n",
    "ORDER BY COUNT(o.order_id) DESC\n",
    "LIMIT 10;\n",
    "\n",
    "-- Typical plan structure:\n",
    "-- Limit  (cost=100.00..110.00 rows=10 width=36) (actual time=5.234..5.240 rows=10 loops=1)\n",
    "--   -> Sort  (cost=100.00..105.00 rows=100 width=36) (actual time=5.230..5.232 rows=10 loops=1)\n",
    "--         Sort Key: (count(o.order_id)) DESC\n",
    "--         Sort Method: top-N heapsort  Memory: 26kB\n",
    "--         -> HashAggregate  (cost=50.00..60.00 rows=100 width=36) (actual time=4.800..4.900 rows=150 loops=1)\n",
    "--               Group Key: u.email\n",
    "--               Batches: 1  Memory Usage: 40kB\n",
    "--               -> Hash Join  (cost=20.00..40.00 rows=500 width=72) (actual time=0.500..3.200 rows=1000 loops=1)\n",
    "--                     Hash Cond: (o.user_id = u.user_id)\n",
    "--                     -> Seq Scan on orders o  (cost=0.00..15.00 rows=500 width=16) (actual time=0.100..2.000 rows=5000 loops=1)\n",
    "--                           Filter: (created_at > '2024-01-01')\n",
    "--                           Rows Removed by Filter: 45000\n",
    "--                     -> Hash  (cost=15.00..15.00 rows=1000 width=72) (actual time=0.200..0.200 rows=1000 loops=1)\n",
    "--                           Buckets: 1024  Batches: 1  Memory Usage: 50kB\n",
    "--                           -> Seq Scan on users u  (cost=0.00..15.00 rows=1000 width=72) (actual time=0.050..0.150 rows=1000 loops=1)\n",
    "--                                 Filter: (status = 'active')\n",
    "--                                 Rows Removed by Filter: 500\n",
    "\n",
    "-- Reading methodology:\n",
    "-- 1. Start at bottom: Seq Scan on users (line 15)\n",
    "--    - Reads users table, filters for status='active'\n",
    "--    - Returns 1000 rows (actual), estimated 1000 (rows=1000)\n",
    "--    - Cost: 0.00..15.00 (startup..total)\n",
    "--    - Time: 0.050ms to first row, 0.150ms total\n",
    "--    - Filter removed 500 rows (1500 total, 1000 passed)\n",
    "\n",
    "-- 2. Next up: Hash (line 13)\n",
    "--    - Builds hash table from users scan\n",
    "--    - Memory: 50kB (fits in work_mem)\n",
    "--    - Buckets: 1024 (hash table size)\n",
    "\n",
    "-- 3. Next: Seq Scan on orders (line 9)\n",
    "--    - Reads orders table\n",
    "--    - Filter: created_at > '2024-01-01'\n",
    "--    - Returns 5000 rows, but removed 45000 (high selectivity filter)\n",
    "--    - This is the \"outer\" table for the hash join\n",
    "\n",
    "-- 4. Hash Join (line 7)\n",
    "--    - Joins orders (outer) with users hash (inner)\n",
    "--    - Hash Cond: user_id match\n",
    "--    - Returns 1000 rows (actual)\n",
    "--    - loops=1 (executed once)\n",
    "\n",
    "-- 5. HashAggregate (line 5)\n",
    "--    - Groups by email, counts orders\n",
    "--    - Memory: 40kB (hash table for groups)\n",
    "--    - Returns 150 groups\n",
    "\n",
    "-- 6. Sort (line 3)\n",
    "--    - Sorts by count DESC\n",
    "--    - Method: top-N heapsort (efficient for LIMIT)\n",
    "--    - Only sorts enough to get top 10, not full result\n",
    "\n",
    "-- 7. Limit (root, line 1)\n",
    "--    - Stops after 10 rows\n",
    "--    - Final execution time: 5.234ms\n",
    "\n",
    "-- Indentation meaning:\n",
    "-- -> indicates child-parent relationship\n",
    "-- Nodes at same indentation level are siblings\n",
    "-- Data flows upward (child to parent)\n",
    "```\n",
    "\n",
    "### 17.2.2 Understanding Cost Components\n",
    "\n",
    "```sql\n",
    "-- Cost format: (startup_cost..total_cost)\n",
    "-- startup_cost: Cost to produce first row (e.g., sort must complete first)\n",
    "-- total_cost: Cost to produce all rows\n",
    "\n",
    "-- Example where startup matters:\n",
    "EXPLAIN (ANALYZE)\n",
    "SELECT * FROM orders ORDER BY total DESC;\n",
    "\n",
    "-- Sort node:\n",
    "-- (cost=15000.00..17500.00 rows=100000 width=72)\n",
    "-- startup=15000: Must read and sort all rows before returning first\n",
    "-- total=17500: Sorting cost + scanning cost\n",
    "\n",
    "-- Contrast with Index Scan:\n",
    "-- Index Scan using idx_orders_total (cost=0.29..3000.00 rows=100000 width=72)\n",
    "-- startup=0.29: Can return first row immediately (index is sorted)\n",
    "-- total=3000: Just the scan cost, no sort penalty\n",
    "\n",
    "-- Width: Estimated bytes per row\n",
    "-- Important for memory calculations (work_mem usage)\n",
    "-- Wide rows (TOASTed data) consume more memory during sorts/hashes\n",
    "\n",
    "-- Loops: Number of executions\n",
    "-- Critical for nested loops (outer row count = loops)\n",
    "EXPLAIN (ANALYZE)\n",
    "SELECT * FROM users u\n",
    "JOIN orders o ON u.user_id = o.user_id\n",
    "WHERE u.status = 'active';\n",
    "\n",
    "-- Nested Loop (loops=100)\n",
    "--   -> Index Scan on users (loops=1, returns 100 rows)\n",
    "--   -> Index Scan on orders (loops=100, returns 5 rows per loop)\n",
    "-- Total orders scanned: 100 loops * 5 rows = 500 rows\n",
    "-- If loops shows high number, outer table is driving many iterations\n",
    "```\n",
    "\n",
    "## 17.3 Buffer Analysis and I/O Patterns\n",
    "\n",
    "The BUFFERS option reveals physical I/O patterns, distinguishing memory-resident queries from disk-bound performance killers.\n",
    "\n",
    "### 17.3.1 Interpreting Buffer Metrics\n",
    "\n",
    "```sql\n",
    "EXPLAIN (ANALYZE, BUFFERS, FORMAT TEXT)\n",
    "SELECT * FROM large_table WHERE category_id = 5;\n",
    "\n",
    "-- Output:\n",
    "-- Seq Scan on large_table  (cost=0.00..15406.00 rows=50000 width=72)\n",
    "--   (actual time=0.015..125.432 rows=50000 loops=1)\n",
    "--   Filter: (category_id = 5)\n",
    "--   Rows Removed by Filter: 50000\n",
    "--   Buffers: shared read=10834, shared hit=200\n",
    "\n",
    "-- Analysis:\n",
    "-- shared read=10834: Read 10,834 pages from disk (slow)\n",
    "-- shared hit=200: Found 200 pages in shared buffer cache (fast)\n",
    "-- Total pages: 11,034 pages scanned\n",
    "-- If table is 11,034 pages, this is a full table scan (expected for seq scan)\n",
    "\n",
    "-- I/O time estimation:\n",
    "-- shared read * 8KB = bytes read from disk\n",
    "-- 10834 * 8KB = ~84MB read\n",
    "-- On SSD (200MB/s): ~420ms expected I/O time\n",
    "-- Actual time 125ms suggests some cache warming or readahead\n",
    "\n",
    "-- Bad pattern: High shared read with index scan (cache pollution)\n",
    "EXPLAIN (ANALYZE, BUFFERS)\n",
    "SELECT * FROM large_table WHERE id IN (SELECT id FROM small_table);\n",
    "\n",
    "-- Nested Loop\n",
    "--   -> Seq Scan on small_table\n",
    "--   -> Index Scan using large_table_pkey\n",
    "--        Buffers: shared read=1000000  -- Random I/O disaster\n",
    "-- If outer table has 100k rows, that's 100k random lookups (cache misses)\n",
    "\n",
    "-- Good pattern: Sequential I/O\n",
    "EXPLAIN (ANALYZE, BUFFERS)\n",
    "SELECT * FROM large_table WHERE created_at > '2024-01-01';\n",
    "\n",
    "-- Seq Scan\n",
    "--   Buffers: shared read=10000\n",
    "-- Sequential read of 10k pages is much faster than 10k random reads\n",
    "-- SSD throughput makes seq scan viable even for moderate selectivity\n",
    "```\n",
    "\n",
    "### 17.3.2 Work Memory and Disk Spills\n",
    "\n",
    "```sql\n",
    "-- Detecting work_mem spills (slow disk-based operations)\n",
    "EXPLAIN (ANALYZE, BUFFERS)\n",
    "SELECT * FROM orders \n",
    "ORDER BY customer_id, created_at \n",
    "LIMIT 100;\n",
    "\n",
    "-- Sort node indicators:\n",
    "-- Sort Method: external merge  Disk: 5000kB\n",
    "-- OR\n",
    "-- Sort Method: quicksort  Memory: 4096kB\n",
    "\n",
    "-- Analysis:\n",
    "-- \"external merge\" = Spilled to disk (work_mem exceeded)\n",
    "-- \"quicksort\" = In-memory sort (fast)\n",
    "-- \"top-N heapsort\" = In-memory optimized for LIMIT\n",
    "\n",
    "-- Hash operations spills:\n",
    "EXPLAIN (ANALYZE, BUFFERS)\n",
    "SELECT customer_id, COUNT(*) FROM orders GROUP BY customer_id;\n",
    "\n",
    "-- HashAggregate\n",
    "--   Batches: 4  Memory Usage: 1048576kB  Disk Usage: 2048kB\n",
    "-- Batches > 1 indicates hash table didn't fit in work_mem\n",
    "-- Disk Usage shows temp files written\n",
    "\n",
    "-- Fix: Increase work_mem (session or globally)\n",
    "SET work_mem = '128MB';  -- Per-operation limit\n",
    "-- Or optimize query (better indexing, reduce GROUP BY cardinality)\n",
    "\n",
    "-- Caution: work_mem is per-operation, per-connection\n",
    "-- 100 connections * 128MB = 12.8GB potential memory usage\n",
    "-- Set conservatively (default 4MB), increase only for specific queries\n",
    "```\n",
    "\n",
    "## 17.4 Common Anti-Patterns and Solutions\n",
    "\n",
    "Execution plans reveal fundamental query flaws that prevent index usage or cause excessive resource consumption.\n",
    "\n",
    "### 17.4.1 Functions on Indexed Columns\n",
    "\n",
    "```sql\n",
    "-- Anti-pattern: Function prevents index usage\n",
    "EXPLAIN (ANALYZE)\n",
    "SELECT * FROM users WHERE LOWER(email) = 'alice@example.com';\n",
    "\n",
    "-- Seq Scan on users\n",
    "--   Filter: (lower(email) = 'alice@example.com')\n",
    "--   Rows Removed by Filter: 99999\n",
    "\n",
    "-- Problem: Function on column prevents B-tree index usage\n",
    "-- Index stores original values, not LOWER(values)\n",
    "-- Planner must scan every row and apply function\n",
    "\n",
    "-- Solution 1: Functional index (if function is immutable)\n",
    "CREATE INDEX idx_users_email_lower ON users(LOWER(email));\n",
    "-- Now query uses Index Scan\n",
    "\n",
    "-- Solution 2: Case-insensitive collation (if always case-insensitive)\n",
    "-- Use citext extension:\n",
    "CREATE EXTENSION IF NOT EXISTS citext;\n",
    "ALTER TABLE users ALTER COLUMN email TYPE citext;\n",
    "-- Now standard equality works case-insensitively with B-tree index\n",
    "\n",
    "-- Solution 3: Normalize on write (application layer)\n",
    "-- Store email_lower column, index that, query that column\n",
    "\n",
    "-- Date function anti-pattern:\n",
    "EXPLAIN SELECT * FROM orders WHERE EXTRACT(YEAR FROM created_at) = 2024;\n",
    "\n",
    "-- Seq Scan with Filter: (EXTRACT(year FROM created_at) = 2024)\n",
    "-- Cannot use index on created_at\n",
    "\n",
    "-- Solution: Range query (SARGable)\n",
    "EXPLAIN SELECT * FROM orders \n",
    "WHERE created_at >= '2024-01-01' \n",
    "  AND created_at < '2025-01-01';\n",
    "-- Index Scan using idx_orders_created_at\n",
    "\n",
    "-- Timestamp truncation anti-pattern:\n",
    "EXPLAIN SELECT * FROM events \n",
    "WHERE DATE_TRUNC('day', event_time) = '2024-01-01';\n",
    "\n",
    "-- Solution: Range query\n",
    "WHERE event_time >= '2024-01-01' \n",
    "  AND event_time < '2024-01-02';\n",
    "```\n",
    "\n",
    "### 17.4.2 Implicit Type Conversion\n",
    "\n",
    "```sql\n",
    "-- Anti-pattern: Mismatched types cause function application\n",
    "EXPLAIN SELECT * FROM users WHERE user_id = '123';\n",
    "-- user_id is BIGINT, '123' is TEXT\n",
    "\n",
    "-- Execution plan shows:\n",
    "-- Filter: (user_id = '123'::bigint)\n",
    "-- OR worse:\n",
    "-- Filter: (to_char(user_id) = '123')  -- Cannot use index!\n",
    "\n",
    "-- PostgreSQL applies to_char() to user_id (column), not to literal\n",
    "-- This is a function on the column = Seq Scan\n",
    "\n",
    "-- Solution: Explicit casting on literal side\n",
    "EXPLAIN SELECT * FROM users WHERE user_id = 123::BIGINT;\n",
    "-- Or ensure application sends correct type\n",
    "\n",
    "-- Common type mismatch scenarios:\n",
    "-- 1. UUIDs sent as text without cast\n",
    "-- 2. Timestamps compared to text dates\n",
    "-- 3. Integer IDs compared to floating point (JavaScript numbers)\n",
    "\n",
    "-- JSONB type pitfalls:\n",
    "EXPLAIN SELECT * FROM events WHERE payload->>'user_id' = 123;\n",
    "-- payload->>'user_id' returns text, comparing to integer 123\n",
    "-- Implicit cast on column side: ((payload ->> 'user_id'))::integer = 123\n",
    "-- Cannot use GIN index on JSONB\n",
    "\n",
    "-- Solution: Compare to text literal\n",
    "WHERE payload->>'user_id' = '123';\n",
    "-- Or create expression index on (payload->>'user_id')::integer\n",
    "```\n",
    "\n",
    "### 17.4.3 Leading Wildcards and Pattern Matching\n",
    "\n",
    "```sql\n",
    "-- Anti-pattern: Leading wildcard prevents index usage\n",
    "EXPLAIN SELECT * FROM users WHERE email LIKE '%@example.com';\n",
    "\n",
    "-- Seq Scan\n",
    "-- Filter: (email ~~ '%@example.com'::text)\n",
    "\n",
    "-- B-tree indexes support only prefix patterns: 'alice@%'\n",
    "-- Leading wildcard requires full table scan\n",
    "\n",
    "-- Solution 1: Trigram index (GIN or GiST)\n",
    "CREATE EXTENSION IF NOT EXISTS pg_trgm;\n",
    "CREATE INDEX idx_users_email_trgm ON users USING GIN(email gin_trgm_ops);\n",
    "\n",
    "-- Now LIKE '%@example.com' uses Bitmap Index Scan\n",
    "\n",
    "-- Solution 2: Reverse string functional index (for suffix matching)\n",
    "CREATE INDEX idx_users_email_reverse ON users(REVERSE(email));\n",
    "-- Query: WHERE REVERSE(email) LIKE REVERSE('%@example.com')  -- becomes 'moc.elpmaxe%'\n",
    "\n",
    "-- Solution 3: Full-text search (for word matching)\n",
    "-- Use tsvector/tsquery instead of LIKE for word-level matching\n",
    "```\n",
    "\n",
    "### 17.4.4 OR Conditions and Union Alternatives\n",
    "\n",
    "```sql\n",
    "-- Anti-pattern: OR across different columns prevents index usage\n",
    "EXPLAIN SELECT * FROM users \n",
    "WHERE email = 'test@example.com' \n",
    "   OR phone = '555-1234';\n",
    "\n",
    "-- Seq Scan\n",
    "-- Filter: ((email = 'test@example.com') OR (phone = '555-1234'))\n",
    "\n",
    "-- B-tree index can only be used for single-column lookup\n",
    "-- OR requires scanning both conditions (effectively full table)\n",
    "\n",
    "-- Solution 1: Rewrite as UNION (can use two indexes)\n",
    "EXPLAIN\n",
    "SELECT * FROM users WHERE email = 'test@example.com'\n",
    "UNION\n",
    "SELECT * FROM users WHERE phone = '555-1234';\n",
    "\n",
    "-- Plan:\n",
    "-- Append\n",
    "--   -> Index Scan using idx_users_email\n",
    "--   -> Index Scan using idx_users_phone\n",
    "\n",
    "-- Solution 2: Create composite index (if OR is actually AND)\n",
    "-- If query should be AND not OR:\n",
    "CREATE INDEX idx_users_email_phone ON users(email, phone);\n",
    "-- Supports: WHERE email = 'x' AND phone = 'y'\n",
    "\n",
    "-- Solution 3: Use pg_trgm for multi-column text search\n",
    "-- If email and phone are both text, single GIN index on expression\n",
    "\n",
    "-- Complex OR across tables:\n",
    "SELECT * FROM orders \n",
    "WHERE customer_id = 123 OR status = 'urgent';\n",
    "-- If customer_id is selective and status is not, planner may choose wrong index\n",
    "-- Consider UNION approach or partial indexes\n",
    "```\n",
    "\n",
    "### 17.4.5 OFFSET Pagination Performance Cliff\n",
    "\n",
    "```sql\n",
    "-- Anti-pattern: OFFSET for pagination (linear slowdown)\n",
    "EXPLAIN (ANALYZE)\n",
    "SELECT * FROM orders \n",
    "ORDER BY created_at DESC \n",
    "LIMIT 10 OFFSET 10000;\n",
    "\n",
    "-- Limit  (cost=1000.00..1010.00 rows=10 width=72)\n",
    "--   -> Sort  (cost=1000.00..1025.00 rows=10010 width=72)\n",
    "--         Sort Key: created_at DESC\n",
    "--   -> Seq Scan on orders  (cost=0.00..500.00 rows=10010 width=72)\n",
    "\n",
    "-- Analysis:\n",
    "-- Must scan and sort 10,010 rows, discard first 10,000\n",
    "-- Time increases linearly with OFFSET\n",
    "-- At OFFSET 1,000,000, scanning 1M+ rows\n",
    "\n",
    "-- Solution: Keyset pagination (seek method)\n",
    "EXPLAIN (ANALYZE)\n",
    "SELECT * FROM orders \n",
    "WHERE created_at < '2024-01-15 10:00:00'  -- Last seen value\n",
    "ORDER BY created_at DESC \n",
    "LIMIT 10;\n",
    "\n",
    "-- Index Scan using idx_orders_created\n",
    "-- Index Cond: (created_at < '2024-01-15 10:00:00')\n",
    "-- Constant time regardless of page number\n",
    "\n",
    "-- Implementation requires:\n",
    "-- 1. Unique sort key (or composite: created_at, id)\n",
    "-- 2. Store last seen values from previous page\n",
    "-- 3. Cannot jump to arbitrary page number (sequential navigation only)\n",
    "```\n",
    "\n",
    "## 17.5 Parameter Sniffing and Plan Stability\n",
    "\n",
    "PostgreSQL caches execution plans, which can cause performance regression when data distribution varies significantly across parameter values.\n",
    "\n",
    "### 17.5.1 Generic vs Custom Plans\n",
    "\n",
    "```sql\n",
    "-- Prepared statements and plan caching:\n",
    "PREPARE get_orders_by_status (TEXT) AS\n",
    "SELECT * FROM orders WHERE status = $1;\n",
    "\n",
    "-- First 5 executions: Custom plans generated for specific parameter values\n",
    "-- PostgreSQL estimates cost of generic plan vs custom plan\n",
    "-- If custom plan is significantly cheaper, continues using custom plans\n",
    "-- After 5 executions: May switch to generic plan (if cheaper overall)\n",
    "\n",
    "-- Check which plan is being used:\n",
    "EXPLAIN EXECUTE get_orders_by_status('pending');\n",
    "-- vs\n",
    "EXPLAIN EXECUTE get_orders_by_status('completed');\n",
    "\n",
    "-- Problem: 'pending' might be 10 rows, 'completed' might be 1M rows\n",
    "-- Plan optimized for 10 rows (nested loop) is disaster for 1M rows\n",
    "\n",
    "-- Solution 1: Force custom plans\n",
    "SET plan_cache_mode = 'force_custom_plan';\n",
    "-- Generates new plan for each execution\n",
    "-- Cost: Planning overhead on every execution (acceptable for OLTP, not for analytics)\n",
    "\n",
    "-- Solution 2: Partitioning (separate plans per partition)\n",
    "-- If orders partitioned by status, each partition has own statistics\n",
    "\n",
    "-- Solution 3: Query structure that works for both\n",
    "-- Use hash join instead of nested loop (robust for both small and large inputs)\n",
    "-- Add hints via enable_nestloop = off (last resort)\n",
    "```\n",
    "\n",
    "### 17.5.2 Statistics and Correlation\n",
    "\n",
    "```sql\n",
    "-- Plan changes when statistics become stale:\n",
    "-- Table grows from 10k to 10M rows, but stats still show 10k\n",
    "EXPLAIN SELECT * FROM users WHERE created_at > NOW() - INTERVAL '1 day';\n",
    "-- May show Seq Scan (thinks table is small)\n",
    "-- Actually should be Index Scan (recent data is small % of large table)\n",
    "\n",
    "-- Detecting statistics issues:\n",
    "EXPLAIN (ANALYZE)\n",
    "SELECT * FROM large_table WHERE rare_column = 'unique_value';\n",
    "-- Index Scan (cost=0.29..8.30 rows=1 width=72)\n",
    "-- (actual time=0.010..150.000 rows=50000 loops=1)\n",
    "-- ^^^ Estimated 1 row, got 50,000 = statistics are wrong!\n",
    "\n",
    "-- Fix:\n",
    "ANALYZE large_table;\n",
    "-- Or for specific columns:\n",
    "ANALYZE large_table (rare_column);\n",
    "\n",
    "-- Extended statistics for correlated columns:\n",
    "CREATE STATISTICS stats_orders_status_date ON status, created_at FROM orders;\n",
    "ANALYZE orders;\n",
    "-- Helps planner understand that 'pending' orders are recent (correlated)\n",
    "-- Without this, assumes independence (multiplies selectivities)\n",
    "```\n",
    "\n",
    "## 17.6 Measuring Improvements Correctly\n",
    "\n",
    "Validating optimizations requires statistical rigor to distinguish genuine improvements from measurement noise.\n",
    "\n",
    "### 17.6.1 Timing Methodology\n",
    "\n",
    "```sql\n",
    "-- Bad practice: Measuring once\n",
    "EXPLAIN (ANALYZE) SELECT ...;  -- 5ms\n",
    "-- Add index\n",
    "EXPLAIN (ANALYZE) SELECT ...;  -- 3ms\n",
    "-- Conclusion: \"40% faster!\" (maybe)\n",
    "\n",
    "-- Good practice: Multiple samples with cache warming\n",
    "-- 1. Run query 5 times to warm caches\n",
    "-- 2. Run EXPLAIN (ANALYZE, TIMING) 10 times\n",
    "-- 3. Discard outliers (first run often slower due to parsing)\n",
    "-- 4. Average remaining times\n",
    "\n",
    "-- Better: Use pg_bench or custom script for load testing\n",
    "-- Single query timing != production performance under concurrency\n",
    "\n",
    "-- Buffer analysis is more stable than timing:\n",
    "-- Compare Buffers: shared read (disk I/O)\n",
    "-- If shared read drops from 10000 to 10, improvement is real regardless of timing noise\n",
    "\n",
    "-- EXPLAIN (ANALYZE, BUFFERS, FORMAT JSON) for programmatic analysis:\n",
    "-- Extract actual_time, actual_rows, shared_hit, shared_read\n",
    "-- Calculate ratios: shared_hit / (shared_hit + shared_read) = cache hit ratio\n",
    "```\n",
    "\n",
    "### 17.6.2 Common Measurement Pitfalls\n",
    "\n",
    "```sql\n",
    "-- Pitfall 1: Cold cache vs warm cache\n",
    "-- First run after restart: All shared read (slow)\n",
    "-- Second run: All shared hit (fast)\n",
    "-- Always specify which you're measuring:\n",
    "-- \"Cold cache performance\" = after restart or DISCARD BUFFERS (superuser only)\n",
    "-- \"Warm cache performance\" = after running query 3+ times\n",
    "\n",
    "-- Pitfall 2: Planning time dominates\n",
    "EXPLAIN (ANALYZE) SELECT * FROM complex_view WHERE id = 1;\n",
    "-- Planning Time: 45.000 ms  -- Complex view expansion\n",
    "-- Execution Time: 0.500 ms   -- Fast execution\n",
    "-- \"Optimization\" that adds 1ms to execution but reduces planning by 40ms is win\n",
    "\n",
    "-- Pitfall 3: Timing overhead of EXPLAIN ANALYZE itself\n",
    "-- EXPLAIN ANALYZE adds instrumentation overhead (especially TIMING)\n",
    "-- For very fast queries (<0.1ms), overhead can exceed actual execution time\n",
    "-- Use EXPLAIN (ANALYZE, TIMING OFF) to reduce overhead (still counts rows, buffers)\n",
    "\n",
    "-- Pitfall 4: Concurrency effects\n",
    "-- Single-user EXPLAIN != multi-user performance\n",
    "-- Lock contention, buffer cache eviction, I/O saturation not visible in isolation\n",
    "-- Always test under realistic concurrency (pgbench, pgbench-tools, custom scripts)\n",
    "\n",
    "-- Pitfall 5: Different data volumes\n",
    "-- EXPLAIN on dev (10k rows) != production (10M rows)\n",
    "-- Plan types change (nested loop -> hash join) at different scales\n",
    "-- Use production-like data volumes for valid comparisons\n",
    "```\n",
    "\n",
    "## 17.7 Visual Plan Analysis Tools\n",
    "\n",
    "While text EXPLAIN is essential, visualization tools reveal plan structure and bottlenecks more intuitively.\n",
    "\n",
    "### 17.7.1 Using explain.depesz.com\n",
    "\n",
    "```sql\n",
    "-- Generate plan with all options:\n",
    "EXPLAIN (ANALYZE, BUFFERS, COSTS, VERBOSE, FORMAT JSON)\n",
    "YOUR_QUERY_HERE;\n",
    "\n",
    "-- Copy JSON output to https://explain.depesz.com\n",
    "-- Features:\n",
    "-- 1. Hierarchical tree view with color coding\n",
    "-- 2. \"Rows x\" column showing estimated vs actual row ratios\n",
    "--    - Red: Underestimated (bad for nested loop outer tables)\n",
    "--    - Blue: Overestimated (bad for memory allocation)\n",
    "-- 3. Exclusive vs inclusive time breakdown\n",
    "-- 4. Buffer usage visualization\n",
    "\n",
    "-- Interpretation:\n",
    "-- Look for \"fat\" nodes (high exclusive time)\n",
    "-- Look for red rows (underestimation indicates statistics issues)\n",
    "```\n",
    "\n",
    "### 17.7.2 explain.dalibo.com (PEV2)\n",
    "\n",
    "```sql\n",
    "-- Alternative: https://explain.dalibo.com (PEV2: PostgreSQL Explain Visualizer)\n",
    "-- Paste JSON plan output\n",
    "-- Features:\n",
    "-- 1. Flame graph style visualization\n",
    "-- 2. I/O highlighting (high buffer usage nodes)\n",
    "-- 3. Row estimation error highlighting\n",
    "-- 4. Shareable URLs for team collaboration\n",
    "\n",
    "-- Both tools support:\n",
    "-- - Comparing before/after plans (side-by-side)\n",
    "-- - Highlighting nodes with high \"loops\" counts (nested loop issues)\n",
    "-- - Identifying sequential scans on large tables\n",
    "```\n",
    "\n",
    "## 17.8 Advanced Plan Diagnostics\n",
    "\n",
    "### 17.8.1 Partition Pruning Verification\n",
    "\n",
    "```sql\n",
    "-- Check if partition pruning is working:\n",
    "EXPLAIN (ANALYZE, VERBOSE)\n",
    "SELECT * FROM events \n",
    "WHERE event_date BETWEEN '2024-01-01' AND '2024-01-31';\n",
    "\n",
    "-- Output should show:\n",
    "-- Append\n",
    "--   -> Index Scan using events_2024_01_pkey on events_2024_01\n",
    "--         Index Cond: ((event_date >= '2024-01-01') AND (event_date <= '2024-01-31'))\n",
    "--   -> Index Scan using events_2024_02_pkey on events_2024_02\n",
    "--         Index Cond: ((event_date >= '2024-01-01') AND (event_date <= '2024-01-31'))\n",
    "--   -> Index Scan using events_2024_03_pkey on events_2024_03\n",
    "--         Index Cond: ((event_date >= '2024-01-01') AND (event_date <= '2024-01-31'))\n",
    "\n",
    "-- If you see Seq Scan on events_2023_12 or events_2024_04, pruning failed\n",
    "-- Check: Constraint exclusion enabled, partition bounds correct\n",
    "```\n",
    "\n",
    "### 17.8.2 Parallel Query Plans\n",
    "\n",
    "```sql\n",
    "-- Check parallel execution:\n",
    "EXPLAIN (ANALYZE, VERBOSE)\n",
    "SELECT COUNT(*) FROM large_table WHERE amount > 100;\n",
    "\n",
    "-- Gather  (cost=1000.00..5000.00 rows=1 width=8)\n",
    "--   Workers Planned: 2\n",
    "--   Workers Launched: 2\n",
    "--   -> Parallel Seq Scan on large_table\n",
    "--         Filter: (amount > 100)\n",
    "--         Rows Removed by Filter: 300000\n",
    "\n",
    "-- Key indicators:\n",
    "-- Workers Launched: Actually used parallel workers\n",
    "-- If Workers Planned > Workers Launched: max_parallel_workers insufficient\n",
    "-- Parallel Seq Scan / Parallel Index Scan / Parallel Bitmap Heap Scan\n",
    "\n",
    "-- Not all operations parallelize:\n",
    "-- Hash joins parallelize, nested loops do not (usually)\n",
    "-- Sorts parallelize (Parallel Sort), but final gather is serial\n",
    "-- Cursors disable parallelism\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, you learned:\n",
    "\n",
    "1. **EXPLAIN Modes**: `EXPLAIN` shows planner estimates (no execution); `EXPLAIN ANALYZE` executes and shows actuals; `EXPLAIN (ANALYZE, BUFFERS)` adds I/O statistics essential for performance analysis. Always use `BUFFERS` for disk I/O visibility.\n",
    "\n",
    "2. **Plan Reading**: Read bottom-up (leaf scans to root). Indentation indicates parent-child relationships. `actual time=X..Y` shows first-row and total time; `loops=N` indicates iteration count (critical for nested loops); `rows=N` vs `actual rows=M` reveals estimation errors.\n",
    "\n",
    "3. **Buffer Analysis**: `shared hit` = cache reads (fast); `shared read` = disk reads (slow). High `shared read` with index scans indicates cache misses or random I/O. `external merge` or `Batches: 2+` indicates `work_mem` spills to disk (performance killer).\n",
    "\n",
    "4. **Anti-Patterns**: Functions on columns prevent index usage (use functional indexes or rewrite queries); implicit type conversions cause `to_char()` or casting on columns; leading wildcards (`%text`) require trigram indexes; `OR` conditions across columns force sequential scans (use `UNION` instead); `OFFSET` pagination is O(n) (use keyset pagination).\n",
    "\n",
    "5. **Plan Stability**: Parameter sniffing occurs when generic plans optimized for one parameter value perform poorly for others. Use `plan_cache_mode = 'force_custom_plan'` for variable data distributions. Stale statistics cause cardinality misestimates (red rows in Depesz); run `ANALYZE` after significant data changes.\n",
    "\n",
    "6. **Measurement Rigor**: Single timings are noise; measure buffer counts (deterministic) over wall-clock time where possible. Warm caches before benchmarking. Account for planning time in complex views. Test at production scale and concurrency, not just single-user EXPLAIN.\n",
    "\n",
    "**Next:** In Chapter 18, we will explore Performance Tuning Playbookâ€”covering query refactoring patterns, index selection workflows, batch operations, N+1 elimination strategies, and practical checklists for slow endpoint diagnosis."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
