{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 40: Performance Under Production Constraints\n",
    "\n",
    "Production environments impose constraints that laboratory benchmarks rarely replicate: network partitions occur during peak traffic, single rows become contention hot spots under viral load spikes, and thundering herds overwhelm circuit breakers during recovery. This chapter addresses the architectural patterns and defensive configurations required to maintain throughput and availability when ideal conditions deteriorate into resource contention and partial failures.\n",
    "\n",
    "## 40.1 Hot Spots and Contention Patterns\n",
    "\n",
    "Database contention manifests when concurrent transactions compete for the same resources—rows, pages, indexes, or sequence values. Identifying and mitigating these patterns separates architectures that scale linearly from those that collapse under load.\n",
    "\n",
    "### 40.1.1 Row-Level Contention (The Update Hotspot)\n",
    "\n",
    "The most common production bottleneck occurs when many transactions attempt to update the same row simultaneously—a counter table, inventory stock, or user session state.\n",
    "\n",
    "```sql\n",
    "-- Anti-pattern: Centralized counter update\n",
    "UPDATE visit_counters SET count = count + 1 WHERE page_id = '/home';\n",
    "-- Under 1000 concurrent requests, this serializes through a single row lock\n",
    "-- Result: Lock waits cascade, connection pool exhaustion, timeout cascades\n",
    "```\n",
    "\n",
    "**Diagnostic Identification:**\n",
    "```sql\n",
    "-- Detect row-level contention in real-time\n",
    "SELECT \n",
    "    pid,\n",
    "    wait_event_type,\n",
    "    wait_event,\n",
    "    blocking_pid,\n",
    "    blocking_lock_mode,\n",
    "    blocked_lock_mode,\n",
    "    query\n",
    "FROM pg_stat_activity sa\n",
    "LEFT JOIN pg_locks blocking ON (\n",
    "    sa.pid = blocking.pid AND \n",
    "    blocking.granted = false\n",
    ")\n",
    "WHERE sa.wait_event_type = 'Lock'\n",
    "  AND sa.wait_event = 'transactionid';\n",
    "-- Multiple sessions waiting on the same transactionid indicates row contention\n",
    "```\n",
    "\n",
    "**Mitigation Strategies:**\n",
    "\n",
    "1. **Partitioned Counters** (Eventual Consistency):\n",
    "```sql\n",
    "-- Create per-session counters that aggregate periodically\n",
    "CREATE TABLE visit_counter_shards (\n",
    "    shard_id int,\n",
    "    page_id text,\n",
    "    count int,\n",
    "    PRIMARY KEY (shard_id, page_id)\n",
    ");\n",
    "\n",
    "-- Application determines shard via hash(client_id) % 10\n",
    "INSERT INTO visit_counter_shards (shard_id, page_id, count) \n",
    "VALUES (4, '/home', 1)\n",
    "ON CONFLICT (shard_id, page_id) \n",
    "DO UPDATE SET count = visit_counter_shards.count + 1;\n",
    "\n",
    "-- Background job aggregates to master table every minute\n",
    "INSERT INTO visit_counters (page_id, count)\n",
    "SELECT page_id, SUM(count) FROM visit_counter_shards GROUP BY page_id\n",
    "ON CONFLICT (page_id) DO UPDATE SET count = visit_counters.count + EXCLUDED.count;\n",
    "\n",
    "-- Truncate shards after aggregation (fast, minimal logging)\n",
    "TRUNCATE visit_counter_shards;\n",
    "```\n",
    "\n",
    "2. **Slotted Updates** (Deterministic Distribution):\n",
    "```sql\n",
    "-- Use modulo arithmetic to distribute updates across 100 slots\n",
    "UPDATE inventory_slots \n",
    "SET reserved_count = reserved_count + 1 \n",
    "WHERE product_id = 123 \n",
    "  AND slot_id = (pg_backend_pid() % 100);\n",
    "\n",
    "-- Sum at read time (slight computational cost, massive write scalability)\n",
    "SELECT product_id, SUM(reserved_count) as total_reserved\n",
    "FROM inventory_slots \n",
    "WHERE product_id = 123\n",
    "GROUP BY product_id;\n",
    "```\n",
    "\n",
    "3. **Advisory Locks for Application-Level Serialization**:\n",
    "When business logic requires strict serialization but you want to avoid database row locks:\n",
    "\n",
    "```sql\n",
    "-- Acquire advisory lock before update (allows queuing in application logic)\n",
    "SELECT pg_try_advisory_lock(hashtext('inventory:' || product_id));\n",
    "\n",
    "-- If acquired, proceed with update\n",
    "UPDATE inventory SET stock = stock - 1 WHERE product_id = 123;\n",
    "\n",
    "-- Release in same session or transaction end\n",
    "SELECT pg_advisory_unlock(hashtext('inventory:' || product_id));\n",
    "```\n",
    "\n",
    "### 40.1.2 Index Contention (Right-Growing Indexes)\n",
    "\n",
    "B-tree indexes on sequential values (timestamps, serial IDs) create \"hotspots\" at the rightmost leaf page where all inserts converge.\n",
    "\n",
    "```sql\n",
    "-- High-contention index pattern\n",
    "CREATE TABLE events (\n",
    "    event_id bigserial PRIMARY KEY,  -- Sequential, right-growing\n",
    "    created_at timestamptz DEFAULT now(),  -- Also sequential\n",
    "    payload jsonb\n",
    ");\n",
    "-- 10,000 inserts/sec all compete for the same leaf page latch\n",
    "```\n",
    "\n",
    "**Solutions:**\n",
    "\n",
    "1. **Hash Sharding for PK**:\n",
    "```sql\n",
    "-- Use UUIDv4 or hash-distributed IDs instead of sequential\n",
    "CREATE TABLE events (\n",
    "    event_id uuid PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "    -- or: bigint generated always as (hashtext(some_unique_data)::bigint) stored\n",
    "    created_at timestamptz,\n",
    "    payload jsonb\n",
    ");\n",
    "-- Distributes writes across entire index structure\n",
    "```\n",
    "\n",
    "2. **BRIN Indexes for Time Series**:\n",
    "```sql\n",
    "-- For time-series where queries are range-based on insertion order\n",
    "CREATE INDEX idx_events_created_brin ON events USING BRIN (created_at) \n",
    "WITH (pages_per_range = 128);\n",
    "-- Tiny index size, minimal write overhead, no right-growing hotspot\n",
    "```\n",
    "\n",
    "3. **Fillfactor Tuning for Update-Heavy Tables**:\n",
    "```sql\n",
    "-- Leave space for HOT updates (Heap-Only Tuple) to avoid index churn\n",
    "ALTER TABLE user_sessions SET (fillfactor = 70);\n",
    "-- 30% free space allows updates to stay on same page, avoiding index updates\n",
    "-- for non-indexed columns\n",
    "```\n",
    "\n",
    "### 40.1.3 Sequence Contention (High-Volume ID Generation)\n",
    "\n",
    "`SERIAL` and `IDENTITY` columns use sequences that generate next values via shared memory buffers, but cache exhaustion creates contention.\n",
    "\n",
    "```sql\n",
    "-- High-throughput sequence configuration\n",
    "CREATE SEQUENCE high_volume_seq \n",
    "    CACHE 1000  -- Pre-allocate 1000 values in memory per session\n",
    "    NO CYCLE;\n",
    "\n",
    "-- Default cache is 1, causing disk writes and lock contention every value\n",
    "```\n",
    "\n",
    "**Industry Pattern: Snowflake IDs** (Application-side generation):\n",
    "```python\n",
    "# Distributed ID generation without database sequence\n",
    "# 41-bit timestamp + 10-bit node ID + 12-bit sequence\n",
    "# Application generates IDs, database inserts without sequence overhead\n",
    "```\n",
    "\n",
    "## 40.2 Batch Writes, Idempotency, and Retries\n",
    "\n",
    "Network partitions and client timeouts create ambiguity: did the write succeed or not? Production systems must handle partial failures gracefully.\n",
    "\n",
    "### 40.2.1 Batch Insert Strategies\n",
    "\n",
    "**Single-Statement Atomicity**:\n",
    "```sql\n",
    "-- Batch insert with conflict handling\n",
    "INSERT INTO events (event_id, payload, processed_at)\n",
    "VALUES \n",
    "    ('evt-001', '{\"data\": 1}', now()),\n",
    "    ('evt-002', '{\"data\": 2}', now()),\n",
    "    ('evt-003', '{\"data\": 3}', now())\n",
    "ON CONFLICT (event_id) DO NOTHING;\n",
    "-- Atomic: all succeed or none succeed\n",
    "-- Idempotent: duplicate event_ids silently ignored\n",
    "```\n",
    "\n",
    "**Chunked Batches for Large Loads**:\n",
    "```sql\n",
    "-- Process 1000 rows at a time to prevent long transactions\n",
    "DO $$\n",
    "DECLARE\n",
    "    batch_size CONSTANT int := 1000;\n",
    "    rows_affected int;\n",
    "BEGIN\n",
    "    LOOP\n",
    "        INSERT INTO target_table (id, data)\n",
    "        SELECT id, data \n",
    "        FROM staging_table \n",
    "        WHERE processed = false\n",
    "        LIMIT batch_size\n",
    "        ON CONFLICT DO NOTHING;\n",
    "        \n",
    "        GET DIAGNOSTICS rows_affected = ROW_COUNT;\n",
    "        \n",
    "        UPDATE staging_table \n",
    "        SET processed = true \n",
    "        WHERE id IN (\n",
    "            SELECT id FROM staging_table \n",
    "            WHERE processed = false \n",
    "            LIMIT batch_size\n",
    "        );\n",
    "        \n",
    "        COMMIT;  -- Frequent commits prevent lock escalation\n",
    "        EXIT WHEN rows_affected = 0;\n",
    "        \n",
    "        PERFORM pg_sleep(0.01);  -- Micro-backoff between batches\n",
    "    END LOOP;\n",
    "END $$;\n",
    "```\n",
    "\n",
    "### 40.2.2 Idempotency Patterns\n",
    "\n",
    "**Exactly-Once Semantics** require unique business keys:\n",
    "\n",
    "```sql\n",
    "-- Invoice processing with idempotency key\n",
    "CREATE TABLE invoice_processing (\n",
    "    idempotency_key text PRIMARY KEY,\n",
    "    invoice_id bigint,\n",
    "    amount_cents int,\n",
    "    processed_at timestamptz DEFAULT now(),\n",
    "    status processing_status DEFAULT 'pending'\n",
    ");\n",
    "\n",
    "-- Client retries with same idempotency_key\n",
    "INSERT INTO invoice_processing (idempotency_key, invoice_id, amount_cents)\n",
    "VALUES ('req-2024-001', 12345, 10000)\n",
    "ON CONFLICT (idempotency_key) \n",
    "DO UPDATE SET \n",
    "    -- Only update if previous attempt failed\n",
    "    status = CASE \n",
    "        WHEN invoice_processing.status = 'failed' THEN 'pending'::processing_status\n",
    "        ELSE invoice_processing.status \n",
    "    END,\n",
    "    processed_at = CASE \n",
    "        WHEN invoice_processing.status = 'failed' THEN now()\n",
    "        ELSE invoice_processing.processed_at \n",
    "    END\n",
    "WHERE invoice_processing.status = 'failed'\n",
    "RETURNING invoice_id, status, \n",
    "    (xmax = 0) as is_new_insert;  -- True if first attempt, false if conflict/update\n",
    "```\n",
    "\n",
    "**State Machine Idempotency**:\n",
    "```sql\n",
    "-- Ensure transitions only happen from valid prior states\n",
    "UPDATE orders \n",
    "SET status = 'shipped', shipped_at = now()\n",
    "WHERE order_id = 123 \n",
    "  AND status = 'paid';  -- Only ship paid orders, not cancelled ones\n",
    "\n",
    "-- Check rows_affected: 0 means invalid transition (already shipped/cancelled)\n",
    "```\n",
    "\n",
    "### 40.2.3 Retry Strategies with Exponential Backoff\n",
    "\n",
    "Client-side retry logic prevents thundering herds:\n",
    "\n",
    "```python\n",
    "# Industry-standard retry with jitter\n",
    "import random\n",
    "import time\n",
    "\n",
    "def execute_with_retry(func, max_retries=5):\n",
    "    for attempt in range(max_retries):\n",
    "        try:\n",
    "            return func()\n",
    "        except TransientError as e:\n",
    "            if attempt == max_retries - 1:\n",
    "                raise\n",
    "            \n",
    "            # Exponential backoff: 100ms, 200ms, 400ms, 800ms, 1.6s\n",
    "            base_delay = (2 ** attempt) * 0.1\n",
    "            \n",
    "            # Full jitter prevents synchronized retries across clients\n",
    "            jitter = random.uniform(0, base_delay)\n",
    "            sleep_time = base_delay + jitter\n",
    "            \n",
    "            time.sleep(sleep_time)\n",
    "```\n",
    "\n",
    "**Database-Side Retry Queues**:\n",
    "```sql\n",
    "-- Retry queue with exponential backoff\n",
    "CREATE TABLE retry_queue (\n",
    "    id bigserial PRIMARY KEY,\n",
    "    payload jsonb,\n",
    "    attempt_count int DEFAULT 0,\n",
    "    next_attempt_at timestamptz DEFAULT now(),\n",
    "    error_message text\n",
    ");\n",
    "\n",
    "-- Worker query (index on next_attempt_at)\n",
    "SELECT * FROM retry_queue \n",
    "WHERE next_attempt_at <= now()\n",
    "ORDER BY next_attempt_at\n",
    "LIMIT 10\n",
    "FOR UPDATE SKIP LOCKED;  -- Skip items being processed by other workers\n",
    "\n",
    "-- On failure, increment with backoff\n",
    "UPDATE retry_queue \n",
    "SET attempt_count = attempt_count + 1,\n",
    "    next_attempt_at = now() + (interval '1 second' * (2 ^ attempt_count)),\n",
    "    error_message = 'Error details...'\n",
    "WHERE id = :id;\n",
    "```\n",
    "\n",
    "## 40.3 Timeouts, Circuit Breakers, and Backpressure\n",
    "\n",
    "Defensive timeout hierarchies prevent cascading failures when dependencies slow down.\n",
    "\n",
    "### 40.3.1 PostgreSQL Timeout Configuration\n",
    "\n",
    "Layered timeouts protect resources at different granularities:\n",
    "\n",
    "```sql\n",
    "-- Connection-level (applies to all statements in session)\n",
    "SET statement_timeout = '30s';        -- Maximum duration for any statement\n",
    "SET lock_timeout = '5s';               -- Maximum time to wait for lock acquisition\n",
    "SET idle_in_transaction_session_timeout = '60s';  -- Kill idle transactions holding locks\n",
    "SET tcp_keepalives_idle = 60;        -- Detect dead connections (OS level)\n",
    "\n",
    "-- Transaction-specific (overrides session)\n",
    "BEGIN;\n",
    "SET LOCAL statement_timeout = '5min';  -- Allow longer for specific batch operation\n",
    "-- ... batch work ...\n",
    "COMMIT;\n",
    "```\n",
    "\n",
    "**Application-Driver Timeout Alignment**:\n",
    "```yaml\n",
    "# Application configuration (pseudocode)\n",
    "database:\n",
    "  connect_timeout: 10s         # TCP connection establishment\n",
    "  socket_timeout: 35s          # Must be > statement_timeout (30s) + network latency\n",
    "  pool:\n",
    "    connection_timeout: 5s     # Wait for connection from pool\n",
    "    idle_timeout: 300s         # Return idle connections to pool\n",
    "    max_lifetime: 1800s       # Force reconnect to clear any leaked state\n",
    "```\n",
    "\n",
    "### 40.3.2 Circuit Breaker Patterns\n",
    "\n",
    "When error rates exceed thresholds, fail fast to prevent resource exhaustion:\n",
    "\n",
    "```python\n",
    "# Circuit breaker state machine\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, failure_threshold=5, recovery_timeout=30):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.state = 'CLOSED'  # CLOSED (normal), OPEN (failing), HALF_OPEN (testing)\n",
    "        self.failures = 0\n",
    "        self.last_failure_time = None\n",
    "    \n",
    "    def call(self, func, *args):\n",
    "        if self.state == 'OPEN':\n",
    "            if time.time() - self.last_failure_time > self.recovery_timeout:\n",
    "                self.state = 'HALF_OPEN'\n",
    "            else:\n",
    "                raise CircuitOpenError(\"Database circuit open\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args)\n",
    "            self.on_success()\n",
    "            return result\n",
    "        except DatabaseError:\n",
    "            self.on_failure()\n",
    "            raise\n",
    "    \n",
    "    def on_failure(self):\n",
    "        self.failures += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        if self.failures >= self.failure_threshold:\n",
    "            self.state = 'OPEN'\n",
    "            # Alert: Stop hammering the struggling database\n",
    "    \n",
    "    def on_success(self):\n",
    "        self.failures = 0\n",
    "        self.state = 'CLOSED'\n",
    "```\n",
    "\n",
    "**Database-Side Circuit Indicators**:\n",
    "```sql\n",
    "-- Health check query that fails fast if database struggling\n",
    "SELECT \n",
    "    CASE \n",
    "        WHEN count(*) FILTER (WHERE state = 'active') > \n",
    "             (SELECT setting::int * 0.8 FROM pg_settings WHERE name = 'max_connections')\n",
    "        THEN pg_raise_error('Database overloaded')\n",
    "        ELSE 'healthy'\n",
    "    END as health_status\n",
    "FROM pg_stat_activity;\n",
    "```\n",
    "\n",
    "### 40.3.3 Backpressure Implementation\n",
    "\n",
    "When ingestion exceeds processing capacity, shed load gracefully rather than failing catastrophically.\n",
    "\n",
    "**Connection Pool Backpressure**:\n",
    "```sql\n",
    "-- Check pool saturation before accepting work\n",
    "SELECT \n",
    "    numbackends as active_connections,\n",
    "    (SELECT setting::int FROM pg_settings WHERE name='max_connections') as max_conn,\n",
    "    (SELECT count(*) FROM pg_stat_activity WHERE wait_event_type IS NOT NULL) as waiting,\n",
    "    CASE \n",
    "        WHEN numbackends > max_conn * 0.9 THEN 'reject'\n",
    "        WHEN numbackends > max_conn * 0.8 THEN 'throttle'\n",
    "        ELSE 'accept'\n",
    "    END as admission_status\n",
    "FROM pg_stat_database \n",
    "WHERE datname = current_database();\n",
    "```\n",
    "\n",
    "**Queue-Based Load Leveling**:\n",
    "```sql\n",
    "-- Insert into queue with depth limit\n",
    "WITH queue_depth AS (\n",
    "    SELECT count(*) as depth FROM ingestion_queue WHERE status = 'pending'\n",
    ")\n",
    "INSERT INTO ingestion_queue (payload, status)\n",
    "SELECT :payload, 'pending'\n",
    "FROM queue_depth\n",
    "WHERE depth < 10000;  -- Reject if queue full (backpressure)\n",
    "\n",
    "-- Check rows_affected: 0 means queue full, return 503 Service Unavailable to client\n",
    "```\n",
    "\n",
    "## 40.4 Load Testing Methodology and Pitfalls\n",
    "\n",
    "Validation in production-like conditions reveals constraints invisible to unit tests.\n",
    "\n",
    "### 40.4.1 Realistic Data Generation\n",
    "\n",
    "**Statistical Distribution Matching**:\n",
    "```sql\n",
    "-- Generate skewed data matching production cardinality\n",
    "-- (80% of traffic hits 20% of users - Pareto distribution)\n",
    "WITH RECURSIVE user_skew AS (\n",
    "    SELECT \n",
    "        CASE \n",
    "            WHEN random() < 0.8 THEN (random() * 1000)::int  -- Hot users\n",
    "            ELSE (random() * 50000 + 1000)::int              -- Long tail\n",
    "        END as user_id,\n",
    "        gen_random_uuid() as event_id\n",
    "    FROM generate_series(1, 1000000)\n",
    ")\n",
    "INSERT INTO events (user_id, event_id, created_at)\n",
    "SELECT user_id, event_id, now() - (random() * interval '30 days')\n",
    "FROM user_skew;\n",
    "```\n",
    "\n",
    "**Contention Simulation**:\n",
    "```bash\n",
    "# pgbench custom script simulating hotspot\n",
    "# hotspot.sql\n",
    "\\set aid random(1, 100)  -- Only 100 accounts (high contention)\n",
    "BEGIN;\n",
    "UPDATE pgbench_accounts SET abalance = abalance + 1 WHERE aid = :aid;\n",
    "END;\n",
    "\n",
    "# Run with high concurrency\n",
    "pgbench -c 100 -j 10 -T 300 -f hotspot.sql -P 1\n",
    "```\n",
    "\n",
    "### 40.4.2 Common Load Testing Pitfalls\n",
    "\n",
    "1. **Uniform Randomness** (The Testing Trap):\n",
    "   ```sql\n",
    "   -- WRONG: Uniform distribution hides lock contention\n",
    "   WHERE user_id = (random() * 1000000)::int\n",
    "   \n",
    "   -- RIGHT: Pareto distribution reveals real hotspots\n",
    "   WHERE user_id = CASE WHEN random() < 0.5 THEN 1 ELSE (random()*100000)::int END\n",
    "   ```\n",
    "\n",
    "2. **Empty Buffer Cache** (Cold Start Bias):\n",
    "   - Production runs with hot cache; testing from cold shows disk-bound performance that doesn't reflect steady state\n",
    "   - Warm up: Run 5 minutes of traffic before measuring\n",
    "\n",
    "3. **Unrealistic Transaction Sizes**:\n",
    "   ```sql\n",
    "   -- WRONG: Single row operations only\n",
    "   UPDATE accounts SET balance = balance + 1 WHERE id = 1;\n",
    "   \n",
    "   -- RIGHT: Mix of operations matching production ratios\n",
    "   -- 80% reads, 15% single updates, 5% batch operations\n",
    "   ```\n",
    "\n",
    "4. **Ignoring Network Latency**:\n",
    "   - Testing on localhost eliminates round-trip time (RTT)\n",
    "   - Production RTT of 1-5ms dramatically changes transaction throughput\n",
    "   - Use traffic control (`tc`) to simulate latency: `tc qdisc add dev eth0 root netem delay 5ms`\n",
    "\n",
    "5. **Vacuum Interference**:\n",
    "   - Long tests without vacuum monitoring show improving performance as bloat grows, then sudden cliffs\n",
    "   - Monitor `pg_stat_progress_vacuum` during test to correlate dips with maintenance\n",
    "\n",
    "### 40.4.3 Metrics to Capture During Load Tests\n",
    "\n",
    "```sql\n",
    "-- Snapshot script run every 10 seconds during test\n",
    "SELECT \n",
    "    now() as sample_time,\n",
    "    (SELECT count(*) FROM pg_stat_activity WHERE state = 'active') as active_queries,\n",
    "    (SELECT count(*) FROM pg_stat_activity WHERE wait_event_type = 'Lock') as lock_waits,\n",
    "    (SELECT count(*) FROM pg_stat_activity WHERE wait_event_type = 'IO') as io_waits,\n",
    "    (SELECT sum(xact_commit + xact_rollback) FROM pg_stat_database) as total_xacts,\n",
    "    (SELECT sum(blks_hit) FROM pg_stat_database) as cache_hits,\n",
    "    (SELECT sum(blks_read) FROM pg_stat_database) as disk_reads,\n",
    "    (SELECT count(*) FROM pg_stat_activity WHERE state = 'idle in transaction') as idle_in_tx;\n",
    "```\n",
    "\n",
    "**Throughput vs Latency Curves**:\n",
    "Plot latency percentiles (p50, p95, p99, p999) against throughput. Healthy systems show flat latency until saturation point, then graceful degradation. Unhealthy systems show latency spikes early due to contention.\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, you learned:\n",
    "\n",
    "1. **Hot Spot Mitigation**: Distribute write contention using sharded counters, modulo-based row distribution, or advisory locks. Avoid sequential ID hotspots with UUIDs or hash sharding, and tune fillfactor for update-heavy tables to enable HOT updates.\n",
    "\n",
    "2. **Batch Processing**: Use single-statement atomicity with `ON CONFLICT` for idempotent inserts. Chunk large operations to prevent long transactions and lock escalation. Implement client-side exponential backoff with jitter to prevent thundering herds during recovery.\n",
    "\n",
    "3. **Idempotency Architecture**: Design for exactly-once semantics using database-enforced unique keys (idempotency tokens) and state machine transitions that validate prior states. Never rely on \"check then act\" patterns which race under concurrency.\n",
    "\n",
    "4. **Defensive Timeouts**: Layer `statement_timeout`, `lock_timeout`, and `idle_in_transaction_session_timeout` to prevent runaway queries from consuming resources. Align application timeouts with database timeouts accounting for network latency.\n",
    "\n",
    "5. **Circuit Breakers and Backpressure**: Implement fail-fast mechanisms when error rates exceed thresholds. Use queue depth monitoring and connection pool saturation metrics to shed load gracefully (HTTP 503) rather than accepting work that will timeout, wasting resources.\n",
    "\n",
    "6. **Load Testing Realism**: Generate skewed data distributions (Pareto, not uniform) to expose lock contention. Warm caches before measuring, simulate network latency, and monitor vacuum interference. Measure latency percentiles, not just throughput, and watch for early latency spikes indicating architectural bottlenecks.\n",
    "\n",
    "---\n",
    "\n",
    "**Next:** In Chapter 41, we will explore JSONB in Production—covering modeling tradeoffs between JSONB and normalized tables, effective indexing strategies (GIN, expression indexes), query patterns that leverage PostgreSQL's JSON operators, and schema evolution strategies for document-oriented data within a relational system."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
