{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 35: Choosing the Right Data Structure\n",
    "\n",
    "> *\"The right data structure can make an algorithm sing; the wrong one can make it crawl. Choosing wisely is the hallmark of an expert engineer.\"* \u2014 Anonymous\n",
    "\n",
    "---\n",
    "\n",
    "## 35.1 Introduction\n",
    "\n",
    "In the preceding chapters, we've explored a vast arsenal of data structures\u2014from simple arrays and linked lists to sophisticated trees, heaps, and graphs. Each structure has its own strengths and weaknesses. The key to solving any computational problem efficiently lies not just in the algorithm, but in selecting the appropriate data structure to support it. This chapter provides a framework for making that choice, considering not only asymptotic complexity but also practical factors like cache behavior, memory footprint, code complexity, and the environment (in\u2011memory vs. external storage).\n",
    "\n",
    "### 35.1.1 Why the Choice Matters\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    IMPORTANCE OF DATA STRUCTURE SELECTION             \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  1. PERFORMANCE: The right structure can reduce an O(n\u00b2) algorithm  \u2502\n",
    "\u2502     to O(n log n) or better.                                        \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  2. SCALABILITY: A structure that works for small datasets may      \u2502\n",
    "\u2502     collapse under larger loads (e.g., O(n) search in a linked list \u2502\n",
    "\u2502     vs. O(log n) in a balanced tree).                               \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  3. MAINTAINABILITY: Complex structures increase code complexity    \u2502\n",
    "\u2502     and bug risk; simplicity should be favored unless performance   \u2502\n",
    "\u2502     demands otherwise.                                              \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  4. MEMORY EFFICIENCY: Some structures have high overhead (e.g.,    \u2502\n",
    "\u2502     each node in a tree carries pointers); others are compact       \u2502\n",
    "\u2502     (e.g., arrays).                                                 \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  5. CACHE LOCALITY: Modern CPUs rely heavily on caches; structures  \u2502\n",
    "\u2502     that access memory sequentially (arrays) often outperform       \u2502\n",
    "\u2502     pointer\u2011chasing structures (linked lists) even when asymptotics \u2502\n",
    "\u2502     are similar.                                                    \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  6. CONCURRENCY: In multi\u2011threaded environments, some structures    \u2502\n",
    "\u2502     are easier to make thread\u2011safe than others.                     \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 35.2 Criteria for Selection\n",
    "\n",
    "When faced with a problem, consider the following dimensions:\n",
    "\n",
    "### 35.2.1 Time Complexity\n",
    "\n",
    "- **Worst\u2011case:** Essential for real\u2011time systems or when adversarial input is possible.\n",
    "- **Average\u2011case:** Often more relevant for typical use.\n",
    "- **Amortized:** Important when occasional expensive operations are offset by many cheap ones (e.g., dynamic array resizing).\n",
    "\n",
    "Always ask: What operations will be performed most frequently? (insert, delete, search, iteration, etc.)\n",
    "\n",
    "### 35.2.2 Space Complexity\n",
    "\n",
    "- Overhead per element (pointers, balance information, etc.).\n",
    "- Total memory footprint, especially for large datasets.\n",
    "- If data is too large to fit in RAM, you need an **external memory** structure (e.g., B\u2011tree).\n",
    "\n",
    "### 35.2.3 Memory Access Patterns (Cache Behavior)\n",
    "\n",
    "- **Sequential access:** Arrays, dynamic arrays (good cache locality).\n",
    "- **Random access:** Trees, hash tables (poor locality, many cache misses).\n",
    "- **Spatial locality:** Structures that store related data contiguously (e.g., array of structs vs. struct of arrays) matter for performance.\n",
    "\n",
    "### 35.2.4 Code Complexity and Maintainability\n",
    "\n",
    "- How hard is it to implement correctly?\n",
    "- Are there library implementations available? (e.g., Python\u2019s `list`, `dict`, `heapq`, `bisect`; C++ STL; Java Collections)\n",
    "- Does the team understand the structure?\n",
    "\n",
    "### 35.2.5 Adaptability and Future Requirements\n",
    "\n",
    "- Will the data need to support new operations later? Choose a more general structure.\n",
    "- Is the data size expected to grow? Plan for scalability.\n",
    "\n",
    "### 35.2.6 Concurrency\n",
    "\n",
    "- Does the structure need to be thread\u2011safe?\n",
    "- Can we use locks, or do we need lock\u2011free structures?\n",
    "\n",
    "### 35.2.7 External Memory Constraints\n",
    "\n",
    "- When data exceeds RAM, we need structures optimized for disk I/O (e.g., B\u2011trees, LSM trees).\n",
    "- Operations should minimize the number of disk accesses (block reads/writes).\n",
    "\n",
    "---\n",
    "\n",
    "## 35.3 Common Data Structures and Their Trade\u2011offs\n",
    "\n",
    "The following table summarizes typical use cases for fundamental data structures. (Complexities are for the most common operations; see Appendix B for detailed cheat sheets.)\n",
    "\n",
    "| Structure               | Strong Points                                           | Weak Points                                           | Typical Use Cases                          |\n",
    "|-------------------------|--------------------------------------------------------|-------------------------------------------------------|---------------------------------------------|\n",
    "| **Array**               | O(1) index access, cache\u2011friendly                      | O(n) search, fixed size (static)                      | Lookup by index, small datasets             |\n",
    "| **Dynamic Array (list)**| Amortized O(1) append, cache\u2011friendly, flexible size   | O(n) insert/delete at arbitrary position              | General\u2011purpose sequence, stack             |\n",
    "| **Linked List**         | O(1) insert/delete at known position                    | O(n) search, poor cache locality                       | When many inserts/deletes at arbitrary pos  |\n",
    "| **Stack**               | LIFO, O(1) push/pop                                    | Limited access                                        | Expression evaluation, backtracking         |\n",
    "| **Queue**               | FIFO, O(1) enqueue/dequeue                             | Limited access                                        | BFS, task scheduling                         |\n",
    "| **Hash Table**          | O(1) average insert/search/delete                      | O(n) worst case, no order, memory overhead            | Dictionaries, caches, set operations        |\n",
    "| **Binary Search Tree**  | O(log n) search/insert/delete (if balanced)            | Unbalanced \u2192 O(n), overhead per node                   | Ordered dynamic set, range queries          |\n",
    "| **Balanced BST (AVL, Red\u2011Black)** | Guaranteed O(log n) operations               | Complex implementation, higher constant                | When worst\u2011case log n needed                |\n",
    "| **Heap**                | O(log n) insert/extract min/max, O(1) peek             | No search for arbitrary element                        | Priority queues, scheduling                  |\n",
    "| **Segment Tree**        | O(log n) range queries/updates                         | O(n) space, static size (unless dynamic)               | Range queries (sum, min, etc.)               |\n",
    "| **Fenwick Tree**        | O(log n) prefix sums, point updates, less memory       | Only prefix queries, not arbitrary ranges              | Frequency counting, cumulative sums          |\n",
    "| **Trie**                | O(m) search/insert (m = key length)                    | Memory\u2011heavy if many keys                               | Prefix matching, autocomplete, IP routing   |\n",
    "| **Suffix Array / LCP**  | Powerful string queries (pattern, repeats)             | O(n log n) construction, large memory                  | Bioinformatics, text indexing                |\n",
    "| **Graph (adj. list)**   | Space\u2011efficient for sparse graphs                       | Edge existence O(degree)                                | Most graph algorithms                        |\n",
    "| **Graph (adj. matrix)** | O(1) edge existence, good for dense graphs             | O(V\u00b2) memory                                           | Small graphs, Floyd\u2011Warshall                 |\n",
    "| **Disjoint Set (Union\u2011Find)** | Nearly O(1) union/find                            | Only connectivity queries                               | Kruskal\u2019s, dynamic connectivity              |\n",
    "\n",
    "---\n",
    "\n",
    "## 35.4 Cache\u2011Oblivious Algorithms\n",
    "\n",
    "Modern computers have a hierarchy of memory (registers, L1/L2/L3 cache, RAM, disk). **Cache\u2011oblivious algorithms** are designed to work well without knowing the cache size or block size. They achieve this by using divide\u2011and\u2011conquer that naturally exploits spatial and temporal locality.\n",
    "\n",
    "**Examples:**\n",
    "- **Vanilla matrix multiplication** is not cache\u2011oblivious; **tiled multiplication** is cache\u2011aware. **Cache\u2011oblivious** matrix multiplication uses recursive subdivision until submatrices fit in cache.\n",
    "- **Binary search** on a sorted array has poor cache behavior; a **cache\u2011oblivious B\u2011tree** (e.g., van Emde Boas layout) stores the tree recursively to improve locality.\n",
    "\n",
    "**Takeaway:** For performance\u2011critical applications, consider how your data structure interacts with the memory hierarchy. Even an O(log n) tree can be slower than an O(n) array scan if the array fits in cache and the tree causes many cache misses.\n",
    "\n",
    "---\n",
    "\n",
    "## 35.5 External Memory Algorithms\n",
    "\n",
    "When data is too large to fit in main memory, we must design algorithms that minimize disk I/O. Disk accesses are many orders of magnitude slower than RAM accesses. The standard model for external memory analysis counts **block transfers** (size B) between disk and RAM, with memory size M.\n",
    "\n",
    "### 35.5.1 B\u2011Trees\n",
    "\n",
    "B\u2011trees (and B+ trees) are the quintessential external memory data structure. They keep height low by storing many keys per node (order = block size). Operations require O(log_B N) disk accesses.\n",
    "\n",
    "- Used in almost all databases and file systems.\n",
    "- B+ trees store all data in leaves and link leaves for efficient range scans.\n",
    "\n",
    "### 35.5.2 Buffer Trees\n",
    "\n",
    "Buffer trees are designed for batch operations (like bulk inserts). They buffer updates in memory and periodically flush them to disk, achieving amortized O((N/B) log_{M/B} (N/B)) I/Os for a sequence of operations.\n",
    "\n",
    "### 35.5.3 External Sorting\n",
    "\n",
    "When sorting data that doesn't fit in memory, we use a **k\u2011way merge sort**:\n",
    "- Create sorted runs of size M (using internal sort).\n",
    "- Merge runs using a heap, reading one block from each run at a time.\n",
    "- I/O complexity: O((N/B) log_{M/B} (N/B)).\n",
    "\n",
    "### 35.5.4 LSM Trees (Log\u2011Structured Merge Trees)\n",
    "\n",
    "LSM trees are used in modern key\u2011value stores (LevelDB, RocksDB). They consist of a memory\u2011resident table (memtable) and a series of disk\u2011resident sorted tables (SSTables). Writes go to the memtable; when full, it is flushed to disk. Reads check the memtable and then the SSTables (using bloom filters to avoid many I/Os). Compactions merge SSTables periodically to maintain order and reclaim space.\n",
    "\n",
    "**Trade\u2011offs:** High write throughput, but read amplification (may need to check multiple levels).\n",
    "\n",
    "---\n",
    "\n",
    "## 35.6 Decision Framework\n",
    "\n",
    "When faced with a problem, follow these steps:\n",
    "\n",
    "1. **Understand the operations:**\n",
    "   - List all operations needed (insert, delete, search, min/max, range query, etc.).\n",
    "   - Identify which operations are most frequent.\n",
    "   - Determine if the data is static or dynamic.\n",
    "\n",
    "2. **Consider the environment:**\n",
    "   - In\u2011memory or on\u2011disk?\n",
    "   - Multi\u2011threaded?\n",
    "   - Real\u2011time constraints?\n",
    "\n",
    "3. **Evaluate candidate structures:**\n",
    "   - Use the table above to shortlist structures that support the required operations efficiently.\n",
    "   - Compare asymptotic complexities (worst\u2011case, average, amortized).\n",
    "   - Factor in practical considerations (cache, overhead, ease of implementation).\n",
    "\n",
    "4. **Prototype and measure if possible:**\n",
    "   - For performance\u2011critical applications, benchmark with realistic data.\n",
    "   - Sometimes the simplest structure is \u201cgood enough\u201d and much easier to maintain.\n",
    "\n",
    "5. **Plan for growth:**\n",
    "   - Will the data volume increase? Will new operations be added?\n",
    "   - Choose a structure that scales gracefully or can be migrated.\n",
    "\n",
    "### Example: Implementing a Cache\n",
    "\n",
    "- **Operations:** insert (key, value), lookup (key), delete (key) \u2013 all fast.\n",
    "- **Requirement:** Must evict least\u2011recently\u2011used (LRU) item when full.\n",
    "- **Candidate structures:** Hash table + doubly linked list.\n",
    "  - Hash table gives O(1) lookup.\n",
    "  - Linked list maintains order of use; moving an item to front is O(1) if we have the node pointer (stored in hash table).\n",
    "- **Why not a balanced tree?** O(log n) is slower; also, maintaining order by access time is more complex with a tree.\n",
    "\n",
    "Thus the classic combination: HashMap + LinkedList.\n",
    "\n",
    "---\n",
    "\n",
    "## 35.7 Summary\n",
    "\n",
    "```\n",
    "\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "\u2502                    CHOOSING THE RIGHT DATA STRUCTURE                  \u2502\n",
    "\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  Key Criteria:                                                      \u2502\n",
    "\u2502    \u2022 Time complexity (worst, average, amortized)                    \u2502\n",
    "\u2502    \u2022 Space complexity                                                \u2502\n",
    "\u2502    \u2022 Cache locality                                                  \u2502\n",
    "\u2502    \u2022 Code complexity and maintainability                            \u2502\n",
    "\u2502    \u2022 Concurrency requirements                                        \u2502\n",
    "\u2502    \u2022 External memory constraints                                     \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  Common Structures and Their Sweet Spots:                           \u2502\n",
    "\u2502    \u2022 Array: index access, fixed size                                 \u2502\n",
    "\u2502    \u2022 Dynamic array: flexible sequence, stack                        \u2502\n",
    "\u2502    \u2022 Linked list: frequent insert/delete at known position           \u2502\n",
    "\u2502    \u2022 Hash table: fast key\u2011value lookup                               \u2502\n",
    "\u2502    \u2022 Balanced BST: ordered operations                                \u2502\n",
    "\u2502    \u2022 Heap: priority queue                                            \u2502\n",
    "\u2502    \u2022 Segment/Fenwick tree: range queries                             \u2502\n",
    "\u2502    \u2022 Trie: string prefixes                                           \u2502\n",
    "\u2502    \u2022 B\u2011tree: external memory                                         \u2502\n",
    "\u2502    \u2022 LSM tree: high write throughput                                 \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2502  Decision Process:                                                   \u2502\n",
    "\u2502    1. List required operations                                       \u2502\n",
    "\u2502    2. Identify frequency                                             \u2502\n",
    "\u2502    3. Consider environment (RAM/disk, concurrency)                   \u2502\n",
    "\u2502    4. Compare candidate structures                                   \u2502\n",
    "\u2502    5. Prototype if needed                                            \u2502\n",
    "\u2502                                                                      \u2502\n",
    "\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 35.8 Practice Problems\n",
    "\n",
    "These problems are less about implementing a single structure and more about reasoning about the best choice.\n",
    "\n",
    "1. **Design a phone book** that supports lookup by name, insertion, deletion, and listing all names in alphabetical order. What data structure(s) would you use?\n",
    "\n",
    "2. **Implement an LRU cache** (LeetCode 146) \u2013 discuss why the chosen combination works.\n",
    "\n",
    "3. **Design a system** that records and queries the number of visits to a website per minute over the last hour. You need to support updating the count for the current minute and querying the total for the last hour. (Hint: circular buffer or deque.)\n",
    "\n",
    "4. **Design an autocomplete system** for a search engine. What data structure would you use for storing the dictionary and for retrieving suggestions based on prefix?\n",
    "\n",
    "5. **External memory sorting:** You have a file of 100 million integers (4 bytes each) and 1 GB of RAM. Describe how you would sort the file efficiently.\n",
    "\n",
    "6. **Database indexing:** Why do most relational databases use B+ trees rather than hash tables for indexing?\n",
    "\n",
    "7. **Social network friends list:** You need to store each user's friends and support operations like \"add friend\", \"remove friend\", \"check if two users are friends\", and \"list all friends of a user\". Which structure?\n",
    "\n",
    "8. **Multi\u2011level feedback queue** for CPU scheduling: which data structure(s) would you use?\n",
    "\n",
    "9. **Design a spell checker** that can quickly check if a word is in a dictionary and suggest corrections (edit distance). What data structures and algorithms would you combine?\n",
    "\n",
    "10. **Real\u2011time stock price feed:** You receive millions of price updates per second and need to answer queries for the current price of any stock, as well as the top 10 highest prices. What structures?\n",
    "\n",
    "---\n",
    "\n",
    "## 35.9 Further Reading\n",
    "\n",
    "1. **\"Introduction to Algorithms\" (CLRS)** \u2013 Chapters on data structures throughout.\n",
    "2. **\"The Algorithm Design Manual\"** by Steven Skiena \u2013 Has a catalog of data structures.\n",
    "3. **\"Data Structures and Algorithms in Python\"** by Goodrich, Tamassia, Goldwasser \u2013 Practical coverage.\n",
    "4. **\"Database System Concepts\"** by Silberschatz, Korth, Sudarshan \u2013 For B\u2011trees and external storage.\n",
    "5. **\"Designing Data\u2011Intensive Applications\"** by Martin Kleppmann \u2013 Excellent for real\u2011world storage systems (LSM trees, etc.).\n",
    "6. **\"Cache\u2011Oblivious Algorithms\"** \u2013 Research papers by Frigo et al. (1999).\n",
    "\n",
    "---\n",
    "\n",
    "> **Coming in Chapter 36**: **DSA in Production Systems** \u2013 We'll look at practical applications like database indexing, caching, rate limiting, and probabilistic data structures.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 35**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../9. problem_solving_patterns_techniques/34. intervals_and_merging.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='36. dsa_in_production_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}