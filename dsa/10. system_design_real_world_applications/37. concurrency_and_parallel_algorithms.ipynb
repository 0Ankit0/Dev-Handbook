{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 37: Concurrency and Parallel Algorithms\n",
    "\n",
    "> *\"Concurrency is about dealing with lots of things at once. Parallelism is about doing lots of things at once.\"* — Rob Pike\n",
    "\n",
    "---\n",
    "\n",
    "## 37.1 Introduction\n",
    "\n",
    "As hardware evolves toward multi‑core processors and distributed systems become ubiquitous, writing correct and efficient concurrent and parallel programs is essential. This chapter explores data structures and algorithms designed to work in such environments. We will cover:\n",
    "\n",
    "- **Lock‑free data structures** that use atomic operations to avoid the pitfalls of locking.\n",
    "- **Concurrent hash maps** that allow multiple threads to read and write simultaneously.\n",
    "- **Parallel sorting algorithms** that divide work across cores.\n",
    "- The **MapReduce** programming model for large‑scale data processing.\n",
    "\n",
    "### 37.1.1 Why Concurrency and Parallelism Matter\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    IMPORTANCE OF CONCURRENCY & PARALLELISM           │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  1. PERFORMANCE: Utilise multiple cores to speed up computation.    │\n",
    "│  2. SCALABILITY: Handle increasing workloads by adding resources.   │\n",
    "│  3. RESPONSIVENESS: Keep UI responsive while background work runs.  │\n",
    "│  4. THROUGHPUT: Process many requests simultaneously (servers).     │\n",
    "│  5. FAULT TOLERANCE: Distribute computation across machines.        │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "**Key challenge:** Coordinating access to shared data without introducing bugs like race conditions, deadlocks, or livelocks.\n",
    "\n",
    "---\n",
    "\n",
    "## 37.2 Lock‑Free Data Structures\n",
    "\n",
    "Lock‑free data structures guarantee that at least one thread makes progress even if others are delayed, and they avoid problems like deadlock and priority inversion. They rely on **atomic operations** provided by hardware (e.g., compare‑and‑swap, fetch‑and‑add).\n",
    "\n",
    "### 37.2.1 Compare‑and‑Swap (CAS)\n",
    "\n",
    "CAS is a CPU instruction that atomically compares the contents of a memory location to a given value and, if they are the same, modifies the memory location to a new value. It returns whether the swap succeeded.\n",
    "\n",
    "```cpp\n",
    "bool compare_and_swap(int* p, int oldval, int newval) {\n",
    "    if (*p == oldval) {\n",
    "        *p = newval;\n",
    "        return true;\n",
    "    }\n",
    "    return false;\n",
    "}\n",
    "```\n",
    "\n",
    "In C++11, it's `std::atomic<T>::compare_exchange_strong`. In Java, `AtomicInteger.compareAndSet`. In Python, `ctypes` can access it, but typically we use higher‑level libraries like `multiprocessing` or `concurrent.futures`.\n",
    "\n",
    "### 37.2.2 The ABA Problem\n",
    "\n",
    "Consider a thread reading a shared variable `*p = A`, then later it CASes expecting it still to be A. Between the read and CAS, another thread might change it to B and then back to A. The CAS would succeed, but the structure might have changed (e.g., the node was freed and reallocated). This is the **ABA problem**.\n",
    "\n",
    "**Solutions:**\n",
    "- Use double‑width CAS (tagged pointers) that include a version counter.\n",
    "- Use hazard pointers or epoch‑based reclamation.\n",
    "\n",
    "### 37.2.3 Lock‑Free Stack (Treiber Stack)\n",
    "\n",
    "The Treiber stack is a classic lock‑free stack using a linked list and CAS.\n",
    "\n",
    "```cpp\n",
    "#include <atomic>\n",
    "\n",
    "template<typename T>\n",
    "class TreiberStack {\n",
    "private:\n",
    "    struct Node {\n",
    "        T data;\n",
    "        Node* next;\n",
    "        Node(const T& d) : data(d), next(nullptr) {}\n",
    "    };\n",
    "    std::atomic<Node*> head;\n",
    "\n",
    "public:\n",
    "    TreiberStack() : head(nullptr) {}\n",
    "\n",
    "    void push(const T& data) {\n",
    "        Node* new_node = new Node(data);\n",
    "        new_node->next = head.load(std::memory_order_relaxed);\n",
    "        while (!head.compare_exchange_weak(new_node->next, new_node,\n",
    "                                           std::memory_order_release,\n",
    "                                           std::memory_order_relaxed)) {\n",
    "            // retry if head changed\n",
    "        }\n",
    "    }\n",
    "\n",
    "    bool pop(T& result) {\n",
    "        Node* old_head = head.load(std::memory_order_relaxed);\n",
    "        while (old_head &&\n",
    "               !head.compare_exchange_weak(old_head, old_head->next,\n",
    "                                           std::memory_order_acquire,\n",
    "                                           std::memory_order_relaxed)) {\n",
    "            // retry\n",
    "        }\n",
    "        if (old_head) {\n",
    "            result = old_head->data;\n",
    "            // For production, need safe memory reclamation (hazard pointers)\n",
    "            delete old_head;\n",
    "            return true;\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "};\n",
    "```\n",
    "\n",
    "**Note:** Memory reclamation is non‑trivial; simply deleting a node that might still be accessed by other threads is unsafe. Production implementations use hazard pointers, epoch‑based reclamation, or rely on garbage collection.\n",
    "\n",
    "### 37.2.4 Lock‑Free Queue (Michael‑Scott Queue)\n",
    "\n",
    "The Michael‑Scott queue is a lock‑less FIFO queue using a linked list with a dummy node and CAS on both head and tail.\n",
    "\n",
    "**Simplified pseudocode (C++‑like):**\n",
    "\n",
    "```cpp\n",
    "template<typename T>\n",
    "class MSQueue {\n",
    "    struct Node {\n",
    "        T data;\n",
    "        std::atomic<Node*> next;\n",
    "        Node(const T& d) : data(d), next(nullptr) {}\n",
    "    };\n",
    "    std::atomic<Node*> head;\n",
    "    std::atomic<Node*> tail;\n",
    "\n",
    "public:\n",
    "    MSQueue() {\n",
    "        Node* dummy = new Node(T());\n",
    "        head.store(dummy, std::memory_order_relaxed);\n",
    "        tail.store(dummy, std::memory_order_relaxed);\n",
    "    }\n",
    "\n",
    "    void enqueue(const T& data) {\n",
    "        Node* new_node = new Node(data);\n",
    "        Node* last = tail.load(std::memory_order_relaxed);\n",
    "        Node* null = nullptr;\n",
    "        while (!last->next.compare_exchange_weak(null, new_node)) {\n",
    "            last = tail.load(std::memory_order_relaxed);\n",
    "            null = nullptr;\n",
    "        }\n",
    "        tail.compare_exchange_strong(last, new_node);\n",
    "    }\n",
    "\n",
    "    bool dequeue(T& result) {\n",
    "        Node* first = head.load(std::memory_order_relaxed);\n",
    "        Node* next = first->next.load(std::memory_order_relaxed);\n",
    "        if (next == nullptr) return false; // empty\n",
    "        result = next->data;\n",
    "        if (head.compare_exchange_weak(first, next)) {\n",
    "            // reclaim first (dummy) node\n",
    "            delete first;\n",
    "            return true;\n",
    "        }\n",
    "        return false; // retry in real impl\n",
    "    }\n",
    "};\n",
    "```\n",
    "\n",
    "Again, careful memory reclamation is needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 37.3 Concurrent Hash Maps\n",
    "\n",
    "Concurrent hash maps allow multiple threads to insert, delete, and look up without blocking each other unnecessarily. Several strategies exist:\n",
    "\n",
    "### 37.3.1 Coarse‑Grained Locking\n",
    "\n",
    "Wrap the entire hash map with a single lock. Simple but poor scalability.\n",
    "\n",
    "### 37.3.2 Fine‑Grained Locking (Striping)\n",
    "\n",
    "Partition the hash table into segments (buckets) each with its own lock. Threads lock only the segment they need. This is how `ConcurrentHashMap` in Java (pre‑8) and `tbb::concurrent_hash_map` work.\n",
    "\n",
    "**Example (simplified):**\n",
    "\n",
    "```python\n",
    "import threading\n",
    "\n",
    "class ConcurrentHashMap:\n",
    "    def __init__(self, num_buckets=16):\n",
    "        self.num_buckets = num_buckets\n",
    "        self.buckets = [{} for _ in range(num_buckets)]\n",
    "        self.locks = [threading.Lock() for _ in range(num_buckets)]\n",
    "\n",
    "    def _bucket(self, key):\n",
    "        return hash(key) % self.num_buckets\n",
    "\n",
    "    def put(self, key, value):\n",
    "        b = self._bucket(key)\n",
    "        with self.locks[b]:\n",
    "            self.buckets[b][key] = value\n",
    "\n",
    "    def get(self, key):\n",
    "        b = self._bucket(key)\n",
    "        with self.locks[b]:\n",
    "            return self.buckets[b].get(key)\n",
    "```\n",
    "\n",
    "**Scalability:** Good if hash distributes evenly; lock contention only within each bucket.\n",
    "\n",
    "### 37.3.3 Lock‑Free Hash Maps\n",
    "\n",
    "Fully lock‑free hash maps are more complex but offer even higher scalability. They often use atomic compare‑and‑swap on pointers to the bucket arrays. Example: Java's `ConcurrentHashMap` in later versions uses a mix of techniques including CAS for certain operations.\n",
    "\n",
    "**Idea:**\n",
    "- Maintain an array of atomic pointers to nodes (linked lists per bucket).\n",
    "- Use CAS to insert a node at the head of a bucket.\n",
    "- For resizing, use a multi‑step process where threads help with the resize.\n",
    "\n",
    "Implementation is beyond the scope of this handbook, but libraries like `libcds` provide them.\n",
    "\n",
    "---\n",
    "\n",
    "## 37.4 Parallel Sorting\n",
    "\n",
    "Parallel sorting algorithms divide the array among threads, sort subarrays in parallel, and then merge.\n",
    "\n",
    "### 37.4.1 Parallel Merge Sort\n",
    "\n",
    "Split the array into chunks, sort each chunk in parallel, then merge the results. The merging step can also be parallelized (e.g., using parallel merge of two sorted arrays).\n",
    "\n",
    "**Simple implementation using Python's `multiprocessing` (conceptual):**\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool\n",
    "\n",
    "def parallel_merge_sort(arr, num_workers=4):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    chunk_size = len(arr) // num_workers\n",
    "    chunks = [arr[i:i+chunk_size] for i in range(0, len(arr), chunk_size)]\n",
    "    with Pool(num_workers) as pool:\n",
    "        sorted_chunks = pool.map(sorted, chunks)  # simple sort per chunk\n",
    "    # merge sorted_chunks (merge two at a time)\n",
    "    while len(sorted_chunks) > 1:\n",
    "        new_chunks = []\n",
    "        for i in range(0, len(sorted_chunks), 2):\n",
    "            if i+1 < len(sorted_chunks):\n",
    "                new_chunks.append(merge(sorted_chunks[i], sorted_chunks[i+1]))\n",
    "            else:\n",
    "                new_chunks.append(sorted_chunks[i])\n",
    "        sorted_chunks = new_chunks\n",
    "    return sorted_chunks[0]\n",
    "```\n",
    "\n",
    "**Efficiency:** O((n/p) log (n/p)) per chunk, plus merging O(n log p). Overall O(n log n) work, but with improved constant.\n",
    "\n",
    "### 37.4.2 Bitonic Sort\n",
    "\n",
    "Bitonic sort is a sorting network that can be parallelized efficiently on parallel architectures (e.g., GPUs). It has O(log² n) stages, each performing O(n) comparisons, but they can be done in parallel.\n",
    "\n",
    "**Idea:** A bitonic sequence is one that first increases then decreases (or vice‑versa). Bitonic sort recursively builds bitonic sequences and then sorts them with bitonic merge.\n",
    "\n",
    "**C++ pseudocode (using OpenMP):**\n",
    "\n",
    "```cpp\n",
    "void bitonic_sort(int* arr, int n) {\n",
    "    for (int k = 2; k <= n; k *= 2) {\n",
    "        for (int j = k/2; j > 0; j /= 2) {\n",
    "            #pragma omp parallel for\n",
    "            for (int i = 0; i < n; i++) {\n",
    "                int ixj = i ^ j;\n",
    "                if (ixj > i) {\n",
    "                    if (( (i & k) == 0 && arr[i] > arr[ixj] ) ||\n",
    "                        ( (i & k) != 0 && arr[i] < arr[ixj] )) {\n",
    "                        std::swap(arr[i], arr[ixj]);\n",
    "                    }\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Complexity:** O(n log² n) operations but with good parallelism.\n",
    "\n",
    "### 37.4.3 Sample Sort\n",
    "\n",
    "Sample sort is a parallel version of quicksort that selects a set of splitters from a random sample of elements, partitions the array into buckets, sorts each bucket in parallel, and concatenates.\n",
    "\n",
    "**Algorithm:**\n",
    "1. Choose p‑1 splitters by sorting a random sample of size p * s (s is oversampling factor).\n",
    "2. Partition the array into p buckets according to these splitters (using binary search to find bucket index).\n",
    "3. Sort each bucket in parallel (e.g., using quicksort or sample sort recursively).\n",
    "4. Concatenate buckets.\n",
    "\n",
    "**Advantages:** Good load balancing, works well on distributed systems.\n",
    "\n",
    "**Python‑like pseudocode (ignoring actual parallelism):**\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def sample_sort(arr, p):\n",
    "    if len(arr) <= 1:\n",
    "        return arr\n",
    "    # choose splitters\n",
    "    sample = random.sample(arr, p*16)\n",
    "    sample.sort()\n",
    "    splitters = [sample[i * len(sample) // p] for i in range(1, p)]\n",
    "    # create buckets\n",
    "    buckets = [[] for _ in range(p)]\n",
    "    for x in arr:\n",
    "        idx = bisect.bisect_right(splitters, x)  # find bucket\n",
    "        buckets[idx].append(x)\n",
    "    # sort buckets in parallel (here sequentially)\n",
    "    sorted_buckets = [sorted(b) for b in buckets]\n",
    "    # concatenate\n",
    "    result = []\n",
    "    for b in sorted_buckets:\n",
    "        result.extend(b)\n",
    "    return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 37.5 MapReduce Paradigm\n",
    "\n",
    "MapReduce is a programming model for processing large datasets in a distributed cluster. It was popularized by Google and is implemented in frameworks like Hadoop.\n",
    "\n",
    "### 37.5.1 Model\n",
    "\n",
    "The computation is expressed as two phases: **Map** and **Reduce**.\n",
    "\n",
    "- **Map:** The master node splits the input into independent chunks and assigns them to worker nodes. Each worker applies the user‑defined `map` function to each record, producing intermediate key/value pairs.\n",
    "- **Shuffle:** The system sorts and groups intermediate pairs by key.\n",
    "- **Reduce:** Workers apply the user‑defined `reduce` function to each group, producing final output.\n",
    "\n",
    "### 37.5.2 Word Count Example\n",
    "\n",
    "```python\n",
    "def map(document):\n",
    "    # document is a string\n",
    "    for word in document.split():\n",
    "        emit(word, 1)\n",
    "\n",
    "def reduce(key, values):\n",
    "    # key: a word, values: list of counts\n",
    "    emit(key, sum(values))\n",
    "```\n",
    "\n",
    "### 37.5.3 Data Structures and Algorithms in MapReduce\n",
    "\n",
    "- **Map phase:** Typically stateless; can be done in parallel without communication.\n",
    "- **Shuffle:** Requires distributed sorting and partitioning (often using a hash of the key) to send all values for a key to the same reducer. This is similar to external sorting.\n",
    "- **Reduce:** Aggregates values; may need to handle large groups by streaming or combining.\n",
    "\n",
    "**Combiners** are optional “mini‑reducers” that run on the map side to reduce network traffic.\n",
    "\n",
    "### 37.5.4 Distributed Consistency\n",
    "\n",
    "MapReduce handles failures by re‑executing failed tasks. It assumes a shared‑nothing architecture and relies on the file system for intermediate data.\n",
    "\n",
    "---\n",
    "\n",
    "## 37.6 Summary\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────────────┐\n",
    "│                    CONCURRENCY & PARALLEL ALGORITHMS SUMMARY         │\n",
    "├─────────────────────────────────────────────────────────────────────┤\n",
    "│                                                                      │\n",
    "│  Lock‑Free Data Structures:                                         │\n",
    "│    • Use CAS to coordinate without locks.                           │\n",
    "│    • Treiber stack, Michael‑Scott queue.                            │\n",
    "│    • Must handle ABA problem and memory reclamation.                │\n",
    "│                                                                      │\n",
    "│  Concurrent Hash Maps:                                              │\n",
    "│    • Fine‑grained locking (bucket locks).                           │\n",
    "│    • Lock‑free variants use CAS on pointers.                        │\n",
    "│                                                                      │\n",
    "│  Parallel Sorting:                                                  │\n",
    "│    • Parallel merge sort, bitonic sort, sample sort.                │\n",
    "│    • Aim to distribute work evenly across cores.                    │\n",
    "│                                                                      │\n",
    "│  MapReduce:                                                         │\n",
    "│    • Distributed programming model: Map, Shuffle, Reduce.           │\n",
    "│    • Handles large‑scale data processing on clusters.               │\n",
    "│                                                                      │\n",
    "└─────────────────────────────────────────────────────────────────────┘\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 37.7 Practice Problems\n",
    "\n",
    "### Lock‑Free Structures\n",
    "1. Implement a lock‑free stack using CAS in your language of choice.\n",
    "2. Explain the ABA problem and propose a solution.\n",
    "3. Implement a lock‑free queue (Michael‑Scott) with hazard pointers (conceptually).\n",
    "\n",
    "### Concurrent Hash Maps\n",
    "4. Design a concurrent hash map with fine‑grained locking and dynamic resizing.\n",
    "5. Compare the performance of coarse‑grained vs. fine‑grained locking under high contention.\n",
    "\n",
    "### Parallel Sorting\n",
    "6. Write a parallel merge sort using Python's `multiprocessing`.\n",
    "7. Implement bitonic sort in OpenMP or C++ threads.\n",
    "8. Simulate sample sort on a large array and analyze load balancing.\n",
    "\n",
    "### MapReduce\n",
    "9. Design a MapReduce algorithm to compute the average of values per key.\n",
    "10. Explain how you would handle stragglers in a MapReduce job.\n",
    "\n",
    "### General\n",
    "11. Describe the differences between concurrency and parallelism.\n",
    "12. What is the role of atomic operations in lock‑free programming?\n",
    "13. How would you measure the scalability of a concurrent data structure?\n",
    "\n",
    "---\n",
    "\n",
    "## 37.8 Further Reading\n",
    "\n",
    "1. **\"The Art of Multiprocessor Programming\"** by Herlihy & Shavit – The definitive text.\n",
    "2. **\"Java Concurrency in Practice\"** by Goetz et al. – Practical guide, includes concurrent collections.\n",
    "3. **\"C++ Concurrency in Action\"** by Anthony Williams – For lock‑free programming in C++.\n",
    "4. **\"MapReduce: Simplified Data Processing on Large Clusters\"** – Original Google paper by Dean & Ghemawat.\n",
    "5. **\"Introduction to Algorithms\" (CLRS)** – Chapter on parallel algorithms (sorting).\n",
    "6. **Online resources:** Intel Threading Building Blocks (TBB) documentation, OpenMP tutorials.\n",
    "\n",
    "---\n",
    "\n",
    "> **Coming in Chapter 38**: **Problem‑Solving Framework** – We'll cover the UMPIRE method, time management, communication strategies, and edge case identification.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 37**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
