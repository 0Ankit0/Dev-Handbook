{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "56754645",
   "metadata": {},
   "source": [
    "Here is **Chapter 15: Large Language Models (LLMs) & Generative AI** \u2014 the frontier of artificial intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 15: LARGE LANGUAGE MODELS (LLMs) & GENERATIVE AI**\n",
    "\n",
    "*The Era of Foundation Models*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Large Language Models (LLMs) have democratized artificial general intelligence capabilities, demonstrating emergent abilities in reasoning, code generation, and instruction following at unprecedented scale. This chapter covers the engineering and scientific principles behind models like GPT-4, LLaMA, and Claude: from scaling laws and distributed training infrastructure to alignment via RLHF and production deployment strategies including RAG and efficient serving.\n",
    "\n",
    "**Estimated Time:** 70-80 hours (5-6 weeks)  \n",
    "**Prerequisites:** Chapters 11-14 (Deep Learning Frameworks, Transformers, Attention mechanisms)\n",
    "\n",
    "---\n",
    "\n",
    "## **15.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Apply scaling laws to estimate optimal model size and training compute (Chinchilla optimal)\n",
    "2. Implement efficient large-scale training using DeepSpeed ZeRO, FSDP, and FlashAttention\n",
    "3. Understand and apply alignment techniques: RLHF, DPO, and Constitutional AI\n",
    "4. Build RAG (Retrieval-Augmented Generation) systems with vector databases and embedding models\n",
    "5. Engineer complex prompt patterns: Chain-of-Thought, ReAct, and Tree-of-Thoughts\n",
    "6. Deploy and serve LLMs efficiently using quantization (GPTQ, AWQ), vLLM, and speculative decoding\n",
    "7. Fine-tune open-source LLMs (LLaMA-2, Mistral) using LoRA and QLoRA for domain adaptation\n",
    "\n",
    "---\n",
    "\n",
    "## **15.1 Scaling Laws and Compute-Optimal Training**\n",
    "\n",
    "#### **15.1.1 The Scaling Laws**\n",
    "\n",
    "Kaplan et al. (OpenAI, 2020) established that loss scales as a power law with model size ($N$), dataset size ($D$), and compute ($C$):\n",
    "\n",
    "$$L(N) = \\left(\\frac{N_c}{N}\\right)^{\\alpha_N}, \\quad L(D) = \\left(\\frac{D_c}{D}\\right)^{\\alpha_D}$$\n",
    "\n",
    "Where $\\alpha_N \\approx 0.076$, $\\alpha_D \\approx 0.095$.\n",
    "\n",
    "**Implication:** To halve the loss, you need ~10x more parameters or data.\n",
    "\n",
    "#### **15.1.2 Chinchilla Optimal (Hoffmann et al., 2022)**\n",
    "\n",
    "Kaplan suggested training large models on relatively little data. Chinchilla showed this was suboptimal.\n",
    "\n",
    "**Optimal allocation:** For compute budget $C$ (FLOPs), model size $N$ and tokens $D$ should scale equally:\n",
    "\n",
    "$$N_{opt} \\propto C^{0.5}, \\quad D_{opt} \\propto C^{0.5}$$\n",
    "\n",
    "**Rule of Thumb:** ~20 tokens per parameter (e.g., 70B model needs ~1.4T tokens, not 300B as in GPT-3).\n",
    "\n",
    "**Practical Impact:**\n",
    "- GPT-3 (175B, 300B tokens): Undertrained by Chinchilla standards\n",
    "- LLaMA-2 (70B, 2T tokens): Closer to compute-optimal\n",
    "- Smaller models trained longer often outperform larger models trained briefly\n",
    "\n",
    "---\n",
    "\n",
    "## **15.2 Modern LLM Architectures**\n",
    "\n",
    "#### **15.2.1 Decoder-Only Dominance**\n",
    "\n",
    "Modern LLMs (GPT-4, LLaMA, Claude, PaLM) use decoder-only Transformer variants:\n",
    "\n",
    "**Key Modifications from Original Transformer:**\n",
    "1. **Pre-LayerNorm:** LayerNorm before attention/FFN (stabilizes training at scale)\n",
    "2. **Rotary Position Embeddings (RoPE):** $f(q, m) = qe^{im\\theta}$ \u2014 relative position encoding that generalizes to longer sequences than seen during training\n",
    "3. **SwiGLU Activation:** $\\text{SwiGLU}(x) = \\text{Swish}_1(xW) \\otimes xV$ \u2014 improves performance over ReLU/GeLU\n",
    "4. **Grouped Query Attention (GQA):** Share key/value heads across query heads (reduces memory bandwidth during inference)\n",
    "\n",
    "```python\n",
    "# Simplified RoPE implementation\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # q, k: (batch, heads, seq, dim)\n",
    "    # Rotate half dimensions\n",
    "    q_rot = torch.stack([-q[..., 1::2], q[..., ::2]], dim=-1).flatten(-2)\n",
    "    k_rot = torch.stack([-k[..., 1::2], k[..., ::2]], dim=-1).flatten(-2)\n",
    "    \n",
    "    q = q * cos + q_rot * sin\n",
    "    k = k * cos + k_rot * sin\n",
    "    return q, k\n",
    "```\n",
    "\n",
    "#### **15.2.2 Mixture of Experts (MoE)**\n",
    "\n",
    "Models like GPT-4 and Mixtral use sparse layers: instead of one dense FFN, use $N$ experts, route to top-$k$ per token.\n",
    "\n",
    "**Benefits:**\n",
    "- Scale parameters without scaling compute (only activate subset)\n",
    "- Specialization: Experts specialize in different domains (code, math, science)\n",
    "\n",
    "```python\n",
    "# Simplified MoE layer\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, d_model, num_experts=8, top_k=2):\n",
    "        super().__init__()\n",
    "        self.experts = nn.ModuleList([FFN(d_model) for _ in range(num_experts)])\n",
    "        self.gate = nn.Linear(d_model, num_experts)\n",
    "        self.top_k = top_k\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch*seq, d_model)\n",
    "        gates = torch.softmax(self.gate(x), dim=-1)  # (batch*seq, num_experts)\n",
    "        \n",
    "        # Select top-k experts\n",
    "        top_gates, top_indices = torch.topk(gates, self.top_k, dim=-1)\n",
    "        top_gates = top_gates / top_gates.sum(dim=-1, keepdim=True)  # Normalize\n",
    "        \n",
    "        output = torch.zeros_like(x)\n",
    "        for i in range(self.top_k):\n",
    "            expert_idx = top_indices[:, i]\n",
    "            expert_gate = top_gates[:, i:i+1]\n",
    "            \n",
    "            # Route to expert (simplified - actual implementation uses efficient grouping)\n",
    "            for j in range(len(self.experts)):\n",
    "                mask = (expert_idx == j)\n",
    "                if mask.any():\n",
    "                    output[mask] += self.experts[j](x[mask]) * expert_gate[mask]\n",
    "        \n",
    "        return output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **15.3 Training at Scale**\n",
    "\n",
    "Training 7B+ parameter models requires specialized distributed strategies.\n",
    "\n",
    "#### **15.3.1 ZeRO (Zero Redundancy Optimizer) \u2014 DeepSpeed**\n",
    "\n",
    "Partitions optimizer states, gradients, and parameters across data parallel processes.\n",
    "\n",
    "**Stages:**\n",
    "- **ZeRO-1:** Partition optimizer states (4x memory reduction)\n",
    "- **ZeRO-2:** + Partition gradients (8x reduction)\n",
    "- **ZeRO-3:** + Partition parameters (linear reduction with degree)\n",
    "\n",
    "```python\n",
    "# DeepSpeed config\n",
    "ds_config = {\n",
    "    \"fp16\": {\"enabled\": True},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,  # ZeRO-2\n",
    "        \"allgather_partitions\": True,\n",
    "        \"reduce_scatter\": True,\n",
    "    },\n",
    "    \"train_batch_size\": \"auto\",\n",
    "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
    "}\n",
    "\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
    "    model=model,\n",
    "    model_parameters=model.parameters(),\n",
    "    config=ds_config\n",
    ")\n",
    "```\n",
    "\n",
    "#### **15.3.2 FlashAttention**\n",
    "\n",
    "IO-aware exact attention algorithm that reduces HBM (high bandwidth memory) reads/writes from $O(N^2)$ to $O(N)$.\n",
    "\n",
    "**Key Idea:** Tiling to fit in SRAM (fast on-chip memory), recomputing softmax statistics online.\n",
    "\n",
    "**Speedup:** 2-4x faster, 10-20x memory efficient for long sequences (2k+).\n",
    "\n",
    "```python\n",
    "# Using FlashAttention 2\n",
    "from transformers import AutoModelForCausalLM\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"meta-llama/Llama-2-7b-hf\",\n",
    "    attn_implementation=\"flash_attention_2\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### **15.3.3 Gradient Checkpointing (Activation Checkpointing)**\n",
    "\n",
    "Trade compute for memory: Don't store activations during forward, recompute during backward.\n",
    "\n",
    "**Memory:** Linear in layers instead of quadratic (enables training 10x larger models).\n",
    "\n",
    "```python\n",
    "model.gradient_checkpointing_enable()\n",
    "# Or in PyTorch directly:\n",
    "from torch.utils.checkpoint import checkpoint\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **15.4 Alignment: RLHF and Alternatives**\n",
    "\n",
    "Raw pretrained models predict internet text; alignment makes them helpful, harmless, and honest.\n",
    "\n",
    "#### **15.4.1 Supervised Fine-Tuning (SFT)**\n",
    "\n",
    "Train on high-quality instruction-response pairs.\n",
    "\n",
    "**Data Format:**\n",
    "```\n",
    "[INST] <<SYS>>\n",
    "You are a helpful assistant.\n",
    "<</SYS>>\n",
    "\n",
    "What is the capital of France? [/INST]\n",
    "The capital of France is Paris.</s>\n",
    "```\n",
    "\n",
    "#### **15.4.2 RLHF (Reinforcement Learning from Human Feedback)**\n",
    "\n",
    "**Three-Step Process:**\n",
    "\n",
    "1. **Collect Preferences:** Humans rank multiple model outputs (A > B)\n",
    "2. **Train Reward Model (RM):** $r_\\theta(x, y)$ predicts human preference\n",
    "3. **Optimize Policy with PPO:**\n",
    "   \n",
    "   $$\\max_{\\pi} \\mathbb{E}_{x \\sim D, y \\sim \\pi}[r_\\theta(x, y)] - \\beta \\mathbb{D}_{KL}[\\pi || \\pi_{ref}]$$\n",
    "\n",
    "   KL penalty prevents model from drifting too far from pretrained distribution.\n",
    "\n",
    "**PPO (Proximal Policy Optimization):** Clipped surrogate objective for stable training.\n",
    "\n",
    "```python\n",
    "# Simplified PPO update (conceptual)\n",
    "ratio = torch.exp(new_logprob - old_logprob)\n",
    "surrogate1 = ratio * advantages\n",
    "surrogate2 = torch.clamp(ratio, 1-eps, 1+eps) * advantages\n",
    "policy_loss = -torch.min(surrogate1, surrogate2).mean()\n",
    "```\n",
    "\n",
    "#### **15.4.3 DPO (Direct Preference Optimization)**\n",
    "\n",
    "Bypass reward model and PPO. Optimize directly on preference data with classification loss.\n",
    "\n",
    "$$\\mathcal{L}_{DPO} = -\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)$$\n",
    "\n",
    "**Advantages:** Simpler, more stable, no reward model training, often better performance.\n",
    "\n",
    "#### **15.4.4 Constitutional AI (Anthropic)**\n",
    "\n",
    "Self-improvement via AI feedback:\n",
    "1. Model generates responses\n",
    "2. AI critiques responses based on \"constitution\" (principles)\n",
    "3. Model revises based on critique\n",
    "4. Train on revised responses (RLAIF - RL from AI Feedback)\n",
    "\n",
    "---\n",
    "\n",
    "## **15.5 Prompt Engineering and Advanced Inference**\n",
    "\n",
    "#### **15.5.1 Chain-of-Thought (CoT)**\n",
    "\n",
    "Prompt the model to show reasoning steps: \"Let's think step by step.\"\n",
    "\n",
    "**Automatic CoT:** Generate multiple reasoning paths, vote on final answer (Self-Consistency).\n",
    "\n",
    "#### **15.5.2 ReAct (Reasoning + Acting)**\n",
    "\n",
    "Interleave reasoning traces with actions (API calls, tool use).\n",
    "\n",
    "```\n",
    "Thought: I need to find the current weather in Paris.\n",
    "Action: search_weather[Paris]\n",
    "Observation: 15\u00b0C, sunny\n",
    "Thought: Now I can answer the user.\n",
    "Final Answer: It's 15\u00b0C and sunny in Paris.\n",
    "```\n",
    "\n",
    "#### **15.5.3 Tree of Thoughts (ToT)**\n",
    "\n",
    "Maintain multiple reasoning paths, evaluate each, prune unpromising branches (like beam search but for reasoning).\n",
    "\n",
    "---\n",
    "\n",
    "## **15.6 Retrieval-Augmented Generation (RAG)**\n",
    "\n",
    "Ground LLMs in external knowledge to reduce hallucinations and provide source attribution.\n",
    "\n",
    "#### **15.6.1 Architecture**\n",
    "\n",
    "1. **Indexing:** Chunk documents, embed with model (OpenAI text-embedding-ada-002 or BGE, E5), store in vector DB\n",
    "2. **Retrieval:** Embed query, find top-k similar chunks (cosine similarity, MIP)\n",
    "3. **Generation:** Concatenate retrieved context with query, generate answer\n",
    "\n",
    "```python\n",
    "# Simplified RAG pipeline\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import faiss\n",
    "\n",
    "# 1. Indexing\n",
    "encoder = SentenceTransformer('BAAI/bge-large-en-v1.5')\n",
    "documents = load_docs()\n",
    "embeddings = encoder.encode(documents)\n",
    "\n",
    "index = faiss.IndexFlatIP(embeddings.shape[1])  # Inner product = cosine for normalized vectors\n",
    "index.add(embeddings)\n",
    "\n",
    "# 2. Retrieval\n",
    "query = \"What are the leave policies?\"\n",
    "query_emb = encoder.encode([query])\n",
    "D, I = index.search(query_emb, k=3)  # Top 3 chunks\n",
    "retrieved_docs = [documents[i] for i in I[0]]\n",
    "\n",
    "# 3. Generation\n",
    "context = \"\\n\".join(retrieved_docs)\n",
    "prompt = f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\"\n",
    "response = llm.generate(prompt)\n",
    "```\n",
    "\n",
    "#### **15.6.2 Advanced RAG**\n",
    "\n",
    "- **Hybrid Search:** Combine vector similarity with keyword (BM25) using Reciprocal Rank Fusion\n",
    "- **Reranking:** Use cross-encoder to rerank retrieved chunks (more accurate than bi-encoder)\n",
    "- **Query Expansion:** Generate hypothetical answer embedding (HyDE) to improve retrieval\n",
    "- **Iterative RAG:** Generate, check if answer complete, retrieve more if needed\n",
    "\n",
    "---\n",
    "\n",
    "## **15.7 LLM Application Development**\n",
    "\n",
    "#### **15.7.1 LangChain / LlamaIndex Frameworks**\n",
    "\n",
    "**LangChain Components:**\n",
    "- **Chains:** Sequences of calls (LLM, tool, LLM)\n",
    "- **Agents:** Dynamic chain that decides which tools to use\n",
    "- **Memory:** Conversation buffer, vector store memory\n",
    "- **Retrievers:** Interface to vector DBs\n",
    "\n",
    "```python\n",
    "from langchain import OpenAI, LLMChain, PromptTemplate\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "\n",
    "template = \"\"\"You are a helpful assistant.\n",
    "\n",
    "History: {history}\n",
    "Human: {input}\n",
    "Assistant:\"\"\"\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"history\", \"input\"],\n",
    "    template=template\n",
    ")\n",
    "\n",
    "memory = ConversationBufferMemory()\n",
    "llm = OpenAI(temperature=0)\n",
    "chain = LLMChain(llm=llm, prompt=prompt, memory=memory)\n",
    "\n",
    "chain.predict(input=\"Hi there!\")\n",
    "```\n",
    "\n",
    "#### **15.7.2 Efficient Serving**\n",
    "\n",
    "**Quantization:**\n",
    "- **GPTQ:** 4-bit quantization, suitable for inference (slight quality loss)\n",
    "- **AWQ:** Activation-aware quantization (better than GPTQ for same bits)\n",
    "- **GGUF/llama.cpp:** CPU inference, optimized for consumer hardware\n",
    "\n",
    "**vLLM:**\n",
    "PagedAttention algorithm for throughput serving (continuous batching).\n",
    "\n",
    "```bash\n",
    "# Serve with vLLM\n",
    "python -m vllm.entrypoints.openai.api_server \\\n",
    "    --model meta-llama/Llama-2-7b-hf \\\n",
    "    --tensor-parallel-size 1 \\\n",
    "    --quantization awq\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **15.8 Workbook Labs**\n",
    "\n",
    "### **Lab 1: QLoRA Fine-tuning**\n",
    "Fine-tune Llama-2-7B on custom dataset using QLoRA (4-bit quantization + LoRA):\n",
    "1. Load model in 4-bit (bitsandbytes)\n",
    "2. Add LoRA adapters (rank 64)\n",
    "3. Train on instruction dataset (Alpaca format)\n",
    "4. Merge adapters and evaluate vs base model\n",
    "\n",
    "**Deliverable:** Fine-tuned model capable of domain-specific instruction following (e.g., medical QA).\n",
    "\n",
    "### **Lab 2: RAG Pipeline**\n",
    "Build RAG for technical documentation:\n",
    "1. Chunk markdown docs with overlap\n",
    "2. Embed using BGE-large\n",
    "3. FAISS index with HNSW (fast approximate search)\n",
    "4. Evaluate retrieval accuracy (hit rate @k)\n",
    "5. Compare generation with vs without retrieval (hallucination reduction)\n",
    "\n",
    "**Deliverable:** Working RAG system with evaluation metrics.\n",
    "\n",
    "### **Lab 3: RLHF Simulation**\n",
    "Implement DPO from scratch (simplified):\n",
    "1. Create synthetic preference dataset (pairs of good/bad responses)\n",
    "2. Implement DPO loss\n",
    "3. Fine-tune small GPT-2 (125M) using DPO\n",
    "4. Show win rate improvement vs SFT baseline\n",
    "\n",
    "**Deliverable:** DPO training script and preference learning curves.\n",
    "\n",
    "### **Lab 4: Multi-Agent System**\n",
    "Build ReAct agent with tool use:\n",
    "1. Tools: Calculator, Wikipedia search, Weather API\n",
    "2. Agent loop: Thought \u2192 Action \u2192 Observation\n",
    "3. Handle tool errors gracefully\n",
    "4. Evaluate on multi-hop questions requiring tool chaining\n",
    "\n",
    "**Deliverable:** Agent that answers \"What is the temperature in the capital of France?\" by calling tools in sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## **15.9 Common Pitfalls**\n",
    "\n",
    "1. **Context Length Exceedance:** Sending 10k tokens to 4k context model causes silent truncation or errors. Always check tokenizer length.\n",
    "\n",
    "2. **Prompt Injection:** User input like \"Ignore previous instructions and...\" can hijack system prompts. Use input sanitization and delimiters.\n",
    "\n",
    "3. **Temperature=0 Non-Determinism:** Even with temperature 0, GPU operations can have slight non-determinism. For reproducibility, set seeds and use CPU if critical.\n",
    "\n",
    "4. **Hallucinations in RAG:** Retrieved context helps but doesn't eliminate hallucinations. Always verify critical facts.\n",
    "\n",
    "5. **Fine-tuning Catastrophic Forgetting:** Training on narrow domain makes model forget general knowledge. Use lower learning rates and mix with general instruction data.\n",
    "\n",
    "---\n",
    "\n",
    "## **15.10 Interview Questions**\n",
    "\n",
    "**Q1:** Explain the difference between ZeRO-2 and ZeRO-3 in DeepSpeed.\n",
    "*A: ZeRO-2 partitions optimizer states and gradients across data parallel processes, but keeps full parameters on each GPU. ZeRO-3 also partitions the model parameters, so each GPU only holds a slice of parameters. When needed, parameters are gathered via all-gather communication. ZeRO-3 enables training much larger models (trillion parameters across GPUs) but with more communication overhead.*\n",
    "\n",
    "**Q2:** What is the \"Chinchilla optimal\" training regime and why did it change LLM training practices?\n",
    "*A: Kaplan scaling laws suggested model size should grow faster than data (train large models on ~300B tokens). Chinchilla showed compute is optimally allocated when model size and training tokens scale equally (~20 tokens per parameter). This means smaller models trained on more data (e.g., 70B on 2T tokens) outperform larger undertrained models (175B on 300B tokens) for same compute, shifting focus to data quality and longer training.*\n",
    "\n",
    "**Q3:** Compare RLHF with DPO. When would you choose one over the other?\n",
    "*A: RLHF trains a separate reward model then uses PPO to optimize policy against it. Complex, unstable (KL divergence tuning), but flexible for complex reward functions. DPO optimizes directly on preference data using classification loss (implicit reward). Simpler, more stable, no reward model needed, often performs as well or better. Choose DPO for simplicity and stability; RLHF if you need explicit reward model for inspection or complex multi-objective rewards.*\n",
    "\n",
    "**Q4:** How does FlashAttention reduce memory usage from $O(N^2)$ to $O(N)$?\n",
    "*A: Standard attention materializes the full $N \\times N$ attention matrix in HBM (high bandwidth memory). FlashAttention uses tiling to compute attention in blocks that fit in fast SRAM (on-chip memory). It computes softmax incrementally (online softmax) without storing the full matrix, and only writes the final output to HBM. This reduces HBM accesses from quadratic to linear in sequence length.*\n",
    "\n",
    "**Q5:** What is the \"reversal curse\" in LLMs and how does it relate to training?\n",
    "*A: Models trained on \"A is B\" often fail to answer \"B is A\" (e.g., knowing \"Olaf Scholz is Chancellor of Germany\" but failing \"Who is Chancellor of Germany? \u2192 Olaf Scholz\"). Arises because auto-regressive models see ordered sequences during training; the reverse order is rare. Solution: bidirectional training or ensuring question-answer pairs appear in both directions in training data.*\n",
    "\n",
    "---\n",
    "\n",
    "## **15.11 Further Reading**\n",
    "\n",
    "**Papers:**\n",
    "- \"Scaling Laws for Neural Language Models\" (Kaplan et al., 2020)\n",
    "- \"Training Compute-Optimal Large Language Models\" (Hoffmann et al., 2022 - Chinchilla)\n",
    "- \"Llama 2: Open Foundation and Fine-Tuned Chat Models\" (Touvron et al., 2023)\n",
    "- \"Direct Preference Optimization\" (Rafailov et al., 2023)\n",
    "- \"FlashAttention: Fast and Memory-Efficient Exact Attention\" (Dao et al., 2022)\n",
    "\n",
    "**Resources:**\n",
    "- DeepSpeed Documentation: https://www.deepspeed.ai/\n",
    "- Hugging Face PEFT: Parameter-Efficient Fine-Tuning\n",
    "- \"Building LLM Applications\" by Chip Huyen\n",
    "\n",
    "---\n",
    "\n",
    "## **15.12 Checkpoint Project: Production LLM Chatbot**\n",
    "\n",
    "Build a domain-specific AI assistant (e.g., legal, medical, or technical support) deployable at scale.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Base Model:** LLaMA-2-13B or Mistral-7B (open source)\n",
    "\n",
    "2. **Fine-tuning:**\n",
    "   - Prepare 10k+ instruction-response pairs in domain\n",
    "   - Use QLoRA (4-bit) to fit on single A100 40GB\n",
    "   - DPO training on preference pairs (human-ranked responses)\n",
    "\n",
    "3. **RAG Integration:**\n",
    "   - Ingest 1000+ domain documents (PDFs, markdown)\n",
    "   - Chunk with semantic boundaries (not just fixed size)\n",
    "   - Hybrid retrieval: Dense (embeddings) + Sparse (BM25)\n",
    "   - Rerank with cross-encoder\n",
    "\n",
    "4. **Safety & Alignment:**\n",
    "   - System prompt with safety guidelines\n",
    "   - Content moderation filter (toxicity classifier)\n",
    "   - Refusal training for out-of-scope queries\n",
    "\n",
    "5. **Deployment:**\n",
    "   - vLLM serving with continuous batching\n",
    "   - Quantization to AWQ 4-bit (reduce VRAM, increase throughput)\n",
    "   - REST API with rate limiting and request validation\n",
    "   - Streaming responses (Server-Sent Events)\n",
    "\n",
    "6. **Evaluation:**\n",
    "   - Benchmark against GPT-3.5 on domain-specific questions\n",
    "   - Human evaluation (helpfulness, accuracy, safety) on 100 conversations\n",
    "   - Latency: p95 < 500ms for 500 token generation\n",
    "\n",
    "**Deliverables:**\n",
    "- `llm_chatbot/` repository with training, RAG, and serving code\n",
    "- Docker Compose setup: vLLM server + Vector DB (Chroma/Weaviate) + API gateway\n",
    "- Evaluation report: \"Model achieves 85% accuracy vs GPT-3.5's 90%, but 5x cheaper to run\"\n",
    "- Demo video showing multi-turn conversation with RAG grounding\n",
    "\n",
    "**Success Criteria:**\n",
    "- Handles 10 concurrent users with <2s latency\n",
    "- Cites sources for factual claims (RAG attribution)\n",
    "- Refuses harmful requests appropriately\n",
    "- Maintains context across 5+ turn conversations\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 15**\n",
    "\n",
    "*You now master the engineering of Large Language Models. Chapter 16 will cover Computer Vision Advanced \u2014 Vision Transformers, Diffusion Models, and Multimodal AI.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../3. Deep_learning_and_neural_networks.ipynb/14. transformers_and_modern_nlp.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='16. computer_vision_advanced.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}