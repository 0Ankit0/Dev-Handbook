{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e78c3d87",
   "metadata": {},
   "source": [
    "Here is **Chapter 17: Reinforcement Learning (RL)** — teaching machines to make decisions.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 17: REINFORCEMENT LEARNING (RL)**\n",
    "\n",
    "*Learning Through Interaction*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Reinforcement Learning is the paradigm of learning through trial and error, maximizing cumulative reward in an environment. From mastering games like Go and StarCraft to powering the alignment of Large Language Models via RLHF, RL enables autonomous decision-making in complex, dynamic environments. This chapter builds from Markov Decision Processes to state-of-the-art algorithms like PPO and SAC, culminating in the RLHF techniques that align modern AI systems with human intent.\n",
    "\n",
    "**Estimated Time:** 60-70 hours (4-5 weeks)  \n",
    "**Prerequisites:** Chapters 10-11 (Neural networks, Optimization), Chapter 15 (LLMs, for RLHF connection)\n",
    "\n",
    "---\n",
    "\n",
    "## **17.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Formulate problems as Markov Decision Processes (MDPs) and derive Bellman equations for optimality\n",
    "2. Implement value-based methods (DQN, Double DQN, Dueling DQN) with experience replay and target networks\n",
    "3. Apply policy gradient methods (REINFORCE, Actor-Critic) and understand the bias-variance tradeoff in gradients\n",
    "4. Train agents using Proximal Policy Optimization (PPO), the industry standard for continuous control and LLM alignment\n",
    "5. Understand Model-Based RL and planning with learned dynamics\n",
    "6. Implement Multi-Agent RL systems for competitive and cooperative scenarios\n",
    "7. Apply RLHF (Reinforcement Learning from Human Feedback) to align language models with human preferences\n",
    "\n",
    "---\n",
    "\n",
    "## **17.1 Foundations: The Markov Decision Process**\n",
    "\n",
    "#### **17.1.1 Formal Definition**\n",
    "\n",
    "An MDP is defined by the tuple $(\\mathcal{S}, \\mathcal{A}, \\mathcal{P}, \\mathcal{R}, \\gamma)$:\n",
    "\n",
    "- **$\\mathcal{S}$:** Set of states (observations of the environment)\n",
    "- **$\\mathcal{A}$:** Set of actions available to the agent\n",
    "- **$\\mathcal{P}(s'|s,a)$:** Transition dynamics (probability of next state given current state and action)\n",
    "- **$\\mathcal{R}(s,a,s')$:** Reward function (immediate feedback)\n",
    "- **$\\gamma \\in [0,1]$:** Discount factor (importance of future rewards vs immediate)\n",
    "\n",
    "**Markov Property:** The future is independent of the past given the present: $P(s_{t+1}|s_t, a_t) = P(s_{t+1}|s_0, a_0, ..., s_t, a_t)$.\n",
    "\n",
    "#### **17.1.2 Policies and Value Functions**\n",
    "\n",
    "**Policy $\\pi(a|s)$:** Probability distribution over actions given state. Can be deterministic ($a = \\pi(s)$) or stochastic.\n",
    "\n",
    "**State Value Function $V^\\pi(s)$:** Expected cumulative reward starting from state $s$ and following policy $\\pi$:\n",
    "$$V^\\pi(s) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s\\right]$$\n",
    "\n",
    "**Action Value Function (Q-Function) $Q^\\pi(s,a)$:** Expected cumulative reward starting from state $s$, taking action $a$, then following $\\pi$:\n",
    "$$Q^\\pi(s,a) = \\mathbb{E}_\\pi\\left[\\sum_{t=0}^{\\infty} \\gamma^t r_t \\mid s_0 = s, a_0 = a\\right]$$\n",
    "\n",
    "**Relationship:** $V^\\pi(s) = \\sum_a \\pi(a|s) Q^\\pi(s,a)$\n",
    "\n",
    "#### **17.1.3 Bellman Equations**\n",
    "\n",
    "The recursive structure of value functions:\n",
    "\n",
    "**Bellman Expectation Equation:**\n",
    "$$V^\\pi(s) = \\sum_a \\pi(a|s) \\sum_{s'} \\mathcal{P}(s'|s,a) [\\mathcal{R}(s,a,s') + \\gamma V^\\pi(s')]$$\n",
    "\n",
    "**Bellman Optimality Equation:**\n",
    "$$V^*(s) = \\max_a \\sum_{s'} \\mathcal{P}(s'|s,a) [\\mathcal{R}(s,a,s') + \\gamma V^*(s')]$$\n",
    "\n",
    "$$Q^*(s,a) = \\sum_{s'} \\mathcal{P}(s'|s,a) [\\mathcal{R}(s,a,s') + \\gamma \\max_{a'} Q^*(s',a')]$$\n",
    "\n",
    "The optimal policy $\\pi^*$ is greedy with respect to $Q^*$: $\\pi^*(s) = \\arg\\max_a Q^*(s,a)$.\n",
    "\n",
    "---\n",
    "\n",
    "## **17.2 Model-Free Value-Based Methods**\n",
    "\n",
    "When the transition dynamics $\\mathcal{P}$ are unknown, we learn from experience.\n",
    "\n",
    "#### **17.2.1 Q-Learning (Off-Policy)**\n",
    "\n",
    "Update Q-values using temporal difference (TD) learning:\n",
    "\n",
    "$$Q(s,a) \\leftarrow Q(s,a) + \\alpha [r + \\gamma \\max_{a'} Q(s',a') - Q(s,a)]$$\n",
    "\n",
    "**Off-policy:** Can learn about optimal policy while following exploratory policy (e.g., $\\epsilon$-greedy).\n",
    "\n",
    "**Exploration vs Exploitation:**\n",
    "- **$\\epsilon$-greedy:** With probability $\\epsilon$ take random action, else greedy.\n",
    "- **Boltzmann:** Sample from softmax of Q-values: $\\pi(a|s) \\propto \\exp(Q(s,a)/\\tau)$\n",
    "\n",
    "#### **17.2.2 Deep Q-Network (DQN)**\n",
    "\n",
    "Use neural network $Q(s,a; \\theta)$ to approximate Q-function for high-dimensional state spaces (e.g., pixels).\n",
    "\n",
    "**Key Innovations:**\n",
    "1. **Experience Replay:** Store transitions $(s,a,r,s')$ in buffer, sample random mini-batches to break correlation.\n",
    "2. **Target Network:** Separate network $Q(s',a'; \\theta^-)$ for computing target values, updated periodically to stabilize learning.\n",
    "\n",
    "**Loss Function:**\n",
    "$$\\mathcal{L}(\\theta) = \\mathbb{E}_{(s,a,r,s') \\sim \\mathcal{D}} \\left[(r + \\gamma \\max_{a'} Q(s',a'; \\theta^-) - Q(s,a; \\theta))^2\\right]$$\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import random\n",
    "\n",
    "class DQN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, output_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "class ReplayBuffer:\n",
    "    def __init__(self, capacity=10000):\n",
    "        self.buffer = deque(maxlen=capacity)\n",
    "    \n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        self.buffer.append((state, action, reward, next_state, done))\n",
    "    \n",
    "    def sample(self, batch_size):\n",
    "        batch = random.sample(self.buffer, batch_size)\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "        return (np.array(states), np.array(actions), np.array(rewards), \n",
    "                np.array(next_states), np.array(dones))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_dim, action_dim, lr=1e-3, gamma=0.99):\n",
    "        self.policy_net = DQN(state_dim, action_dim)\n",
    "        self.target_net = DQN(state_dim, action_dim)\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.policy_net.parameters(), lr=lr)\n",
    "        self.buffer = ReplayBuffer()\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = 1.0\n",
    "        self.epsilon_decay = 0.995\n",
    "        self.epsilon_min = 0.01\n",
    "        self.action_dim = action_dim\n",
    "        \n",
    "    def select_action(self, state):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randrange(self.action_dim)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            state = torch.FloatTensor(state).unsqueeze(0)\n",
    "            q_values = self.policy_net(state)\n",
    "            return q_values.argmax().item()\n",
    "    \n",
    "    def train(self, batch_size=64):\n",
    "        if len(self.buffer) < batch_size:\n",
    "            return\n",
    "        \n",
    "        states, actions, rewards, next_states, dones = self.buffer.sample(batch_size)\n",
    "        \n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        rewards = torch.FloatTensor(rewards)\n",
    "        next_states = torch.FloatTensor(next_states)\n",
    "        dones = torch.FloatTensor(dones)\n",
    "        \n",
    "        # Current Q values\n",
    "        current_q = self.policy_net(states).gather(1, actions.unsqueeze(1)).squeeze()\n",
    "        \n",
    "        # Target Q values (Double DQN style)\n",
    "        with torch.no_grad():\n",
    "            next_actions = self.policy_net(next_states).argmax(1)\n",
    "            next_q = self.target_net(next_states).gather(1, next_actions.unsqueeze(1)).squeeze()\n",
    "            target_q = rewards + (1 - dones) * self.gamma * next_q\n",
    "        \n",
    "        # Huber loss (smooth L1) for stability\n",
    "        loss = nn.functional.smooth_l1_loss(current_q, target_q)\n",
    "        \n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        # Gradient clipping\n",
    "        torch.nn.utils.clip_grad_norm_(self.policy_net.parameters(), max_norm=1.0)\n",
    "        self.optimizer.step()\n",
    "        \n",
    "        # Decay epsilon\n",
    "        self.epsilon = max(self.epsilon_min, self.epsilon * self.epsilon_decay)\n",
    "    \n",
    "    def update_target(self):\n",
    "        self.target_net.load_state_dict(self.policy_net.state_dict())\n",
    "```\n",
    "\n",
    "**Improvements:**\n",
    "- **Double DQN:** Decouples action selection and evaluation to reduce overestimation bias.\n",
    "- **Dueling DQN:** Separates value and advantage streams: $Q(s,a) = V(s) + A(s,a) - \\frac{1}{|\\mathcal{A}|}\\sum_{a'} A(s,a')$.\n",
    "- **Prioritized Replay:** Sample transitions with higher TD error more frequently.\n",
    "\n",
    "---\n",
    "\n",
    "## **17.3 Policy Gradient Methods**\n",
    "\n",
    "Instead of learning value functions, directly parameterize and optimize the policy $\\pi_\\theta(a|s)$.\n",
    "\n",
    "#### **17.3.1 REINFORCE (Monte-Carlo Policy Gradient)**\n",
    "\n",
    "The policy gradient theorem:\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot G_t]$$\n",
    "\n",
    "Where $G_t = \\sum_{k=0}^{\\infty} \\gamma^k r_{t+k}$ is the return from time $t$.\n",
    "\n",
    "**Intuition:** Increase probability of actions that lead to high returns, decrease for low returns.\n",
    "\n",
    "**High Variance:** Uses Monte-Carlo returns (no bootstrapping), leading to noisy gradients.\n",
    "\n",
    "#### **17.3.2 Actor-Critic Methods**\n",
    "\n",
    "Combine policy gradient (Actor) with value function approximation (Critic) to reduce variance.\n",
    "\n",
    "**Advantage Function:** $A(s,a) = Q(s,a) - V(s)$ (how much better is action $a$ than average).\n",
    "\n",
    "**Gradient:**\n",
    "$$\\nabla_\\theta J(\\theta) = \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(a|s) \\cdot A(s,a)]$$\n",
    "\n",
    "**A2C (Advantage Actor-Critic):** Synchronous update. Both networks share layers often.\n",
    "\n",
    "```python\n",
    "class ActorCritic(nn.Module):\n",
    "    def __init__(self, state_dim, action_dim):\n",
    "        super().__init__()\n",
    "        self.shared = nn.Sequential(\n",
    "            nn.Linear(state_dim, 128),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.actor = nn.Linear(128, action_dim)  # Policy head\n",
    "        self.critic = nn.Linear(128, 1)          # Value head\n",
    "    \n",
    "    def forward(self, state):\n",
    "        x = self.shared(state)\n",
    "        logits = self.actor(x)\n",
    "        value = self.critic(x)\n",
    "        return logits, value\n",
    "    \n",
    "    def get_action(self, state):\n",
    "        logits, value = self.forward(state)\n",
    "        dist = torch.distributions.Categorical(logits=logits)\n",
    "        action = dist.sample()\n",
    "        return action, dist.log_prob(action), value\n",
    "\n",
    "# Training step\n",
    "def train_step(self, states, actions, log_probs, rewards, next_states, dones):\n",
    "    # Compute returns and advantages\n",
    "    returns = []\n",
    "    advantages = []\n",
    "    gae = 0  # Generalized Advantage Estimation\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        _, next_values = self.model(next_states)\n",
    "    \n",
    "    # Compute GAE (Generalized Advantage Estimation)\n",
    "    for t in reversed(range(len(rewards))):\n",
    "        if t == len(rewards) - 1:\n",
    "            next_value = next_values[t] * (1 - dones[t])\n",
    "        else:\n",
    "            next_value = values[t+1]\n",
    "        \n",
    "        delta = rewards[t] + self.gamma * next_value - values[t]\n",
    "        gae = delta + self.gamma * self.gae_lambda * gae * (1 - dones[t])\n",
    "        advantages.insert(0, gae)\n",
    "        returns.insert(0, gae + values[t])\n",
    "    \n",
    "    # Normalize advantages\n",
    "    advantages = torch.tensor(advantages)\n",
    "    advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "    \n",
    "    # Update actor and critic\n",
    "    logits, values = self.model(states)\n",
    "    dist = torch.distributions.Categorical(logits=logits)\n",
    "    \n",
    "    new_log_probs = dist.log_prob(actions)\n",
    "    entropy = dist.entropy().mean()\n",
    "    \n",
    "    # Actor loss (policy gradient)\n",
    "    policy_loss = -(new_log_probs * advantages).mean()\n",
    "    \n",
    "    # Critic loss (value function)\n",
    "    value_loss = nn.functional.mse_loss(values.squeeze(), torch.tensor(returns))\n",
    "    \n",
    "    loss = policy_loss + 0.5 * value_loss - 0.01 * entropy  # Entropy bonus for exploration\n",
    "    \n",
    "    self.optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    self.optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **17.4 Proximal Policy Optimization (PPO)**\n",
    "\n",
    "The current industry standard for continuous control and LLM alignment (RLHF). Addresses instability in vanilla policy gradients.\n",
    "\n",
    "**Problem:** Large policy updates can collapse performance.\n",
    "**Solution:** Clipped surrogate objective to prevent large changes.\n",
    "\n",
    "**Surrogate Objective:**\n",
    "$$L^{CLIP}(\\theta) = \\mathbb{E}_t\\left[\\min\\left(r_t(\\theta)\\hat{A}_t, \\text{clip}(r_t(\\theta), 1-\\epsilon, 1+\\epsilon)\\hat{A}_t\\right)\\right]$$\n",
    "\n",
    "Where $r_t(\\theta) = \\frac{\\pi_\\theta(a_t|s_t)}{\\pi_{\\theta_{old}}(a_t|s_t)}$ (probability ratio).\n",
    "\n",
    "If advantage is positive, don't increase probability by more than $\\epsilon$. If negative, don't decrease by more than $\\epsilon$.\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "class PPO:\n",
    "    def __init__(self, state_dim, action_dim, lr=3e-4, gamma=0.99, gae_lambda=0.95, \n",
    "                 clip_epsilon=0.2, epochs=4, batch_size=64):\n",
    "        self.policy = ActorCritic(state_dim, action_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.policy.parameters(), lr=lr)\n",
    "        self.gamma = gamma\n",
    "        self.gae_lambda = gae_lambda\n",
    "        self.clip_epsilon = clip_epsilon\n",
    "        self.epochs = epochs\n",
    "        self.batch_size = batch_size\n",
    "        \n",
    "    def compute_gae(self, rewards, values, next_values, dones):\n",
    "        advantages = []\n",
    "        gae = 0\n",
    "        for t in reversed(range(len(rewards))):\n",
    "            if t == len(rewards) - 1:\n",
    "                next_val = next_values[t]\n",
    "            else:\n",
    "                next_val = values[t+1]\n",
    "            \n",
    "            delta = rewards[t] + self.gamma * next_val * (1-dones[t]) - values[t]\n",
    "            gae = delta + self.gamma * self.gae_lambda * (1-dones[t]) * gae\n",
    "            advantages.insert(0, gae)\n",
    "        return torch.tensor(advantages, dtype=torch.float32)\n",
    "    \n",
    "    def update(self, states, actions, old_log_probs, rewards, next_states, dones):\n",
    "        # Convert to tensors\n",
    "        states = torch.FloatTensor(states)\n",
    "        actions = torch.LongTensor(actions)\n",
    "        old_log_probs = torch.FloatTensor(old_log_probs)\n",
    "        \n",
    "        # Get values and next_values\n",
    "        with torch.no_grad():\n",
    "            _, values = self.policy(states)\n",
    "            _, next_values = self.policy(torch.FloatTensor(next_states))\n",
    "            values = values.squeeze()\n",
    "            next_values = next_values.squeeze()\n",
    "        \n",
    "        # Compute advantages using GAE\n",
    "        advantages = self.compute_gae(rewards, values.numpy(), next_values.numpy(), dones)\n",
    "        returns = advantages + values\n",
    "        \n",
    "        # Normalize advantages\n",
    "        advantages = (advantages - advantages.mean()) / (advantages.std() + 1e-8)\n",
    "        \n",
    "        # PPO epochs\n",
    "        dataset_size = len(states)\n",
    "        indices = np.arange(dataset_size)\n",
    "        \n",
    "        for _ in range(self.epochs):\n",
    "            np.random.shuffle(indices)\n",
    "            \n",
    "            for start in range(0, dataset_size, self.batch_size):\n",
    "                end = start + self.batch_size\n",
    "                batch_idx = indices[start:end]\n",
    "                \n",
    "                batch_states = states[batch_idx]\n",
    "                batch_actions = actions[batch_idx]\n",
    "                batch_old_log_probs = old_log_probs[batch_idx]\n",
    "                batch_advantages = advantages[batch_idx]\n",
    "                batch_returns = returns[batch_idx]\n",
    "                \n",
    "                # Evaluate current policy\n",
    "                logits, curr_values = self.policy(batch_states)\n",
    "                dist = torch.distributions.Categorical(logits=logits)\n",
    "                curr_log_probs = dist.log_prob(batch_actions)\n",
    "                entropy = dist.entropy().mean()\n",
    "                \n",
    "                # Probability ratio\n",
    "                ratio = torch.exp(curr_log_probs - batch_old_log_probs)\n",
    "                \n",
    "                # Clipped surrogate loss\n",
    "                surr1 = ratio * batch_advantages\n",
    "                surr2 = torch.clamp(ratio, 1-self.clip_epsilon, 1+self.clip_epsilon) * batch_advantages\n",
    "                actor_loss = -torch.min(surr1, surr2).mean()\n",
    "                \n",
    "                # Value loss\n",
    "                critic_loss = nn.functional.mse_loss(curr_values.squeeze(), batch_returns)\n",
    "                \n",
    "                # Total loss\n",
    "                loss = actor_loss + 0.5 * critic_loss - 0.01 * entropy\n",
    "                \n",
    "                self.optimizer.zero_grad()\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(self.policy.parameters(), 0.5)\n",
    "                self.optimizer.step()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **17.5 Model-Based RL (Brief Overview)**\n",
    "\n",
    "Learn a model of the environment dynamics $\\hat{P}(s'|s,a)$ and $\\hat{R}(s,a)$, then plan.\n",
    "\n",
    "**Methods:**\n",
    "- **Dyna-Q:** Mix real experience with simulated experience from learned model\n",
    "- **MPC (Model Predictive Control):** Plan action sequences using learned model, execute first action, replan\n",
    "- **MuZero:** Combines model-based planning with model-free learning (used in AlphaZero)\n",
    "\n",
    "**Trade-off:** Sample efficient (less environment interaction) but compounding model errors.\n",
    "\n",
    "---\n",
    "\n",
    "## **17.6 Multi-Agent RL**\n",
    "\n",
    "Multiple agents interact in shared environment. Can be:\n",
    "- **Cooperative:** Shared reward (team success)\n",
    "- **Competitive:** Zero-sum (one wins, one loses)\n",
    "- **Mixed:** General sum (prisoner's dilemma)\n",
    "\n",
    "**Challenges:**\n",
    "- Non-stationarity: Other agents are part of the environment, constantly changing\n",
    "- Credit assignment: Which agent caused success/failure?\n",
    "\n",
    "**Algorithms:**\n",
    "- **MADDPG:** Multi-Agent DDPG (centralized training, decentralized execution)\n",
    "- **QMIX:** Value factorization for cooperative agents $Q_{tot} = f(Q_1, Q_2, ..., Q_n)$\n",
    "\n",
    "---\n",
    "\n",
    "## **17.7 Reinforcement Learning from Human Feedback (RLHF)**\n",
    "\n",
    "The technique behind ChatGPT, Claude, and aligned LLMs.\n",
    "\n",
    "#### **17.7.1 The Pipeline (Review and Expansion from Chapter 15)**\n",
    "\n",
    "1. **Supervised Fine-Tuning (SFT):** Train on high-quality human demonstrations (instruction-following).\n",
    "2. **Reward Modeling (RM):** Train model to predict human preferences $r_\\theta(x,y)$.\n",
    "   - Data: Comparisons $(x, y_w, y_l)$ where $y_w$ is preferred over $y_l$\n",
    "   - Loss: $-\\log \\sigma(r_\\theta(x, y_w) - r_\\theta(x, y_l))$ (Bradley-Terry model)\n",
    "3. **RL Optimization:** Use PPO to maximize expected reward while staying close to SFT policy (KL penalty).\n",
    "\n",
    "**The RLHF Objective:**\n",
    "$$\\max_{\\pi_\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y|x)} [r_\\phi(x,y)] - \\beta \\mathbb{D}_{KL}(\\pi_\\theta(y|x) || \\pi_{SFT}(y|x))$$\n",
    "\n",
    "#### **17.7.2 PPO for Language Models**\n",
    "\n",
    "When applying PPO to LLMs:\n",
    "- **State:** Context/prompt $x$ and tokens generated so far $y_{<t}$\n",
    "- **Action:** Next token $y_t$ (vocabulary size ~50k)\n",
    "- **Policy:** The language model $\\pi_\\theta(y_t|x, y_{<t})$\n",
    "- **Reward:** Reward model score at end of sequence + KL penalty per token\n",
    "\n",
    "**Important Implementation Details:**\n",
    "- **Reward Hacking:** Reward models can be exploited (generate high reward but gibberish). Mitigate with KL penalty and diverse sampling.\n",
    "- **Token-Level Rewards:** Since reward is only at sequence end, use value function to estimate future returns for intermediate tokens.\n",
    "\n",
    "```python\n",
    "# Simplified RLHF PPO for LLMs (conceptual)\n",
    "def compute_rewards(sequences, reward_model, sft_model, kl_coeff=0.1):\n",
    "    \"\"\"\n",
    "    sequences: token IDs of generated text\n",
    "    \"\"\"\n",
    "    with torch.no_grad():\n",
    "        # End-of-sequence reward from RM\n",
    "        rm_scores = reward_model(sequences)\n",
    "        \n",
    "        # KL divergence penalty per token\n",
    "        sft_logits = sft_model(sequences)\n",
    "        policy_logits = policy_model(sequences)\n",
    "        \n",
    "        kl_div = torch.sum(\n",
    "            torch.softmax(policy_logits, dim=-1) * \n",
    "            (torch.log_softmax(policy_logits, dim=-1) - torch.log_softmax(sft_logits, dim=-1)),\n",
    "            dim=-1\n",
    "        )\n",
    "    \n",
    "    # Combined reward: RM score at end - KL penalty for each token\n",
    "    rewards = -kl_coeff * kl_div\n",
    "    rewards[:, -1] += rm_scores  # Add RM score to last token\n",
    "    \n",
    "    return rewards\n",
    "```\n",
    "\n",
    "#### **17.7.3 Direct Preference Optimization (DPO)**\n",
    "\n",
    "As mentioned in Chapter 15, DPO bypasses explicit reward modeling and PPO by deriving optimal policy directly from preference data.\n",
    "\n",
    "**Loss:**\n",
    "$$\\mathcal{L}_{DPO} = -\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)}\\right)$$\n",
    "\n",
    "**Advantages:** Simpler, more stable, often performs as well as PPO.\n",
    "\n",
    "---\n",
    "\n",
    "## **17.8 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Q-Learning from Scratch**\n",
    "Implement tabular Q-learning on FrozenLake (OpenAI Gym):\n",
    "1. Epsilon-greedy exploration with decay\n",
    "2. Q-table updates\n",
    "3. Solve 4x4 and 8x8 maps\n",
    "4. Visualize value function and policy\n",
    "\n",
    "**Deliverable:** Solved environment with visualization of learned Q-values.\n",
    "\n",
    "### **Lab 2: DQN on Atari**\n",
    "Train DQN on CartPole or Atari Pong:\n",
    "1. Experience replay with prioritized sampling\n",
    "2. Target network updates\n",
    "3. Frame stacking (for Atari)\n",
    "4. Plot reward curves and epsilon decay\n",
    "\n",
    "**Deliverable:** Training script achieving average reward > threshold for 100 episodes.\n",
    "\n",
    "### **Lab 3: PPO for Continuous Control**\n",
    "Use Stable Baselines3 or custom implementation for BipedalWalker or LunarLander:\n",
    "1. Actor-Critic with separate networks\n",
    "2. GAE for advantage estimation\n",
    "3. Multiple epochs per update\n",
    "4. Monitor KL divergence to detect too-large steps\n",
    "\n",
    "**Deliverable:** Agent successfully walking/landing with video recording.\n",
    "\n",
    "### **Lab 4: Multi-Agent Gridworld**\n",
    "Implement independent Q-learning vs MADDPG on simple pursuit-evasion or cooperative navigation:\n",
    "1. Centralized training with decentralized execution\n",
    "2. Experience sharing between agents\n",
    "3. Analysis of emergent behaviors\n",
    "\n",
    "**Deliverable:** Visualization of multi-agent coordination.\n",
    "\n",
    "---\n",
    "\n",
    "## **17.9 Common Pitfalls**\n",
    "\n",
    "1. **Reward Hacking:** Agent finds loophole in reward function (e.g., crashing quickly to avoid negative step penalty). Solution: Careful reward shaping, human oversight.\n",
    "\n",
    "2. **Catastrophic Forgetting:** In continual RL, forgetting old tasks when learning new ones. Solution: Elastic Weight Consolidation (EWC), progressive networks.\n",
    "\n",
    "3. **Exploration Collapse:** Policy becomes deterministic too early, stops exploring. Solution: Entropy regularization, noise injection (parameter space noise), count-based exploration.\n",
    "\n",
    "4. **Deadly Triad:** Function approximation + Bootstrapping + Off-policy learning can diverge (e.g., DQN with large learning rates). Solution: Target networks, gradient clipping, conservative updates.\n",
    "\n",
    "5. **Sample Inefficiency:** Model-free RL needs millions of steps. Solution: Model-based methods, demonstration data (RLHF), better replay buffers.\n",
    "\n",
    "---\n",
    "\n",
    "## **17.10 Interview Questions**\n",
    "\n",
    "**Q1:** What is the difference between on-policy and off-policy RL? Give examples of each.\n",
    "*A: On-policy learns about the policy currently being followed (must generate new experience after each update). Examples: REINFORCE, A2C, PPO. Off-policy can learn from experience generated by old policies or other policies (can reuse old data). Examples: Q-Learning, DQN, DDPG. Off-policy is more sample efficient but can be less stable; on-policy is more stable but sample inefficient.*\n",
    "\n",
    "**Q2:** Why does DQN use a target network, and how is it updated?\n",
    "*A: Without target networks, DQN chases its own tail: as parameters update, the target $r + \\gamma \\max Q(s',a'; \\theta)$ changes simultaneously with the current estimate $Q(s,a; \\theta)$, leading to oscillations or divergence. The target network $\\theta^-$ is a lagged copy of the policy network, updated periodically (hard update: copy every N steps) or smoothly (soft update: $\\theta^- \\leftarrow \\tau\\theta + (1-\\tau)\\theta^-$). This stabilizes learning by providing consistent targets.*\n",
    "\n",
    "**Q3:** Explain the purpose of the clipping objective in PPO. Why not use a simple trust region like TRPO?\n",
    "*A: PPO prevents policy updates that are too large (which can collapse performance) by clipping the probability ratio $r(\\theta)$ to $[1-\\epsilon, 1+\\epsilon]$. This penalizes changes that make actions much more/less probable than under the old policy. TRPO uses constrained optimization (KL divergence < $\\delta$) requiring second-order methods or conjugate gradient—complex and computationally expensive. PPO's clipped surrogate is a first-order approximation that's simpler to implement and often works as well or better.*\n",
    "\n",
    "**Q4:** What is the advantage function, and why is it used in Actor-Critic methods instead of raw returns?\n",
    "*A: Advantage $A(s,a) = Q(s,a) - V(s)$ measures how much better action $a$ is than the average action at state $s$. Subtracting the value baseline reduces variance in policy gradients (we only care about relative action quality, not absolute state value). Lower variance means more stable learning and faster convergence, though it introduces bias which is mitigated by good value function approximation.*\n",
    "\n",
    "**Q5:** How does RLHF prevent the policy from drifting too far from the original language model?\n",
    "*A: RLHF adds a KL divergence penalty $\\beta D_{KL}(\\pi_\\theta || \\pi_{SFT})$ to the reward function. This penalizes the RL policy for generating outputs with probability distributions too different from the supervised fine-tuned model. It prevents reward hacking (exploiting the reward model's weaknesses) and maintains language fluency/coherence from the base model. The $\\beta$ coefficient trades off alignment vs diversity.*\n",
    "\n",
    "---\n",
    "\n",
    "## **17.11 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Reinforcement Learning: An Introduction* (Sutton & Barto) - The bible, free PDF available\n",
    "- *Spinning Up in Deep RL* (OpenAI) - Practical guide with implementations\n",
    "\n",
    "**Papers:**\n",
    "- \"Human-level control through deep reinforcement learning\" (Mnih et al., 2015) - DQN\n",
    "- \"Proximal Policy Optimization Algorithms\" (Schulman et al., 2017) - PPO\n",
    "- \"Multi-Agent Actor-Critic for Mixed Cooperative-Competitive Environments\" (Lowe et al., 2017) - MADDPG\n",
    "- \"Training language models to follow instructions with human feedback\" (Ouyang et al., 2022) - InstructGPT/RLHF\n",
    "- \"Direct Preference Optimization\" (Rafailov et al., 2023) - DPO\n",
    "\n",
    "**Libraries:**\n",
    "- **Stable Baselines3:** Reliable implementations of DQN, PPO, A2C, SAC\n",
    "- **RLlib (Ray):** Scalable RL for multi-agent and distributed training\n",
    "- **CleanRL:** Single-file implementations for educational purposes\n",
    "\n",
    "---\n",
    "\n",
    "## **17.12 Checkpoint Project: Autonomous Trading Agent**\n",
    "\n",
    "Build an RL agent for simulated algorithmic trading (or use OpenAI Gym trading environment).\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Environment:**\n",
    "   - State: Price history (OHLCV), technical indicators (RSI, MACD), portfolio state (cash, position, PnL)\n",
    "   - Actions: Discrete [Buy, Sell, Hold] or continuous [position sizing -1 to 1]\n",
    "   - Reward: Sharpe ratio of returns, or PnL with risk penalty (drawdown penalty)\n",
    "\n",
    "2. **Algorithms:**\n",
    "   - Baseline: DQN with discretized actions\n",
    "   - Advanced: PPO with continuous action space for position sizing\n",
    "   - Compare against Buy-and-Hold and simple moving average crossover\n",
    "\n",
    "3. **Risk Management:**\n",
    "   - Stop-loss logic in environment (forced exit on large drawdown)\n",
    "   - Position limits (max leverage)\n",
    "   - Transaction cost modeling (slippage, fees)\n",
    "\n",
    "4. **Evaluation:**\n",
    "   - Backtest on unseen 6-month period\n",
    "   - Metrics: Total return, Sharpe ratio, max drawdown, Calmar ratio\n",
    "   - Visualization: Equity curve, drawdown periods, action distribution\n",
    "\n",
    "5. **Safety:**\n",
    "   - Sanity checks: No lookahead bias (only past data in state)\n",
    "   - Overfitting detection: Performance degradation from train to test\n",
    "\n",
    "**Deliverables:**\n",
    "- `trading_rl/` package with custom Gym environment and agents\n",
    "- Backtesting report with risk metrics\n",
    "- Analysis: \"Agent learned to avoid high-volatility periods, achieving 1.5 Sharpe ratio vs 0.8 buy-and-hold\"\n",
    "\n",
    "**Success Criteria:**\n",
    "- Positive returns on test set (out-of-sample)\n",
    "- Sharpe ratio > 1.0\n",
    "- Max drawdown < 20%\n",
    "- Interpretable policy (e.g., buys dips in uptrend)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 17**\n",
    "\n",
    "*You now master reinforcement learning from fundamentals to RLHF. Chapter 18 will cover Specialized Applications (Time Series, Recommendation Systems, Graph Neural Networks) — applying deep learning to specific domains.*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
