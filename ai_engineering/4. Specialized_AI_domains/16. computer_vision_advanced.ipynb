{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a5ca55a",
   "metadata": {},
   "source": [
    "Here is **Chapter 16: Computer Vision Advanced** — the convergence of vision and language, and the generative revolution.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 16: COMPUTER VISION ADVANCED**\n",
    "\n",
    "*Beyond Convolution*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "While CNNs dominated computer vision for a decade, the Transformer architecture has now conquered vision through Vision Transformers (ViT), enabling unprecedented scale and multimodal understanding. Simultaneously, generative models have evolved from GANs to Diffusion Models, enabling photorealistic image synthesis. This chapter bridges CNNs and Transformers, covers self-supervised learning at scale, and explores the multimodal frontier where vision meets language.\n",
    "\n",
    "**Estimated Time:** 60-70 hours (4-5 weeks)  \n",
    "**Prerequisites:** Chapters 12 (CNNs), 14 (Transformers), 15 (LLMs)\n",
    "\n",
    "---\n",
    "\n",
    "## **16.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement Vision Transformers (ViT) from scratch and understand patch embedding strategies\n",
    "2. Train self-supervised vision models using contrastive learning (SimCLR) and masked autoencoding (MAE)\n",
    "3. Build and train diffusion models for image generation, understanding the forward/reverse processes\n",
    "4. Implement and fine-tune multimodal vision-language models (CLIP, LLaVA) for zero-shot classification and retrieval\n",
    "5. Apply computer vision models to video understanding (temporal modeling, 3D convolutions)\n",
    "6. Optimize vision models for efficient deployment (knowledge distillation, patching strategies)\n",
    "\n",
    "---\n",
    "\n",
    "## **16.1 Vision Transformers (ViT)**\n",
    "\n",
    "#### **16.1.1 From CNNs to Transformers**\n",
    "\n",
    "CNNs excel at local feature extraction but struggle with global relationships without deep stacks. ViT applies Transformers directly to image patches.\n",
    "\n",
    "**Architecture:**\n",
    "1. **Patch Embedding:** Split image $x \\in \\mathbb{R}^{H \\times W \\times C}$ into patches $x_p \\in \\mathbb{R}^{N \\times (P^2 \\cdot C)}$\n",
    "   - $P$ = patch resolution (typically 16)\n",
    "   - $N = HW/P^2$ = number of patches (196 for 224×224 image with P=16)\n",
    "\n",
    "2. **Linear Projection:** Map each patch to embedding dimension $D$ using trainable matrix $\\mathbf{E} \\in \\mathbb{R}^{(P^2 \\cdot C) \\times D}$\n",
    "\n",
    "3. **Position Embeddings:** Add learned 1D position embeddings (or 2D sin-cos)\n",
    "\n",
    "4. **Transformer Encoder:** Standard Transformer blocks (Multi-head Self-Attention + MLP)\n",
    "\n",
    "5. **Classification Head:** Prepend learnable [CLS] token, use its final state for classification\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PatchEmbedding(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.img_size = img_size\n",
    "        self.patch_size = patch_size\n",
    "        self.n_patches = (img_size // patch_size) ** 2\n",
    "        \n",
    "        # Conv2d with stride = kernel_size is equivalent to patch extraction\n",
    "        self.proj = nn.Conv2d(\n",
    "            in_channels, embed_dim, \n",
    "            kernel_size=patch_size, stride=patch_size\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (B, C, H, W)\n",
    "        x = self.proj(x)  # (B, embed_dim, H/P, W/P)\n",
    "        x = x.flatten(2)  # (B, embed_dim, n_patches)\n",
    "        x = x.transpose(1, 2)  # (B, n_patches, embed_dim)\n",
    "        return x\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, in_channels=3, \n",
    "                 num_classes=1000, embed_dim=768, depth=12, \n",
    "                 num_heads=12, mlp_ratio=4.0):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_channels, embed_dim)\n",
    "        num_patches = self.patch_embed.n_patches\n",
    "        \n",
    "        # Class token and position embeddings\n",
    "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
    "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Transformer Encoder\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=embed_dim, nhead=num_heads, \n",
    "            dim_feedforward=int(embed_dim * mlp_ratio),\n",
    "            dropout=0.1, activation='gelu', batch_first=True\n",
    "        )\n",
    "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=depth)\n",
    "        \n",
    "        self.norm = nn.LayerNorm(embed_dim)\n",
    "        self.head = nn.Linear(embed_dim, num_classes)\n",
    "        \n",
    "        # Initialize\n",
    "        nn.init.normal_(self.cls_token, std=0.02)\n",
    "        nn.init.normal_(self.pos_embed, std=0.02)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        B = x.shape[0]\n",
    "        x = self.patch_embed(x)  # (B, n_patches, embed_dim)\n",
    "        \n",
    "        # Add cls token\n",
    "        cls_tokens = self.cls_token.expand(B, -1, -1)  # (B, 1, embed_dim)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # (B, n_patches+1, embed_dim)\n",
    "        \n",
    "        # Add position embeddings\n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        # Transformer\n",
    "        x = self.transformer(x)\n",
    "        x = self.norm(x)\n",
    "        \n",
    "        # Classifier on CLS token\n",
    "        cls_output = x[:, 0]\n",
    "        return self.head(cls_output)\n",
    "```\n",
    "\n",
    "#### **16.1.2 Inductive Bias Trade-off**\n",
    "\n",
    "**CNNs:** Locality and translation equivariance baked into architecture (prior knowledge about images).\n",
    "\n",
    "**ViT:** No image-specific inductive bias except patch extraction. Learns spatial relationships from scratch.\n",
    "\n",
    "**Implication:** ViT requires more data (ImageNet-21k or JFT-300M) to outperform CNNs, but scales better to huge datasets.\n",
    "\n",
    "**Hybrid Approaches:**\n",
    "- **CoAtNet:** Combines convolutions and attention\n",
    "- **Swin Transformer:** Hierarchical ViT with shifted windows (local attention)\n",
    "\n",
    "---\n",
    "\n",
    "## **16.2 Self-Supervised Learning for Vision**\n",
    "\n",
    "Learning representations without labels by predicting parts of the input from other parts.\n",
    "\n",
    "#### **16.2.1 Contrastive Learning (SimCLR, MoCo)**\n",
    "\n",
    "**Core Idea:** Pull together augmented views of same image, push apart different images.\n",
    "\n",
    "**SimCLR Loss (NT-Xent):**\n",
    "\n",
    "$$\\mathcal{L}_{i,j} = -\\log \\frac{\\exp(\\text{sim}(z_i, z_j)/\\tau)}{\\sum_{k=1}^{2N} \\mathbb{1}_{k \\neq i} \\exp(\\text{sim}(z_i, z_k)/\\tau)}$$\n",
    "\n",
    "Where $\\text{sim}(u,v) = \\frac{u^T v}{\\|u\\| \\|v\\|}$ (cosine similarity)\n",
    "\n",
    "**Key Components:**\n",
    "1. **Data Augmentation:** Random crop, color jitter, Gaussian blur (stronger than supervised)\n",
    "2. **Projection Head:** Small MLP maps representations to contrastive loss space (removed after pretraining)\n",
    "3. **Large Batch Size:** Needs many negatives (4096+)\n",
    "\n",
    "```python\n",
    "# Simplified SimCLR loss\n",
    "def nt_xent_loss(z_i, z_j, temperature=0.5):\n",
    "    \"\"\"\n",
    "    z_i, z_j: (batch, dim) representations of two views\n",
    "    \"\"\"\n",
    "    batch_size = z_i.shape[0]\n",
    "    z = torch.cat([z_i, z_j], dim=0)  # (2*batch, dim)\n",
    "    z = F.normalize(z, dim=1)\n",
    "    \n",
    "    # Cosine similarity matrix\n",
    "    similarity = torch.mm(z, z.T) / temperature  # (2*batch, 2*batch)\n",
    "    \n",
    "    # Mask out self-similarity\n",
    "    mask = torch.eye(2*batch_size, device=z.device).bool()\n",
    "    similarity = similarity.masked_fill(mask, -9e15)\n",
    "    \n",
    "    # Positive pairs: (i, i+batch) and (i+batch, i)\n",
    "    positives = torch.cat([\n",
    "        similarity[range(batch_size), range(batch_size, 2*batch_size)].unsqueeze(1),\n",
    "        similarity[range(batch_size, 2*batch_size), range(batch_size)].unsqueeze(1)\n",
    "    ], dim=0)  # (2*batch, 1)\n",
    "    \n",
    "    # Denominator: all negatives\n",
    "    negatives = similarity  # Already masked self\n",
    "    \n",
    "    # Loss\n",
    "    logits = torch.cat([positives, negatives], dim=1)\n",
    "    labels = torch.zeros(2*batch_size, device=z.device, dtype=torch.long)\n",
    "    \n",
    "    return F.cross_entropy(logits, labels)\n",
    "```\n",
    "\n",
    "**MoCo (Momentum Contrast):** Uses queue of past representations as negatives, allowing smaller batches.\n",
    "\n",
    "#### **16.2.2 Masked Autoencoders (MAE)**\n",
    "\n",
    "BERT-style pretraining for images: Mask random patches, reconstruct pixel values.\n",
    "\n",
    "**Asymmetric Design:**\n",
    "- **Encoder:** ViT processing only visible patches (25% of image) → efficient\n",
    "- **Decoder:** Lightweight Transformer reconstructing all patches\n",
    "- **Reconstruction Target:** Normalized pixel values (per-patch)\n",
    "\n",
    "**High Masking Ratio:** 75% (unlike BERT's 15%), because images have high redundancy.\n",
    "\n",
    "```python\n",
    "class MAE(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, mask_ratio=0.75):\n",
    "        super().__init__()\n",
    "        self.patch_embed = PatchEmbedding(img_size, patch_size, 3, 768)\n",
    "        self.mask_ratio = mask_ratio\n",
    "        \n",
    "        # Encoder (standard ViT)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(768, 12, 3072, batch_first=True)\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, 12)\n",
    "        \n",
    "        # Decoder (lightweight)\n",
    "        self.mask_token = nn.Parameter(torch.zeros(1, 1, 768))\n",
    "        decoder_layer = nn.TransformerEncoderLayer(768, 16, 2048, batch_first=True)\n",
    "        self.decoder = nn.TransformerEncoder(decoder_layer, 8)\n",
    "        \n",
    "        # Reconstruction head\n",
    "        self.head = nn.Linear(768, patch_size**2 * 3)\n",
    "        \n",
    "    def random_masking(self, x):\n",
    "        N, L, D = x.shape  # batch, length, dim\n",
    "        len_keep = int(L * (1 - self.mask_ratio))\n",
    "        \n",
    "        noise = torch.rand(N, L, device=x.device)\n",
    "        ids_shuffle = torch.argsort(noise, dim=1)\n",
    "        ids_restore = torch.argsort(ids_shuffle, dim=1)\n",
    "        \n",
    "        # Keep subset\n",
    "        ids_keep = ids_shuffle[:, :len_keep]\n",
    "        x_masked = torch.gather(x, dim=1, index=ids_keep.unsqueeze(-1).expand(-1, -1, D))\n",
    "        \n",
    "        # Generate mask (0 is keep, 1 is remove)\n",
    "        mask = torch.ones([N, L], device=x.device)\n",
    "        mask[:, :len_keep] = 0\n",
    "        mask = torch.gather(mask, dim=1, index=ids_restore)\n",
    "        \n",
    "        return x_masked, mask, ids_restore\n",
    "        \n",
    "    def forward(self, imgs):\n",
    "        # Embed patches\n",
    "        x = self.patch_embed(imgs)\n",
    "        \n",
    "        # Masking\n",
    "        x, mask, ids_restore = self.random_masking(x)\n",
    "        \n",
    "        # Add pos embed to visible patches...\n",
    "        \n",
    "        # Encode\n",
    "        x = self.encoder(x)\n",
    "        \n",
    "        # Decode (add mask tokens)\n",
    "        mask_tokens = self.mask_token.expand(x.shape[0], ids_restore.shape[1] - x.shape[1], -1)\n",
    "        x_full = torch.cat([x, mask_tokens], dim=1)\n",
    "        x_full = torch.gather(x_full, dim=1, index=ids_restore.unsqueeze(-1).expand(-1, -1, x.shape[2]))\n",
    "        \n",
    "        # Decode and reconstruct\n",
    "        x = self.decoder(x_full)\n",
    "        pred = self.head(x)\n",
    "        \n",
    "        return pred, mask\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **16.3 Generative Models**\n",
    "\n",
    "#### **16.3.1 Generative Adversarial Networks (GANs)**\n",
    "\n",
    "Two-player game: Generator $G$ creates fake images, Discriminator $D$ distinguishes real from fake.\n",
    "\n",
    "$$\\min_G \\max_D V(D, G) = \\mathbb{E}_{x \\sim p_{data}}[\\log D(x)] + \\mathbb{E}_{z \\sim p_z}[\\log(1 - D(G(z)))]$$\n",
    "\n",
    "**Modern GANs:**\n",
    "- **StyleGAN:** Style-based generator with progressive growing, mapping latent to intermediate latent space (disentanglement)\n",
    "- **CycleGAN:** Unpaired image-to-image translation using cycle consistency loss\n",
    "\n",
    "**GAN Limitations:** Mode collapse, training instability, hard to evaluate.\n",
    "\n",
    "#### **16.3.2 Variational Autoencoders (VAEs)**\n",
    "\n",
    "Learn latent distribution $p(z|x)$ via encoder, generate via decoder.\n",
    "\n",
    "**ELBO (Evidence Lower Bound):**\n",
    "$$\\mathcal{L} = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z))$$\n",
    "\n",
    "Reconstruction loss + KL divergence (regularization).\n",
    "\n",
    "**VQ-VAE (Vector Quantized):** Discrete latents using codebook.\n",
    "- Encoder outputs continuous → nearest neighbor lookup in codebook\n",
    "- Straight-through estimator for backpropagation\n",
    "- Used in DALL-E, SoundStream\n",
    "\n",
    "#### **16.3.3 Diffusion Models**\n",
    "\n",
    "Current state-of-the-art for image generation (Stable Diffusion, DALL-E 2, Imagen).\n",
    "\n",
    "**Forward Process (Diffusion):**\n",
    "Gradually add Gaussian noise over $T$ timesteps:\n",
    "$$q(x_t | x_{t-1}) = \\mathcal{N}(x_t; \\sqrt{1-\\beta_t} x_{t-1}, \\beta_t \\mathbf{I})$$\n",
    "\n",
    "After reparameterization:\n",
    "$$x_t = \\sqrt{\\bar{\\alpha}_t} x_0 + \\sqrt{1-\\bar{\\alpha}_t} \\epsilon, \\quad \\epsilon \\sim \\mathcal{N}(0, \\mathbf{I})$$\n",
    "\n",
    "**Reverse Process (Denoising):**\n",
    "Learn $p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$\n",
    "\n",
    "**Training Objective (Simple):**\n",
    "Predict the noise $\\epsilon$ added to $x_0$:\n",
    "$$\\mathcal{L} = \\mathbb{E}_{x_0, t, \\epsilon} \\|\\epsilon - \\epsilon_\\theta(x_t, t)\\|^2$$\n",
    "\n",
    "```python\n",
    "# Simplified Diffusion Model (DDPM)\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(self, model, timesteps=1000):\n",
    "        super().__init__()\n",
    "        self.model = model  # U-Net\n",
    "        self.timesteps = timesteps\n",
    "        \n",
    "        # Pre-compute noise schedule (cosine or linear)\n",
    "        betas = torch.linspace(0.0001, 0.02, timesteps)\n",
    "        alphas = 1.0 - betas\n",
    "        alphas_cumprod = torch.cumprod(alphas, dim=0)\n",
    "        \n",
    "        self.register_buffer('betas', betas)\n",
    "        self.register_buffer('alphas_cumprod', alphas_cumprod)\n",
    "        self.register_buffer('sqrt_alphas_cumprod', torch.sqrt(alphas_cumprod))\n",
    "        self.register_buffer('sqrt_one_minus_alphas_cumprod', torch.sqrt(1.0 - alphas_cumprod))\n",
    "        \n",
    "    def forward(self, x_0, t):\n",
    "        # Add noise\n",
    "        noise = torch.randn_like(x_0)\n",
    "        x_t = self.sqrt_alphas_cumprod[t] * x_0 + self.sqrt_one_minus_alphas_cumprod[t] * noise\n",
    "        \n",
    "        # Predict noise\n",
    "        predicted_noise = self.model(x_t, t)\n",
    "        \n",
    "        return F.mse_loss(predicted_noise, noise)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size, channels, height, width):\n",
    "        # Start from pure noise\n",
    "        x = torch.randn(batch_size, channels, height, width, device=self.betas.device)\n",
    "        \n",
    "        for t in reversed(range(self.timesteps)):\n",
    "            t_batch = torch.full((batch_size,), t, device=x.device, dtype=torch.long)\n",
    "            predicted_noise = self.model(x, t_batch)\n",
    "            \n",
    "            alpha_t = self.alphas_cumprod[t]\n",
    "            alpha_t_prev = self.alphas_cumprod[t-1] if t > 0 else torch.tensor(1.0)\n",
    "            beta_t = self.betas[t]\n",
    "            \n",
    "            # Denoise step (simplified)\n",
    "            x = (x - beta_t / torch.sqrt(1 - alpha_t) * predicted_noise) / torch.sqrt(1 - beta_t)\n",
    "            \n",
    "            if t > 0:\n",
    "                noise = torch.randn_like(x)\n",
    "                x = x + torch.sqrt(beta_t) * noise\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "**Latent Diffusion Models (Stable Diffusion):** Apply diffusion in VAE latent space (lower dimensional), conditioned on text embeddings (CLIP).\n",
    "\n",
    "---\n",
    "\n",
    "## **16.4 Multimodal Vision-Language Models**\n",
    "\n",
    "#### **16.4.1 CLIP (Contrastive Language-Image Pre-training)**\n",
    "\n",
    "Jointly train image encoder and text encoder to maximize cosine similarity of (image, text) pairs, minimize for non-matching pairs.\n",
    "\n",
    "**Zero-shot Classification:**\n",
    "```python\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "\n",
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Prepare text descriptions\n",
    "texts = [\"a photo of a cat\", \"a photo of a dog\", \"a photo of a car\"]\n",
    "inputs = processor(text=texts, images=image, return_tensors=\"pt\", padding=True)\n",
    "\n",
    "outputs = model(**inputs)\n",
    "logits_per_image = outputs.logits_per_image  # (n_images, n_texts)\n",
    "probs = logits_per_image.softmax(dim=1)\n",
    "```\n",
    "\n",
    "#### **16.4.2 LLaVA (Large Language and Vision Assistant)**\n",
    "\n",
    "Connect CLIP vision encoder to LLM (Vicuna/Llama) via projection layer. Fine-tune on instruction-following data.\n",
    "\n",
    "**Architecture:**\n",
    "1. Image → ViT → [CLS] token features\n",
    "2. Projection layer → LLM embedding space\n",
    "3. Concatenate with text tokens → LLM generates response\n",
    "\n",
    "---\n",
    "\n",
    "## **16.5 Video Understanding**\n",
    "\n",
    "#### **16.5.1 3D Convolutions**\n",
    "\n",
    "Extend 2D conv to temporal dimension: $(C_{out}, C_{in}, k_h, k_w, k_t)$\n",
    "\n",
    "**I3D (Inflated 3D CNN):** Inflate 2D ImageNet pre-trained filters to 3D by repeating across time.\n",
    "\n",
    "#### **16.5.2 Video Transformers**\n",
    "\n",
    "**TimeSformer:** Divided space-time attention (attend spatially then temporally to reduce complexity $O((HW)^2 T)$ to $O((HW)^2) + O(T^2)$).\n",
    "\n",
    "**VideoMAE:** Masked autoencoding for video (tube masking).\n",
    "\n",
    "---\n",
    "\n",
    "## **16.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Vision Transformer from Scratch**\n",
    "Implement ViT-Tiny on CIFAR-10:\n",
    "1. Patch embedding (4x4 patches for 32x32 images)\n",
    "2. Custom Transformer encoder (no PyTorch nn.Transformer)\n",
    "3. Train from scratch (no pretraining)\n",
    "4. Compare to ResNet-18 accuracy\n",
    "\n",
    "**Deliverable:** ViT achieving >80% on CIFAR-10.\n",
    "\n",
    "### **Lab 2: Contrastive Pretraining**\n",
    "SimCLR pretraining on unlabeled subset of ImageNet:\n",
    "1. Strong augmentation pipeline\n",
    "2. ResNet-50 encoder + projection head\n",
    "3. Train with NT-Xent loss\n",
    "4. Evaluate: Linear probing (freeze encoder, train linear classifier) vs supervised baseline\n",
    "\n",
    "**Deliverable:** Pretrained checkpoint with competitive linear probe accuracy.\n",
    "\n",
    "### **Lab 3: Fine-tuning Stable Diffusion**\n",
    "Use diffusers library to fine-tune Stable Diffusion on custom concept (e.g., \"a photo of [V] dog\"):\n",
    "1. DreamBooth or LoRA fine-tuning\n",
    "2. Inference pipeline generating new images of the concept\n",
    "3. Evaluation: CLIP similarity to text prompt, FID score\n",
    "\n",
    "**Deliverable:** Personalized diffusion model generating consistent concept images.\n",
    "\n",
    "### **Lab 4: Multimodal RAG**\n",
    "Build CLIP-based image search:\n",
    "1. Index 10k images with CLIP embeddings (FAISS)\n",
    "2. Text-to-image search (natural language queries)\n",
    "3. Image-to-image search (similarity search)\n",
    "4. Hybrid: Combine with text metadata\n",
    "\n",
    "**Deliverable:** Image search engine with web interface.\n",
    "\n",
    "---\n",
    "\n",
    "## **16.7 Common Pitfalls**\n",
    "\n",
    "1. **ViT on Small Data:** Training ViT from scratch on ImageNet-1k (1.2M images) performs worse than ResNet. Use transfer learning or augmentation (RandAug, Mixup, CutMix).\n",
    "\n",
    "2. **Diffusion Sampling Steps:** Using too few steps (<20) with standard DDPM gives poor quality. Use DDIM or DPM-Solver for fast sampling (10-20 steps).\n",
    "\n",
    "3. **GAN Mode Collapse:** Generator produces limited variety. Monitor diversity, use techniques like MiniBatch Discrimination or switch to Diffusion.\n",
    "\n",
    "4. **Contrastive Learning Temperature:** Temperature $\\tau$ is crucial. Too low (0.01) causes collapse; too high (1.0) reduces signal.\n",
    "\n",
    "---\n",
    "\n",
    "## **16.8 Interview Questions**\n",
    "\n",
    "**Q1:** Why do Vision Transformers need more data than CNNs to perform well?\n",
    "*A: CNNs have strong inductive biases built in: locality (local receptive fields), translation equivariance (weight sharing), and hierarchical structure. ViT has no image-specific prior except patch extraction—it must learn spatial relationships from scratch. With limited data, CNNs generalize better due to these biases; with massive data (ImageNet-21k, JFT), ViT's flexibility and global attention allow better scaling.*\n",
    "\n",
    "**Q2:** Explain the difference between SimCLR (contrastive) and MAE (masked autoencoding) for self-supervised learning.\n",
    "*A: SimCLR uses contrastive learning: pulls together augmented views of same image, pushes apart different images. Requires large batches or memory banks for negatives. MAE uses reconstruction: masks large portions of input (75%), reconstructs missing pixels via encoder-decoder. No negatives needed, asymmetric design (encoder only on visible patches) makes it efficient. SimCLR learns semantic representations; MAE learns both semantic and low-level details.*\n",
    "\n",
    "**Q3:** How does Latent Diffusion (Stable Diffusion) differ from pixel-space diffusion?\n",
    "*A: Pixel-space diffusion (original DDPM) operates on full resolution RGB images (expensive). Latent Diffusion first compresses images to latent space using a pre-trained VAE (e.g., 64x64x4 instead of 256x256x3, 48x reduction), applies diffusion process in this compressed space, then decodes with VAE. Additionally, it's conditioned on text embeddings (CLIP) via cross-attention in the U-Net, enabling text-to-image generation. Much faster and enables high-resolution generation.*\n",
    "\n",
    "**Q4:** What is the purpose of the projection head in SimCLR, and why is it discarded for downstream tasks?\n",
    "*A: The projection head (small MLP) maps representations to the space where contrastive loss is applied. It allows the backbone encoder to learn representations that are not necessarily normalized or linearly separable in the contrastive task. Empirically, representations before the projection head (h) perform better on downstream tasks than after (z), suggesting the projection head removes information useful for downstream but harmful for contrastive loss (e.g., augmentation-invariant features).*\n",
    "\n",
    "**Q5:** How does CLIP enable zero-shot classification, and what are its limitations?\n",
    "*A: CLIP jointly trains image and text encoders to align image-text pairs in embedding space. For zero-shot classification, you embed class names as text prompts (\"a photo of a [class]\"), embed the image, and take the highest cosine similarity. Limitations: 1) Sensitive to prompt engineering (\"a photo of\" vs \"a picture of\"), 2) Poor on fine-grained distinctions (breeds of dogs), 3) Biases from internet training data, 4) Struggles with abstract concepts or counting.*\n",
    "\n",
    "---\n",
    "\n",
    "## **16.9 Further Reading**\n",
    "\n",
    "**Papers:**\n",
    "- \"An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale\" (Dosovitskiy et al., 2020) - ViT\n",
    "- \"Masked Autoencoders Are Scalable Vision Learners\" (He et al., 2021) - MAE\n",
    "- \"Denoising Diffusion Probabilistic Models\" (Ho et al., 2020) - DDPM\n",
    "- \"High-Resolution Image Synthesis with Latent Diffusion Models\" (Rombach et al., 2021) - Stable Diffusion\n",
    "- \"Learning Transferable Visual Models From Natural Language Supervision\" (Radford et al., 2021) - CLIP\n",
    "\n",
    "---\n",
    "\n",
    "## **16.10 Checkpoint Project: Multimodal Content Generation Platform**\n",
    "\n",
    "Build a system that generates images from text descriptions with editing capabilities.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Base Model:** Stable Diffusion 1.5 or XL (open source)\n",
    "\n",
    "2. **Features:**\n",
    "   - Text-to-image generation with prompt optimization (expand simple prompts to detailed)\n",
    "   - Image editing: Inpainting (fill masked regions) and Outpainting (extend borders)\n",
    "   - Style transfer: Combine content from one image, style from another (using CLIP embeddings)\n",
    "   - Upscaling: 4x super-resolution using Real-ESRGAN\n",
    "\n",
    "3. **Personalization:**\n",
    "   - DreamBooth fine-tuning on user-provided 5-10 images of concept\n",
    "   - LoRA training for efficient style adaptation\n",
    "\n",
    "4. **Safety:**\n",
    "   - NSFW content detection (CLIP-based classifier)\n",
    "   - Watermarking generated images\n",
    "   - Metadata injection (C2PA standard)\n",
    "\n",
    "5. **Deployment:**\n",
    "   - FastAPI backend with async generation queue (Celery/Redis)\n",
    "   - WebSocket streaming for generation progress\n",
    "   - Model weights cached in VRAM, LoRA adapters hot-swappable\n",
    "   - Quantization (INT8) for memory efficiency\n",
    "\n",
    "**Deliverables:**\n",
    "- `genai_platform/` with generation, editing, and training modules\n",
    "- Frontend (React/Gradio) showing prompt input, gallery, editing canvas\n",
    "- Performance: 512x512 image generation < 5 seconds on A100\n",
    "- Report: \"Platform supports 10 concurrent users with LoRA hot-swapping\"\n",
    "\n",
    "**Success Criteria:**\n",
    "- Text alignment (CLIP score > 0.3 between prompt and generated image)\n",
    "- User can train personalized concept in < 10 minutes\n",
    "- Editing maintains consistency with original image (structural similarity > 0.8)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 16**\n",
    "\n",
    "*You now master advanced computer vision and multimodal AI. Chapter 17 will cover Reinforcement Learning (RL) — the foundation of RLHF and autonomous decision-making systems.*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
