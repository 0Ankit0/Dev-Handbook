{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c1c081ce",
   "metadata": {},
   "source": [
    "Here is **Chapter 18: Specialized Applications** \u2014 domain-specific deep learning architectures.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 18: SPECIALIZED APPLICATIONS**\n",
    "\n",
    "*AI for Every Domain*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "While CNNs and Transformers form the foundation, specialized domains require architectural adaptations. Time series data violates the i.i.d. assumption; recommendation systems handle sparse, high-dimensional user-item interactions; graph data has irregular structure; and tabular data remains stubbornly resistant to standard deep learning. This chapter equips you with the specialized tools for these critical industry applications.\n",
    "\n",
    "**Estimated Time:** 50-60 hours (3-4 weeks)  \n",
    "**Prerequisites:** Chapters 12-17 (CNNs, RNNs, Transformers, GNN fundamentals from Chapter 8)\n",
    "\n",
    "---\n",
    "\n",
    "## **18.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Build deep learning forecasting models (DeepAR, N-BEATS, Temporal Fusion Transformers) that handle multiple time series and uncertainty quantification\n",
    "2. Design two-tower neural architectures for large-scale recommendation and candidate retrieval\n",
    "3. Implement Graph Neural Networks (GCN, GAT, GraphSAGE) for node classification and link prediction\n",
    "4. Apply deep learning to tabular data using embeddings and architectures like TabNet that rival XGBoost\n",
    "5. Process audio signals using spectrograms and implement speech recognition (wav2vec) and text-to-speech systems\n",
    "\n",
    "---\n",
    "\n",
    "## **18.1 Time Series Forecasting**\n",
    "\n",
    "Beyond ARIMA: Deep learning for temporal prediction with multiple covariates.\n",
    "\n",
    "#### **18.1.1 DeepAR (Probabilistic Forecasting)**\n",
    "\n",
    "Amazon's DeepAR treats forecasting as a sequence-to-sequence problem, modeling the full conditional distribution $P(z_{i,t_0:T} | z_{i,1:t_0-1}, \\mathbf{x}_{i,1:T})$.\n",
    "\n",
    "**Key Innovations:**\n",
    "- **Autoregressive Recurrent Network:** LSTM generates hidden state $h_{i,t}$, parameters of likelihood (e.g., Gaussian) depend on $h_{i,t}$.\n",
    "- **Global Model:** Single model trained on thousands of related time series (e.g., sales of all products), enabling transfer learning.\n",
    "- **Covariates:** Time-varying features (price, promotion) and static features (category, location).\n",
    "\n",
    "```python\n",
    "class DeepAR(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers=2, likelihood=\"gaussian\"):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.likelihood = likelihood\n",
    "        \n",
    "        # Output layers for distribution parameters\n",
    "        if likelihood == \"gaussian\":\n",
    "            self.mu_proj = nn.Linear(hidden_size, 1)\n",
    "            self.sigma_proj = nn.Linear(hidden_size, 1)\n",
    "        \n",
    "    def forward(self, x, target=None):\n",
    "        # x: (batch, seq_len, features)\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        \n",
    "        if self.likelihood == \"gaussian\":\n",
    "            mu = self.mu_proj(lstm_out)\n",
    "            sigma = F.softplus(self.sigma_proj(lstm_out)) + 1e-6  # Ensure positive\n",
    "            return mu, sigma\n",
    "        \n",
    "    def loss(self, x, target):\n",
    "        mu, sigma = self.forward(x)\n",
    "        # Negative log-likelihood of Gaussian\n",
    "        loss = torch.log(sigma) + 0.5 * ((target - mu) / sigma) ** 2\n",
    "        return loss.mean()\n",
    "    \n",
    "    def sample(self, x, num_samples=100):\n",
    "        # Monte Carlo sampling for prediction intervals\n",
    "        mu, sigma = self.forward(x)\n",
    "        dist = torch.distributions.Normal(mu, sigma)\n",
    "        samples = dist.sample((num_samples,))\n",
    "        return samples  # (num_samples, batch, seq, 1)\n",
    "```\n",
    "\n",
    "#### **18.1.2 N-BEATS (Neural Basis Expansion Analysis)**\n",
    "\n",
    "Pure deep learning decomposition: Express forecast as sum of basis functions.\n",
    "\n",
    "$$y_{t+h} = \\sum_{i=1}^{\\text{stacks}} \\sum_{j=1}^{\\text{blocks}} g_{i,j}(h) \\cdot f_{i,j}(y_{1:t})$$\n",
    "\n",
    "**Interpretable Version:** Separate stacks for trend (monotonic) and seasonality (periodic).\n",
    "\n",
    "```python\n",
    "class NBeatsBlock(nn.Module):\n",
    "    def __init__(self, input_size, output_size, hidden_size, type=\"generic\"):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Sequential(\n",
    "            nn.Linear(input_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_size, hidden_size),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        \n",
    "        if type == \"trend\":\n",
    "            # Polynomial basis: [1, t, t^2, ..., t^p]\n",
    "            self.degree = 3\n",
    "            self.basis = nn.Linear(hidden_size, 2 * self.degree)  # backcast + forecast coeffs\n",
    "        elif type == \"seasonality\":\n",
    "            # Fourier basis\n",
    "            self.num_harmonics = 10\n",
    "            self.basis = nn.Linear(hidden_size, 2 * self.num_harmonics)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: (batch, input_size)\n",
    "        x = self.fc(x)\n",
    "        theta = self.basis(x)\n",
    "        # ... basis expansion logic ...\n",
    "        return backcast, forecast\n",
    "```\n",
    "\n",
    "#### **18.1.3 Temporal Fusion Transformer (TFT)**\n",
    "\n",
    "Interpretable multi-horizon forecasting combining several mechanisms:\n",
    "\n",
    "- **Gating Mechanisms:** GLU (Gated Linear Units) to skip unused components\n",
    "- **Variable Selection Networks:** Learn which covariates are relevant\n",
    "- **Static Covariate Encoders:** Encode time-invariant features\n",
    "- **Interpretable Multi-Head Attention:** Temporal attention showing which past time steps are important\n",
    "\n",
    "**Use Case:** Demand forecasting where you need to explain *why* a spike is predicted (holiday? promotion?).\n",
    "\n",
    "---\n",
    "\n",
    "## **18.2 Recommendation Systems**\n",
    "\n",
    "Moving beyond matrix factorization to deep neural architectures.\n",
    "\n",
    "#### **18.2.1 Two-Tower Architecture (Candidate Generation)**\n",
    "\n",
    "Separates user and item encoders for efficient retrieval at scale (millions of items).\n",
    "\n",
    "$$\\text{score}(u, i) = \\langle \\text{UserTower}(u), \\text{ItemTower}(i) \\rangle$$\n",
    "\n",
    "**Training:** Sampled softmax or batch softmax on in-batch negatives.\n",
    "\n",
    "```python\n",
    "class TwoTowerModel(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=64):\n",
    "        super().__init__()\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Embedding(num_users, embedding_dim),\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Embedding(num_items, embedding_dim),\n",
    "            nn.Linear(embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64)\n",
    "        )\n",
    "        \n",
    "    def forward(self, user_ids, item_ids):\n",
    "        user_emb = F.normalize(self.user_tower(user_ids), dim=1)\n",
    "        item_emb = F.normalize(self.item_tower(item_ids), dim=1)\n",
    "        scores = torch.sum(user_emb * item_emb, dim=1)  # Dot product\n",
    "        return scores\n",
    "    \n",
    "    def get_item_embeddings(self, item_ids):\n",
    "        return F.normalize(self.item_tower(item_ids), dim=1)\n",
    "    \n",
    "    def retrieve_candidates(self, user_id, item_index, k=100):\n",
    "        # user_id: single user\n",
    "        # item_index: FAISS index of all item embeddings\n",
    "        user_emb = self.user_tower(torch.tensor([user_id]))\n",
    "        D, I = item_index.search(user_emb.numpy(), k)\n",
    "        return I  # Top-k item IDs\n",
    "```\n",
    "\n",
    "**Serving:**\n",
    "1. Pre-compute all item embeddings, index in FAISS (GPU or IVF for millions)\n",
    "2. User query \u2192 encode user \u2192 FAISS search (sub-millisecond)\n",
    "3. Re-rank top-1000 with heavy ranking model (see below)\n",
    "\n",
    "#### **18.2.2 DeepFM (Factorization Machines + DNN)**\n",
    "\n",
    "Combines low-order feature interactions (FM) with high-order (DNN).\n",
    "\n",
    "$$y_{FM} = \\sum_{i=1}^n \\sum_{j=i+1}^n \\langle v_i, v_j \\rangle x_i x_j$$\n",
    "\n",
    "```python\n",
    "class DeepFM(nn.Module):\n",
    "    def __init__(self, field_dims, embed_dim, mlp_dims):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(sum(field_dims), embed_dim)\n",
    "        self.offsets = torch.cumsum(torch.tensor([0] + field_dims[:-1]), dim=0)\n",
    "        \n",
    "        # FM component\n",
    "        self.fm = FactorizationMachine()\n",
    "        \n",
    "        # Deep component\n",
    "        layers = []\n",
    "        input_dim = len(field_dims) * embed_dim\n",
    "        for dim in mlp_dims:\n",
    "            layers.extend([nn.Linear(input_dim, dim), nn.ReLU(), nn.Dropout(0.2)])\n",
    "            input_dim = dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, num_fields) with categorical indices\n",
    "        x = x + self.offsets.unsqueeze(0).to(x.device)\n",
    "        embeds = self.embedding(x)  # (batch, num_fields, embed_dim)\n",
    "        \n",
    "        fm_out = self.fm(embeds)\n",
    "        deep_out = self.mlp(embeds.view(embeds.size(0), -1))\n",
    "        \n",
    "        return torch.sigmoid(fm_out + deep_out)\n",
    "```\n",
    "\n",
    "#### **18.2.3 Session-Based Recommendation (SASRec)**\n",
    "\n",
    "Use Transformer to model user click sequences, predicting next item.\n",
    "\n",
    "- **Self-attention** over past items to capture sequential patterns\n",
    "- **Position embeddings** for order\n",
    "- Similar to language modeling, but with item IDs as tokens\n",
    "\n",
    "---\n",
    "\n",
    "## **18.3 Graph Neural Networks (GNNs)**\n",
    "\n",
    "Deep learning on non-Euclidean data: social networks, molecules, knowledge graphs.\n",
    "\n",
    "#### **18.3.1 Message Passing Framework**\n",
    "\n",
    "The fundamental GNN paradigm: nodes update their representations by aggregating messages from neighbors.\n",
    "\n",
    "$$h_v^{(l+1)} = \\text{UPDATE}^{(l)}\\left(h_v^{(l)}, \\text{AGGREGATE}^{(l)}\\left(\\{h_u^{(l)}, \\forall u \\in \\mathcal{N}(v)\\}\\right)\\right)$$\n",
    "\n",
    "Where $\\mathcal{N}(v)$ are neighbors of node $v$.\n",
    "\n",
    "#### **18.3.2 Graph Convolutional Network (GCN)**\n",
    "\n",
    "Spectral approach: normalized adjacency matrix multiplication.\n",
    "\n",
    "$$H^{(l+1)} = \\sigma\\left(\\tilde{D}^{-1/2} \\tilde{A} \\tilde{D}^{-1/2} H^{(l)} W^{(l)}\\right)$$\n",
    "\n",
    "Where $\\tilde{A} = A + I$ (add self-loops), $\\tilde{D}$ is degree matrix.\n",
    "\n",
    "**Implementation (PyTorch Geometric):**\n",
    "```python\n",
    "import torch_geometric.nn as gnn\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = gnn.GCNConv(in_channels, hidden_channels)\n",
    "        self.conv2 = gnn.GCNConv(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # x: Node features (N, F)\n",
    "        # edge_index: Graph connectivity (2, E) COO format\n",
    "        x = self.conv1(x, edge_index).relu()\n",
    "        x = F.dropout(x, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "```\n",
    "\n",
    "#### **18.3.3 Graph Attention Network (GAT)**\n",
    "\n",
    "Attention over neighbors: learn which neighbors are important.\n",
    "\n",
    "$$h_v' = \\sigma\\left(\\sum_{u \\in \\mathcal{N}(v)} \\alpha_{vu} W h_u\\right)$$\n",
    "\n",
    "Where $\\alpha_{vu}$ are attention coefficients computed via softmax over learned attention scores.\n",
    "\n",
    "**Multi-head attention:** $K$ independent attention mechanisms concatenated.\n",
    "\n",
    "#### **18.3.4 GraphSAGE (Sample and Aggregate)**\n",
    "\n",
    "Inductive learning: Generalize to unseen nodes/graphs.\n",
    "\n",
    "**Sampling:** Fixed-size neighborhood sampling (computationally efficient for large graphs).\n",
    "\n",
    "**Aggregation:** Mean, LSTM, or Pooling aggregator.\n",
    "\n",
    "```python\n",
    "class GraphSAGE(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, num_layers=2):\n",
    "        super().__init__()\n",
    "        self.convs = nn.ModuleList()\n",
    "        self.convs.append(gnn.SAGEConv(in_channels, hidden_channels))\n",
    "        for _ in range(num_layers - 2):\n",
    "            self.convs.append(gnn.SAGEConv(hidden_channels, hidden_channels))\n",
    "        self.convs.append(gnn.SAGEConv(hidden_channels, out_channels))\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        for conv in self.convs[:-1]:\n",
    "            x = conv(x, edge_index).relu()\n",
    "            x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.convs[-1](x, edge_index)\n",
    "        return x\n",
    "```\n",
    "\n",
    "#### **18.3.5 Knowledge Graph Embeddings (TransE, RotatE)**\n",
    "\n",
    "Represent entities and relations in vector space for link prediction.\n",
    "\n",
    "**TransE:** $\\mathbf{h} + \\mathbf{r} \\approx \\mathbf{t}$ (head relation tail)\n",
    "\n",
    "**RotatE:** Rotational model in complex space: $\\mathbf{t} = \\mathbf{h} \\circ \\mathbf{r}$ (where $\\circ$ is Hadamard product in complex numbers), models relation as rotation.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.4 Tabular Deep Learning**\n",
    "\n",
    "Deep learning for structured data (tables), where XGBoost/LightGBM traditionally dominate.\n",
    "\n",
    "#### **18.4.1 Entity Embeddings for Categoricals**\n",
    "\n",
    "Map high-cardinality categorical variables (e.g., zip codes, product IDs) to dense vectors.\n",
    "\n",
    "```python\n",
    "class TabularNN(nn.Module):\n",
    "    def __init__(self, categorical_dims, embedding_dims, numerical_dim, hidden_dims):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Embeddings for each categorical feature\n",
    "        self.embeddings = nn.ModuleList([\n",
    "            nn.Embedding(dim, emb_dim) \n",
    "            for dim, emb_dim in zip(categorical_dims, embedding_dims)\n",
    "        ])\n",
    "        \n",
    "        total_emb_dim = sum(embedding_dims)\n",
    "        input_dim = total_emb_dim + numerical_dim\n",
    "        \n",
    "        # MLP\n",
    "        layers = []\n",
    "        for hidden_dim in hidden_dims:\n",
    "            layers.extend([\n",
    "                nn.Linear(input_dim, hidden_dim),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.ReLU(),\n",
    "                nn.Dropout(0.3)\n",
    "            ])\n",
    "            input_dim = hidden_dim\n",
    "        layers.append(nn.Linear(input_dim, 1))\n",
    "        \n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x_categorical, x_numerical):\n",
    "        # x_categorical: (batch, num_cat_features) with indices\n",
    "        embeddings = [emb(x_categorical[:, i]) for i, emb in enumerate(self.embeddings)]\n",
    "        x = torch.cat(embeddings + [x_numerical], dim=1)\n",
    "        return self.mlp(x)\n",
    "```\n",
    "\n",
    "#### **18.4.2 TabNet**\n",
    "\n",
    "Attention-based tabular learning with sequential decision steps (similar to additive models).\n",
    "\n",
    "**Sparse Attention:** Selects which features to use at each decision step, providing interpretability (feature importance).\n",
    "\n",
    "**Key Components:**\n",
    "- **Feature Transformer:** Shared and step-specific layers\n",
    "- **Attentive Transformer:** Feature selection via sparsemax\n",
    "- **Mask:** Tracks which features have been used\n",
    "\n",
    "#### **18.4.3 Regularization for Tabular**\n",
    "\n",
    "Tabular data overfits easily due to spurious correlations.\n",
    "\n",
    "- **Mixup:** Interpolate between samples: $\\tilde{x} = \\lambda x_i + (1-\\lambda)x_j$\n",
    "- **CutMix:** For images, but variants exist for tabular\n",
    "- **Dropout:** Higher rates than vision (0.5+)\n",
    "- **Weight Decay:** Strong L2 regularization\n",
    "\n",
    "---\n",
    "\n",
    "## **18.5 Speech and Audio**\n",
    "\n",
    "#### **18.5.1 Signal Processing Basics**\n",
    "\n",
    "Audio is 1D waveform (pressure over time). Transform to frequency domain:\n",
    "\n",
    "- **STFT (Short-Time Fourier Transform):** Spectrogram (time x frequency)\n",
    "- **Mel-Spectrogram:** Compress frequency axis using Mel scale (human perception)\n",
    "- **MFCCs:** Mel-Frequency Cepstral Coefficients (compact representation)\n",
    "\n",
    "```python\n",
    "import torchaudio\n",
    "\n",
    "# Load audio\n",
    "waveform, sample_rate = torchaudio.load(\"audio.wav\")\n",
    "\n",
    "# Mel spectrogram transform\n",
    "mel_transform = torchaudio.transforms.MelSpectrogram(\n",
    "    sample_rate=16000,\n",
    "    n_fft=400,\n",
    "    win_length=400,\n",
    "    hop_length=160,\n",
    "    n_mels=80\n",
    ")\n",
    "\n",
    "mel_spec = mel_transform(waveform)  # (channel, mel, time)\n",
    "```\n",
    "\n",
    "#### **18.5.2 Self-Supervised Speech (wav2vec 2.0)**\n",
    "\n",
    "BERT for audio: Pretrain on unlabeled speech, fine-tune on small labeled data for ASR.\n",
    "\n",
    "**Contrastive Task:** Mask spans of latent representations, contrast true quantized latent vs distractors.\n",
    "\n",
    "**Architecture:** CNN encoder \u2192 Transformer \u2192 Quantization module.\n",
    "\n",
    "```python\n",
    "from transformers import Wav2Vec2ForCTC, Wav2Vec2Processor\n",
    "\n",
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2ForCTC.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "\n",
    "# Inference\n",
    "input_values = processor(waveform, return_tensors=\"pt\").input_values\n",
    "with torch.no_grad():\n",
    "    logits = model(input_values).logits\n",
    "predicted_ids = torch.argmax(logits, dim=-1)\n",
    "transcription = processor.batch_decode(predicted_ids)\n",
    "```\n",
    "\n",
    "#### **18.5.3 Text-to-Speech (TTS)**\n",
    "\n",
    "**Tacotron 2:** Sequence-to-sequence with attention, predicts Mel-spectrograms from text, then vocoder (WaveGlow) generates waveform.\n",
    "\n",
    "**VITS:** End-to-end (no vocoder needed), uses normalizing flows for high-quality synthesis.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Time Series Forecasting**\n",
    "Predict electricity demand:\n",
    "1. TFT implementation or DeepAR using PyTorch Forecasting\n",
    "2. Multiple covariates (temperature, hour of day, day of week)\n",
    "3. Quantile forecasting (10th, 50th, 90th percentiles)\n",
    "4. Evaluation: MAPE, RMSE, Coverage of prediction intervals\n",
    "\n",
    "**Deliverable:** Forecast with uncertainty bands and feature importance analysis.\n",
    "\n",
    "### **Lab 2: Recommendation System**\n",
    "MovieLens dataset:\n",
    "1. Two-tower model for candidate generation\n",
    "2. DeepFM for ranking (incorporating user features like age, occupation)\n",
    "3. Evaluate: Recall@10, NDCG@10\n",
    "4. A/B test simulation: CTR lift vs baseline (popular items)\n",
    "\n",
    "**Deliverable:** End-to-end recommendation pipeline with FAISS retrieval.\n",
    "\n",
    "### **Lab 3: Graph Node Classification**\n",
    "Cora citation network:\n",
    "1. GCN implementation from scratch (message passing)\n",
    "2. Compare with GraphSAGE (inductive) and GAT (attention)\n",
    "3. Visualize embeddings with t-SNE (color by class)\n",
    "4. Link prediction: Hide edges, predict missing citations\n",
    "\n",
    "**Deliverable:** Node classifier > 80% accuracy with attention visualization.\n",
    "\n",
    "### **Lab 4: Tabular Deep Learning**\n",
    "Kaggle House Prices or similar:\n",
    "1. TabularNN with embeddings vs XGBoost baseline\n",
    "2. TabNet with interpretability (which features used for each prediction)\n",
    "3. Ensembling: Average of XGBoost + Neural Net\n",
    "\n",
    "**Deliverable:** Report showing neural net matches or beats XGBoost with proper tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.7 Common Pitfalls**\n",
    "\n",
    "1. **Data Leakage in Time Series:** Using future information (rolling mean including target) to predict past. Always use time-based validation.\n",
    "\n",
    "2. **Cold Start in RecSys:** New users/items have no embeddings. Solution: Content-based features, meta-learning, or average embeddings.\n",
    "\n",
    "3. **Graph Oversmoothing:** Deep GNNs make all node embeddings similar. Solution: Residual connections, skip connections (JK-Net), or shallow networks.\n",
    "\n",
    "4. **Categorical Cardinality:** Embedding 1M unique IDs causes overfitting. Solution: Hashing trick, shared embeddings, or entity resolution.\n",
    "\n",
    "5. **Spectral Bias in Audio:** CNNs bias toward local patterns; use dilated convolutions or Transformers for long-range dependencies in audio.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.8 Interview Questions**\n",
    "\n",
    "**Q1:** Why might XGBoost outperform neural networks on tabular data, and when would you switch to deep learning?\n",
    "*A: XGBoost naturally handles heterogeneous features (mix of categorical and numerical), is robust to outliers, requires less tuning, and excels when interactions are tree-like. Neural networks win with: (1) high-cardinality categorical variables (embeddings capture similarity), (2) unstructured inputs (text/images alongside tabular), (3) very large datasets where GBDT slows down, (4) need for end-to-end differentiability (embedding learning).*\n",
    "\n",
    "**Q2:** Explain the inductive bias of GraphSAGE vs GCN.\n",
    "*A: GCN is transductive: requires entire graph during training, uses normalized adjacency matrix fixed for that graph. GraphSAGE is inductive: learns aggregation functions that generalize to unseen nodes/graphs by sampling neighborhoods and learning to aggregate (e.g., mean of neighbor features). GraphSAGE can predict on new nodes without retraining; GCN cannot easily.*\n",
    "\n",
    "**Q3:** How do two-tower models handle the massive scale of candidate retrieval (millions of items)?\n",
    "*A: The dot product or cosine similarity allows pre-computing item tower embeddings offline and indexing them (FAISS, ScaNN). At serving time, only the user query needs forward pass through user tower, then approximate nearest neighbor search in the pre-computed index (sub-millisecond). This decouples the heavy item computation from online serving.*\n",
    "\n",
    "**Q4:** What is the difference between transductive and inductive link prediction in knowledge graphs?\n",
    "*A: Transductive: All entities seen during training; test set contains unseen edges between known entities. Inductive: Test set contains entirely unseen entities (e.g., new drugs in drug discovery). Transductive methods (most KGE like TransE) fail at inductive settings because they learn entity-specific embeddings. Inductive methods use GNNs or textual descriptions to generalize to new entities.*\n",
    "\n",
    "**Q5:** Why do we use Mel scale instead of linear frequency in audio processing?\n",
    "*A: Human hearing is logarithmic and more sensitive to differences at lower frequencies (< 1kHz) than higher frequencies. The Mel scale approximates this non-linear perception, spacing frequencies linearly at low end and logarithmically at high end. This compresses the frequency representation to focus on perceptually relevant differences, improving model efficiency and often performance.*\n",
    "\n",
    "---\n",
    "\n",
    "## **18.9 Further Reading**\n",
    "\n",
    "**Time Series:**\n",
    "- \"DeepAR: Probabilistic Forecasting with Autoregressive Recurrent Networks\" (Salinas et al.)\n",
    "- \"Temporal Fusion Transformers for Interpretable Multi-horizon Time Series Forecasting\" (Lim et al.)\n",
    "\n",
    "**Recommendation:**\n",
    "- \"Deep Learning for Recommender Systems\" (Zhang et al., survey)\n",
    "- \"Self-Attentive Sequential Recommendation\" (SASRec, Kang & McAuley)\n",
    "\n",
    "**GNNs:**\n",
    "- \"Graph Neural Networks: A Review of Methods and Applications\" (Zhou et al.)\n",
    "- \"How Powerful are Graph Neural Networks?\" (GIN paper, Xu et al.)\n",
    "\n",
    "**Tabular:**\n",
    "- \"TabNet: Attentive Interpretable Tabular Learning\" (Arik & Pfister)\n",
    "- \"Neural Networks for Tabular Data: A Survey\" (Borisov et al.)\n",
    "\n",
    "---\n",
    "\n",
    "## **18.10 Checkpoint Project: Multi-Modal Recommendation Engine**\n",
    "\n",
    "Build a recommendation system combining multiple data types.\n",
    "\n",
    "**Domain:** E-commerce (products with images, text descriptions, categorical attributes, user behavior sequences).\n",
    "\n",
    "**Architecture:**\n",
    "1. **Item Tower:**\n",
    "   - Image: Pre-trained EfficientNet (frozen or fine-tuned)\n",
    "   - Text: BERT embeddings of description\n",
    "   - Tabular: Embeddings for category, brand, price bucket\n",
    "   - Fusion: Concatenate \u2192 MLP \u2192 item embedding\n",
    "\n",
    "2. **User Tower:**\n",
    "   - Sequential: Transformer over past item IDs (SASRec style)\n",
    "   - Context: Time of day, device, location\n",
    "   - Fusion: MLP \u2192 user embedding\n",
    "\n",
    "3. **Training:**\n",
    "   - Batch softmax with in-batch negatives\n",
    "   - Auxiliary losses: Predict category from image (multitask)\n",
    "\n",
    "4. **Serving:**\n",
    "   - FAISS index of item embeddings (IVF for 1M+ items)\n",
    "   - Real-time user embedding computation\n",
    "   - Re-ranking with DeepFM using rich features\n",
    "\n",
    "**Deliverables:**\n",
    "- `multimodal_recsys/` with training and serving code\n",
    "- Evaluation: HitRate@10, MRR (Mean Reciprocal Rank)\n",
    "- Ablation study: Show contribution of each modality (image vs text vs tabular)\n",
    "\n",
    "**Success Criteria:**\n",
    "- Cold-start handling (recommend new items with only image/description)\n",
    "- Latency < 100ms for user embedding + retrieval + ranking\n",
    "- Significant lift over text-only or image-only baselines\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 18**\n",
    "\n",
    "This concludes **Phase 4: Specialized AI Domains**. You now possess expertise across computer vision, NLP, reinforcement learning, and domain-specific applications. **Phase 5: MLOps & Production Engineering** begins with Chapter 19, covering the infrastructure and practices required to deploy these models reliably at scale.\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='17. reinforcement_learning.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../5. MLOPs_and_production_engineering/19. ml_system_design_and_architecture.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}