{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "21fff5c9",
   "metadata": {},
   "source": [
    "Here is **Chapter 10: Neural Network Fundamentals** \u2014 the gateway to deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 10: NEURAL NETWORK FUNDAMENTALS**\n",
    "\n",
    "*The Universal Approximator*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Neural networks represent the most powerful and flexible class of machine learning models. From computer vision to natural language processing, they have revolutionized every domain they touch. This chapter builds the theoretical and practical foundation: starting from the mathematical neuron, through the backpropagation algorithm (the engine of deep learning), to modern training techniques that make deep networks trainable.\n",
    "\n",
    "**Estimated Time:** 60-70 hours (4-5 weeks)  \n",
    "**Prerequisites:** Chapters 1 (Math), 6 (Optimization/Gradient Descent), 9 (Regularization)\n",
    "\n",
    "---\n",
    "\n",
    "## **10.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement a multi-layer neural network from scratch using only NumPy (forward and backward pass)\n",
    "2. Derive and compute gradients using the chain rule for any network architecture\n",
    "3. Select appropriate activation functions and understand their impact on gradient flow\n",
    "4. Apply modern optimization algorithms (Adam, RMSprop) with proper hyperparameter tuning\n",
    "5. Implement regularization techniques (Dropout, Batch Normalization) to prevent overfitting\n",
    "6. Initialize networks effectively to avoid vanishing/exploding gradients\n",
    "\n",
    "---\n",
    "\n",
    "## **10.1 From Biological Inspiration to Artificial Neurons**\n",
    "\n",
    "#### **10.1.1 The Perceptron**\n",
    "\n",
    "The fundamental unit: a weighted sum of inputs passed through an activation function.\n",
    "\n",
    "$$z = \\sum_{i=1}^n w_i x_i + b = \\mathbf{w}^T \\mathbf{x} + b$$\n",
    "\n",
    "$$a = \\sigma(z)$$\n",
    "\n",
    "Where:\n",
    "- $\\mathbf{x} \\in \\mathbb{R}^n$: Input features\n",
    "- $\\mathbf{w} \\in \\mathbb{R}^n$: Weights (synaptic strengths)\n",
    "- $b \\in \\mathbb{R}$: Bias (threshold adjustment)\n",
    "- $\\sigma$: Activation function (non-linearity)\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    def __init__(self, input_dim):\n",
    "        # Xavier initialization (explained later)\n",
    "        self.weights = np.random.randn(input_dim) * np.sqrt(1.0 / input_dim)\n",
    "        self.bias = 0.0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward pass\"\"\"\n",
    "        z = np.dot(self.weights, x) + self.bias\n",
    "        a = self.activation(z)\n",
    "        return a\n",
    "    \n",
    "    def activation(self, z):\n",
    "        \"\"\"Step function (historical) or modern alternatives\"\"\"\n",
    "        return 1 if z > 0 else 0  # Step function\n",
    "        # return 1 / (1 + np.exp(-z))  # Sigmoid (modern)\n",
    "```\n",
    "\n",
    "**Limitation:** Single perceptron is a linear classifier (like logistic regression). Cannot solve XOR problem.\n",
    "\n",
    "#### **10.1.2 Multi-Layer Perceptron (MLP)**\n",
    "\n",
    "Stacking layers creates non-linear decision boundaries. A network with one hidden layer can approximate any continuous function (Universal Approximation Theorem).\n",
    "\n",
    "**Architecture:**\n",
    "- **Input Layer:** Receives data (no computation)\n",
    "- **Hidden Layer(s):** Learned representations\n",
    "- **Output Layer:** Predictions (dimensions = number of classes/outputs)\n",
    "\n",
    "```python\n",
    "class MLP:\n",
    "    def __init__(self, layer_sizes):\n",
    "        \"\"\"\n",
    "        layer_sizes: [input_dim, hidden1, hidden2, ..., output_dim]\n",
    "        \"\"\"\n",
    "        self.layers = []\n",
    "        for i in range(len(layer_sizes) - 1):\n",
    "            in_dim = layer_sizes[i]\n",
    "            out_dim = layer_sizes[i + 1]\n",
    "            # Weight matrix: (in_dim, out_dim)\n",
    "            W = np.random.randn(in_dim, out_dim) * np.sqrt(2.0 / in_dim)  # He init\n",
    "            b = np.zeros(out_dim)\n",
    "            self.layers.append({'W': W, 'b': b})\n",
    "    \n",
    "    def forward(self, X):\n",
    "        \"\"\"Full forward pass through all layers\"\"\"\n",
    "        self.activations = [X]  # Store for backprop\n",
    "        current = X\n",
    "        \n",
    "        for i, layer in enumerate(self.layers):\n",
    "            z = np.dot(current, layer['W']) + layer['b']\n",
    "            # Apply activation (ReLU for hidden, identity/softmax for output)\n",
    "            if i < len(self.layers) - 1:  # Hidden layers\n",
    "                a = np.maximum(0, z)  # ReLU\n",
    "            else:  # Output layer\n",
    "                a = z  # Linear (for regression) or softmax (for classification)\n",
    "            self.activations.append(a)\n",
    "            current = a\n",
    "        \n",
    "        return current\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **10.2 Activation Functions: The Non-Linearity**\n",
    "\n",
    "Without non-linear activations, deep networks collapse to linear models: $\\mathbf{W}_3(\\mathbf{W}_2(\\mathbf{W}_1\\mathbf{x})) = \\mathbf{W}_{eff}\\mathbf{x}$.\n",
    "\n",
    "#### **10.2.1 Sigmoid**\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "**Properties:** Output $\\in (0,1)$, smooth gradient.  \n",
    "**Problems:** \n",
    "- **Vanishing gradient:** Saturation at $\\sigma(z) \\approx 0$ or $1$ gives $\\nabla \\approx 0$.\n",
    "- **Not zero-centered:** Outputs always positive (causes zig-zag dynamics in gradient descent).\n",
    "\n",
    "**Derivative:** $\\sigma'(z) = \\sigma(z)(1 - \\sigma(z))$\n",
    "\n",
    "#### **10.2.2 Tanh (Hyperbolic Tangent)**\n",
    "\n",
    "$$\\tanh(z) = \\frac{e^z - e^{-z}}{e^z + e^{-z}}$$\n",
    "\n",
    "**Properties:** Output $\\in (-1, 1)$, zero-centered (better than sigmoid).  \n",
    "**Problem:** Still suffers from vanishing gradients.\n",
    "\n",
    "#### **10.2.3 ReLU (Rectified Linear Unit)**\n",
    "\n",
    "$$\\text{ReLU}(z) = \\max(0, z)$$\n",
    "\n",
    "**Properties:** \n",
    "- Computationally cheap (no exponentials)\n",
    "- No vanishing gradient for $z > 0$ (constant gradient 1)\n",
    "- Sparse activation (typically 50% of neurons inactive)\n",
    "\n",
    "**Problem:** **Dying ReLU** \u2014 if $z < 0$, gradient is 0, neuron never updates (dead).\n",
    "\n",
    "**Variants:**\n",
    "- **Leaky ReLU:** $\\max(\\alpha z, z)$ where $\\alpha = 0.01$ (small negative slope)\n",
    "- **PReLU:** $\\alpha$ is learned parameter\n",
    "- **ELU:** Smooth version with negative saturation\n",
    "\n",
    "```python\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)  # 1 if z>0, else 0\n",
    "\n",
    "def leaky_relu(z, alpha=0.01):\n",
    "    return np.where(z > 0, z, alpha * z)\n",
    "```\n",
    "\n",
    "#### **10.2.4 Softmax (Output Layer)**\n",
    "\n",
    "For multi-class classification. Converts logits to probability distribution.\n",
    "\n",
    "$$\\text{softmax}(z_i) = \\frac{e^{z_i}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "**Numerical Stability:** Subtract $\\max(z)$ before exponentiation to prevent overflow.\n",
    "\n",
    "```python\n",
    "def softmax(z):\n",
    "    exp_z = np.exp(z - np.max(z, axis=-1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=-1, keepdims=True)\n",
    "```\n",
    "\n",
    "#### **10.2.5 GELU and Swish (Modern)**\n",
    "\n",
    "**GELU (Gaussian Error Linear Unit):** Used in BERT, GPT.\n",
    "\n",
    "$$\\text{GELU}(z) = z \\cdot \\Phi(z) = z \\cdot \\frac{1}{2}\\left[1 + \\text{erf}\\left(\\frac{z}{\\sqrt{2}}\\right)\\right]$$\n",
    "\n",
    "Approximation: $z \\cdot \\sigma(1.702z)$\n",
    "\n",
    "**Swish:** $\\sigma(z) \\cdot z$ (smooth, non-monotonic, often better than ReLU)\n",
    "\n",
    "---\n",
    "\n",
    "## **10.3 Loss Functions**\n",
    "\n",
    "#### **10.3.1 Regression Losses**\n",
    "\n",
    "**Mean Squared Error (MSE):**\n",
    "$$\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Mean Absolute Error (MAE):**\n",
    "$$\\mathcal{L} = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n",
    "\n",
    "**Huber Loss:** Quadratic near zero, linear far from zero (robust to outliers).\n",
    "\n",
    "```python\n",
    "def huber_loss(y_true, y_pred, delta=1.0):\n",
    "    error = y_true - y_pred\n",
    "    is_small_error = np.abs(error) <= delta\n",
    "    squared_loss = 0.5 * error ** 2\n",
    "    linear_loss = delta * (np.abs(error) - 0.5 * delta)\n",
    "    return np.where(is_small_error, squared_loss, linear_loss)\n",
    "```\n",
    "\n",
    "#### **10.3.2 Classification Losses**\n",
    "\n",
    "**Binary Cross-Entropy:**\n",
    "$$\\mathcal{L} = -\\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\right]$$\n",
    "\n",
    "**Categorical Cross-Entropy:**\n",
    "$$\\mathcal{L} = -\\frac{1}{n}\\sum_{i=1}^n \\sum_{k=1}^K y_{i,k} \\log(\\hat{y}_{i,k})$$\n",
    "\n",
    "**Implementation with Numerical Stability:**\n",
    "```python\n",
    "def cross_entropy(y_true, y_pred):\n",
    "    epsilon = 1e-15\n",
    "    y_pred = np.clip(y_pred, epsilon, 1 - epsilon)\n",
    "    return -np.mean(y_true * np.log(y_pred))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **10.4 Backpropagation: Learning by Gradient Descent**\n",
    "\n",
    "The algorithm that makes deep learning possible. Computes gradients of loss w.r.t. all parameters using the **chain rule**.\n",
    "\n",
    "#### **10.4.1 The Chain Rule Review**\n",
    "\n",
    "If $L = f(g(x))$, then $\\frac{dL}{dx} = \\frac{dL}{dg} \\cdot \\frac{dg}{dx}$\n",
    "\n",
    "For neural networks: composite functions of layers.\n",
    "\n",
    "#### **10.4.2 Backprop Algorithm**\n",
    "\n",
    "For layer $l$ with pre-activation $z^{[l]}$ and activation $a^{[l]} = \\sigma(z^{[l]})$:\n",
    "\n",
    "1. **Forward pass:** Compute and cache all $z^{[l]}$, $a^{[l]}$\n",
    "2. **Backward pass (output layer):**\n",
    "   $$\\delta^{[L]} = \\nabla_a L \\odot \\sigma'(z^{[L]})$$\n",
    "   (Element-wise product of loss gradient and activation derivative)\n",
    "   \n",
    "3. **Propagate backwards:**\n",
    "   $$\\delta^{[l]} = ((W^{[l+1]})^T \\delta^{[l+1]}) \\odot \\sigma'(z^{[l]})$$\n",
    "   \n",
    "4. **Compute gradients:**\n",
    "   $$\\nabla_{W^{[l]}} L = (a^{[l-1]})^T \\delta^{[l]}$$\n",
    "   $$\\nabla_{b^{[l]}} L = \\sum_{i} \\delta^{[l]}_{(i)}$$\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "class NeuralNetwork:\n",
    "    def __init__(self, layers):\n",
    "        # Initialize weights...\n",
    "        pass\n",
    "    \n",
    "    def forward(self, X):\n",
    "        self.cache = {'A0': X}\n",
    "        A = X\n",
    "        L = len(self.parameters) // 2\n",
    "        \n",
    "        for l in range(1, L + 1):\n",
    "            W = self.parameters[f'W{l}']\n",
    "            b = self.parameters[f'b{l}']\n",
    "            Z = np.dot(A, W) + b\n",
    "            self.cache[f'Z{l}'] = Z\n",
    "            A = np.maximum(0, Z) if l < L else self.softmax(Z)  # ReLU hidden, softmax output\n",
    "            self.cache[f'A{l}'] = A\n",
    "        \n",
    "        return A\n",
    "    \n",
    "    def backward(self, X, Y, AL):\n",
    "        \"\"\"Compute gradients\"\"\"\n",
    "        gradients = {}\n",
    "        L = len(self.parameters) // 2\n",
    "        m = X.shape[0]\n",
    "        \n",
    "        # Output layer gradient\n",
    "        dZL = AL - Y  # For softmax + cross-entropy, this simplifies nicely\n",
    "        \n",
    "        for l in range(L, 0, -1):\n",
    "            A_prev = self.cache[f'A{l-1}']\n",
    "            \n",
    "            # Gradients for current layer\n",
    "            dW = np.dot(A_prev.T, dZL) / m\n",
    "            db = np.sum(dZL, axis=0, keepdims=True) / m\n",
    "            gradients[f'dW{l}'] = dW\n",
    "            gradients[f'db{l}'] = db\n",
    "            \n",
    "            if l > 1:\n",
    "                W = self.parameters[f'W{l}']\n",
    "                dA_prev = np.dot(dZL, W.T)\n",
    "                Z_prev = self.cache[f'Z{l-1}']\n",
    "                dZL = dA_prev * (Z_prev > 0).astype(float)  # ReLU derivative\n",
    "        \n",
    "        return gradients\n",
    "    \n",
    "    def update_parameters(self, gradients, learning_rate):\n",
    "        L = len(self.parameters) // 2\n",
    "        for l in range(1, L + 1):\n",
    "            self.parameters[f'W{l}'] -= learning_rate * gradients[f'dW{l}']\n",
    "            self.parameters[f'b{l}'] -= learning_rate * gradients[f'db{l}']\n",
    "```\n",
    "\n",
    "#### **10.4.3 Computational Graphs**\n",
    "\n",
    "Modern frameworks (PyTorch, TensorFlow) build dynamic computational graphs:\n",
    "\n",
    "```python\n",
    "# PyTorch automatic differentiation\n",
    "import torch\n",
    "\n",
    "x = torch.tensor([2.0], requires_grad=True)\n",
    "y = x ** 2 + 3 * x + 1\n",
    "y.backward()  # Computes dy/dx automatically\n",
    "print(x.grad)  # dy/dx = 2x + 3 = 7\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **10.5 Optimization Algorithms**\n",
    "\n",
    "#### **10.5.1 Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "Update: $\\theta := \\theta - \\alpha \\nabla_\\theta J(\\theta; x^{(i)}, y^{(i)})$\n",
    "\n",
    "**Variants:**\n",
    "- **Batch GD:** Use all $m$ samples (accurate, slow)\n",
    "- **Mini-batch GD:** Use $b$ samples ($b=32, 64, 128$) \u2014 standard\n",
    "- **SGD:** Use 1 sample (noisy, escapes local minima)\n",
    "\n",
    "#### **10.5.2 Momentum**\n",
    "\n",
    "Accumulate velocity to dampen oscillations:\n",
    "\n",
    "$$v_t = \\beta v_{t-1} + \\nabla_\\theta J(\\theta)$$\n",
    "$$\\theta = \\theta - \\alpha v_t$$\n",
    "\n",
    "$\\beta = 0.9$ typically. Like a ball rolling downhill, accelerates in consistent directions.\n",
    "\n",
    "#### **10.5.3 RMSprop**\n",
    "\n",
    "Adapts learning rate per parameter using moving average of squared gradients.\n",
    "\n",
    "$$s_t = \\beta_2 s_{t-1} + (1 - \\beta_2) (\\nabla_\\theta J)^2$$\n",
    "$$\\theta = \\theta - \\alpha \\frac{\\nabla_\\theta J}{\\sqrt{s_t} + \\epsilon}$$\n",
    "\n",
    "Good for non-stationary objectives (RNNs).\n",
    "\n",
    "#### **10.5.4 Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "Combines Momentum + RMSprop. Default choice for most problems.\n",
    "\n",
    "$$m_t = \\beta_1 m_{t-1} + (1 - \\beta_1) \\nabla_\\theta J \\quad \\text{(first moment)}$$\n",
    "$$v_t = \\beta_2 v_{t-1} + (1 - \\beta_2) (\\nabla_\\theta J)^2 \\quad \\text{(second moment)}$$\n",
    "\n",
    "Bias correction:\n",
    "$$\\hat{m}_t = \\frac{m_t}{1 - \\beta_1^t}, \\quad \\hat{v}_t = \\frac{v_t}{1 - \\beta_2^t}$$\n",
    "\n",
    "Update:\n",
    "$$\\theta = \\theta - \\alpha \\frac{\\hat{m}_t}{\\sqrt{\\hat{v}_t} + \\epsilon}$$\n",
    "\n",
    "**Hyperparameters:** $\\beta_1=0.9$, $\\beta_2=0.999$, $\\epsilon=10^{-8}$\n",
    "\n",
    "```python\n",
    "class Adam:\n",
    "    def __init__(self, params, lr=0.001, beta1=0.9, beta2=0.999, eps=1e-8):\n",
    "        self.params = params\n",
    "        self.lr = lr\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.eps = eps\n",
    "        self.m = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.v = {k: np.zeros_like(v) for k, v in params.items()}\n",
    "        self.t = 0\n",
    "    \n",
    "    def step(self, gradients):\n",
    "        self.t += 1\n",
    "        for key in self.params:\n",
    "            g = gradients[f'd{key}']\n",
    "            \n",
    "            # Update biased first moment estimate\n",
    "            self.m[key] = self.beta1 * self.m[key] + (1 - self.beta1) * g\n",
    "            \n",
    "            # Update biased second raw moment estimate  \n",
    "            self.v[key] = self.beta2 * self.v[key] + (1 - self.beta2) * (g ** 2)\n",
    "            \n",
    "            # Compute bias-corrected estimates\n",
    "            m_hat = self.m[key] / (1 - self.beta1 ** self.t)\n",
    "            v_hat = self.v[key] / (1 - self.beta2 ** self.t)\n",
    "            \n",
    "            # Update parameters\n",
    "            self.params[key] -= self.lr * m_hat / (np.sqrt(v_hat) + self.eps)\n",
    "```\n",
    "\n",
    "#### **10.5.5 Learning Rate Schedules**\n",
    "\n",
    "- **Step Decay:** Reduce by factor every $N$ epochs\n",
    "- **Exponential Decay:** $\\alpha = \\alpha_0 e^{-kt}$\n",
    "- **Cosine Annealing:** $\\alpha_t = \\alpha_{min} + \\frac{1}{2}(\\alpha_{max} - \\alpha_{min})(1 + \\cos(\\frac{t}{T}\\pi))$\n",
    "- **ReduceLROnPlateau:** Reduce when validation loss stops improving\n",
    "\n",
    "---\n",
    "\n",
    "## **10.6 Regularization for Deep Networks**\n",
    "\n",
    "#### **10.6.1 L2 Regularization (Weight Decay)**\n",
    "\n",
    "Add $\\frac{\\lambda}{2}\\|\\mathbf{w}\\|^2$ to loss. Penalizes large weights.\n",
    "\n",
    "**In AdamW:** Decouple weight decay from gradient update (more effective than L2 penalty in Adam).\n",
    "\n",
    "#### **10.6.2 Dropout**\n",
    "\n",
    "Randomly set neurons to zero during training. Forces network to learn redundant representations.\n",
    "\n",
    "$$a^{[l]}_{dropped} = a^{[l]} * \\mathbf{m}, \\quad m_i \\sim \\text{Bernoulli}(p)$$\n",
    "\n",
    "At test time: multiply by $p$ (or scale by $1/p$ during training \u2014 inverted dropout).\n",
    "\n",
    "```python\n",
    "def dropout_forward(X, dropout_rate=0.5):\n",
    "    if dropout_rate == 0:\n",
    "        return X, None\n",
    "    \n",
    "    mask = (np.random.rand(*X.shape) < (1 - dropout_rate)) / (1 - dropout_rate)\n",
    "    out = X * mask\n",
    "    return out, mask  # Cache mask for backward pass\n",
    "\n",
    "def dropout_backward(dout, mask):\n",
    "    return dout * mask\n",
    "```\n",
    "\n",
    "#### **10.6.3 Batch Normalization**\n",
    "\n",
    "Normalize layer inputs to have mean 0, variance 1. Allows higher learning rates, reduces internal covariate shift.\n",
    "\n",
    "$$\\hat{x}^{(k)} = \\frac{x^{(k)} - \\mu_B}{\\sqrt{\\sigma_B^2 + \\epsilon}}$$\n",
    "$$y^{(k)} = \\gamma \\hat{x}^{(k)} + \\beta$$\n",
    "\n",
    "$\\gamma$ and $\\beta$ are learned parameters (allows network to learn identity if needed).\n",
    "\n",
    "**Placement:** Usually after linear layer, before activation.\n",
    "\n",
    "```python\n",
    "def batchnorm_forward(x, gamma, beta, eps=1e-8):\n",
    "    # Mini-batch mean and variance\n",
    "    mu = np.mean(x, axis=0)\n",
    "    var = np.var(x, axis=0)\n",
    "    \n",
    "    # Normalize\n",
    "    x_hat = (x - mu) / np.sqrt(var + eps)\n",
    "    \n",
    "    # Scale and shift\n",
    "    out = gamma * x_hat + beta\n",
    "    \n",
    "    cache = (x, x_hat, mu, var, gamma, eps)\n",
    "    return out, cache\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **10.7 Weight Initialization**\n",
    "\n",
    "Bad initialization can cause vanishing or exploding gradients.\n",
    "\n",
    "#### **10.7.1 Xavier/Glorot Initialization**\n",
    "\n",
    "For sigmoid/tanh. Variance of outputs \u2248 variance of inputs.\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in} + n_{out}}}\\right) \\text{ or } \\mathcal{N}\\left(0, \\sqrt{\\frac{1}{n_{in}}}\\right)$$\n",
    "\n",
    "#### **10.7.2 He Initialization**\n",
    "\n",
    "For ReLU activations. Accounts for ReLU killing half the neurons.\n",
    "\n",
    "$$W \\sim \\mathcal{N}\\left(0, \\sqrt{\\frac{2}{n_{in}}}\\right)$$\n",
    "\n",
    "```python\n",
    "def initialize_weights(shape, method='he'):\n",
    "    fan_in, fan_out = shape\n",
    "    if method == 'xavier':\n",
    "        limit = np.sqrt(6 / (fan_in + fan_out))\n",
    "        return np.random.uniform(-limit, limit, shape)\n",
    "    elif method == 'he':\n",
    "        return np.random.randn(*shape) * np.sqrt(2.0 / fan_in)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **10.8 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Neural Network from Scratch**\n",
    "Implement a 2-layer NN (input \u2192 hidden \u2192 output) using only NumPy on MNIST:\n",
    "1. Forward pass with ReLU and Softmax\n",
    "2. Backpropagation computing all gradients manually\n",
    "3. SGD and Adam optimizers\n",
    "4. Achieve >95% accuracy on test set\n",
    "\n",
    "**Deliverable:** `neural_network_numpy.py` with <5% accuracy gap to sklearn MLP.\n",
    "\n",
    "### **Lab 2: Vanishing Gradient Demonstration**\n",
    "Train deep networks (10+ layers) with:\n",
    "1. Sigmoid activation (should fail/vanish)\n",
    "2. ReLU activation (should train)\n",
    "3. Proper initialization (He) vs random initialization\n",
    "\n",
    "Visualize gradient norms at each layer.\n",
    "\n",
    "**Deliverable:** Plots showing gradient flow comparison.\n",
    "\n",
    "### **Lab 3: Optimizer Comparison**\n",
    "Implement SGD, Momentum, RMSprop, Adam from scratch.\n",
    "Train identical network with each, plot loss curves.\n",
    "Show Adam converges fastest, SGD with momentum finds better minima (generalizes better).\n",
    "\n",
    "**Deliverable:** Comparison notebook with convergence plots.\n",
    "\n",
    "### **Lab 4: Regularization Effects**\n",
    "On CIFAR-10 (or synthetic data):\n",
    "1. Train without regularization (overfitting)\n",
    "2. Add Dropout (0.5)\n",
    "3. Add Batch Normalization\n",
    "4. Combine Dropout + BatchNorm + Weight Decay\n",
    "\n",
    "Measure train/val gap for each.\n",
    "\n",
    "**Deliverable:** Table showing regularization impact on generalization gap.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.9 Common Pitfalls**\n",
    "\n",
    "1. **Not Shuffling Data:** Epochs must shuffle mini-batches. Sequential batches create biased gradients.\n",
    "\n",
    "2. **Incorrect Loss Scaling:** For classification, ensure logits not passed through softmax before CrossEntropyLoss (PyTorch combines them for numerical stability).\n",
    "\n",
    "3. **Learning Rate Too High:** Causes divergence (NaN losses). Too low: never converges. Use LR finder.\n",
    "\n",
    "4. **Forgetting Train Mode vs Eval Mode:** Dropout and BatchNorm behave differently! Call `model.train()` and `model.eval()` in PyTorch.\n",
    "\n",
    "5. **Exploding Gradients:** In RNNs or deep networks. Solution: Gradient clipping.\n",
    "   ```python\n",
    "   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "   ```\n",
    "\n",
    "6. **Testing on Training Data:** Always use separate validation set.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.10 Interview Questions**\n",
    "\n",
    "**Q1:** Why do we need activation functions? What happens if we don't use them?\n",
    "*A: Without activation functions, a composition of linear transformations is still linear: $W_3(W_2(W_1x)) = W_{eff}x$. The network couldn't learn non-linear decision boundaries, regardless of depth. Activations introduce non-linearity, allowing approximation of complex functions.*\n",
    "\n",
    "**Q2:** Explain the vanishing gradient problem and how ReLU helps.\n",
    "*A: In deep networks with sigmoid/tanh, gradients become increasingly small as they propagate backward (derivatives < 1 multiply together, approaching zero). This prevents early layers from learning. ReLU has derivative 1 for positive inputs, allowing gradients to flow unchanged through active neurons, mitigating vanishing gradients.*\n",
    "\n",
    "**Q3:** What is the difference between Batch Norm and Layer Norm?\n",
    "*A: Batch Norm normalizes across the batch dimension (computes mean/var over batch for each feature). Layer Norm normalizes across the feature dimension (computes mean/var over all features for each sample). Batch Norm requires batch statistics; Layer Norm works with batch size 1. Layer Norm is preferred in RNNs and Transformers where sequence lengths vary.*\n",
    "\n",
    "**Q4:** Why does Adam often converge faster than SGD, but SGD sometimes generalizes better?\n",
    "*A: Adam adapts learning rates per parameter and uses momentum, navigating complex loss landscapes faster initially. However, SGD with momentum (and proper LR decay) can find wider, flatter minima that generalize better. Adam's adaptive learning rates might cause it to settle in sharp minima. Solutions: AdamW (decoupled weight decay), or fine-tuning with SGD after Adam pretraining.*\n",
    "\n",
    "**Q5:** Implement backprop for a single linear layer: $y = Wx + b$, loss $L = \\frac{1}{2}(y - t)^2$. Compute $\\frac{\\partial L}{\\partial W}$.\n",
    "*A: $\\frac{\\partial L}{\\partial y} = (y - t)$. $\\frac{\\partial L}{\\partial W} = \\frac{\\partial L}{\\partial y} \\cdot \\frac{\\partial y}{\\partial W} = (y - t) \\cdot x^T$ (outer product if vectors). For batch: $\\frac{\\partial L}{\\partial W} = \\frac{1}{N} (y - t) x^T$.*\n",
    "\n",
    "---\n",
    "\n",
    "## **10.11 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Deep Learning* (Goodfellow, Bengio, Courville) - Chapters 6 (Feedforward), 7 (Regularization), 8 (Optimization)\n",
    "- *Neural Networks and Deep Learning* (Michael Nielsen) - Free online, excellent for backprop intuition\n",
    "\n",
    "**Papers:**\n",
    "- \"ImageNet Classification with Deep CNNs\" (Krizhevsky et al., 2012) - ReLU popularization\n",
    "- \"Batch Normalization\" (Ioffe & Szegedy, 2015)\n",
    "- \"Adam: A Method for Stochastic Optimization\" (Kingma & Ba, 2015)\n",
    "- \"Delving Deep into Rectifiers\" (He et al., 2015) - He initialization\n",
    "\n",
    "---\n",
    "\n",
    "## **10.12 Checkpoint Project: Deep Learning Library (MicroTorch)**\n",
    "\n",
    "Build a miniature PyTorch-like library with automatic differentiation.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Tensor Class:** Wrapper around NumPy array with `requires_grad` flag\n",
    "2. **Operations:** Add, Multiply, MatMul, ReLU, Softmax\n",
    "3. **Autograd:** Build computation graph dynamically, implement backward() using reverse-mode autodiff\n",
    "4. **Optimizer:** SGD and Adam implementations\n",
    "5. **NN Module:** Linear layers, Sequential container\n",
    "6. **Training Loop:** Fit on MNIST, achieve >90% accuracy\n",
    "\n",
    "**API Design:**\n",
    "```python\n",
    "from microtorch import Tensor, nn, optim\n",
    "\n",
    "# Should work like PyTorch\n",
    "x = Tensor(np.random.randn(32, 784), requires_grad=True)\n",
    "model = nn.Sequential([\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Linear(256, 10)\n",
    "])\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training\n",
    "for epoch in range(10):\n",
    "    optimizer.zero_grad()\n",
    "    logits = model(x)\n",
    "    loss = nn.cross_entropy(logits, y)\n",
    "    loss.backward()  # Compute all gradients automatically\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "**Deliverables:**\n",
    "- `microtorch/` package with `tensor.py`, `nn.py`, `optim.py`\n",
    "- Tests comparing gradients to numerical differentiation (finite differences)\n",
    "- MNIST training script\n",
    "\n",
    "**Success Criteria:**\n",
    "- Backward pass computes correct gradients (within 1e-5 of numerical)\n",
    "- Training converges on MNIST\n",
    "- Memory efficient (no storing unnecessary intermediate values)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 10**\n",
    "\n",
    "*You now understand how neural networks learn. Chapter 11 will cover Deep Learning Frameworks (PyTorch, TensorFlow, JAX) and how to use them efficiently for large-scale models.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../2. Machine_learning_fundamentals/9. model_evaluation_validation_and_selection.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='11. deep_learning_frameworks.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}