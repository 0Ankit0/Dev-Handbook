{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f665a2e6",
   "metadata": {},
   "source": [
    "Here is **Chapter 11: Deep Learning Frameworks** \u2014 mastering the tools of modern AI.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 11: DEEP LEARNING FRAMEWORKS**\n",
    "\n",
    "*The Engines of Deep Learning*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Building neural networks from scratch taught you the mathematics; now you need industrial-strength tools. PyTorch dominates research, TensorFlow powers production, and JAX represents the future of high-performance computing. This chapter makes you fluent in all three, with emphasis on writing efficient, scalable, and maintainable deep learning code.\n",
    "\n",
    "**Estimated Time:** 50-60 hours (3-4 weeks)  \n",
    "**Prerequisites:** Chapter 10 (Neural Network fundamentals), strong Python skills\n",
    "\n",
    "---\n",
    "\n",
    "## **11.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Build and train models in PyTorch using both imperative and object-oriented patterns\n",
    "2. Utilize TensorFlow's ecosystem (Keras, tf.data, TF Serving) for production pipelines\n",
    "3. Leverage JAX for high-performance research code with functional transformations (jit, vmap, grad)\n",
    "4. Implement distributed training across multiple GPUs (DDP, FSDP, data/model parallelism)\n",
    "5. Export models for production deployment (ONNX, TorchScript, SavedModel)\n",
    "6. Profile and optimize training throughput (mixed precision, gradient accumulation, efficient data loading)\n",
    "\n",
    "---\n",
    "\n",
    "## **11.1 PyTorch: The Research Standard**\n",
    "\n",
    "PyTorch's dynamic computation graphs and Pythonic design make it the preferred choice for research and rapid prototyping.\n",
    "\n",
    "#### **11.1.1 Tensors and Autograd**\n",
    "\n",
    "Tensors are n-dimensional arrays with automatic differentiation capabilities.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Creation\n",
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "x = torch.randn(3, 3, device='cuda', dtype=torch.float32)  # GPU tensor\n",
    "x = torch.zeros(2, 3).to('mps')  # Apple Silicon\n",
    "\n",
    "# Operations (automatically tracked if requires_grad=True)\n",
    "y = x ** 2 + 3 * x + 1\n",
    "z = y.mean()\n",
    "\n",
    "# Backward pass\n",
    "z.backward()  # Computes dz/dx for all tensors with requires_grad=True\n",
    "print(x.grad)  # Gradient of z w.r.t. x\n",
    "\n",
    "# Detaching (stopping gradient flow)\n",
    "with torch.no_grad():\n",
    "    evaluation = model(x)  # No graph built, saves memory\n",
    "    \n",
    "# Or: x.detach() creates new tensor without grad history\n",
    "```\n",
    "\n",
    "**In-place Operations:** Avoid when possible (breaks autograd graph). Operations ending with `_` (e.g., `add_()`, `zero_()`) are in-place.\n",
    "\n",
    "#### **11.1.2 nn.Module and Model Definition**\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, 3, stride, 1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, 3, 1, 1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        \n",
    "        # Shortcut connection\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, 1, stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        identity = self.shortcut(x)\n",
    "        \n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "        \n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        \n",
    "        out += identity  # Residual connection\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "# Sequential API for simple models\n",
    "simple_model = nn.Sequential(\n",
    "    nn.Linear(784, 256),\n",
    "    nn.ReLU(),\n",
    "    nn.Dropout(0.5),\n",
    "    nn.Linear(256, 10)\n",
    ")\n",
    "```\n",
    "\n",
    "#### **11.1.3 Training Loop (The PyTorch Way)**\n",
    "\n",
    "```python\n",
    "def train_epoch(model, dataloader, criterion, optimizer, device):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(dataloader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        # Zero gradients\n",
    "        optimizer.zero_grad()  # or optimizer.zero_grad(set_to_none=True) for speed\n",
    "        \n",
    "        # Forward\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        \n",
    "        # Backward\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clipping (optional)\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "        \n",
    "        # Update\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Metrics\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = outputs.max(1)\n",
    "        total += targets.size(0)\n",
    "        correct += predicted.eq(targets).sum().item()\n",
    "        \n",
    "        if batch_idx % 100 == 0:\n",
    "            print(f'Batch {batch_idx}, Loss: {loss.item():.4f}')\n",
    "    \n",
    "    return running_loss / len(dataloader), 100. * correct / total\n",
    "\n",
    "# Learning rate scheduling\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=200)\n",
    "# Or: ReduceLROnPlateau, StepLR, OneCycleLR\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, trainloader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(model, valloader, criterion, device)\n",
    "    scheduler.step()\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'loss': loss,\n",
    "    }, 'checkpoint.pth')\n",
    "```\n",
    "\n",
    "#### **11.1.4 Data Loading and Transforms**\n",
    "\n",
    "```python\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, csv_file, transform=None):\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        image = load_image(self.data.iloc[idx]['path'])\n",
    "        label = self.data.iloc[idx]['label']\n",
    "        \n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "            \n",
    "        return image, label\n",
    "\n",
    "# Transforms pipeline\n",
    "transform = transforms.Compose([\n",
    "    transforms.RandomResizedCrop(224),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.ColorJitter(brightness=0.2, contrast=0.2),\n",
    "    transforms.ToTensor(),  # Converts PIL [0,255] to Tensor [0,1]\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], \n",
    "                        std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# DataLoader\n",
    "trainloader = DataLoader(\n",
    "    dataset, \n",
    "    batch_size=64, \n",
    "    shuffle=True, \n",
    "    num_workers=4,      # Parallel data loading\n",
    "    pin_memory=True,    # Speeds up CPU->GPU transfer\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=2   # Batches to prefetch per worker\n",
    ")\n",
    "```\n",
    "\n",
    "#### **11.1.5 Mixed Precision Training**\n",
    "\n",
    "Uses FP16 (half precision) for compute, FP32 for master weights. 2-3x speedup on modern GPUs.\n",
    "\n",
    "```python\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for inputs, targets in dataloader:\n",
    "    inputs, targets = inputs.cuda(), targets.cuda()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Automatic Mixed Precision\n",
    "    with autocast():\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "    \n",
    "    # Scale loss, backward, unscale, step\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **11.2 TensorFlow and Keras: The Production Ecosystem**\n",
    "\n",
    "TensorFlow excels in production deployment, mobile optimization (TFLite), and scalable data pipelines (tf.data).\n",
    "\n",
    "#### **11.2.1 Keras Functional API**\n",
    "\n",
    "More flexible than Sequential API for multi-input/multi-output models.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Functional API\n",
    "inputs = keras.Input(shape=(784,), name='img')\n",
    "x = layers.Dense(64, activation='relu')(inputs)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(10, activation='softmax', name='predictions')(x)\n",
    "\n",
    "model = keras.Model(inputs=inputs, outputs=outputs, name='mnist_model')\n",
    "\n",
    "# Multi-input example\n",
    "text_input = keras.Input(shape=(None,), dtype='int32', name='text')\n",
    "image_input = keras.Input(shape=(224, 224, 3), name='image')\n",
    "\n",
    "# Process each\n",
    "x1 = layers.Embedding(10000, 128)(text_input)\n",
    "x1 = layers.LSTM(64)(x1)\n",
    "\n",
    "x2 = layers.Conv2D(32, 3, activation='relu')(image_input)\n",
    "x2 = layers.GlobalMaxPooling2D()(x2)\n",
    "\n",
    "# Combine\n",
    "combined = layers.concatenate([x1, x2])\n",
    "output = layers.Dense(1, activation='sigmoid', name='output')(combined)\n",
    "\n",
    "model = keras.Model(inputs=[text_input, image_input], outputs=output)\n",
    "```\n",
    "\n",
    "#### **11.2.2 Custom Training with tf.GradientTape**\n",
    "\n",
    "PyTorch-style imperative training in TensorFlow.\n",
    "\n",
    "```python\n",
    "optimizer = keras.optimizers.Adam(learning_rate=1e-3)\n",
    "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "train_acc = keras.metrics.SparseCategoricalAccuracy()\n",
    "\n",
    "@tf.function  # Compiles to graph (speeds up execution)\n",
    "def train_step(x, y):\n",
    "    with tf.GradientTape() as tape:\n",
    "        logits = model(x, training=True)\n",
    "        loss_value = loss_fn(y, logits)\n",
    "    \n",
    "    grads = tape.gradient(loss_value, model.trainable_weights)\n",
    "    optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
    "    \n",
    "    train_acc.update_state(y, logits)\n",
    "    return loss_value\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):\n",
    "    for step, (x_batch, y_batch) in enumerate(train_dataset):\n",
    "        loss_value = train_step(x_batch, y_batch)\n",
    "        if step % 200 == 0:\n",
    "            print(f'Epoch {epoch}, Step {step}, Loss: {loss_value:.4f}')\n",
    "```\n",
    "\n",
    "#### **11.2.3 tf.data for High-Performance Input Pipelines**\n",
    "\n",
    "```python\n",
    "def preprocess(image, label):\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    image = tf.image.resize(image, [224, 224])\n",
    "    image = image / 255.0\n",
    "    return image, label\n",
    "\n",
    "# Create dataset\n",
    "list_ds = tf.data.Dataset.list_files('path/to/data/*.jpg')\n",
    "labeled_ds = list_ds.map(lambda x: (load_image(x), get_label(x)))\n",
    "\n",
    "# Optimization pipeline\n",
    "dataset = labeled_ds.cache()  # Cache in memory after first epoch\n",
    "dataset = dataset.shuffle(buffer_size=1000)\n",
    "dataset = dataset.map(preprocess, num_parallel_calls=tf.data.AUTOTUNE)\n",
    "dataset = dataset.batch(32)\n",
    "dataset = dataset.prefetch(tf.data.AUTOTUNE)  # Overlap preprocessing and training\n",
    "\n",
    "# For very large datasets: cache to file instead of memory\n",
    "dataset = dataset.cache(filename='./cache.tf-data')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **11.3 JAX: The Future of High-Performance ML**\n",
    "\n",
    "JAX combines NumPy's syntax with XLA (Accelerated Linear Algebra) compilation for TPUs/GPUs, plus automatic differentiation and vectorization.\n",
    "\n",
    "#### **11.3.1 Functional Approach**\n",
    "\n",
    "```python\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "from jax import grad, jit, vmap\n",
    "from flax import linen as nn  # Neural network library\n",
    "import optax  # Optimization library\n",
    "\n",
    "# Pure functions (no side effects)\n",
    "def predict(params, inputs):\n",
    "    for w, b in params:\n",
    "        outputs = jnp.dot(inputs, w) + b\n",
    "        inputs = jnp.maximum(0, outputs)  # ReLU\n",
    "    return outputs\n",
    "\n",
    "# Gradients\n",
    "def loss_fn(params, x, y):\n",
    "    pred = predict(params, x)\n",
    "    return jnp.mean((pred - y) ** 2)\n",
    "\n",
    "grad_fn = jit(grad(loss_fn))  # JIT compiles to optimized XLA\n",
    "grads = grad_fn(params, x, y)\n",
    "```\n",
    "\n",
    "#### **11.3.2 Vectorization with vmap**\n",
    "\n",
    "Automatically batch operations without manual loops.\n",
    "\n",
    "```python\n",
    "# Without vmap: loop over batch\n",
    "def apply_model_single(params, x):\n",
    "    return predict(params, x)\n",
    "\n",
    "batch_predictions = [apply_model_single(params, x) for x in batch]\n",
    "\n",
    "# With vmap: automatic batching\n",
    "batch_predict = vmap(apply_model_single, in_axes=(None, 0))\n",
    "predictions = batch_predict(params, batch)  # Shape (batch_size, output_dim)\n",
    "```\n",
    "\n",
    "#### **11.3.3 Neural Networks with Flax**\n",
    "\n",
    "```python\n",
    "class MLP(nn.Module):\n",
    "    features: tuple = (256, 256, 10)\n",
    "    \n",
    "    @nn.compact\n",
    "    def __call__(self, x):\n",
    "        for feat in self.features[:-1]:\n",
    "            x = nn.Dense(feat)(x)\n",
    "            x = nn.relu(x)\n",
    "        x = nn.Dense(self.features[-1])(x)\n",
    "        return x\n",
    "\n",
    "# Initialize\n",
    "model = MLP(features=[256, 256, 10])\n",
    "params = model.init(jax.random.PRNGKey(0), jnp.ones((1, 784)))\n",
    "\n",
    "# Training step\n",
    "@jit\n",
    "def train_step(params, opt_state, x, y):\n",
    "    def loss_fn(params):\n",
    "        pred = model.apply(params, x)\n",
    "        return jnp.mean(optax.softmax_cross_entropy(pred, y))\n",
    "    \n",
    "    loss, grads = jax.value_and_grad(loss_fn)(params)\n",
    "    updates, opt_state = optimizer.update(grads, opt_state)\n",
    "    params = optax.apply_updates(params, updates)\n",
    "    return params, opt_state, loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **11.4 Distributed Training**\n",
    "\n",
    "#### **11.4.1 Data Parallelism (DDP - DistributedDataParallel)**\n",
    "\n",
    "Each GPU has copy of model, processes different batch slice, gradients synchronized.\n",
    "\n",
    "**PyTorch DDP:**\n",
    "```python\n",
    "import torch.multiprocessing as mp\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.distributed import init_process_group, destroy_process_group\n",
    "\n",
    "def setup(rank, world_size):\n",
    "    os.environ['MASTER_ADDR'] = 'localhost'\n",
    "    os.environ['MASTER_PORT'] = '12355'\n",
    "    init_process_group(\"nccl\", rank=rank, world_size=world_size)\n",
    "\n",
    "def train(rank, world_size):\n",
    "    setup(rank, world_size)\n",
    "    model = MyModel().to(rank)\n",
    "    ddp_model = DDP(model, device_ids=[rank])\n",
    "    \n",
    "    # DataLoader with DistributedSampler\n",
    "    sampler = torch.utils.data.DistributedSampler(dataset)\n",
    "    dataloader = DataLoader(dataset, sampler=sampler, ...)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        sampler.set_epoch(epoch)  # Important for proper shuffling\n",
    "        for batch in dataloader:\n",
    "            # Training loop\n",
    "            pass\n",
    "    \n",
    "    destroy_process_group()\n",
    "\n",
    "# Launch\n",
    "world_size = torch.cuda.device_count()\n",
    "mp.spawn(train, args=(world_size,), nprocs=world_size, join=True)\n",
    "```\n",
    "\n",
    "#### **11.4.2 Fully Sharded Data Parallel (FSDP)**\n",
    "\n",
    "Shards model parameters across GPUs (needed for models > 1B parameters that don't fit in single GPU memory).\n",
    "\n",
    "```python\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "\n",
    "model = FSDP(\n",
    "    model,\n",
    "    auto_wrap_policy=transformer_auto_wrap_policy,\n",
    "    device_id=torch.cuda.current_device(),\n",
    "    limit_all_gathers=True  # Reduce memory pressure\n",
    ")\n",
    "```\n",
    "\n",
    "#### **11.4.3 Model Parallelism**\n",
    "\n",
    "Split model layers across GPUs (for massive models like GPT-3).\n",
    "\n",
    "```python\n",
    "# Simple pipeline parallelism\n",
    "device_0 = torch.device('cuda:0')\n",
    "device_1 = torch.device('cuda:1')\n",
    "\n",
    "class ModelParallel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.layer1 = nn.Linear(784, 256).to(device_0)\n",
    "        self.layer2 = nn.Linear(256, 10).to(device_1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.to(device_0)\n",
    "        x = self.layer1(x)\n",
    "        x = x.to(device_1)\n",
    "        x = self.layer2(x)\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **11.5 Model Export and Deployment**\n",
    "\n",
    "#### **11.5.1 PyTorch: TorchScript and ONNX**\n",
    "\n",
    "**TorchScript (for C++ deployment):**\n",
    "```python\n",
    "# Tracing\n",
    "model.eval()\n",
    "example_input = torch.rand(1, 3, 224, 224)\n",
    "traced_script_module = torch.jit.trace(model, example_input)\n",
    "traced_script_module.save(\"model.pt\")\n",
    "\n",
    "# Scripting (for control flow)\n",
    "scripted_model = torch.jit.script(model)\n",
    "```\n",
    "\n",
    "**ONNX (framework-agnostic):**\n",
    "```python\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    example_input,\n",
    "    \"model.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=11,\n",
    "    input_names=['input'],\n",
    "    output_names=['output'],\n",
    "    dynamic_axes={'input': {0: 'batch_size'}, 'output': {0: 'batch_size'}}\n",
    ")\n",
    "```\n",
    "\n",
    "#### **11.5.2 TensorFlow: SavedModel**\n",
    "\n",
    "```python\n",
    "# Save the complete model\n",
    "model.save('saved_model/my_model')  # SavedModel format\n",
    "\n",
    "# Load for inference\n",
    "loaded_model = keras.models.load_model('saved_model/my_model')\n",
    "\n",
    "# Convert to TensorFlow Lite (mobile/edge)\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model('saved_model/my_model')\n",
    "tflite_model = converter.convert()\n",
    "open('model.tflite', 'wb').write(tflite_model)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **11.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: PyTorch CIFAR-10 Classifier**\n",
    "Build ResNet-18 from scratch (no torchvision.models):\n",
    "1. Implement BasicBlock with residual connections\n",
    "2. Train with mixed precision\n",
    "3. Implement custom learning rate scheduler (warmup + cosine decay)\n",
    "4. Achieve >85% accuracy\n",
    "\n",
    "**Deliverable:** Training script with tensorboard logging.\n",
    "\n",
    "### **Lab 2: TensorFlow Production Pipeline**\n",
    "Build tf.data pipeline for large image dataset (>100GB):\n",
    "1. TFRecord creation and reading\n",
    "2. Data augmentation in graph (tf.image)\n",
    "3. Train with distribution strategy (MultiWorkerMirroredStrategy)\n",
    "4. Export SavedModel with serving signatures\n",
    "\n",
    "**Deliverable:** End-to-end pipeline with throughput benchmarks (images/second).\n",
    "\n",
    "### **Lab 3: JAX vs PyTorch Comparison**\n",
    "Implement same transformer block in both frameworks:\n",
    "1. Measure forward and backward pass speed\n",
    "2. Implement custom gradient (stop gradient for certain ops)\n",
    "3. Use vmap for batched inference comparison\n",
    "\n",
    "**Deliverable:** Benchmark report showing JAX XLA advantages.\n",
    "\n",
    "### **Lab 4: Distributed Training from Scratch**\n",
    "Train ResNet-50 on ImageNet subset using PyTorch DDP:\n",
    "1. Multi-GPU on single node\n",
    "2. Gradient accumulation to simulate large batch\n",
    "3. Checkpoint saving/loading for fault tolerance\n",
    "4. Measure scaling efficiency (1 GPU time vs N GPU time)\n",
    "\n",
    "**Deliverable:** Distributed training script with 80%+ scaling efficiency (i.e., 4 GPUs should be ~3.2x faster than 1).\n",
    "\n",
    "---\n",
    "\n",
    "## **11.7 Common Pitfalls**\n",
    "\n",
    "1. **CUDA OOM (Out of Memory):**\n",
    "   - Use `torch.cuda.empty_cache()` between epochs (rarely needed, usually symptom of bug)\n",
    "   - Reduce batch size or use gradient accumulation\n",
    "   - Check for retained computation graph: `loss.backward()` not `total_loss += loss` (retains graph!)\n",
    "\n",
    "2. **DDP Hanging:**\n",
    "   - Ensure all processes reach same number of backward calls\n",
    "   - Check `set_epoch()` on DistributedSampler\n",
    "   - Ensure same batch size on all GPUs (drop_last=True if needed)\n",
    "\n",
    "3. **TF Data Pipeline Bottlenecks:**\n",
    "   - Use `tf.data.AUTOTUNE` for num_parallel_calls and prefetch\n",
    "   - Profile with `tf.data.experimental.Analysis`\n",
    "\n",
    "4. **JAX Random Number Gotcha:**\n",
    "   - JAX requires explicit PRNGKey management (not stateful like NumPy). Split keys properly!\n",
    "\n",
    "5. **Mixed Precision Underflow:**\n",
    "   - Some layers (BatchNorm, Softmax) must stay in FP32. `autocast` handles this mostly, but check losses don't become NaN.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.8 Interview Questions**\n",
    "\n",
    "**Q1:** What is the difference between PyTorch's DataParallel and DistributedDataParallel (DDP)?\n",
    "*A: DataParallel is single-process, multi-threaded, replicates model on each GPU, and scatters/gathers data. It's slower due to GIL contention and GPU 0 bottleneck. DDP is multi-process, each GPU has its own process, uses ring-allreduce for gradient synchronization, and is significantly faster. DDP is the production standard.*\n",
    "\n",
    "**Q2:** Explain XLA and why JAX uses it. What are its advantages?\n",
    "*A: XLA (Accelerated Linear Algebra) is a domain-specific compiler for linear algebra that optimizes computations by fusing operations, eliminating intermediate allocations, and generating efficient machine code. JAX uses XLA to compile functional programs to GPU/TPU, enabling aggressive optimization across operations (fusion) that aren't possible with eager mode. Benefits: better memory usage, kernel fusion, and TPU support.*\n",
    "\n",
    "**Q3:** What is gradient accumulation and when would you use it?\n",
    "*A: Gradient accumulation splits a large logical batch into smaller micro-batches that fit in GPU memory. Forward/backward is run on each micro-batch, gradients are accumulated (summed), and optimizer step happens after N micro-batches. Used when you want large batch training (for batch norm stability or convergence properties) but don't have enough GPU memory for the full batch.*\n",
    "\n",
    "**Q4:** How do you debug a distributed training job that hangs?\n",
    "*A: 1) Check all ranks are initialized (print rank at start). 2) Ensure same number of backward calls on all ranks (uneven data causes deadlock). 3) Use `torch.distributed.barrier()` to identify where hang occurs. 4) Check NCCL environment variables (timeouts). 5) Ensure no process crashed leaving others waiting. 6) Use single GPU mode to verify logic works first.*\n",
    "\n",
    "**Q5:** When would you choose TorchScript over ONNX for model deployment?\n",
    "*A: TorchScript is better for PyTorch-specific features (custom ops, complex control flow) and when deploying to PyTorch Mobile/C++ environments. ONNX is better for cross-framework deployment (deploying to TensorRT, OpenVINO, or non-PyTorch runtimes) and when you need optimization tools specific to ONNX ecosystem. TorchScript preserves PyTorch semantics; ONNX is a generic exchange format.*\n",
    "\n",
    "---\n",
    "\n",
    "## **11.9 Further Reading**\n",
    "\n",
    "**Documentation:**\n",
    "- PyTorch Performance Tuning Guide: https://pytorch.org/tutorials/recipes/recipes/tuning_guide.html\n",
    "- JAX The Sharp Bits: https://jax.readthedocs.io/en/latest/notebooks/Common_Gotchas_in_JAX.html\n",
    "- TensorFlow Performance Guide: https://www.tensorflow.org/guide/profiler\n",
    "\n",
    "**Papers:**\n",
    "- \"PyTorch Distributed: Experiences on Accelerating Data Parallel Training\" (Li et al., 2020)\n",
    "- \"JAX: Composable transformations of Python+NumPy programs\" (Bradbury et al.)\n",
    "\n",
    "---\n",
    "\n",
    "## **11.10 Checkpoint Project: Multi-Framework Model Zoo**\n",
    "\n",
    "Implement identical ResNet-50 architecture in all three frameworks and benchmark.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Model Specification:**\n",
    "   - ResNet-50 v1.5 architecture\n",
    "   - ImageNet preprocessing (same in all frameworks)\n",
    "   - Synchronized BatchNorm (for distributed)\n",
    "\n",
    "2. **Implementations:**\n",
    "   - **PyTorch:** Native nn.Module, DDP training\n",
    "   - **TensorFlow:** Keras Model with custom training loop or fit()\n",
    "   - **JAX:** Flax linen module, pmap for multi-GPU\n",
    "\n",
    "3. **Benchmarking:**\n",
    "   - Throughput (images/sec) single GPU and 4-GPU\n",
    "   - Memory usage\n",
    "   - Lines of code (complexity metric)\n",
    "   - Ease of debugging (profile one iteration)\n",
    "\n",
    "4. **Interoperability:**\n",
    "   - Export PyTorch model to ONNX\n",
    "   - Run ONNX in TensorFlow (onnx-tf)\n",
    "   - Compare outputs (numerical accuracy)\n",
    "\n",
    "**Deliverable:**\n",
    "- `benchmarks/` directory with three implementations\n",
    "- `report.md` with performance tables and analysis\n",
    "- Docker Compose setup to run all three with fixed dependencies\n",
    "\n",
    "**Success Criteria:**\n",
    "- Within 5% accuracy of each other on ImageNet validation (top-1)\n",
    "- JAX achieves highest throughput (XLA optimization)\n",
    "- PyTorch easiest to debug (dynamic nature)\n",
    "- TensorFlow best for production deployment features\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 11**\n",
    "\n",
    "*You now command the tools of modern deep learning. Chapter 12 will cover Convolutional Neural Networks (CNNs) for computer vision.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='10. neural_network_fundamentals.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='12. convolutional_neural_networks.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}