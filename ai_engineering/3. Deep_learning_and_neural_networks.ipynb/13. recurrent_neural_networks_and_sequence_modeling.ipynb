{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9a18e41",
   "metadata": {},
   "source": [
    "Here is **Chapter 13: Recurrent Neural Networks & Sequence Modeling** \u2014 understanding sequential data.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 13: RECURRENT NEURAL NETWORKS & SEQUENCE MODELING**\n",
    "\n",
    "*Memory and Sequence*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Not all data is independent and identically distributed. Time series, natural language, DNA sequences, and user clickstreams all exhibit temporal dependencies. Recurrent Neural Networks (RNNs) process sequences by maintaining hidden state, but suffer from short-term memory limitations. This chapter explores LSTM and GRU architectures that solved these limitations, attention mechanisms that revolutionized machine translation, and the foundations upon which Transformers were built.\n",
    "\n",
    "**Estimated Time:** 50-60 hours (3-4 weeks)  \n",
    "**Prerequisites:** Chapters 10-11 (Neural networks, backpropagation, PyTorch)\n",
    "\n",
    "---\n",
    "\n",
    "## **13.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement vanilla RNNs, LSTMs, and GRUs from scratch and understand their computational graphs\n",
    "2. Apply Backpropagation Through Time (BPTT) and handle vanishing/exploding gradients in sequences\n",
    "3. Build sequence-to-sequence models with encoder-decoder architectures for translation and summarization\n",
    "4. Implement attention mechanisms to overcome bottleneck limitations of fixed-size context vectors\n",
    "5. Process variable-length sequences with padding, packing, and masking techniques\n",
    "6. Apply RNNs to time series forecasting, named entity recognition (NER), and text generation\n",
    "\n",
    "---\n",
    "\n",
    "## **13.1 The Recurrent Neural Network (RNN)**\n",
    "\n",
    "#### **13.1.1 Sequential Processing**\n",
    "\n",
    "Unlike feedforward networks, RNNs share parameters across time steps and maintain hidden state.\n",
    "\n",
    "$$h_t = \\tanh(W_{hh} h_{t-1} + W_{xh} x_t + b_h)$$\n",
    "$$\\hat{y}_t = W_{hy} h_t + b_y$$\n",
    "\n",
    "Where:\n",
    "- $x_t$: Input at time $t$\n",
    "- $h_t$: Hidden state (memory) at time $t$\n",
    "- $W_{hh}, W_{xh}$: Shared weight matrices across all time steps\n",
    "- $\\hat{y}_t$: Output at time $t$\n",
    "\n",
    "**Computational Graph:** Unfolded across time, forms a deep network with shared weights.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VanillaRNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, output_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combined weights for efficiency: [W_xh | W_hh]\n",
    "        self.i2h = nn.Linear(input_size + hidden_size, hidden_size)\n",
    "        self.i2o = nn.Linear(input_size + hidden_size, output_size)\n",
    "        self.tanh = nn.Tanh()\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        combined = torch.cat((input, hidden), dim=1)\n",
    "        hidden = self.tanh(self.i2h(combined))\n",
    "        output = self.i2o(combined)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self, batch_size):\n",
    "        return torch.zeros(batch_size, self.hidden_size)\n",
    "```\n",
    "\n",
    "#### **13.1.2 Backpropagation Through Time (BPTT)**\n",
    "\n",
    "To compute gradients, we unroll the network through time and apply backpropagation. However, this creates a very deep computational graph (depth = sequence length).\n",
    "\n",
    "**The Problem:**\n",
    "- **Vanishing Gradients:** Gradients shrink exponentially as they propagate backward through time ($\\tanh$ derivatives < 1 multiplied repeatedly)\n",
    "- **Exploding Gradients:** Gradients grow exponentially (> 1 repeatedly), causing NaN updates\n",
    "\n",
    "**Solutions:**\n",
    "1. **Gradient Clipping:** Limit gradient norm\n",
    "   ```python\n",
    "   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)\n",
    "   ```\n",
    "2. **Truncated BPTT:** Only backpropagate through last $k$ time steps (approximate gradient)\n",
    "3. **Architectural Solutions:** LSTM, GRU (gated mechanisms)\n",
    "\n",
    "---\n",
    "\n",
    "## **13.2 Long Short-Term Memory (LSTM)**\n",
    "\n",
    "#### **13.2.1 The Cell State**\n",
    "\n",
    "LSTMs maintain two vectors:\n",
    "- **Cell State ($C_t$):** The \"conveyor belt\" that runs through the entire chain with minimal interactions (preserves long-term memory)\n",
    "- **Hidden State ($h_t$):** Working memory, output of the cell\n",
    "\n",
    "#### **13.2.2 The Gates**\n",
    "\n",
    "Three sigmoid gates control information flow:\n",
    "\n",
    "1. **Forget Gate ($f_t$):** What to discard from cell state\n",
    "   $$f_t = \\sigma(W_f \\cdot [h_{t-1}, x_t] + b_f)$$\n",
    "\n",
    "2. **Input Gate ($i_t$):** What new information to store\n",
    "   $$i_t = \\sigma(W_i \\cdot [h_{t-1}, x_t] + b_i)$$\n",
    "   $$\\tilde{C}_t = \\tanh(W_C \\cdot [h_{t-1}, x_t] + b_C)$$\n",
    "\n",
    "3. **Update Cell State:**\n",
    "   $$C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$$\n",
    "\n",
    "4. **Output Gate ($o_t$):** What to output based on cell state\n",
    "   $$o_t = \\sigma(W_o \\cdot [h_{t-1}, x_t] + b_o)$$\n",
    "   $$h_t = o_t \\odot \\tanh(C_t)$$\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "class LSTMCell(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "        \n",
    "        # Combined linear transformation for all gates\n",
    "        self.gates = nn.Linear(input_size + hidden_size, 4 * hidden_size)\n",
    "        \n",
    "    def forward(self, x, hidden):\n",
    "        h_prev, c_prev = hidden\n",
    "        \n",
    "        combined = torch.cat([x, h_prev], dim=1)\n",
    "        gates = self.gates(combined)\n",
    "        \n",
    "        # Split into forget, input, cell_candidate, output\n",
    "        f, i, c_tilde, o = gates.chunk(4, dim=1)\n",
    "        \n",
    "        f = torch.sigmoid(f)  # Forget gate\n",
    "        i = torch.sigmoid(i)  # Input gate\n",
    "        c_tilde = torch.tanh(c_tilde)  # Candidate values\n",
    "        o = torch.sigmoid(o)  # Output gate\n",
    "        \n",
    "        c = f * c_prev + i * c_tilde  # Cell state update\n",
    "        h = o * torch.tanh(c)  # Hidden state\n",
    "        \n",
    "        return h, (h, c)\n",
    "```\n",
    "\n",
    "#### **13.2.3 GRU (Gated Recurrent Unit)**\n",
    "\n",
    "Simplified LSTM with fewer gates (faster, fewer parameters).\n",
    "\n",
    "- **Update Gate ($z_t$):** Controls how much past information to keep (like forget + input)\n",
    "- **Reset Gate ($r_t$):** Controls how much past to forget for computing new candidate\n",
    "\n",
    "$$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$$\n",
    "$$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$$\n",
    "$$\\tilde{h}_t = \\tanh(W \\cdot [r_t \\odot h_{t-1}, x_t])$$\n",
    "$$h_t = (1 - z_t) \\odot h_{t-1} + z_t \\odot \\tilde{h}_t$$\n",
    "\n",
    "**LSTM vs GRU:**\n",
    "- **LSTM:** More powerful, better for long sequences, more parameters\n",
    "- **GRU:** Faster, works well for shorter sequences, good default choice\n",
    "\n",
    "---\n",
    "\n",
    "## **13.3 PyTorch RNN Modules**\n",
    "\n",
    "#### **13.3.1 Built-in RNN, LSTM, GRU**\n",
    "\n",
    "```python\n",
    "lstm = nn.LSTM(\n",
    "    input_size=128,    # Feature dimension of input\n",
    "    hidden_size=256,   # Hidden state dimension\n",
    "    num_layers=2,      # Stacked LSTMs\n",
    "    batch_first=True,  # Input shape: (batch, seq, feature) vs (seq, batch, feature)\n",
    "    dropout=0.3,       # Dropout between layers (not applied to last layer)\n",
    "    bidirectional=True # Process forward and backward, concat outputs\n",
    ")\n",
    "\n",
    "# Input: (batch_size, seq_len, input_size)\n",
    "x = torch.randn(32, 50, 128)  # 32 samples, 50 time steps, 128 features\n",
    "\n",
    "# Output: (batch_size, seq_len, hidden_size * 2) if bidirectional\n",
    "output, (hidden, cell) = lstm(x)\n",
    "\n",
    "# output: all hidden states at all time steps\n",
    "# hidden: final hidden state (num_layers * num_directions, batch, hidden_size)\n",
    "# cell: final cell state\n",
    "```\n",
    "\n",
    "#### **13.3.2 Handling Variable Lengths (Padding and Packing)**\n",
    "\n",
    "Sequences have different lengths. We use `pack_padded_sequence` to avoid computing on padding tokens.\n",
    "\n",
    "```python\n",
    "from torch.nn.utils.rnn import pack_padded_sequence, pad_packed_sequence\n",
    "\n",
    "# Assume sequences sorted by length descending\n",
    "seq_lengths = [50, 45, 40, 30]  # Actual lengths\n",
    "padded_seqs = torch.randn(4, 50, 128)  # Batch=4, MaxLen=50\n",
    "\n",
    "# Pack (removes padding from computation)\n",
    "packed = pack_padded_sequence(\n",
    "    padded_seqs, \n",
    "    seq_lengths, \n",
    "    batch_first=True,\n",
    "    enforce_sorted=True\n",
    ")\n",
    "\n",
    "# Pass through RNN\n",
    "packed_output, (hidden, cell) = lstm(packed)\n",
    "\n",
    "# Unpack (restore to padded format)\n",
    "output, lengths = pad_packed_sequence(packed_output, batch_first=True)\n",
    "```\n",
    "\n",
    "**Masking:** For attention or loss computation, create masks to ignore padding.\n",
    "```python\n",
    "mask = (seq != pad_token_id).unsqueeze(-1).float()  # (batch, seq, 1)\n",
    "output = output * mask  # Zero out padding positions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **13.4 Sequence-to-Sequence Models (Seq2Seq)**\n",
    "\n",
    "Architecture for translation, summarization, chatbots: Encoder compresses input to context vector, Decoder generates output.\n",
    "\n",
    "#### **13.4.1 The Encoder**\n",
    "\n",
    "Processes input sequence, final hidden state becomes context vector.\n",
    "\n",
    "```python\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len)\n",
    "        embedded = self.embedding(x)  # (batch, seq_len, embed_size)\n",
    "        outputs, (hidden, cell) = self.lstm(embedded)\n",
    "        return outputs, hidden, cell\n",
    "```\n",
    "\n",
    "#### **13.4.2 The Decoder**\n",
    "\n",
    "Generates output token by token, using previous token as input (autoregressive).\n",
    "\n",
    "```python\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, vocab_size)\n",
    "        \n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x: (batch, 1) single token\n",
    "        x = x.unsqueeze(1) if x.dim() == 1 else x\n",
    "        embedded = self.embedding(x)  # (batch, 1, embed_size)\n",
    "        \n",
    "        output, (hidden, cell) = self.lstm(embedded, (hidden, cell))\n",
    "        prediction = self.fc(output.squeeze(1))  # (batch, vocab_size)\n",
    "        return prediction, hidden, cell\n",
    "```\n",
    "\n",
    "#### **13.4.3 Training vs Inference**\n",
    "\n",
    "**Teacher Forcing (Training):** Feed ground truth previous token as decoder input (faster convergence).\n",
    "\n",
    "**Inference:** Feed model's own prediction as next input (autoregressive generation).\n",
    "\n",
    "```python\n",
    "def translate_sentence(model, sentence, device, max_len=50):\n",
    "    model.eval()\n",
    "    # ... tokenize sentence ...\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        outputs, hidden, cell = model.encoder(src_tensor)\n",
    "        \n",
    "        # Decode\n",
    "        input_token = sos_token\n",
    "        for _ in range(max_len):\n",
    "            output, hidden, cell = model.decoder(input_token, hidden, cell)\n",
    "            pred_token = output.argmax(1)\n",
    "            \n",
    "            if pred_token == eos_token:\n",
    "                break\n",
    "                \n",
    "            translated.append(pred_token.item())\n",
    "            input_token = pred_token  # Autoregressive\n",
    "    \n",
    "    return translated\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **13.5 Attention Mechanism**\n",
    "\n",
    "**The Bottleneck Problem:** Fixed-size context vector (final hidden state) must capture entire input sequence information, especially problematic for long sequences.\n",
    "\n",
    "**Solution:** Attention allows decoder to \"look back\" at encoder outputs at each step.\n",
    "\n",
    "#### **13.5.1 Bahdanau (Additive) Attention**\n",
    "\n",
    "$$s_t = \\text{Decoder hidden state at step } t$$\n",
    "$$h_i = \\text{Encoder output at position } i$$\n",
    "$$e_{ti} = v^T \\tanh(W_s s_t + W_h h_i)$$  (alignment score)\n",
    "$$\\alpha_{ti} = \\frac{\\exp(e_{ti})}{\\sum_j \\exp(e_{tj})}$$  (softmax)\n",
    "$$c_t = \\sum_i \\alpha_{ti} h_i$$  (context vector)\n",
    "\n",
    "```python\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(hidden_size * 3, hidden_size)\n",
    "        self.v = nn.Linear(hidden_size, 1, bias=False)\n",
    "        \n",
    "    def forward(self, hidden, encoder_outputs, mask=None):\n",
    "        # hidden: (batch, hidden_size)\n",
    "        # encoder_outputs: (batch, src_len, hidden_size)\n",
    "        \n",
    "        batch_size = encoder_outputs.shape[0]\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        \n",
    "        # Repeat decoder hidden state src_len times\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)  # (batch, src_len, hidden)\n",
    "        \n",
    "        # Calculate energy\n",
    "        energy = torch.tanh(self.attn(torch.cat((hidden, encoder_outputs), dim=2)))\n",
    "        # energy: (batch, src_len, hidden_size)\n",
    "        \n",
    "        attention = self.v(energy).squeeze(2)  # (batch, src_len)\n",
    "        \n",
    "        if mask is not None:\n",
    "            attention = attention.masked_fill(mask == 0, -1e10)\n",
    "        \n",
    "        return torch.softmax(attention, dim=1)\n",
    "```\n",
    "\n",
    "#### **13.5.2 Luong (Multiplicative) Attention**\n",
    "\n",
    "Simpler, faster: $score(s_t, h_i) = s_t^T W h_i$\n",
    "\n",
    "**Global vs Local Attention:**\n",
    "- **Global:** Attends to all source positions (computationally expensive for long sequences)\n",
    "- **Local:** Attends to window around predicted position (faster)\n",
    "\n",
    "---\n",
    "\n",
    "## **13.6 Applications**\n",
    "\n",
    "#### **13.6.1 Named Entity Recognition (NER)**\n",
    "\n",
    "Token classification: Identify persons, organizations, locations in text.\n",
    "\n",
    "```python\n",
    "class NERModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_tags):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, bidirectional=True)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_tags)  # BIO tagging\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embeds = self.embedding(x)\n",
    "        lstm_out, _ = self.lstm(embeds)\n",
    "        logits = self.fc(lstm_out)  # (seq_len, batch, num_tags)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "**BIO Tagging:** B-PER (begin person), I-PER (inside person), O (outside).\n",
    "\n",
    "#### **13.6.2 Time Series Forecasting**\n",
    "\n",
    "Using LSTM for predicting next values in sequence.\n",
    "\n",
    "```python\n",
    "class TimeSeriesPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, output_size)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, features)\n",
    "        lstm_out, (hn, _) = self.lstm(x)\n",
    "        # Use last hidden state\n",
    "        out = self.fc(hn[-1])  # hn: (num_layers, batch, hidden)\n",
    "        return out\n",
    "```\n",
    "\n",
    "#### **13.6.3 Text Generation (Char-level)**\n",
    "\n",
    "Train on text, generate character by character.\n",
    "\n",
    "```python\n",
    "def generate_text(model, start_str, length=1000):\n",
    "    model.eval()\n",
    "    input_seq = [char2idx[c] for c in start_str]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        hidden = model.init_hidden(1)\n",
    "        \n",
    "        # Prime the model with start string\n",
    "        for char in input_seq[:-1]:\n",
    "            _, hidden = model(torch.tensor([[char]]), hidden)\n",
    "        \n",
    "        input_char = torch.tensor([[input_seq[-1]]])\n",
    "        \n",
    "        for _ in range(length):\n",
    "            output, hidden = model(input_char, hidden)\n",
    "            prob = torch.softmax(output, dim=2)\n",
    "            char_idx = torch.multinomial(prob.squeeze(), 1).item()\n",
    "            \n",
    "            print(idx2char[char_idx], end='')\n",
    "            input_char = torch.tensor([[char_idx]])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **13.7 Workbook Labs**\n",
    "\n",
    "### **Lab 1: RNN from Scratch**\n",
    "Implement vanilla RNN with BPTT (no PyTorch nn.RNN):\n",
    "1. Forward pass through time\n",
    "2. Backward pass computing all gradients manually\n",
    "3. Train on simple sequence (e.g., \"hello\" \u2192 \"elloh\" character shift)\n",
    "4. Show vanishing gradients by comparing gradients at t=0 vs t=20\n",
    "\n",
    "**Deliverable:** `rnn_scratch.py` with gradient flow visualization.\n",
    "\n",
    "### **Lab 2: Sentiment Analysis with LSTM**\n",
    "IMDB movie reviews:\n",
    "1. Embedding layer (pre-trained GloVe or trained from scratch)\n",
    "2. Bidirectional LSTM with attention\n",
    "3. Compare: Last hidden state vs Mean pooling vs Attention mechanism\n",
    "4. Achieve >85% accuracy\n",
    "\n",
    "**Deliverable:** Model with attention visualization (which words contribute to sentiment).\n",
    "\n",
    "### **Lab 3: Neural Machine Translation**\n",
    "English to French (or any language pair):\n",
    "1. Encoder-Decoder with Luong Attention\n",
    "2. Teacher forcing with scheduled sampling (gradually reduce teacher forcing ratio)\n",
    "3. BLEU score evaluation\n",
    "4. Attention alignment visualization (show which source word aligns to target word)\n",
    "\n",
    "**Deliverable:** Working translator with attention heatmaps.\n",
    "\n",
    "### **Lab 4: Anomaly Detection in Time Series**\n",
    "Sensor data (e.g., ECG or machine vibration):\n",
    "1. LSTM Autoencoder (sequence \u2192 sequence reconstruction)\n",
    "2. Anomaly = high reconstruction error\n",
    "3. Compare with Isolation Forest (Chapter 8) on temporal patterns\n",
    "\n",
    "**Deliverable:** Anomaly detection system with precision/recall on labeled anomalies.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.8 Common Pitfalls**\n",
    "\n",
    "1. **Teacher Forcing at Inference:** Using ground truth tokens during test time (cheating). Must use model's own predictions.\n",
    "\n",
    "2. **Ignoring End-of-Sequence:** Generated sequences can grow forever. Always check for EOS token or set max length.\n",
    "\n",
    "3. **Not Shuffling Batches:** For seq2seq, batches must contain sequences of similar length to minimize padding, but shuffling is still needed across batches.\n",
    "\n",
    "4. **Gradient Accumulation in RNNs:** Accumulating gradients over long sequences causes memory explosion. Use truncated BPTT or gradient checkpointing.\n",
    "\n",
    "5. **Wrong Hidden State Initialization:** Forgetting to detach hidden states between batches causes backprop through entire dataset (error) or BPTT across unrelated sequences.\n",
    "\n",
    "   ```python\n",
    "   # Wrong: hidden carries graph from previous batch\n",
    "   # Right:\n",
    "   hidden = tuple(h.detach() for h in hidden)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **13.9 Interview Questions**\n",
    "\n",
    "**Q1:** Why do LSTMs solve the vanishing gradient problem better than vanilla RNNs?\n",
    "*A: The cell state acts as a conveyor belt with minimal interactions (element-wise addition in the update: $C_t = f_t \\odot C_{t-1} + i_t \\odot \\tilde{C}_t$). Gradients can flow through the cell state unchanged (identity connection) without being multiplied by small derivatives repeatedly. The gates control information flow without forcing gradients through squashing functions at every step.*\n",
    "\n",
    "**Q2:** Explain the difference between teacher forcing and scheduled sampling.\n",
    "*A: Teacher forcing feeds ground truth previous token as decoder input during training, leading to fast convergence but exposure bias (model never sees its own errors). Scheduled sampling gradually replaces teacher forcing with model predictions during training (annealing schedule), exposing the model to its own errors and reducing discrepancy between training and inference.*\n",
    "\n",
    "**Q3:** What is the purpose of the attention mechanism in seq2seq models?\n",
    "*A: Attention solves the information bottleneck of fixed-size context vectors. Instead of compressing all source information into final encoder hidden state, attention allows decoder to dynamically focus on different source positions at each decoding step, creating a weighted context vector. This improves performance on long sequences and provides interpretability (alignment visualization).*\n",
    "\n",
    "**Q4:** Why use bidirectional LSTM and when can't you use it?\n",
    "*A: Bidirectional LSTM processes sequence both forwards and backwards, capturing future context for each position (e.g., knowing word is a verb requires seeing object later). Can't use when future information isn't available: real-time streaming, autoregressive generation (decoding), or causal language modeling where we must predict next token without seeing it.*\n",
    "\n",
    "**Q5:** How do you handle variable-length sequences in batches?\n",
    "*A: Pad sequences to max length in batch, then use pack_padded_sequence to tell PyTorch to skip computations on padding tokens. Alternatively, use masking in loss function and attention. Must sort sequences by length descending before packing (or use enforce_sorted=False with lengths argument).*\n",
    "\n",
    "---\n",
    "\n",
    "## **13.10 Further Reading**\n",
    "\n",
    "**Papers:**\n",
    "- \"Long Short-Term Memory\" (Hochreiter & Schmidhuber, 1997) - Original LSTM\n",
    "- \"Learning Phrase Representations using RNN Encoder-Decoder\" (Cho et al., 2014) - GRU, seq2seq\n",
    "- \"Neural Machine Translation by Jointly Learning to Align and Translate\" (Bahdanau et al., 2015) - Attention\n",
    "- \"Effective Approaches to Attention-based Neural Machine Translation\" (Luong et al., 2015)\n",
    "\n",
    "**Books:**\n",
    "- *Deep Learning with PyTorch* (Stevens, Antiga, Viehmann) - Chapter on sequences\n",
    "\n",
    "---\n",
    "\n",
    "## **13.11 Checkpoint Project: Conversational AI Bot**\n",
    "\n",
    "Build a retrieval-based chatbot with attention-based matching.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Architecture:**\n",
    "   - Dual Encoder (one LSTM for context, one for response)\n",
    "   - Attention matching layer between context and candidate responses\n",
    "   - Similarity score (cosine) between encoded context and response\n",
    "\n",
    "2. **Dataset:**\n",
    "   - Reddit conversations or customer service logs (pairs of context-response)\n",
    "   - Negative sampling: For each positive pair, sample 5 random responses as negatives\n",
    "\n",
    "3. **Training:**\n",
    "   - Contrastive loss: Positive pairs close together, negatives far apart\n",
    "   - Recall@1, Recall@5 metrics (is correct response in top K?)\n",
    "\n",
    "4. **Inference:**\n",
    "   - Index 10,000 candidate responses using FAISS (approximate nearest neighbors)\n",
    "   - Real-time retrieval (<100ms) for given user query\n",
    "\n",
    "5. **Evaluation:**\n",
    "   - Human evaluation on 100 test conversations (appropriateness, fluency)\n",
    "   - A/B test simulation vs baseline (TF-IDF retrieval)\n",
    "\n",
    "**Deliverables:**\n",
    "- `chatbot/` package with training and inference\n",
    "- API endpoint: POST /reply {message} \u2192 Returns top 3 candidate responses with confidence scores\n",
    "- Report: \"Bot retrieves appropriate response 65% of the time in top-1, 90% in top-3\"\n",
    "\n",
    "**Success Criteria:**\n",
    "- Recall@5 > 0.85 on test set\n",
    "- Inference latency < 100ms for 10k candidate index\n",
    "- Attention visualization shows bot focuses on key entities in user query\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 13**\n",
    "\n",
    "*You now master sequential modeling. Chapter 14 will cover Transformers and Modern NLP \u2014 the architecture that replaced RNNs and powers GPT, BERT, and the current AI revolution.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='12. convolutional_neural_networks.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='14. transformers_and_modern_nlp.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}