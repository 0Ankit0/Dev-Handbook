{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ac874e98",
   "metadata": {},
   "source": [
    "Here is **Chapter 14: Transformers & Modern NLP** — the architecture that revolutionized artificial intelligence.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 14: TRANSFORMERS & MODERN NLP**\n",
    "\n",
    "*Attention Is All You Need*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "The Transformer architecture, introduced in 2017, eliminated recurrence and replaced it with self-attention, enabling parallelization and long-range dependencies at scale. This chapter covers the complete modern NLP stack: from the mathematical mechanics of multi-head attention to fine-tuning strategies for BERT and GPT-style models, tokenization algorithms, and the implementation details that separate toy models from production systems like ChatGPT.\n",
    "\n",
    "**Estimated Time:** 60-70 hours (4-5 weeks)  \n",
    "**Prerequisites:** Chapters 10-13 (Neural networks, attention mechanisms from Seq2Seq, PyTorch proficiency)\n",
    "\n",
    "---\n",
    "\n",
    "## **14.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement multi-head self-attention and positional encodings from scratch\n",
    "2. Build and train Transformer encoder and decoder blocks with proper layer normalization and residual connections\n",
    "3. Fine-tune pre-trained models (BERT, RoBERTa, GPT, T5) for downstream tasks using Hugging Face Transformers\n",
    "4. Implement tokenization algorithms (BPE, WordPiece, SentencePiece) and understand their trade-offs\n",
    "5. Apply advanced fine-tuning techniques: LoRA, prompt tuning, and instruction tuning\n",
    "6. Build complete NLP pipelines for classification, NER, question answering, and text generation\n",
    "\n",
    "---\n",
    "\n",
    "## **14.1 The Transformer Architecture**\n",
    "\n",
    "The seminal paper \"Attention Is All You Need\" (Vaswani et al., 2017) introduced an architecture based solely on attention mechanisms, dispensing with recurrence and convolutions entirely.\n",
    "\n",
    "#### **14.1.1 High-Level Structure**\n",
    "\n",
    "**Encoder:** Processes input sequence, produces contextualized representations.  \n",
    "**Decoder:** Generates output sequence autoregressively, attending to encoder outputs and previously generated tokens.\n",
    "\n",
    "**Key Innovations:**\n",
    "1. **Self-Attention:** Direct global dependencies between any positions\n",
    "2. **Multi-Head Attention:** Multiple attention \"views\" in parallel\n",
    "3. **Positional Encodings:** Inject sequence order information\n",
    "4. **Layer Normalization:** Stabilize deep network training\n",
    "5. **Residual Connections:** Enable gradient flow in deep stacks (12-24+ layers)\n",
    "\n",
    "#### **14.1.2 Self-Attention Mechanism**\n",
    "\n",
    "For input $\\mathbf{X} \\in \\mathbb{R}^{n \\times d_{model}}$:\n",
    "\n",
    "1. **Linear Projections:**\n",
    "   $$\\mathbf{Q} = \\mathbf{X}\\mathbf{W}^Q, \\quad \\mathbf{K} = \\mathbf{X}\\mathbf{W}^K, \\quad \\mathbf{V} = \\mathbf{X}\\mathbf{W}^V$$\n",
    "   \n",
    "   Where $\\mathbf{W}^Q, \\mathbf{W}^K, \\mathbf{W}^V \\in \\mathbb{R}^{d_{model} \\times d_k}$\n",
    "\n",
    "2. **Scaled Dot-Product Attention:**\n",
    "   $$\\text{Attention}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{softmax}\\left(\\frac{\\mathbf{Q}\\mathbf{K}^T}{\\sqrt{d_k}}\\right)\\mathbf{V}$$\n",
    "\n",
    "**Why scale by $\\sqrt{d_k}$?** For large $d_k$, dot products grow large in magnitude, pushing softmax into regions with small gradients.\n",
    "\n",
    "**Implementation:**\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        d_k = query.size(-1)\n",
    "        \n",
    "        # Matmul and scale\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Mask (for padding or causal attention)\n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "        \n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        output = torch.matmul(attn_weights, value)\n",
    "        return output, attn_weights\n",
    "```\n",
    "\n",
    "#### **14.1.3 Multi-Head Attention**\n",
    "\n",
    "Project into $h$ subspaces, apply attention in parallel, concatenate:\n",
    "\n",
    "$$\\text{MultiHead}(\\mathbf{Q}, \\mathbf{K}, \\mathbf{V}) = \\text{Concat}(\\text{head}_1, \\dots, \\text{head}_h)\\mathbf{W}^O$$\n",
    "\n",
    "Where $\\text{head}_i = \\text{Attention}(\\mathbf{Q}\\mathbf{W}_i^Q, \\mathbf{K}\\mathbf{W}_i^K, \\mathbf{V}\\mathbf{W}_i^V)$\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % num_heads == 0, \"d_model must be divisible by num_heads\"\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.num_heads = num_heads\n",
    "        self.d_k = d_model // num_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # Linear projections and reshape for multi-head\n",
    "        # (batch, seq, d_model) -> (batch, heads, seq, d_k)\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.num_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Apply attention\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # Concatenate heads\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attn_output)\n",
    "        return output, attn_weights\n",
    "```\n",
    "\n",
    "#### **14.1.4 Positional Encodings**\n",
    "\n",
    "Since Transformers have no recurrence or convolution, we must inject position information.\n",
    "\n",
    "**Sinusoidal (Original):**\n",
    "$$PE_{(pos, 2i)} = \\sin(pos / 10000^{2i/d_{model}})$$\n",
    "$$PE_{(pos, 2i+1)} = \\cos(pos / 10000^{2i/d_{model}})$$\n",
    "\n",
    "**Learned Positional Embeddings:** Trainable vectors (used in BERT, GPT).\n",
    "\n",
    "**Relative Positional Encodings (RoPE, ALiBi):** Encode relative distances rather than absolute positions (better for long sequences).\n",
    "\n",
    "```python\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0).transpose(0, 1)  # (max_len, 1, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:x.size(0), :]\n",
    "        return x\n",
    "```\n",
    "\n",
    "#### **14.1.5 Transformer Block (Encoder)**\n",
    "\n",
    "```\n",
    "Input → Multi-Head Attention → Add & Norm → Feed Forward → Add & Norm → Output\n",
    "```\n",
    "\n",
    "**Feed-Forward Network:** Two linear transformations with ReLU (or GELU):\n",
    "$$\\text{FFN}(x) = \\max(0, xW_1 + b_1)W_2 + b_2$$\n",
    "\n",
    "**Layer Normalization:** Normalize across feature dimension (unlike BatchNorm which normalizes across batch).\n",
    "\n",
    "```python\n",
    "class TransformerEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model=512, num_heads=8, d_ff=2048, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.self_attn = MultiHeadAttention(d_model, num_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.ReLU(),  # or GELU for modern variants\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        \n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout1 = nn.Dropout(dropout)\n",
    "        self.dropout2 = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, src, src_mask=None):\n",
    "        # Self-attention block with residual and norm\n",
    "        src2, _ = self.self_attn(src, src, src, src_mask)\n",
    "        src = src + self.dropout1(src2)\n",
    "        src = self.norm1(src)\n",
    "        \n",
    "        # Feed-forward block with residual and norm\n",
    "        src2 = self.feed_forward(src)\n",
    "        src = src + self.dropout2(src2)\n",
    "        src = self.norm2(src)\n",
    "        \n",
    "        return src\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.2 Pre-trained Models and Fine-Tuning**\n",
    "\n",
    "#### **14.2.1 BERT (Bidirectional Encoder Representations from Transformers)**\n",
    "\n",
    "**Architecture:** Stack of Transformer encoder layers.  \n",
    "**Training:** \n",
    "1. **MLM (Masked Language Modeling):** Mask 15% of tokens, predict original. Enables bidirectional context.\n",
    "2. **NSP (Next Sentence Prediction):** Predict if sentence B follows A (later found less important, removed in RoBERTa).\n",
    "\n",
    "**Input Representation:**\n",
    "```\n",
    "[CLS] The cat sat [MASK] the mat [SEP] It was comfortable [SEP]\n",
    " |<------------------ Segment A ---------------->|<- B ->|\n",
    "```\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertForSequenceClassification.from_pretrained(\n",
    "    'bert-base-uncased', \n",
    "    num_labels=2\n",
    ")\n",
    "\n",
    "# Tokenize\n",
    "inputs = tokenizer(\n",
    "    \"This movie was great!\", \n",
    "    return_tensors=\"pt\", \n",
    "    padding=True, \n",
    "    truncation=True,\n",
    "    max_length=512\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "outputs = model(**inputs)\n",
    "logits = outputs.logits\n",
    "```\n",
    "\n",
    "#### **14.2.2 GPT (Generative Pre-trained Transformer)**\n",
    "\n",
    "**Architecture:** Decoder-only (autoregressive).  \n",
    "**Training:** Causal language modeling (predict next token given previous).  \n",
    "**Key Feature:** Unidirectional (left-to-right) attention with causal masking.\n",
    "\n",
    "**Causal Mask:** Prevents attending to future positions.\n",
    "```python\n",
    "def generate_causal_mask(size):\n",
    "    \"\"\"Lower triangular matrix (including diagonal)\"\"\"\n",
    "    mask = torch.tril(torch.ones(size, size)).unsqueeze(0).unsqueeze(0)\n",
    "    return mask  # 1 = attend, 0 = mask\n",
    "```\n",
    "\n",
    "#### **14.2.3 T5 (Text-to-Text Transfer Transformer)**\n",
    "\n",
    "**Architecture:** Encoder-Decoder.  \n",
    "**Philosophy:** Frame all NLP tasks as text generation.\n",
    "- Translation: `translate English to German: Hello world`\n",
    "- Classification: `sentiment: This movie was great`\n",
    "- Summarization: `summarize: [article text]`\n",
    "\n",
    "---\n",
    "\n",
    "## **14.3 Tokenization**\n",
    "\n",
    "Neural networks process numbers, not text. Tokenization converts text to IDs.\n",
    "\n",
    "#### **14.3.1 Byte-Pair Encoding (BPE)**\n",
    "\n",
    "Start with character vocabulary, iteratively merge most frequent pairs.\n",
    "\n",
    "```\n",
    "Initial: l o w </w>  ->  5, 6, 7, 8\n",
    "Corpus: low, lower, lowest, new, newer\n",
    "Merge 'e'+'r' -> 'er': lower, newer\n",
    "Merge 'er'+'</w>' -> 'er</w>': lower, newer\n",
    "...\n",
    "Final vocab: low, er, est, new, ...\n",
    "```\n",
    "\n",
    "**SubwordUnit:** Handles OOV (out-of-vocabulary) words by breaking into subwords: `unhappiness` → `un`, `happiness` → `happiness` (or further).\n",
    "\n",
    "**Libraries:** Hugging Face `tokenizers`, OpenAI `tiktoken`.\n",
    "\n",
    "#### **14.3.2 WordPiece (BERT)**\n",
    "\n",
    "Similar to BPE but uses likelihood maximization instead of frequency.\n",
    "\n",
    "#### **14.3.3 SentencePiece (T5, XLNet)**\n",
    "\n",
    "Language-agnostic, operates on raw text (including whitespace as tokens). Uses BPE or Unigram algorithm.\n",
    "\n",
    "```python\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "\n",
    "text = \"Transformers are amazing!\"\n",
    "tokens = tokenizer.tokenize(text)\n",
    "# ['transformers', 'are', 'amazing', '!']\n",
    "\n",
    "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "# [19082, 2024, 6429, 999]\n",
    "\n",
    "# Special tokens\n",
    "encoded = tokenizer(text, return_tensors=\"pt\")\n",
    "# input_ids: [101, 19082, 2024, 6429, 999, 102]\n",
    "# 101 = [CLS], 102 = [SEP]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.4 Advanced Fine-Tuning Techniques**\n",
    "\n",
    "#### **14.4.1 LoRA (Low-Rank Adaptation)**\n",
    "\n",
    "Instead of fine-tuning all parameters (BERT-base: 110M), inject low-rank matrices into attention layers.\n",
    "\n",
    "$$W = W_0 + \\Delta W = W_0 + BA$$\n",
    "\n",
    "Where $B \\in \\mathbb{R}^{d \\times r}$, $A \\in \\mathbb{R}^{r \\times k}$, and $r \\ll d,k$ (rank 4-64).\n",
    "\n",
    "**Benefits:**\n",
    "- Trainable params: 0.1-1% of full model\n",
    "- No inference latency (can merge $W$)\n",
    "- Store separate adapters per task\n",
    "\n",
    "```python\n",
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16,\n",
    "    lora_alpha=32,\n",
    "    target_modules=[\"query\", \"value\"],  # Which layers to adapt\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\"\n",
    ")\n",
    "\n",
    "model = get_peft_model(base_model, config)\n",
    "# Only LoRA parameters require gradients\n",
    "```\n",
    "\n",
    "#### **14.4.2 Prompt Tuning / Prefix Tuning**\n",
    "\n",
    "Add trainable \"soft prompts\" (continuous vectors) to input, keep model frozen.\n",
    "\n",
    "```\n",
    "Original: [CLS] The movie was good [SEP]\n",
    "Prompt tuned: [P1] [P2] ... [P10] [CLS] The movie was good [SEP]\n",
    "                (trainable embeddings)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.5 Modern NLP Tasks**\n",
    "\n",
    "#### **14.5.1 Named Entity Recognition (NER) with BERT**\n",
    "\n",
    "Token classification: Each token gets BIO tag.\n",
    "\n",
    "```python\n",
    "from transformers import BertForTokenClassification\n",
    "\n",
    "model = BertForTokenClassification.from_pretrained(\n",
    "    'bert-base-cased',\n",
    "    num_labels=9  # B-PER, I-PER, B-ORG, I-ORG, B-LOC, I-LOC, B-MISC, I-MISC, O\n",
    ")\n",
    "\n",
    "# Handle subword tokenization (WordPiece splits \"Washington\" -> \"Wash\", \"##ington\")\n",
    "# Align labels with word_ids()\n",
    "```\n",
    "\n",
    "#### **14.5.2 Question Answering (Extractive)**\n",
    "\n",
    "Span prediction: Predict start and end indices of answer in context.\n",
    "\n",
    "```python\n",
    "from transformers import BertForQuestionAnswering\n",
    "\n",
    "model = BertForQuestionAnswering.from_pretrained('bert-large-uncased-whole-word-masking-finetuned-squad')\n",
    "\n",
    "inputs = tokenizer(\n",
    "    question=\"What is the capital of France?\",\n",
    "    context=\"The capital of France is Paris.\",\n",
    "    return_tensors=\"pt\"\n",
    ")\n",
    "\n",
    "outputs = model(**inputs)\n",
    "start_scores = outputs.start_logits\n",
    "end_scores = outputs.end_logits\n",
    "\n",
    "# Get best span\n",
    "answer_start = torch.argmax(start_scores)\n",
    "answer_end = torch.argmax(end_scores) + 1\n",
    "answer = tokenizer.convert_tokens_to_string(\n",
    "    tokenizer.convert_ids_to_tokens(inputs['input_ids'][0][answer_start:answer_end])\n",
    ")\n",
    "```\n",
    "\n",
    "#### **14.5.3 Text Generation**\n",
    "\n",
    "Strategies to improve quality:\n",
    "- **Temperature:** Scale logits by $T$ before softmax ($T<1$ = more focused, $T>1$ = more random)\n",
    "- **Top-k Sampling:** Sample from $k$ most likely tokens\n",
    "- **Top-p (Nucleus) Sampling:** Sample from smallest set whose cumulative probability $\\geq p$\n",
    "- **Beam Search:** Keep $k$ best partial sequences at each step (better for translation, worse for creative writing)\n",
    "\n",
    "```python\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\"gpt2\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"gpt2\")\n",
    "\n",
    "inputs = tokenizer(\"Once upon a time\", return_tensors=\"pt\")\n",
    "\n",
    "outputs = model.generate(\n",
    "    **inputs,\n",
    "    max_length=100,\n",
    "    num_return_sequences=3,\n",
    "    temperature=0.8,\n",
    "    top_k=50,\n",
    "    top_p=0.95,\n",
    "    do_sample=True,\n",
    "    no_repeat_ngram_size=2  # Prevent repetition\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Transformer from Scratch**\n",
    "Implement complete Transformer (Encoder only) for classification:\n",
    "1. Multi-head attention with masking\n",
    "2. Positional encodings\n",
    "3. LayerNorm and residuals\n",
    "4. Train on IMDB sentiment (achieve >80% without pre-training)\n",
    "\n",
    "**Deliverable:** `transformer_scratch.py` training to convergence.\n",
    "\n",
    "### **Lab 2: BERT Fine-Tuning for NER**\n",
    "CoNLL-2003 dataset:\n",
    "1. Token alignment (handle subwords)\n",
    "2. Fine-tune `bert-base-cased`\n",
    "3. Evaluate with entity-level F1 (not token-level)\n",
    "4. Error analysis: Which entity types confuse the model?\n",
    "\n",
    "**Deliverable:** NER pipeline with F1 > 0.90.\n",
    "\n",
    "### **Lab 3: Instruction Tuning (Mini)**\n",
    "Create small instruction dataset (100 examples):\n",
    "1. Use LoRA to fine-tune GPT-2/Llama-2-7b\n",
    "2. Format: `### Instruction: ... ### Input: ... ### Response: ...`\n",
    "3. Compare zero-shot vs fine-tuned performance on held-out instructions\n",
    "\n",
    "**Deliverable:** LoRA adapter weights and inference script showing improved instruction following.\n",
    "\n",
    "### **Lab 4: Efficient Inference Optimization**\n",
    "Optimize BERT for production:\n",
    "1. ONNX export and quantization (INT8)\n",
    "2. Distillation: Train smaller student (6-layer) from teacher (12-layer)\n",
    "3. Benchmark: Latency vs Accuracy trade-off curve\n",
    "\n",
    "**Deliverable:** Speed/accuracy report showing 3x speedup with <2% accuracy drop.\n",
    "\n",
    "---\n",
    "\n",
    "## **14.7 Common Pitfalls**\n",
    "\n",
    "1. **Attention Mask Confusion:** Padding tokens must be masked (set to -inf before softmax), but causal masking for GPT is different (lower triangular).\n",
    "\n",
    "2. **Position IDs Wrong:** When using left-padding for batching, position IDs must reflect actual token positions, not indices in tensor.\n",
    "\n",
    "3. **Learning Rate Too High for Fine-Tuning:** Pre-trained models need small LR (1e-5 to 3e-5), not default 1e-3, or they catastrophically forget.\n",
    "\n",
    "4. **Max Length Issues:** BERT limited to 512 tokens. For longer documents, use sliding window, Longformer, or hierarchical approaches.\n",
    "\n",
    "5. **Not Freezing Embeddings for Rare Words:** When adding new tokens to vocabulary, only train new embeddings, keep others frozen initially.\n",
    "\n",
    "---\n",
    "\n",
    "## **14.8 Interview Questions**\n",
    "\n",
    "**Q1:** Why does the Transformer use LayerNorm instead of BatchNorm?\n",
    "*A: BatchNorm computes statistics across batch dimension, which varies with sequence length and is problematic for small batches or RNN-style processing. LayerNorm normalizes across feature dimension for each sample independently, making it suitable for variable-length sequences and autoregressive models where batch statistics are unstable.*\n",
    "\n",
    "**Q2:** Explain the difference between encoder-only, decoder-only, and encoder-decoder models. Give examples of each.\n",
    "*A: Encoder-only (BERT, RoBERTa): Bidirectional attention, good for understanding tasks (classification, NER, embedding). Decoder-only (GPT, LLaMA): Causal (left-to-right) attention, autoregressive generation (text completion, chat). Encoder-decoder (T5, BART): Encoder processes input bidirectionally, decoder generates output autoregressively; good for translation, summarization, where input and output are distinct sequences.*\n",
    "\n",
    "**Q3:** What is the purpose of scaling the dot product attention by $\\sqrt{d_k}$?\n",
    "*A: For large $d_k$, dot products grow large in magnitude (variance scales with $d_k$). This pushes the softmax function into regions with extremely small gradients (saturation). Scaling by $\\sqrt{d_k}$ counteracts this effect, keeping values in a range where softmax gradients remain healthy and training stable.*\n",
    "\n",
    "**Q4:** How does BPE tokenization handle out-of-vocabulary words?\n",
    "*A: BPE breaks OOV words into subword units (character n-grams or frequent substrings). For example, \"unhappiness\" might become [\"un\", \"happiness\"] or [\"un\", \"happ\", \"iness\"] depending on vocabulary. This allows the model to understand morphemes and generalize to new words composed of known subwords.*\n",
    "\n",
    "**Q5:** What are the advantages of LoRA over full fine-tuning?\n",
    "*A: 1) Memory efficiency: Only train 0.1-1% of parameters. 2) Storage: Save small adapter files per task instead of full model copies. 3) No inference latency: Can merge weights or keep separate. 4) Less catastrophic forgetting: Original model preserved. 5) Better for low-data regimes by reducing trainable parameters.*\n",
    "\n",
    "---\n",
    "\n",
    "## **14.9 Further Reading**\n",
    "\n",
    "**Papers:**\n",
    "- \"Attention Is All You Need\" (Vaswani et al., 2017) - Original Transformer\n",
    "- \"BERT: Pre-training of Deep Bidirectional Transformers\" (Devlin et al., 2019)\n",
    "- \"Language Models are Few-Shot Learners\" (Brown et al., 2020) - GPT-3\n",
    "- \"LoRA: Low-Rank Adaptation of Large Language Models\" (Hu et al., 2021)\n",
    "\n",
    "**Resources:**\n",
    "- Hugging Face Course: https://huggingface.co/course\n",
    "- \"The Illustrated Transformer\" (Jay Alammar) - Visual guide\n",
    "\n",
    "---\n",
    "\n",
    "## **14.10 Checkpoint Project: Production NLP API**\n",
    "\n",
    "Build a multi-task NLP service serving BERT-based models.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Architecture:**\n",
    "   - FastAPI backend with three endpoints:\n",
    "     - `/classify`: Sentiment analysis (BERT fine-tuned)\n",
    "     - `/ner`: Entity extraction (token classification)\n",
    "     - `/embed`: Sentence embeddings (mean pooling of last hidden states)\n",
    "\n",
    "2. **Optimization:**\n",
    "   - Model quantization (INT8) for 3x speedup\n",
    "   - Batching: Dynamic batching of concurrent requests\n",
    "   - Caching: Redis cache for frequent queries\n",
    "\n",
    "3. **Monitoring:**\n",
    "   - Track latency percentiles (p50, p95, p99)\n",
    "   - Log prediction confidence scores for drift detection\n",
    "   - A/B test endpoint comparing base vs quantized model\n",
    "\n",
    "4. **Deployment:**\n",
    "   - Docker container with nginx load balancer\n",
    "   - GPU support (CUDA) with fallback to CPU\n",
    "   - Health checks and graceful shutdown\n",
    "\n",
    "**Deliverables:**\n",
    "- `nlp_api/` with FastAPI app, model loading, and batching logic\n",
    "- `docker-compose.yml` with Redis and monitoring (Prometheus/Grafana)\n",
    "- Load test script (Locust) showing 100+ req/sec on single GPU\n",
    "- Documentation: \"API handles 3 concurrent tasks with <100ms p95 latency\"\n",
    "\n",
    "**Success Criteria:**\n",
    "- End-to-end latency < 50ms for classification (batch_size=1)\n",
    "- Support batching up to 32 requests dynamically\n",
    "- 99.9% uptime over 24-hour stress test\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 14**\n",
    "\n",
    "*You now master the Transformer architecture and modern NLP. Chapter 15 will cover Large Language Models (LLMs) and Generative AI — scaling to billions of parameters, RLHF, and production deployment of systems like ChatGPT.*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
