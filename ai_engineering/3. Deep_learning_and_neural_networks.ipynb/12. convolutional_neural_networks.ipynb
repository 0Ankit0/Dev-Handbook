{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c9a30c",
   "metadata": {},
   "source": [
    "Here is **Chapter 12: Convolutional Neural Networks (CNNs)** — the architecture that revolutionized computer vision.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 12: CONVOLUTIONAL NEURAL NETWORKS (CNNs)**\n",
    "\n",
    "*Seeing Through the Eyes of Machines*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Convolutional Neural Networks transformed artificial intelligence by enabling machines to see. From medical imaging to autonomous vehicles, CNNs extract hierarchical visual features through learned convolutional filters. This chapter progresses from the mathematical operation of convolution to state-of-the-art architectures, preparing you to build vision systems that rival human perception.\n",
    "\n",
    "**Estimated Time:** 60-70 hours (4-5 weeks)  \n",
    "**Prerequisites:** Chapters 10-11 (Neural Network fundamentals, PyTorch/TensorFlow)\n",
    "\n",
    "---\n",
    "\n",
    "## **12.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement convolution and pooling operations from scratch and understand their computational complexity\n",
    "2. Design and train modern CNN architectures (ResNet, EfficientNet) using transfer learning\n",
    "3. Apply advanced data augmentation strategies (Albumentations, CutMix, MixUp) to improve generalization\n",
    "4. Implement object detection pipelines (YOLO, R-CNN family) for localization and classification\n",
    "5. Build semantic and instance segmentation models for pixel-level understanding\n",
    "6. Optimize CNNs for mobile/edge deployment using quantization and pruning\n",
    "\n",
    "---\n",
    "\n",
    "## **12.1 The Convolution Operation**\n",
    "\n",
    "#### **12.1.1 Mathematical Definition**\n",
    "\n",
    "Convolution slides a filter (kernel) across the input, computing dot products at each position:\n",
    "\n",
    "$$(I * K)(i, j) = \\sum_{m} \\sum_{n} I(i+m, j+n) \\cdot K(m, n)$$\n",
    "\n",
    "Where $I$ is the input image, $K$ is the kernel (typically 3×3 or 5×5).\n",
    "\n",
    "**Key Parameters:**\n",
    "- **Kernel Size (F):** Spatial dimensions of filter (typically 3)\n",
    "- **Stride (S):** Step size when sliding (1 for dense, 2 for downsampling)\n",
    "- **Padding (P):** Zeros added to borders (maintains spatial dimensions when P = (F-1)/2)\n",
    "- **Output Size:** $O = \\lfloor \\frac{I - F + 2P}{S} \\rfloor + 1$\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# 2D Convolution\n",
    "conv = nn.Conv2d(\n",
    "    in_channels=3,      # RGB input\n",
    "    out_channels=64,    # Number of filters\n",
    "    kernel_size=3,      # 3x3 filters\n",
    "    stride=1,           # Step size\n",
    "    padding=1,          # Zero-padding to maintain size\n",
    "    bias=False          # Usually False when using BatchNorm\n",
    ")\n",
    "\n",
    "# Input: (Batch, Channels, Height, Width)\n",
    "input_tensor = torch.randn(32, 3, 224, 224)\n",
    "output = conv(input_tensor)  # Shape: (32, 64, 224, 224)\n",
    "```\n",
    "\n",
    "#### **12.1.2 Intuition: What Do Filters Learn?**\n",
    "\n",
    "- **Layer 1:** Edge detectors (horizontal, vertical, diagonal lines)\n",
    "- **Layer 2:** Simple textures (circles, grids, color blobs)\n",
    "- **Layer 3:** Complex patterns (wheels, eyes, textures)\n",
    "- **Layer 4+:** Object parts (faces, car wheels, doors)\n",
    "- **Final layers:** Complete objects and semantic concepts\n",
    "\n",
    "**Visualization:**\n",
    "```python\n",
    "# Visualize first layer filters\n",
    "filters = model.conv1.weight.data  # Shape: (64, 3, 7, 7)\n",
    "# Normalize and plot as grid to see learned edge detectors\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **12.2 Pooling and Downsampling**\n",
    "\n",
    "#### **12.2.1 Max Pooling**\n",
    "\n",
    "Reduces spatial dimensions by taking maximum value in each window. Provides translation invariance and reduces computation.\n",
    "\n",
    "```python\n",
    "pool = nn.MaxPool2d(kernel_size=2, stride=2)  # Reduces H,W by half\n",
    "# Input: (32, 64, 224, 224) -> Output: (32, 64, 112, 112)\n",
    "```\n",
    "\n",
    "**Why Max over Average?** Max pooling preserves the most salient features (strongest activations). Average pooling blurs features and is rarely used today except in final global pooling layers.\n",
    "\n",
    "#### **12.2.2 Strided Convolutions**\n",
    "\n",
    "Modern alternative to pooling: use stride=2 in convolution layers to downsample. Preserves more information and is fully learnable.\n",
    "\n",
    "```python\n",
    "# Downsampling via strided convolution (preferred in modern architectures)\n",
    "downsample = nn.Conv2d(64, 128, kernel_size=3, stride=2, padding=1)\n",
    "# Input: (32, 64, 56, 56) -> Output: (32, 128, 28, 28)\n",
    "```\n",
    "\n",
    "#### **12.2.3 Global Average Pooling (GAP)**\n",
    "\n",
    "Replaces fully connected layers: average each channel to single value. Reduces parameters drastically and improves generalization.\n",
    "\n",
    "```python\n",
    "gap = nn.AdaptiveAvgPool2d((1, 1))  # Output is (B, C, 1, 1)\n",
    "# Flatten to (B, C) for classification\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **12.3 Modern CNN Architectures**\n",
    "\n",
    "#### **12.3.1 LeNet-5 (1998)**\n",
    "The pioneer. 2 convolutional layers, subsampling, fully connected.\n",
    "- **Lesson:** Hierarchical feature extraction works.\n",
    "\n",
    "#### **12.3.2 AlexNet (2012)**\n",
    "Deep Learning breakthrough (ImageNet 2012 winner).\n",
    "- **Innovations:** ReLU, Dropout, GPU training, Data Augmentation\n",
    "- **Architecture:** 5 conv layers, 3 FC layers, 60M parameters\n",
    "\n",
    "```python\n",
    "class AlexNet(nn.Module):\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=11, stride=4, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(64, 192, kernel_size=5, padding=2),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "            nn.Conv2d(192, 384, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(384, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=3, stride=2),\n",
    "        )\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((6, 6))\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(256 * 6 * 6, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "```\n",
    "\n",
    "#### **12.3.3 VGGNet (2014)**\n",
    "Key insight: Use small 3×3 filters repeatedly instead of large 5×5 or 7×7.\n",
    "- **Advantage:** Same receptive field with fewer parameters and more non-linearities.\n",
    "- **VGG-16:** 13 conv layers + 3 FC, 138M parameters.\n",
    "\n",
    "#### **12.3.4 ResNet (2015)**\n",
    "Solved the vanishing gradient problem in deep networks using **skip connections** (residual learning).\n",
    "\n",
    "$$y = F(x, \\{W_i\\}) + x$$\n",
    "\n",
    "If $F(x) \\rightarrow 0$, gradient flows directly through $x$ (identity mapping).\n",
    "\n",
    "```python\n",
    "class ResidualBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    \n",
    "    def __init__(self, in_planes, planes, stride=1):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_planes, planes, kernel_size=3, \n",
    "                               stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(planes)\n",
    "        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, \n",
    "                               stride=1, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(planes)\n",
    "        \n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_planes != planes:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_planes, planes, kernel_size=1, \n",
    "                         stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(planes)\n",
    "            )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        out = torch.relu(self.bn1(self.conv1(x)))\n",
    "        out = self.bn2(self.conv2(out))\n",
    "        out += self.shortcut(x)  # Skip connection\n",
    "        out = torch.relu(out)\n",
    "        return out\n",
    "```\n",
    "\n",
    "**ResNet Variants:**\n",
    "- **ResNet-18/34:** Basic blocks (two 3×3 convs)\n",
    "- **ResNet-50/101/152:** Bottleneck blocks (1×1, 3×3, 1×1) for efficiency\n",
    "\n",
    "#### **12.3.5 DenseNet (2017)**\n",
    "Every layer connects to every other layer in a feed-forward fashion.\n",
    "- **Advantage:** Feature reuse, fewer parameters, strong gradients\n",
    "- **Disadvantage:** High memory consumption (concatenation grows channels)\n",
    "\n",
    "#### **12.3.6 EfficientNet (2019)**\n",
    "Compound scaling: uniformly scale depth, width, and resolution with fixed coefficients.\n",
    "- **EfficientNet-B0 to B7:** Increasing scale and accuracy\n",
    "- **Mobile-optimized:** EfficientNet-Lite variants for edge devices\n",
    "\n",
    "---\n",
    "\n",
    "## **12.4 Training Techniques for Vision**\n",
    "\n",
    "#### **12.4.1 Data Augmentation**\n",
    "\n",
    "**Albumentations (Industry Standard):**\n",
    "```python\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "train_transform = A.Compose([\n",
    "    A.RandomResizedCrop(height=224, width=224, scale=(0.08, 1.0)),\n",
    "    A.HorizontalFlip(p=0.5),\n",
    "    A.ColorJitter(brightness=0.4, contrast=0.4, saturation=0.4, hue=0.1),\n",
    "    A.CoarseDropout(max_holes=8, max_height=32, max_width=32, p=0.5),  # Cutout\n",
    "    A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ToTensorV2()\n",
    "])\n",
    "```\n",
    "\n",
    "**Advanced Augmentations:**\n",
    "- **MixUp:** Blend two images and labels: $\\tilde{x} = \\lambda x_i + (1-\\lambda)x_j$\n",
    "- **CutMix:** Cut and paste patches between images\n",
    "- **AutoAugment/RandAugment:** Learned augmentation policies\n",
    "\n",
    "#### **12.4.2 Transfer Learning**\n",
    "\n",
    "Leverage pre-trained models (ImageNet) for new tasks.\n",
    "\n",
    "**Strategies:**\n",
    "1. **Feature Extraction:** Freeze backbone, train only final layer (small dataset)\n",
    "2. **Fine-tuning:** Unfreeze all layers with small LR (large dataset)\n",
    "3. **Discriminative LR:** Different learning rates for different layers (lower LR for early layers)\n",
    "\n",
    "```python\n",
    "# Load pre-trained ResNet\n",
    "model = torchvision.models.resnet50(pretrained=True)\n",
    "\n",
    "# Freeze all layers\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Replace final layer for new task (10 classes)\n",
    "model.fc = nn.Linear(model.fc.in_features, 10)\n",
    "\n",
    "# Unfreeze last block for fine-tuning\n",
    "for param in model.layer4.parameters():\n",
    "    param.requires_grad = True\n",
    "\n",
    "# Optimizer with different LRs\n",
    "optimizer = torch.optim.Adam([\n",
    "    {'params': model.layer4.parameters(), 'lr': 1e-5},\n",
    "    {'params': model.fc.parameters(), 'lr': 1e-3}\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **12.5 Object Detection**\n",
    "\n",
    "#### **12.5.1 Two-Stage Detectors (R-CNN Family)**\n",
    "\n",
    "**Faster R-CNN:**\n",
    "1. **RPN (Region Proposal Network):** Proposes object bounding boxes\n",
    "2. **ROI Pooling:** Extracts fixed-size features for each proposal\n",
    "3. **Classification + Bounding Box Regression:** Final predictions\n",
    "\n",
    "**Pros:** High accuracy  \n",
    "**Cons:** Slow (10-15 FPS)\n",
    "\n",
    "#### **12.5.2 Single-Stage Detectors (YOLO, SSD)**\n",
    "\n",
    "**YOLO (You Only Look Once):**\n",
    "Divide image into S×S grid. Each cell predicts B bounding boxes and class probabilities in single forward pass.\n",
    "\n",
    "```python\n",
    "# YOLO-style output: (S, S, B*5 + C)\n",
    "# 5 = [x, y, w, h, confidence], C = class probabilities\n",
    "```\n",
    "\n",
    "**YOLOv8 Architecture:** Anchor-free, decoupled head, CIoU loss.\n",
    "\n",
    "**Pros:** Fast (real-time, 60+ FPS)  \n",
    "**Cons:** Lower accuracy on small objects than two-stage\n",
    "\n",
    "#### **12.5.3 Evaluation: mAP (mean Average Precision)**\n",
    "\n",
    "Intersection over Union (IoU) determines if detection is correct.\n",
    "\n",
    "$$\\text{IoU} = \\frac{\\text{Area of Intersection}}{\\text{Area of Union}}$$\n",
    "\n",
    "mAP@0.5: Average precision at IoU threshold 0.5  \n",
    "mAP@0.5:0.95: Average over multiple IoU thresholds (COCO standard)\n",
    "\n",
    "---\n",
    "\n",
    "## **12.6 Image Segmentation**\n",
    "\n",
    "#### **12.6.1 Semantic Segmentation**\n",
    "\n",
    "Classify every pixel (no distinction between instances).\n",
    "\n",
    "**U-Net Architecture:**\n",
    "- **Encoder:** Downsampling (ResNet-style)\n",
    "- **Bottleneck:** Deepest features\n",
    "- **Decoder:** Upsampling with skip connections (preserves spatial detail)\n",
    "\n",
    "```python\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super().__init__()\n",
    "        # Encoder...\n",
    "        self.encoder = ...  # Contracting path\n",
    "        # Decoder with skip connections\n",
    "        self.upconv1 = nn.ConvTranspose2d(1024, 512, kernel_size=2, stride=2)\n",
    "        self.decoder1 = ...  # Concatenate with skip, then conv\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Encoder features\n",
    "        enc1 = self.enc1(x)\n",
    "        enc2 = self.enc2(enc1)\n",
    "        ...\n",
    "        \n",
    "        # Decoder with skips\n",
    "        dec1 = self.upconv1(bottleneck)\n",
    "        dec1 = torch.cat([dec1, enc4], dim=1)  # Skip connection\n",
    "        ...\n",
    "        return final_conv\n",
    "```\n",
    "\n",
    "#### **12.6.2 Instance Segmentation**\n",
    "\n",
    "Separate individual objects of same class.\n",
    "\n",
    "**Mask R-CNN:** Extends Faster R-CNN with mask head for pixel-wise segmentation per instance.\n",
    "\n",
    "#### **12.6.3 Panoptic Segmentation**\n",
    "\n",
    "Combines semantic (stuff: sky, road) and instance (things: cars, people) segmentation.\n",
    "\n",
    "---\n",
    "\n",
    "## **12.7 Optimization for Deployment**\n",
    "\n",
    "#### **12.7.1 Quantization**\n",
    "\n",
    "Reduce precision from FP32 to INT8 (4x smaller, faster inference on specialized hardware).\n",
    "\n",
    "```python\n",
    "# PyTorch Post-Training Static Quantization\n",
    "model.qconfig = torch.quantization.get_default_qconfig('fbgemm')\n",
    "torch.quantization.prepare(model, inplace=True)\n",
    "# Calibrate with representative data...\n",
    "torch.quantization.convert(model, inplace=True)\n",
    "\n",
    "# Quantization Aware Training (QAT) - better accuracy\n",
    "model.train()\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "torch.quantization.prepare_qat(model, inplace=True)\n",
    "# Train normally...\n",
    "```\n",
    "\n",
    "#### **12.7.2 Pruning**\n",
    "\n",
    "Remove unimportant weights (near zero) to sparsify network.\n",
    "\n",
    "```python\n",
    "import torch.nn.utils.prune as prune\n",
    "\n",
    "# Unstructured pruning (individual weights)\n",
    "prune.l1_unstructured(module, name='weight', amount=0.3)  # 30% sparsity\n",
    "\n",
    "# Structured pruning (entire channels/filters) - better hardware acceleration\n",
    "```\n",
    "\n",
    "#### **12.7.3 Knowledge Distillation**\n",
    "\n",
    "Train small \"student\" network to mimic large \"teacher\" network using soft targets.\n",
    "\n",
    "$$\\mathcal{L} = \\alpha \\mathcal{L}_{\\text{CE}}(y_{\\text{student}}, y_{\\text{true}}) + (1-\\alpha) \\tau^2 \\mathcal{L}_{\\text{KL}}(\\sigma(y_{\\text{teacher}}/\\tau), \\sigma(y_{\\text{student}}/\\tau))$$\n",
    "\n",
    "Where $\\tau$ is temperature (softens probability distribution to reveal more information).\n",
    "\n",
    "---\n",
    "\n",
    "## **12.8 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Convolution from Scratch**\n",
    "Implement 2D convolution using only NumPy (no scipy.signal):\n",
    "1. Handle multiple channels and batches\n",
    "2. Implement stride and padding\n",
    "3. Verify against PyTorch nn.Conv2d output\n",
    "\n",
    "**Deliverable:** `conv2d_numpy.py` with unit tests matching PyTorch within 1e-5.\n",
    "\n",
    "### **Lab 2: Custom CNN Architecture Design**\n",
    "Design a CNN for CIFAR-10 (< 1M parameters, > 90% accuracy):\n",
    "1. Use depthwise separable convolutions (MobileNet-style) for efficiency\n",
    "2. Implement squeeze-and-excitation blocks (channel attention)\n",
    "3. Train with MixUp/CutMix augmentation\n",
    "4. Achieve inference time < 10ms on CPU\n",
    "\n",
    "**Deliverable:** Model definition, training log, and benchmark results.\n",
    "\n",
    "### **Lab 3: Transfer Learning for Medical Imaging**\n",
    "Chest X-ray classification (pneumonia detection):\n",
    "1. Use pre-trained EfficientNet-B0\n",
    "2. Implement Grad-CAM for visualization (explainability)\n",
    "3. Handle class imbalance (normal vs pneumonia)\n",
    "4. Calculate sensitivity/specificity (medical metrics)\n",
    "\n",
    "**Deliverable:** Jupyter notebook with model and saliency maps showing what network looks at.\n",
    "\n",
    "### **Lab 4: Object Detection Mini-YOLO**\n",
    "Implement simplified YOLO for single-class detection (e.g., faces):\n",
    "1. Grid-based prediction (S=7)\n",
    "2. Loss function: MSE for coordinates, BCE for confidence/class\n",
    "3. Non-Maximum Suppression (NMS) for post-processing\n",
    "4. Evaluate with mAP\n",
    "\n",
    "**Deliverable:** Training script that outputs bounding boxes on test images.\n",
    "\n",
    "---\n",
    "\n",
    "## **12.9 Common Pitfalls**\n",
    "\n",
    "1. **Ignoring Input Normalization:** ImageNet pre-trained models expect specific mean/std. Failing to normalize causes garbage predictions.\n",
    "   ```python\n",
    "   # Wrong: input / 255.0 only\n",
    "   # Right: (input - mean) / std where mean=[0.485, 0.456, 0.406]\n",
    "   ```\n",
    "\n",
    "2. **Transfer Learning Catastrophic Forgetting:** Fine-tuning with high LR on small dataset destroys pre-trained features. Use discriminative learning rates.\n",
    "\n",
    "3. **Batch Size vs BatchNorm:** BatchNorm requires sufficient batch size (>32). For small batches, use GroupNorm or LayerNorm instead.\n",
    "\n",
    "4. **Ignoring Aspect Ratio:** Resizing images without preserving aspect ratio distorts objects. Use letterboxing or center cropping.\n",
    "\n",
    "5. **Test Time Augmentation (TTA) Leakage:** Using TTA during validation but not in final deployment creates performance gap.\n",
    "\n",
    "---\n",
    "\n",
    "## **12.10 Interview Questions**\n",
    "\n",
    "**Q1:** Why use 3×3 convolutions instead of 5×5 or 7×7?\n",
    "*A: Two 3×3 convs have receptive field of 5×5 (3+3-1) with fewer parameters (2×3²=18 vs 5²=25) and more non-linearities (two ReLUs vs one), increasing expressiveness while reducing computation. Three 3×3 convs approximate 7×7 with even greater efficiency.*\n",
    "\n",
    "**Q2:** Explain why ResNet's skip connections help with gradient flow.\n",
    "*A: In deep networks, gradients multiply through many layers (chain rule). If gradients < 1, they vanish; if > 1, they explode. Skip connections create shortcut paths where gradient can flow directly: ∂L/∂x = ∂L/∂F · ∂F/∂x + 1. The +1 term ensures gradient doesn't vanish even if ∂F/∂x is small.*\n",
    "\n",
    "**Q3:** What is the difference between semantic and instance segmentation?\n",
    "*A: Semantic segmentation classifies every pixel into a class (e.g., all cars are same color). Instance segmentation separates individual objects (car 1, car 2, car 3). Panoptic segmentation combines both: semantic for \"stuff\" (sky, road) and instance for \"things\" (cars, people).*\n",
    "\n",
    "**Q4:** How does YOLO achieve real-time speed compared to Faster R-CNN?\n",
    "*A: YOLO is single-stage: single forward pass directly predicts bounding boxes and classes from full image. Faster R-CNN is two-stage: first generates region proposals (RPN), then classifies each region separately. YOLO trades some accuracy for massive speed gain by formulating detection as regression problem.*\n",
    "\n",
    "**Q5:** Why use depthwise separable convolutions in MobileNet?\n",
    "*A: Standard conv mixes spatial and channel information simultaneously (costly). Depthwise separable splits into: (1) Depthwise conv - applies single filter per input channel (spatial filtering), (2) Pointwise conv (1×1) - combines channels. Reduces computation from $D_K \\cdot D_K \\cdot M \\cdot N \\cdot D_F \\cdot D_F$ to $D_K \\cdot D_K \\cdot M \\cdot D_F^2 + M \\cdot N \\cdot D_F^2$, ~8-9x cheaper for 3×3 kernels.*\n",
    "\n",
    "---\n",
    "\n",
    "## **12.11 Further Reading**\n",
    "\n",
    "**Papers:**\n",
    "- \"ImageNet Classification with Deep CNNs\" (AlexNet, 2012)\n",
    "- \"Deep Residual Learning for Image Recognition\" (ResNet, 2015)\n",
    "- \"MobileNets: Efficient CNNs for Mobile Vision\" (2017)\n",
    "- \"You Only Look Once: Unified, Real-Time Object Detection\" (YOLO, 2016)\n",
    "- \"U-Net: Convolutional Networks for Biomedical Image Segmentation\" (2015)\n",
    "\n",
    "**Courses:**\n",
    "- CS231n (Stanford): Convolutional Neural Networks for Visual Recognition (free online)\n",
    "\n",
    "---\n",
    "\n",
    "## **12.12 Checkpoint Project: Production-Grade Vision System**\n",
    "\n",
    "Build an end-to-end visual inspection system for manufacturing defect detection.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Dataset:**\n",
    "   - Use MVTec AD (anomaly detection) or similar industrial dataset\n",
    "   - Normal samples: 1000+, Defect samples: 50-100 (realistic imbalance)\n",
    "\n",
    "2. **Architecture:**\n",
    "   - Backbone: EfficientNet-B3 (pre-trained on ImageNet)\n",
    "   - Head: Custom segmentation head for pixel-level defect localization\n",
    "   - Auxiliary: Classification head for defect/norma decision\n",
    "\n",
    "3. **Training Strategy:**\n",
    "   - Self-supervised pretraining on normal images (contrastive learning or autoencoder)\n",
    "   - Fine-tuning with heavy augmentation (defects may vary in appearance)\n",
    "   - Focal Loss or Dice Loss for segmentation (handle class imbalance)\n",
    "\n",
    "4. **Evaluation:**\n",
    "   - Pixel-level: IoU for defect regions\n",
    "   - Image-level: AUROC (Area Under ROC) for anomaly detection\n",
    "   - False Positive Rate @ 95% Recall (industrial standard)\n",
    "\n",
    "5. **Deployment:**\n",
    "   - Export to TorchScript for C++ inference\n",
    "   - Quantization to INT8 (edge device constraint: <100ms on CPU)\n",
    "   - Simple web interface: Upload image → Returns heatmap + defect probability\n",
    "\n",
    "**Deliverables:**\n",
    "- `vision_system/` package with training and inference\n",
    "- Technical report: \"System achieves 98% recall with 2% false positive rate, processing 15 FPS on Intel i7\"\n",
    "- Demo video showing detection on held-out defect samples\n",
    "\n",
    "**Success Criteria:**\n",
    "- AUROC > 0.95 on test set\n",
    "- Inference < 100ms on CPU (single image)\n",
    "- Visualizations clearly highlight defect regions (explainable to factory workers)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 12**\n",
    "\n",
    "*You now possess computer vision expertise from classification to detection. Chapter 13 will cover Recurrent Neural Networks and Sequence Modeling — essential for time series and natural language processing.*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
