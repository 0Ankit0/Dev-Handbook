{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1cd5a47c",
   "metadata": {},
   "source": [
    "Here is **Chapter 25: Transformer Architecture Deep Dive** — the technical foundation of modern large language models.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 25: TRANSFORMER ARCHITECTURE DEEP DIVE**\n",
    "\n",
    "*Engineering the Modern Foundation Model*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Transformers have become the universal architecture for AI, powering everything from GPT-4 to Stable Diffusion. This chapter moves beyond API usage to the underlying engineering principles: attention mechanisms, positional encodings, and efficiency optimizations that enable models to scale to trillions of parameters. You will implement these components from scratch and optimize them for production hardware.\n",
    "\n",
    "**Estimated Time:** 50-60 hours (4 weeks)  \n",
    "**Prerequisites:** Chapters 10-14 (Deep Learning fundamentals, NLP), Chapter 21 (Distributed Training), strong PyTorch proficiency\n",
    "\n",
    "---\n",
    "\n",
    "## **25.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement scaled dot-product attention, multi-head attention, and cross-attention from scratch\n",
    "2. Engineer and compare positional encoding strategies (absolute, relative, rotary, ALiBi) and their impact on length generalization\n",
    "3. Architect encoder-only, decoder-only, and encoder-decoder variants with appropriate pre-training objectives\n",
    "4. Implement memory-efficient attention (FlashAttention) and sparse attention patterns (sliding window, global-local)\n",
    "5. Engineer Multi-Query Attention (MQA) and Grouped-Query Attention (GQA) for inference optimization\n",
    "6. Design Sparse Mixture of Experts (MoE) layers with load-balanced routing\n",
    "\n",
    "---\n",
    "\n",
    "## **25.1 Attention Mechanisms**\n",
    "\n",
    "#### **25.1.1 Scaled Dot-Product Attention (Mathematical Foundation)**\n",
    "\n",
    "The core operation underlying all transformer variants:\n",
    "\n",
    "$$\\text{Attention}(Q, K, V) = \\text{softmax}\\left(\\frac{QK^T}{\\sqrt{d_k}}\\right)V$$\n",
    "\n",
    "Where $Q \\in \\mathbb{R}^{n \\times d_k}$, $K \\in \\mathbb{R}^{m \\times d_k}$, $V \\in \\mathbb{R}^{m \\times d_v}$\n",
    "\n",
    "```python\n",
    "# naive_attention.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class ScaledDotProductAttention(nn.Module):\n",
    "    def __init__(self, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        query: (batch, n_heads, seq_len_q, d_k)\n",
    "        key: (batch, n_heads, seq_len_k, d_k)\n",
    "        value: (batch, n_heads, seq_len_v, d_v) where seq_len_k == seq_len_v\n",
    "        \"\"\"\n",
    "        d_k = query.size(-1)\n",
    "        \n",
    "        # Compute attention scores: Q @ K^T / sqrt(d_k)\n",
    "        scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        # scores: (batch, n_heads, seq_len_q, seq_len_k)\n",
    "        \n",
    "        if mask is not None:\n",
    "            # mask: (batch, 1, 1, seq_len_k) or broadcastable shape\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "        \n",
    "        # Softmax along the key dimension\n",
    "        attn_weights = torch.softmax(scores, dim=-1)\n",
    "        attn_weights = self.dropout(attn_weights)\n",
    "        \n",
    "        # Weighted sum of values\n",
    "        output = torch.matmul(attn_weights, value)\n",
    "        # output: (batch, n_heads, seq_len_q, d_v)\n",
    "        \n",
    "        return output, attn_weights\n",
    "```\n",
    "\n",
    "**Why $\\sqrt{d_k}$?** For large $d_k$, dot products grow in magnitude, pushing softmax into regions with small gradients. Scaling stabilizes training.\n",
    "\n",
    "#### **25.1.2 Multi-Head Attention**\n",
    "\n",
    "Parallel attention heads allow the model to attend to information from different representation subspaces.\n",
    "\n",
    "```python\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model=512, n_heads=8, dropout=0.1):\n",
    "        super().__init__()\n",
    "        assert d_model % n_heads == 0\n",
    "        \n",
    "        self.d_model = d_model\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Linear projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        self.attention = ScaledDotProductAttention(dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        batch_size = query.size(0)\n",
    "        \n",
    "        # 1) Linear projections and reshape for multi-head\n",
    "        # (batch, seq, d_model) -> (batch, seq, n_heads, d_k) -> (batch, n_heads, seq, d_k)\n",
    "        Q = self.W_q(query).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # 2) Apply attention\n",
    "        attn_output, attn_weights = self.attention(Q, K, V, mask)\n",
    "        \n",
    "        # 3) Concatenate heads and final linear\n",
    "        # (batch, n_heads, seq, d_k) -> (batch, seq, n_heads, d_k) -> (batch, seq, d_model)\n",
    "        attn_output = attn_output.transpose(1, 2).contiguous().view(\n",
    "            batch_size, -1, self.d_model\n",
    "        )\n",
    "        \n",
    "        output = self.W_o(attn_output)\n",
    "        return output, attn_weights\n",
    "```\n",
    "\n",
    "#### **25.1.3 Cross-Attention vs. Self-Attention**\n",
    "\n",
    "**Self-Attention:** $Q$, $K$, $V$ all come from same source (previous layer of same sequence). Used in both encoder and decoder for processing input/target sequences.\n",
    "\n",
    "**Cross-Attention:** $Q$ comes from decoder (target), $K$ and $V$ come from encoder final output (source). Enables decoder to attend to encoder representations (machine translation, T5).\n",
    "\n",
    "```python\n",
    "# In encoder-decoder architecture\n",
    "# Encoder self-attention (source text attends to itself)\n",
    "enc_output = self.self_attn(enc_input, enc_input, enc_input, src_mask)\n",
    "\n",
    "# Decoder self-attention (target text attends to itself, masked)\n",
    "dec_output = self.self_attn(dec_input, dec_input, dec_input, tgt_mask)\n",
    "\n",
    "# Cross-attention (decoder queries encoder)\n",
    "cross_output = self.cross_attn(dec_output, enc_output, enc_output, cross_mask)\n",
    "```\n",
    "\n",
    "#### **25.1.4 Linear Attention & Efficient Variants**\n",
    "\n",
    "Standard attention is $O(n^2)$ in sequence length. Linear attention approximates softmax with feature maps to achieve $O(n)$.\n",
    "\n",
    "```python\n",
    "class LinearAttention(nn.Module):\n",
    "    \"\"\"\n",
    "    Linear attention from \"Transformers are RNNs\" (Katharopoulos et al.)\n",
    "    Replaces softmax(Q @ K.T) with elu(Q) @ elu(K).T\n",
    "    \"\"\"\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        self.W_k = nn.Linear(d_model, d_model)\n",
    "        self.W_v = nn.Linear(d_model, d_model)\n",
    "        \n",
    "    def forward(self, query, key, value):\n",
    "        batch_size, seq_len, _ = query.size()\n",
    "        \n",
    "        Q = self.W_q(query).view(batch_size, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(key).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(value).view(batch_size, -1, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Feature map (elu + 1 for non-negativity)\n",
    "        Q = torch.nn.functional.elu(Q) + 1\n",
    "        K = torch.nn.functional.elu(K) + 1\n",
    "        \n",
    "        # KV matrix: (batch, heads, d_k, d_k)\n",
    "        KV = torch.matmul(K.transpose(-2, -1), V)\n",
    "        \n",
    "        # Z denominator: (batch, heads, seq, d_k)\n",
    "        Z = 1 / (torch.matmul(Q, K.sum(dim=-2).unsqueeze(-1)) + 1e-6)\n",
    "        \n",
    "        # Linear attention output\n",
    "        output = torch.matmul(Q, KV) * Z\n",
    "        \n",
    "        return output.transpose(1, 2).contiguous().view(batch_size, seq_len, -1)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **25.2 Positional Encodings**\n",
    "\n",
    "Transformers are permutation-invariant; positional encodings inject sequence order information.\n",
    "\n",
    "#### **25.2.1 Sinusoidal (Original Transformer)**\n",
    "\n",
    "```python\n",
    "class SinusoidalPositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        \n",
    "        div_term = torch.exp(\n",
    "            torch.arange(0, d_model, 2).float() * \n",
    "            (-math.log(10000.0) / d_model)\n",
    "        )\n",
    "        \n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        \n",
    "        pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "        self.register_buffer('pe', pe)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "```\n",
    "\n",
    "**Properties:** Can extrapolate to longer sequences than training, but relative positions decay for distant tokens.\n",
    "\n",
    "#### **25.2.2 Rotary Position Embedding (RoPE)**\n",
    "\n",
    "Used in LLaMA, Mistral, PaLM. Rotates query/key vectors by position-dependent angles, encoding relative position naturally in dot product.\n",
    "\n",
    "```python\n",
    "class RotaryPositionalEmbedding(nn.Module):\n",
    "    \"\"\"\n",
    "    RoPE implementation from RoFormer\n",
    "    \"\"\"\n",
    "    def __init__(self, dim, max_seq_len=2048, base=10000):\n",
    "        super().__init__()\n",
    "        inv_freq = 1.0 / (base ** (torch.arange(0, dim, 2).float() / dim))\n",
    "        self.register_buffer('inv_freq', inv_freq)\n",
    "        self.max_seq_len = max_seq_len\n",
    "        self.dim = dim\n",
    "        \n",
    "    def forward(self, x, seq_len):\n",
    "        # x: (batch, n_heads, seq_len, head_dim)\n",
    "        t = torch.arange(seq_len, device=x.device).type_as(self.inv_freq)\n",
    "        freqs = torch.einsum('i,j->ij', t, self.inv_freq)  # (seq_len, dim/2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)  # (seq_len, dim)\n",
    "        \n",
    "        # Apply rotation\n",
    "        cos, sin = emb.cos(), emb.sin()\n",
    "        x1, x2 = x[..., ::2], x[..., 1::2]\n",
    "        \n",
    "        # Rotate: [x1, x2] @ [[cos, -sin], [sin, cos]]\n",
    "        rotated = torch.stack([\n",
    "            x1 * cos - x2 * sin,\n",
    "            x1 * sin + x2 * cos\n",
    "        ], dim=-1).flatten(-2)\n",
    "        \n",
    "        return rotated\n",
    "\n",
    "# Usage in attention\n",
    "def apply_rotary_pos_emb(q, k, cos, sin):\n",
    "    # q, k: (batch, heads, seq, dim)\n",
    "    q_embed = (q * cos) + (rotate_half(q) * sin)\n",
    "    k_embed = (k * cos) + (rotate_half(k) * sin)\n",
    "    return q_embed, k_embed\n",
    "\n",
    "def rotate_half(x):\n",
    "    x1, x2 = x[..., :x.shape[-1]//2], x[..., x.shape[-1]//2:]\n",
    "    return torch.cat((-x2, x1), dim=-1)\n",
    "```\n",
    "\n",
    "**Advantages:** Better length generalization, relative position encoded naturally in attention scores.\n",
    "\n",
    "#### **25.2.3 ALiBi (Attention with Linear Biases)**\n",
    "\n",
    "No explicit position embeddings; adds static bias to attention scores based on query-key distance.\n",
    "\n",
    "```python\n",
    "class ALiBiAttention(nn.Module):\n",
    "    def __init__(self, n_heads):\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        \n",
    "        # Learned slopes per head\n",
    "        slopes = torch.tensor(self._get_slopes(n_heads))\n",
    "        self.register_buffer('slopes', slopes)  # (n_heads,)\n",
    "        \n",
    "    def _get_slopes(self, n):\n",
    "        # Geometric sequence of slopes: 2^(-8/n) to 2^(-8)\n",
    "        start = 2 ** (-8 / n)\n",
    "        return [start ** i for i in range(n)]\n",
    "    \n",
    "    def forward(self, Q, K, V, mask=None):\n",
    "        batch, heads, seq_len, d_k = Q.shape\n",
    "        \n",
    "        # Compute standard attention scores\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "        \n",
    "        # Generate distance matrix: (seq_len, seq_len)\n",
    "        # distances[i, j] = j - i (how far ahead j is from i)\n",
    "        distances = torch.arange(seq_len, device=Q.device).unsqueeze(0) - \\\n",
    "                   torch.arange(seq_len, device=Q.device).unsqueeze(1)\n",
    "        \n",
    "        # ALiBi bias: m * distance (broadcasted: batch, heads, seq, seq)\n",
    "        bias = self.slopes.view(1, -1, 1, 1) * distances.unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        # Add bias (negative, so farther tokens get lower attention)\n",
    "        scores = scores + bias\n",
    "        \n",
    "        if mask is not None:\n",
    "            scores = scores.masked_fill(mask == 0, float('-inf'))\n",
    "            \n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        return torch.matmul(attn, V)\n",
    "```\n",
    "\n",
    "**Advantages:** Excellent length extrapolation (trained on 1k tokens, inference on 100k), simpler than RoPE.\n",
    "\n",
    "---\n",
    "\n",
    "## **25.3 Advanced Architectures**\n",
    "\n",
    "#### **25.3.1 Encoder-Only (BERT-style)**\n",
    "\n",
    "**Architecture:** Stack of transformer encoder layers (bidirectional attention).  \n",
    "**Pre-training:** Masked Language Modeling (MLM) - predict 15% masked tokens.  \n",
    "**Use cases:** Classification, token-level tasks (NER), embeddings.\n",
    "\n",
    "```python\n",
    "class BertEncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.feed_forward = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        \n",
    "    def forward(self, x, mask=None):\n",
    "        # Pre-norm architecture (more stable for deep networks)\n",
    "        attn_output, _ = self.self_attn(self.norm1(x), self.norm1(x), self.norm1(x), mask)\n",
    "        x = x + self.dropout(attn_output)\n",
    "        \n",
    "        ff_output = self.feed_forward(self.norm2(x))\n",
    "        x = x + self.dropout(ff_output)\n",
    "        \n",
    "        return x\n",
    "```\n",
    "\n",
    "#### **25.3.2 Decoder-Only (GPT/LLaMA-style)**\n",
    "\n",
    "**Architecture:** Causal (autoregressive) masking - each position attends only to previous positions.  \n",
    "**Pre-training:** Causal Language Modeling (CLM) - predict next token.  \n",
    "**Use cases:** Text generation, few-shot learning, modern LLMs.\n",
    "\n",
    "```python\n",
    "class CausalSelfAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, seq_len, _ = x.size()\n",
    "        \n",
    "        # Causal mask: upper triangular (including diagonal) = 1, else 0\n",
    "        mask = torch.tril(torch.ones(seq_len, seq_len, device=x.device)).unsqueeze(0).unsqueeze(0)\n",
    "        \n",
    "        return self.attn(x, x, x, mask)[0]\n",
    "\n",
    "class GPTDecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.ln_1 = nn.LayerNorm(d_model)\n",
    "        self.attn = CausalSelfAttention(d_model, n_heads, dropout)\n",
    "        self.ln_2 = nn.LayerNorm(d_model)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(d_ff, d_model),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln_1(x))\n",
    "        x = x + self.mlp(self.ln_2(x))\n",
    "        return x\n",
    "```\n",
    "\n",
    "#### **25.3.3 Encoder-Decoder (T5/BART/UL2)**\n",
    "\n",
    "**Architecture:** Encoder processes input bidirectionally; decoder generates output autoregressively with cross-attention to encoder.  \n",
    "**Pre-training:** Span corruption (T5: replace spans with sentinel tokens; BART: denoising).  \n",
    "**Use cases:** Translation, summarization, structured prediction.\n",
    "\n",
    "```python\n",
    "class T5Block(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout=0.1):\n",
    "        super().__init__()\n",
    "        # Self-attention (encoder: bidirectional, decoder: causal)\n",
    "        self.self_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        # Cross-attention (decoder only)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, n_heads, dropout)\n",
    "        self.ff = nn.Sequential(\n",
    "            nn.Linear(d_model, d_ff), nn.ReLU(), nn.Linear(d_ff, d_model)\n",
    "        )\n",
    "        self.norms = nn.ModuleList([nn.LayerNorm(d_model) for _ in range(3)])\n",
    "        \n",
    "    def forward(self, x, encoder_output=None, mask=None, cross_mask=None):\n",
    "        # Self-attention\n",
    "        x = x + self.self_attn(self.norms[0](x), self.norms[0](x), \n",
    "                              self.norms[0](x), mask)[0]\n",
    "        \n",
    "        # Cross-attention (if encoder output provided)\n",
    "        if encoder_output is not None:\n",
    "            x = x + self.cross_attn(self.norms[1](x), encoder_output, \n",
    "                                   encoder_output, cross_mask)[0]\n",
    "        \n",
    "        # FFN\n",
    "        x = x + self.ff(self.norms[2](x))\n",
    "        return x\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **25.4 Efficiency Techniques**\n",
    "\n",
    "#### **25.4.1 FlashAttention**\n",
    "\n",
    "Memory-efficient exact attention using tiling to avoid materializing full $N \\times N$ attention matrix in HBM (high bandwidth memory).\n",
    "\n",
    "```python\n",
    "# Conceptual implementation (actual requires CUDA kernels)\n",
    "# Use PyTorch FlashAttention via xFormers or PyTorch 2.0\n",
    "from torch.nn.functional import scaled_dot_product_attention\n",
    "\n",
    "# PyTorch 2.0+ native FlashAttention (automatically selects backend)\n",
    "def efficient_attention(query, key, value, attn_mask=None, dropout_p=0.0, is_causal=False):\n",
    "    \"\"\"\n",
    "    Automatically uses FlashAttention if available (CUDA, head_dim in {64,128})\n",
    "    \"\"\"\n",
    "    return scaled_dot_product_attention(\n",
    "        query, key, value, \n",
    "        attn_mask=attn_mask, \n",
    "        dropout_p=dropout_p, \n",
    "        is_causal=is_causal\n",
    "    )\n",
    "\n",
    "# Memory comparison:\n",
    "# Standard: O(N^2) memory for attention matrix\n",
    "# FlashAttention: O(N) memory (online softmax trick)\n",
    "```\n",
    "\n",
    "#### **25.4.2 Sparse Attention Patterns**\n",
    "\n",
    "**Sliding Window:** Each token attends to $w$ tokens on each side (local attention).  \n",
    "**Global-Local:** Some tokens (e.g., CLS, periodic) attend globally; others locally.\n",
    "\n",
    "```python\n",
    "class SlidingWindowAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, window_size=512):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        # Create banded matrix: 1 if |i-j| <= window_size, else 0\n",
    "        mask = torch.zeros(seq_len, seq_len, device=x.device)\n",
    "        for i in range(seq_len):\n",
    "            start = max(0, i - self.window_size)\n",
    "            end = min(seq_len, i + self.window_size + 1)\n",
    "            mask[i, start:end] = 1\n",
    "            \n",
    "        mask = mask.unsqueeze(0).unsqueeze(0)  # Broadcast over batch, heads\n",
    "        \n",
    "        return self.attn(x, x, x, mask)[0]\n",
    "```\n",
    "\n",
    "#### **25.4.3 Multi-Query & Grouped-Query Attention**\n",
    "\n",
    "**Problem:** Standard MHA caches $K$ and $V$ for each head during inference (memory bandwidth bound).\n",
    "\n",
    "**MQA:** Share single $K$ and $V$ across all heads (used in PaLM, Falcon).  \n",
    "**GQA:** Group heads to share $K$/$V$ (middle ground, used in LLaMA-2).\n",
    "\n",
    "```python\n",
    "class GroupedQueryAttention(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, n_kv_heads=None):\n",
    "        \"\"\"\n",
    "        n_heads: number of query heads\n",
    "        n_kv_heads: number of key/value heads (n_kv_heads <= n_heads)\n",
    "                   If None, standard MHA. If 1, MQA.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.n_heads = n_heads\n",
    "        self.n_kv_heads = n_kv_heads or n_heads\n",
    "        self.d_k = d_model // n_heads\n",
    "        \n",
    "        # Queries: n_heads separate projections\n",
    "        self.W_q = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Keys/Values: n_kv_heads projections (shared)\n",
    "        self.W_k = nn.Linear(d_model, self.n_kv_heads * self.d_k)\n",
    "        self.W_v = nn.Linear(d_model, self.n_kv_heads * self.d_k)\n",
    "        \n",
    "        self.W_o = nn.Linear(d_model, d_model)\n",
    "        \n",
    "        # Repeat K/V heads to match Q heads\n",
    "        self.n_rep = self.n_heads // self.n_kv_heads\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, seq_len, _ = x.shape\n",
    "        \n",
    "        Q = self.W_q(x).view(batch, seq_len, self.n_heads, self.d_k).transpose(1, 2)\n",
    "        K = self.W_k(x).view(batch, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n",
    "        V = self.W_v(x).view(batch, seq_len, self.n_kv_heads, self.d_k).transpose(1, 2)\n",
    "        \n",
    "        # Expand K, V to match Q heads: (batch, n_kv, seq, d_k) -> (batch, n_heads, seq, d_k)\n",
    "        K = K.repeat_interleave(self.n_rep, dim=1)\n",
    "        V = V.repeat_interleave(self.n_rep, dim=1)\n",
    "        \n",
    "        # Standard attention\n",
    "        scores = torch.matmul(Q, K.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "        attn = torch.softmax(scores, dim=-1)\n",
    "        output = torch.matmul(attn, V)\n",
    "        \n",
    "        output = output.transpose(1, 2).contiguous().view(batch, seq_len, -1)\n",
    "        return self.W_o(output)\n",
    "```\n",
    "\n",
    "**Memory Reduction:** MQA reduces KV cache memory by `n_heads`x, critical for long-context inference.\n",
    "\n",
    "---\n",
    "\n",
    "## **25.5 Mixture of Experts (MoE)**\n",
    "\n",
    "Sparse activation: Only subset of parameters active per token, enabling massive scale without proportional compute increase.\n",
    "\n",
    "```python\n",
    "class MoELayer(nn.Module):\n",
    "    def __init__(self, d_model, num_experts=8, top_k=2, expert_capacity=1.0):\n",
    "        super().__init__()\n",
    "        self.num_experts = num_experts\n",
    "        self.top_k = top_k\n",
    "        self.expert_capacity = int(expert_capacity * (d_model / num_experts))\n",
    "        \n",
    "        # Router: which expert(s) for each token\n",
    "        self.router = nn.Linear(d_model, num_experts)\n",
    "        \n",
    "        # Experts (simple FFNs)\n",
    "        self.experts = nn.ModuleList([\n",
    "            nn.Sequential(\n",
    "                nn.Linear(d_model, d_model * 4),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(d_model * 4, d_model)\n",
    "            ) for _ in range(num_experts)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        batch, seq_len, d_model = x.shape\n",
    "        \n",
    "        # Route: (batch, seq, num_experts)\n",
    "        router_logits = self.router(x)\n",
    "        \n",
    "        # Select top-k experts per token\n",
    "        weights, selected_experts = torch.topk(\n",
    "            torch.softmax(router_logits, dim=-1), \n",
    "            self.top_k, \n",
    "            dim=-1\n",
    "        )  # weights: (batch, seq, top_k), selected: (batch, seq, top_k)\n",
    "        \n",
    "        # Initialize output\n",
    "        output = torch.zeros_like(x)\n",
    "        \n",
    "        # Route to experts (simplified, no capacity factor for clarity)\n",
    "        for i, expert in enumerate(self.experts):\n",
    "            # Find which tokens route to this expert\n",
    "            mask = (selected_experts == i).any(dim=-1)  # (batch, seq)\n",
    "            if mask.any():\n",
    "                expert_input = x[mask]  # (num_tokens, d_model)\n",
    "                expert_output = expert(expert_input)\n",
    "                \n",
    "                # Get weights for this expert\n",
    "                expert_weights = weights[mask][selected_experts[mask] == i].unsqueeze(-1)\n",
    "                \n",
    "                output[mask] += expert_weights * expert_output\n",
    "        \n",
    "        # Load balancing loss (auxiliary)\n",
    "        router_prob = torch.softmax(router_logits, dim=-1).mean(dim=[0, 1])\n",
    "        aux_loss = self.num_experts * (router_prob ** 2).mean()  # Encourage uniformity\n",
    "        \n",
    "        return output, aux_loss\n",
    "\n",
    "# Usage in transformer layer\n",
    "class MoETransformerLayer(nn.Module):\n",
    "    def __init__(self, d_model, n_heads, num_experts):\n",
    "        super().__init__()\n",
    "        self.attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.moe = MoELayer(d_model, num_experts)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.norm1(x), self.norm1(x), self.norm1(x))[0]\n",
    "        moe_out, aux_loss = self.moe(self.norm2(x))\n",
    "        x = x + moe_out\n",
    "        return x, aux_loss\n",
    "```\n",
    "\n",
    "**Load Balancing:** Critical to prevent all tokens routing to same expert. Auxiliary loss penalizes uneven routing distributions.\n",
    "\n",
    "---\n",
    "\n",
    "## **25.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Transformer from Scratch**\n",
    "Implement a complete decoder-only transformer (GPT-style):\n",
    "\n",
    "1. **Components:** Implement RoPE, RMSNorm (used in LLaMA), SwiGLU activation\n",
    "2. **Training:** Train on TinyShakespeare dataset for 100k steps\n",
    "3. **Evaluation:** Perplexity, generation quality\n",
    "4. **Analysis:** Visualize attention patterns (which tokens attend to which)\n",
    "\n",
    "**Deliverable:** Clean PyTorch implementation, training curves, generated text samples.\n",
    "\n",
    "### **Lab 2: Positional Encoding Comparison**\n",
    "Benchmark encoding strategies:\n",
    "\n",
    "1. **Implement:** Sinusoidal, Learned, RoPE, ALiBi\n",
    "2. **Length Extrapolation:** Train on 512 tokens, evaluate on 4k tokens. Measure perplexity degradation\n",
    "3. **Efficiency:** Measure training speed and memory for each\n",
    "4. **Visualization:** Plot attention patterns showing how each encodes position\n",
    "\n",
    "**Deliverable:** Comparative analysis report with recommendations per use case.\n",
    "\n",
    "### **Lab 3: FlashAttention Integration**\n",
    "Optimize attention implementation:\n",
    "\n",
    "1. **Baseline:** Standard PyTorch attention, profile memory and speed (sequence lengths 1k, 2k, 4k, 8k)\n",
    "2. **FlashAttention:** Use `torch.nn.functional.scaled_dot_product_attention` or xFormers\n",
    "3. **Memory Analysis:** Plot memory usage vs. sequence length (should be linear vs. quadratic)\n",
    "4. **Throughput:** Measure tokens/sec at different sequence lengths\n",
    "\n",
    "**Deliverable:** Performance benchmark showing FlashAttention benefits at scale.\n",
    "\n",
    "### **Lab 4: Grouped Query Attention**\n",
    "Implement memory-efficient inference:\n",
    "\n",
    "1. **Standard MHA:** Implement baseline with KV caching\n",
    "2. **GQA:** Convert to 8 query heads, 2 key/value heads\n",
    "3. **KV Cache Comparison:** Measure memory usage for 4k context window, batch size 32\n",
    "4. **Quality Check:** Fine-tune both on downstream task, compare accuracy\n",
    "\n",
    "**Deliverable:** Memory/speed analysis with minimal quality degradation validation.\n",
    "\n",
    "---\n",
    "\n",
    "## **25.7 Common Pitfalls**\n",
    "\n",
    "1. **Attention Head Dimension Mismatch:** Forgetting that $d_{model}$ must be divisible by $n_{heads}$. **Solution:** Assert in `__init__`, use integer division carefully.\n",
    "\n",
    "2. **Causal Mask Leakage:** Incorrect mask shape allowing future tokens to be seen in decoder. **Solution:** Verify mask is lower triangular including diagonal; test with small sequence manually.\n",
    "\n",
    "3. **RoPE Frequency Issues:** Incorrect application of rotation (applying to Q/K after projection instead of decomposed components). **Solution:** Apply to queries and keys before matmul, verify rotation matrix properties.\n",
    "\n",
    "4. **MoE Load Imbalance:** All experts converge to similar functions or one expert dominates. **Solution:** Ensure auxiliary loss weight is sufficient (0.01-0.1), implement capacity factor to limit tokens per expert.\n",
    "\n",
    "5. **FlashAttention Compatibility:** Trying to use FlashAttention on CPU or with incompatible head dimensions. **Solution:** Always check `torch.backends.cuda.flash_sdp_enabled()` and fall back to standard attention gracefully.\n",
    "\n",
    "---\n",
    "\n",
    "## **25.8 Interview Questions**\n",
    "\n",
    "**Q1:** Explain why Transformers use LayerNorm instead of BatchNorm, and why Pre-LN (before attention) is preferred over Post-LN in deep models.\n",
    "*A: BatchNorm computes statistics across batch dimension, problematic for variable-length sequences (padding masks complicate statistics) and small batches in NLP. LayerNorm normalizes across feature dimension, independent of batch size and sequence length. Pre-LN (residual around normalized input) vs. Post-LN (normalize after residual): Pre-LN prevents gradient vanishing in deep networks (>24 layers) by keeping gradients flowing through residual connections unattenuated. Post-LN can cause training instability as depth increases, though it sometimes achieves slightly better final performance if it converges.*\n",
    "\n",
    "**Q2:** Compare RoPE and ALiBi for length extrapolation. When would you choose one over the other?\n",
    "*A: RoPE encodes position via rotation matrices in Q/K dot products, providing good relative position bias and extrapolation (especially with NTK-aware scaling). ALiBi adds linear bias to attention scores based on distance, no learned position embeddings. ALiBi extrapolates better to very long sequences (100k+) without any fine-tuning, simpler implementation. RoPE is more flexible (base frequency adjustments) and widely used in open-source models (LLaMA). Choose ALiBi for applications requiring extreme length generalization without training; choose RoPE for fine-grained control and compatibility with existing ecosystem.*\n",
    "\n",
    "**Q3:** How does Multi-Query Attention reduce memory bandwidth during inference, and what is the quality trade-off?\n",
    "*A: In standard MHA, each head has separate K/V projections, so KV cache stores (batch, n_heads, seq_len, d_k). In MQA, single K/V shared across all heads, reducing cache size by n_heads. During autoregressive generation, memory bandwidth bound loading KV cache from HBM to SRAM; MQA reduces this by 8-32x (depending on head count), significant speedup for long contexts. Trade-off: Reduced expressiveness (less diverse attention patterns), but GQA (grouped-query, intermediate between MHA and MQA) typically recovers 95%+ of MHA quality with 50% memory reduction.*\n",
    "\n",
    "**Q4:** Explain the MoE load balancing problem and how to solve it.\n",
    "*A: Without regularization, router networks collapse to always selecting few \"favored\" experts (self-reinforcing: expert gets trained more → better performance → selected more). This defeats purpose of conditional computation. Solutions: (1) Auxiliary load balancing loss (weighted sum of router probability times fraction of tokens routed), encouraging uniform distribution, (2) Noisy Top-K Gating (add noise to router logits before softmax, annealed during training), (3) Capacity factor: Hard limit tokens per expert, overflow tokens routed to next expert or dropped, (4) Expert choice routing (reverse: tokens choose top-k experts vs. experts choose top-k tokens).*\n",
    "\n",
    "**Q5:** Why does FlashAttention reduce memory usage from $O(N^2)$ to $O(N)$, and what hardware characteristics enable this?\n",
    "*A: Standard attention materializes $N \\times N$ attention matrix in HBM (high bandwidth memory) to compute softmax and weighted sum. FlashAttention uses tiling: split Q/K/V into blocks fitting in SRAM (fast on-chip memory, ~100x faster than HBM). Compute softmax incrementally using online softmax algorithm (track running max and sum), fuse all operations (load Q/K/V blocks, compute attention, write output) in one CUDA kernel, avoiding HBM round-trips. Key hardware: Large SRAM capacity (A100: 192KB per SM) and high HBM bandwidth to feed compute. FlashAttention is IO-aware, accounting for memory hierarchy.*\n",
    "\n",
    "---\n",
    "\n",
    "## **25.9 Further Reading**\n",
    "\n",
    "**Papers:**\n",
    "- \"Attention Is All You Need\" (Vaswani et al., 2017) - Original transformer\n",
    "- \"RoFormer: Enhanced Transformer with Rotary Position Embedding\" (Su et al., 2021)\n",
    "- \"ALiBi: Train Short, Test Long\" (Press et al., 2022)\n",
    "- \"FlashAttention: Fast and Memory-Efficient Exact Attention\" (Dao et al., 2022)\n",
    "- \"GQA: Training Generalized Multi-Query Transformer Models\" (Ainslie et al., 2023)\n",
    "- \"Outrageously Large Neural Networks: The Sparsely-Gated Mixture-of-Experts Layer\" (Shazeer et al., 2017)\n",
    "\n",
    "**Code References:**\n",
    "- **LLaMA implementation:** facebookresearch/llama (RMSNorm, RoPE, SwiGLU)\n",
    "- **xFormers:** Memory-efficient attention blocks\n",
    "- **FlashAttention:** Dao-AILab/flash-attention (CUDA kernels)\n",
    "\n",
    "---\n",
    "\n",
    "## **25.10 Checkpoint Project: Efficient Large Language Model**\n",
    "\n",
    "Implement a production-ready 1B parameter decoder-only language model with efficiency optimizations.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Architecture:**\n",
    "   - 24 layers, d_model=2048, n_heads=32\n",
    "   - RoPE positional embeddings (base 10000, later experiment with NTK scaling)\n",
    "   - SwiGLU activation (2/3d FF dimension for parameter efficiency)\n",
    "   - RMSNorm instead of LayerNorm (better training stability)\n",
    "\n",
    "2. **Efficiency:**\n",
    "   - FlashAttention integration (via PyTorch 2.0 or xFormers)\n",
    "   - Grouped-Query Attention: 32 query heads, 8 key/value heads\n",
    "   - Gradient checkpointing (trade compute for memory to fit larger batch sizes)\n",
    "\n",
    "3. **Training:**\n",
    "   - Pre-train on C4 or OpenWebText subset\n",
    "   - Mixed precision (BF16) with gradient scaling\n",
    "   - Learning rate warmup (4k steps) then cosine decay\n",
    "   - Validate on perplexity and downstream tasks (HellaSwag, PIQA zero-shot)\n",
    "\n",
    "4. **Scaling:**\n",
    "   - Demonstrate training on 8k sequence length (enable FlashAttention-2 if possible)\n",
    "   - Measure throughput: tokens/sec/GPU\n",
    "   - Memory profiling: Peak activation memory per layer\n",
    "\n",
    "5. **Evaluation:**\n",
    "   - Length extrapolation test: Train on 2k, test on 8k without fine-tuning\n",
    "   - Compare GQA vs. MHA inference speed at batch size 1, 4k context (should show 2-3x speedup)\n",
    "\n",
    "**Deliverables:**\n",
    "- `efficient_llm/` directory with model implementation\n",
    "- Training scripts with distributed data parallel support\n",
    "- Benchmark report: Training loss curves, perplexity, downstream task scores\n",
    "- Technical report: \"Engineering Trade-offs in Efficient Transformer Design\"\n",
    "\n",
    "**Success Criteria:**\n",
    "- Model trains to < 15 perplexity on validation set\n",
    "- FlashAttention shows >2x speedup over standard attention at 4k sequence length\n",
    "- GQA model achieves < 2% perplexity degradation vs. MHA baseline\n",
    "- Successfully generates coherent text at 8k tokens without position interpolation\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 25**\n",
    "\n",
    "*You now understand the architectural foundations of modern foundation models. Chapter 26 covers Generative AI & Diffusion Models.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
