{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "69a0cbb8",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 27: MULTIMODAL AI**\n",
    "\n",
    "*Unified Perception Across Vision, Language, and Audio*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "While unimodal models excel within single domains, real-world intelligence requires integrating information across vision, language, audio, and sensor data. This chapter covers the architectures and training paradigms that align heterogeneous modalities into unified representation spaces, enabling capabilities like visual question answering, image captioning, and audio-visual speech recognition.\n",
    "\n",
    "**Estimated Time:** 40-50 hours (3-4 weeks)  \n",
    "**Prerequisites:** Chapters 14, 25 (Transformers), Chapter 26 (Generative AI), computer vision fundamentals\n",
    "\n",
    "---\n",
    "\n",
    "## **27.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement contrastive learning objectives (InfoNCE) to align vision and language representations\n",
    "2. Architect vision-language models using projection layers and Q-Formers (BLIP-2 style)\n",
    "3. Fine-tune large language models for multimodal understanding (LLaVA, InstructBLIP patterns)\n",
    "4. Process audio spectrograms and implement speech recognition pipelines (Whisper architecture)\n",
    "5. Design video understanding systems that model temporal dependencies across frames\n",
    "6. Evaluate multimodal systems for grounding, hallucination, and cross-modal retrieval\n",
    "\n",
    "---\n",
    "\n",
    "## **27.1 Vision-Language Foundations**\n",
    "\n",
    "#### **27.1.1 Contrastive Language-Image Pre-training (CLIP)**\n",
    "\n",
    "Aligns images and text in shared embedding space via contrastive loss.\n",
    "\n",
    "```python\n",
    "# clip_model.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import CLIPTokenizer, CLIPTextModel\n",
    "from torchvision import models\n",
    "\n",
    "class CLIPModel(nn.Module):\n",
    "    def __init__(self, embed_dim=512, temperature=0.07):\n",
    "        super().__init__()\n",
    "        self.temperature = nn.Parameter(torch.ones([]) * temperature)\n",
    "        \n",
    "        # Image encoder (Vision Transformer or ResNet)\n",
    "        self.image_encoder = models.vision_transformer.vit_b_16(pretrained=True)\n",
    "        self.image_encoder.heads = nn.Identity()  # Remove classification head\n",
    "        \n",
    "        # Project to common space\n",
    "        self.image_projection = nn.Linear(768, embed_dim)\n",
    "        \n",
    "        # Text encoder (Transformer)\n",
    "        self.text_encoder = CLIPTextModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "        self.text_projection = nn.Linear(512, embed_dim)\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Encode images: (batch, 3, 224, 224) -> (batch, embed_dim)\n",
    "        image_features = self.image_encoder(images)\n",
    "        image_features = self.image_projection(image_features)\n",
    "        image_features = F.normalize(image_features, dim=-1)\n",
    "        \n",
    "        # Encode text: (batch, seq_len) -> (batch, embed_dim)\n",
    "        text_outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        text_features = text_outputs.pooler_output  # [CLS] token\n",
    "        text_features = self.text_projection(text_features)\n",
    "        text_features = F.normalize(text_features, dim=-1)\n",
    "        \n",
    "        # Compute similarity matrix: (batch, batch)\n",
    "        logits = torch.matmul(image_features, text_features.t()) / self.temperature\n",
    "        \n",
    "        # Symmetric contrastive loss (InfoNCE)\n",
    "        batch_size = images.size(0)\n",
    "        labels = torch.arange(batch_size, device=images.device)\n",
    "        \n",
    "        loss_i2t = F.cross_entropy(logits, labels)  # Image-to-text\n",
    "        loss_t2i = F.cross_entropy(logits.t(), labels)  # Text-to-image\n",
    "        \n",
    "        return (loss_i2t + loss_t2i) / 2\n",
    "    \n",
    "    def encode_image(self, images):\n",
    "        features = self.image_encoder(images)\n",
    "        return F.normalize(self.image_projection(features), dim=-1)\n",
    "    \n",
    "    def encode_text(self, input_ids, attention_mask):\n",
    "        outputs = self.text_encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        features = self.text_projection(outputs.pooler_output)\n",
    "        return F.normalize(features, dim=-1)\n",
    "\n",
    "# Zero-shot classification\n",
    "def zero_shot_predict(model, images, class_names, tokenizer):\n",
    "    text_inputs = tokenizer([f\"a photo of a {c}\" for c in class_names], \n",
    "                           return_tensors=\"pt\", padding=True)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        image_features = model.encode_image(images)\n",
    "        text_features = model.encode_text(**text_inputs)\n",
    "        \n",
    "        similarity = (image_features @ text_features.T).softmax(dim=-1)\n",
    "    \n",
    "    return similarity.argmax(dim=-1)\n",
    "```\n",
    "\n",
    "**Training Details:**\n",
    "- **Dataset:** 400M image-text pairs (web-crawled)\n",
    "- **Batch Size:** 32,768 (larger batches improve contrastive learning)\n",
    "- **Data Augmentation:** Random square crop, color jitter for images; random truncation for text\n",
    "\n",
    "#### **27.1.2 ALIGN (Alternative Approach)**\n",
    "\n",
    "Uses larger noisier datasets with dual encoder architecture and normalized temperature-scaled cross entropy.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.2 Multimodal Architectures**\n",
    "\n",
    "#### **27.2.1 Flamingo-Style: Frozen LLM with Perceiver Resampler**\n",
    "\n",
    "Inject visual tokens into frozen language model using gated cross-attention.\n",
    "\n",
    "```python\n",
    "class PerceiverResampler(nn.Module):\n",
    "    \"\"\"\n",
    "    Compresses variable-length image features to fixed number of latents\n",
    "    \"\"\"\n",
    "    def __init__(self, dim=768, num_latents=64, num_layers=6):\n",
    "        super().__init__()\n",
    "        self.latents = nn.Parameter(torch.randn(num_latents, dim))\n",
    "        \n",
    "        self.layers = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(d_model=dim, nhead=8, batch_first=True)\n",
    "            for _ in range(num_layers)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, image_features):\n",
    "        # image_features: (batch, num_patches, dim)\n",
    "        batch_size = image_features.size(0)\n",
    "        latents = self.latents.unsqueeze(0).expand(batch_size, -1, -1)\n",
    "        \n",
    "        # Cross-attention: latents attend to image features\n",
    "        x = torch.cat([latents, image_features], dim=1)\n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "            \n",
    "        return x[:, :self.latents.size(0), :]  # Return only latent queries\n",
    "\n",
    "class GatedCrossAttention(nn.Module):\n",
    "    def __init__(self, dim, num_heads):\n",
    "        super().__init__()\n",
    "        self.attn = nn.MultiheadAttention(dim, num_heads, batch_first=True)\n",
    "        self.gate = nn.Parameter(torch.zeros(1))\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        \n",
    "    def forward(self, x, context):\n",
    "        # x: language hidden states, context: visual features\n",
    "        attn_out, _ = self.attn(x, context, context)\n",
    "        # Gating mechanism: gradually introduce visual info during training\n",
    "        return self.norm(x + torch.tanh(self.gate) * attn_out)\n",
    "\n",
    "class MultimodalLLM(nn.Module):\n",
    "    def __init__(self, llm_model_name, vision_encoder):\n",
    "        super().__init__()\n",
    "        # Load frozen LLM\n",
    "        self.llm = AutoModelForCausalLM.from_pretrained(llm_model_name)\n",
    "        self.llm.requires_grad_(False)  # Freeze LLM\n",
    "        \n",
    "        # Vision components\n",
    "        self.vision_encoder = vision_encoder\n",
    "        self.perceiver = PerceiverResampler(dim=768)\n",
    "        \n",
    "        # Insert cross-attention layers every N transformer blocks\n",
    "        self.gated_layers = nn.ModuleList([\n",
    "            GatedCrossAttention(dim=4096, num_heads=32) \n",
    "            for _ in range(4)  # 4 visual injection points\n",
    "        ])\n",
    "        \n",
    "    def forward(self, images, input_ids, attention_mask):\n",
    "        # Extract visual tokens\n",
    "        visual_features = self.vision_encoder(images)  # (batch, 256, 768)\n",
    "        visual_tokens = self.perceiver(visual_features)  # (batch, 64, 768)\n",
    "        \n",
    "        # Project to LLM dimension if needed\n",
    "        visual_tokens = self.visual_projection(visual_tokens)\n",
    "        \n",
    "        # Process through LLM with gated cross-attention\n",
    "        hidden_states = self.llm.get_input_embeddings()(input_ids)\n",
    "        \n",
    "        for i, layer in enumerate(self.llm.model.layers):\n",
    "            hidden_states = layer(hidden_states, attention_mask=attention_mask)[0]\n",
    "            \n",
    "            # Inject vision at specific layers\n",
    "            if i in [3, 7, 11, 15]:\n",
    "                visual_idx = [3, 7, 11, 15].index(i)\n",
    "                hidden_states = self.gated_layers[visual_idx](hidden_states, visual_tokens)\n",
    "        \n",
    "        # Language modeling head\n",
    "        logits = self.llm.lm_head(hidden_states)\n",
    "        return logits\n",
    "```\n",
    "\n",
    "#### **27.2.2 LLaVA Architecture (Linear Projection)**\n",
    "\n",
    "Simpler approach: single linear layer projects vision features to LLM embedding space.\n",
    "\n",
    "```python\n",
    "class LLaVAModel(nn.Module):\n",
    "    def __init__(self, vision_tower, llm, mm_projector_type=\"linear\"):\n",
    "        super().__init__()\n",
    "        self.vision_tower = vision_tower  # CLIP ViT\n",
    "        self.llm = llm  # Vicuna/LLaMA\n",
    "        \n",
    "        if mm_projector_type == \"linear\":\n",
    "            self.mm_projector = nn.Linear(vision_tower.config.hidden_size, \n",
    "                                         llm.config.hidden_size)\n",
    "        else:\n",
    "            # MLP projector\n",
    "            self.mm_projector = nn.Sequential(\n",
    "                nn.Linear(vision_tower.config.hidden_size, llm.config.hidden_size),\n",
    "                nn.GELU(),\n",
    "                nn.Linear(llm.config.hidden_size, llm.config.hidden_size)\n",
    "            )\n",
    "        \n",
    "    def prepare_inputs_labels_for_multimodal(\n",
    "        self, input_ids, images, attention_mask\n",
    "    ):\n",
    "        # Encode images\n",
    "        with torch.no_grad():\n",
    "            image_features = self.vision_tower(images).last_hidden_state[:, 1:]  # Remove CLS\n",
    "        \n",
    "        # Project to LLM space\n",
    "        image_embeds = self.mm_projector(image_features)\n",
    "        \n",
    "        # Insert image tokens into text sequence\n",
    "        # <image> token in input_ids is replaced with image_embeds\n",
    "        batch_size = input_ids.size(0)\n",
    "        new_input_embeds = []\n",
    "        \n",
    "        for batch_idx in range(batch_size):\n",
    "            cur_input_ids = input_ids[batch_idx]\n",
    "            image_token_idx = torch.where(cur_input_ids == IMAGE_TOKEN_INDEX)[0]\n",
    "            \n",
    "            cur_input_embeds = self.llm.get_input_embeddings()(cur_input_ids)\n",
    "            \n",
    "            # Replace <image> token position with image embeddings\n",
    "            num_images = image_token_idx.size(0)\n",
    "            if num_images > 0:\n",
    "                cur_image_features = image_embeds[batch_idx]\n",
    "                cur_input_embeds[image_token_idx] = cur_image_features[:num_images]\n",
    "            \n",
    "            new_input_embeds.append(cur_input_embeds)\n",
    "        \n",
    "        # Stack and pass to LLM\n",
    "        inputs_embeds = torch.stack(new_input_embeds)\n",
    "        return self.llm(inputs_embeds=inputs_embeds, attention_mask=attention_mask)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **27.3 Training Strategies**\n",
    "\n",
    "#### **27.3.1 Instruction Tuning for Multimodal Models**\n",
    "\n",
    "Convert diverse tasks to instruction-following format:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"instruction\": \"What is unusual about this image?\",\n",
    "  \"input\": \"<image>\",\n",
    "  \"output\": \"The cat is sitting on a computer keyboard while code is being written, which is unusual because cats typically obstruct keyboards rather than assist with programming.\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Data Formatting:**\n",
    "- **Pre-training:** Alignment (captioning) on image-text pairs\n",
    "- **Instruction Tuning:** Visual question answering, OCR, reasoning with chain-of-thought\n",
    "\n",
    "#### **27.3.2 Parameter-Efficient Fine-Tuning**\n",
    "\n",
    "Freeze vision encoder and LLM, only train projection layers and LoRA adapters.\n",
    "\n",
    "```python\n",
    "from peft import get_peft_model, LoraConfig, TaskType\n",
    "\n",
    "# Configure LoRA for LLM attention layers\n",
    "peft_config = LoraConfig(\n",
    "    task_type=TaskType.CAUSAL_LM,\n",
    "    inference_mode=False,\n",
    "    r=8,  # LoRA rank\n",
    "    lora_alpha=32,\n",
    "    lora_dropout=0.1,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"]  # Target attention projections\n",
    ")\n",
    "\n",
    "model.llm = get_peft_model(model.llm, peft_config)\n",
    "\n",
    "# Only unfreeze projection layer\n",
    "for param in model.mm_projector.parameters():\n",
    "    param.requires_grad = True\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **27.4 Speech & Audio**\n",
    "\n",
    "#### **27.4.1 Whisper Architecture (Encoder-Decoder for ASR)**\n",
    "\n",
    "```python\n",
    "# Whisper-style model\n",
    "class WhisperModel(nn.Module):\n",
    "    def __init__(self, n_mels=80, n_audio_ctx=1500, n_text_ctx=448):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Audio encoder (conv + transformer)\n",
    "        self.conv1 = nn.Conv1d(n_mels, 384, kernel_size=3, padding=1)\n",
    "        self.conv2 = nn.Conv1d(384, 384, kernel_size=3, stride=2, padding=1)\n",
    "        \n",
    "        self.audio_encoder = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(d_model=384, nhead=6, dim_feedforward=1536),\n",
    "            num_layers=6\n",
    "        )\n",
    "        \n",
    "        # Text decoder (causal)\n",
    "        self.token_embedding = nn.Embedding(51864, 384)  # Vocab size\n",
    "        self.text_decoder = nn.TransformerDecoder(\n",
    "            nn.TransformerDecoderLayer(d_model=384, nhead=6),\n",
    "            num_layers=6\n",
    "        )\n",
    "        \n",
    "        # Multi-task format tokens <|transcribe|>, <|translate|>, <|notimestamps|>\n",
    "        self.special_tokens = {\n",
    "            \"startoftranscript\": 50257,\n",
    "            \"transcribe\": 50358,\n",
    "            \"translate\": 50359,\n",
    "            \"notimestamps\": 50362\n",
    "        }\n",
    "        \n",
    "    def forward(self, mel_spectrogram, decoder_input_ids):\n",
    "        # Audio encoding\n",
    "        x = F.gelu(self.conv1(mel_spectrogram))\n",
    "        x = F.gelu(self.conv2(x))\n",
    "        x = x.permute(0, 2, 1)  # (batch, time, features)\n",
    "        \n",
    "        audio_features = self.audio_encoder(x)\n",
    "        \n",
    "        # Text decoding with cross-attention to audio\n",
    "        text_embeds = self.token_embedding(decoder_input_ids)\n",
    "        \n",
    "        # Causal mask for decoder\n",
    "        tgt_mask = nn.Transformer.generate_square_subsequent_mask(decoder_input_ids.size(1))\n",
    "        \n",
    "        output = self.text_decoder(\n",
    "            text_embeds, \n",
    "            memory=audio_features,\n",
    "            tgt_mask=tgt_mask.to(text_embeds.device)\n",
    "        )\n",
    "        \n",
    "        return output\n",
    "\n",
    "# Preprocessing: Log-Mel spectrogram\n",
    "def log_mel_spectrogram(audio, n_mels=80, n_fft=400, hop_length=160):\n",
    "    \"\"\"\n",
    "    audio: torch tensor of raw waveform (16kHz)\n",
    "    \"\"\"\n",
    "    window = torch.hann_window(n_fft)\n",
    "    stft = torch.stft(audio, n_fft, hop_length, window=window, return_complex=True)\n",
    "    magnitudes = stft.abs() ** 2\n",
    "    \n",
    "    # Mel filterbank\n",
    "    mel_filters = torch.load(\"mel_filters.pt\")  # Pre-computed\n",
    "    mel_spec = mel_filters @ magnitudes\n",
    "    \n",
    "    log_spec = torch.clamp(mel_spec, min=1e-10).log10()\n",
    "    log_spec = torch.maximum(log_spec, log_spec.max() - 8.0)\n",
    "    log_spec = (log_spec + 4.0) / 4.0  # Normalize\n",
    "    \n",
    "    return log_spec\n",
    "```\n",
    "\n",
    "#### **27.4.2 Audio-Language Alignment (AudioCLIP)**\n",
    "\n",
    "Extend CLIP to audio by adding audio encoder (AST or CNN) and tri-modal contrastive loss.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.5 Video Understanding**\n",
    "\n",
    "#### **27.5.1 Video Vision Transformers**\n",
    "\n",
    "Handle temporal dimension: tubelet embedding (3D patches) or frame sampling + temporal attention.\n",
    "\n",
    "```python\n",
    "class VideoTransformer(nn.Module):\n",
    "    def __init__(self, img_size=224, patch_size=16, num_frames=8, embed_dim=768):\n",
    "        super().__init__()\n",
    "        self.patch_embed = nn.Conv3d(\n",
    "            3, embed_dim, \n",
    "            kernel_size=(2, patch_size, patch_size),  # Temporal patch size 2\n",
    "            stride=(2, patch_size, patch_size)\n",
    "        )\n",
    "        \n",
    "        # Positional embedding: spatial + temporal\n",
    "        num_patches_per_frame = (img_size // patch_size) ** 2\n",
    "        num_temporal = num_frames // 2\n",
    "        self.pos_embed = nn.Parameter(\n",
    "            torch.zeros(1, num_temporal * num_patches_per_frame, embed_dim)\n",
    "        )\n",
    "        \n",
    "        self.blocks = nn.ModuleList([\n",
    "            nn.TransformerEncoderLayer(embed_dim, num_heads=12, batch_first=True)\n",
    "            for _ in range(12)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, time, height, width)\n",
    "        x = self.patch_embed(x)  # (batch, embed, T', H', W')\n",
    "        x = x.flatten(2).transpose(1, 2)  # (batch, T'*H'*W', embed)\n",
    "        \n",
    "        x = x + self.pos_embed\n",
    "        \n",
    "        for block in self.blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        return x\n",
    "```\n",
    "\n",
    "#### **27.5.2 Video-Text Retrieval**\n",
    "\n",
    "Similar to CLIP but with temporal aggregation (mean pooling or transformer pooling over time).\n",
    "\n",
    "---\n",
    "\n",
    "## **27.6 Advanced: Binding Multiple Modalities**\n",
    "\n",
    "#### **27.6.1 ImageBind (One Embedding Space for 6 Modalities)**\n",
    "\n",
    "Aligns images, text, audio, depth, thermal, and IMU data using image as binding mechanism.\n",
    "\n",
    "```python\n",
    "class ImageBindModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        # Separate encoders for each modality\n",
    "        self.image_encoder = ViTModel()\n",
    "        self.text_encoder = TextTransformer()\n",
    "        self.audio_encoder = AudioSpectrogramTransformer()\n",
    "        self.depth_encoder = ViTModel()  # Shares weights with image or separate\n",
    "        \n",
    "        # Projection heads to common space\n",
    "        self.image_proj = nn.Linear(768, 1024)\n",
    "        self.text_proj = nn.Linear(768, 1024)\n",
    "        self.audio_proj = nn.Linear(768, 1024)\n",
    "        \n",
    "    def forward(self, image=None, text=None, audio=None, depth=None):\n",
    "        embeddings = {}\n",
    "        \n",
    "        if image is not None:\n",
    "            embeddings['image'] = F.normalize(self.image_proj(self.image_encoder(image)), dim=-1)\n",
    "        if text is not None:\n",
    "            embeddings['text'] = F.normalize(self.text_proj(self.text_encoder(text)), dim=-1)\n",
    "        if audio is not None:\n",
    "            embeddings['audio'] = F.normalize(self.audio_proj(self.audio_encoder(audio)), dim=-1)\n",
    "            \n",
    "        return embeddings\n",
    "    \n",
    "    def contrastive_loss(self, embeddings):\n",
    "        # All pairs of modalities present in batch should align\n",
    "        loss = 0\n",
    "        modalities = list(embeddings.keys())\n",
    "        \n",
    "        for i in range(len(modalities)):\n",
    "            for j in range(i+1, len(modalities)):\n",
    "                m1, m2 = modalities[i], modalities[j]\n",
    "                logits = embeddings[m1] @ embeddings[m2].T / self.temperature\n",
    "                labels = torch.arange(len(logits))\n",
    "                loss += (F.cross_entropy(logits, labels) + \n",
    "                        F.cross_entropy(logits.T, labels)) / 2\n",
    "                        \n",
    "        return loss\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **27.7 Workbook Labs**\n",
    "\n",
    "### **Lab 1: CLIP from Scratch**\n",
    "Train a CLIP-style model on COCO or Conceptual Captions subset:\n",
    "\n",
    "1. **Data Pipeline:** Load image-text pairs, apply augmentations\n",
    "2. **Architecture:** ViT-B/32 + Text Transformer\n",
    "3. **Training:** Implement symmetric InfoNCE loss, train for 10 epochs\n",
    "4. **Evaluation:** Zero-shot classification on CIFAR-10, image-text retrieval Recall@K\n",
    "\n",
    "**Deliverable:** Trained model with retrieval accuracy metrics.\n",
    "\n",
    "### **Lab 2: Visual Instruction Tuning**\n",
    "Fine-tune multimodal model on visual QA dataset:\n",
    "\n",
    "1. **Base Model:** Load LLaVA-1.5 architecture (pretrained projector + Vicuna)\n",
    "2. **Dataset:** Convert VQA v2 to instruction format\n",
    "3. **Fine-tuning:** LoRA on LLM, full tuning on projection layer\n",
    "4. **Inference:** Generate answers for held-out images, evaluate with GPT-4 judge or exact match\n",
    "\n",
    "**Deliverable:** Fine-tuned checkpoint and example outputs showing reasoning.\n",
    "\n",
    "### **Lab 3: Speech Recognition Pipeline**\n",
    "Implement Whisper-style ASR:\n",
    "\n",
    "1. **Preprocessing:** Convert audio to log-Mel spectrograms\n",
    "2. **Model:** Encoder-decoder transformer\n",
    "3. **Training:** Train on LibriSpeech subset with CTC or cross-entropy\n",
    "4. **Decoding:** Implement greedy and beam search decoding\n",
    "\n",
    "**Deliverable:** WER (Word Error Rate) evaluation on test-clean.\n",
    "\n",
    "### **Lab 4: Multimodal Retrieval System**\n",
    "Build cross-modal search engine:\n",
    "\n",
    "1. **Indexing:** Encode image database with CLIP\n",
    "2. **Search:** Text-to-image and image-to-image retrieval\n",
    "3. **Ranking:** Re-rank with fine-tuned model on domain data\n",
    "4. **UI:** Gradio interface for searching photo collection\n",
    "\n",
    "**Deliverable:** Working retrieval system with mAP (mean Average Precision) evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.8 Common Pitfalls**\n",
    "\n",
    "1. **Modality Collapse:** One modality dominates, others ignored. **Solution:** Gradient masking, balanced sampling, or contrastive loss temperature tuning per modality.\n",
    "\n",
    "2. **Misalignment in Tokenization:** Different sequence lengths for vision patches vs. text tokens causing fusion issues. **Solution:** Careful position ID management, separate position embeddings per modality.\n",
    "\n",
    "3. **Hallucination in VLM:** Model generates text not grounded in image. **Solution:** Instruction tuning with negative examples (unanswerable questions), grounding losses (text-image matching during generation).\n",
    "\n",
    "4. **Audio Preprocessing Errors:** Incorrect spectrogram parameters (window size, hop length) causing information loss. **Solution:** Validate reconstruction from spectrogram, standardize on 16kHz, 80-bin mel.\n",
    "\n",
    "5. **Temporal Misalignment in Video:** Treating all frames equally ignores temporal structure. **Solution:** Temporal positional encodings, temporal attention pooling, or 3D convolutions.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.9 Interview Questions**\n",
    "\n",
    "**Q1:** How does CLIP enable zero-shot classification, and what are its limitations?\n",
    "*A: CLIP learns joint embedding space where text descriptions and images are close if semantically similar. Zero-shot works by embedding class names as text prompts (\"a photo of a [class]\"), computing similarity with image, selecting highest score. Limitations: (1) Requires good text descriptions (poor on rare or fine-grained concepts), (2) Biases from training data (web-crawled pairs often noisy or biased), (3) Struggles with abstract concepts or systematic generalization (e.g., counting), (4) Poor performance on specialized domains not in pre-training (medical, satellite).*\n",
    "\n",
    "**Q2:** Explain the difference between Flamingo-style and LLaVA-style multimodal architectures.\n",
    "*A: Flamingo: Keeps LLM frozen, inserts gated cross-attention layers at specific transformer depths to inject visual information. Uses Perceiver resampler to compress variable image features to fixed tokens. More parameter-efficient, better for few-shot. LLaVA: Projects vision features directly into LLM embedding space via simple MLP/linear layer, treating images as \"foreign language\" tokens. Fine-tunes LLM with LoRA. Simpler, easier to train, but may disrupt pre-trained LLM knowledge if not careful. Flamingo better preserves LLM capabilities; LLaVA more scalable for instruction tuning.*\n",
    "\n",
    "**Q3:** How do you handle different sequence lengths (e.g., 50 vision tokens vs. 512 text tokens) in multimodal transformers?\n",
    "*A: (1) Projection to same dimension then concatenation along sequence dimension, (2) Separate position embeddings for each modality (vision positions 0-49, text 50-561), (3) Modality-specific segment embeddings added to indicate source, (4) Attention masks ensure causality only within text (vision can attend bidirectionally), (5) Perceiver resampler to compress vision to fixed small number of tokens (e.g., 64) regardless of image resolution, simplifying sequence management.*\n",
    "\n",
    "**Q4:** What is the \"curse of multimodality\" in training, and how do you mitigate it?\n",
    "*A: Different modalities have different scales, convergence rates, and optimal hyperparameters. Vision often converges slower than text; audio has different spectral characteristics. Mitigation: (1) Modality-specific optimizers or learning rates, (2) Gradient clipping per modality, (3) Balanced sampling ensuring equal representation, (4) Frozen encoders for majority of training then gradual unfreezing, (5) Normalization layers per modality before fusion, (6) Contrastive losses balance gradient contributions.*\n",
    "\n",
    "**Q5:** Design a system to detect inconsistency between video and audio (e.g., lip-sync deepfake detection).\n",
    "*A: Architecture: Two-stream network processing video frames (mouth region) and audio spectrograms. Options: (1) Late fusion: Embed both with modality-specific encoders, compute cosine similarity (should be high for real, low for fake), (2) Early fusion: Concatenate features at multiple time scales, train binary classifier, (3) SyncNet-style: Contrastive learning where aligned AV pairs are positive, temporally shifted are negative. Key: Temporal alignment is critical\u2014use audio features shifted by video latency (typically 0-200ms). Augmentation: Train with various audio delays to learn robustness.*\n",
    "\n",
    "---\n",
    "\n",
    "## **27.10 Further Reading**\n",
    "\n",
    "**Papers:**\n",
    "- \"Learning Transferable Visual Models From Natural Language Supervision\" (CLIP, Radford et al., 2021)\n",
    "- \"Flamingo: A Visual Language Model for Few-Shot Learning\" (Alayrac et al., 2022)\n",
    "- \"Visual Instruction Tuning\" (LLaVA, Liu et al., 2023)\n",
    "- \"Robust Speech Recognition via Large-Scale Weak Supervision\" (Whisper, Radford et al., 2022)\n",
    "- \"ImageBind: One Embedding Space To Bind Them All\" (Girdhar et al., 2023)\n",
    "\n",
    "**Tools:**\n",
    "- **Hugging Face Transformers:** CLIP, LLaVA, Whisper implementations\n",
    "- **OpenCLIP:** Open-source CLIP training code\n",
    "- **Salesforce LAVIS:** Library for vision-language tasks\n",
    "\n",
    "---\n",
    "\n",
    "## **27.11 Checkpoint Project: Multimodal Document AI**\n",
    "\n",
    "Build a system to understand and reason about PDF documents containing text, figures, and tables.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Document Parsing:**\n",
    "   - Extract text (OCR if needed), images, and layout information\n",
    "   - Use LayoutLM or custom detection model for region classification (text block, figure, table)\n",
    "\n",
    "2. **Multimodal Encoding:**\n",
    "   - Text: Standard transformer tokenizer\n",
    "   - Figures: CLIP or fine-tuned ResNet visual encoder\n",
    "   - Layout: 2D positional embeddings (bounding box coordinates)\n",
    "\n",
    "3. **Reasoning Engine:**\n",
    "   - Fine-tuned multimodal LLM (LLaVA-style) for document QA\n",
    "   - Support questions requiring joint reasoning over text and figures (\"What does Figure 3 indicate about the trend in Table 1?\")\n",
    "\n",
    "4. **Retrieval:**\n",
    "   - Index documents by multimodal embeddings\n",
    "   - Cross-modal search: Find documents containing specific visual concepts\n",
    "\n",
    "5. **Evaluation:**\n",
    "   - Dataset: DocVQA or custom annotated technical manuals\n",
    "   - Metrics: ANLS (Average Normalized Levenshtein Similarity) for text answers, accuracy for multiple choice\n",
    "\n",
    "**Deliverables:**\n",
    "- `document_ai/` pipeline with OCR, layout analysis, and QA\n",
    "- Fine-tuned model checkpoint\n",
    "- Evaluation report on document understanding benchmark\n",
    "- Demo: Interactive QA over technical manual PDF\n",
    "\n",
    "**Success Criteria:**\n",
    "- Answer questions requiring joint text+figure reasoning with >70% accuracy\n",
    "- Successfully retrieve documents based on visual content descriptions\n",
    "- Handle multi-page documents with cross-page references\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 27**\n",
    "\n",
    "*You now master multimodal AI systems. Chapter 28 covers AI Safety, Alignment, and Robustness.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='26. generative_AI_and_diffusion_models.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='28. ai_safety_alignment_and_robustness.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}