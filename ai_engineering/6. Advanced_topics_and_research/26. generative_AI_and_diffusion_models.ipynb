{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cc47c66d",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 26: GENERATIVE AI & DIFFUSION MODELS**\n",
    "\n",
    "*Engineering Systems for Creation and Synthesis*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Generative AI represents a paradigm shift from discriminative models (classification, prediction) to creative systems that produce novel content. This chapter covers the full spectrum of generative architectures: autoregressive models for discrete sequences, Variational Autoencoders (VAEs) for structured latent spaces, Generative Adversarial Networks (GANs) for high-fidelity synthesis, and Diffusion Models that power modern image generation systems like Stable Diffusion and DALL-E 3.\n",
    "\n",
    "**Estimated Time:** 45-55 hours (4 weeks)  \n",
    "**Prerequisites:** Chapters 10-14 (Deep Learning, CNNs, Transformers), Chapter 25 (Advanced Transformers), strong PyTorch/TensorFlow skills\n",
    "\n",
    "---\n",
    "\n",
    "## **26.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement autoregressive generative models (PixelCNN, WaveNet) for discrete and continuous data\n",
    "2. Engineer Variational Autoencoders with stable training objectives (ELBO, KL-annealing, \u03b2-VAE)\n",
    "3. Architect and train GANs with spectral normalization, progressive growing, and mode collapse mitigation\n",
    "4. Implement Denoising Diffusion Probabilistic Models (DDPM) from scratch with variance scheduling\n",
    "5. Optimize diffusion inference with DDIM, classifier-free guidance, and latent diffusion architectures\n",
    "6. Design conditional generation systems (ControlNet, inpainting) for controlled content creation\n",
    "\n",
    "---\n",
    "\n",
    "## **26.1 Autoregressive Models**\n",
    "\n",
    "Autoregressive models factorize the joint distribution as a product of conditionals: $p(x) = \\prod_{i} p(x_i | x_{<i})$\n",
    "\n",
    "#### **26.1.1 PixelCNN (Image Generation)**\n",
    "\n",
    "Uses masked convolutions to preserve autoregressive property (predict pixel given previous pixels).\n",
    "\n",
    "```python\n",
    "# pixelcnn.py\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class MaskedConv2d(nn.Conv2d):\n",
    "    \"\"\"\n",
    "    Type A mask: Current pixel not included (for first layer)\n",
    "    Type B mask: Current pixel included (for subsequent layers)\n",
    "    \"\"\"\n",
    "    def __init__(self, mask_type, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.register_buffer('mask', torch.zeros_like(self.weight))\n",
    "        \n",
    "        _, _, kH, kW = self.weight.size()\n",
    "        self.mask[:, :, :kH//2, :] = 1\n",
    "        self.mask[:, :, kH//2, :kW//2] = 1\n",
    "        \n",
    "        if mask_type == 'B':\n",
    "            self.mask[:, :, kH//2, kW//2] = 1\n",
    "    \n",
    "    def forward(self, x):\n",
    "        self.weight.data *= self.mask\n",
    "        return super().forward(x)\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = MaskedConv2d('B', in_channels, in_channels//2, 1)\n",
    "        self.conv2 = MaskedConv2d('B', in_channels//2, in_channels//2, 3, padding=1)\n",
    "        self.conv3 = MaskedConv2d('B', in_channels//2, in_channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        return F.relu(x + residual)\n",
    "\n",
    "class PixelCNN(nn.Module):\n",
    "    def __init__(self, in_channels=3, hidden_channels=128, num_classes=256):\n",
    "        super().__init__()\n",
    "        self.in_conv = MaskedConv2d('A', in_channels, hidden_channels, 7, padding=3)\n",
    "        \n",
    "        self.residual_blocks = nn.ModuleList([\n",
    "            ResidualBlock(hidden_channels) for _ in range(15)\n",
    "        ])\n",
    "        \n",
    "        # Output 256-way softmax per color channel (quantized to 8-bit)\n",
    "        self.out_conv = nn.Sequential(\n",
    "            MaskedConv2d('B', hidden_channels, hidden_channels, 1),\n",
    "            nn.ReLU(),\n",
    "            MaskedConv2d('B', hidden_channels, in_channels * num_classes, 1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (batch, 3, H, W) normalized to [0, 1]\n",
    "        x = self.in_conv(x)\n",
    "        \n",
    "        for block in self.residual_blocks:\n",
    "            x = block(x)\n",
    "            \n",
    "        x = self.out_conv(x)\n",
    "        return x.view(x.size(0), 256, 3, x.size(2), x.size(3))  # (B, 256, 3, H, W)\n",
    "    \n",
    "    def sample(self, batch_size=16, image_size=28, device='cuda'):\n",
    "        samples = torch.zeros(batch_size, 3, image_size, image_size).to(device)\n",
    "        \n",
    "        for i in range(image_size):\n",
    "            for j in range(image_size):\n",
    "                for c in range(3):  # RGB channels\n",
    "                    logits = self.forward(samples)[:, :, c, i, j]\n",
    "                    probs = F.softmax(logits, dim=1)\n",
    "                    pixel = torch.multinomial(probs, 1).float() / 255.0\n",
    "                    samples[:, c, i, j] = pixel.squeeze(1)\n",
    "        \n",
    "        return samples\n",
    "```\n",
    "\n",
    "**Limitations:** Slow sequential sampling (must generate pixel-by-pixel), difficulty modeling long-range dependencies compared to transformers.\n",
    "\n",
    "#### **26.1.2 WaveNet (Audio Generation)**\n",
    "\n",
    "Dilated causal convolutions for raw audio generation (24kHz samples).\n",
    "\n",
    "```python\n",
    "class CausalConv1d(nn.Conv1d):\n",
    "    def __init__(self, *args, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.causal_padding = self.kernel_size[0] - 1\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = F.pad(x, (self.causal_padding, 0))\n",
    "        return super().forward(x)\n",
    "\n",
    "class WaveNetBlock(nn.Module):\n",
    "    def __init__(self, channels, kernel_size=2, dilation=1):\n",
    "        super().__init__()\n",
    "        self.filter_conv = CausalConv1d(channels, channels, kernel_size, dilation=dilation)\n",
    "        self.gate_conv = CausalConv1d(channels, channels, kernel_size, dilation=dilation)\n",
    "        self.residual_conv = nn.Conv1d(channels, channels, 1)\n",
    "        self.skip_conv = nn.Conv1d(channels, channels, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        filter_out = torch.tanh(self.filter_conv(x))\n",
    "        gate_out = torch.sigmoid(self.gate_conv(x))\n",
    "        z = filter_out * gate_out\n",
    "        \n",
    "        residual = self.residual_conv(z)\n",
    "        skip = self.skip_conv(z)\n",
    "        \n",
    "        return x + residual, skip  # Residual for next layer, skip for output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **26.2 Variational Autoencoders (VAEs)**\n",
    "\n",
    "#### **26.2.1 The ELBO Objective**\n",
    "\n",
    "Maximize the Evidence Lower Bound:\n",
    "\n",
    "$$\\mathcal{L}(\\theta, \\phi) = \\mathbb{E}_{q_\\phi(z|x)}[\\log p_\\theta(x|z)] - D_{KL}(q_\\phi(z|x) || p(z))$$\n",
    "\n",
    "Reconstruction term + KL divergence regularization.\n",
    "\n",
    "```python\n",
    "# vae.py\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim=784, hidden_dim=400, latent_dim=20):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Encoder\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc_mu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_var = nn.Linear(hidden_dim, latent_dim)\n",
    "        \n",
    "        # Decoder\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(latent_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, input_dim),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def encode(self, x):\n",
    "        h = self.encoder(x)\n",
    "        mu = self.fc_mu(h)\n",
    "        log_var = self.fc_var(h)  # Log variance for stability\n",
    "        return mu, log_var\n",
    "    \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        \"\"\"\n",
    "        Reparameterization trick: z = mu + sigma * epsilon\n",
    "        Allows backpropagation through stochastic node\n",
    "        \"\"\"\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    def decode(self, z):\n",
    "        return self.decoder(z)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mu, log_var = self.encode(x.view(-1, 784))\n",
    "        z = self.reparameterize(mu, log_var)\n",
    "        recon = self.decode(z)\n",
    "        return recon, mu, log_var\n",
    "    \n",
    "    def loss_function(self, recon, x, mu, log_var, kl_weight=1.0):\n",
    "        # Reconstruction loss (binary cross-entropy for MNIST)\n",
    "        BCE = F.binary_cross_entropy(recon, x.view(-1, 784), reduction='sum')\n",
    "        \n",
    "        # KL Divergence: -0.5 * sum(1 + log(sigma^2) - mu^2 - sigma^2)\n",
    "        KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "        \n",
    "        return BCE + kl_weight * KLD, BCE, KLD\n",
    "```\n",
    "\n",
    "**Training Tips:**\n",
    "- **KL Annealing:** Gradually increase KL weight from 0 to 1 over epochs (prevents posterior collapse early in training)\n",
    "- **Free Bits:** Modify ELBO to only penalize KL if it drops below threshold (encourages use of latent dimensions)\n",
    "\n",
    "#### **26.2.2 \u03b2-VAE (Disentangled Representations)**\n",
    "\n",
    "Scale KL term by \u03b2 > 1 to encourage factorized latent representations.\n",
    "\n",
    "```python\n",
    "def beta_vae_loss(recon, x, mu, log_var, beta=4.0):\n",
    "    BCE = F.binary_cross_entropy(recon, x, reduction='sum')\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + beta * KLD\n",
    "```\n",
    "\n",
    "#### **26.2.3 VQ-VAE (Vector Quantized VAE)**\n",
    "\n",
    "Discrete latent space using codebook quantization, used as foundation for DALL-E and Stable Diffusion autoencoders.\n",
    "\n",
    "```python\n",
    "class VectorQuantizer(nn.Module):\n",
    "    def __init__(self, num_embeddings, embedding_dim, commitment_cost=0.25):\n",
    "        super().__init__()\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_embeddings = num_embeddings\n",
    "        self.commitment_cost = commitment_cost\n",
    "        \n",
    "        self.embeddings = nn.Embedding(num_embeddings, embedding_dim)\n",
    "        self.embeddings.weight.data.uniform_(-1/num_embeddings, 1/num_embeddings)\n",
    "        \n",
    "    def forward(self, inputs):\n",
    "        # inputs: (batch, height, width, embedding_dim)\n",
    "        # Flatten to (B*H*W, D)\n",
    "        flat_input = inputs.reshape(-1, self.embedding_dim)\n",
    "        \n",
    "        # Calculate distances to codebook vectors\n",
    "        distances = (torch.sum(flat_input**2, dim=1, keepdim=True) \n",
    "                    + torch.sum(self.embeddings.weight**2, dim=1)\n",
    "                    - 2 * torch.matmul(flat_input, self.embeddings.weight.t()))\n",
    "        \n",
    "        # Encoding\n",
    "        encoding_indices = torch.argmin(distances, dim=1).unsqueeze(1)\n",
    "        encodings = torch.zeros(encoding_indices.size(0), self.num_embeddings, device=inputs.device)\n",
    "        encodings.scatter_(1, encoding_indices, 1)\n",
    "        \n",
    "        # Quantize\n",
    "        quantized = torch.matmul(encodings, self.embeddings.weight).view(inputs.shape)\n",
    "        \n",
    "        # Loss: Straight-through estimator\n",
    "        e_latent_loss = F.mse_loss(quantized.detach(), inputs)  # Commitment loss\n",
    "        q_latent_loss = F.mse_loss(quantized, inputs.detach())   # Codebook loss\n",
    "        loss = q_latent_loss + self.commitment_cost * e_latent_loss\n",
    "        \n",
    "        # Straight-through estimator (gradient flows through quantized as if identity)\n",
    "        quantized = inputs + (quantized - inputs).detach()\n",
    "        \n",
    "        return quantized, loss, encoding_indices.view(inputs.shape[:-1])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **26.3 Generative Adversarial Networks (GANs)**\n",
    "\n",
    "#### **26.3.1 DCGAN Architecture**\n",
    "\n",
    "```python\n",
    "class Generator(nn.Module):\n",
    "    def __init__(self, latent_dim=100, ngf=64, nc=3):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            # Input: latent_dim x 1 x 1\n",
    "            nn.ConvTranspose2d(latent_dim, ngf * 8, 4, 1, 0, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 8),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*8) x 4 x 4\n",
    "            nn.ConvTranspose2d(ngf * 8, ngf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 4),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*4) x 8 x 8\n",
    "            nn.ConvTranspose2d(ngf * 4, ngf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf * 2),\n",
    "            nn.ReLU(True),\n",
    "            # State: (ngf*2) x 16 x 16\n",
    "            nn.ConvTranspose2d(ngf * 2, ngf, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ngf),\n",
    "            nn.ReLU(True),\n",
    "            # State: ngf x 32 x 32\n",
    "            nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n",
    "            nn.Tanh()\n",
    "            # Output: nc x 64 x 64\n",
    "        )\n",
    "        \n",
    "    def forward(self, z):\n",
    "        return self.main(z.unsqueeze(-1).unsqueeze(-1))\n",
    "\n",
    "class Discriminator(nn.Module):\n",
    "    def __init__(self, nc=3, ndf=64):\n",
    "        super().__init__()\n",
    "        self.main = nn.Sequential(\n",
    "            nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf, ndf * 2, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 2),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 2, ndf * 4, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 4),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 4, ndf * 8, 4, 2, 1, bias=False),\n",
    "            nn.BatchNorm2d(ndf * 8),\n",
    "            nn.LeakyReLU(0.2, inplace=True),\n",
    "            nn.Conv2d(ndf * 8, 1, 4, 1, 0, bias=False),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        return self.main(x).view(-1, 1).squeeze(1)\n",
    "```\n",
    "\n",
    "#### **26.3.2 Training Stability Improvements**\n",
    "\n",
    "**WGAN-GP (Wasserstein GAN with Gradient Penalty):**\n",
    "```python\n",
    "def gradient_penalty(discriminator, real_data, fake_data, device):\n",
    "    batch_size = real_data.size(0)\n",
    "    alpha = torch.rand(batch_size, 1, 1, 1, device=device)\n",
    "    interpolates = alpha * real_data + (1 - alpha) * fake_data\n",
    "    interpolates.requires_grad_(True)\n",
    "    \n",
    "    disc_interpolates = discriminator(interpolates)\n",
    "    gradients = torch.autograd.grad(\n",
    "        outputs=disc_interpolates,\n",
    "        inputs=interpolates,\n",
    "        grad_outputs=torch.ones_like(disc_interpolates),\n",
    "        create_graph=True,\n",
    "        retain_graph=True\n",
    "    )[0]\n",
    "    \n",
    "    gradients = gradients.view(batch_size, -1)\n",
    "    gradient_norm = gradients.norm(2, dim=1)\n",
    "    penalty = ((gradient_norm - 1) ** 2).mean()\n",
    "    return penalty\n",
    "\n",
    "# Loss: D_loss = -mean(D(real)) + mean(D(fake)) + lambda * GP\n",
    "# G_loss = -mean(D(fake))\n",
    "```\n",
    "\n",
    "**Spectral Normalization:** Constrain Lipschitz constant of discriminator by normalizing weights.\n",
    "\n",
    "```python\n",
    "from torch.nn.utils import spectral_norm\n",
    "\n",
    "# Apply to discriminator layers\n",
    "self.conv = spectral_norm(nn.Conv2d(in_channels, out_channels, kernel_size))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **26.4 Diffusion Models**\n",
    "\n",
    "#### **26.4.1 DDPM (Denoising Diffusion Probabilistic Models)**\n",
    "\n",
    "Forward process adds Gaussian noise over $T$ timesteps; reverse process learns to denoise.\n",
    "\n",
    "```python\n",
    "# ddpm.py\n",
    "import math\n",
    "\n",
    "class Diffusion:\n",
    "    def __init__(self, timesteps=1000, beta_start=1e-4, beta_end=0.02, device='cuda'):\n",
    "        self.timesteps = timesteps\n",
    "        self.device = device\n",
    "        \n",
    "        # Linear variance schedule\n",
    "        self.betas = torch.linspace(beta_start, beta_end, timesteps).to(device)\n",
    "        self.alphas = 1.0 - self.betas\n",
    "        self.alphas_cumprod = torch.cumprod(self.alphas, dim=0)\n",
    "        self.alphas_cumprod_prev = F.pad(self.alphas_cumprod[:-1], (1, 0), value=1.0)\n",
    "        \n",
    "        # Precompute values for sampling\n",
    "        self.sqrt_alphas_cumprod = torch.sqrt(self.alphas_cumprod)\n",
    "        self.sqrt_one_minus_alphas_cumprod = torch.sqrt(1.0 - self.alphas_cumprod)\n",
    "        self.sqrt_recip_alphas = torch.sqrt(1.0 / self.alphas)\n",
    "        \n",
    "        # Posterior variance\n",
    "        self.posterior_variance = (self.betas * (1.0 - self.alphas_cumprod_prev) / \n",
    "                                  (1.0 - self.alphas_cumprod))\n",
    "        \n",
    "    def q_sample(self, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Forward diffusion: q(x_t | x_0)\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "            \n",
    "        sqrt_alphas_cumprod_t = extract(self.sqrt_alphas_cumprod, t, x_start.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x_start.shape)\n",
    "        \n",
    "        return sqrt_alphas_cumprod_t * x_start + sqrt_one_minus_alphas_cumprod_t * noise\n",
    "    \n",
    "    def p_losses(self, denoise_model, x_start, t, noise=None):\n",
    "        \"\"\"\n",
    "        Training loss: MSE between predicted and actual noise\n",
    "        \"\"\"\n",
    "        if noise is None:\n",
    "            noise = torch.randn_like(x_start)\n",
    "            \n",
    "        x_noisy = self.q_sample(x_start, t, noise)\n",
    "        predicted_noise = denoise_model(x_noisy, t)\n",
    "        \n",
    "        loss = F.mse_loss(predicted_noise, noise)\n",
    "        return loss\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, model, x, t, t_index):\n",
    "        \"\"\"\n",
    "        Single step of reverse diffusion\n",
    "        \"\"\"\n",
    "        betas_t = extract(self.betas, t, x.shape)\n",
    "        sqrt_one_minus_alphas_cumprod_t = extract(self.sqrt_one_minus_alphas_cumprod, t, x.shape)\n",
    "        sqrt_recip_alphas_t = extract(self.sqrt_recip_alphas, t, x.shape)\n",
    "        \n",
    "        # Equation 11 in DDPM paper\n",
    "        model_mean = sqrt_recip_alphas_t * (\n",
    "            x - betas_t * model(x, t) / sqrt_one_minus_alphas_cumprod_t\n",
    "        )\n",
    "        \n",
    "        if t_index == 0:\n",
    "            return model_mean\n",
    "        else:\n",
    "            posterior_variance_t = extract(self.posterior_variance, t, x.shape)\n",
    "            noise = torch.randn_like(x)\n",
    "            return model_mean + torch.sqrt(posterior_variance_t) * noise\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=16, channels=3):\n",
    "        \"\"\"\n",
    "        Generate images by iteratively denoising\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        shape = (batch_size, channels, image_size, image_size)\n",
    "        img = torch.randn(shape, device=self.device)\n",
    "        \n",
    "        for i in tqdm(reversed(range(self.timesteps)), desc='Sampling'):\n",
    "            t = torch.full((batch_size,), i, device=self.device, dtype=torch.long)\n",
    "            img = self.p_sample(model, img, t, i)\n",
    "        \n",
    "        return img\n",
    "\n",
    "def extract(a, t, shape):\n",
    "    \"\"\"Extract values at timesteps t and reshape\"\"\"\n",
    "    batch_size = t.shape[0]\n",
    "    out = a.gather(-1, t.cpu())\n",
    "    return out.reshape(batch_size, *((1,) * (len(shape) - 1))).to(t.device)\n",
    "\n",
    "# U-Net architecture for denoising (simplified)\n",
    "class UNet(nn.Module):\n",
    "    def __init__(self, dim=64, dim_mults=(1, 2, 4, 8), channels=3):\n",
    "        super().__init__()\n",
    "        # Time embedding\n",
    "        time_dim = dim * 4\n",
    "        self.time_mlp = nn.Sequential(\n",
    "            SinusoidalPositionEmbeddings(dim),\n",
    "            nn.Linear(dim, time_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(time_dim, time_dim)\n",
    "        )\n",
    "        \n",
    "        # Downsampling/upsampling with attention\n",
    "        self.downs = nn.ModuleList([])\n",
    "        self.ups = nn.ModuleList([])\n",
    "        # ... (standard U-Net with ResNet blocks and attention)\n",
    "        \n",
    "    def forward(self, x, time):\n",
    "        t = self.time_mlp(time)\n",
    "        # U-Net forward with skip connections\n",
    "        return x\n",
    "```\n",
    "\n",
    "#### **26.4.2 DDIM (Denoising Diffusion Implicit Models)**\n",
    "\n",
    "Deterministic sampling (DDPM is stochastic), faster inference (10-50 steps vs. 1000).\n",
    "\n",
    "```python\n",
    "class DDIM(Diffusion):\n",
    "    def __init__(self, timesteps=1000, ddim_timesteps=50, **kwargs):\n",
    "        super().__init__(timesteps, **kwargs)\n",
    "        # Subsample timesteps for sampling\n",
    "        self.ddim_timesteps = torch.linspace(0, timesteps-1, ddim_timesteps).long()\n",
    "        \n",
    "    @torch.no_grad()\n",
    "    def sample(self, model, image_size, batch_size=16, channels=3, eta=0.0):\n",
    "        \"\"\"\n",
    "        eta=0: deterministic (DDIM)\n",
    "        eta=1: stochastic (DDPM)\n",
    "        \"\"\"\n",
    "        model.eval()\n",
    "        shape = (batch_size, channels, image_size, image_size)\n",
    "        img = torch.randn(shape, device=self.device)\n",
    "        \n",
    "        for i in tqdm(reversed(range(len(self.ddim_timesteps))), desc='DDIM Sampling'):\n",
    "            t = torch.full((batch_size,), self.ddim_timesteps[i], device=self.device, dtype=torch.long)\n",
    "            prev_t = self.ddim_timesteps[i-1] if i > 0 else torch.tensor([0])\n",
    "            \n",
    "            # Predict x_0 from x_t\n",
    "            alpha_t = self.alphas_cumprod[t]\n",
    "            alpha_prev = self.alphas_cumprod[prev_t] if i > 0 else torch.tensor([1.0])\n",
    "            \n",
    "            predicted_noise = model(img, t)\n",
    "            \n",
    "            # Predict x_0\n",
    "            pred_x0 = (img - torch.sqrt(1 - alpha_t) * predicted_noise) / torch.sqrt(alpha_t)\n",
    "            \n",
    "            # Direction pointing to x_t\n",
    "            dir_xt = torch.sqrt(1 - alpha_prev - eta**2 * (1 - alpha_prev)/(1 - alpha_t) * (1 - alpha_t/alpha_prev)) * predicted_noise\n",
    "            \n",
    "            # Random noise (only if eta > 0)\n",
    "            noise = torch.randn_like(img) if i > 0 else torch.zeros_like(img)\n",
    "            sigma_t = eta * torch.sqrt((1 - alpha_prev)/(1 - alpha_t) * (1 - alpha_t/alpha_prev))\n",
    "            \n",
    "            img = torch.sqrt(alpha_prev) * pred_x0 + dir_xt + sigma_t * noise\n",
    "        \n",
    "        return img\n",
    "```\n",
    "\n",
    "#### **26.4.3 Classifier-Free Guidance (CFG)**\n",
    "\n",
    "Trade-off between diversity and fidelity by combining conditional and unconditional predictions.\n",
    "\n",
    "```python\n",
    "def guided_denoise_step(model, x, t, context, guidance_scale=7.5):\n",
    "    \"\"\"\n",
    "    context: text embedding or class label\n",
    "    guidance_scale: 1 = no guidance, 7.5 = high guidance (standard)\n",
    "    \"\"\"\n",
    "    # Conditional prediction\n",
    "    noise_pred_cond = model(x, t, context=context)\n",
    "    \n",
    "    # Unconditional prediction (null embedding)\n",
    "    noise_pred_uncond = model(x, t, context=torch.zeros_like(context))\n",
    "    \n",
    "    # Guided prediction: extrapolate in direction of conditional\n",
    "    noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_cond - noise_pred_uncond)\n",
    "    \n",
    "    return noise_pred\n",
    "```\n",
    "\n",
    "#### **26.4.4 Latent Diffusion Models (Stable Diffusion)**\n",
    "\n",
    "Perform diffusion in latent space of VQ-VAE rather than pixel space (computationally efficient).\n",
    "\n",
    "```python\n",
    "# Conceptual pipeline\n",
    "class StableDiffusion:\n",
    "    def __init__(self, vae, text_encoder, unet, scheduler):\n",
    "        self.vae = vae  # Pretrained VQ-VAE\n",
    "        self.text_encoder = text_encoder  # CLIP\n",
    "        self.unet = unet  # Denoising U-Net\n",
    "        self.scheduler = scheduler\n",
    "        \n",
    "    def encode_prompt(self, prompt):\n",
    "        return self.text_encoder(prompt)\n",
    "    \n",
    "    @torch.no_grad()\n",
    "    def __call__(self, prompt, height=512, width=512, num_inference_steps=50, guidance_scale=7.5):\n",
    "        # 1. Encode text\n",
    "        text_embeddings = self.encode_prompt(prompt)\n",
    "        uncond_embeddings = self.encode_prompt(\"\")  # Negative prompt\n",
    "        \n",
    "        # 2. Prepare latents (random noise in latent space)\n",
    "        latents = torch.randn((1, 4, height//8, width//8))  # 64x64 for 512px image\n",
    "        \n",
    "        # 3. Denoising loop in latent space\n",
    "        self.scheduler.set_timesteps(num_inference_steps)\n",
    "        \n",
    "        for t in self.scheduler.timesteps:\n",
    "            # Expand latents for classifier-free guidance\n",
    "            latent_model_input = torch.cat([latents] * 2)\n",
    "            \n",
    "            # Predict noise\n",
    "            noise_pred = self.unet(latent_model_input, t, encoder_hidden_states=torch.cat([uncond_embeddings, text_embeddings]))\n",
    "            noise_pred_uncond, noise_pred_text = noise_pred.chunk(2)\n",
    "            noise_pred = noise_pred_uncond + guidance_scale * (noise_pred_text - noise_pred_uncond)\n",
    "            \n",
    "            # Compute previous noisy sample\n",
    "            latents = self.scheduler.step(noise_pred, t, latents).prev_sample\n",
    "        \n",
    "        # 4. Decode latents to image\n",
    "        image = self.vae.decode(latents)\n",
    "        return image\n",
    "```\n",
    "\n",
    "#### **26.4.5 ControlNet**\n",
    "\n",
    "Inject spatial conditioning (canny edges, depth maps, poses) into diffusion model via trainable copies of U-Net blocks.\n",
    "\n",
    "```python\n",
    "class ControlNet(nn.Module):\n",
    "    def __init__(self, base_unet, control_channels=3):\n",
    "        super().__init__()\n",
    "        # Lock base model parameters\n",
    "        for param in base_unet.parameters():\n",
    "            param.requires_grad = False\n",
    "            \n",
    "        # Create trainable copies of encoding blocks\n",
    "        self.controlnet_blocks = nn.ModuleList([\n",
    "            copy_of_unet_block() for _ in range(num_encoding_blocks)\n",
    "        ])\n",
    "        \n",
    "        # Zero convolution to connect control features\n",
    "        self.zero_convs = nn.ModuleList([\n",
    "            nn.Conv2d(block_out_channels, block_out_channels, 1) for _ in range(num_blocks)\n",
    "        ])\n",
    "        \n",
    "    def forward(self, x, t, context, control_image):\n",
    "        # Process control image through ControlNet blocks\n",
    "        control_features = []\n",
    "        h = control_image\n",
    "        for block, zero_conv in zip(self.controlnet_blocks, self.zero_convs):\n",
    "            h = block(h)\n",
    "            control_features.append(zero_conv(h))  # Initially zeros, gradually learn\n",
    "        \n",
    "        # Add control features to base U-Net skip connections\n",
    "        output = self.base_unet(x, t, context, control_features=control_features)\n",
    "        return output\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **26.5 Flow Matching & Consistency Models**\n",
    "\n",
    "#### **26.5.1 Flow Matching (Rectified Flow)**\n",
    "\n",
    "Directly regress vector field of probability path, avoiding stochastic sampling.\n",
    "\n",
    "```python\n",
    "class FlowMatching:\n",
    "    def __init__(self, sigma_min=0.0):\n",
    "        self.sigma_min = sigma_min\n",
    "        \n",
    "    def sample_location_and_conditional_flow(self, x0, x1, t):\n",
    "        \"\"\"\n",
    "        x0: noise (source), x1: data (target)\n",
    "        \"\"\"\n",
    "        # Sample probability path\n",
    "        xt = (1 - t) * x0 + t * x1 + sigma_min * torch.randn_like(x0)\n",
    "        \n",
    "        # Vector field: direction from x0 to x1\n",
    "        ut = x1 - x0\n",
    "        \n",
    "        return xt, ut\n",
    "    \n",
    "    def loss(self, model, x1):\n",
    "        \"\"\"\n",
    "        MSE between predicted and target vector field\n",
    "        \"\"\"\n",
    "        x0 = torch.randn_like(x1)\n",
    "        t = torch.rand(x1.size(0), device=x1.device)\n",
    "        \n",
    "        xt, ut = self.sample_location_and_conditional_flow(x0, x1, t.unsqueeze(-1))\n",
    "        \n",
    "        vt = model(xt, t)  # Predicted velocity\n",
    "        return F.mse_loss(vt, ut)\n",
    "```\n",
    "\n",
    "#### **26.5.2 Consistency Models**\n",
    "\n",
    "Learn to map any point on diffusion path directly to data (single-step generation).\n",
    "\n",
    "```python\n",
    "class ConsistencyModel(nn.Module):\n",
    "    def __init__(self, num_distillation_steps=800000):\n",
    "        super().__init__()\n",
    "        self.student = UNetModel()\n",
    "        self.teacher = copy.deepcopy(self.student)\n",
    "        self.teacher.eval()\n",
    "        \n",
    "    def update_teacher(self, student_params, ema_decay=0.999):\n",
    "        # EMA update of teacher from student\n",
    "        for teacher_param, student_param in zip(self.teacher.parameters(), student_params):\n",
    "            teacher_param.data = ema_decay * teacher_param.data + (1 - ema_decay) * student_param.data\n",
    "    \n",
    "    def loss(self, x, t):\n",
    "        # Consistency loss: f(x_t, t) should equal f(x_{t-1}, t-1) approx x_0\n",
    "        z = torch.randn_like(x)\n",
    "        x_t = diffusion_sample(x, t, z)\n",
    "        x_t_next = diffusion_sample(x, t-1, z) if t > 0 else x\n",
    "        \n",
    "        pred_x = self.student(x_t, t)\n",
    "        with torch.no_grad():\n",
    "            target_x = self.teacher(x_t_next, t-1) if t > 0 else x\n",
    "        \n",
    "        return F.mse_loss(pred_x, target_x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **26.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: VAE Implementation**\n",
    "Build a convolutional VAE for CelebA:\n",
    "\n",
    "1. **Architecture:** Encoder (4 conv layers) \u2192 latent (256-dim) \u2192 Decoder (4 transposed conv)\n",
    "2. **Training:** Implement KL-annealing schedule (0 to 1 over 20 epochs)\n",
    "3. **Analysis:** Visualize latent space (interpolation between faces, clustering by attributes)\n",
    "4. **Ablation:** Compare \u03b2=1 vs \u03b2=4 (disentanglement vs reconstruction quality)\n",
    "\n",
    "**Deliverable:** Generated face samples, latent space visualization, disentanglement metrics.\n",
    "\n",
    "### **Lab 2: GAN Training**\n",
    "Train DCGAN on CIFAR-10:\n",
    "\n",
    "1. **Baseline:** Standard GAN (vanilla loss) - document mode collapse\n",
    "2. **Improvement:** Implement WGAN-GP with spectral normalization\n",
    "3. **Metrics:** Calculate FID (Frechet Inception Distance) for both\n",
    "4. **Conditional:** Extend to CGAN (class-conditional generation)\n",
    "\n",
    "**Deliverable:** Comparative FID scores, generated image grids, training stability plots.\n",
    "\n",
    "### **Lab 3: DDPM from Scratch**\n",
    "Implement image diffusion on MNIST:\n",
    "\n",
    "1. **Forward Process:** Implement noise schedule and q_sample\n",
    "2. **Model:** U-Net with time embeddings and attention\n",
    "3. **Training:** Train for 50 epochs, log noise prediction loss\n",
    "4. **Sampling:** Implement both DDPM (1000 steps) and DDIM (50 steps) sampling\n",
    "5. **Interpolation:** Sample two latents, interpolate, decode trajectory\n",
    "\n",
    "**Deliverable:** Training curves, generated digit samples, interpolation GIFs.\n",
    "\n",
    "### **Lab 4: Latent Diffusion**\n",
    "Build simplified Stable Diffusion pipeline:\n",
    "\n",
    "1. **Autoencoder:** Pretrain VQ-VAE on ImageNet subset (64x64 \u2192 16x16 latent)\n",
    "2. **UNet:** Denoising model with cross-attention for text conditioning\n",
    "3. **Text Encoder:** Use pretrained CLIP or train small transformer\n",
    "4. **Inference:** Implement classifier-free guidance, generate conditioned images\n",
    "\n",
    "**Deliverable:** Text-to-image generation demo, guidance scale ablation study.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.7 Common Pitfalls**\n",
    "\n",
    "1. **Mode Collapse (GANs):** Generator finds single output that fools discriminator. **Solution:** WGAN-GP, spectral norm, unrolled GANs, or diversity-regularized losses.\n",
    "\n",
    "2. **Posterior Collapse (VAEs):** Encoder ignores input, KL term drives latent to prior. **Solution:** KL annealing, free bits technique, or aggressive encoder training cycles.\n",
    "\n",
    "3. **Training Instability (Diffusion):** Loss explosion due to variance schedule. **Solution:** Use cosine schedule instead of linear, gradient clipping, mixed precision careful tuning.\n",
    "\n",
    "4. **Codebook Collapse (VQ-VAE):** Only subset of embeddings used. **Solution:** EMA updates for codebook, commitment loss weight tuning, random restarts of unused codes.\n",
    "\n",
    "5. **CFG Ignoring Conditioning:** High guidance scale causes saturation. **Solution:** Scale appropriately (7.5 standard), use negative prompts to steer away from unwanted concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.8 Interview Questions**\n",
    "\n",
    "**Q1:** Compare VAEs, GANs, and Diffusion Models. When would you use each?\n",
    "*A: VAEs: Probabilistic, structured latent space good for representation learning and semi-supervised learning, but blurry samples. GANs: Sharp, high-quality samples, fast sampling (single forward pass), but training instability and mode collapse. Diffusion: State-of-the-art quality, stable training, good mode coverage, but slow sampling (iterative). Use VAEs for compression/feature extraction, GANs for real-time generation (games, video), Diffusion for highest quality image synthesis where latency less critical (art, design tools). Hybrid approaches (Latent Diffusion) combine VAE compression with diffusion quality.*\n",
    "\n",
    "**Q2:** Explain why Diffusion Models require many sampling steps, and how DDIM improves this.\n",
    "*A: DDPM samples by reversing Markov chain with small steps to ensure Gaussian transitions remain valid (approximation quality). Each step requires neural network evaluation \u2192 1000 steps = slow. DDIM uses non-Markovian sampling: defines generative process as implicit model allowing deterministic sampling and jump steps. Can skip timesteps (e.g., every 20th) because it directly predicts x_0 from x_t, then reconstructs x_{t-1} without requiring intermediate steps to be valid diffusion states. Reduces steps 1000\u219250 with minimal quality loss.*\n",
    "\n",
    "**Q3:** What is Classifier-Free Guidance and why does it work?\n",
    "*A: CFG combines conditional (text prompt) and unconditional (empty prompt) predictions: pred = uncond + scale*(cond - uncond). It works by pushing samples toward high-likelihood regions of conditional distribution while using unconditional prediction as baseline. High scale (>1) sharpens distribution, increasing fidelity to prompt but reducing diversity (approaches mode of distribution). Mathematically equivalent to lowering temperature of conditional distribution. Scale 7.5 balances fidelity and diversity; scale 1.0 is pure conditional sampling.*\n",
    "\n",
    "**Q4:** How does the reparameterization trick enable VAE training?\n",
    "*A: Sampling z from N(\u03bc, \u03c3) is stochastic node, blocking backpropagation to encoder parameters. Trick: z = \u03bc + \u03c3*\u03b5 where \u03b5 ~ N(0,1). Now \u03b5 is external noise (stop gradient), \u03bc and \u03c3 are deterministic outputs of encoder. Gradients flow through \u03bc and \u03c3 to encoder, while randomness comes from fixed \u03b5. Allows end-to-end training of encoder/decoder via backpropagation through sampling operation, maintaining stochastic node while keeping computation graph differentiable.*\n",
    "\n",
    "**Q5:** Design a system to generate 1024x1024 images in real-time (30 FPS).\n",
    "*A: Latent Diffusion is required (pixel space diffusion too slow). Architecture: (1) High-compression VAE (f=8 or f=16, 1024\u219264 or 128 latent), (2) Efficient U-Net (MobileNet-style blocks, attention only at lower resolutions), (3) Consistency model or distilled diffusion (LCM/SDXL Turbo) for 4-step sampling instead of 50, (4) Model quantization (INT8) and TensorRT optimization, (5) Tiled generation if memory constrained, (6) Potentially cascaded generation: 256\u2192512\u21921024 with separate upscalers. Hardware: Multiple GPUs or TPUs with pipeline parallelism (different stages on different devices).*\n",
    "\n",
    "---\n",
    "\n",
    "## **26.9 Further Reading**\n",
    "\n",
    "**Papers:**\n",
    "- \"Auto-Encoding Variational Bayes\" (Kingma & Welling, 2014) - VAE foundation\n",
    "- \"Generative Adversarial Networks\" (Goodfellow et al., 2014)\n",
    "- \"Denoising Diffusion Probabilistic Models\" (Ho et al., 2020)\n",
    "- \"Denoising Diffusion Implicit Models\" (Song et al., 2021)\n",
    "- \"High-Resolution Image Synthesis with Latent Diffusion Models\" (Rombach et al., 2022) - Stable Diffusion\n",
    "- \"Adding Conditional Control to Text-to-Image Diffusion Models\" (Zhang et al., 2023) - ControlNet\n",
    "\n",
    "**Tools:**\n",
    "- **diffusers:** Hugging Face diffusion model library\n",
    "- **k-diffusion:** Advanced samplers (DPM++ 2M Karras, etc.)\n",
    "- **ComfyUI:** Node-based diffusion workflow engine\n",
    "- **LoRA:** Low-rank adaptation for efficient fine-tuning\n",
    "\n",
    "---\n",
    "\n",
    "## **26.10 Checkpoint Project: Production Generative System**\n",
    "\n",
    "Build a commercial-grade text-to-image generation service.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Model Architecture:**\n",
    "   - Base: Fine-tuned Stable Diffusion XL or custom Latent Diffusion\n",
    "   - Conditioning: Text (CLIP), Canny edges (ControlNet), Depth (MiDaS)\n",
    "   - Personalization: LoRA adapters for custom styles (loadable at runtime)\n",
    "\n",
    "2. **Inference Optimization:**\n",
    "   - Implement DPM++ 2M Karras sampler (quality/speed trade-off)\n",
    "   - Compile model with TensorRT or ONNX Runtime\n",
    "   - Batch requests dynamically (up to 4 images per batch)\n",
    "   - Memory-efficient attention (xFormers/FlashAttention)\n",
    "\n",
    "3. **API Design:**\n",
    "   - Async generation endpoints (return job ID, webhook on completion)\n",
    "   - Progress streaming (SSE or WebSocket showing denoising steps)\n",
    "   - Negative prompt support, seed control for reproducibility\n",
    "   - Safety filter: NSFW detection and blur/filter pipeline\n",
    "\n",
    "4. **Scaling:**\n",
    "   - Queue-based architecture (Redis/RabbitMQ) with worker pools\n",
    "   - Model sharding: Different GPUs for different resolutions or LoRAs\n",
    "   - Shared KV-cache for batched text embeddings\n",
    "   - Image upscaling pipeline (ESRGAN/Real-ESRGAN) for 4x resolution\n",
    "\n",
    "5. **Evaluation:**\n",
    "   - FID/CLIP score benchmarks on standard prompts\n",
    "   - Human evaluation protocol (A/B testing)\n",
    "   - Latency targets: 1024x1024 in <5 seconds on A100\n",
    "\n",
    "**Deliverables:**\n",
    "- `generative_service/` with FastAPI backend and diffusion pipeline\n",
    "- Docker Compose with GPU worker setup\n",
    "- Postman collection or Swagger docs for API\n",
    "- Benchmark report: Latency vs. batch size, quality metrics\n",
    "\n",
    "**Success Criteria:**\n",
    "- Generate 1024x1024 images in <5 seconds end-to-end\n",
    "- Support 10 concurrent users without queue overflow\n",
    "- Successfully apply and switch LoRA weights without reloading base model\n",
    "- NSFW filtering with <1% false positive rate on safe content\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 26**\n",
    "\n",
    "*You now master generative AI architectures. Chapter 27 covers Multimodal AI\u2014combining vision, language, and audio.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='25. transformer_architecture_deep_dive.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='27. multimodal_ai.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}