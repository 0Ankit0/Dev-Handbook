{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "47654d21",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 29: AI SYSTEM DESIGN & ARCHITECTURE**\n",
    "\n",
    "*Architecting Planet-Scale Intelligent Systems*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "This chapter transitions from implementation details to strategic architecture decisions required by staff+ engineers and architects building AI systems that serve billions of users. We examine real-world systems from Google, Meta, Netflix, and OpenAI, distilling patterns for recommendation, search ranking, autonomous systems, and large language model serving. The focus is on navigating fundamental tensions\u2014latency versus accuracy, cost versus coverage, consistency versus availability\u2014while maintaining the velocity to iterate in production.\n",
    "\n",
    "**Estimated Time:** 35-45 hours (3 weeks)  \n",
    "**Prerequisites:** Chapter 19 (ML System Design fundamentals), Chapter 22 (Deployment), Chapter 25 (Transformers), industry experience or equivalent study\n",
    "\n",
    "---\n",
    "\n",
    "## **29.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Lead system design interviews for ML infrastructure, articulating trade-offs between batch and real-time processing\n",
    "2. Architect feature platforms handling petabyte-scale data with sub-10ms serving latency\n",
    "3. Design training infrastructure supporting trillion-parameter models with fault tolerance and spot instance optimization\n",
    "4. Engineer serving systems for heterogeneous workloads (CV, NLP, Tabular) with unified observability\n",
    "5. Evaluate architectural decisions using total cost of ownership (TCO) and business impact metrics\n",
    "6. Decompose complex AI products (autonomous vehicles, LLM APIs) into verifiable subsystems\n",
    "\n",
    "---\n",
    "\n",
    "## **29.1 The System Design Framework**\n",
    "\n",
    "#### **29.1.1 Requirements Engineering for ML**\n",
    "\n",
    "Before writing architecture documents, translate business requirements into technical constraints using the **RAIL** framework:\n",
    "\n",
    "**R - Requirements (Functional):**\n",
    "- **Prediction Volume:** Queries per second (QPS), peak vs. steady state, burst patterns (Black Friday, viral content)\n",
    "- **Latency Constraints:** P50/P99/P99.9 targets (user-facing <100ms, batch acceptable in minutes)\n",
    "- **Freshness:** Feature staleness tolerance (real-time fraud vs. daily recommendations)\n",
    "- **Accuracy Threshold:** Business metric elasticity (1% accuracy improvement = $X revenue)\n",
    "\n",
    "**A - Assets (Data & Models):**\n",
    "- **Data Volume:** TB/day ingested, retention policies, regulatory constraints (GDPR deletion)\n",
    "- **Model Complexity:** Parameter count, memory footprint, precision requirements (FP16 vs. INT8)\n",
    "- ** intellectual Property:** Proprietary data sensitivity, model encryption requirements\n",
    "\n",
    "**I - Infrastructure Constraints:**\n",
    "- **Compute Budget:** Capex vs. OpEx preferences, GPU availability, carbon neutrality targets\n",
    "- **Latency Budget:** End-to-end allocation (feature fetch: 20ms, inference: 50ms, post-processing: 10ms)\n",
    "- **Availability SLA:** 99.9% (8.76h downtime/year) vs. 99.99% (52m/year), multi-region requirements\n",
    "\n",
    "**L - Legal & Compliance:**\n",
    "- **Explainability:** Right to explanation (GDPR), adverse action notices (credit decisions)\n",
    "- **Bias Auditability:** Demographic parity logging, fairness metric dashboards\n",
    "- **Data Residency:** EU data stays in EU, healthcare data HIPAA isolation\n",
    "\n",
    "#### **29.1.2 Architectural Decision Records (ADRs)**\n",
    "\n",
    "Document irreversible decisions:\n",
    "\n",
    "```markdown\n",
    "## ADR 023: Feature Store Selection (Redis vs. DynamoDB)\n",
    "\n",
    "**Context:** Need <5ms p99 latency for 100k TPS user features  \n",
    "**Decision:** Redis Cluster with RedisJSON for nested features  \n",
    "**Consequences:** \n",
    "- (+) Sub-millisecond latency, rich data structures\n",
    "- (-) Operational complexity (sharding, failover), AWS vendor lock-in\n",
    "- (-) 3x cost vs. DynamoDB at scale\n",
    "**Alternatives Considered:** DynamoDB DAX (higher latency), Aerospike (smaller ecosystem)\n",
    "**Revisit When:** Cross-region replication requirements exceed Redis limitations\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **29.2 Feature Platform Architecture**\n",
    "\n",
    "#### **29.2.1 The Feature Platform Triangle**\n",
    "\n",
    "All feature platforms balance three forces:\n",
    "1. **Freshness:** How recently was the feature computed? (Real-time streaming vs. hourly batch)\n",
    "2. **Fidelity:** How accurate is the feature? (Exact aggregation vs. approximate sketches)\n",
    "3. **Footprint:** Storage and compute cost (Materialized views vs. on-demand computation)\n",
    "\n",
    "**Pattern 1: Lambda Architecture (Batch + Speed Layer)**\n",
    "```\n",
    "Raw Events \u2192 [Kafka] \u2192 [Spark Streaming] \u2192 Real-time Features (Redis) \u2192 Serving\n",
    "                     \u2192 [Spark Batch]     \u2192 Historical Features (S3/Delta) \u2192 Training\n",
    "```\n",
    "*Use when:* Exact historical consistency required, but real-time approximation acceptable (recommendations).\n",
    "\n",
    "**Pattern 2: Kappa Architecture (Streaming Only)**\n",
    "```\n",
    "Raw Events \u2192 [Kafka] \u2192 [Flink] \u2192 Feature Store (Online/Offline unified)\n",
    "```\n",
    "*Use when:* Event sourcing possible, strong consistency between training and serving required (fraud detection).\n",
    "\n",
    "**Pattern 3: Feature Store 2.0 (Materialized Views)**\n",
    "```\n",
    "Offline Store (BigQuery) \u2190\u2192 Feature Registry \u2190\u2192 Online Store (Redis)\n",
    "                                \u2191\n",
    "                         Transformation Logic (DBT/Shared Libs)\n",
    "```\n",
    "*Use when:* Feature reuse across teams > 50%, strict versioning requirements.\n",
    "\n",
    "#### **29.2.2 Case Study: Netflix Recommendation Feature Platform**\n",
    "\n",
    "Netflix serves 450M+ devices with personalized artwork and rankings.\n",
    "\n",
    "**Architecture Highlights:**\n",
    "- **Feature Sources:** Playback events (Kafka), Content metadata (Cassandra), Device signals (real-time)\n",
    "- **Compute:** Spark for batch (user taste clusters), Flink for real-time (continue-watching position)\n",
    "- **Storage:** EVCache (Memcached fork) for online, S3 for offline, Iceberg for time-travel\n",
    "- **Consistency:** **Point-in-time correctness** ensures training uses only features available at prediction time (preventing leakage from future ratings)\n",
    "\n",
    "**Key Insight:** They separate **Contextual features** (device, time, location - injected at API layer) from **Entity features** (user history, movie attributes - pre-materialized).\n",
    "\n",
    "#### **29.2.3 Advanced Feature Engineering Patterns**\n",
    "\n",
    "**Approximate Aggregation for High Cardinality:**\n",
    "```python\n",
    "# HyperLogLog for distinct count features (cardinality estimation)\n",
    "# Use case: \"Number of unique genres watched by user in last 30 days\"\n",
    "# Error: ~2%, Memory: 12KB vs GB for exact set\n",
    "\n",
    "from datasketches import hll_sketch\n",
    "\n",
    "sketch = hll_sketch(12)  # 2^12 bins\n",
    "for genre in user_watch_history:\n",
    "    sketch.update(genre)\n",
    "\n",
    "# Store sketch bytes in feature store, merge at query time\n",
    "feature_value = sketch.serialize()\n",
    "```\n",
    "\n",
    "**Windowed Aggregation with Watermarks:**\n",
    "For late-arriving events (mobile offline viewing syncing hours later):\n",
    "- **Allowed Lateness:** 24 hours for watch events\n",
    "- **Side Inputs:** Update feature values retroactively, mark with `processing_time` vs. `event_time`\n",
    "- **Backfill Trigger:** Rewind stream by 7 days if late data exceeds 5% of window\n",
    "\n",
    "---\n",
    "\n",
    "## **29.3 Training Infrastructure at Scale**\n",
    "\n",
    "#### **29.3.1 The Training Orchestration Stack**\n",
    "\n",
    "**Layer 1: Experiment Management (Control Plane)**\n",
    "- **Metaflow (Netflix):** DAG abstraction for ML workflows\n",
    "- **Kubeflow Pipelines:** Kubernetes-native orchestration\n",
    "- **Ray Train:** Distributed training with auto-scaling\n",
    "\n",
    "**Layer 2: Distributed Execution (Data Plane)**\n",
    "```\n",
    "Scheduler (Airflow/Dagster) \n",
    "    \u2192 Kubernetes Job \n",
    "        \u2192 PyTorch DDP / FSDP / DeepSpeed\n",
    "            \u2192 GPU Cluster (A100/H100)\n",
    "                \u2192 Checkpoint Storage (S3/GCS with S3Guard for consistency)\n",
    "```\n",
    "\n",
    "**Layer 3: Resource Optimization**\n",
    "- **Spot Instances:** 70% cost reduction with checkpointing every 15 minutes\n",
    "- **Multi-Instance Training:** FP8 mixed precision on H100s, gradient compression (1-bit Adam)\n",
    "- **Elastic Training:** Auto-scale workers based on queue depth (Horovod Elastic)\n",
    "\n",
    "#### **29.3.2 Case Study: Meta's Ads Ranking Infrastructure**\n",
    "\n",
    "Meta trains models on billions of examples daily for 3M+ advertisers.\n",
    "\n",
    "**Scale Metrics:**\n",
    "- **Data:** 100+ PB training data\n",
    "- **Models:** 10k+ models trained daily (many small for niche audiences)\n",
    "- **Latency:** New model deployed globally within 2 hours of training completion\n",
    "\n",
    "**Architecture:**\n",
    "- **Data Warehouse:** Hive tables partitioned by (date, advertiser_id)\n",
    "- **Feature Engineering:** Presto for SQL transformations \u2192 TensorFlow Transform for consistency\n",
    "- **Training:** CPU-heavy data preprocessing \u2192 GPU training (A100 pods)\n",
    "- **Continuous Training:** Online learning updates (Follow-the-Regularized-Leader) between batch retrains\n",
    "\n",
    "**Fault Tolerance Pattern:**\n",
    "- **Immutable Training Sets:** Snapshots of features at training start, ensuring reproducibility even if source data changes during 6-hour training job\n",
    "- **Speculative Execution:** Train same model on two different clusters, take result that finishes first (straggler mitigation)\n",
    "\n",
    "#### **29.3.3 Large Model Training (LLM Infrastructure)**\n",
    "\n",
    "For 100B+ parameter models:\n",
    "\n",
    "**Parallelism Strategy:**\n",
    "```\n",
    "Model Parallel (Vertical) \u2192 Splits layers across GPUs (pipeline parallelism)\n",
    "Data Parallel (Horizontal) \u2192 Splits batch across nodes (FSDP/DeepSpeed ZeRO-3)\n",
    "Sequence Parallel \u2192 Splits attention heads (tensor parallelism, Megatron-LM)\n",
    "```\n",
    "\n",
    "**Memory Optimization Checklist:**\n",
    "- **Activation Checkpointing:** Trade 30% compute for 70% memory (recompute activations in backward pass)\n",
    "- **CPU Offloading:** Optimizer states in RAM, computation on GPU (DeepSpeed ZeRO-Offload)\n",
    "- **Mixed Precision:** BF16 forward/backward, FP32 master weights (1-bit Adam reduces communication)\n",
    "\n",
    "**Checkpoint Strategy for 1TB+ Models:**\n",
    "- **Sharded Checkpoints:** Each GPU saves its slice (FSDP)\n",
    "- **Asynchronous Checkpointing:** Copy to storage in background while training continues\n",
    "- **Incremental:** Only save changed parameters (LoRA adapters) between base model iterations\n",
    "\n",
    "---\n",
    "\n",
    "## **29.4 Serving Architecture Patterns**\n",
    "\n",
    "#### **29.4.1 The Serving Taxonomy**\n",
    "\n",
    "| Pattern | Latency | Throughput | Use Case | Example |\n",
    "|---------|---------|------------|----------|---------|\n",
    "| **Online (Sync)** | <50ms | 1-10k QPS | User-facing recommendations | Product ranking |\n",
    "| **Batch (Async)** | Minutes-hours | Millions | Overnight risk scoring | Credit decisions |\n",
    "| **Streaming** | <100ms | 100k+ events/s | Real-time anomaly detection | Fraud, IoT |\n",
    "| **Edge** | <20ms | Device-limited | Mobile inference, autonomous cars | Tesla Autopilot |\n",
    "| **Hybrid** | Variable | Mixed | Complex workflows | LLM Agents |\n",
    "\n",
    "#### **29.4.2 Case Study: Google Search Ranking**\n",
    "\n",
    "Google processes 8.5B searches daily with <200ms p95 latency.\n",
    "\n",
    "**Two-Phase Retrieval:**\n",
    "1. **Candidate Generation (Recall):** \n",
    "   - Dual-encoder neural networks (query/doc embeddings)\n",
    "   - FAISS/ScaNN for approximate nearest neighbor (ANN) search over billions of docs\n",
    "   - Returns ~1000 candidates from index of trillions\n",
    "\n",
    "2. **Ranking (Precision):**\n",
    "   - Heavy ranker: 500+ features, deep neural net with cross-attention between query and document\n",
    "   - Re-ranks 1000 \u2192 10 results\n",
    "   - Multi-objective: Click-through rate, dwell time, authority, diversity\n",
    "\n",
    "**Caching Strategy:**\n",
    "- **Query Cache:** 30% of traffic served from cache (popular queries)\n",
    "- **Model Cache:** Embedding lookup table sharded across TPU pods\n",
    "- **Stale Model Fallback:** If ranking model fails, serve from candidate generator alone (graceful degradation)\n",
    "\n",
    "#### **29.4.3 Case Study: Tesla Autopilot (Edge + Cloud Hybrid)**\n",
    "\n",
    "**Edge Inference (In-Car):**\n",
    "- **Hardware:** Custom FSD Chip (144 TOPS, 72W power envelope)\n",
    "- **Model:** HydraNet (multi-task CNN) for object detection, lane prediction, depth estimation\n",
    "- **Constraints:** <10ms latency for emergency braking (hard real-time)\n",
    "\n",
    "**Cloud Training:**\n",
    "- **Data Collection:** Fleet learning from 4M+ vehicles (shadow mode triggers on uncertainty)\n",
    "- **Trigger:** Disengagement (human took over), new road type detected, or model disagreement\n",
    "- **Training:** Dojo supercomputer (exaflop-scale) trains on video clips with auto-labeling (3D reconstruction from multiple cameras)\n",
    "\n",
    "**OTA Updates:**\n",
    "- Differential compression for model weights (only changed parameters)\n",
    "- A/B testing in shadow mode before activation\n",
    "\n",
    "---\n",
    "\n",
    "## **29.5 Case Study: LLM Serving Infrastructure (ChatGPT Scale)**\n",
    "\n",
    "#### **29.5.1 The Inference Challenge**\n",
    "\n",
    "Serving GPT-4 class models (1.8T parameters, Mixture of Experts) requires:\n",
    "- **Memory:** 8x A100 (80GB) just to hold parameters in FP16\n",
    "- **Compute:** Thousands of GPUs for 100M+ daily active users\n",
    "- **Bottleneck:** Memory bandwidth (loading weights for each token), not compute\n",
    "\n",
    "#### **29.5.2 Architecture Patterns**\n",
    "\n",
    "**Continuous Batching (In-flight Batching):**\n",
    "Unlike static batching (wait for 32 requests), vLLM/Orca dynamically add/remove requests from GPU batch as they complete. Increases GPU utilization 10-20x for variable-length generation.\n",
    "\n",
    "**PagedAttention (vLLM):**\n",
    "Manage KV cache memory like OS virtual memory (non-contiguous blocks). Reduces memory waste from 60% to <4%, enabling 2-4x higher throughput.\n",
    "\n",
    "**Speculative Decoding:**\n",
    "- Small draft model (7B) generates 5 tokens ahead\n",
    "- Large target model (70B) verifies all 5 in parallel (single forward pass)\n",
    "- Accept rate ~80% \u2192 2-3x speedup with identical output distribution\n",
    "\n",
    "**Multi-Query Attention (MQA) / Grouped-Query (GQA):**\n",
    "Reduce KV cache memory by sharing key/value heads (discussed in Chapter 25). Critical for long-context (100k+ tokens).\n",
    "\n",
    "#### **29.5.3 Deployment Topology**\n",
    "\n",
    "```\n",
    "User Request \u2192 Load Balancer (Least-Connections)\n",
    "    \u2192 API Gateway (Rate limiting, API key validation)\n",
    "        \u2192 Routing Logic:\n",
    "            - Short prompt (<1k tokens) \u2192 A100 cluster (fast)\n",
    "            - Long context (>32k) \u2192 H100 cluster (high memory)\n",
    "            - Code generation \u2192 Specialized code-optimized model variant\n",
    "        \u2192 Kubernetes Pod (vLLM inference engine)\n",
    "            \u2192 GPU (TensorRT-LLM optimized)\n",
    "                \u2192 Model Shards (TP=8 for 70B models)\n",
    "```\n",
    "\n",
    "**Cost Optimization:**\n",
    "- **Spot Preemption:** Move inference to on-demand only if spot unavailable (batch jobs can wait)\n",
    "- **Model Quantization:** AWQ/GPTQ 4-bit quantization reduces memory 4x, minimal quality loss\n",
    "- **Prefix Caching:** Cache system prompts (common across requests) in KV cache, only compute new tokens\n",
    "\n",
    "---\n",
    "\n",
    "## **29.6 Advanced Design Patterns**\n",
    "\n",
    "#### **29.6.1 The Strangler Fig Pattern**\n",
    "\n",
    "Migrate from legacy ML system to new architecture incrementally:\n",
    "1. Shadow mode: New model runs parallel to old, logs only\n",
    "2. Canary: 1% traffic to new system, monitor for errors\n",
    "3. **Circuit Breaker:** If new system error rate > 0.1%, automatic fallback to legacy\n",
    "4. **Dark Launch:** New features computed but not used, validating latency/cost\n",
    "5. **Cutover:** Shift 100% traffic, keep legacy on standby for 30 days\n",
    "\n",
    "#### **29.6.2 Multi-Model Ensembles**\n",
    "\n",
    "**Cascade Architecture:**\n",
    "```\n",
    "Light Model (1M params) \u2192 Confidence > 0.9? \u2192 Serve\n",
    "                        \u2192 Confidence < 0.9? \u2192 Heavy Model (1B params)\n",
    "```\n",
    "*Use case:* 90% of easy queries handled cheaply, 10% hard queries get heavy compute.\n",
    "\n",
    "**Mixture of Experts (MoE) Routing:**\n",
    "Train router to send inputs to specialized models (e.g., code expert, math expert, creative writing expert). Gating mechanism ensures only 2/8 experts active per token.\n",
    "\n",
    "#### **29.6.3 Event-Driven ML (Kappa for Inference)**\n",
    "```\n",
    "User Action \u2192 Kafka \u2192 Feature Enrichment (Flink) \u2192 Model Inference (KServe) \n",
    "    \u2192 Action taken \u2192 Feedback loop (reward signal) \u2192 Online Learning\n",
    "```\n",
    "Enables reinforcement learning loops with <100ms end-to-end latency.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.7 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Design YouTube Recommendation System**\n",
    "Design for 2B users, 1B hours watched daily:\n",
    "\n",
    "**Requirements:**\n",
    "- P99 latency < 100ms\n",
    "- Handle viral videos (10x traffic spike in 5 minutes)\n",
    "- Diversity: Don't show same channel repeatedly\n",
    "- Freshness: New videos (<1 hour) should be recommendable\n",
    "\n",
    "**Deliverables:**\n",
    "- Data flow diagram (candidate generation \u2192 ranking \u2192 re-ranking)\n",
    "- Feature list with online/offline categorization\n",
    "- Scaling calculation: QPS, storage, cost estimate\n",
    "- Failure mode analysis: What if candidate generator returns empty?\n",
    "\n",
    "### **Lab 2: LLM Serving Cost Analysis**\n",
    "Compare serving strategies for 10k QPS LLM API:\n",
    "\n",
    "1. **Option A:** Dedicated A100 cluster (always-on)\n",
    "2. **Option B:** Serverless (AWS SageMaker) with auto-scaling\n",
    "3. **Option C:** Mixed (spot instances for batch, on-demand for real-time)\n",
    "\n",
    "Calculate TCO for 1 year, including:\n",
    "- GPU costs (reserved vs. on-demand)\n",
    "- Networking (egress charges)\n",
    "- Engineering maintenance overhead\n",
    "\n",
    "**Deliverable:** Cost comparison spreadsheet with breakeven analysis.\n",
    "\n",
    "### **Lab 3: Feature Platform Migration**\n",
    "Design migration from batch-only to real-time features for fraud detection:\n",
    "\n",
    "**Current State:** Daily batch features, 24-hour delay in fraud detection  \n",
    "**Target State:** <5 minute feature freshness  \n",
    "**Constraints:** Cannot lose historical data, must maintain training-serving consistency\n",
    "\n",
    "**Deliverables:**\n",
    "- Migration plan with rollback strategy\n",
    "- Dual-write pattern (write to old and new simultaneously during transition)\n",
    "- Validation strategy (compare batch vs. real-time feature values for 30 days)\n",
    "\n",
    "### **Lab 4: Multi-Region AI Architecture**\n",
    "Design for EU data residency requirements with US model training:\n",
    "\n",
    "**Challenge:**\n",
    "- Training data (EU users) cannot leave EU\n",
    "- But models trained on global data perform 15% better\n",
    "- Inference must be <50ms globally\n",
    "\n",
    "**Deliverables:**\n",
    "- Data flow diagram showing GDPR-compliant pipeline\n",
    "- Model sharding strategy (EU-specific vs. Global models)\n",
    "- Consistency mechanism for user traveling EU\u2192US (feature synchronization)\n",
    "\n",
    "---\n",
    "\n",
    "## **29.8 Common Pitfalls**\n",
    "\n",
    "1. **Over-Engineering for Scale:** Building for 1B users when you have 10k. **Solution:** \"Solve for 10x, design for 100x, worry about 1000x later.\" Start with monolith, split when team size forces it.\n",
    "\n",
    "2. **Ignoring the Long Tail:** Optimizing P50 latency while P99 suffers (bad for user experience). **Solution:** Always measure P99/P99.9, use tail latency hedging (send request to two backends, use first response).\n",
    "\n",
    "3. **Training-Serving Skew Redux:** Even with feature stores, subtle differences in data pipelines (Python 3.9 vs. 3.10 float precision). **Solution:** Integration tests on production data samples, schema validation with strict types.\n",
    "\n",
    "4. **Cold Start Neglect:** New users/items have no features. **Solution:** Content-based features for cold items, onboarding flows for cold users, exploration-exploitation trade-offs (multi-armed bandits).\n",
    "\n",
    "5. **Monitoring Blind Spots:** Tracking infrastructure metrics (CPU/GPU) but not business metrics (revenue per prediction). **Solution:** Unified dashboard with both technical and business KPIs.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.9 Interview Questions**\n",
    "\n",
    "**Q1:** Design a Twitter feed ranking system (home timeline) for 500M DAU.\n",
    "*A: Hybrid push-pull model. For celebrities (>1M followers): Fanout-on-write (push) to dedicated followers, materialized in timelines. For normal users: Fanout-on-read (pull), rank at request time. Candidate generation: Follow graph + interest graph embeddings (approximate nearest neighbors). Ranking: Heavy GBDT/NN with real-time features (recency, engagement likelihood). Write path: Tweet \u2192 Kafka \u2192 Compute embeddings \u2192 Update follower timelines (async). Read path: Fetch timeline IDs \u2192 Multi-get from Redis \u2192 Rank \u2192 Return. Handle spikes: Rate limiting, eventual consistency acceptable (5s delay OK).*\n",
    "\n",
    "**Q2:** How would you design a system to detect AI-generated text at internet scale?\n",
    "*A: Architecture: Distributed inference on text streams. Models: Lightweight logistic regression on stylometric features (fast filter) \u2192 Transformer classifier (heavy verification) for suspicious content. Features: Perplexity (but modern LLMs have low perplexity), burstiness (human writing has variance in sentence length), watermark detection (if model provider embedded signals). Scale: Spark Streaming for batch processing of archived content, edge deployment for real-time chat filtering. Challenges: Adversarial attacks (paraphrasing breaks detection), false positives (penalizing non-native speakers), evasion via iterative refinement. Mitigation: Ensemble of detectors, human review queue for borderline cases.*\n",
    "\n",
    "**Q3:** Compare microservices vs. monoliths for ML serving infrastructure.\n",
    "*A: Monolith: Single deployable unit containing feature engineering, inference, post-processing. Pros: Easy testing, no network latency between stages, simple debugging. Cons: Tech lock-in, cannot scale components independently (if feature store needs 10x but model doesn't). Microservices: Separate services for Features, Inference, Business Logic. Pros: Independent scaling, team autonomy, technology heterogeneity (Python for ML, Go for API). Cons: Network overhead, distributed tracing complexity, cascading failures. Hybrid: \"Modular monolith\"\u2014logically separate but same process, async communication via queues for heavy tasks. Choose microservices when teams > 50 engineers, otherwise monolith.*\n",
    "\n",
    "**Q4:** Design data infrastructure for autonomous vehicles (Tesla scale).\n",
    "*A: Ingestion: 8 cameras \u00d7 30fps \u00d7 4M cars = massive bandwidth. Edge filtering: Only upload \"interesting\" clips (disengagements, uncertainty triggers, new scenarios) via LTE/WiFi, not raw stream. Cloud: Object storage (S3) for video, Parquet for CAN bus telemetry. Labeling: Automated (3D reconstruction from fleet data) + Human-in-loop for edge cases. Training: Distributed training on video clips, curriculum learning (easy scenarios first). Simulation: Parallel simulation of safety-critical scenarios (rare events) to augment real data. Validation: Shadow mode in production fleet (model runs but doesn't act), compare to human driver. Rollout: A/B test by geographic region, weather condition.*\n",
    "\n",
    "**Q5:** How do you handle model updates without downtime in a real-time bidding (RTB) system processing 1M QPS?\n",
    "*A: Blue-green deployment with traffic mirroring. New model (green) deployed to separate pod pool. Shadow traffic: Duplicate 1% of production traffic to green, compare predictions (log differences). Canary: Shift 1% traffic to green, monitor business metrics (CTR, revenue) for 30 minutes. If metrics neutral or positive, gradual shift 10% \u2192 50% \u2192 100%. Rollback: Instant via load balancer if error rate > threshold. Data consistency: Feature store versioned, new model consumes v2 features while old consumes v1 during transition. State management: Model weights in shared memory (mmap) for zero-copy hot-swapping in some frameworks.*\n",
    "\n",
    "---\n",
    "\n",
    "## **29.10 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Designing Data-Intensive Applications* (Kleppmann) - Stream processing, consistency models\n",
    "- *The Data Warehouse Toolkit* (Kimball) - Dimensional modeling for ML features\n",
    "\n",
    "**Papers:**\n",
    "- \"Monolith: Real Time Recommendation System With Collisionless Embedding Table\" (Meta, 2022) - End-to-end unified system\n",
    "- \"GPU Cluster Scheduling for Deep Learning\" (Microsoft Philly)\n",
    "- \"Efficient Large Scale Language Model Training on GPU Clusters Using Megatron-LM\" (NVIDIA)\n",
    "\n",
    "**Architecture References:**\n",
    "- **Netflix Tech Blog:** \"How Netflix Scales Its ML Infrastructure\"\n",
    "- **Uber Engineering:** \"Michelangelo\" (ML Platform case study)\n",
    "- **OpenAI:** \"Scaling Kubernetes to 7,500 Nodes\" (LLM training infrastructure)\n",
    "\n",
    "---\n",
    "\n",
    "## **29.11 Checkpoint Project: Planet-Scale System Design**\n",
    "\n",
    "Design a complete AI platform for a global food delivery company (DoorDash/UberEats scale).\n",
    "\n",
    "**Context:**\n",
    "- 50M daily active users, 10M restaurants, 5M drivers\n",
    "- Real-time ETA prediction (food preparation + travel time)\n",
    "- Dynamic pricing (surge during demand spikes)\n",
    "- Fraud detection (stolen cards, fake restaurants)\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Feature Platform:**\n",
    "   - Real-time: Driver GPS (5s freshness), Restaurant busy status\n",
    "   - Batch: User taste profiles, neighborhood demand patterns\n",
    "   - Consistency: Training data must reflect features available at order time (point-in-time correctness)\n",
    "\n",
    "2. **Model Portfolio:**\n",
    "   - ETA: GBDT (lightweight, explainable) for base, Neural net for refinement\n",
    "   - Pricing: Contextual bandit (explore-exploit) for surge multiplier\n",
    "   - Fraud: Deep neural net with embeddings for transaction sequences\n",
    "\n",
    "3. **Serving:**\n",
    "   - ETA: <50ms p99 (user waiting for quote)\n",
    "   - Pricing: <10ms (blocking order confirmation)\n",
    "   - Throughput: 100k orders/minute peak\n",
    "\n",
    "4. **Reliability:**\n",
    "   - Multi-region (US-East, US-West, EU)\n",
    "   - Automatic failover if ETA model fails (fallback to historical averages)\n",
    "   - Circuit breakers for external APIs (restaurant POS systems)\n",
    "\n",
    "5. **Observability:**\n",
    "   - Business metrics: Orders per hour, customer satisfaction, driver utilization\n",
    "   - ML metrics: ETA prediction error (MAE), fraud catch rate, false positive rate\n",
    "   - Cost tracking: Cost per prediction, total inference spend vs. revenue\n",
    "\n",
    "**Deliverables:**\n",
    "- Architecture diagram (C4 model: Context, Containers, Components, Code)\n",
    "- Technology selection matrix with 3 alternatives per component\n",
    "- Capacity planning spreadsheet (QPS \u2192 GPU/CPU count \u2192 Cost)\n",
    "- Runbook: \"Incident Response: ETA Service Outage During Peak Dinner\"\n",
    "- 30-minute presentation to \"VP of Engineering\" justifying architectural trade-offs\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Defensible technology choices (Redis vs. Memcached, Spark vs. Flink)\n",
    "- Clear failure modes and mitigation strategies\n",
    "- Cost efficiency (<5% of order value spent on ML infrastructure)\n",
    "- Compliance: PCI-DSS for payment fraud, GDPR for EU data\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 29**\n",
    "\n",
    "*You have mastered AI system architecture at scale. Chapter 30 covers Building Production AI Portfolios\u2014end-to-end projects that demonstrate your capabilities.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../6. Advanced_topics_and_research/28. ai_safety_alignment_and_robustness.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='30. building_production_ai_portfolio.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}