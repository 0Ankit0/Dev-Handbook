{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5c7d420c",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 30: BUILDING PRODUCTION AI PORTFOLIO**\n",
    "\n",
    "*From Tutorial Projects to Production-Grade Systems*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Technical interviews and hiring decisions increasingly rely on demonstrated ability to ship complete systems rather than Kaggle leaderboard rankings. This chapter provides the scaffolding for five distinct portfolio projects that showcase MLOps proficiency, system design skills, and domain expertise. Each project is designed to be interview-defensible: complex enough to discuss trade-offs, scoped enough to complete in 40-60 hours, and practical enough that companies could theoretically deploy it.\n",
    "\n",
    "**Estimated Time:** 60-80 hours (self-paced, typically 6-8 weeks part-time)  \n",
    "**Prerequisites:** Completion of Chapters 19-24 (MLOps, Deployment, System Design), Git proficiency, cloud platform access (AWS/GCP/Azure)\n",
    "\n",
    "---\n",
    "\n",
    "## **30.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will have:\n",
    "1. Scoped five distinct AI projects from problem formulation to production architecture\n",
    "2. Implemented production-grade codebases with type hints, testing, and CI/CD pipelines\n",
    "3. Deployed at least one project to a public cloud with persistent infrastructure\n",
    "4. Documented system designs and trade-offs suitable for technical interviews\n",
    "5. Created a portfolio narrative that connects technical choices to business impact\n",
    "\n",
    "---\n",
    "\n",
    "## **30.1 Project Scoping Framework**\n",
    "\n",
    "#### **30.1.1 The PRD Template for ML Projects**\n",
    "\n",
    "Before writing code, define the **Product Requirements Document**:\n",
    "\n",
    "```markdown\n",
    "## Project: Real-Time Fraud Detection API\n",
    "\n",
    "### Problem Statement\n",
    "E-commerce platform loses $2M annually to card-not-present fraud. Current rule-based \n",
    "system has 5% false positive rate (annoying customers) and catches only 60% of fraud.\n",
    "\n",
    "### Success Metrics\n",
    "- Business: Reduce fraud losses by 40% ($800K savings), maintain false positive <3%\n",
    "- Technical: P99 latency <50ms, availability 99.9%, handle 10K TPS burst\n",
    "- Model: AUC-ROC >0.92, Precision@Recall=0.8 >0.85\n",
    "\n",
    "### Constraints\n",
    "- Must explain declined transactions to customer service (XGBoost, not black-box NN)\n",
    "- GDPR compliant: Delete user data within 30 days, right to explanation\n",
    "- Budget: <$5K/month cloud spend at target scale\n",
    "\n",
    "### MVP vs Production\n",
    "MVP: Batch inference on hourly transactions, 80% fraud catch rate acceptable\n",
    "Production: Real-time inference, circuit breakers, shadow mode deployment\n",
    "```\n",
    "\n",
    "#### **30.1.2 Anti-Patterns in Portfolio Projects**\n",
    "\n",
    "Avoid these common traps:\n",
    "\n",
    "**The Notebook Dump:** A Jupyter notebook with no tests, no modular code, and hardcoded paths. *Fix:* Convert to Python package with `src/`, `tests/`, and `config/` directories.\n",
    "\n",
    "**The Kaggle Copy:** Using competition data with leaked features (e.g., target encoded IDs that won't exist in production). *Fix:* Simulate real data drift, use time-based splits.\n",
    "\n",
    "**The Un-Deployable Model:** A 10GB pickled model with dependencies that can't be containerized. *Fix:* ONNX export, dependency pinning, multi-stage Docker builds.\n",
    "\n",
    "**The Missing Negative:** Only showing happy path metrics. *Fix:* Document failure modes, error analysis by demographic group, bias audit results.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.2 The Five Core Projects**\n",
    "\n",
    "### **Project 1: Tabular MLOps Pipeline**\n",
    "**Complexity:** Intermediate | **Domain:** Fintech/Retail | **Tech Stack:** Scikit-learn/XGBoost, Feast, MLflow, Airflow\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Raw Data (S3) \u2192 Great Expectations Validation \u2192 Feature Engineering (Feast)\n",
    "    \u2192 Training Pipeline (Airflow + MLflow) \u2192 Model Registry \u2192 FastAPI Service\n",
    "    \u2192 Prometheus Monitoring \u2192 Grafana Dashboard\n",
    "```\n",
    "\n",
    "**Implementation Requirements:**\n",
    "- **Data Validation:** Schema validation with Pandera, drift detection with Evidently\n",
    "- **Feature Store:** Feast with Redis online store (point-in-time correctness for training)\n",
    "- **Training:** Automated hyperparameter tuning (Optuna), experiment tracking\n",
    "- **Serving:** FastAPI with Pydantic validation, batch and single-record endpoints\n",
    "- **Testing:** Unit tests for feature engineering (pytest), integration tests for API, load tests with Locust\n",
    "\n",
    "**Key Interview Talking Points:**\n",
    "- *Why XGBoost over Neural Net?* Interpretability for regulatory compliance, faster inference\n",
    "- *Why Feature Store?* Prevents training-serving skew, enables feature reuse across models\n",
    "- *Handling Cold Start:* Fallback to demographic averages for new users\n",
    "\n",
    "**Deliverables:**\n",
    "- GitHub repo with `make test` running full test suite\n",
    "- Live demo endpoint (can be on free tier)\n",
    "- Architecture diagram showing data flow\n",
    "- Cost analysis: \"$0.001 per prediction at scale\"\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 2: Computer Vision API**\n",
    "**Complexity:** Intermediate | **Domain:** Manufacturing/Retail | **Tech Stack:** PyTorch, TorchServe, OpenCV, Kubernetes\n",
    "\n",
    "**Scope:** Multi-class defect detection on industrial parts (or retail product recognition).\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Image Upload \u2192 S3 \u2192 SQS Queue \u2192 Inference Service (TorchServe on EKS)\n",
    "    \u2192 Post-processing (NMS, thresholding) \u2192 DynamoDB Results\n",
    "    \u2192 Notification (SNS) \u2192 Client Webhook\n",
    "```\n",
    "\n",
    "**Technical Depth:**\n",
    "- **Model:** Fine-tuned ResNet-50 or EfficientNet-B3 with transfer learning\n",
    "- **Optimization:** TensorRT FP16 quantization, ONNX export for CPU fallback\n",
    "- **Data Pipeline:** Albumentations for augmentation, FiftyOne for dataset visualization\n",
    "- **Deployment:** Kubernetes with HPA, rolling updates with zero downtime\n",
    "- **Monitoring:** Track confidence distribution drift, input image quality metrics (blur detection)\n",
    "\n",
    "**Differentiation:**\n",
    "Implement **active learning loop**: Low-confidence predictions trigger human review, automatically adding labeled data to retraining pool.\n",
    "\n",
    "**Interview Angle:**\n",
    "- *Handling Imbalanced Data:* Use focal loss, class weights, or oversampling via Albumentations\n",
    "- *Latency Optimization:* Dynamic batching in TorchServe, async pre-processing\n",
    "- *Edge Considerations:* Demonstrate TensorFlow Lite conversion for mobile deployment\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 3: NLP Service with Fine-Tuned Transformer**\n",
    "**Complexity:** Advanced | **Domain:** Legal/Healthcare | **Tech Stack:** Hugging Face Transformers, LoRA, Docker, FastAPI\n",
    "\n",
    "**Scope:** Named Entity Recognition (NER) for legal contracts or clinical notes, or document classification.\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "PDF/Text Input \u2192 LangChain Parsing \u2192 HuggingFace Pipeline \n",
    "    \u2192 Fine-tuned BERT/DeBERTa (LoRA adapter) \u2192 Structured Output (JSON)\n",
    "    \u2192 Validation (Pydantic) \u2192 Response\n",
    "```\n",
    "\n",
    "**Production Considerations:**\n",
    "- **Efficiency:** LoRA adapters for multi-tenant serving (swap adapters per customer without reloading base model)\n",
    "- **Long Documents:** Implement sliding window or hierarchical attention for documents >512 tokens\n",
    "- **Evaluation:** Entity-level F1 (not token-level), error analysis by entity type\n",
    "- **Safety:** PII redaction using Presidio before model inference\n",
    "\n",
    "**MLOps Integration:**\n",
    "- **CI/CD:** Retraining triggered on new labeled data (GitOps with ArgoCD)\n",
    "- **A/B Testing:** Shadow deployment comparing base model vs. fine-tuned version\n",
    "- **Explainability:** LIME/SHAP for token importance visualization\n",
    "\n",
    "**Portfolio Value:**\n",
    "Demonstrates ability to handle unstructured data, HuggingFace ecosystem mastery, and domain adaptation (transfer learning).\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 4: LLM Application with RAG**\n",
    "**Complexity:** Advanced | **Domain:** Enterprise Knowledge Management | **Tech Stack:** LangChain/LlamaIndex, Vector DB (Pinecone/Weaviate), OpenAI/Local LLM\n",
    "\n",
    "**Scope:** Chatbot answering questions over private documents (PDFs, Confluence, Slack).\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Documents \u2192 Loaders (Unstructured.io) \u2192 Chunking (RecursiveTextSplitter)\n",
    "    \u2192 Embeddings (OpenAI/HuggingFace) \u2192 VectorStore (Pinecone)\n",
    "    \n",
    "Query \u2192 Embedding \u2192 Similarity Search \u2192 Top-K Chunks \u2192 Prompt Engineering\n",
    "    \u2192 LLM (GPT-4/Claude/Llama-2) \u2192 Streaming Response \u2192 Citation Layer\n",
    "```\n",
    "\n",
    "**Critical Engineering Decisions:**\n",
    "- **Chunking Strategy:** Experiment with 256, 512, 1024 token chunks; overlap of 50 tokens; semantic chunking vs. fixed size\n",
    "- **Retrieval:** Hybrid search (BM25 + Dense), re-ranking (Cohere Rerank or cross-encoders)\n",
    "- **Evaluation:** RAGAS framework (faithfulness, answer relevance, context precision)\n",
    "- **Cost Control:** Caching layer for common queries, query classification (simple FAQ vs. complex reasoning) to route to cheaper models\n",
    "\n",
    "**Advanced Features:**\n",
    "- **Agentic RAG:** Tool use for calculations, API calls to verify real-time data\n",
    "- **Guardrails:** NeMo Guardrails or LlamaGuard for safety, topic restriction\n",
    "- **Multi-modal:** Process images in documents (charts, diagrams) via CLIP embeddings\n",
    "\n",
    "**Interview Narrative:**\n",
    "Focus on evaluation methodology (how do you know RAG is working?), cost optimization strategies, and handling hallucinations via citation verification.\n",
    "\n",
    "---\n",
    "\n",
    "### **Project 5: Real-Time Recommendation System**\n",
    "**Complexity:** Expert | **Domain:** Media/E-commerce | **Tech Stack:** Spark, Redis, Kafka, TensorFlow/PyTorch, Cassandra\n",
    "\n",
    "**Scope:** Session-based recommendation (similar to Amazon \"Customers who viewed X\") or content feed ranking.\n",
    "\n",
    "**Architecture (Two-Tower Neural Network):**\n",
    "```\n",
    "Real-time Events (Click, View) \u2192 Kafka \u2192 Flink Feature Computation \u2192 Redis\n",
    "User Profile (Batch) \u2192 Spark \u2192 Feature Store (Feast) \u2192 Redis\n",
    "\n",
    "Request \u2192 Candidate Generation (ANN: FAISS/Milvus) \u2192 1000 Items\n",
    "    \u2192 Ranking Model (Two-Tower or DeepFM) \u2192 Top 100\n",
    "    \u2192 Business Logic (Diversity, Filtering) \u2192 Top 20\n",
    "    \u2192 Response\n",
    "```\n",
    "\n",
    "**Technical Implementation:**\n",
    "- **Candidate Generation:** Approximate Nearest Neighbors (HNSW index) over item embeddings\n",
    "- **Ranking:** Contextual bandit or deep neural net with wide-and-deep architecture\n",
    "- **Feature Engineering:** Real-time session features (view count in last 10 min), user historical features\n",
    "- **Evaluation:** Offline (NDCG, MAP), Online (A/B testing framework with impression tracking)\n",
    "\n",
    "**Scale Considerations:**\n",
    "- **Latency Budget:** 20ms for candidate generation, 30ms for ranking, 10ms for business logic\n",
    "- **Cold Start:** Content-based features for new items, exploration via epsilon-greedy\n",
    "- **Infrastructure:** Redis Cluster for sub-millisecond feature lookup, Kafka for event streaming\n",
    "\n",
    "**Portfolio Presentation:**\n",
    "Include offline evaluation metrics showing lift over baseline (popular items), and architecture diagram proving you understand the serving constraints.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.3 Code Quality & Engineering Excellence**\n",
    "\n",
    "#### **30.3.1 Project Structure Template**\n",
    "\n",
    "```\n",
    "ai_project/\n",
    "\u251c\u2500\u2500 .github/\n",
    "\u2502   \u2514\u2500\u2500 workflows/\n",
    "\u2502       \u251c\u2500\u2500 ci.yml          # Lint, test, build\n",
    "\u2502       \u2514\u2500\u2500 cd.yml          # Deploy to staging/prod\n",
    "\u251c\u2500\u2500 config/\n",
    "\u2502   \u251c\u2500\u2500 config.yaml         # Hydra/OmegaConf configuration\n",
    "\u2502   \u2514\u2500\u2500 schema.py           # Pydantic models for validation\n",
    "\u251c\u2500\u2500 data/\n",
    "\u2502   \u251c\u2500\u2500 raw/                # Gitignored, versioned with DVC\n",
    "\u2502   \u2514\u2500\u2500 processed/\n",
    "\u251c\u2500\u2500 docker/\n",
    "\u2502   \u251c\u2500\u2500 Dockerfile.api\n",
    "\u2502   \u2514\u2500\u2500 Dockerfile.training\n",
    "\u251c\u2500\u2500 notebooks/\n",
    "\u2502   \u2514\u2500\u2500 01_eda.ipynb        # Exploratory only, not production code\n",
    "\u251c\u2500\u2500 src/\n",
    "\u2502   \u251c\u2500\u2500 __init__.py\n",
    "\u2502   \u251c\u2500\u2500 features/           # Feature engineering logic\n",
    "\u2502   \u251c\u2500\u2500 models/             # Model definitions\n",
    "\u2502   \u251c\u2500\u2500 api/                # FastAPI/Flask app\n",
    "\u2502   \u2514\u2500\u2500 pipeline/           # Training scripts\n",
    "\u251c\u2500\u2500 tests/\n",
    "\u2502   \u251c\u2500\u2500 unit/\n",
    "\u2502   \u251c\u2500\u2500 integration/\n",
    "\u2502   \u2514\u2500\u2500 load/\n",
    "\u251c\u2500\u2500 Makefile                # Standard commands: make test, make train\n",
    "\u251c\u2500\u2500 pyproject.toml          # Poetry dependencies, black/isort config\n",
    "\u2514\u2500\u2500 README.md               # Setup instructions, architecture diagram\n",
    "```\n",
    "\n",
    "#### **30.3.2 Type Safety in ML**\n",
    "\n",
    "```python\n",
    "# src/features/engineering.py\n",
    "from typing import Protocol, TypedDict\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class FeatureConfig(TypedDict):\n",
    "    window_size: int\n",
    "    aggregation: Literal[\"mean\", \"sum\", \"max\"]\n",
    "\n",
    "class FeatureEngineer(Protocol):\n",
    "    \"\"\"Protocol for feature transformers\"\"\"\n",
    "    def fit(self, X: pd.DataFrame) -> \"FeatureEngineer\": ...\n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame: ...\n",
    "\n",
    "class RollingAggregator:\n",
    "    def __init__(self, config: FeatureConfig):\n",
    "        self.window = config[\"window_size\"]\n",
    "        self.agg = config[\"aggregation\"]\n",
    "        self._fitted = False\n",
    "        \n",
    "    def fit(self, X: pd.DataFrame) -> \"RollingAggregator\":\n",
    "        # Validation\n",
    "        if X.empty:\n",
    "            raise ValueError(\"Empty dataframe\")\n",
    "        self.columns_ = X.columns.tolist()\n",
    "        self._fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        if not self._fitted:\n",
    "            raise RuntimeError(\"Call fit before transform\")\n",
    "        return X.rolling(window=self.window).agg(self.agg)\n",
    "```\n",
    "\n",
    "#### **30.3.3 Testing Strategy**\n",
    "\n",
    "**Unit Tests (pytest):**\n",
    "```python\n",
    "def test_feature_engineering_handles_missing():\n",
    "    engineer = RollingAggregator({\"window_size\": 3, \"aggregation\": \"mean\"})\n",
    "    df = pd.DataFrame({\"value\": [1, np.nan, 3, 4]})\n",
    "    result = engineer.fit(df).transform(df)\n",
    "    assert not result.isna().all().all(), \"Should handle NaN gracefully\"\n",
    "```\n",
    "\n",
    "**Contract Tests (Pact):**\n",
    "Verify API consumer (frontend) and provider (ML service) agree on schema.\n",
    "\n",
    "**Load Tests (Locust):**\n",
    "```python\n",
    "from locust import HttpUser, task\n",
    "\n",
    "class MLAPIUser(HttpUser):\n",
    "    @task\n",
    "    def predict(self):\n",
    "        self.client.post(\"/predict\", json={\"features\": [1.0, 2.0, 3.0]})\n",
    "```\n",
    "\n",
    "#### **30.3.4 CI/CD for ML**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/ml-pipeline.yml\n",
    "name: ML Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - 'src/**'\n",
    "      - 'config/**'\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.10'\n",
    "      \n",
    "      - name: Install dependencies\n",
    "        run: pip install -r requirements-dev.txt\n",
    "      \n",
    "      - name: Lint\n",
    "        run: |\n",
    "          black --check src/\n",
    "          ruff check src/\n",
    "          mypy src/\n",
    "      \n",
    "      - name: Unit tests\n",
    "        run: pytest tests/unit --cov=src --cov-report=xml\n",
    "      \n",
    "      - name: Data validation\n",
    "        run: |\n",
    "          great_expectations checkpoint run raw_data_validation\n",
    "      \n",
    "      - name: Model performance regression\n",
    "        run: |\n",
    "          python -m src.pipeline.train --config config/test.yaml\n",
    "          python -m src.evaluation.compare_baseline --threshold 0.05\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **30.4 Portfolio Presentation**\n",
    "\n",
    "#### **30.4.1 GitHub Repository Hygiene**\n",
    "\n",
    "**README Structure:**\n",
    "1. **One-sentence description:** \"Production-grade fraud detection API handling 10K TPS with <50ms latency\"\n",
    "2. **Architecture diagram:** Draw.io or Excalidraw embedded via image\n",
    "3. **Quick start:** `docker-compose up` command to run locally\n",
    "4. **Key metrics:** Latency, throughput, accuracy benchmarks\n",
    "5. **Tech stack:** Badges for Python, FastAPI, AWS, etc.\n",
    "6. **Blog post link:** Deep dive into technical decisions\n",
    "\n",
    "**Code Documentation:**\n",
    "- **Docstrings:** Google style, with Args/Returns/Raises\n",
    "- **Architecture Decision Records (ADRs):** `docs/adr/001-feature-store.md`\n",
    "- **API Docs:** Auto-generated Swagger UI screenshots\n",
    "\n",
    "#### **30.4.2 Technical Blog Content**\n",
    "\n",
    "Write 1,000-2,000 words on one challenging aspect:\n",
    "- \"Why we moved from batch to streaming features (and the 3 attempts that failed)\"\n",
    "- \"Optimizing Transformer inference: From 2s to 20ms\"\n",
    "- \"The hidden cost of Pandas: A memory optimization journey\"\n",
    "\n",
    "**Platforms:** Medium, Dev.to, or personal site (SEO-friendly).\n",
    "\n",
    "#### **30.4.3 Demo Videos**\n",
    "\n",
    "**The 3-Minute Demo:**\n",
    "1. **0:00-0:30:** Problem statement (\"E-commerce loses $X to fraud\")\n",
    "2. **0:30-1:30:** System walkthrough (show Grafana dashboard, API call in Postman)\n",
    "3. **1:30-2:30:** Load test visualization (watch 1K requests/second handled)\n",
    "4. **2:30-3:00:** GitHub repo walkthrough (highlight test coverage, CI/CD)\n",
    "\n",
    "**Tools:** Loom (free), OBS Studio, or simple screen recording with voiceover.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.5 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Portfolio Scoping Workshop**\n",
    "For each of the 5 projects:\n",
    "\n",
    "1. **Write PRD:** Define metrics, constraints, MVP vs. v1 scope\n",
    "2. **Architecture Decision:** Choose 3 key technologies, document alternatives rejected\n",
    "3. **Risk Assessment:** What will most likely fail? (data quality, latency, model drift)\n",
    "4. **30-60-90 Day Plan:** Week 1 (EDA), Week 2 (Baseline), Week 3 (MLOps), etc.\n",
    "\n",
    "**Deliverable:** 5 markdown files in `portfolio_planning/` directory.\n",
    "\n",
    "### **Lab 2: Code Quality Audit**\n",
    "Take an old Kaggle notebook and refactor:\n",
    "\n",
    "1. **Structure:** Convert to `src/` package structure\n",
    "2. **Typing:** Add type hints to all functions\n",
    "3. **Testing:** Achieve >80% test coverage on feature engineering\n",
    "4. **CI/CD:** GitHub Actions passing lint, test, and build stages\n",
    "\n",
    "**Deliverable:** Before/after comparison showing lines of code, test count, documentation coverage.\n",
    "\n",
    "### **Lab 3: Deployment Challenge**\n",
    "Deploy Project 1 (Tabular) or Project 2 (CV) to cloud:\n",
    "\n",
    "1. **Infrastructure as Code:** Terraform or Pulumi (not click-ops)\n",
    "2. **Monitoring:** Live dashboard showing predictions/second, latency histogram\n",
    "3. **Load Test:** Demonstrate handling 10x normal load via auto-scaling\n",
    "4. **Cost Optimization:** Show < $50/month spend (using spot instances or serverless)\n",
    "\n",
    "**Deliverable:** Public endpoint (or screenshot if sensitive), architecture diagram, cost breakdown.\n",
    "\n",
    "### **Lab 4: Interview Prep Documentation**\n",
    "Create \"Interview Cheat Sheet\" for each project:\n",
    "\n",
    "1. **Elevator Pitch:** 30-second description\n",
    "2. **Deep Dive Questions:** \n",
    "   - \"Why XGBoost not Random Forest?\"\n",
    "   - \"How do you handle cold start?\"\n",
    "   - \"What happens if feature store is down?\"\n",
    "3. **Failure Modes:** \"Tell me about a bug you encountered\"\n",
    "4. **Scale Estimation:** \"How would this handle 10x traffic?\"\n",
    "\n",
    "**Deliverable:** `INTERVIEW_PREP.md` in each project repo.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.6 Common Pitfalls**\n",
    "\n",
    "1. **Perfectionism Paralysis:** Waiting for \"perfect\" architecture before shipping. **Fix:** Ship MVP with hardcoded model, iterate. Done > Perfect.\n",
    "\n",
    "2. **Resume-Driven Development:** Using Kubernetes for 100-requests/day hobby project. **Fix:** Match complexity to requirements. Flask + SQLite is fine for demos.\n",
    "\n",
    "3. **Neglecting Documentation:** Code works but README says \"TODO\". **Fix:** Document setup steps as you build, not after.\n",
    "\n",
    "4. **Fake Data Only:** Using `make_classification` synthetic data. **Fix:** Use real public datasets (even if messy) to show data cleaning skills.\n",
    "\n",
    "5. **No Error Handling:** Happy path only, no try-catch, no validation. **Fix:** Pydantic validation, circuit breakers, fallback predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.7 Interview Questions**\n",
    "\n",
    "**Q1:** Walk me through your portfolio project. What was the hardest technical challenge?\n",
    "*A: Structure: (1) Context: Business problem, scale, constraints, (2) Architecture: High-level diagram, key technologies, (3) Challenge: Specific issue (e.g., training-serving skew, latency), (4) Solution: Technical decision made, alternatives considered, (5) Result: Metrics improved, lessons learned. Focus on decision-making process, not just final code.*\n",
    "\n",
    "**Q2:** Why did you choose [Technology X] over [Technology Y] in Project Z?\n",
    "*A: Demonstrate trade-off analysis. \"I chose Redis over Memcached for the feature store because I needed data structures (sorted sets for top-N features) and persistence. The trade-off was operational complexity\u2014we needed Redis Sentinel for failover. For a simpler cache-only use case, Memcached would be better.\" Show you can defend decisions but acknowledge limitations.*\n",
    "\n",
    "**Q3:** How would you scale this project to 10x the traffic?\n",
    "*A: Identify current bottlenecks: (1) Database: Read replicas, sharding, or move to NoSQL, (2) Model: Quantization, distillation, or model parallelism, (3) Caching: Add CDN or edge caching, (4) Async: Move heavy work to queues, (5) Horizontal: Kubernetes HPA, multi-region deployment. Show understanding of vertical vs. horizontal scaling trade-offs.*\n",
    "\n",
    "**Q4:** What would you do differently if you started over?\n",
    "*A: Honest reflection shows growth. \"I initially used Airflow for the training pipeline, but for this latency requirement, I should have used a message queue (SQS) + Lambda for faster iteration. Also, I hardcoded feature names initially; I should have used a feature registry from day one.\" Avoid \"nothing\"\u2014always have learnings.*\n",
    "\n",
    "**Q5:** How do you know your model is still working in production?\n",
    "*A: Monitoring strategy: (1) Data drift detection (Kolmogorov-Smirnov tests), (2) Performance monitoring (accuracy if labels available, proxy metrics if delayed), (3) Business metrics (conversion rate, fraud catch rate), (4) Alerting: PagerDuty for data pipeline failures, Slack for drift warnings, (5) Automated retraining triggers. Show operational maturity.*\n",
    "\n",
    "---\n",
    "\n",
    "## **30.8 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Building Machine Learning Pipelines* (O'Reilly) - Kubeflow patterns\n",
    "- *Designing Machine Learning Systems* (Chip Huyen) - Chapter on testing and deployment\n",
    "\n",
    "**Resources:**\n",
    "- **Made With ML:** GokuMohandas's MLOps course (excellent portfolio examples)\n",
    "- **Evidently AI:** ML System Design case studies\n",
    "- **AWS Architecture Center:** ML patterns (real-time inference, batch processing)\n",
    "\n",
    "---\n",
    "\n",
    "## **30.9 Checkpoint Project: The Capstone**\n",
    "\n",
    "Complete **one** end-to-end project from Section 30.2 with the following production criteria:\n",
    "\n",
    "**Must Haves:**\n",
    "- [ ] GitHub repo with >90% test coverage (measured by pytest-cov)\n",
    "- [ ] Live deployment (Heroku free tier, AWS free tier, or personal server)\n",
    "- [ ] CI/CD pipeline running (GitHub Actions green badge)\n",
    "- [ ] Architecture diagram (PNG in repo)\n",
    "- [ ] Technical blog post published (Medium/Dev.to)\n",
    "- [ ] Monitoring dashboard (Grafana/Datadog screenshot showing traffic)\n",
    "\n",
    "**Evaluation Rubric:**\n",
    "| Criteria | Poor | Good | Excellent |\n",
    "|----------|------|------|-----------|\n",
    "| **Code Quality** | No tests, no types | Some tests, basic types | Full type hints, >90% coverage, linting |\n",
    "| **Architecture** | Monolithic script | Modular but simple | Microservices/event-driven where appropriate |\n",
    "| **Deployment** | Local only | Docker but not deployed | Live endpoint with HTTPS, auto-scaling |\n",
    "| **Documentation** | README only | Basic setup docs | ADRs, API docs, architecture blog |\n",
    "| **Monitoring** | None | Logs only | Metrics, alerts, dashboards |\n",
    "\n",
    "**Success Criteria:**\n",
    "You should be able to spend 45 minutes in an interview discussing only this project\u2014covering data collection choices, model selection trade-offs, scaling challenges, and failure modes\u2014without repeating yourself.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 30**\n",
    "\n",
    "*You now have the roadmap to build a portfolio that demonstrates production AI engineering capabilities. Chapter 31 covers Interview Preparation & Career Strategy.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='29. ai_system_design_and_architecture.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='31. interview_preparation_and_career_strategy.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}