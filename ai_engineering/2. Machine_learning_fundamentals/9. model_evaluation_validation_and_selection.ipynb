{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d16eee71",
   "metadata": {},
   "source": [
    "Here is **Chapter 9: Model Evaluation, Validation & Selection** — ensuring your models generalize to the real world.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 9: MODEL EVALUATION, VALIDATION & SELECTION**\n",
    "\n",
    "*Separating Signal from Noise*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Training a model is easy; training one that generalizes is hard. This chapter provides the rigorous methodology to evaluate whether your model will succeed in production or fail spectacularly on unseen data. From cross-validation strategies that prevent leakage to Bayesian optimization that finds optimal hyperparameters efficiently, we cover the engineering discipline of model selection.\n",
    "\n",
    "**Estimated Time:** 40-50 hours (3 weeks)  \n",
    "**Prerequisites:** Chapters 6-8 (Supervised Learning fundamentals)\n",
    "\n",
    "---\n",
    "\n",
    "## **9.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement appropriate cross-validation strategies for different data types (i.i.d., time series, grouped)\n",
    "2. Diagnose bias vs. variance problems and apply targeted solutions\n",
    "3. Optimize hyperparameters using Bayesian methods and early stopping strategies\n",
    "4. Interpret model predictions using SHAP and LIME for debugging and compliance\n",
    "5. Compare models statistically to determine if performance differences are significant\n",
    "6. Select deployment candidates using business-aware criteria beyond pure accuracy\n",
    "\n",
    "---\n",
    "\n",
    "## **9.1 Cross-Validation: Robust Performance Estimation**\n",
    "\n",
    "A single train-test split is risky. Cross-validation (CV) provides robust estimates by rotating validation sets.\n",
    "\n",
    "#### **9.1.1 K-Fold Cross-Validation**\n",
    "\n",
    "Partition data into $k$ folds. Train on $k-1$ folds, validate on the remaining. Average performance.\n",
    "\n",
    "**Standard K-Fold (for i.i.d. data):**\n",
    "```python\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "scores = cross_val_score(model, X, y, cv=kf, scoring='roc_auc')\n",
    "\n",
    "print(f\"CV Scores: {scores}\")\n",
    "print(f\"Mean: {scores.mean():.3f} (+/- {scores.std() * 2:.3f})\")\n",
    "```\n",
    "\n",
    "**When to use:** Standard tabular data where samples are independent.\n",
    "\n",
    "#### **9.1.2 Stratified K-Fold**\n",
    "\n",
    "Preserves class distribution in each fold (critical for imbalanced classification).\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "# Manual loop for inspection\n",
    "for train_idx, val_idx in skf.split(X, y):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    # Verify stratification\n",
    "    print(f\"Train class ratio: {y_train.mean():.3f}, Val: {y_val.mean():.3f}\")\n",
    "```\n",
    "\n",
    "**Always use for:** Classification with <20% minority class.\n",
    "\n",
    "#### **9.1.3 Time Series Split**\n",
    "\n",
    "For temporal data, validation must be in the future (past → future).\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    # Train indices always before validation indices\n",
    "    print(f\"Fold {fold}: Train ends at {train_idx[-1]}, Val starts at {val_idx[0]}\")\n",
    "    \n",
    "    # Check for leakage: max(train_date) < min(val_date)\n",
    "    assert X.iloc[train_idx].index.max() < X.iloc[val_idx].index.min()\n",
    "```\n",
    "\n",
    "**Variants:**\n",
    "- **Expanding Window:** Training set grows (all past data)\n",
    "- **Rolling Window:** Fixed-size training set (slides forward)\n",
    "\n",
    "#### **9.1.4 Group K-Fold**\n",
    "\n",
    "Ensures all samples from same group stay together (prevents leakage when multiple samples per subject).\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Groups: patient IDs, user IDs, etc.\n",
    "groups = df['patient_id'].values\n",
    "gkf = GroupKFold(n_splits=5)\n",
    "\n",
    "for train_idx, val_idx in gkf.split(X, y, groups=groups):\n",
    "    # No patient appears in both train and validation\n",
    "    train_groups = set(groups[train_idx])\n",
    "    val_groups = set(groups[val_idx])\n",
    "    assert len(train_groups & val_groups) == 0\n",
    "```\n",
    "\n",
    "**Use for:** Medical data (multiple scans per patient), user behavior data (multiple sessions per user).\n",
    "\n",
    "#### **9.1.5 Leave-One-Out (LOO) and Leave-One-Group-Out (LOGO)**\n",
    "\n",
    "Extreme CV: $n$ folds, each validation is single sample (or single group).\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import LeaveOneOut, LeaveOneGroupOut\n",
    "\n",
    "loo = LeaveOneOut()\n",
    "# Use only for small datasets (n < 1000) - computationally expensive\n",
    "\n",
    "logo = LeaveOneGroupOut()\n",
    "# Leave out entire patients/users one at a time\n",
    "```\n",
    "\n",
    "**Computational Cost:** LOO is $O(n)$ expensive. Use only for small datasets or when groups are large.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.2 The Bias-Variance Tradeoff**\n",
    "\n",
    "Fundamental tension in machine learning.\n",
    "\n",
    "#### **9.2.1 Decomposition**\n",
    "\n",
    "Expected prediction error at point $x$:\n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = \\underbrace{\\text{Bias}^2[\\hat{f}(x)]}_{\\text{underfitting}} + \\underbrace{\\text{Var}[\\hat{f}(x)]}_{\\text{overfitting}} + \\underbrace{\\sigma^2}_{\\text{irreducible error}}$$\n",
    "\n",
    "- **Bias:** Error from erroneous assumptions (too simple model). High bias = underfitting.\n",
    "- **Variance:** Error from sensitivity to training set fluctuations (too complex model). High variance = overfitting.\n",
    "\n",
    "#### **9.2.2 Diagnosis**\n",
    "\n",
    "**High Bias (Underfitting):**\n",
    "- High training error\n",
    "- Validation error ≈ Training error\n",
    "- Both errors high\n",
    "\n",
    "**High Variance (Overfitting):**\n",
    "- Low training error\n",
    "- High validation error\n",
    "- Large gap between train/val\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import validation_curve\n",
    "\n",
    "# Validation curve: Vary model complexity, plot train/val scores\n",
    "param_range = np.arange(1, 20)  # e.g., max_depth in trees\n",
    "\n",
    "train_scores, val_scores = validation_curve(\n",
    "    estimator=model,\n",
    "    X=X, y=y,\n",
    "    param_name='max_depth',\n",
    "    param_range=param_range,\n",
    "    cv=5,\n",
    "    scoring='neg_mean_squared_error'\n",
    ")\n",
    "\n",
    "# Plot means\n",
    "train_mean = -np.mean(train_scores, axis=1)\n",
    "val_mean = -np.mean(val_scores, axis=1)\n",
    "\n",
    "plt.plot(param_range, train_mean, label='Training Error')\n",
    "plt.plot(param_range, val_mean, label='Validation Error')\n",
    "plt.xlabel('Model Complexity (max_depth)')\n",
    "plt.ylabel('Error')\n",
    "plt.legend()\n",
    "# Look for: Train ↓, Val ↓ then ↑ (sweet spot at minimum val)\n",
    "```\n",
    "\n",
    "#### **9.2.3 Solutions**\n",
    "\n",
    "**High Bias (Add Complexity):**\n",
    "- Add features / polynomial terms\n",
    "- Decrease regularization ($\\lambda$)\n",
    "- Use more complex model (linear → tree → ensemble)\n",
    "- Train longer (reduce early stopping patience)\n",
    "\n",
    "**High Variance (Add Constraints):**\n",
    "- More training data (most effective)\n",
    "- Feature selection / dimensionality reduction\n",
    "- Increase regularization (L1/L2)\n",
    "- Simplify model (reduce depth, neurons)\n",
    "- Ensemble methods (averaging reduces variance)\n",
    "- Data augmentation\n",
    "\n",
    "---\n",
    "\n",
    "## **9.3 Hyperparameter Tuning**\n",
    "\n",
    "#### **9.3.1 Grid Search**\n",
    "\n",
    "Exhaustive search over specified parameter values.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'C': [0.001, 0.01, 0.1, 1, 10, 100],\n",
    "    'gamma': ['scale', 'auto', 0.001, 0.01],\n",
    "    'kernel': ['rbf', 'poly']\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(\n",
    "    SVC(),\n",
    "    param_grid,\n",
    "    cv=5,\n",
    "    scoring='f1_macro',\n",
    "    n_jobs=-1,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "grid.fit(X_train, y_train)\n",
    "print(f\"Best: {grid.best_params_}\")\n",
    "print(f\"Score: {grid.best_score_:.3f}\")\n",
    "\n",
    "# Access best model directly\n",
    "best_model = grid.best_estimator_\n",
    "```\n",
    "\n",
    "**Cost:** $O(n^k)$ for $k$ parameters with $n$ values each. Exponential explosion!\n",
    "\n",
    "#### **9.3.2 Random Search**\n",
    "\n",
    "Sample random combinations. Often finds good solutions faster than grid search.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_distributions = {\n",
    "    'n_estimators': randint(100, 1000),\n",
    "    'max_depth': randint(3, 20),\n",
    "    'learning_rate': uniform(0.01, 0.3),  # uniform(low, high)\n",
    "    'subsample': uniform(0.6, 0.4)  # 0.6 to 1.0\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(\n",
    "    XGBClassifier(),\n",
    "    param_distributions,\n",
    "    n_iter=100,  # Try 100 random combinations\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "**Why Random > Grid:** Not all parameters matter equally. Grid wastes time on unimportant parameters.\n",
    "\n",
    "#### **9.3.3 Bayesian Optimization**\n",
    "\n",
    "Intelligent search using probabilistic models (Gaussian Processes or Tree-structured Parzen Estimators) to predict which hyperparameters are worth evaluating.\n",
    "\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer, Categorical\n",
    "\n",
    "search_space = {\n",
    "    'learning_rate': Real(0.01, 0.3, prior='log-uniform'),\n",
    "    'max_depth': Integer(3, 10),\n",
    "    'n_estimators': Integer(100, 1000),\n",
    "    'subsample': Real(0.5, 1.0)\n",
    "}\n",
    "\n",
    "opt = BayesSearchCV(\n",
    "    XGBClassifier(),\n",
    "    search_space,\n",
    "    n_iter=50,  # Evaluations\n",
    "    cv=5,\n",
    "    scoring='roc_auc',\n",
    "    n_jobs=-1,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "opt.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Libraries:**\n",
    "- **Optuna:** Industry standard (efficient pruning, distributed optimization)\n",
    "- **Hyperopt:** Classic choice, uses TPE\n",
    "- **Ray Tune:** Distributed hyperparameter tuning at scale\n",
    "\n",
    "**Optuna Example:**\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 100, 1000),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.5, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0)\n",
    "    }\n",
    "    \n",
    "    model = XGBClassifier(**params)\n",
    "    score = cross_val_score(model, X_train, y_train, cv=5, scoring='roc_auc').mean()\n",
    "    \n",
    "    return score\n",
    "\n",
    "study = optuna.create_study(direction='maximize')\n",
    "study.optimize(objective, n_trials=100, show_progress_bar=True)\n",
    "\n",
    "print(f\"Best: {study.best_params}\")\n",
    "```\n",
    "\n",
    "#### **9.3.4 Hyperband and Early Stopping**\n",
    "\n",
    "Successive Halving: Allocate more resources to promising configurations.\n",
    "\n",
    "```python\n",
    "from sklearn.experimental import enable_halving_search_cv\n",
    "from sklearn.model_selection import HalvingGridSearchCV\n",
    "\n",
    "# Eliminates poor configs early, allocates more budget to promising ones\n",
    "halving = HalvingGridSearchCV(\n",
    "    estimator,\n",
    "    param_grid,\n",
    "    factor=3,  # Discard 2/3 of configs each round\n",
    "    resource='n_samples',  # Can also use 'n_iterations' for iterative models\n",
    "    max_resources=10000,\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.4 Model Interpretability**\n",
    "\n",
    "Essential for debugging, compliance (GDPR \"right to explanation\"), and trust.\n",
    "\n",
    "#### **9.4.1 Feature Importance (Caution)**\n",
    "\n",
    "Tree-based default importance is biased toward high-cardinality features. Use Permutation Importance instead.\n",
    "\n",
    "```python\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "# Trained model\n",
    "result = permutation_importance(\n",
    "    model, X_val, y_val,\n",
    "    n_repeats=10,\n",
    "    random_state=42,\n",
    "    scoring='roc_auc'\n",
    ")\n",
    "\n",
    "sorted_idx = result.importances_mean.argsort()\n",
    "plt.barh(range(len(sorted_idx)), result.importances_mean[sorted_idx])\n",
    "plt.yticks(range(len(sorted_idx)), X.columns[sorted_idx])\n",
    "```\n",
    "\n",
    "**How it works:** Shuffle feature values, measure performance drop. Larger drop = more important.\n",
    "\n",
    "#### **9.4.2 SHAP (SHapley Additive exPlanations)**\n",
    "\n",
    "Game-theoretic approach: fair allocation of prediction attribution among features.\n",
    "\n",
    "```python\n",
    "import shap\n",
    "\n",
    "# TreeExplainer for tree models (fast)\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "\n",
    "# Summary plot (global feature importance)\n",
    "shap.summary_plot(shap_values, X_test, plot_type=\"bar\")\n",
    "\n",
    "# Detailed dot plot (shows direction)\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "\n",
    "# Individual prediction explanation (waterfall plot)\n",
    "shap.waterfall_plot(shap.Explanation(\n",
    "    values=shap_values[0],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_test.iloc[0],\n",
    "    feature_names=X_test.columns\n",
    "))\n",
    "```\n",
    "\n",
    "**Interpretation:**\n",
    "- **Red:** Increases prediction (higher risk/score)\n",
    "- **Blue:** Decreases prediction\n",
    "- **Width:** Magnitude of impact\n",
    "\n",
    "#### **9.4.3 LIME (Local Interpretable Model-agnostic Explanations)**\n",
    "\n",
    "Approximates complex model locally with simple interpretable model.\n",
    "\n",
    "```python\n",
    "import lime\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    X_train.values,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['Class 0', 'Class 1'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "# Explain single instance\n",
    "exp = explainer.explain_instance(\n",
    "    X_test.iloc[0].values,\n",
    "    model.predict_proba,\n",
    "    num_features=5\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "```\n",
    "\n",
    "**When to use:** Model-agnostic (works for any classifier), good for text/images (LIME has specialized explainers).\n",
    "\n",
    "#### **9.4.4 Partial Dependence Plots (PDP)**\n",
    "\n",
    "Show marginal effect of one or two features on predicted outcome.\n",
    "\n",
    "```python\n",
    "from sklearn.inspection import partial_dependence, PartialDependenceDisplay\n",
    "\n",
    "# Single feature\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    model, X_train, features=['age', 'income']\n",
    ")\n",
    "\n",
    "# Two-way interaction\n",
    "PartialDependenceDisplay.from_estimator(\n",
    "    model, X_train, features=[('age', 'income')]\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.5 Model Comparison and Selection**\n",
    "\n",
    "#### **9.5.1 Statistical Significance Testing**\n",
    "\n",
    "Is Model A really better than Model B, or just lucky?\n",
    "\n",
    "**McNemar's Test (for classifiers):**\n",
    "```python\n",
    "from statsmodels.stats.contingency_tables import mcnemar\n",
    "\n",
    "# Both models predictions\n",
    "model_a_correct = (y_pred_a == y_true)\n",
    "model_b_correct = (y_pred_b == y_true)\n",
    "\n",
    "# Contingency table\n",
    "# Both wrong | A right, B wrong\n",
    "# A wrong, B right | Both right\n",
    "table = [[sum(~model_a_correct & ~model_b_correct), sum(model_a_correct & ~model_b_correct)],\n",
    "         [sum(~model_a_correct & model_b_correct), sum(model_a_correct & model_b_correct)]]\n",
    "\n",
    "result = mcnemar(table, exact=True)\n",
    "print(f\"p-value: {result.pvalue}\")  # < 0.05 => significant difference\n",
    "```\n",
    "\n",
    "**Paired T-Test (for regression metrics):**\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# CV scores from same folds (paired)\n",
    "scores_a = cross_val_score(model_a, X, y, cv=5)\n",
    "scores_b = cross_val_score(model_b, X, y, cv=5)\n",
    "\n",
    "t_stat, p_value = stats.ttest_rel(scores_a, scores_b)\n",
    "```\n",
    "\n",
    "**Correction for Multiple Comparisons:**\n",
    "If comparing 10 models, use Bonferroni correction ($\\alpha' = \\alpha/10$) to avoid false positives.\n",
    "\n",
    "#### **9.5.2 Business-Aware Selection**\n",
    "\n",
    "Don't just pick highest accuracy. Consider:\n",
    "\n",
    "- **Inference Speed:** Is 1% accuracy gain worth 10x latency?\n",
    "- **Memory:** Can model fit on edge device?\n",
    "- **Maintenance:** Is complex ensemble worth operational overhead?\n",
    "- **Calibration:** Do probabilities matter for downstream decisions?\n",
    "\n",
    "**Decision Framework:**\n",
    "```python\n",
    "def score_model(model, X_val, y_val, business_params):\n",
    "    accuracy = model.score(X_val, y_val)\n",
    "    latency = measure_latency(model, X_val)\n",
    "    memory = get_model_size(model)\n",
    "    \n",
    "    # Weighted score\n",
    "    score = (0.6 * accuracy + \n",
    "             0.3 * (1 / latency) * business_params['speed_weight'] +\n",
    "             0.1 * (1 / memory) * business_params['memory_weight'])\n",
    "    return score\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Nested Cross-Validation**\n",
    "Implement nested CV to get unbiased estimate of model performance while tuning hyperparameters.\n",
    "\n",
    "**Outer loop:** 5-fold CV for performance estimation  \n",
    "**Inner loop:** Grid search for hyperparameter selection\n",
    "\n",
    "Compare against non-nested CV (optimistic bias demonstration).\n",
    "\n",
    "**Deliverable:** Show that nested CV gives lower, more realistic score than standard CV.\n",
    "\n",
    "### **Lab 2: Bias-Variance Analysis**\n",
    "Using polynomial regression on synthetic data:\n",
    "1. Fit degrees 1, 3, 5, 10, 20\n",
    "2. Plot train vs validation error curves\n",
    "3. Identify bias/variance regions\n",
    "4. Apply regularization to high-variance models and show improvement\n",
    "\n",
    "**Deliverable:** Visualization with annotations showing underfitting/overfitting zones.\n",
    "\n",
    "### **Lab 3: Hyperparameter Optimization Comparison**\n",
    "On same dataset, compare:\n",
    "- Grid Search (coarse grid)\n",
    "- Random Search (same budget as grid)\n",
    "- Bayesian Optimization (Optuna, 100 trials)\n",
    "\n",
    "Measure:\n",
    "- Best score found\n",
    "- Wall clock time\n",
    "- Number of model evaluations\n",
    "\n",
    "**Deliverable:** Report showing Bayesian finds better solution with fewer evaluations.\n",
    "\n",
    "### **Lab 4: Model Debugging with SHAP**\n",
    "Train a Random Forest on biased data (e.g., gender correlated with target spuriously):\n",
    "1. Show model relies on protected attribute (gender, race)\n",
    "2. Use SHAP to identify this bias\n",
    "3. Remove feature and retrain\n",
    "4. Show performance drop vs fairness gain\n",
    "\n",
    "**Deliverable:** Bias audit report using SHAP for transparency.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.7 Common Pitfalls**\n",
    "\n",
    "1. **Data Leakage in CV:** Feature selection before CV (must be inside CV loop).\n",
    "   ```python\n",
    "   # WRONG\n",
    "   selector.fit(X, y)  # Uses all data!\n",
    "   X_selected = selector.transform(X)\n",
    "   cross_val_score(model, X_selected, y, cv=5)\n",
    "   \n",
    "   # RIGHT\n",
    "   pipeline = Pipeline([('select', selector), ('model', model)])\n",
    "   cross_val_score(pipeline, X, y, cv=5)\n",
    "   ```\n",
    "\n",
    "2. **Multiple Comparison Problem:** Testing 20 hyperparameter sets, one will be \"significant\" by chance. Use validation set or nested CV.\n",
    "\n",
    "3. **Overfitting the Validation Set:** Repeatedly tweaking model based on same validation set = overfitting. Use test set only once at end, or use hold-out validation for final selection.\n",
    "\n",
    "4. **Ignoring Confidence Intervals:** Reporting mean CV score without std error. Always show variance!\n",
    "\n",
    "5. **Tuning on Test Set:** Using test set for hyperparameter selection. Fatal error—test set becomes validation set, no unbiased estimate remains.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.8 Interview Questions**\n",
    "\n",
    "**Q1:** What is the difference between stratified k-fold and standard k-fold? When must you use stratified?\n",
    "*A: Stratified preserves class distribution in each fold (e.g., 10% positives in each fold if that's the global rate). Standard k-fold randomly samples. Must use stratified for imbalanced classification (e.g., fraud detection with 0.1% fraud) to ensure rare class represented in every fold. Also critical for small datasets where random chance might omit minority class entirely from a fold.*\n",
    "\n",
    "**Q2:** Explain nested cross-validation and why it's necessary for small datasets.\n",
    "*A: Standard CV uses the same data to tune hyperparameters and evaluate performance, leading to optimistic bias (information leakage from validation to training via hyperparameters). Nested CV has an outer loop for performance estimation and inner loop for hyperparameter search, keeping them separate. Necessary for small datasets where every sample matters and overfitting to validation set is likely.*\n",
    "\n",
    "**Q3:** Why does Bayesian optimization typically outperform grid search?\n",
    "*A: Bayesian optimization builds a probabilistic model (surrogate) of the objective function, using previous evaluations to predict which hyperparameters are most promising (exploration vs exploitation). Grid search wastes evaluations on unpromising regions and requires exponentially more trials as dimensions increase. Bayesian focuses search on high-performance regions adaptively.*\n",
    "\n",
    "**Q4:** What's the difference between SHAP and LIME?\n",
    "*A: SHAP is based on game theory (Shapley values) providing global consistency (sum of feature contributions equals prediction minus baseline) and local accuracy. It's exact for tree models (TreeSHAP). LIME approximates the model locally with an interpretable surrogate (e.g., linear model) around a specific prediction. SHAP is more theoretically grounded but computationally expensive for non-tree models; LIME is model-agnostic and faster but only approximate.*\n",
    "\n",
    "**Q5:** Your model has 95% accuracy on training, 70% on validation. What do you try first?\n",
    "*A: High variance (overfitting). First try: increase regularization (L2 penalty, dropout, reduce model complexity), collect more training data if possible, or feature selection to reduce noise. Check for data leakage if gap is extreme (>30%). Also verify validation set is representative (not distribution shift).*\n",
    "\n",
    "---\n",
    "\n",
    "## **9.9 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *The Elements of Statistical Learning* (Hastie et al.) - Chapter 7 (Model Assessment and Selection)\n",
    "- *Applied Predictive Modeling* (Kuhn & Johnson) - Comprehensive CV and tuning strategies\n",
    "\n",
    "**Papers:**\n",
    "- \"A Survey of Cross-Validation Procedures for Model Selection\" (Arlot & Celisse, 2010)\n",
    "- \"Algorithms for Hyper-Parameter Optimization\" (Bergstra et al., 2011) - Random Search vs Grid\n",
    "- \"A Unified Approach to Interpreting Model Predictions\" (Lundberg & Lee, 2017) - SHAP paper\n",
    "\n",
    "**Tools:**\n",
    "- **Optuna:** https://optuna.org/ (best for hyperparameter optimization)\n",
    "- **SHAP:** https://shap.readthedocs.io/\n",
    "- **Weights & Biases:** Experiment tracking and visualization\n",
    "\n",
    "---\n",
    "\n",
    "## **9.10 Checkpoint Project: Automated Model Selection System**\n",
    "\n",
    "Build an AutoML-lite system that automatically selects the best model for a given dataset.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Data Analyzer:**\n",
    "   - Detect problem type (classification/regression)\n",
    "   - Detect imbalance ratio\n",
    "   - Suggest appropriate CV strategy (Stratified/TimeSeries/Group)\n",
    "\n",
    "2. **Model Portfolio:**\n",
    "   - Baseline: Logistic/Linear Regression\n",
    "   - Trees: Random Forest, XGBoost, LightGBM\n",
    "   - Linear: Ridge/Lasso with polynomial features\n",
    "\n",
    "3. **Hyperparameter Optimization:**\n",
    "   - Use Optuna for each model type (50 trials each)\n",
    "   - Pruning: early stopping of unpromising trials\n",
    "   - Time budget: 1 hour max total\n",
    "\n",
    "4. **Model Evaluation:**\n",
    "   - Nested CV for unbiased performance estimate\n",
    "   - Statistical testing to determine if top 2 models are significantly different\n",
    "   - Calibration check (Brier score or reliability diagram)\n",
    "\n",
    "5. **Reporting:**\n",
    "   - Leaderboard with confidence intervals\n",
    "   - SHAP summary for best model\n",
    "   - Recommendation: \"Deploy XGBoost (AUC 0.85 ± 0.02), but consider Logistic Regression (AUC 0.83 ± 0.01) if interpretability required\"\n",
    "\n",
    "**Deliverables:**\n",
    "- `autoselect/` Python package with CLI interface\n",
    "- `autoselect train --data data.csv --target y --time-budget 3600`\n",
    "- Output: `results.json` with ranked models and selected best model artifact\n",
    "- Documentation of when system fails (e.g., too high cardinality categorical features)\n",
    "\n",
    "**Success Criteria:**\n",
    "- System beats random baseline by >10% on 3 diverse test datasets\n",
    "- Runs within time budget\n",
    "- Provides calibrated probability estimates (reliability diagram diagonal)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 9**\n",
    "\n",
    "*You can now rigorously evaluate models and select appropriate candidates for production. Chapter 10 begins Phase 3: Deep Learning & Neural Networks — starting with Neural Network Fundamentals.*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
