{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3811540a",
   "metadata": {},
   "source": [
    "# CHAPTER 5: DATA PREPROCESSING & FEATURE ENGINEERING\n",
    "\n",
    "*The Foundation of Every Great Model*\n",
    "\n",
    "## Chapter Overview\n",
    "\n",
    "In production ML, 80% of your time will be spent on data — cleaning it, understanding it, transforming it into features that models can learn from. This chapter transforms you from someone who runs `.fit()` on clean CSVs into an engineer who can rescue messy real-world data, engineer features that capture domain knowledge, and build preprocessing pipelines that scale from laptops to distributed clusters.\n",
    "\n",
    "**Estimated Time:** 35-45 hours (2-3 weeks)  \n",
    "**Prerequisites:** Chapters 1-4, pandas proficiency, basic statistics\n",
    "\n",
    "---\n",
    "\n",
    "## 5.0 Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "1. Diagnose and fix real-world data quality issues: missing values, outliers, duplicates, and inconsistencies\n",
    "2. Apply appropriate scaling and encoding techniques based on model requirements and data distributions\n",
    "3. Engineer domain-aware features that capture temporal patterns, interactions, and aggregates\n",
    "4. Reduce dimensionality while preserving signal using PCA, t-SNE, and feature selection methods\n",
    "5. Build production-ready preprocessing pipelines with scikit-learn that transform consistently across train/inference\n",
    "6. Detect data drift and validate incoming data against training distributions\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Data Cleaning: The 80% Work\n",
    "\n",
    "Real-world data is never clean. Your first job is to make it usable without introducing bias or losing signal.\n",
    "\n",
    "### 5.1.1 Exploratory Data Analysis (EDA) First\n",
    "\n",
    "Before cleaning, understand what you're working with:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load and inspect\n",
    "df = pd.read_csv('customer_data.csv')\n",
    "\n",
    "# First glance\n",
    "print(df.shape)\n",
    "print(df.info())                 # Data types, non-null counts\n",
    "print(df.describe(include='all')) # Summary statistics\n",
    "\n",
    "# Missing values\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df)) * 100\n",
    "print(pd.DataFrame({'missing': missing, 'percent': missing_pct}).sort_values('missing', ascending=False))\n",
    "\n",
    "# Data types and unique values\n",
    "for col in df.columns:\n",
    "    print(f\"{col}: {df[col].dtype} - {df[col].nunique()} unique values\")\n",
    "    \n",
    "# Visual distributions\n",
    "df.hist(bins=50, figsize=(20,15))\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Correlation matrix (numerical features only)\n",
    "plt.figure(figsize=(12,10))\n",
    "sns.heatmap(df.select_dtypes(include=[np.number]).corr(), annot=True, fmt='.2f')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Key Questions to Answer:**\n",
    "- What's the data type? (Should it be numeric, categorical, datetime?)\n",
    "- What's the range/domain? (Negative ages? Future birth dates?)\n",
    "- How much data is missing? Is it random or systematic?\n",
    "- Are there obvious outliers? (Visualize with box plots)\n",
    "- Are categorical variables balanced?\n",
    "\n",
    "### 5.1.2 Missing Data: The Silent Model Killer\n",
    "\n",
    "Missing values aren't just empty cells — they're information about your data collection process.\n",
    "\n",
    "**Types of Missing Data:**\n",
    "- **MCAR (Missing Completely at Random):** No pattern (e.g., sensor randomly failed)\n",
    "- **MAR (Missing at Random):** Pattern related to observed data (e.g., women less likely to report income)\n",
    "- **MNAR (Missing Not at Random):** Pattern related to unobserved data (e.g., high-income people hide income)\n",
    "\n",
    "**Detection:**\n",
    "```python\n",
    "# Visualize missing patterns\n",
    "import missingno as msno\n",
    "\n",
    "msno.matrix(df)              # Missing pattern visualization\n",
    "msno.heatmap(df)             # Correlation between missing columns\n",
    "msno.dendrogram(df)          # Hierarchical clustering of missingness\n",
    "\n",
    "# Test if missing is random (simplified)\n",
    "# Group by a categorical column and check missing rates\n",
    "for col in df.select_dtypes(include=['object']).columns:\n",
    "    missing_by_group = df.groupby(col)['target_column'].apply(lambda x: x.isnull().mean())\n",
    "    print(f\"\\nMissing rates by {col}:\")\n",
    "    print(missing_by_group)\n",
    "```\n",
    "\n",
    "**Handling Strategies:**\n",
    "\n",
    "| Strategy | When to Use | Code | Pros/Cons |\n",
    "|----------|------------|------|-----------|\n",
    "| **Delete rows** | <5% missing, MCAR | `df.dropna(subset=['important_col'])` | Simple, loses data |\n",
    "| **Delete columns** | >70% missing, low importance | `df.drop(columns=['bad_col'])` | Preserves rows, loses feature |\n",
    "| **Mean/Median** | Numerical, low missing, no relationship | `df['age'].fillna(df['age'].median())` | Fast, reduces variance |\n",
    "| **Mode** | Categorical, low missing | `df['gender'].fillna(df['gender'].mode()[0])` | Simple for categories |\n",
    "| **Forward/Backward fill** | Time series | `df['sales'].fillna(method='ffill')` | Respects temporal order |\n",
    "| **Interpolation** | Time series with trend | `df['temp'].interpolate(method='linear')` | More accurate than ffill |\n",
    "| **KNN Imputation** | MAR, relationships exist | `from sklearn.impute import KNNImputer` | Captures patterns, slow |\n",
    "| **Model-based** | Complex patterns | `from sklearn.experimental import enable_iterative_imputer` | Best accuracy, expensive |\n",
    "| **Add \"missing\" indicator** | Missingness itself is informative | `df['age_missing'] = df['age'].isnull().astype(int)` | Preserves signal |\n",
    "\n",
    "**Production-Grade Imputation:**\n",
    "```python\n",
    "from sklearn.impute import SimpleImputer, KNNImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "# Numerical imputation\n",
    "num_imputer = SimpleImputer(strategy='median')  # Robust to outliers\n",
    "\n",
    "# Categorical imputation\n",
    "cat_imputer = SimpleImputer(strategy='constant', fill_value='MISSING')\n",
    "\n",
    "# KNN imputation for complex patterns\n",
    "knn_imputer = KNNImputer(n_neighbors=5, weights='distance')\n",
    "\n",
    "# Column transformer for mixed types\n",
    "preprocessor = ColumnTransformer([\n",
    "    ('num_impute', num_imputer, numerical_cols),\n",
    "    ('cat_impute', cat_imputer, categorical_cols)\n",
    "])\n",
    "\n",
    "# Fit on training data only (never on validation/test!)\n",
    "X_train_imputed = preprocessor.fit_transform(X_train)\n",
    "X_test_imputed = preprocessor.transform(X_test)\n",
    "```\n",
    "\n",
    "### 5.1.3 Outlier Detection and Treatment\n",
    "\n",
    "Outliers can be genuine rare events or data errors. Your treatment depends on which.\n",
    "\n",
    "**Detection Methods:**\n",
    "\n",
    "```python\n",
    "# 1. Z-Score (assumes normal distribution)\n",
    "from scipy import stats\n",
    "z_scores = np.abs(stats.zscore(df['sales']))\n",
    "outliers_z = df[z_scores > 3]  # Beyond 3 std deviations\n",
    "\n",
    "# 2. IQR Method (distribution-agnostic)\n",
    "Q1 = df['sales'].quantile(0.25)\n",
    "Q3 = df['sales'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "outliers_iqr = df[(df['sales'] < Q1 - 1.5 * IQR) | (df['sales'] > Q3 + 1.5 * IQR)]\n",
    "\n",
    "# 3. Isolation Forest (multivariate)\n",
    "from sklearn.ensemble import IsolationForest\n",
    "iso_forest = IsolationForest(contamination=0.05, random_state=42)\n",
    "outliers_if = iso_forest.fit_predict(df[numerical_cols]) == -1\n",
    "\n",
    "# 4. DBSCAN (density-based)\n",
    "from sklearn.cluster import DBSCAN\n",
    "clustering = DBSCAN(eps=0.5, min_samples=10).fit(df[numerical_cols])\n",
    "outliers_db = clustering.labels_ == -1\n",
    "\n",
    "# 5. Visualization\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "df.boxplot(column='sales', ax=axes[0])\n",
    "axes[0].set_title('Box Plot')\n",
    "df['sales'].hist(bins=50, ax=axes[1])\n",
    "axes[1].set_title('Histogram')\n",
    "axes[2].scatter(df.index, df['sales'])\n",
    "axes[2].set_title('Scatter Plot')\n",
    "plt.tight_layout()\n",
    "```\n",
    "\n",
    "**Treatment Options:**\n",
    "\n",
    "```python\n",
    "# 1. Cap/Clip (Winsorization)\n",
    "upper_limit = df['sales'].quantile(0.99)\n",
    "lower_limit = df['sales'].quantile(0.01)\n",
    "df['sales_capped'] = df['sales'].clip(lower_limit, upper_limit)\n",
    "\n",
    "# 2. Log transform (pulls in long tails)\n",
    "df['sales_log'] = np.log1p(df['sales'])  # log(1+x) handles zeros\n",
    "\n",
    "# 3. Box-Cox transform (finds optimal normalization)\n",
    "from scipy import stats\n",
    "df['sales_boxcox'], lambda_opt = stats.boxcox(df['sales'] + 1)  # +1 for zeros\n",
    "print(f\"Optimal lambda: {lambda_opt}\")\n",
    "\n",
    "# 4. Remove if clearly erroneous\n",
    "df = df[df['age'].between(0, 120)]  # Remove impossible ages\n",
    "\n",
    "# 5. Treat as separate class (for anomalies)\n",
    "df['is_outlier'] = (z_scores > 3).astype(int)\n",
    "```\n",
    "\n",
    "### 5.1.4 Duplicates and Inconsistencies\n",
    "\n",
    "```python\n",
    "# Exact duplicates\n",
    "duplicates = df[df.duplicated(keep=False)]  # Show all duplicates\n",
    "df = df.drop_duplicates()  # Keep first occurrence\n",
    "\n",
    "# Near duplicates (fuzzy matching for text)\n",
    "from rapidfuzz import fuzz\n",
    "def find_duplicate_strings(series, threshold=90):\n",
    "    \"\"\"Find near-duplicate strings using Levenshtein distance\"\"\"\n",
    "    duplicates = []\n",
    "    for i, val1 in enumerate(series):\n",
    "        for j, val2 in enumerate(series[i+1:], i+1):\n",
    "            if fuzz.ratio(str(val1), str(val2)) > threshold:\n",
    "                duplicates.append((i, j, val1, val2))\n",
    "    return duplicates\n",
    "\n",
    "# Inconsistent categorical values\n",
    "df['country'] = df['country'].str.lower().str.strip()\n",
    "country_mapping = {\n",
    "    'usa': 'US', 'united states': 'US', 'america': 'US',\n",
    "    'uk': 'UK', 'united kingdom': 'UK', 'england': 'UK'\n",
    "}\n",
    "df['country_clean'] = df['country'].map(country_mapping).fillna(df['country'])\n",
    "\n",
    "# Date inconsistencies\n",
    "df['date'] = pd.to_datetime(df['date'], errors='coerce')  # Invalid dates become NaT\n",
    "df = df.dropna(subset=['date'])  # Drop rows with invalid dates\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Feature Scaling: Giving All Features Equal Voice\n",
    "\n",
    "Models that use distance (KNN, SVM, neural nets) or regularization (Ridge, Lasso) require scaled features. Tree-based models don't.\n",
    "\n",
    "### 5.2.1 Scaling Techniques Compared\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler, MaxAbsScaler\n",
    "import numpy as np\n",
    "\n",
    "# Generate sample data with outliers\n",
    "np.random.seed(42)\n",
    "data = np.random.normal(100, 20, 1000)  # Normal distribution\n",
    "data = np.append(data, [500, -200])     # Add outliers\n",
    "\n",
    "# Create DataFrame for comparison\n",
    "df_scaling = pd.DataFrame({'original': data})\n",
    "\n",
    "# Apply different scalers\n",
    "scalers = {\n",
    "    'Standard (Z-score)': StandardScaler(),\n",
    "    'Min-Max': MinMaxScaler(),\n",
    "    'Robust (IQR)': RobustScaler(),\n",
    "    'MaxAbs': MaxAbsScaler()\n",
    "}\n",
    "\n",
    "for name, scaler in scalers.items():\n",
    "    df_scaling[name] = scaler.fit_transform(data.reshape(-1, 1))\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "axes = axes.ravel()\n",
    "\n",
    "for i, (name, scaler) in enumerate(scalers.items()):\n",
    "    axes[i].hist(df_scaling[name], bins=50, edgecolor='black')\n",
    "    axes[i].set_title(f'{name}\\nMean: {df_scaling[name].mean():.2f}, Std: {df_scaling[name].std():.2f}')\n",
    "    axes[i].axvline(x=0, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "| Scaler | Formula | Range | Best For | Sensitivity |\n",
    "|--------|---------|-------|----------|-------------|\n",
    "| **StandardScaler** | (x - μ) / σ | ~[-3,3] | Normally distributed data | Outliers |\n",
    "| **MinMaxScaler** | (x - min) / (max - min) | [0,1] | Bounded data, neural nets | Outliers |\n",
    "| **RobustScaler** | (x - median) / IQR | ~[-4,4] | Data with outliers | Robust |\n",
    "| **MaxAbsScaler** | x / max\\|x\\| | [-1,1] | Sparse data | Outliers |\n",
    "\n",
    "### 5.2.2 When to Scale What\n",
    "\n",
    "```python\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, RobustScaler\n",
    "\n",
    "# Mixed scaling based on feature characteristics\n",
    "preprocessor = ColumnTransformer([\n",
    "    # Features with outliers -> RobustScaler\n",
    "    ('robust', RobustScaler(), ['income', 'transaction_amount']),\n",
    "    \n",
    "    # Features bounded by nature -> MinMaxScaler\n",
    "    ('minmax', MinMaxScaler(), ['age', 'satisfaction_score']),\n",
    "    \n",
    "    # Normally distributed features -> StandardScaler\n",
    "    ('standard', StandardScaler(), ['height', 'weight']),\n",
    "    \n",
    "    # Leave binary features as-is\n",
    "    ('passthrough', 'passthrough', ['is_male', 'has_children'])\n",
    "])\n",
    "\n",
    "# Fit on training\n",
    "X_train_scaled = preprocessor.fit_transform(X_train)\n",
    "X_test_scaled = preprocessor.transform(X_test)\n",
    "```\n",
    "\n",
    "### 5.2.3 Critical Pitfall: Data Leakage\n",
    "\n",
    "Never fit scalers on the entire dataset before splitting!\n",
    "\n",
    "```python\n",
    "# WRONG - leaks test information into training\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)  # Uses test data in fit!\n",
    "X_train, X_test = train_test_split(X_scaled, ...)\n",
    "\n",
    "# RIGHT - fit only on training\n",
    "X_train, X_test = train_test_split(X, ...)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)  # Uses training statistics\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3 Encoding Categorical Variables: Speaking the Model's Language\n",
    "\n",
    "Models understand numbers, not strings. Encoding transforms categories into numbers while preserving information.\n",
    "\n",
    "### 5.3.1 Nominal vs Ordinal Categories\n",
    "\n",
    "```python\n",
    "# Ordinal (has order) - preserve ordering\n",
    "education_order = {'High School': 1, 'Bachelor': 2, 'Master': 3, 'PhD': 4}\n",
    "df['education_encoded'] = df['education'].map(education_order)\n",
    "\n",
    "# Nominal (no order) - don't imply relationships\n",
    "# WRONG: df['color'] = {'red':1, 'green':2, 'blue':3}  # Implies green > red\n",
    "```\n",
    "\n",
    "### 5.3.2 One-Hot Encoding (The Workhorse)\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Pandas way\n",
    "df_encoded = pd.get_dummies(df, columns=['color', 'city'], drop_first=True)\n",
    "\n",
    "# Scikit-learn way (better for pipelines)\n",
    "encoder = OneHotEncoder(drop='first', sparse_output=False, handle_unknown='ignore')\n",
    "encoded = encoder.fit_transform(df[['color', 'city']])\n",
    "\n",
    "# Create DataFrame with feature names\n",
    "feature_names = encoder.get_feature_names_out(['color', 'city'])\n",
    "df_encoded = pd.DataFrame(encoded, columns=feature_names, index=df.index)\n",
    "\n",
    "# Handling high cardinality (many categories)\n",
    "# Option 1: Keep top K categories, group rest as 'other'\n",
    "top_cities = df['city'].value_counts().nlargest(10).index\n",
    "df['city_grouped'] = df['city'].where(df['city'].isin(top_cities), 'Other')\n",
    "\n",
    "# Option 2: Frequency encoding (replace with count)\n",
    "city_counts = df['city'].value_counts()\n",
    "df['city_freq'] = df['city'].map(city_counts) / len(df)  # Normalized frequency\n",
    "```\n",
    "\n",
    "### 5.3.3 Target Encoding (Powerful but Dangerous)\n",
    "\n",
    "Replace category with mean of target for that category. Excellent for tree models but causes target leakage if done wrong.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import KFold\n",
    "import numpy as np\n",
    "\n",
    "def target_encode(X, y, cat_col, n_folds=5):\n",
    "    \"\"\"\n",
    "    Safe target encoding with cross-validation to prevent leakage\n",
    "    \"\"\"\n",
    "    X = X.copy()\n",
    "    X['target_encoded'] = np.nan\n",
    "    \n",
    "    kfold = KFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    \n",
    "    for train_idx, val_idx in kfold.split(X):\n",
    "        # Compute mean on training fold only\n",
    "        cat_means = y.iloc[train_idx].groupby(X.iloc[train_idx][cat_col]).mean()\n",
    "        \n",
    "        # Apply to validation fold\n",
    "        X.loc[val_idx, 'target_encoded'] = X.loc[val_idx, cat_col].map(cat_means)\n",
    "    \n",
    "    # For new categories, use global mean\n",
    "    global_mean = y.mean()\n",
    "    X['target_encoded'].fillna(global_mean, inplace=True)\n",
    "    \n",
    "    return X['target_encoded']\n",
    "\n",
    "# Usage (for binary classification)\n",
    "df['city_target_encoded'] = target_encode(df, df['churned'], 'city')\n",
    "```\n",
    "\n",
    "### 5.3.4 Advanced: Embeddings for High Cardinality\n",
    "\n",
    "For categories with hundreds/thousands of values (user IDs, ZIP codes), learn embeddings during model training.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CategoryEmbedding(nn.Module):\n",
    "    def __init__(self, num_categories, embedding_dim):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(num_categories, embedding_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: category indices\n",
    "        return self.embedding(x)\n",
    "\n",
    "# In practice, this is integrated into your neural network\n",
    "# embedding_dim = min(50, num_categories // 2) is a common heuristic\n",
    "```\n",
    "\n",
    "### 5.3.5 Encoding Decision Matrix\n",
    "\n",
    "| Encoding | Cardinality | Model Type | Pros | Cons |\n",
    "|----------|-------------|------------|------|------|\n",
    "| **One-Hot** | Low (<10) | Any | No false ordinality | Creates many columns |\n",
    "| **Label** | Low, ordinal | Tree-based | Simple | Implies order |\n",
    "| **Frequency** | Medium | Tree-based | Single column, captures popularity | Collisions possible |\n",
    "| **Target** | Medium | Tree-based | Powerful signal | Leakage risk, overfitting |\n",
    "| **Embeddings** | High | Neural nets | Learns semantic similarity | Complex, needs data |\n",
    "| **Binary** | Medium | Any | Fewer cols than one-hot | Less interpretable |\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4 Feature Engineering: Where Domain Expertise Shines\n",
    "\n",
    "The best features come from understanding the problem, not just the data.\n",
    "\n",
    "### 5.4.1 Temporal Features\n",
    "\n",
    "```python\n",
    "# From timestamp\n",
    "df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "\n",
    "# Time components\n",
    "df['hour'] = df['timestamp'].dt.hour\n",
    "df['day'] = df['timestamp'].dt.day\n",
    "df['dayofweek'] = df['timestamp'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "df['month'] = df['timestamp'].dt.month\n",
    "df['quarter'] = df['timestamp'].dt.quarter\n",
    "df['year'] = df['timestamp'].dt.year\n",
    "df['is_weekend'] = df['dayofweek'].isin([5, 6]).astype(int)\n",
    "\n",
    "# Cyclical encoding (preserves circular nature)\n",
    "df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "\n",
    "# Lag features (past values)\n",
    "df['sales_lag_1'] = df.groupby('store_id')['sales'].shift(1)\n",
    "df['sales_lag_7'] = df.groupby('store_id')['sales'].shift(7)\n",
    "\n",
    "# Rolling statistics\n",
    "df['sales_rolling_mean_7'] = df.groupby('store_id')['sales'].transform(\n",
    "    lambda x: x.rolling(7, min_periods=1).mean()\n",
    ")\n",
    "df['sales_rolling_std_7'] = df.groupby('store_id')['sales'].transform(\n",
    "    lambda x: x.rolling(7, min_periods=1).std()\n",
    ")\n",
    "\n",
    "# Time since last event\n",
    "df['days_since_last_purchase'] = df.groupby('customer_id')['date'].diff().dt.days\n",
    "```\n",
    "\n",
    "### 5.4.2 Interaction Features\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Manual interactions\n",
    "df['income_per_capita'] = df['household_income'] / df['household_size']\n",
    "df['price_per_unit'] = df['total_price'] / df['quantity']\n",
    "df['age_squared'] = df['age'] ** 2  # Capture non-linear effects\n",
    "\n",
    "# Polynomial features (auto-generate interactions)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=False, include_bias=False)\n",
    "interactions = poly.fit_transform(df[['income', 'age', 'education']])\n",
    "interaction_df = pd.DataFrame(\n",
    "    interactions, \n",
    "    columns=poly.get_feature_names_out(['income', 'age', 'education'])\n",
    ")\n",
    "\n",
    "# Domain-specific interactions\n",
    "# e.g., in e-commerce: user browsing behavior × item category\n",
    "df['user_cat_click_rate'] = df.groupby(['user_id', 'category'])['clicked'].transform('mean')\n",
    "```\n",
    "\n",
    "### 5.4.3 Aggregation Features\n",
    "\n",
    "```python\n",
    "# Customer-level aggregations\n",
    "customer_features = df.groupby('customer_id').agg({\n",
    "    'transaction_amount': ['mean', 'std', 'max', 'min', 'sum', 'count'],\n",
    "    'timestamp': lambda x: (x.max() - x.min()).days,  # customer lifetime\n",
    "    'product_id': 'nunique',  # distinct products bought\n",
    "    'is_return': 'mean'  # return rate\n",
    "}).reset_index()\n",
    "\n",
    "# Flatten column names\n",
    "customer_features.columns = ['_'.join(col).strip() for col in customer_features.columns.values]\n",
    "\n",
    "# Window aggregations (last N days)\n",
    "df['last_7d_spend'] = df.groupby('customer_id')['amount'].transform(\n",
    "    lambda x: x.rolling('7D', on='date').sum()\n",
    ")\n",
    "\n",
    "# Ratio features\n",
    "df['return_rate'] = df.groupby('customer_id')['is_return'].transform('mean')\n",
    "df['avg_order_value'] = df['amount'] / df['order_count']\n",
    "```\n",
    "\n",
    "### 5.4.4 Text Features\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# Basic text features\n",
    "df['text_length'] = df['description'].str.len()\n",
    "df['word_count'] = df['description'].str.split().str.len()\n",
    "df['avg_word_length'] = df['text_length'] / df['word_count']\n",
    "\n",
    "# Bag of words (for ML, not deep learning)\n",
    "vectorizer = TfidfVectorizer(max_features=1000, stop_words='english')\n",
    "text_features = vectorizer.fit_transform(df['description'])\n",
    "text_df = pd.DataFrame(\n",
    "    text_features.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out()\n",
    ")\n",
    "\n",
    "# Sentiment scores (using pre-trained models)\n",
    "from textblob import TextBlob\n",
    "df['sentiment'] = df['review'].apply(lambda x: TextBlob(x).sentiment.polarity)\n",
    "df['subjectivity'] = df['review'].apply(lambda x: TextBlob(x).sentiment.subjectivity)\n",
    "```\n",
    "\n",
    "### 5.4.5 Feature Engineering Checklist\n",
    "\n",
    "- [ ] Did I create features that capture domain knowledge?\n",
    "- [ ] Did I handle temporal patterns (seasonality, trends, lags)?\n",
    "- [ ] Did I create interaction features that might matter?\n",
    "- [ ] Did I aggregate to appropriate levels (customer, product, store)?\n",
    "- [ ] Did I avoid leakage (using future information)?\n",
    "- [ ] Did I validate that new features improve model performance?\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5 Dimensionality Reduction: When Less is More\n",
    "\n",
    "Too many features cause overfitting, slow training, and the curse of dimensionality.\n",
    "\n",
    "### 5.5.1 Feature Selection Methods\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import (\n",
    "    SelectKBest, SelectPercentile,\n",
    "    f_classif, mutual_info_classif,\n",
    "    RFE, SelectFromModel\n",
    ")\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 1. Filter Methods (univariate)\n",
    "selector_kbest = SelectKBest(score_func=mutual_info_classif, k=20)\n",
    "X_selected = selector_kbest.fit_transform(X, y)\n",
    "\n",
    "# Get selected features\n",
    "selected_features = X.columns[selector_kbest.get_support()].tolist()\n",
    "\n",
    "# 2. Wrapper Methods (RFE - Recursive Feature Elimination)\n",
    "estimator = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "selector_rfe = RFE(estimator, n_features_to_select=20, step=5)\n",
    "selector_rfe.fit(X, y)\n",
    "selected_rfe = X.columns[selector_rfe.support_].tolist()\n",
    "\n",
    "# 3. Embedded Methods (from model)\n",
    "selector_model = SelectFromModel(\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    threshold='median',  # Keep features above median importance\n",
    "    max_features=20\n",
    ")\n",
    "selector_model.fit(X, y)\n",
    "selected_model = X.columns[selector_model.get_support()].tolist()\n",
    "\n",
    "# Feature importance visualization\n",
    "importances = pd.DataFrame({\n",
    "    'feature': X.columns,\n",
    "    'importance': selector_model.estimator_.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.barh(importances.head(20)['feature'], importances.head(20)['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 20 Feature Importances')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.tight_layout()\n",
    "```\n",
    "\n",
    "### 5.5.2 PCA (Principal Component Analysis)\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Always scale before PCA\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X_scaled)\n",
    "\n",
    "# Explained variance ratio\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "# Find number of components for 95% variance\n",
    "n_components = np.argmax(cumulative_variance >= 0.95) + 1\n",
    "print(f\"Components needed for 95% variance: {n_components}\")\n",
    "\n",
    "# Plot elbow curve\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(range(1, len(explained_variance) + 1), explained_variance, 'bo-')\n",
    "plt.xlabel('Principal Component')\n",
    "plt.ylabel('Explained Variance Ratio')\n",
    "plt.title('Scree Plot')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, 'ro-')\n",
    "plt.axhline(y=0.95, color='k', linestyle='--', label='95% threshold')\n",
    "plt.xlabel('Number of Components')\n",
    "plt.ylabel('Cumulative Explained Variance')\n",
    "plt.title('Cumulative Variance')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Transform with optimal components\n",
    "pca_optimal = PCA(n_components=n_components)\n",
    "X_pca_optimal = pca_optimal.fit_transform(X_scaled)\n",
    "\n",
    "# Interpret components (what do they represent?)\n",
    "components_df = pd.DataFrame(\n",
    "    pca_optimal.components_,\n",
    "    columns=X.columns,\n",
    "    index=[f'PC{i+1}' for i in range(n_components)]\n",
    ")\n",
    "print(\"Top features in PC1:\")\n",
    "print(components_df.loc['PC1'].abs().sort_values(ascending=False).head(10))\n",
    "```\n",
    "\n",
    "### 5.5.3 t-SNE and UMAP for Visualization\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "import umap\n",
    "\n",
    "# t-SNE (slow, for visualization only)\n",
    "tsne = TSNE(n_components=2, random_state=42, perplexity=30)\n",
    "X_tsne = tsne.fit_transform(X_scaled)\n",
    "\n",
    "# UMAP (faster, often better)\n",
    "reducer = umap.UMAP(n_components=2, random_state=42)\n",
    "X_umap = reducer.fit_transform(X_scaled)\n",
    "\n",
    "# Visualize\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "axes[0].scatter(X_tsne[:, 0], X_tsne[:, 1], c=y, cmap='viridis', s=10, alpha=0.7)\n",
    "axes[0].set_title('t-SNE Visualization')\n",
    "axes[0].set_xlabel('t-SNE 1')\n",
    "axes[0].set_ylabel('t-SNE 2')\n",
    "\n",
    "axes[1].scatter(X_umap[:, 0], X_umap[:, 1], c=y, cmap='viridis', s=10, alpha=0.7)\n",
    "axes[1].set_title('UMAP Visualization')\n",
    "axes[1].set_xlabel('UMAP 1')\n",
    "axes[1].set_ylabel('UMAP 2')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### 5.5.4 Autoencoders for Non-linear Reduction\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim)\n",
    "        )\n",
    "        \n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, input_dim)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "    \n",
    "    def encode(self, x):\n",
    "        return self.encoder(x)\n",
    "\n",
    "# Training (simplified)\n",
    "autoencoder = Autoencoder(input_dim=X.shape[1], encoding_dim=32)\n",
    "optimizer = optim.Adam(autoencoder.parameters())\n",
    "criterion = nn.MSELoss()\n",
    "\n",
    "# Convert to tensors\n",
    "X_tensor = torch.FloatTensor(X_scaled)\n",
    "\n",
    "for epoch in range(100):\n",
    "    reconstructed = autoencoder(X_tensor)\n",
    "    loss = criterion(reconstructed, X_tensor)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch {epoch}, Loss: {loss.item():.4f}')\n",
    "\n",
    "# Get reduced features\n",
    "with torch.no_grad():\n",
    "    X_encoded = autoencoder.encode(X_tensor).numpy()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.6 Building Production Preprocessing Pipelines\n",
    "\n",
    "### 5.6.1 Scikit-learn Pipelines\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Define feature types\n",
    "numeric_features = ['age', 'income', 'transaction_count']\n",
    "categorical_features = ['gender', 'city', 'education']\n",
    "binary_features = ['is_active', 'has_children']\n",
    "\n",
    "# Numeric pipeline\n",
    "numeric_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Categorical pipeline\n",
    "categorical_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore', sparse_output=False))\n",
    "])\n",
    "\n",
    "# Binary pipeline (just impute)\n",
    "binary_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent'))\n",
    "])\n",
    "\n",
    "# Combine all\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', numeric_transformer, numeric_features),\n",
    "        ('cat', categorical_transformer, categorical_features),\n",
    "        ('bin', binary_transformer, binary_features)\n",
    "    ])\n",
    "\n",
    "# Full pipeline\n",
    "pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))\n",
    "])\n",
    "\n",
    "# Train\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "predictions = pipeline.predict(X_test)\n",
    "\n",
    "# Save pipeline\n",
    "import joblib\n",
    "joblib.dump(pipeline, 'model_pipeline.pkl')\n",
    "\n",
    "# Load and use\n",
    "loaded_pipeline = joblib.load('model_pipeline.pkl')\n",
    "new_predictions = loaded_pipeline.predict(new_data)\n",
    "```\n",
    "\n",
    "### 5.6.2 Custom Transformers\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class DateFeatureExtractor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"Extract features from datetime column\"\"\"\n",
    "    \n",
    "    def __init__(self, date_column):\n",
    "        self.date_column = date_column\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = X.copy()\n",
    "        dates = pd.to_datetime(X[self.date_column])\n",
    "        \n",
    "        X['hour'] = dates.dt.hour\n",
    "        X['dayofweek'] = dates.dt.dayofweek\n",
    "        X['month'] = dates.dt.month\n",
    "        X['is_weekend'] = (dates.dt.dayofweek >= 5).astype(int)\n",
    "        \n",
    "        # Cyclical encoding\n",
    "        X['hour_sin'] = np.sin(2 * np.pi * X['hour'] / 24)\n",
    "        X['hour_cos'] = np.cos(2 * np.pi * X['hour'] / 24)\n",
    "        \n",
    "        # Drop original if desired\n",
    "        X = X.drop(columns=[self.date_column])\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Usage in pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('date_features', DateFeatureExtractor('transaction_date')),\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('classifier', RandomForestClassifier())\n",
    "])\n",
    "```\n",
    "\n",
    "### 5.6.3 Handling Inference vs Training Consistency\n",
    "\n",
    "```python\n",
    "# Critical: Save all fitted transformers\n",
    "import pickle\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class SafePreprocessor:\n",
    "    def __init__(self):\n",
    "        self.pipeline = None\n",
    "        self.feature_names = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        # Build pipeline with fitted statistics\n",
    "        self.pipeline = Pipeline([\n",
    "            ('imputer', SimpleImputer(strategy='median')),\n",
    "            ('scaler', StandardScaler())\n",
    "        ])\n",
    "        self.pipeline.fit(X)\n",
    "        \n",
    "        # Save feature names for validation\n",
    "        self.feature_names = X.columns.tolist()\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        # Validate input has same columns\n",
    "        missing_cols = set(self.feature_names) - set(X.columns)\n",
    "        if missing_cols:\n",
    "            raise ValueError(f\"Missing columns: {missing_cols}\")\n",
    "        \n",
    "        # Ensure column order matches training\n",
    "        X = X[self.feature_names]\n",
    "        \n",
    "        return self.pipeline.transform(X)\n",
    "    \n",
    "    def save(self, path):\n",
    "        with open(path, 'wb') as f:\n",
    "            pickle.dump({'pipeline': self.pipeline, \n",
    "                        'feature_names': self.feature_names}, f)\n",
    "    \n",
    "    def load(self, path):\n",
    "        with open(path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "            self.pipeline = data['pipeline']\n",
    "            self.feature_names = data['feature_names']\n",
    "        return self\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.7 Workbook Labs\n",
    "\n",
    "### Lab 1: Rescue the Titanic Dataset\n",
    "The Titanic dataset is famously messy. Your job: clean it for modeling.\n",
    "\n",
    "**Tasks:**\n",
    "1. Handle missing values in Age, Cabin, Embarked\n",
    "2. Extract titles from Name (Mr., Mrs., etc.) and create feature\n",
    "3. Create family size feature from SibSp + Parch + 1\n",
    "4. Engineer \"is_alone\" feature\n",
    "5. Create age groups (child, adult, elderly)\n",
    "6. Build a preprocessing pipeline and compare model performance before/after\n",
    "\n",
    "**Deliverable:** Jupyter notebook with EDA, preprocessing steps, and model comparison.\n",
    "\n",
    "### Lab 2: Time Series Feature Engineering for Retail Sales\n",
    "Given daily sales data for multiple stores:\n",
    "\n",
    "**Tasks:**\n",
    "1. Create lag features (1, 7, 30 days)\n",
    "2. Create rolling statistics (7-day mean, 30-day std)\n",
    "3. Add calendar features (day of week, month, holiday indicator)\n",
    "4. Create price elasticity features (sales/price ratio)\n",
    "5. Handle outliers in sales (spikes during promotions vs genuine anomalies)\n",
    "6. Build pipeline that handles temporal ordering correctly (no future leakage)\n",
    "\n",
    "**Deliverable:** Python script with functions for each feature type, plus validation that features improve forecast accuracy.\n",
    "\n",
    "### Lab 3: High-Cardinality Categorical Encoding\n",
    "Dataset with 10,000+ product categories and 1M users.\n",
    "\n",
    "**Tasks:**\n",
    "1. Compare one-hot (impossible), frequency encoding, target encoding, and embeddings\n",
    "2. Implement safe target encoding with 5-fold cross-validation\n",
    "3. Measure impact on model performance vs baseline\n",
    "4. Create embedding layer in PyTorch and compare to traditional methods\n",
    "\n",
    "**Deliverable:** Performance comparison table, code for each method, recommendation for production.\n",
    "\n",
    "### Lab 4: Production Pipeline with Validation\n",
    "Build a complete preprocessing pipeline that:\n",
    "\n",
    "**Tasks:**\n",
    "1. Reads raw CSV\n",
    "2. Performs cleaning (missing values, outliers)\n",
    "3. Engineers features (temporal, interactions, aggregates)\n",
    "4. Scales appropriately\n",
    "5. Saves fitted preprocessor to disk\n",
    "6. Includes validation script that checks new data against training distribution\n",
    "7. Handles errors gracefully (missing columns, new categories)\n",
    "\n",
    "**Deliverable:** `preprocess.py` with classes, `test_preprocess.py` with unit tests, and example usage.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.8 Common Pitfalls\n",
    "\n",
    "1. **Data Leakage in Feature Engineering:**\n",
    "   - Using future information (e.g., computing customer's total spend before prediction date)\n",
    "   - Using target to create features (e.g., mean target encoding without CV)\n",
    "   - Scaling before train/test split\n",
    "\n",
    "2. **Assuming Missing is Random:**\n",
    "   - Always investigate *why* data is missing\n",
    "   - Consider adding \"missing indicator\" for important features\n",
    "\n",
    "3. **One-Hot Encoding High Cardinality:**\n",
    "   - Creates thousands of sparse columns\n",
    "   - Memory explosion, slow training\n",
    "   - Solution: frequency encoding, embeddings, or grouping\n",
    "\n",
    "4. **Ignoring Domain Constraints:**\n",
    "   - Negative predictions for inherently positive values (prices, counts)\n",
    "   - Probabilities outside [0,1] without proper transformation\n",
    "   - Always clip predictions to valid ranges post-model\n",
    "\n",
    "5. **Inconsistent Transformations:**\n",
    "   - Different preprocessing between training and inference\n",
    "   - Forgetting to save fitted scalers/encoders\n",
    "   - Solution: Always use pipelines and save entire fitted pipeline\n",
    "\n",
    "6. **Outlier Removal Without Investigation:**\n",
    "   - \"Outliers\" might be the most interesting cases (fraud, rare diseases)\n",
    "   - Consider separate modeling for outliers or treat as separate class\n",
    "\n",
    "---\n",
    "\n",
    "## 5.9 Interview Questions\n",
    "\n",
    "**Q1:** How do you handle missing values in a production system where new data arrives with different missing patterns?\n",
    "\n",
    "*A: I'd use a pipeline with saved imputation parameters. For new missing patterns, I'd log alerts for data drift, have fallback strategies (global median if category missing), and regularly retrain the imputer on recent data. Critical: never impute with statistics from future data.*\n",
    "\n",
    "**Q2:** Explain the difference between normalization and standardization. When would you use each?\n",
    "\n",
    "*A: Standardization (z-score) centers data to mean 0, std 1. Good for normally distributed data and algorithms assuming Gaussian. Normalization (min-max) scales to [0,1]. Good for bounded data, neural nets with sigmoid outputs, and algorithms requiring same scale (SVM, KNN). Robust scaler better with outliers.*\n",
    "\n",
    "**Q3:** Your categorical feature has 10,000 unique values. How do you encode it?\n",
    "\n",
    "*A: One-hot is impossible (10k columns). I'd try: (1) Frequency encoding (count or ratio), (2) Target encoding with proper CV to avoid leakage, (3) Embedding layer in neural net, (4) Hash encoding for memory efficiency, (5) Group rare categories as \"other\" then one-hot. Choice depends on model type and data size.*\n",
    "\n",
    "**Q4:** What is the curse of dimensionality and how do you combat it?\n",
    "\n",
    "*A: As dimensions increase, data becomes sparse, distances become less meaningful, and models overfit. Combat with: (1) Feature selection (filter/wrapper/embedded), (2) Dimensionality reduction (PCA/t-SNE/UMAP), (3) Regularization, (4) More data, (5) Domain knowledge to remove irrelevant features.*\n",
    "\n",
    "**Q5:** How do you detect and handle data drift in preprocessing?\n",
    "\n",
    "*A: Monitor distributions of input features vs training: (1) Statistical tests (KS-test, chi-square) for each feature, (2) Population Stability Index (PSI), (3) Feature importance drift. If drift detected: alert, investigate root cause, potentially retrain model on recent data with updated preprocessing statistics.*\n",
    "\n",
    "**Q6:** You notice that after one-hot encoding, your model performance drops. Why?\n",
    "\n",
    "*A: Possible reasons: (1) Too many sparse features causing overfitting, (2) Loss of ordinal information if categories had natural order, (3) Curse of dimensionality, (4) Insufficient data for rare categories. Solution: try different encoding (frequency, target) or add regularization.*\n",
    "\n",
    "---\n",
    "\n",
    "## 5.10 Further Reading\n",
    "\n",
    "**Classic Papers:**\n",
    "- *A Comparative Study of Categorical Variable Encoding Techniques* (Pargent et al., 2022)\n",
    "- *Random Search for Hyper-Parameter Optimization* (Bergstra & Bengio, 2012) - feature selection context\n",
    "- *Visualizing Data using t-SNE* (Van der Maaten & Hinton, 2008)\n",
    "\n",
    "**Books:**\n",
    "- *Feature Engineering for Machine Learning* (Alice Zheng, O'Reilly)\n",
    "- *The Kaggle Book* (Konrad Banachewicz, Luca Massaron) - practical feature engineering examples\n",
    "- *Python Feature Engineering Cookbook* (Soledad Galli)\n",
    "\n",
    "**Tools to Explore:**\n",
    "- **Feature-engine:** Library with advanced feature engineering transformers\n",
    "- **Category Encoders:** scikit-learn-contrib with many encoding methods\n",
    "- **Pandas Profiling / ydata-profiling:** Automated EDA report generation\n",
    "- **Great Expectations:** Data validation and testing\n",
    "\n",
    "---\n",
    "\n",
    "## 5.11 Checkpoint Project: End-to-End Data Pipeline for Customer Churn\n",
    "\n",
    "Build a complete, production-ready data preprocessing system for a telecom customer churn dataset.\n",
    "\n",
    "### Dataset\n",
    "Use a publicly available churn dataset (e.g., Telco Customer Churn from Kaggle) or simulate one with:\n",
    "- Customer demographics (age, gender, income)\n",
    "- Account information (tenure, contract type, payment method)\n",
    "- Service usage (monthly charges, total charges, number of calls)\n",
    "- Support interactions (tickets, complaints)\n",
    "- Target: churned (yes/no)\n",
    "\n",
    "### Requirements\n",
    "\n",
    "**Phase 1: EDA and Cleaning**\n",
    "- Profile data with automated and manual EDA\n",
    "- Document missing data patterns and handling strategy\n",
    "- Identify and decide on outliers\n",
    "- Create data quality report with issues found and fixes applied\n",
    "\n",
    "**Phase 2: Feature Engineering**\n",
    "- Create at least 5 new features from existing data\n",
    "- Document rationale for each (why it should predict churn)\n",
    "- Handle temporal aspects correctly (no future leakage)\n",
    "- Create interaction features between demographics and usage\n",
    "\n",
    "**Phase 3: Encoding and Scaling**\n",
    "- Apply appropriate encoding to all categorical features\n",
    "- Scale numerical features appropriately\n",
    "- Compare at least 3 encoding methods for high-cardinality features\n",
    "- Justify your final choices\n",
    "\n",
    "**Phase 4: Dimensionality Reduction**\n",
    "- Apply feature selection to reduce features by 30% while maintaining performance\n",
    "- Try PCA and compare interpretability vs performance\n",
    "- Document which features were kept and why\n",
    "\n",
    "**Phase 5: Production Pipeline**\n",
    "- Build a complete sklearn pipeline with all preprocessing steps\n",
    "- Include custom transformers for your engineered features\n",
    "- Add validation to check new data matches training schema\n",
    "- Save fitted pipeline to disk\n",
    "- Create inference script that loads pipeline and scores new customers\n",
    "\n",
    "**Phase 6: Monitoring Setup**\n",
    "- Define data drift metrics for each feature\n",
    "- Create a simple dashboard (even static HTML) showing training vs current distributions\n",
    "- Implement alerting if drift exceeds thresholds\n",
    "\n",
    "### Deliverables\n",
    "\n",
    "1. **Code Repository:**\n",
    "   - `eda.py` - EDA and visualization\n",
    "   - `features.py` - feature engineering functions\n",
    "   - `pipeline.py` - complete preprocessing pipeline\n",
    "   - `validate.py` - data validation checks\n",
    "   - `inference.py` - scoring new data\n",
    "   - `tests/` - unit tests for critical functions\n",
    "\n",
    "2. **Documentation:**\n",
    "   - `FEATURE_CATALOG.md` - description of each feature\n",
    "   - `PIPELINE.md` - how to use the pipeline\n",
    "   - `DATA_DICTIONARY.md` - expected schema\n",
    "\n",
    "3. **Visualization:**\n",
    "   - Before/after distributions for key features\n",
    "   - Feature importance plot\n",
    "   - Drift detection dashboard (static or interactive)\n",
    "\n",
    "4. **Model Comparison:**\n",
    "   - Train 3 models (logistic regression, random forest, XGBoost) using your pipeline\n",
    "   - Compare performance with baseline (minimal preprocessing)\n",
    "   - Show that your feature engineering improves performance\n",
    "\n",
    "### Success Criteria\n",
    "- Pipeline runs end-to-end in <5 minutes on a laptop\n",
    "- All transformations are reversible/documented\n",
    "- New unseen data can be scored without errors\n",
    "- Code includes error handling for edge cases\n",
    "- Clear demonstration that engineered features improve model performance (AUC improvement >0.05)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 5**\n",
    "\n",
    "*You've now mastered the art and science of turning raw data into model-ready features. Chapter 6 begins Supervised Learning with Regression — where you'll finally train models on the beautiful, clean data you've prepared.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "705fec70",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../1. Foundations/4. development_enironment_and_tools.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='6. supervised_learning_regression.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
