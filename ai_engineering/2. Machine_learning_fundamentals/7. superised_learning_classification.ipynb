{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a139e7b",
   "metadata": {},
   "source": [
    "Here is **Chapter 7: Supervised Learning - Classification** \u2014 predicting discrete categories with confidence.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 7: SUPERVISED LEARNING - CLASSIFICATION**\n",
    "\n",
    "*Decisions and Boundaries*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Classification is the workhorse of industry ML: spam detection, fraud prevention, medical diagnosis, and image recognition. Unlike regression, we care about decision boundaries, probability calibration, and the severe consequences of false negatives versus false positives. This chapter covers everything from probabilistic foundations to handling severe class imbalance.\n",
    "\n",
    "**Estimated Time:** 50-60 hours (3-4 weeks)  \n",
    "**Prerequisites:** Chapters 1-6 (Math, Python, Preprocessing, Regression foundations)\n",
    "\n",
    "---\n",
    "\n",
    "## **7.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement logistic regression from scratch using maximum likelihood estimation\n",
    "2. Select appropriate evaluation metrics beyond accuracy (F1, AUC-ROC, AUC-PR, Matthews Correlation Coefficient)\n",
    "3. Handle class imbalance using resampling, cost-sensitive learning, and threshold optimization\n",
    "4. Apply multiclass strategies (One-vs-Rest, One-vs-One, Softmax) for multi-category problems\n",
    "5. Calibrate probability estimates to ensure they reflect true confidence levels\n",
    "6. Build complete classification pipelines for imbalanced and cost-sensitive domains\n",
    "\n",
    "---\n",
    "\n",
    "## **7.1 Logistic Regression: The Probabilistic Foundation**\n",
    "\n",
    "#### **7.1.1 From Linear to Logistic**\n",
    "\n",
    "We want probabilities $P(y=1|\\mathbf{x}) \\in [0,1]$, but linear regression outputs $\\in (-\\infty, \\infty)$. Use the sigmoid (logistic) function:\n",
    "\n",
    "$$\\sigma(z) = \\frac{1}{1 + e^{-z}}$$\n",
    "\n",
    "The model:\n",
    "$$P(y=1|\\mathbf{x}) = \\sigma(\\mathbf{w}^T\\mathbf{x} + b) = \\frac{1}{1 + e^{-(\\mathbf{w}^T\\mathbf{x} + b)}}$$\n",
    "\n",
    "**Decision Boundary:** Where $P(y=1|\\mathbf{x}) = 0.5$, i.e., $\\mathbf{w}^T\\mathbf{x} + b = 0$ (linear boundary).\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "def sigmoid(z):\n",
    "    # Clip for numerical stability\n",
    "    return np.clip(1 / (1 + np.exp(-z)), 1e-7, 1 - 1e-7)\n",
    "\n",
    "class LogisticRegression:\n",
    "    def __init__(self, lr=0.01, n_iter=1000):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.zeros(n_features)\n",
    "        self.bias = 0\n",
    "        \n",
    "        for _ in range(self.n_iter):\n",
    "            # Linear model\n",
    "            linear = np.dot(X, self.weights) + self.bias\n",
    "            y_pred = sigmoid(linear)\n",
    "            \n",
    "            # Gradients (derived from cross-entropy loss)\n",
    "            dw = (1/n_samples) * np.dot(X.T, (y_pred - y))\n",
    "            db = (1/n_samples) * np.sum(y_pred - y)\n",
    "            \n",
    "            # Update\n",
    "            self.weights -= self.lr * dw\n",
    "            self.bias -= self.lr * db\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        linear = np.dot(X, self.weights) + self.bias\n",
    "        return sigmoid(linear)\n",
    "    \n",
    "    def predict(self, X, threshold=0.5):\n",
    "        return (self.predict_proba(X) >= threshold).astype(int)\n",
    "```\n",
    "\n",
    "#### **7.1.2 Cross-Entropy Loss (Log Loss)**\n",
    "\n",
    "Maximum Likelihood Estimation: Maximize probability of observed data.\n",
    "\n",
    "$$\\mathcal{L}(\\mathbf{w}) = -\\frac{1}{n}\\sum_{i=1}^n \\left[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\right]$$\n",
    "\n",
    "**Why this loss?** Convex, differentiable, heavy penalty for confident wrong predictions.\n",
    "\n",
    "**Gradient:**\n",
    "$$\\nabla_{\\mathbf{w}} \\mathcal{L} = \\frac{1}{n}\\mathbf{X}^T(\\hat{\\mathbf{y}} - \\mathbf{y})$$\n",
    "\n",
    "(Same form as linear regression, but $\\hat{y}$ is now sigmoid-transformed.)\n",
    "\n",
    "#### **7.1.3 Multiclass: Softmax Regression**\n",
    "\n",
    "For $K$ classes, output vector $\\mathbf{z} \\in \\mathbb{R}^K$. Softmax converts to probabilities:\n",
    "\n",
    "$$P(y=k|\\mathbf{x}) = \\frac{e^{z_k}}{\\sum_{j=1}^K e^{z_j}}$$\n",
    "\n",
    "**Loss:** Categorical Cross-Entropy\n",
    "$$\\mathcal{L} = -\\frac{1}{n}\\sum_{i=1}^n \\sum_{k=1}^K \\mathbb{1}(y_i=k) \\log(\\hat{y}_{i,k})$$\n",
    "\n",
    "```python\n",
    "def softmax(z):\n",
    "    # Subtract max for numerical stability\n",
    "    exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
    "    return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7.2 Evaluation Metrics: Beyond Accuracy**\n",
    "\n",
    "**The Accuracy Paradox:** If 99% of emails are not spam, predicting \"not spam\" always gives 99% accuracy but is useless.\n",
    "\n",
    "#### **7.2.1 The Confusion Matrix**\n",
    "\n",
    "|                | Predicted Positive | Predicted Negative |\n",
    "|----------------|-------------------|-------------------|\n",
    "| **Actual Positive** | True Positive (TP)  | False Negative (FN) |\n",
    "| **Actual Negative** | False Positive (FP) | True Negative (TN)  |\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Class 0', 'Class 1'])\n",
    "disp.plot()\n",
    "```\n",
    "\n",
    "#### **7.2.2 Precision, Recall, and F1**\n",
    "\n",
    "**Precision (Positive Predictive Value):** Of predicted positives, how many are actual?\n",
    "$$\\text{Precision} = \\frac{TP}{TP + FP}$$\n",
    "\n",
    "**Recall (Sensitivity, True Positive Rate):** Of actual positives, how many did we catch?\n",
    "$$\\text{Recall} = \\frac{TP}{TP + FN}$$\n",
    "\n",
    "**F1-Score:** Harmonic mean (punishes extreme imbalances)\n",
    "$$F1 = 2 \\cdot \\frac{\\text{Precision} \\cdot \\text{Recall}}{\\text{Precision} + \\text{Recall}}$$\n",
    "\n",
    "**When to use what:**\n",
    "- **Precision-focused:** Spam detection (don't block important emails), search ranking\n",
    "- **Recall-focused:** Disease screening (don't miss sick patients), fraud detection\n",
    "- **F1:** Balanced need for both\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, classification_report\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=['Class 0', 'Class 1']))\n",
    "```\n",
    "\n",
    "#### **7.2.3 ROC Curve and AUC**\n",
    "\n",
    "**True Positive Rate (Recall):** $TPR = \\frac{TP}{TP + FN}$  \n",
    "**False Positive Rate:** $FPR = \\frac{FP}{FP + TN}$\n",
    "\n",
    "ROC Curve plots TPR vs FPR at various thresholds. AUC = Area Under Curve (1.0 = perfect, 0.5 = random).\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "y_scores = model.predict_proba(X_test)[:, 1]  # Probability of positive class\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_scores)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.legend()\n",
    "```\n",
    "\n",
    "**When AUC fails:** Severe class imbalance. Use Precision-Recall (PR) AUC instead.\n",
    "\n",
    "#### **7.2.4 Precision-Recall Curve and AUC-PR**\n",
    "\n",
    "Plots Precision vs Recall. More informative than ROC for imbalanced data (dominated by negatives).\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, _ = precision_recall_curve(y_test, y_scores)\n",
    "avg_precision = average_precision_score(y_test, y_scores)\n",
    "\n",
    "plt.plot(recall, precision, label=f'PR curve (AP = {avg_precision:.2f})')\n",
    "```\n",
    "\n",
    "**Baseline for PR AUC:** Not 0.5, but the proportion of positive class.\n",
    "\n",
    "#### **7.2.5 Matthews Correlation Coefficient (MCC)**\n",
    "\n",
    "The best single metric for imbalanced binary classification. Range $[-1, 1]$ (1 = perfect, 0 = random, -1 = inverse).\n",
    "\n",
    "$$MCC = \\frac{TP \\cdot TN - FP \\cdot FN}{\\sqrt{(TP+FP)(TP+FN)(TN+FP)(TN+FN)}}$$\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import matthews_corrcoef\n",
    "mcc = matthews_corrcoef(y_true, y_pred)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7.3 Handling Class Imbalance**\n",
    "\n",
    "#### **7.3.1 Resampling Techniques**\n",
    "\n",
    "**Random Undersampling:** Remove majority class samples. Fast but loses information.\n",
    "\n",
    "**Random Oversampling:** Duplicate minority samples. Prone to overfitting.\n",
    "\n",
    "**SMOTE (Synthetic Minority Over-sampling Technique):** Generate synthetic examples by interpolating between nearest neighbors.\n",
    "\n",
    "```python\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from imblearn.under_sampling import RandomUnderSampler\n",
    "from imblearn.pipeline import Pipeline as ImbPipeline\n",
    "\n",
    "# Combine SMOTE with undersampling for balance\n",
    "resampling = ImbPipeline([\n",
    "    ('over', SMOTE(sampling_strategy=0.5, k_neighbors=5)),  # Upsample to 50% minority\n",
    "    ('under', RandomUnderSampler(sampling_strategy=0.8))     # Downsample to 80% majority\n",
    "])\n",
    "\n",
    "X_res, y_res = resampling.fit_resample(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Advanced:** BorderlineSMOTE (focus on hard examples), ADASYN (adaptive synthetic sampling).\n",
    "\n",
    "#### **7.3.2 Class Weights**\n",
    "\n",
    "Penalize mistakes on minority class more heavily.\n",
    "\n",
    "$$\\mathcal{L} = -\\frac{1}{n}\\sum_{i=1}^n w_{y_i} \\left[y_i \\log(\\hat{y}_i) + (1-y_i)\\log(1-\\hat{y}_i)\\right]$$\n",
    "\n",
    "```python\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Automatic balancing\n",
    "model = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "# Manual weights\n",
    "class_weights = {0: 1, 1: 10}  # Penalize minority 10x more\n",
    "model = RandomForestClassifier(class_weight=class_weights)\n",
    "```\n",
    "\n",
    "#### **7.3.3 Threshold Tuning**\n",
    "\n",
    "Default threshold is 0.5, but optimal threshold depends on business costs.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "# Find threshold that maximizes F1\n",
    "thresholds = np.arange(0.1, 0.9, 0.05)\n",
    "f1_scores = [f1_score(y_test, (y_scores >= t).astype(int)) for t in thresholds]\n",
    "optimal_threshold = thresholds[np.argmax(f1_scores)]\n",
    "\n",
    "# Or use Youden's J statistic for ROC\n",
    "# J = Sensitivity + Specificity - 1 = TPR - FPR\n",
    "j_scores = tpr - fpr\n",
    "optimal_idx = np.argmax(j_scores)\n",
    "optimal_threshold = thresholds[optimal_idx]\n",
    "```\n",
    "\n",
    "**Cost-Sensitive Thresholding:**\n",
    "If false negatives cost 5x more than false positives:\n",
    "$$\\text{Threshold} = \\frac{\\text{Cost}_{FP}}{\\text{Cost}_{FP} + \\text{Cost}_{FN}} = \\frac{1}{1+5} = 0.167$$\n",
    "\n",
    "---\n",
    "\n",
    "## **7.4 Multiclass Classification Strategies**\n",
    "\n",
    "#### **7.4.1 One-vs-Rest (OvR / One-vs-All)**\n",
    "\n",
    "Train $K$ binary classifiers. Classify as the class with highest confidence score.\n",
    "\n",
    "- **Pros:** Simple, parallelizable, works with any binary classifier\n",
    "- **Cons:** Calibration issues (scores not comparable across classifiers), imbalanced binary problems\n",
    "\n",
    "```python\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "ovr = OneVsRestClassifier(SVC(probability=True))\n",
    "ovr.fit(X, y)  # y has classes 0, 1, 2, ...\n",
    "```\n",
    "\n",
    "#### **7.4.2 One-vs-One (OvO)**\n",
    "\n",
    "Train $\\binom{K}{2}$ classifiers (one per pair). Vote for final class.\n",
    "\n",
    "- **Pros:** Each classifier trains on balanced subset (good for SVM)\n",
    "- **Cons:** $O(K^2)$ classifiers (slow for many classes)\n",
    "\n",
    "```python\n",
    "from sklearn.multiclass import OneVsOneClassifier\n",
    "\n",
    "ovo = OneVsOneClassifier(SVC())\n",
    "ovo.fit(X, y)\n",
    "```\n",
    "\n",
    "#### **7.4.3 Multinomial (Softmax)**\n",
    "\n",
    "Direct multi-class probability distribution. Native to logistic regression, neural networks, and gradient boosting.\n",
    "\n",
    "- **Pros:** Probabilities sum to 1, calibrated, efficient\n",
    "- **Cons:** Requires classifier support (not all algorithms)\n",
    "\n",
    "```python\n",
    "# Logistic Regression with multinomial loss\n",
    "model = LogisticRegression(multi_class='multinomial', solver='lbfgs')\n",
    "model.fit(X, y)\n",
    "probs = model.predict_proba(X)  # Shape (n_samples, n_classes), sums to 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **7.5 Classification Algorithms**\n",
    "\n",
    "#### **7.5.1 Naive Bayes**\n",
    "\n",
    "Probabilistic classifier based on Bayes' theorem with \"naive\" independence assumption.\n",
    "\n",
    "$$P(y_k | \\mathbf{x}) \\propto P(y_k) \\prod_{i=1}^d P(x_i | y_k)$$\n",
    "\n",
    "**Variants:**\n",
    "- **GaussianNB:** Continuous features (assumes Gaussian distribution)\n",
    "- **MultinomialNB:** Discrete counts (text classification)\n",
    "- **ComplementNB:** Good for imbalanced text data\n",
    "\n",
    "```python\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "# Extremely fast, works well with high dimensions despite \"naive\" assumption\n",
    "model = GaussianNB()\n",
    "model.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**When to use:** Text classification (spam), very large feature spaces, baseline model, real-time prediction (extremely fast).\n",
    "\n",
    "#### **7.5.2 K-Nearest Neighbors (KNN)**\n",
    "\n",
    "Non-parametric: Classify based on majority vote of $k$ nearest training examples.\n",
    "\n",
    "**Distance Metrics:**\n",
    "- Euclidean: $\\sqrt{\\sum(x_i - y_i)^2}$\n",
    "- Manhattan: $\\sum|x_i - y_i|$ (robust to outliers)\n",
    "- Cosine: $1 - \\frac{\\mathbf{x} \\cdot \\mathbf{y}}{\\|\\mathbf{x}\\| \\|\\mathbf{y}\\|}$ (good for text/high dimensions)\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "# Weight by distance (closer neighbors count more)\n",
    "knn = KNeighborsClassifier(n_neighbors=5, weights='distance', metric='minkowski', p=2)\n",
    "knn.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**Complexity:** Training $O(1)$ (just stores data), Prediction $O(nd)$ (search all points). Use KD-Tree or Ball-Tree for faster search: $O(d \\log n)$.\n",
    "\n",
    "#### **7.5.3 Support Vector Machines (SVM)**\n",
    "\n",
    "Find hyperplane that maximizes margin between classes.\n",
    "\n",
    "**Kernel Trick:** Maps to high-dimensional space without explicit computation.\n",
    "\n",
    "- **Linear:** $\\mathbf{x} \\cdot \\mathbf{y}$ (fast, good for high dim)\n",
    "- **RBF:** $\\exp(-\\gamma \\|\\mathbf{x} - \\mathbf{y}\\|^2)$ (non-linear, universal approximator)\n",
    "- **Polynomial:** $(\\gamma \\mathbf{x} \\cdot \\mathbf{y} + r)^d$\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# C: regularization (inverse of regularization strength)\n",
    "# gamma: kernel coefficient ('scale' = 1/(n_features * X.var()))\n",
    "svm = SVC(kernel='rbf', C=1.0, gamma='scale', probability=True)  # probability=True enables predict_proba (slow)\n",
    "svm.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "**When to use:** High-dimensional data (text, genomics), small-to-medium datasets (scales poorly with $n$), when interpretability not required.\n",
    "\n",
    "#### **7.5.4 Tree-Based Methods**\n",
    "\n",
    "**Decision Tree:** Recursive partitioning to minimize impurity (Gini or Entropy).\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "# max_depth prevents overfitting\n",
    "# min_samples_leaf ensures leaves have enough samples (smooths decision boundary)\n",
    "tree = DecisionTreeClassifier(\n",
    "    criterion='gini',  # or 'entropy'\n",
    "    max_depth=5,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    class_weight='balanced'\n",
    ")\n",
    "```\n",
    "\n",
    "**Random Forest:** Bagging of trees. Reduces variance.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_leaf=5,\n",
    "    max_features='sqrt',  # sqrt(n_features) considered at each split\n",
    "    class_weight='balanced_subsample',  # Balance in each bootstrap sample\n",
    "    n_jobs=-1\n",
    ")\n",
    "```\n",
    "\n",
    "**Gradient Boosting:** XGBoost/LightGBM/CatBoost (see Chapter 6 for regression, same API for classification with `objective='binary:logistic'` or `objective='multi:softprob'`).\n",
    "\n",
    "---\n",
    "\n",
    "## **7.6 Probability Calibration**\n",
    "\n",
    "Predicted probabilities should reflect true confidence. If model predicts 0.8 for 100 samples, ~80 should be positive.\n",
    "\n",
    "**Calibration Curves (Reliability Diagrams):**\n",
    "Plot predicted probability vs actual frequency.\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_scores, n_bins=10)\n",
    "\n",
    "plt.plot(prob_pred, prob_true, marker='o', label='Model')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--', label='Perfectly calibrated')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Fraction of Positives')\n",
    "```\n",
    "\n",
    "**Calibration Methods:**\n",
    "\n",
    "1. **Platt Scaling (Sigmoid Calibration):** Fit logistic regression on model outputs.\n",
    "2. **Isotonic Regression:** Non-parametric, monotonic calibration. Better if enough data.\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "\n",
    "# Wrap base estimator\n",
    "calibrated = CalibratedClassifierCV(\n",
    "    base_estimator=RandomForestClassifier(),\n",
    "    method='isotonic',  # or 'sigmoid'\n",
    "    cv=5\n",
    ")\n",
    "calibrated.fit(X_train, y_train)\n",
    "# Now predict_proba is calibrated\n",
    "```\n",
    "\n",
    "**When to calibrate:** When probabilities are used for decision-making (threshold selection, cost-sensitive learning, medical risk assessment).\n",
    "\n",
    "---\n",
    "\n",
    "## **7.7 Multilabel Classification**\n",
    "\n",
    "Each sample can belong to multiple classes simultaneously (e.g., tags on a blog post).\n",
    "\n",
    "**Strategies:**\n",
    "- **Binary Relevance:** Train independent binary classifier per label\n",
    "- **Classifier Chains:** Chain classifiers, using previous predictions as features\n",
    "- **Label Powerset:** Treat each label combination as single class (explodes for many labels)\n",
    "\n",
    "```python\n",
    "from sklearn.multioutput import MultiOutputClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Y is shape (n_samples, n_labels) with binary indicators\n",
    "multi_label_model = MultiOutputClassifier(RandomForestClassifier())\n",
    "multi_label_model.fit(X_train, Y_train)\n",
    "predictions = multi_label_model.predict(X_test)  # Shape (n_samples, n_labels)\n",
    "```\n",
    "\n",
    "**Metrics:**\n",
    "- **Hamming Loss:** Fraction of wrong labels (normalized by total labels)\n",
    "- **Subset Accuracy:** Exact match of all labels (very strict)\n",
    "- **Macro/Micro F1:** Average across labels\n",
    "\n",
    "---\n",
    "\n",
    "## **7.8 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Logistic Regression from Scratch**\n",
    "Implement binary logistic regression with:\n",
    "1. Batch gradient descent\n",
    "2. L2 regularization\n",
    "3. Learning rate decay\n",
    "4. Early stopping based on validation loss\n",
    "\n",
    "**Deliverable:** Match sklearn's `LogisticRegression` coefficients within 0.1% on test dataset.\n",
    "\n",
    "### **Lab 2: Imbalanced Classification Challenge**\n",
    "Credit card fraud detection (0.1% fraud rate):\n",
    "1. Compare baseline vs SMOTE vs Class weights vs Threshold tuning\n",
    "2. Plot Precision-Recall curves for all methods\n",
    "3. Calculate cost savings: Assume FN costs $500, FP costs $10, find optimal operating point\n",
    "\n",
    "**Deliverable:** Report showing money saved vs naive accuracy-based model.\n",
    "\n",
    "### **Lab 3: Multiclass Calibration**\n",
    "On 10-class classification (e.g., CIFAR-10 or MNIST):\n",
    "1. Train Random Forest and SVM\n",
    "2. Plot calibration curves per class\n",
    "3. Apply temperature scaling or Platt scaling\n",
    "4. Measure Expected Calibration Error (ECE) before/after\n",
    "\n",
    "**Deliverable:** Visualization showing calibration improvement and ECE numbers.\n",
    "\n",
    "### **Lab 4: Error Analysis**\n",
    "Build a text classifier (e.g., sentiment analysis):\n",
    "1. Identify false positives and false negatives\n",
    "2. Cluster errors by type (confused classes, specific keywords)\n",
    "3. Engineer features to fix top 3 error categories\n",
    "4. Show error rate reduction per category\n",
    "\n",
    "**Deliverable:** Error analysis report with before/after confusion matrices.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.9 Common Pitfalls**\n",
    "\n",
    "1. **Using Accuracy on Imbalanced Data:** Always check class distribution first. Use F1, MCC, or AUC-PR.\n",
    "\n",
    "2. **Data Leakage in SMOTE:** Never apply SMOTE before train/test split! Synthetic samples leak information.\n",
    "\n",
    "3. **Ignoring Decision Costs:** Not all errors are equal. Medical screening needs high recall even at cost of precision.\n",
    "\n",
    "4. **Threshold at 0.5 by Default:** 0.5 is arbitrary. Tune based on business metrics or ROC/PR analysis.\n",
    "\n",
    "5. **One-Hot Encoding Target for Binary Classification:** Don't create 2 columns for binary target (dummy variable trap). Use single column with 0/1.\n",
    "\n",
    "6. **Not Calibrating Probabilities:** Using `predict_proba` from SVM or Random Forest directly for risk scoring without calibration.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.10 Interview Questions**\n",
    "\n",
    "**Q1:** Why does logistic regression use sigmoid and cross-entropy instead of MSE?\n",
    "*A: MSE with sigmoid leads to non-convex loss (vanishing gradients when predictions are confident and wrong). Cross-entropy provides convex loss and stronger gradients for confident errors. Additionally, cross-entropy is the negative log-likelihood of Bernoulli distribution, making it the statistically principled choice.*\n",
    "\n",
    "**Q2:** Explain the difference between macro-average and micro-average F1.\n",
    "*A: Macro averages F1 per class then averages (treats all classes equally, good for balanced classes). Micro aggregates contributions globally (counts total TP, FP, FN then computes F1, good for imbalanced or when you care about total performance). Weighted macro accounts for class support.*\n",
    "\n",
    "**Q3:** When would you use PR AUC over ROC AUC?\n",
    "*A: ROC AUC can be optimistic on imbalanced datasets (dominated by TN rate). PR AUC focuses on positive class performance (precision vs recall) and is more informative when positives are rare. Use PR AUC when positive class is important and rare.*\n",
    "\n",
    "**Q4:** How do you handle a dataset with 99.9% negatives and 0.1% positives?\n",
    "*A: 1) Don't use accuracy. 2) Try class weights (inverse frequency). 3) Use appropriate sampling (SMOTEENN combination). 4) Use proper metrics (AUC-PR, F1, MCC). 5) Consider anomaly detection instead of classification. 6) Cost-sensitive learning if business costs known. 7) Use stratified sampling for train/test splits.*\n",
    "\n",
    "**Q5:** What's the difference between One-vs-Rest and Softmax for multiclass?\n",
    "*A: OvR trains K independent binary classifiers (each class vs all others). Scores aren't probabilities (don't sum to 1) and calibration varies. Softmax is a single model with joint probability distribution (mutually exclusive classes, probabilities sum to 1). Softmax is preferred when classes are mutually exclusive and model supports it; OvR works with any binary classifier.*\n",
    "\n",
    "---\n",
    "\n",
    "## **7.11 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Pattern Recognition and Machine Learning* (Bishop) - Bayesian classification, Gaussian processes\n",
    "- *Applied Predictive Modeling* (Kuhn & Johnson) - Comprehensive metrics and preprocessing\n",
    "\n",
    "**Papers:**\n",
    "- \"SMOTE: Synthetic Minority Over-sampling Technique\" (Chawla et al., 2002)\n",
    "- \"Predicting Good Probabilities with Supervised Learning\" (Niculescu-Mizil & Caruana, 2005) - Calibration\n",
    "\n",
    "**Imbalanced Learning:**\n",
    "- Imbalanced-learn documentation: https://imbalanced-learn.org/\n",
    "\n",
    "---\n",
    "\n",
    "## **7.12 Checkpoint Project: Fraud Detection System**\n",
    "\n",
    "Build a production-ready fraud detection classifier for financial transactions.\n",
    "\n",
    "**Dataset:** Credit card transactions (highly imbalanced, 0.1% fraud).\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - Time-based features (velocity: transactions per hour)\n",
    "   - Amount statistics (z-score relative to user history)\n",
    "   - Merchant category encoding (target encoding with smoothing)\n",
    "   - Geolocation features (distance from home, unusual locations)\n",
    "\n",
    "2. **Modeling Strategy:**\n",
    "   - Baseline: Logistic Regression with class weights\n",
    "   - Advanced: XGBoost with scale_pos_weight tuning\n",
    "   - Ensemble: Stacking of 3 models with different strengths\n",
    "\n",
    "3. **Threshold Optimization:**\n",
    "   - Business cost matrix: FN = $500 (missed fraud), FP = $10 (investigation cost)\n",
    "   - Find threshold minimizing total cost, not maximizing F1\n",
    "\n",
    "4. **Explainability:**\n",
    "   - SHAP values for top fraud predictions (investigators need reasons)\n",
    "   - Feature importance stability across time windows\n",
    "\n",
    "5. **Monitoring:**\n",
    "   - Concept drift detection (fraud patterns change)\n",
    "   - Weekly retraining pipeline simulation\n",
    "\n",
    "**Deliverables:**\n",
    "- `fraud_detector/` package with train, predict, and explain modules\n",
    "- API endpoint `/predict` returning {fraud_probability, explanation, threshold_recommendation}\n",
    "- Report: \"At optimal threshold, system saves $X per month vs current rule-based system\"\n",
    "\n",
    "**Success Criteria:**\n",
    "- Catch >80% of fraud (recall) with <5% false positive rate\n",
    "- Model inference time <50ms per transaction\n",
    "- Calibrated probabilities (Brier score <0.1)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 7**\n",
    "\n",
    "*You can now classify data with proper evaluation and handle real-world challenges like imbalance. Chapter 8 will cover Unsupervised Learning \u2014 finding patterns without labels.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='6. supervised_learning_regression.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='8. unsupervised_learning.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}