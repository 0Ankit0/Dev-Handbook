{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a013eb86",
   "metadata": {},
   "source": [
    "Here is **Chapter 6: Supervised Learning - Regression** — predicting continuous values with mathematical rigor.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 6: SUPERVISED LEARNING - REGRESSION**\n",
    "\n",
    "*Predicting the Continuous*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Regression is the foundation of predictive modeling. From forecasting stock prices to estimating house values, regression algorithms map input features to continuous outputs. This chapter progresses from the elegant simplicity of linear regression to the powerful complexity of gradient boosting, with mathematical derivations and production-ready implementations.\n",
    "\n",
    "**Estimated Time:** 50-60 hours (3-4 weeks)  \n",
    "**Prerequisites:** Chapters 1-5 (Math, Python, Data Preprocessing)\n",
    "\n",
    "---\n",
    "\n",
    "## **6.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Derive and implement linear regression using both closed-form (normal equations) and iterative (gradient descent) solutions\n",
    "2. Apply regularization techniques (Ridge, Lasso, Elastic Net) to prevent overfitting and perform feature selection\n",
    "3. Select appropriate evaluation metrics based on business requirements and error distributions\n",
    "4. Implement and tune advanced regression algorithms (SVR, Random Forest, XGBoost, LightGBM)\n",
    "5. Handle non-linear relationships through polynomial features and kernel methods\n",
    "6. Build regression pipelines for time series forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## **6.1 Linear Regression: The Foundation**\n",
    "\n",
    "#### **6.1.1 The Model**\n",
    "\n",
    "Given features $\\mathbf{X} \\in \\mathbb{R}^{n \\times d}$ and targets $\\mathbf{y} \\in \\mathbb{R}^n$, find weights $\\mathbf{w}$ and bias $b$ such that:\n",
    "\n",
    "$$\\hat{y} = \\mathbf{X}\\mathbf{w} + b$$\n",
    "\n",
    "Or in matrix form (absorbing bias into $\\mathbf{w}$ by adding column of 1s to $\\mathbf{X}$):\n",
    "\n",
    "$$\\hat{\\mathbf{y}} = \\mathbf{X}\\mathbf{w}$$\n",
    "\n",
    "**Objective:** Minimize Mean Squared Error (MSE):\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2n} \\sum_{i=1}^n (y_i - \\hat{y}_i)^2 = \\frac{1}{2n} \\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_2^2$$\n",
    "\n",
    "#### **6.1.2 Closed-Form Solution (Normal Equations)**\n",
    "\n",
    "Set gradient to zero:\n",
    "\n",
    "$$\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -\\frac{1}{n}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w}) = 0$$\n",
    "\n",
    "Solving:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class LinearRegressionClosedForm:\n",
    "    def fit(self, X, y):\n",
    "        # Add bias term\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        \n",
    "        # Normal equations: w = (X^T X)^-1 X^T y\n",
    "        self.w = np.linalg.inv(X_b.T @ X_b) @ X_b.T @ y\n",
    "        \n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.w\n",
    "\n",
    "# Usage\n",
    "model = LinearRegressionClosedForm()\n",
    "model.fit(X_train, y_train)\n",
    "predictions = model.predict(X_test)\n",
    "```\n",
    "\n",
    "**Computational Complexity:** $O(d^2n + d^3)$ due to matrix multiplication and inversion. Impractical for large $d$ (10,000+ features).\n",
    "\n",
    "#### **6.1.3 Gradient Descent Solution**\n",
    "\n",
    "When $d$ is large or data doesn't fit in memory, use iterative optimization.\n",
    "\n",
    "**Gradient:**\n",
    "\n",
    "$$\\nabla_{\\mathbf{w}} J(\\mathbf{w}) = -\\frac{1}{n}\\mathbf{X}^T(\\mathbf{y} - \\mathbf{X}\\mathbf{w})$$\n",
    "\n",
    "**Update Rule:**\n",
    "\n",
    "$$\\mathbf{w} := \\mathbf{w} - \\alpha \\nabla_{\\mathbf{w}} J(\\mathbf{w})$$\n",
    "\n",
    "```python\n",
    "class LinearRegressionGD:\n",
    "    def __init__(self, lr=0.01, n_iter=1000, tol=1e-6):\n",
    "        self.lr = lr\n",
    "        self.n_iter = n_iter\n",
    "        self.tol = tol\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.w = np.zeros(n_features + 1)  # +1 for bias\n",
    "        \n",
    "        X_b = np.c_[np.ones((n_samples, 1)), X]\n",
    "        \n",
    "        for i in range(self.n_iter):\n",
    "            # Predictions\n",
    "            y_pred = X_b @ self.w\n",
    "            \n",
    "            # Gradient\n",
    "            gradient = -(1/n_samples) * X_b.T @ (y - y_pred)\n",
    "            \n",
    "            # Update\n",
    "            self.w -= self.lr * gradient\n",
    "            \n",
    "            # Convergence check\n",
    "            if np.linalg.norm(gradient) < self.tol:\n",
    "                print(f\"Converged at iteration {i}\")\n",
    "                break\n",
    "                \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        X_b = np.c_[np.ones((X.shape[0], 1)), X]\n",
    "        return X_b @ self.w\n",
    "```\n",
    "\n",
    "**Variants:**\n",
    "- **Batch GD:** Uses all $n$ samples (accurate but slow)\n",
    "- **Stochastic GD (SGD):** Uses 1 sample at a time (fast but noisy)\n",
    "- **Mini-batch GD:** Uses $b$ samples (balance of speed and stability)\n",
    "\n",
    "#### **6.1.4 Polynomial Regression**\n",
    "\n",
    "Capture non-linear relationships by adding polynomial features:\n",
    "\n",
    "$$\\hat{y} = w_0 + w_1x + w_2x^2 + w_3x^3 + \\dots + w_dx^d$$\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Create pipeline\n",
    "model = Pipeline([\n",
    "    ('poly', PolynomialFeatures(degree=3, include_bias=False)),\n",
    "    ('linear', LinearRegression())\n",
    "])\n",
    "\n",
    "model.fit(X, y)\n",
    "```\n",
    "\n",
    "**Danger:** High-degree polynomials overfit badly. Always use regularization (Ridge/Lasso) with degree > 2.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.2 Regularization: Taming Complexity**\n",
    "\n",
    "#### **6.2.1 Ridge Regression (L2 Regularization)**\n",
    "\n",
    "Add penalty proportional to square of weights:\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_2^2 + \\lambda\\|\\mathbf{w}\\|_2^2$$\n",
    "\n",
    "Closed-form solution:\n",
    "\n",
    "$$\\mathbf{w} = (\\mathbf{X}^T\\mathbf{X} + \\lambda\\mathbf{I})^{-1}\\mathbf{X}^T\\mathbf{y}$$\n",
    "\n",
    "**Effect:** Shrinks weights toward zero but rarely exactly zero. Good for multicollinearity (correlated features).\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Find optimal alpha (lambda) via cross-validation\n",
    "params = {'alpha': [0.001, 0.01, 0.1, 1, 10, 100]}\n",
    "ridge = Ridge()\n",
    "grid = GridSearchCV(ridge, params, cv=5, scoring='neg_mean_squared_error')\n",
    "grid.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best alpha: {grid.best_params_['alpha']}\")\n",
    "```\n",
    "\n",
    "#### **6.2.2 Lasso Regression (L1 Regularization)**\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_2^2 + \\lambda\\|\\mathbf{w}\\|_1$$\n",
    "\n",
    "**Effect:** Drives some weights to exactly zero → **feature selection**.\n",
    "\n",
    "**Why L1 induces sparsity:** The diamond-shaped constraint region (L1 norm) intersects the loss contours at corners (axes), where some coordinates are zero.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(alpha=0.1)\n",
    "lasso.fit(X_train, y_train)\n",
    "\n",
    "# Selected features (non-zero coefficients)\n",
    "selected_features = X.columns[lasso.coef_ != 0]\n",
    "print(f\"Selected {len(selected_features)} out of {X.shape[1]} features\")\n",
    "```\n",
    "\n",
    "#### **6.2.3 Elastic Net**\n",
    "\n",
    "Combines L1 and L2:\n",
    "\n",
    "$$J(\\mathbf{w}) = \\frac{1}{2n}\\|\\mathbf{y} - \\mathbf{X}\\mathbf{w}\\|_2^2 + \\lambda_1\\|\\mathbf{w}\\|_1 + \\lambda_2\\|\\mathbf{w}\\|_2^2$$\n",
    "\n",
    "Best of both worlds: feature selection (L1) + handling correlated features (L2).\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "# l1_ratio: 0=Ridge, 1=Lasso, 0.5=equal mix\n",
    "elastic = ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "elastic.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "#### **6.2.4 Early Stopping**\n",
    "\n",
    "For gradient descent: Stop training when validation error stops decreasing.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "\n",
    "sgd = SGDRegressor(max_iter=1000, tol=1e-3, early_stopping=True, \n",
    "                   validation_fraction=0.1, n_iter_no_change=5)\n",
    "sgd.fit(X_train, y_train)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6.3 Evaluation Metrics: Measuring Error**\n",
    "\n",
    "Choosing the right metric determines what \"good\" means.\n",
    "\n",
    "#### **6.3.1 MSE and RMSE**\n",
    "\n",
    "**Mean Squared Error:**\n",
    "$$\\text{MSE} = \\frac{1}{n}\\sum_{i=1}^n (y_i - \\hat{y}_i)^2$$\n",
    "\n",
    "**Root Mean Squared Error:**\n",
    "$$\\text{RMSE} = \\sqrt{\\text{MSE}}$$\n",
    "\n",
    "- **Properties:** Differentiable (good for optimization), penalizes large errors heavily (quadratic), same units as target (RMSE)\n",
    "- **Use when:** Large errors are particularly bad, errors are Gaussian-distributed\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "mse = mean_squared_error(y_true, y_pred)\n",
    "rmse = np.sqrt(mse)\n",
    "```\n",
    "\n",
    "#### **6.3.2 MAE**\n",
    "\n",
    "**Mean Absolute Error:**\n",
    "$$\\text{MAE} = \\frac{1}{n}\\sum_{i=1}^n |y_i - \\hat{y}_i|$$\n",
    "\n",
    "- **Properties:** Robust to outliers, not differentiable at zero\n",
    "- **Use when:** Outliers are common and shouldn't dominate the metric\n",
    "\n",
    "#### **6.3.3 R² (Coefficient of Determination)**\n",
    "\n",
    "$$R^2 = 1 - \\frac{\\sum(y_i - \\hat{y}_i)^2}{\\sum(y_i - \\bar{y})^2}$$\n",
    "\n",
    "- **Interpretation:** Proportion of variance explained. Range $(-\\infty, 1]$, where 1 is perfect prediction, 0 is as good as predicting the mean.\n",
    "- **Warning:** Can be negative if model is worse than predicting the mean.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import r2_score\n",
    "r2 = r2_score(y_true, y_pred)\n",
    "```\n",
    "\n",
    "#### **6.3.4 MAPE and SMAPE**\n",
    "\n",
    "**Mean Absolute Percentage Error:**\n",
    "$$\\text{MAPE} = \\frac{100\\%}{n}\\sum_{i=1}^n \\left|\\frac{y_i - \\hat{y}_i}{y_i}\\right|$$\n",
    "\n",
    "- **Problem:** Undefined when $y_i = 0$, asymmetric (penalizes under-prediction more)\n",
    "\n",
    "**Symmetric MAPE:**\n",
    "$$\\text{SMAPE} = \\frac{100\\%}{n}\\sum_{i=1}^n \\frac{|y_i - \\hat{y}_i|}{(|y_i| + |\\hat{y}_i|)/2}$$\n",
    "\n",
    "#### **6.3.5 Custom Business Metrics**\n",
    "\n",
    "Often more important than statistical metrics:\n",
    "\n",
    "```python\n",
    "def revenue_weighted_mse(y_true, y_pred, revenue):\n",
    "    \"\"\"MSE weighted by customer revenue\"\"\"\n",
    "    return np.mean(revenue * (y_true - y_pred)**2)\n",
    "\n",
    "def quantile_loss(y_true, y_pred, quantile=0.9):\n",
    "    \"\"\"Pinball loss for quantile regression\"\"\"\n",
    "    errors = y_true - y_pred\n",
    "    return np.mean(np.maximum(quantile * errors, (quantile - 1) * errors))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6.4 Advanced Regression Algorithms**\n",
    "\n",
    "#### **6.4.1 Support Vector Regression (SVR)**\n",
    "\n",
    "Finds a function that deviates from actual targets by at most $\\epsilon$ (epsilon-insensitive tube) while being as flat as possible.\n",
    "\n",
    "**Kernel Trick:** Maps to high-dimensional space without computing coordinates.\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "# RBF kernel for non-linear relationships\n",
    "svr = SVR(kernel='rbf', C=100, epsilon=0.1, gamma='scale')\n",
    "svr.fit(X_train, y_train)\n",
    "\n",
    "# C: regularization (inverse of alpha in Ridge)\n",
    "# epsilon: width of tube where no penalty is applied\n",
    "```\n",
    "\n",
    "**When to use:** High-dimensional data, non-linear relationships, small-to-medium datasets (scales poorly with $n$).\n",
    "\n",
    "#### **6.4.2 Decision Tree Regression**\n",
    "\n",
    "Partitions feature space into regions, predicts mean of each region.\n",
    "\n",
    "**Splitting Criterion:** Minimize MSE (or MAE for robustness):\n",
    "\n",
    "$$\\text{MSE}_{\\text{node}} = \\frac{1}{n_{\\text{node}}}\\sum_{i \\in \\text{node}} (y_i - \\bar{y}_{\\text{node}})^2$$\n",
    "\n",
    "**Parameters to tune:**\n",
    "- `max_depth`: Prevent overfitting (shallow = underfit, deep = overfit)\n",
    "- `min_samples_leaf`: Minimum samples required at leaf node\n",
    "- `min_samples_split`: Minimum samples to allow a split\n",
    "\n",
    "```python\n",
    "from sklearn.tree import DecisionTreeRegressor, plot_tree\n",
    "\n",
    "tree = DecisionTreeRegressor(max_depth=5, min_samples_leaf=10)\n",
    "tree.fit(X_train, y_train)\n",
    "\n",
    "# Visualize\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(tree, feature_names=X.columns, filled=True, max_depth=3)\n",
    "```\n",
    "\n",
    "#### **6.4.3 Ensemble Methods**\n",
    "\n",
    "**Random Forest:** Bagging (Bootstrap Aggregating) of trees.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(\n",
    "    n_estimators=100,    # Number of trees\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    max_features='sqrt', # Number of features to consider at each split\n",
    "    n_jobs=-1,           # Use all CPU cores\n",
    "    random_state=42\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Feature importance (caution: biased toward high-cardinality features)\n",
    "importances = pd.Series(rf.feature_importances_, index=X.columns)\n",
    "importances.sort_values(ascending=False).plot(kind='barh')\n",
    "```\n",
    "\n",
    "**Gradient Boosting:** Sequential trees, each correcting errors of previous.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "gb = GradientBoostingRegressor(\n",
    "    n_estimators=100,\n",
    "    learning_rate=0.1,   # Shrinkage (prevent overfitting)\n",
    "    max_depth=3,         # Usually shallow for boosting\n",
    "    subsample=0.8,       # Stochastic gradient boosting\n",
    "    random_state=42\n",
    ")\n",
    "```\n",
    "\n",
    "#### **6.4.4 XGBoost / LightGBM / CatBoost**\n",
    "\n",
    "Industry standard for tabular regression.\n",
    "\n",
    "**XGBoost (Extreme Gradient Boosting):**\n",
    "- Regularization (L1/L2) built-in\n",
    "- Handles missing values automatically\n",
    "- Parallel processing\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix (optimized data structure)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dval = xgb.DMatrix(X_val, label=y_val)\n",
    "\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 6,\n",
    "    'eta': 0.1,                    # Learning rate\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'lambda': 1,                   # L2 regularization\n",
    "    'alpha': 0,                    # L1 regularization\n",
    "    'eval_metric': 'rmse'\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_boost_round=1000,\n",
    "    evals=[(dval, 'validation')],\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=100\n",
    ")\n",
    "```\n",
    "\n",
    "**LightGBM:** Faster training, leaf-wise tree growth (can overfit if not careful).\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "val_data = lgb.Dataset(X_val, label=y_val, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.05,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    num_boost_round=1000,\n",
    "    valid_sets=[val_data],\n",
    "    callbacks=[lgb.early_stopping(50), lgb.log_evaluation(100)]\n",
    ")\n",
    "```\n",
    "\n",
    "**CatBoost:** Best for categorical features (handles them natively without encoding).\n",
    "\n",
    "---\n",
    "\n",
    "## **6.5 Time Series Regression**\n",
    "\n",
    "Time series violates the i.i.d. assumption (independent and identically distributed). Use specialized techniques.\n",
    "\n",
    "#### **6.5.1 Feature Engineering for Time Series**\n",
    "\n",
    "```python\n",
    "# Lag features\n",
    "df['sales_lag_7'] = df['sales'].shift(7)  # Sales 7 days ago\n",
    "\n",
    "# Rolling statistics\n",
    "df['sales_ma_7'] = df['sales'].rolling(window=7).mean()  # Moving average\n",
    "df['sales_std_30'] = df['sales'].rolling(window=30).std()\n",
    "\n",
    "# Expanding window (cumulative statistics)\n",
    "df['sales_expanding_mean'] = df['sales'].expanding().mean()\n",
    "\n",
    "# Date features (cyclical encoding)\n",
    "df['day_of_week'] = df.index.dayofweek\n",
    "df['month'] = df.index.month\n",
    "df['day_of_week_sin'] = np.sin(2 * np.pi * df['day_of_week'] / 7)\n",
    "df['day_of_week_cos'] = np.cos(2 * np.pi * df['day_of_week'] / 7)\n",
    "```\n",
    "\n",
    "#### **6.5.2 Train-Test Split for Time Series**\n",
    "\n",
    "**Never use random split!** Use temporal split.\n",
    "\n",
    "```python\n",
    "# Sort by time first\n",
    "df = df.sort_index()\n",
    "\n",
    "# Split: 80% train, 20% test temporally\n",
    "split_idx = int(len(df) * 0.8)\n",
    "train = df.iloc[:split_idx]\n",
    "test = df.iloc[split_idx:]\n",
    "\n",
    "# Or use TimeSeriesSplit for cross-validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, val_idx in tscv.split(X):\n",
    "    X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    # Train and evaluate\n",
    "```\n",
    "\n",
    "#### **6.5.3 ARIMA (Brief Introduction)**\n",
    "\n",
    "AutoRegressive Integrated Moving Average. Classical statistical approach.\n",
    "\n",
    "- **AR(p):** Uses past $p$ values\n",
    "- **I(d):** Differencing to make stationary\n",
    "- **MA(q):** Uses past $q$ forecast errors\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Fit ARIMA(1,1,1)\n",
    "model = ARIMA(train['sales'], order=(1,1,1))\n",
    "fitted = model.fit()\n",
    "forecast = fitted.forecast(steps=30)\n",
    "```\n",
    "\n",
    "**Stationarity:** ARIMA requires stationary series (constant mean/variance). Check with Augmented Dickey-Fuller test.\n",
    "\n",
    "#### **6.5.4 Prophet (Facebook/Meta)**\n",
    "\n",
    "Automated forecasting with trend, seasonality, and holidays.\n",
    "\n",
    "```python\n",
    "from prophet import Prophet\n",
    "\n",
    "df_prophet = df.reset_index().rename(columns={'date': 'ds', 'sales': 'y'})\n",
    "\n",
    "model = Prophet(\n",
    "    yearly_seasonality=True,\n",
    "    weekly_seasonality=True,\n",
    "    daily_seasonality=False\n",
    ")\n",
    "model.add_country_holidays(country_name='US')\n",
    "model.fit(df_prophet)\n",
    "\n",
    "future = model.make_future_dataframe(periods=30)\n",
    "forecast = model.predict(future)\n",
    "model.plot(forecast)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **6.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: From Scratch Implementation**\n",
    "Implement Linear Regression, Ridge, and Lasso from scratch using only NumPy:\n",
    "1. Normal equations for Linear and Ridge\n",
    "2. Coordinate descent for Lasso (update one coordinate at a time)\n",
    "3. Compare your implementation's coefficients with sklearn\n",
    "\n",
    "**Deliverable:** `regression_from_scratch.py` with unit tests matching sklearn within 1e-6 tolerance.\n",
    "\n",
    "### **Lab 2: House Price Prediction Kaggle**\n",
    "Use the Ames Housing dataset:\n",
    "1. Comprehensive EDA and preprocessing (from Chapter 5)\n",
    "2. Compare Linear, Ridge, Lasso, Random Forest, XGBoost\n",
    "3. Hyperparameter tuning with RandomizedSearchCV\n",
    "4. Ensemble top 3 models (stacking)\n",
    "5. Submit to Kaggle and document leaderboard position\n",
    "\n",
    "**Deliverable:** Jupyter notebook with complete pipeline, achieving RMSE < 0.12 (log-transformed).\n",
    "\n",
    "### **Lab 3: Time Series Forecasting**\n",
    "Predict energy consumption:\n",
    "1. Create lag features, rolling statistics, calendar features\n",
    "2. Compare XGBoost vs Prophet vs ARIMA\n",
    "3. Implement walk-forward validation (expanding window)\n",
    "4. Calculate MAPE and directional accuracy (did you predict up/down correctly?)\n",
    "\n",
    "**Deliverable:** `time_series_forecast.py` with automated retraining schedule logic.\n",
    "\n",
    "### **Lab 4: Regularization Path Analysis**\n",
    "On a high-dimensional dataset (d > n):\n",
    "1. Fit Lasso with 100 different alpha values\n",
    "2. Plot the \"regularization path\" (coefficient vs alpha)\n",
    "3. Identify when each feature enters the model\n",
    "4. Compare with Elastic Net paths\n",
    "\n",
    "**Deliverable:** Visualization showing feature selection dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.7 Common Pitfalls**\n",
    "\n",
    "1. **Multicollinearity in Linear Regression:**\n",
    "   - Symptoms: High standard errors, unstable coefficients, high condition number\n",
    "   - Solutions: Ridge regression, PCA, drop correlated features, combine into indices\n",
    "\n",
    "2. **Extrapolation:**\n",
    "   - Linear models assume linearity holds outside training range. Tree-based models can't extrapolate beyond training data range.\n",
    "   - **Test:** Check if test set features are within training distribution (use KDE plots).\n",
    "\n",
    "3. **Log-transform Bias:**\n",
    "   - If you train on $\\log(y)$, predictions are biased when exponentiated.\n",
    "   - **Correction:** $\\hat{y} = \\exp(\\hat{\\log y} + \\frac{\\sigma^2}{2})$ (Duan's smearing estimator)\n",
    "\n",
    "4. **Data Leakage in Time Series:**\n",
    "   - Using future information (rolling mean including current point) to predict current point.\n",
    "   - **Solution:** Use `shift(1)` for lag features, `rolling().apply()` with careful window alignment.\n",
    "\n",
    "5. **Ignoring Heteroscedasticity:**\n",
    "   - Error variance changes with target magnitude (common in financial data).\n",
    "   - **Solution:** Weighted least squares, or transform target (log), or use quantile regression.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.8 Interview Questions**\n",
    "\n",
    "**Q1:** Why might you choose MAE over RMSE as a loss function?\n",
    "*A: MAE is more robust to outliers (doesn't square errors). RMSE penalizes large errors heavily, which might be dominated by a few outliers. MAE is also more interpretable (average dollar error vs squared dollar error). However, MAE is not differentiable at zero, making optimization harder.*\n",
    "\n",
    "**Q2:** Explain the bias-variance tradeoff in the context of Ridge vs Lasso regression.\n",
    "*A: High regularization (large lambda) increases bias (underfitting) but decreases variance (stable predictions). Ridge reduces variance by shrinking coefficients but keeps all features (higher bias than Lasso for irrelevant features). Lasso increases bias more aggressively by zeroing out features, potentially missing relevant ones (high bias) but reducing variance through sparsity.*\n",
    "\n",
    "**Q3:** Why does XGBoost often outperform Random Forest on tabular data?\n",
    "*A: XGBoost uses boosting (sequential error correction) and second-order gradients (Hessian), allowing it to learn complex patterns more efficiently. It also has built-in regularization (L1/L2) and handles missing values optimally. Random Forest uses bagging (parallel trees averaging), which reduces variance but doesn't correct bias as aggressively.*\n",
    "\n",
    "**Q4:** How do you handle categorical variables with high cardinality in linear regression?\n",
    "*A: One-hot encoding creates too many columns (curse of dimensionality). Better approaches: Target encoding (mean of target per category), embeddings (neural networks), or hashing trick. For linear models specifically, target encoding with smoothing/cross-validation to prevent overfitting is standard.*\n",
    "\n",
    "**Q5:** Your model has high $R^2$ on training but low on test. What regularization techniques do you try, in what order?\n",
    "*A: 1) Check for data leakage first (most common cause). 2) If legitimate overfitting: Add Ridge (L2) regression as baseline. 3) If feature selection needed, try Lasso (L1) or Elastic Net. 4) For trees, reduce max_depth, increase min_samples_leaf, or use early stopping (XGBoost/LightGBM). 5) Collect more data or reduce feature dimensionality if possible.*\n",
    "\n",
    "---\n",
    "\n",
    "## **6.9 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *The Elements of Statistical Learning* (Hastie et al.) - Chapters 3 (Linear), 7 (Model Selection), 9 (Additive Models), 10 (Boosting), 15 (Random Forests)\n",
    "- *Pattern Recognition and Machine Learning* (Bishop) - Bayesian linear regression\n",
    "- *Forecasting: Principles and Practice* (Hyndman & Athanasopoulos) - Time series focus\n",
    "\n",
    "**Papers:**\n",
    "- \"XGBoost: A Scalable Tree Boosting System\" (Chen & Guestrin, 2016)\n",
    "- \"LightGBM: A Highly Efficient Gradient Boosting Decision Tree\" (Ke et al., 2017)\n",
    "\n",
    "---\n",
    "\n",
    "## **6.10 Checkpoint Project: Automated Pricing Model**\n",
    "\n",
    "Build an end-to-end price prediction system for an e-commerce platform.\n",
    "\n",
    "**Dataset:** Product listings with features (category, brand, description, historical sales, competitor prices).\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Feature Engineering:**\n",
    "   - NLP features from description (TF-IDF, length, sentiment)\n",
    "   - Competitor price ratios and statistics\n",
    "   - Category target encoding with smoothing\n",
    "   - Time-based features (seasonality, trend)\n",
    "\n",
    "2. **Model Comparison:**\n",
    "   - Baseline: Mean price per category\n",
    "   - Linear: Ridge with polynomial features (degree 2)\n",
    "   - Tree: LightGBM with early stopping\n",
    "   - Ensemble: Weighted average of top models\n",
    "\n",
    "3. **Business Constraints:**\n",
    "   - Model must be interpretable (feature importance for pricing team)\n",
    "   - Prediction intervals (not just point estimates) - use Quantile Regression or bootstrapping\n",
    "   - Max 100ms inference time (optimize with ONNX or lightweight model)\n",
    "\n",
    "4. **Evaluation:**\n",
    "   - MAPE primary metric (business cares about % error)\n",
    "   - Bias analysis: Does model systematically underprice luxury items?\n",
    "   - A/B test simulation: Would using model prices increase revenue?\n",
    "\n",
    "**Deliverables:**\n",
    "- `pricing_model/` package with training and inference modules\n",
    "- FastAPI endpoint `/predict` with input validation (Pydantic)\n",
    "- Docker container with model artifact\n",
    "- Report: \"If we deployed this, expected revenue impact is X% with Y% risk\"\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 6**\n",
    "\n",
    "*You can now predict continuous values with statistical rigor. Chapter 7 will cover Classification, where we predict discrete categories - the most common ML task in industry.*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
