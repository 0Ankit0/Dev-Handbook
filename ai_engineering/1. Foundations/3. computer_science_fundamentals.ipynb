{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ce936c4b",
   "metadata": {},
   "source": [
    "Here is **Chapter 3: Computer Science Fundamentals** \u2014 the algorithmic foundation for scalable AI systems.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 3: COMPUTER SCIENCE FUNDAMENTALS**\n",
    "\n",
    "*The Engineer's Discipline*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Mathematics gives you models; Python gives you tools; Computer Science gives you scalability. This chapter covers the data structures and algorithms that separate prototype hackers from production engineers. Every concept is tied to real ML system challenges: from indexing billions of embeddings to optimizing autoregressive generation.\n",
    "\n",
    "**Estimated Time:** 40-50 hours (3 weeks)  \n",
    "**Prerequisites:** Chapters 1-2, basic programming logic\n",
    "\n",
    "---\n",
    "\n",
    "## **3.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Select optimal data structures for ML pipelines (O(1) feature lookups, O(log n) neighbor search)\n",
    "2. Analyze algorithmic complexity of training loops and inference pipelines\n",
    "3. Implement core algorithms from scratch (sorting, graph traversal, dynamic programming)\n",
    "4. Design object-oriented ML systems using SOLID principles and design patterns\n",
    "5. Write comprehensive test suites for stochastic ML code\n",
    "6. Optimize memory and compute using algorithmic insights\n",
    "\n",
    "---\n",
    "\n",
    "## **3.1 Data Structures for ML Systems**\n",
    "\n",
    "#### **3.1.1 Arrays and Dynamic Arrays (Lists)**\n",
    "\n",
    "**ML Applications:** Feature vectors, batches, model weights.\n",
    "\n",
    "**Time Complexity:**\n",
    "- Access: $O(1)$\n",
    "- Append (amortized): $O(1)$ \n",
    "- Insert/Delete at index: $O(n)$ (elements shift)\n",
    "\n",
    "**The Amortized Analysis:**\n",
    "Python lists over-allocate (growth factor ~1.125) to make append $O(1)$ amortized. Critical for online learning buffers.\n",
    "\n",
    "```python\n",
    "import sys\n",
    "\n",
    "# Demonstrate list overallocation\n",
    "lst = []\n",
    "for i in range(10):\n",
    "    print(f\"Length: {len(lst)}, Size in memory: {sys.getsizeof(lst)}\")\n",
    "    lst.append(i)\n",
    "    \n",
    "# Output shows size doubles at 4, 8, 16... geometric growth\n",
    "```\n",
    "\n",
    "**Memory Layout:**\n",
    "Contiguous memory = cache efficiency. NumPy arrays exploit this with SIMD. Linked lists (non-contiguous) kill cache performance\u2014avoid for numerical data.\n",
    "\n",
    "#### **3.1.2 Hash Tables (Dictionaries)**\n",
    "\n",
    "**ML Applications:** \n",
    "- Feature hashing (hashing trick for high-cardinality categorical features)\n",
    "- Vocabulary mappings (word \u2192 index)\n",
    "- Memoization in dynamic programming\n",
    "- Deduplication in data pipelines\n",
    "\n",
    "**Collision Resolution:**\n",
    "Python uses open addressing (not chaining). Load factor > 0.66 triggers resize.\n",
    "\n",
    "**Feature Hashing Example:**\n",
    "```python\n",
    "def hash_features(features: dict, n_buckets: int = 1000) -> dict:\n",
    "    \"\"\"Hashing trick for memory-efficient feature representation\"\"\"\n",
    "    hashed = {}\n",
    "    for key, value in features.items():\n",
    "        # Deterministic hash to bucket\n",
    "        bucket = hash(key) % n_buckets\n",
    "        hashed[bucket] = hashed.get(bucket, 0) + value\n",
    "    return hashed\n",
    "\n",
    "# Use case: 1M unique words \u2192 1000 dimensional vector\n",
    "vocab_features = {f\"word_{i}\": 1.0 for i in range(1000000)}\n",
    "compressed = hash_features(vocab_features, n_buckets=1000)\n",
    "print(f\"Compressed {len(vocab_features)} features to {len(compressed)}\")\n",
    "```\n",
    "\n",
    "**Complexity:** Average $O(1)$ lookup, worst $O(n)$ (all collisions). Use `collections.Counter` for frequency counts.\n",
    "\n",
    "#### **3.1.3 Linked Lists**\n",
    "\n",
    "**ML Applications:** \n",
    "- LRU Cache for model predictions (eviction policy)\n",
    "- Gradient accumulation buffers (unbounded growth)\n",
    "- Implementing custom autograd engines (computation graphs as linked structures)\n",
    "\n",
    "**When NOT to use:** Random access to training samples. Use arrays/tensors instead.\n",
    "\n",
    "**LRU Cache Implementation:**\n",
    "```python\n",
    "from collections import OrderedDict\n",
    "\n",
    "class LRUCache:\n",
    "    \"\"\"Cache for model predictions (e.g., embedding lookups)\"\"\"\n",
    "    def __init__(self, capacity: int):\n",
    "        self.cache = OrderedDict()\n",
    "        self.capacity = capacity\n",
    "    \n",
    "    def get(self, key):\n",
    "        if key not in self.cache:\n",
    "            return None\n",
    "        # Move to end (most recent)\n",
    "        self.cache.move_to_end(key)\n",
    "        return self.cache[key]\n",
    "    \n",
    "    def put(self, key, value):\n",
    "        if key in self.cache:\n",
    "            self.cache.move_to_end(key)\n",
    "        self.cache[key] = value\n",
    "        if len(self.cache) > self.capacity:\n",
    "            # Pop first (least recent)\n",
    "            self.cache.popitem(last=False)\n",
    "\n",
    "# Usage: Cache expensive embedding computations\n",
    "cache = LRUCache(capacity=10000)\n",
    "```\n",
    "\n",
    "#### **3.1.4 Trees**\n",
    "\n",
    "**Binary Search Trees (BST):** $O(\\log n)$ lookup if balanced. Rarely used directly in ML (unpredictable branch prediction hurts SIMD), but essential for understanding:\n",
    "\n",
    "**Decision Trees:** The fundamental ML algorithm. Each node is a binary split.\n",
    "- **Random Forests:** Ensembles of trees\n",
    "- **Gradient Boosting:** Sequential trees (XGBoost, LightGBM)\n",
    "\n",
    "**Implementation Insight:**\n",
    "```python\n",
    "class DecisionNode:\n",
    "    def __init__(self, feature_idx, threshold, left, right, info_gain):\n",
    "        self.feature_idx = feature_idx  # Which feature to split\n",
    "        self.threshold = threshold      # Split value\n",
    "        self.left = left                # Left subtree (<= threshold)\n",
    "        self.right = right              # Right subtree (> threshold)\n",
    "        self.info_gain = info_gain      # Information gain metric\n",
    "\n",
    "class TreeClassifier:\n",
    "    def predict(self, x, node):\n",
    "        \"\"\"Traverse tree for inference (O(tree depth))\"\"\"\n",
    "        if not isinstance(node, DecisionNode):\n",
    "            return node  # Leaf value\n",
    "        \n",
    "        if x[node.feature_idx] <= node.threshold:\n",
    "            return self.predict(x, node.left)\n",
    "        return self.predict(x, node.right)\n",
    "```\n",
    "\n",
    "**Segment Trees:** For range queries on time-series data (min/max/mean over intervals). Used in financial ML for rolling statistics.\n",
    "\n",
    "#### **3.1.5 Heaps (Priority Queues)**\n",
    "\n",
    "**ML Applications:**\n",
    "- **Beam Search:** In sequence generation (NLP), keep top-$k$ hypotheses\n",
    "- **Top-K Selection:** Finding largest gradients for importance sampling\n",
    "- **A* Search:** Pathfinding in RL environments\n",
    "\n",
    "**Properties:** \n",
    "- Min-heap: Parent < children. Root is minimum.\n",
    "- Operations: Insert $O(\\log n)$, Extract-min $O(\\log n)$, Peek $O(1)$\n",
    "\n",
    "**Beam Search Implementation:**\n",
    "```python\n",
    "import heapq\n",
    "\n",
    "def beam_search_decoder(predict_fn, start_token, beam_width=3, max_len=20):\n",
    "    \"\"\"Beam search for sequence generation\"\"\"\n",
    "    # Priority queue: (negative_score, sequence) \n",
    "    # (negative because heapq is min-heap, we want max probability)\n",
    "    beams = [(0.0, [start_token])]\n",
    "    \n",
    "    for _ in range(max_len):\n",
    "        candidates = []\n",
    "        for score, seq in beams:\n",
    "            if seq[-1] == \"<EOS>\":\n",
    "                candidates.append((score, seq))\n",
    "                continue\n",
    "            \n",
    "            # Get next token probabilities\n",
    "            probs = predict_fn(seq)  # Dict[token] = log_prob\n",
    "            \n",
    "            # Keep top beam_width extensions\n",
    "            for token, log_prob in probs.items():\n",
    "                new_score = score + log_prob\n",
    "                new_seq = seq + [token]\n",
    "                heapq.heappush(candidates, (new_score, new_seq))\n",
    "        \n",
    "        # Keep only beam_width best\n",
    "        beams = heapq.nlargest(beam_width, candidates)\n",
    "    \n",
    "    return beams[0][1]  # Best sequence\n",
    "```\n",
    "\n",
    "#### **3.1.6 Graphs**\n",
    "\n",
    "**ML Applications:**\n",
    "- **Graph Neural Networks (GNNs):** Social networks, molecules, knowledge graphs\n",
    "- **Nearest Neighbor Search:** K-d trees, Ball trees (scikit-learn)\n",
    "- **Bayesian Networks:** Probabilistic graphical models\n",
    "- **Flow Networks:** Max-flow min-cut for image segmentation\n",
    "\n",
    "**Representations:**\n",
    "1. **Adjacency Matrix:** $O(V^2)$ space. Good for dense graphs (GNNs use this with sparse matrices).\n",
    "2. **Adjacency List:** $O(V + E)$ space. Better for sparse graphs (social networks).\n",
    "\n",
    "```python\n",
    "class Graph:\n",
    "    def __init__(self, directed=False):\n",
    "        self.adj_list = defaultdict(list)\n",
    "        self.directed = directed\n",
    "    \n",
    "    def add_edge(self, u, v, weight=1):\n",
    "        self.adj_list[u].append((v, weight))\n",
    "        if not self.directed:\n",
    "            self.adj_list[v].append((u, weight))\n",
    "    \n",
    "    def bfs(self, start):\n",
    "        \"\"\"Breadth-first search (shortest path in unweighted graph)\"\"\"\n",
    "        visited = {start}\n",
    "        queue = deque([(start, 0)])  # (node, distance)\n",
    "        distances = {start: 0}\n",
    "        \n",
    "        while queue:\n",
    "            node, dist = queue.popleft()\n",
    "            for neighbor, _ in self.adj_list[node]:\n",
    "                if neighbor not in visited:\n",
    "                    visited.add(neighbor)\n",
    "                    distances[neighbor] = dist + 1\n",
    "                    queue.append((neighbor, dist + 1))\n",
    "        return distances\n",
    "    \n",
    "    def dfs(self, start, visited=None):\n",
    "        \"\"\"Depth-first search (for topological sort, cycle detection)\"\"\"\n",
    "        if visited is None:\n",
    "            visited = set()\n",
    "        visited.add(start)\n",
    "        print(start)  # Process node\n",
    "        \n",
    "        for neighbor, _ in self.adj_list[start]:\n",
    "            if neighbor not in visited:\n",
    "                self.dfs(neighbor, visited)\n",
    "        return visited\n",
    "```\n",
    "\n",
    "**Topological Sort:** For neural network computation graphs (forward/backward pass ordering).\n",
    "\n",
    "#### **3.1.7 Tries (Prefix Trees)**\n",
    "\n",
    "**ML Applications:**\n",
    "- **Autocomplete systems:** Language model tokenization\n",
    "- **String matching:** Bioinformatics (DNA sequence alignment)\n",
    "- **Efficient vocabulary storage:** Especially for BPE tokenizers\n",
    "\n",
    "```python\n",
    "class TrieNode:\n",
    "    def __init__(self):\n",
    "        self.children = {}\n",
    "        self.is_end = False\n",
    "        self.frequency = 0  # For predicting most likely completion\n",
    "\n",
    "class AutocompleteTrie:\n",
    "    def __init__(self):\n",
    "        self.root = TrieNode()\n",
    "    \n",
    "    def insert(self, word, freq=1):\n",
    "        node = self.root\n",
    "        for char in word:\n",
    "            if char not in node.children:\n",
    "                node.children[char] = TrieNode()\n",
    "            node = node.children[char]\n",
    "        node.is_end = True\n",
    "        node.frequency = freq\n",
    "    \n",
    "    def search(self, prefix):\n",
    "        \"\"\"Return all words with given prefix\"\"\"\n",
    "        node = self.root\n",
    "        for char in prefix:\n",
    "            if char not in node.children:\n",
    "                return []\n",
    "            node = node.children[char]\n",
    "        return self._collect(node, prefix)\n",
    "    \n",
    "    def _collect(self, node, prefix):\n",
    "        results = []\n",
    "        if node.is_end:\n",
    "            results.append((prefix, node.frequency))\n",
    "        for char, child in node.children.items():\n",
    "            results.extend(self._collect(child, prefix + char))\n",
    "        return results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3.2 Algorithms for ML Optimization**\n",
    "\n",
    "#### **3.2.1 Sorting and Selection**\n",
    "\n",
    "**ML Use Cases:**\n",
    "- Computing percentiles (median absolute deviation for outlier detection)\n",
    "- Ranking predictions (precision@k, NDCG metrics)\n",
    "- Finding nearest neighbors (sort by distance)\n",
    "\n",
    "**Key Algorithms:**\n",
    "- **Quicksort:** Average $O(n \\log n)$, worst $O(n^2)$. Used in `numpy.sort()` (introsort hybrid).\n",
    "- **Mergesort:** Stable $O(n \\log n)$. Used in `sorted()` (Timsort).\n",
    "- **Heapselect:** $O(n \\log k)$ for top-k (more efficient than full sort).\n",
    "\n",
    "**Partial Sorting for Top-K:**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Efficient top-k (faster than full sort)\n",
    "scores = np.random.randn(1000000)\n",
    "k = 100\n",
    "\n",
    "# Method 1: Full sort (O(n log n))\n",
    "top_k_slow = np.sort(scores)[-k:]\n",
    "\n",
    "# Method 2: Partition (O(n))\n",
    "# np.argpartition does introselect (quickselect)\n",
    "idx = np.argpartition(scores, -k)[-k:]\n",
    "top_k_fast = scores[idx]\n",
    "# Note: top_k_fast is not sorted; sort just these k if needed\n",
    "top_k_sorted = np.sort(top_k_fast)\n",
    "```\n",
    "\n",
    "**Quickselect for Median:** $O(n)$ average to find median without sorting.\n",
    "\n",
    "#### **3.2.2 Search Algorithms**\n",
    "\n",
    "**Binary Search:** $O(\\log n)$. Used in:\n",
    "- Hyperparameter search (monotonic validation curves)\n",
    "- Threshold tuning (finding optimal classification threshold)\n",
    "- Sparse matrix lookups\n",
    "\n",
    "```python\n",
    "def binary_search_threshold(scores, labels, target_precision=0.95):\n",
    "    \"\"\"Find threshold achieving target precision\"\"\"\n",
    "    low, high = 0.0, 1.0\n",
    "    best_thresh = 0.5\n",
    "    \n",
    "    for _ in range(20):  # 20 iterations = precision of 2^-20\n",
    "        mid = (low + high) / 2\n",
    "        preds = (scores >= mid).astype(int)\n",
    "        precision = np.mean(labels[preds == 1]) if np.sum(preds) > 0 else 0\n",
    "        \n",
    "        if precision >= target_precision:\n",
    "            best_thresh = mid\n",
    "            high = mid  # Try higher threshold (fewer positives, higher precision)\n",
    "        else:\n",
    "            low = mid   # Need lower threshold to get more positives\n",
    "    \n",
    "    return best_thresh\n",
    "```\n",
    "\n",
    "#### **3.2.3 Dynamic Programming (DP)**\n",
    "\n",
    "**ML Applications:**\n",
    "- **Sequence Alignment:** Needleman-Wunsch (bioinformatics)\n",
    "- **Edit Distance:** Levenshtein distance for fuzzy string matching\n",
    "- **Viterbi Algorithm:** Finding most likely state sequence in HMMs (CRFs, speech recognition)\n",
    "- **Knapsack Problem:** Feature selection with budget constraints\n",
    "- **Coin Change:** Optimal model ensemble selection (weighted majority)\n",
    "\n",
    "**Edit Distance Example:**\n",
    "```python\n",
    "def levenshtein_distance(s1, s2):\n",
    "    \"\"\"Minimum edits to transform s1 into s2\"\"\"\n",
    "    m, n = len(s1), len(s2)\n",
    "    # DP table: dp[i][j] = distance between s1[:i] and s2[:j]\n",
    "    dp = [[0] * (n + 1) for _ in range(m + 1)]\n",
    "    \n",
    "    # Base cases\n",
    "    for i in range(m + 1): dp[i][0] = i  # Delete all\n",
    "    for j in range(n + 1): dp[0][j] = j  # Insert all\n",
    "    \n",
    "    for i in range(1, m + 1):\n",
    "        for j in range(1, n + 1):\n",
    "            if s1[i-1] == s2[j-1]:\n",
    "                dp[i][j] = dp[i-1][j-1]  # No operation needed\n",
    "            else:\n",
    "                dp[i][j] = 1 + min(\n",
    "                    dp[i-1][j],     # Deletion\n",
    "                    dp[i][j-1],     # Insertion\n",
    "                    dp[i-1][j-1]    # Substitution\n",
    "                )\n",
    "    return dp[m][n]\n",
    "\n",
    "# Use case: Fuzzy matching of OCR errors to dictionary\n",
    "```\n",
    "\n",
    "**Space Optimization:** Notice we only need previous row, not full table. Reduce $O(mn)$ space to $O(n)$.\n",
    "\n",
    "#### **3.2.4 Greedy Algorithms**\n",
    "\n",
    "**When to use:** When local optimal choices lead to global optimum (e.g., matroids).\n",
    "\n",
    "**ML Applications:**\n",
    "- **Decision Tree Splitting:** Greedy information gain maximization\n",
    "- **Feature Selection:** Greedy forward selection (add feature with highest marginal gain)\n",
    "- **Set Cover:** Selecting minimal training set that covers all feature space regions\n",
    "\n",
    "**Greedy vs DP:**\n",
    "- Greedy: Faster, no guarantee of optimal (but often good enough)\n",
    "- DP: Optimal but expensive (exponential state space in worst case)\n",
    "\n",
    "---\n",
    "\n",
    "## **3.3 Complexity Analysis**\n",
    "\n",
    "#### **3.3.1 Big O Notation**\n",
    "\n",
    "**Definitions:**\n",
    "- $O(f(n))$: Upper bound (worst case)\n",
    "- $\\Omega(f(n))$: Lower bound (best case)  \n",
    "- $\\Theta(f(n))$: Tight bound (average case)\n",
    "\n",
    "**Common Complexities in ML:**\n",
    "\n",
    "| Operation | Complexity | Example |\n",
    "|-----------|-----------|---------|\n",
    "| Matrix Multiply | $O(n^3)$ (naive), $O(n^{2.37})$ (Strassen) | Transformer attention |\n",
    "| Matrix Inversion | $O(n^3)$ | Linear regression normal equations |\n",
    "| Sorting | $O(n \\log n)$ | Ranking predictions |\n",
    "| K-NN Search | $O(n)$ naive, $O(\\log n)$ with KD-tree | Nearest neighbors |\n",
    "| Gradient Descent | $O(iterations \\times data\\_size \\times params)$ | Neural network training |\n",
    "\n",
    "#### **3.3.2 Amortized Analysis**\n",
    "\n",
    "**Example:** Python list append is $O(1)$ amortized, even though occasional resizes are $O(n)$. Over $n$ appends, total cost is $O(n)$, so amortized $O(1)$ per operation.\n",
    "\n",
    "**ML Application:** Hash table resizing during feature counting. Batch inserts to amortize cost.\n",
    "\n",
    "#### **3.3.3 Space Complexity**\n",
    "\n",
    "**Critical for ML:**\n",
    "- **Model Parameters:** $O(d \\times h)$ for neural network (d=input dim, h=hidden dim)\n",
    "- **Activations:** $O(b \\times h \\times l)$ for backprop (batch, hidden, layers) \u2014 memory bottleneck\n",
    "- **Data:** $O(n \\times d)$ for dataset\n",
    "\n",
    "**Trade-offs:**\n",
    "- Memoization (DP): Time $O(n)$ \u2192 Space $O(n)$ vs Time $O(2^n)$ \u2192 Space $O(1)$\n",
    "- Checkpoints in training: Recompute vs Store (memory vs compute tradeoff)\n",
    "\n",
    "#### **3.3.4 Profiling Complexity**\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "def measure_time(func, *args, n_trials=5):\n",
    "    \"\"\"Empirical complexity measurement\"\"\"\n",
    "    times = []\n",
    "    for _ in range(n_trials):\n",
    "        start = time.perf_counter()\n",
    "        func(*args)\n",
    "        times.append(time.perf_counter() - start)\n",
    "    return np.mean(times)\n",
    "\n",
    "# Verify O(n log n) sorting\n",
    "sizes = [100, 1000, 10000, 100000]\n",
    "times = []\n",
    "for n in sizes:\n",
    "    arr = np.random.randn(n)\n",
    "    t = measure_time(np.sort, arr)\n",
    "    times.append(t)\n",
    "    print(f\"n={n}, time={t:.4f}, ratio={times[-1]/times[-2] if len(times)>1 else 0:.2f}\")\n",
    "# Ratio should approach log(n) growth\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **3.4 Software Engineering for ML**\n",
    "\n",
    "#### **3.4.1 SOLID Principles**\n",
    "\n",
    "**S - Single Responsibility:** A class should have one reason to change.\n",
    "```python\n",
    "# BAD: Data loading AND augmentation AND model training\n",
    "class MLSystem:\n",
    "    def load_data(self): ...\n",
    "    def augment(self): ...\n",
    "    def train(self): ...\n",
    "\n",
    "# GOOD: Separate concerns\n",
    "class DataLoader: ...\n",
    "class Augmenter: ...\n",
    "class Trainer: ...\n",
    "```\n",
    "\n",
    "**O - Open/Closed:** Open for extension, closed for modification.\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "\n",
    "class LossFunction(ABC):\n",
    "    @abstractmethod\n",
    "    def compute(self, y_pred, y_true): ...\n",
    "\n",
    "class MSELoss(LossFunction):\n",
    "    def compute(self, y_pred, y_true):\n",
    "        return np.mean((y_pred - y_true)**2)\n",
    "\n",
    "class CrossEntropyLoss(LossFunction):\n",
    "    def compute(self, y_pred, y_true):\n",
    "        return -np.sum(y_true * np.log(y_pred + 1e-8))\n",
    "\n",
    "# Can add new losses without modifying Trainer\n",
    "class Trainer:\n",
    "    def __init__(self, loss_fn: LossFunction):\n",
    "        self.loss_fn = loss_fn\n",
    "```\n",
    "\n",
    "**L - Liskov Substitution:** Subtypes must be substitutable.\n",
    "```python\n",
    "class BaseModel:\n",
    "    def predict(self, X): ...\n",
    "\n",
    "class NeuralNet(BaseModel):\n",
    "    def predict(self, X): ...  # Same signature, stricter preconditions OK\n",
    "\n",
    "# Violation: Changing return type from array to list breaks downstream code\n",
    "```\n",
    "\n",
    "**I - Interface Segregation:** Clients shouldn't depend on methods they don't use.\n",
    "```python\n",
    "# BAD: One huge interface\n",
    "class Dataset:\n",
    "    def load_image(self): ...\n",
    "    def load_text(self): ...\n",
    "    def load_audio(self): ...\n",
    "\n",
    "# GOOD: Split interfaces\n",
    "class ImageDataset: ...\n",
    "class TextDataset: ...\n",
    "```\n",
    "\n",
    "**D - Dependency Inversion:** Depend on abstractions, not concretions.\n",
    "```python\n",
    "# BAD\n",
    "class Trainer:\n",
    "    def __init__(self):\n",
    "        self.model = ResNet50()  # Concrete dependency\n",
    "\n",
    "# GOOD\n",
    "class Trainer:\n",
    "    def __init__(self, model: ModelInterface):\n",
    "        self.model = model  # Injectable dependency\n",
    "```\n",
    "\n",
    "#### **3.4.2 Design Patterns in ML**\n",
    "\n",
    "**Strategy Pattern:** Interchangeable algorithms (optimizers, schedulers).\n",
    "```python\n",
    "class OptimizerStrategy(ABC):\n",
    "    @abstractmethod\n",
    "    def step(self, gradients, params): ...\n",
    "\n",
    "class SGDOptimizer(OptimizerStrategy): ...\n",
    "class AdamOptimizer(OptimizerStrategy): ...\n",
    "```\n",
    "\n",
    "**Factory Pattern:** Create models based on config.\n",
    "```python\n",
    "class ModelFactory:\n",
    "    @staticmethod\n",
    "    def create(model_type: str):\n",
    "        if model_type == \"resnet\":\n",
    "            return ResNet()\n",
    "        elif model_type == \"transformer\":\n",
    "            return Transformer()\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model: {model_type}\")\n",
    "```\n",
    "\n",
    "**Observer Pattern:** Logging, early stopping, model checkpointing.\n",
    "```python\n",
    "class TrainerObservable:\n",
    "    def __init__(self):\n",
    "        self.observers = []\n",
    "    \n",
    "    def register(self, observer):\n",
    "        self.observers.append(observer)\n",
    "    \n",
    "    def notify(self, epoch, metrics):\n",
    "        for obs in self.observers:\n",
    "            obs.update(epoch, metrics)\n",
    "\n",
    "class EarlyStoppingObserver:\n",
    "    def update(self, epoch, metrics):\n",
    "        if metrics['val_loss'] > self.best_loss:\n",
    "            self.patience_counter += 1\n",
    "```\n",
    "\n",
    "**Decorator Pattern:** Data augmentation pipelines (composable transforms).\n",
    "```python\n",
    "class Transform(ABC):\n",
    "    @abstractmethod\n",
    "    def apply(self, image): ...\n",
    "\n",
    "class Compose(Transform):\n",
    "    def __init__(self, transforms):\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def apply(self, image):\n",
    "        for t in self.transforms:\n",
    "            image = t.apply(image)\n",
    "        return image\n",
    "```\n",
    "\n",
    "#### **3.4.3 Testing ML Code**\n",
    "\n",
    "**Challenges:**\n",
    "- Stochasticity (random initialization, dropout)\n",
    "- Large data (slow tests)\n",
    "- Floating point precision\n",
    "\n",
    "**Strategies:**\n",
    "\n",
    "1. **Seed Fixing:**\n",
    "```python\n",
    "def test_model_convergence():\n",
    "    np.random.seed(42)\n",
    "    torch.manual_seed(42)\n",
    "    # ... test deterministic behavior\n",
    "```\n",
    "\n",
    "2. **Mocking Data:**\n",
    "```python\n",
    "import pytest\n",
    "\n",
    "@pytest.fixture\n",
    "def dummy_data():\n",
    "    return np.random.randn(100, 10), np.random.randint(0, 2, 100)\n",
    "\n",
    "def test_forward_pass(dummy_data):\n",
    "    X, y = dummy_data\n",
    "    model = SmallModel()\n",
    "    out = model(X)\n",
    "    assert out.shape == (100, 2)\n",
    "```\n",
    "\n",
    "3. **Property-Based Testing:**\n",
    "```python\n",
    "from hypothesis import given, strategies as st\n",
    "\n",
    "@given(st.lists(st.floats(), min_size=1))\n",
    "def test_normalization_mean_zero(data):\n",
    "    arr = np.array(data)\n",
    "    normalized = (arr - np.mean(arr)) / np.std(arr)\n",
    "    assert abs(np.mean(normalized)) < 1e-10\n",
    "```\n",
    "\n",
    "4. **Regression Tests:** Save model outputs for fixed inputs; ensure they don't change unexpectedly (snapshot testing).\n",
    "\n",
    "5. **Integration Tests:** End-to-end training for 1 epoch on toy data to catch shape mismatches.\n",
    "\n",
    "---\n",
    "\n",
    "## **3.5 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Custom Autograd Engine**\n",
    "Implement a simplified version of PyTorch's autograd using computational graphs (linked nodes) and reverse-mode automatic differentiation.\n",
    "\n",
    "**Requirements:**\n",
    "- Support operations: add, multiply, ReLU, matrix multiply\n",
    "- Build computation graph dynamically\n",
    "- Backward pass using topological sort\n",
    "- Verify gradients match numerical differentiation\n",
    "\n",
    "**Deliverable:** `micrograd.py` with test cases showing correct gradients for a small neural network.\n",
    "\n",
    "### **Lab 2: Approximate Nearest Neighbor (ANN) Search**\n",
    "Implement a basic Locality Sensitive Hashing (LSH) or KD-Tree for finding similar embeddings.\n",
    "\n",
    "**Requirements:**\n",
    "- Index 100k vectors of dimension 128\n",
    "- Query time <10ms for top-10 neighbors (vs O(n) brute force)\n",
    "- Recall@10 > 0.95 compared to exact search\n",
    "- Memory usage <500MB\n",
    "\n",
    "**Deliverable:** `ann_index.py` with benchmarking against `sklearn.neighbors.KDTree`.\n",
    "\n",
    "### **Lab 3: Optimal Binning Algorithm**\n",
    "Implement a dynamic programming algorithm to find optimal binning thresholds for continuous features (maximizing information gain).\n",
    "\n",
    "**Requirements:**\n",
    "- O(n log n) complexity using sorted splits\n",
    "- Support for multi-class targets\n",
    "- Handles missing values efficiently\n",
    "\n",
    "**Deliverable:** `optimal_binner.py` integrated with a Decision Tree classifier.\n",
    "\n",
    "### **Lab 4: Distributed Feature Counter**\n",
    "Using only standard library (no external databases), implement a disk-based feature counter that can handle 10B features using external merge sort.\n",
    "\n",
    "**Requirements:**\n",
    "- Process data in chunks that fit in RAM (1GB limit)\n",
    "- Exact counts (not approximate like Count-Min Sketch)\n",
    "- Parallel processing with multiprocessing\n",
    "\n",
    "**Deliverable:** `external_counter.py` with complexity analysis documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## **3.6 Common Pitfalls**\n",
    "\n",
    "1. **$O(n^2)$ Distance Matrix Computation:**\n",
    "   ```python\n",
    "   # BAD: Double loop\n",
    "   for i in range(n):\n",
    "       for j in range(n):\n",
    "           dist[i,j] = np.linalg.norm(X[i] - X[j])\n",
    "   \n",
    "   # GOOD: Vectorized broadcasting\n",
    "   dist = np.sqrt(((X[:, None, :] - X[None, :, :]) ** 2).sum(-1))\n",
    "   # Or better: use scipy.spatial.distance.cdist\n",
    "   ```\n",
    "\n",
    "2. **Repeated Dictionary Lookups in Loops:**\n",
    "   ```python\n",
    "   # Slow\n",
    "   for i in range(1000000):\n",
    "       val = big_dict[keys[i]]\n",
    "   \n",
    "   # Faster: Local variable binding\n",
    "   get_val = big_dict.get\n",
    "   for i in range(1000000):\n",
    "       val = get_val(keys[i])\n",
    "   ```\n",
    "\n",
    "3. **Recursive DFS on Deep Graphs:** Python recursion limit (~1000). Use iterative stack implementation for deep computation graphs.\n",
    "\n",
    "4. **Not Using `__slots__` for Small Objects:**\n",
    "   ```python\n",
    "   class Node:\n",
    "       __slots__ = ['feature', 'threshold', 'left', 'right']\n",
    "       # Saves ~50% memory per node (critical for large trees)\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **3.7 Interview Questions**\n",
    "\n",
    "**Q1:** Why is QuickSort generally preferred over MergeSort for arrays, but MergeSort for linked lists?\n",
    "*A: QuickSort has better cache locality for arrays (in-place partition) but $O(\\log n)$ stack space. MergeSort requires $O(n)$ extra space but works efficiently with linked lists (no random access needed) and is stable (preserves order of equal elements).*\n",
    "\n",
    "**Q2:** How would you find the median of a stream of integers (online algorithm)?\n",
    "*A: Use two heaps: max-heap for lower half, min-heap for upper half. Balance sizes so max-heap has equal or one more element. Median is top of max-heap (or average of both tops if even count). Operations $O(\\log n)$ per insertion.*\n",
    "\n",
    "**Q3:** Explain why hash table lookup is $O(1)$ average but $O(n)$ worst case.\n",
    "*A: Average: uniform hashing distributes keys evenly, few collisions. Worst: all keys hash to same bucket (degrades to linked list). In practice, universal hashing or cryptographic hashing prevents adversarial attacks.*\n",
    "\n",
    "**Q4:** Implement a Trie and analyze space complexity vs hash table for string storage.\n",
    "*A: Trie shares prefixes (e.g., \"cat\", \"car\" share \"ca\"). Space $O(\\text{total characters})$ vs hash table $O(\\text{total characters} \\times \\text{pointer overhead})$. Trie wins on prefix queries; hash table wins on exact match.*\n",
    "\n",
    "**Q5:** How does dynamic programming differ from greedy algorithms? Give an ML example.\n",
    "*A: DP explores all subproblems and stores solutions (optimal substructure, overlapping subproblems). Greedy makes locally optimal choice without reconsidering. Example: DP for global sequence alignment (Needleman-Wunsch) vs greedy local alignment (not guaranteed optimal).*\n",
    "\n",
    "---\n",
    "\n",
    "## **3.8 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Introduction to Algorithms* (CLRS) - Chapters 6 (Heaps), 15 (DP), 22 (Graphs), 35 (Approximation algorithms)\n",
    "- *Design Patterns* (Gang of Four) - Strategy, Observer, Factory patterns\n",
    "- *Clean Code* (Robert Martin) - SOLID principles with examples\n",
    "\n",
    "**Specific to ML:**\n",
    "- \"Efficient Data Structures for Tiny Machine Learning\" (TinyML research)\n",
    "- \"Algorithmic Efficiency in Transformers\" (FlashAttention, sparse attention patterns)\n",
    "- Scikit-learn source code for tree implementations (Cython optimized)\n",
    "\n",
    "---\n",
    "\n",
    "## **3.9 Checkpoint Project: High-Performance Feature Store**\n",
    "\n",
    "Build a feature store that serves pre-computed ML features with low latency (<5ms p99).\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "**Architecture:**\n",
    "- **Storage Layer:** In-memory hash table + disk-backed SSTables (LSM-tree concept)\n",
    "- **Serving Layer:** gRPC API (or FastAPI) with batch and single lookup endpoints\n",
    "- **Ingestion Layer:** Stream processing simulation (Kafka-like queue consumption)\n",
    "\n",
    "**Data Structures:**\n",
    "1. **Hash Index:** Feature name \u2192 offset in SSTable (O(1) lookup)\n",
    "2. **LRU Cache:** Hot feature vectors cached in memory (eviction when full)\n",
    "3. **Bloom Filter:** Check if feature exists before disk lookup (reduce I/O)\n",
    "4. **Segment Tree:** For time-windowed features (rolling aggregates)\n",
    "\n",
    "**Algorithms:**\n",
    "- **Consistent Hashing:** For distributed feature storage (future scaling)\n",
    "- **Binary Search:** On SSTable index for range queries\n",
    "- **Top-K:** Maintain most frequently accessed features for cache warming\n",
    "\n",
    "**Performance Targets:**\n",
    "- Single feature lookup: <1ms (in-memory), <10ms (disk)\n",
    "- Batch lookup (100 features): <5ms\n",
    "- Ingestion throughput: 10k features/second\n",
    "- Memory usage: <2GB for 1M features (512-dim vectors)\n",
    "\n",
    "**Testing:**\n",
    "- Unit tests for each data structure\n",
    "- Integration test with 1M random features\n",
    "- Load test with Locust (1000 concurrent users)\n",
    "- Chaos test: random disk failures, verify consistency\n",
    "\n",
    "**Deliverables:**\n",
    "- GitHub repo with architecture diagram\n",
    "- Benchmark report comparing against Redis (baseline)\n",
    "- Documentation on consistency guarantees (eventual vs strong)\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 3**\n",
    "\n",
    "*You now possess the algorithmic foundations to build scalable ML systems. Chapter 4 will cover Development Environment & Tools (Git, Linux, Docker, Cloud) \u2014 the infrastructure layer of AI engineering.*\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='2. python_for_ai_development.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='4. development_enironment_and_tools.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}