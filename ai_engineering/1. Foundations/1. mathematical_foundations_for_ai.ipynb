{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68894deb",
   "metadata": {},
   "source": [
    "Here is **Chapter 1: Mathematical Foundations for AI** — the complete first chapter of your AI Engineer Workbook.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 1: MATHEMATICAL FOUNDATIONS FOR AI**\n",
    "\n",
    "*The Language of Intelligence*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Before writing a single line of machine learning code, you must understand the mathematical machinery that powers modern AI. This chapter transforms abstract mathematics into practical tools. We will not just learn formulas—we will implement them in NumPy to build intuition through code.\n",
    "\n",
    "**Estimated Time:** 40-50 hours (2-3 weeks)  \n",
    "**Prerequisites:** High school algebra, basic Python syntax\n",
    "\n",
    "---\n",
    "\n",
    "## **1.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Manipulate vectors, matrices, and tensors as fluently as Python lists\n",
    "2. Compute gradients by hand and verify them programmatically\n",
    "3. Apply Bayes' theorem to update beliefs given evidence\n",
    "4. Design hypothesis tests to validate model performance\n",
    "5. Recognize when correlation implies causation (and when it doesn't)\n",
    "\n",
    "---\n",
    "\n",
    "## **1.1 Linear Algebra: The Language of Data**\n",
    "\n",
    "### **Why Linear Algebra Matters**\n",
    "\n",
    "Every image is a matrix of pixels. Every sentence is a vector of embeddings. Every dataset is a tensor of features. Linear algebra provides the syntax for describing high-dimensional spaces where AI operates.\n",
    "\n",
    "#### **1.1.1 Scalars, Vectors, and Matrices**\n",
    "\n",
    "**Scalar:** A single number (0-dimensional). Temperature, age, price.\n",
    "$$ s \\in \\mathbb{R} $$\n",
    "\n",
    "**Vector:** An ordered list of numbers (1-dimensional array). Represents a point in space or a direction.\n",
    "$$ \\mathbf{v} = \\begin{bmatrix} v_1 \\\\ v_2 \\\\ \\vdots \\\\ v_n \\end{bmatrix} \\in \\mathbb{R}^n $$\n",
    "\n",
    "In AI, vectors represent:\n",
    "- Feature vectors (house: [sqft, bedrooms, age])\n",
    "- Word embeddings (\"king\" → [0.2, -0.5, ...])\n",
    "- Model weights\n",
    "\n",
    "**Matrix:** A 2-dimensional array of numbers.\n",
    "$$ \\mathbf{A} = \\begin{bmatrix} a_{11} & a_{12} & \\cdots & a_{1n} \\\\ a_{21} & a_{22} & \\cdots & a_{2n} \\\\ \\vdots & \\vdots & \\ddots & \\vdots \\\\ a_{m1} & a_{m2} & \\cdots & a_{mn} \\end{bmatrix} \\in \\mathbb{R}^{m \\times n} $$\n",
    "\n",
    "Matrices represent:\n",
    "- Image data (height × width × channels)\n",
    "- Layers of neural networks (weight matrices)\n",
    "- Tabular datasets (samples × features)\n",
    "\n",
    "**Tensor:** Generalization to $n$ dimensions. A scalar is a 0-tensor, vector is 1-tensor, matrix is 2-tensor.\n",
    "$$ \\mathcal{T} \\in \\mathbb{R}^{d_1 \\times d_2 \\times \\cdots \\times d_n} $$\n",
    "\n",
    "In deep learning, a batch of 32 RGB images of size 224×224 is a tensor of shape $(32, 224, 224, 3)$.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Creating tensors\n",
    "scalar = np.array(5)\n",
    "vector = np.array([1, 2, 3])\n",
    "matrix = np.array([[1, 2], [3, 4], [5, 6]])  # 3x2 matrix\n",
    "tensor = np.random.rand(32, 224, 224, 3)     # Batch of images\n",
    "\n",
    "print(f\"Scalar shape: {scalar.shape}\")       # ()\n",
    "print(f\"Vector shape: {vector.shape}\")       # (3,)\n",
    "print(f\"Matrix shape: {matrix.shape}\")       # (3, 2)\n",
    "print(f\"Tensor shape: {tensor.shape}\")       # (32, 224, 224, 3)\n",
    "```\n",
    "\n",
    "#### **1.1.2 Vector Operations**\n",
    "\n",
    "**Dot Product (Inner Product):** Measures alignment between vectors.\n",
    "$$ \\mathbf{a} \\cdot \\mathbf{b} = \\sum_{i=1}^n a_i b_i = \\|\\mathbf{a}\\| \\|\\mathbf{b}\\| \\cos(\\theta) $$\n",
    "\n",
    "The dot product is the heart of neural networks—every layer computes weighted sums (dot products between inputs and weights).\n",
    "\n",
    "```python\n",
    "a = np.array([1, 2, 3])\n",
    "b = np.array([4, 5, 6])\n",
    "\n",
    "# Dot product\n",
    "dot_product = np.dot(a, b)  # 1*4 + 2*5 + 3*6 = 32\n",
    "\n",
    "# Geometric interpretation\n",
    "norm_a = np.linalg.norm(a)\n",
    "norm_b = np.linalg.norm(b)\n",
    "cos_theta = dot_product / (norm_a * norm_b)\n",
    "angle = np.arccos(cos_theta)  # In radians\n",
    "```\n",
    "\n",
    "**Hadamard Product (Element-wise):**\n",
    "$$ (\\mathbf{a} \\circ \\mathbf{b})_i = a_i \\cdot b_i $$\n",
    "\n",
    "**Outer Product:** Creates a matrix from two vectors.\n",
    "$$ (\\mathbf{a} \\otimes \\mathbf{b})_{ij} = a_i \\cdot b_j $$\n",
    "\n",
    "```python\n",
    "# Hadamard product\n",
    "hadamard = a * b  # [4, 10, 18]\n",
    "\n",
    "# Outer product\n",
    "outer = np.outer(a, b)  \n",
    "# [[ 4,  5,  6],\n",
    "#  [ 8, 10, 12],\n",
    "#  [12, 15, 18]]\n",
    "```\n",
    "\n",
    "#### **1.1.3 Vector Norms**\n",
    "\n",
    "Norms measure magnitude. Critical for regularization (L1/L2) and optimization.\n",
    "\n",
    "- **L1 Norm (Manhattan):** $\\|\\mathbf{x}\\|_1 = \\sum |x_i|$ — Encourages sparsity\n",
    "- **L2 Norm (Euclidean):** $\\|\\mathbf{x}\\|_2 = \\sqrt{\\sum x_i^2}$ — Default distance metric\n",
    "- **L∞ Norm (Max):** $\\|\\mathbf{x}\\|_\\infty = \\max |x_i|$\n",
    "\n",
    "```python\n",
    "l1 = np.linalg.norm(a, ord=1)   # Sum of absolute values\n",
    "l2 = np.linalg.norm(a, ord=2)   # Euclidean length\n",
    "linf = np.linalg.norm(a, ord=np.inf)  # Maximum absolute value\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **1.2 Matrix Operations: The Mechanics of Transformation**\n",
    "\n",
    "Matrices transform spaces. When you multiply a vector by a matrix, you rotate, scale, or project it into a new space. This is literally what happens in every layer of a neural network.\n",
    "\n",
    "#### **1.2.1 Matrix Multiplication**\n",
    "\n",
    "For matrices $\\mathbf{A} \\in \\mathbb{R}^{m \\times n}$ and $\\mathbf{B} \\in \\mathbb{R}^{n \\times p}$:\n",
    "$$ (\\mathbf{A}\\mathbf{B})_{ij} = \\sum_{k=1}^n A_{ik} B_{kj} $$\n",
    "\n",
    "**Key Rules:**\n",
    "- Not commutative: $\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}$ (usually)\n",
    "- Associative: $(\\mathbf{A}\\mathbf{B})\\mathbf{C} = \\mathbf{A}(\\mathbf{B}\\mathbf{C})$\n",
    "- Transpose rule: $(\\mathbf{A}\\mathbf{B})^T = \\mathbf{B}^T \\mathbf{A}^T$\n",
    "\n",
    "```python\n",
    "A = np.array([[1, 2], [3, 4], [5, 6]])      # 3x2\n",
    "B = np.array([[7, 8, 9], [10, 11, 12]])     # 2x3\n",
    "\n",
    "# Matrix multiplication\n",
    "C = np.matmul(A, B)  # or A @ B (Python 3.5+)\n",
    "# Result: 3x3 matrix\n",
    "\n",
    "# Note: B @ A would give 2x2 (different result)\n",
    "```\n",
    "\n",
    "**Geometric Interpretation:** Matrix multiplication is composition of linear transformations. If $\\mathbf{A}$ rotates and $\\mathbf{B}$ scales, $\\mathbf{A}\\mathbf{B}$ does both.\n",
    "\n",
    "#### **1.2.2 Special Matrices**\n",
    "\n",
    "**Identity Matrix ($\\mathbf{I}$):** The \"1\" of matrices. $\\mathbf{A}\\mathbf{I} = \\mathbf{A}$.\n",
    "$$ \\mathbf{I}_3 = \\begin{bmatrix} 1 & 0 & 0 \\\\ 0 & 1 & 0 \\\\ 0 & 0 & 1 \\end{bmatrix} $$\n",
    "\n",
    "**Diagonal Matrix:** Non-zero elements only on diagonal. Efficient storage ($O(n)$ vs $O(n^2)$).\n",
    "```python\n",
    "diag = np.diag([1, 2, 3])  # Creates diagonal matrix\n",
    "extracted = np.diag(diag)  # Extracts diagonal: [1, 2, 3]\n",
    "```\n",
    "\n",
    "**Symmetric Matrix:** $\\mathbf{A} = \\mathbf{A}^T$. Common in covariance matrices and graph adjacency matrices.\n",
    "\n",
    "**Orthogonal Matrix:** $\\mathbf{Q}^T \\mathbf{Q} = \\mathbf{I}$. Columns are orthonormal (perpendicular unit vectors). Preserves lengths and angles—essential in attention mechanisms.\n",
    "\n",
    "#### **1.2.3 Matrix Transpose and Inverse**\n",
    "\n",
    "**Transpose:** Flip over diagonal. $(\\mathbf{A}^T)_{ij} = A_{ji}$.\n",
    "\n",
    "**Inverse:** Matrix $\\mathbf{A}^{-1}$ such that $\\mathbf{A}\\mathbf{A}^{-1} = \\mathbf{I}$. Exists only if $\\mathbf{A}$ is square and full rank (determinant ≠ 0).\n",
    "\n",
    "In AI, we rarely compute inverses directly (too expensive: $O(n^3)$), but understanding them is crucial for:\n",
    "- Solving linear systems $\\mathbf{A}\\mathbf{x} = \\mathbf{b}$\n",
    "- Understanding covariance matrices\n",
    "- Deriving normal equations for linear regression\n",
    "\n",
    "```python\n",
    "A_square = np.array([[4, 7], [2, 6]])\n",
    "\n",
    "# Transpose\n",
    "A_T = A_square.T\n",
    "\n",
    "# Inverse\n",
    "A_inv = np.linalg.inv(A_square)\n",
    "\n",
    "# Verification\n",
    "identity = A_square @ A_inv  # Should be close to I (with floating point error)\n",
    "```\n",
    "\n",
    "**Pseudo-inverse (Moore-Penrose):** For non-square matrices, used in least squares solutions.\n",
    "```python\n",
    "A_pinv = np.linalg.pinv(A)  # Works for rectangular matrices\n",
    "```\n",
    "\n",
    "#### **1.2.4 Eigenvalues and Eigenvectors**\n",
    "\n",
    "For a square matrix $\\mathbf{A}$, an eigenvector $\\mathbf{v}$ is a non-zero vector that only gets scaled (not rotated) when multiplied by $\\mathbf{A}$:\n",
    "$$ \\mathbf{A}\\mathbf{v} = \\lambda \\mathbf{v} $$\n",
    "\n",
    "Where $\\lambda$ is the eigenvalue (scaling factor).\n",
    "\n",
    "**Why this matters for AI:**\n",
    "- **PCA (Dimensionality reduction):** Eigenvectors of covariance matrix = principal components\n",
    "- **PageRank:** Eigenvector of the web graph adjacency matrix\n",
    "- **Stability analysis:** Eigenvalues of Hessian matrix determine optimization convergence\n",
    "- **Spectral clustering:** Uses eigenvectors of graph Laplacian\n",
    "\n",
    "```python\n",
    "A = np.array([[4, 2], [1, 3]])\n",
    "\n",
    "# Compute eigenvalues and eigenvectors\n",
    "eigenvalues, eigenvectors = np.linalg.eig(A)\n",
    "\n",
    "# eigenvectors[:, i] is the eigenvector for eigenvalues[i]\n",
    "print(f\"Eigenvalue: {eigenvalues[0]}\")\n",
    "print(f\"Corresponding eigenvector: {eigenvectors[:, 0]}\")\n",
    "\n",
    "# Verify: A @ v should equal lambda * v\n",
    "v = eigenvectors[:, 0]\n",
    "lhs = A @ v\n",
    "rhs = eigenvalues[0] * v\n",
    "print(f\"Verification (should be ~0): {np.allclose(lhs, rhs)}\")\n",
    "```\n",
    "\n",
    "**Spectral Decomposition:** For symmetric matrices:\n",
    "$$ \\mathbf{A} = \\mathbf{Q} \\mathbf{\\Lambda} \\mathbf{Q}^T $$\n",
    "Where $\\mathbf{Q}$ is orthogonal matrix of eigenvectors, $\\mathbf{\\Lambda}$ is diagonal matrix of eigenvalues.\n",
    "\n",
    "---\n",
    "\n",
    "## **1.3 Calculus: The Optimization Engine**\n",
    "\n",
    "Machine learning is optimization. We minimize loss functions using gradients. This section builds the machinery for understanding backpropagation.\n",
    "\n",
    "#### **1.3.1 Derivatives and Gradients**\n",
    "\n",
    "**Derivative:** Rate of change. Slope of the tangent line.\n",
    "$$ f'(x) = \\frac{df}{dx} = \\lim_{h \\to 0} \\frac{f(x+h) - f(x)}{h} $$\n",
    "\n",
    "**Partial Derivative:** Derivative with respect to one variable, holding others constant.\n",
    "$$ \\frac{\\partial f}{\\partial x_i} $$\n",
    "\n",
    "**Gradient:** Vector of partial derivatives. Points in direction of steepest ascent.\n",
    "$$ \\nabla f = \\begin{bmatrix} \\frac{\\partial f}{\\partial x_1} \\\\ \\frac{\\partial f}{\\partial x_2} \\\\ \\vdots \\\\ \\frac{\\partial f}{\\partial x_n} \\end{bmatrix} $$\n",
    "\n",
    "In ML, we move **against** the gradient (gradient descent) to minimize loss.\n",
    "\n",
    "```python\n",
    "# Numerical gradient checking (finite differences)\n",
    "def numerical_gradient(f, x, h=1e-5):\n",
    "    \"\"\"Compute gradient of f at x using central difference\"\"\"\n",
    "    grad = np.zeros_like(x)\n",
    "    it = np.nditer(x, flags=['multi_index'], op_flags=['readwrite'])\n",
    "    \n",
    "    while not it.finished:\n",
    "        idx = it.multi_index\n",
    "        old_val = x[idx]\n",
    "        \n",
    "        x[idx] = old_val + h\n",
    "        fx_plus = f(x)\n",
    "        \n",
    "        x[idx] = old_val - h\n",
    "        fx_minus = f(x)\n",
    "        \n",
    "        grad[idx] = (fx_plus - fx_minus) / (2 * h)\n",
    "        x[idx] = old_val\n",
    "        it.iternext()\n",
    "    \n",
    "    return grad\n",
    "\n",
    "# Example: f(x, y) = x^2 + y^2\n",
    "def f(point):\n",
    "    x, y = point\n",
    "    return x**2 + y**2\n",
    "\n",
    "point = np.array([3.0, 4.0])\n",
    "grad = numerical_gradient(f, point)\n",
    "print(f\"Gradient at (3,4): {grad}\")  # Should be close to [6, 8]\n",
    "```\n",
    "\n",
    "#### **1.3.2 The Chain Rule**\n",
    "\n",
    "The chain rule is the mathematical foundation of **backpropagation**. If $z = f(y)$ and $y = g(x)$, then:\n",
    "$$ \\frac{dz}{dx} = \\frac{dz}{dy} \\cdot \\frac{dy}{dx} $$\n",
    "\n",
    "For multiple variables (multivariate chain rule):\n",
    "$$ \\frac{\\partial z}{\\partial x} = \\sum_i \\frac{\\partial z}{\\partial y_i} \\frac{\\partial y_i}{\\partial x} $$\n",
    "\n",
    "**Example:** Neural network layer\n",
    "$$ z = \\text{ReLU}(\\mathbf{w}^T \\mathbf{x} + b) $$\n",
    "\n",
    "To find $\\frac{\\partial z}{\\partial \\mathbf{w}}$, we apply chain rule through:\n",
    "1. Linear transformation: $a = \\mathbf{w}^T \\mathbf{x} + b$\n",
    "2. Activation: $z = \\max(0, a)$\n",
    "\n",
    "$$ \\frac{\\partial z}{\\partial \\mathbf{w}} = \\frac{\\partial z}{\\partial a} \\cdot \\frac{\\partial a}{\\partial \\mathbf{w}} = \\mathbb{I}(a > 0) \\cdot \\mathbf{x}^T $$\n",
    "\n",
    "Where $\\mathbb{I}$ is the indicator function (1 if true, 0 else).\n",
    "\n",
    "```python\n",
    "# Manual backpropagation example\n",
    "def relu(x):\n",
    "    return np.maximum(0, x)\n",
    "\n",
    "def relu_derivative(x):\n",
    "    return (x > 0).astype(float)\n",
    "\n",
    "# Forward pass\n",
    "x = np.array([2.0, -1.0, 0.5])\n",
    "w = np.array([0.1, 0.5, -0.2])\n",
    "b = 0.1\n",
    "\n",
    "z = np.dot(w, x) + b\n",
    "a = relu(z)\n",
    "\n",
    "# Backward pass (assuming da = 1 for final output)\n",
    "da = 1.0\n",
    "dz = da * relu_derivative(z)  # Chain rule through ReLU\n",
    "dw = dz * x                   # Gradient w.r.t weights\n",
    "db = dz                       # Gradient w.r.t bias\n",
    "\n",
    "print(f\"Gradient w.r.t w: {dw}\")\n",
    "print(f\"Gradient w.r.t b: {db}\")\n",
    "```\n",
    "\n",
    "#### **1.3.3 Jacobian and Hessian**\n",
    "\n",
    "**Jacobian Matrix:** For vector-valued function $\\mathbf{f}: \\mathbb{R}^n \\to \\mathbb{R}^m$, the Jacobian is the matrix of all first-order partial derivatives.\n",
    "$$ \\mathbf{J}_{ij} = \\frac{\\partial f_i}{\\partial x_j} $$\n",
    "\n",
    "Used when layers output vectors rather than scalars.\n",
    "\n",
    "**Hessian Matrix:** Matrix of second derivatives.\n",
    "$$ \\mathbf{H}_{ij} = \\frac{\\partial^2 f}{\\partial x_i \\partial x_j} $$\n",
    "\n",
    "Used in:\n",
    "- Second-order optimization methods (Newton's method)\n",
    "- Determining convexity (positive definite = convex)\n",
    "- Analyzing curvature of loss landscape (sharp vs flat minima)\n",
    "\n",
    "```python\n",
    "from scipy.optimize import approx_fprime\n",
    "\n",
    "# Jacobian example\n",
    "def vector_func(x):\n",
    "    return np.array([\n",
    "        x[0]**2 + x[1],\n",
    "        x[0] * x[1]\n",
    "    ])\n",
    "\n",
    "x = np.array([1.0, 2.0])\n",
    "jacobian = approx_fprime(x, vector_func, epsilon=1e-6)\n",
    "print(f\"Jacobian shape: {jacobian.shape}\")  # 2x2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **1.4 Probability Theory: Handling Uncertainty**\n",
    "\n",
    "AI must reason under uncertainty. Probability provides the framework for:\n",
    "- Quantifying prediction confidence\n",
    "- Bayesian neural networks\n",
    "- Generative models (diffusion, VAEs)\n",
    "- Reinforcement learning (Markov Decision Processes)\n",
    "\n",
    "#### **1.4.1 Fundamentals**\n",
    "\n",
    "**Random Variable:** A variable whose value is subject to chance.\n",
    "- Discrete (e.g., number of heads in 10 coin flips)\n",
    "- Continuous (e.g., height of a person)\n",
    "\n",
    "**Probability Mass Function (PMF):** For discrete variables. $P(X = x)$.\n",
    "**Probability Density Function (PDF):** For continuous variables. $p(x)$ where $\\int p(x) dx = 1$.\n",
    "\n",
    "**Key Distributions for AI:**\n",
    "\n",
    "1. **Bernoulli:** Binary outcome (coin flip). $P(X=1) = p$\n",
    "2. **Categorical/Multinoulli:** K outcomes (class labels). Used in classification output\n",
    "3. **Normal (Gaussian):** $p(x) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp\\left(-\\frac{(x-\\mu)^2}{2\\sigma^2}\\right)$\n",
    "   - Central to diffusion models, noise assumptions, weight initialization\n",
    "4. **Uniform:** All outcomes equally likely. Used in random initialization, dropout\n",
    "5. **Beta/Dirichlet:** Distributions over probabilities. Used in Bayesian methods\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "# Sampling from distributions\n",
    "normal_samples = np.random.normal(loc=0, scale=1, size=1000)  # Gaussian\n",
    "uniform_samples = np.random.uniform(low=0, high=1, size=1000)\n",
    "categorical_sample = np.random.choice([0, 1, 2], p=[0.2, 0.5, 0.3])\n",
    "\n",
    "# Probability density\n",
    "x = np.linspace(-3, 3, 100)\n",
    "pdf = stats.norm.pdf(x, loc=0, scale=1)\n",
    "```\n",
    "\n",
    "#### **1.4.2 Expectation and Variance**\n",
    "\n",
    "**Expectation (Mean):** Weighted average.\n",
    "$$ \\mathbb{E}[X] = \\sum x P(X=x) \\quad \\text{(discrete)} $$\n",
    "$$ \\mathbb{E}[X] = \\int x p(x) dx \\quad \\text{(continuous)} $$\n",
    "\n",
    "**Variance:** Spread of distribution.\n",
    "$$ \\text{Var}(X) = \\mathbb{E}[(X - \\mu)^2] = \\mathbb{E}[X^2] - (\\mathbb{E}[X])^2 $$\n",
    "\n",
    "**Covariance:** How two variables change together.\n",
    "$$ \\text{Cov}(X,Y) = \\mathbb{E}[(X - \\mu_X)(Y - \\mu_Y)] $$\n",
    "\n",
    "**Covariance Matrix:** For random vector $\\mathbf{X}$, $\\Sigma_{ij} = \\text{Cov}(X_i, X_j)$. Symmetric, positive semi-definite. Diagonal elements are variances.\n",
    "\n",
    "```python\n",
    "# Empirical estimation from data\n",
    "data = np.random.multivariate_normal(\n",
    "    mean=[0, 0], \n",
    "    cov=[[1, 0.8], [0.8, 1]], \n",
    "    size=1000\n",
    ")\n",
    "\n",
    "mean = np.mean(data, axis=0)\n",
    "cov_matrix = np.cov(data.T)  # Note: rowvar=False in some conventions\n",
    "\n",
    "print(f\"Mean: {mean}\")\n",
    "print(f\"Covariance matrix:\\n{cov_matrix}\")\n",
    "```\n",
    "\n",
    "#### **1.4.3 Bayes' Theorem**\n",
    "\n",
    "The single most important equation for modern AI (Bayesian methods, spam filters, medical diagnosis):\n",
    "\n",
    "$$ P(A|B) = \\frac{P(B|A) \\cdot P(A)}{P(B)} $$\n",
    "\n",
    "Where:\n",
    "- $P(A|B)$: Posterior (belief after seeing evidence)\n",
    "- $P(B|A)$: Likelihood (probability of evidence given hypothesis)\n",
    "- $P(A)$: Prior (initial belief)\n",
    "- $P(B)$: Marginal likelihood (normalizing constant)\n",
    "\n",
    "**Example:** Medical Test\n",
    "- Disease prevalence: $P(\\text{Disease}) = 0.01$ (1%)\n",
    "- Test sensitivity: $P(\\text{Positive}|\\text{Disease}) = 0.99$\n",
    "- False positive rate: $P(\\text{Positive}|\\text{No Disease}) = 0.05$\n",
    "\n",
    "What's $P(\\text{Disease}|\\text{Positive})$?\n",
    "\n",
    "```python\n",
    "# Bayesian updating example\n",
    "def bayesian_update(prior, likelihood, marginal):\n",
    "    return (likelihood * prior) / marginal\n",
    "\n",
    "prior_disease = 0.01\n",
    "sensitivity = 0.99\n",
    "false_positive = 0.05\n",
    "\n",
    "# Marginal likelihood: P(Pos) = P(Pos|D)P(D) + P(Pos|~D)P(~D)\n",
    "p_positive = (sensitivity * prior_disease + \n",
    "              false_positive * (1 - prior_disease))\n",
    "\n",
    "posterior = bayesian_update(prior_disease, sensitivity, p_positive)\n",
    "print(f\"Probability of disease given positive test: {posterior:.4f}\")\n",
    "# Result: ~0.167 (16.7%) - surprisingly low due to low base rate!\n",
    "```\n",
    "\n",
    "#### **1.4.4 Maximum Likelihood Estimation (MLE)**\n",
    "\n",
    "How do we train models? We maximize the likelihood of observed data.\n",
    "\n",
    "Given data $D$ and parameters $\\theta$, find:\n",
    "$$ \\hat{\\theta} = \\arg\\max_\\theta P(D|\\theta) $$\n",
    "\n",
    "Usually minimizes negative log-likelihood (NLL):\n",
    "$$ \\mathcal{L}(\\theta) = -\\sum_i \\log P(x_i|\\theta) $$\n",
    "\n",
    "This is why we use cross-entropy loss for classification—it's the negative log-likelihood of the categorical distribution.\n",
    "\n",
    "```python\n",
    "# MLE for Gaussian mean\n",
    "data = np.random.normal(loc=5.0, scale=2.0, size=1000)\n",
    "\n",
    "# Analytical MLE for mean is just the sample mean\n",
    "mle_mean = np.mean(data)\n",
    "mle_std = np.std(data, ddof=0)  # Population std (MLE uses N, not N-1)\n",
    "\n",
    "print(f\"True mean: 5.0, Estimated: {mle_mean:.4f}\")\n",
    "print(f\"True std: 2.0, Estimated: {mle_std:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **1.5 Statistics: Validating Knowledge**\n",
    "\n",
    "Mathematics tells us what is true; statistics tells us what we can infer from finite, noisy data.\n",
    "\n",
    "#### **1.5.1 Sampling and Estimation**\n",
    "\n",
    "**Population vs Sample:** We rarely have population data; we infer from samples.\n",
    "\n",
    "**Estimator Properties:**\n",
    "- **Bias:** $\\mathbb{E}[\\hat{\\theta}] - \\theta$ (systematic error)\n",
    "- **Variance:** $\\text{Var}(\\hat{\\theta})$ (sensitivity to sample)\n",
    "- **Consistency:** $\\hat{\\theta} \\to \\theta$ as $n \\to \\infty$\n",
    "\n",
    "**Bias-Variance Tradeoff** (preview of ML concepts):\n",
    "- High bias: Underfitting (too simple)\n",
    "- High variance: Overfitting (too complex)\n",
    "\n",
    "#### **1.5.2 Hypothesis Testing**\n",
    "\n",
    "**Null Hypothesis ($H_0$):** Default assumption (e.g., \"Model A and B perform equally\")\n",
    "**Alternative Hypothesis ($H_1$):** What we want to prove (e.g., \"Model A is better\")\n",
    "\n",
    "**p-value:** Probability of observing data as extreme as we did, assuming $H_0$ is true. If $p < 0.05$, we reject $H_0$.\n",
    "\n",
    "**T-test:** Compare means of two groups.\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "# Model A accuracy scores across 10 folds\n",
    "model_a = [0.85, 0.87, 0.84, 0.86, 0.88, 0.85, 0.86, 0.87, 0.85, 0.86]\n",
    "# Model B accuracy scores\n",
    "model_b = [0.82, 0.83, 0.81, 0.84, 0.82, 0.83, 0.81, 0.82, 0.83, 0.82]\n",
    "\n",
    "t_stat, p_value = stats.ttest_ind(model_a, model_b)\n",
    "print(f\"T-statistic: {t_stat:.4f}, p-value: {p_value:.6f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"Significant difference between models!\")\n",
    "else:\n",
    "    print(\"No significant difference detected.\")\n",
    "```\n",
    "\n",
    "**Multiple Testing Problem:** If you test 20 hypotheses at $p < 0.05$, you expect 1 false positive by chance. Use Bonferroni correction: $\\alpha_{adjusted} = \\alpha / n$.\n",
    "\n",
    "#### **1.5.3 Confidence Intervals**\n",
    "\n",
    "A 95% confidence interval means: if we repeated the experiment many times, 95% of intervals would contain the true parameter.\n",
    "\n",
    "$$ \\text{CI} = \\bar{x} \\pm t_{\\alpha/2} \\cdot \\frac{s}{\\sqrt{n}} $$\n",
    "\n",
    "```python\n",
    "def confidence_interval(data, confidence=0.95):\n",
    "    n = len(data)\n",
    "    mean = np.mean(data)\n",
    "    std_err = stats.sem(data)  # Standard error of mean\n",
    "    margin = std_err * stats.t.ppf((1 + confidence) / 2, n - 1)\n",
    "    return mean - margin, mean + margin\n",
    "\n",
    "lower, upper = confidence_interval(model_a)\n",
    "print(f\"95% CI for Model A: [{lower:.4f}, {upper:.4f}]\")\n",
    "```\n",
    "\n",
    "#### **1.5.4 Correlation vs. Causation**\n",
    "\n",
    "**Correlation:** Statistical relationship ($\\rho$ or $r$)\n",
    "$$ r = \\frac{\\text{Cov}(X,Y)}{\\sigma_X \\sigma_Y} $$\n",
    "\n",
    "Range: $[-1, 1]$. $r=1$ perfect positive linear relationship.\n",
    "\n",
    "**Causation:** $X$ causes $Y$ requires:\n",
    "1. Temporal precedence ($X$ before $Y$)\n",
    "2. Covariation (correlation)\n",
    "3. Elimination of confounds (controlled experiments, instrumental variables, or causal inference methods like do-calculus)\n",
    "\n",
    "**Simpson's Paradox:** Trend appears in subgroups but disappears/reverses in aggregate.\n",
    "\n",
    "```python\n",
    "# Correlation example\n",
    "x = np.random.randn(100)\n",
    "y = 2 * x + np.random.randn(100) * 0.5  # y depends on x with noise\n",
    "\n",
    "correlation = np.corrcoef(x, y)[0, 1]\n",
    "print(f\"Correlation: {correlation:.4f}\")\n",
    "\n",
    "# Spurious correlation example (both depend on time)\n",
    "time = np.arange(100)\n",
    "ice_cream_sales = 10 + 0.5 * time + np.random.randn(100)\n",
    "drowning_deaths = 5 + 0.3 * time + np.random.randn(100)\n",
    "\n",
    "spurious_corr = np.corrcoef(ice_cream_sales, drowning_deaths)[0, 1]\n",
    "print(f\"Spurious correlation (ice cream vs drowning): {spurious_corr:.4f}\")\n",
    "# High correlation! But ice cream doesn't cause drowning (confound: summer/temperature)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **1.6 Workbook Labs**\n",
    "\n",
    "Complete these labs to solidify understanding. Solutions should be committed to your GitHub portfolio.\n",
    "\n",
    "### **Lab 1: Linear Algebra from Scratch**\n",
    "Implement matrix multiplication, transpose, and inverse without using `np.linalg` (use only basic loops and NumPy indexing). Verify against NumPy implementations.\n",
    "\n",
    "**Deliverable:** `linear_algebra_scratch.py` with functions `matmul_custom()`, `transpose_custom()`, and comparison tests.\n",
    "\n",
    "### **Lab 2: Gradient Descent Visualizer**\n",
    "Implement gradient descent on $f(x, y) = x^2 + 2y^2$. Track the path taken from initial point $(2, 3)$. Plot contours of the function and overlay the optimization path using Matplotlib.\n",
    "\n",
    "**Deliverable:** Jupyter notebook with visualization and convergence analysis for different learning rates (0.01, 0.1, 0.5, 1.0).\n",
    "\n",
    "### **Lab 3: Principal Component Analysis (PCA)**\n",
    "Using only eigenvalue decomposition (no `sklearn`), implement PCA on the Iris dataset.\n",
    "1. Center the data\n",
    "2. Compute covariance matrix\n",
    "3. Find eigenvectors/values\n",
    "4. Project data to 2D\n",
    "5. Plot with colors for species\n",
    "\n",
    "**Deliverable:** `pca_from_scratch.py` + comparison plot with `sklearn.decomposition.PCA`.\n",
    "\n",
    "### **Lab 4: Maximum Likelihood Estimation**\n",
    "Given coin flip data (0=tail, 1=head), implement gradient ascent to find MLE of probability $p$ of heads. Compare with analytical solution ($\\hat{p} = \\frac{\\text{#heads}}{n}$).\n",
    "\n",
    "**Deliverable:** Notebook showing convergence of optimization and final probability estimate.\n",
    "\n",
    "### **Lab 5: Statistical Significance in Model Selection**\n",
    "You have accuracy scores from 5-fold cross-validation for three algorithms. Perform ANOVA test to check if any difference exists, then pairwise t-tests with Bonferroni correction to identify which differ significantly.\n",
    "\n",
    "**Deliverable:** `model_comparison.py` with statistical report.\n",
    "\n",
    "---\n",
    "\n",
    "## **1.7 Common Pitfalls**\n",
    "\n",
    "1. **Matrix Dimension Mismatch:** Always check shapes. $(m \\times n) \\cdot (n \\times p) = (m \\times p)$. Common error: forgetting to transpose.\n",
    "   \n",
    "2. **Confusing Row vs Column Vectors:** NumPy 1D arrays are neither (shape `(n,)`). Explicitly reshape to `(n,1)` or `(1,n)` for matrix operations.\n",
    "\n",
    "3. **Numerical Instability:** Computing softmax as $\\frac{e^x}{\\sum e^x}$ overflows for large $x$. Solution: subtract $\\max(x)$ before exponentiation.\n",
    "\n",
    "4. **Sample vs Population Std:** Use `ddof=1` (Delta Degrees of Freedom) for sample standard deviation, `ddof=0` for population/MLE.\n",
    "\n",
    "5. **p-hacking:** Running tests until you find significance. Always pre-register hypotheses or use correction methods.\n",
    "\n",
    "6. **Assuming Causation:** \"Users who click recommendations have higher LTV\" doesn't mean \"Make them click more\" increases LTV (selection bias).\n",
    "\n",
    "---\n",
    "\n",
    "## **1.8 Interview Questions**\n",
    "\n",
    "**Q1:** Why is matrix multiplication not commutative? Give an ML example where order matters.\n",
    "*A: Generally $\\mathbf{A}\\mathbf{B} \\neq \\mathbf{B}\\mathbf{A}$. In neural networks, applying rotation then scaling is different from scaling then rotation. In backpropagation, weight matrices must multiply gradients in correct order (chain rule direction).*\n",
    "\n",
    "**Q2:** What is the geometric interpretation of eigenvalues/eigenvectors? Why are they important in PCA?\n",
    "*A: Eigenvectors are directions invariant under linear transformation; eigenvalues are scaling factors along those directions. In PCA, eigenvectors of covariance matrix are principal directions of variance; eigenvalues indicate variance magnitude.*\n",
    "\n",
    "**Q3:** Explain the chain rule and why it's crucial for neural networks.\n",
    "*A: Chain rule allows computing derivatives of composite functions. Neural networks are compositions of layers $f_n(f_{n-1}(...f_1(x)))$. Backpropagation applies chain rule iteratively from output to input to compute gradients efficiently.*\n",
    "\n",
    "**Q4:** What's the difference between L1 and L2 regularization mathematically? Why does L1 induce sparsity?\n",
    "*A: L1 adds $\\lambda \\sum |w_i|$ to loss; L2 adds $\\lambda \\sum w_i^2$. L1 has diamond-shaped level sets with corners at axes, pushing weights to exactly zero. L2 has circular level sets, shrinking weights uniformly but rarely to zero.*\n",
    "\n",
    "**Q5:** A medical test is 99% accurate. You test positive. What's the probability you're sick?\n",
    "*A: Depends on base rate (Bayes' theorem). If disease is rare (0.1% prevalence), even with 99% accuracy, false positives outnumber true positives, so posterior probability might be <10%. This illustrates importance of prior probability.*\n",
    "\n",
    "---\n",
    "\n",
    "## **1.9 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Mathematics for Machine Learning* (Deisenroth, Faisal, Ong) - Free PDF available\n",
    "- *The Elements of Statistical Learning* (Hastie, Tibshirani, Friedman) - Chapter 2 (Math prerequisites)\n",
    "- *Deep Learning* (Goodfellow, Bengio, Courville) - Part I (Applied Math)\n",
    "\n",
    "**Courses:**\n",
    "- Khan Academy: Linear Algebra, Calculus, Statistics\n",
    "- MIT 18.06 (Linear Algebra) - Gilbert Strang\n",
    "- Stanford CS229 (Andrew Ng) - Mathematical foundations of ML\n",
    "\n",
    "**Interactive:**\n",
    "- 3Blue1Brown \"Essence of Linear Algebra\" and \"Essence of Calculus\" (YouTube)\n",
    "- Distill.pub: \"Why Momentum Really Works\" (optimization visualizations)\n",
    "\n",
    "---\n",
    "\n",
    "## **1.10 Checkpoint Project: Mathematical Toolkit Library**\n",
    "\n",
    "Build a Python package `ai_math_toolkit` that implements:\n",
    "\n",
    "**Module 1: `linalg_tools.py`**\n",
    "- Custom matrix operations (no NumPy linalg)\n",
    "- Singular Value Decomposition (SVD) using power iteration\n",
    "- Function to check if matrix is positive definite\n",
    "\n",
    "**Module 2: `autodiff.py`**\n",
    "- Simple automatic differentiation engine using computational graphs\n",
    "- Support for +, *, sin, exp, log operations\n",
    "- Verify gradients against numerical differentiation\n",
    "\n",
    "**Module 3: `probability_distributions.py`**\n",
    "- Classes for Gaussian, Bernoulli, Categorical distributions\n",
    "- Methods for PDF/PMF, sampling, MLE fitting\n",
    "- KL-divergence calculation between distributions\n",
    "\n",
    "**Module 4: `stats_tests.py`**\n",
    "- Implement t-test from scratch\n",
    "- Bootstrap confidence intervals\n",
    "- Permutation test for independence\n",
    "\n",
    "**Deliverables:**\n",
    "- GitHub repository with tests (pytest)\n",
    "- Documentation showing mathematical formulas implemented\n",
    "- Jupyter notebook demonstrating usage on real dataset (e.g., comparing two marketing campaigns)\n",
    "\n",
    "**Success Criteria:**\n",
    "- All functions pass unit tests against scipy/numpy equivalents (within numerical tolerance)\n",
    "- Code is vectorized (no Python loops for matrix ops)\n",
    "- Documentation includes mathematical derivation in LaTeX\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 1**\n",
    "\n",
    "*Proceed to Chapter 2 only after completing all labs and the Checkpoint Project. The mathematical intuition built here is the foundation upon which all neural network architectures rest.*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
