{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "692edf69",
   "metadata": {},
   "source": [
    "Here is **Chapter 2: Python for AI Development** — the practical programming foundation.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 2: PYTHON FOR AI DEVELOPMENT**\n",
    "\n",
    "*The Craftsman's Toolkit*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Mathematics provides the theory; Python provides the tools. This chapter transforms you from a Python user into a Python engineer. We focus on the specific patterns, libraries, and optimizations used in production AI systems, not generic programming tutorials.\n",
    "\n",
    "**Estimated Time:** 50-60 hours (3-4 weeks)  \n",
    "**Prerequisites:** Chapter 1 (Math foundations), basic programming logic\n",
    "\n",
    "---\n",
    "\n",
    "## **2.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Write Pythonic code using advanced features (decorators, generators, context managers)\n",
    "2. Manipulate multi-dimensional data with NumPy at C-speed (vectorization)\n",
    "3. Perform complex data transformations with Pandas (groupby, merge, pivot, apply)\n",
    "4. Create publication-quality visualizations for model analysis and EDA\n",
    "5. Optimize Jupyter workflows for large-scale experimentation\n",
    "6. Structure ML projects following industry standards (src, tests, configs)\n",
    "\n",
    "---\n",
    "\n",
    "## **2.1 Python Fundamentals: The AI Engineer's Way**\n",
    "\n",
    "We assume basic syntax (variables, loops, conditionals). Here we focus on Python patterns essential for ML codebases.\n",
    "\n",
    "#### **2.1.1 Data Structures for ML**\n",
    "\n",
    "**Lists vs Tuples vs Sets:**\n",
    "- **Lists:** Mutable, ordered. Use for sequences of samples (but convert to arrays for math).\n",
    "- **Tuples:** Immutable, hashable. Use for dataset samples (image_path, label), dictionary keys.\n",
    "- **Sets:** O(1) lookup. Use for train/val/test split verification, unique label collections.\n",
    "\n",
    "```python\n",
    "# Train/validation leakage check (critical in ML)\n",
    "train_ids = set(train_df['user_id'])\n",
    "val_ids = set(val_df['user_id'])\n",
    "\n",
    "if train_ids & val_ids:  # Intersection\n",
    "    raise ValueError(f\"Data leakage! {len(train_ids & val_ids)} IDs in both splits\")\n",
    "\n",
    "# Tuple unpacking for dataset samples\n",
    "sample = (\"image_001.jpg\", 5, 0.8)  # (path, class_idx, confidence)\n",
    "path, label, conf = sample\n",
    "```\n",
    "\n",
    "**Dictionaries: The Config King**\n",
    "ML experiments are dictionaries of hyperparameters. Master dictionary operations.\n",
    "\n",
    "```python\n",
    "config = {\n",
    "    'model': 'ResNet50',\n",
    "    'lr': 0.001,\n",
    "    'batch_size': 32,\n",
    "    'augmentation': {\n",
    "        'rotation': 15,\n",
    "        'flip': True\n",
    "    }\n",
    "}\n",
    "\n",
    "# Safe nested access with get()\n",
    "rotation = config.get('augmentation', {}).get('rotation', 0)\n",
    "\n",
    "# Dictionary merging (Python 3.9+)\n",
    "default_config = {'lr': 0.01, 'epochs': 10}\n",
    "user_config = {'lr': 0.001}\n",
    "final_config = default_config | user_config  # {'lr': 0.001, 'epochs': 10}\n",
    "\n",
    "# Dictionary comprehension for feature engineering\n",
    "feature_means = {f\"feature_{i}\": 0.0 for i in range(100)}\n",
    "```\n",
    "\n",
    "**Collections Module:**\n",
    "```python\n",
    "from collections import defaultdict, Counter, namedtuple\n",
    "\n",
    "# Count class distributions (imbalanced datasets)\n",
    "labels = [0, 1, 1, 2, 0, 1, 2, 2, 2]\n",
    "class_dist = Counter(labels)\n",
    "print(class_dist)  # Counter({2: 4, 1: 3, 0: 2})\n",
    "\n",
    "# Defaultdict for grouping samples by label\n",
    "samples_by_class = defaultdict(list)\n",
    "for sample, label in zip(image_paths, labels):\n",
    "    samples_by_class[label].append(sample)\n",
    "\n",
    "# NamedTuple for type-safe dataset items (better than plain tuples)\n",
    "Sample = namedtuple('Sample', ['image', 'label', 'metadata'])\n",
    "sample = Sample(image=array([...]), label=5, metadata={'source': 'camera_1'})\n",
    "```\n",
    "\n",
    "#### **2.1.2 List Comprehensions and Generators**\n",
    "\n",
    "**Memory-Efficient Data Processing:**\n",
    "ML datasets don't fit in RAM. Use generators for lazy loading.\n",
    "\n",
    "```python\n",
    "# BAD: Loads all 1M images into memory\n",
    "images = [load_image(path) for path in all_paths]  \n",
    "\n",
    "# GOOD: Generator yields one at a time\n",
    "def image_generator(paths):\n",
    "    for path in paths:\n",
    "        yield load_image(path)  # Lazy evaluation\n",
    "\n",
    "# Process in batches without loading everything\n",
    "batch = []\n",
    "for img in image_generator(paths):\n",
    "    batch.append(img)\n",
    "    if len(batch) == 32:\n",
    "        process_batch(batch)\n",
    "        batch = []\n",
    "```\n",
    "\n",
    "**Generator Expressions vs List Comprehensions:**\n",
    "```python\n",
    "# List comprehension (eager, high memory)\n",
    "squared = [x**2 for x in range(10_000_000)]  \n",
    "\n",
    "# Generator expression (lazy, low memory)\n",
    "squared_gen = (x**2 for x in range(10_000_000))  \n",
    "\n",
    "sum_of_squares = sum(squared_gen)  # Computes on the fly\n",
    "```\n",
    "\n",
    "#### **2.1.3 Object-Oriented Programming for ML**\n",
    "\n",
    "Design classes that encapsulate data processing logic, following PyTorch/TensorFlow patterns.\n",
    "\n",
    "```python\n",
    "from abc import ABC, abstractmethod\n",
    "import numpy as np\n",
    "\n",
    "class Dataset(ABC):\n",
    "    \"\"\"Abstract base class for datasets (similar to PyTorch's Dataset)\"\"\"\n",
    "    \n",
    "    def __init__(self, data_dir: str, transform=None):\n",
    "        self.data_dir = data_dir\n",
    "        self.transform = transform\n",
    "        self.samples = self._load_samples()\n",
    "    \n",
    "    @abstractmethod\n",
    "    def _load_samples(self):\n",
    "        \"\"\"Subclasses must implement data loading\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.samples)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.samples[idx]\n",
    "        if self.transform:\n",
    "            sample = self.transform(sample)\n",
    "        return sample\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def _load_samples(self):\n",
    "        # Implementation specific to images\n",
    "        return [...]  # List of (image_path, label)\n",
    "\n",
    "# Usage\n",
    "dataset = ImageDataset(\"./data\", transform=normalize)\n",
    "batch = [dataset[i] for i in range(4)]  # Mini-batch\n",
    "```\n",
    "\n",
    "**Dataclasses (Python 3.7+):**\n",
    "Cleaner than namedtuples for config objects.\n",
    "```python\n",
    "from dataclasses import dataclass, field\n",
    "from typing import List\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfig:\n",
    "    lr: float = 0.001\n",
    "    batch_size: int = 32\n",
    "    layers: List[int] = field(default_factory=lambda: [256, 128, 64])\n",
    "    device: str = \"cuda\"\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.lr <= 0:\n",
    "            raise ValueError(\"Learning rate must be positive\")\n",
    "\n",
    "config = TrainingConfig(lr=0.01)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2.2 Advanced Python: The Professional Edge**\n",
    "\n",
    "#### **2.2.1 Decorators for ML Workflows**\n",
    "\n",
    "Decorators wrap functions to add logging, timing, or validation without modifying core logic.\n",
    "\n",
    "```python\n",
    "import time\n",
    "import functools\n",
    "from typing import Callable\n",
    "\n",
    "def timer(func: Callable) -> Callable:\n",
    "    \"\"\"Decorator to measure function execution time\"\"\"\n",
    "    @functools.wraps(func)  # Preserves function metadata\n",
    "    def wrapper(*args, **kwargs):\n",
    "        start = time.perf_counter()\n",
    "        result = func(*args, **kwargs)\n",
    "        end = time.perf_counter()\n",
    "        print(f\"{func.__name__} took {end - start:.4f}s\")\n",
    "        return result\n",
    "    return wrapper\n",
    "\n",
    "def validate_shapes(*expected_shapes):\n",
    "    \"\"\"Decorator to validate tensor shapes\"\"\"\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for i, (arg, shape) in enumerate(zip(args, expected_shapes)):\n",
    "                if hasattr(arg, 'shape') and arg.shape != shape:\n",
    "                    raise ValueError(f\"Arg {i}: expected {shape}, got {arg.shape}\")\n",
    "            return func(*args, **kwargs)\n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "# Usage\n",
    "@timer\n",
    "@validate_shapes((None, 784), (None, 10))  # (batch, features), (batch, classes)\n",
    "def train_step(X, y):\n",
    "    # Training logic\n",
    "    time.sleep(0.1)  # Simulate work\n",
    "    return {\"loss\": 0.5}\n",
    "\n",
    "# Decorator for caching expensive computations (memoization)\n",
    "@functools.lru_cache(maxsize=128)\n",
    "def compute_features(image_path: str):\n",
    "    \"\"\"Cache feature extraction for repeated images\"\"\"\n",
    "    return expensive_feature_extraction(image_path)\n",
    "```\n",
    "\n",
    "#### **2.2.2 Context Managers for Resource Management**\n",
    "\n",
    "Essential for managing GPU memory, file handles, and database connections.\n",
    "\n",
    "```python\n",
    "from contextlib import contextmanager\n",
    "import torch\n",
    "\n",
    "@contextmanager\n",
    "def gpu_memory_tracker():\n",
    "    \"\"\"Context manager to track GPU memory usage\"\"\"\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.reset_peak_memory_stats()\n",
    "        start_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        yield\n",
    "        end_mem = torch.cuda.memory_allocated() / 1024**2\n",
    "        peak_mem = torch.cuda.max_memory_allocated() / 1024**2\n",
    "        print(f\"GPU Memory: {end_mem - start_mem:.2f}MB allocated, Peak: {peak_mem:.2f}MB\")\n",
    "    else:\n",
    "        yield\n",
    "\n",
    "# Usage\n",
    "with gpu_memory_tracker():\n",
    "    model = LargeNeuralNetwork().cuda()\n",
    "    output = model(batch)  # Memory tracked automatically\n",
    "\n",
    "# Built-in context managers\n",
    "with open('data.txt', 'r') as f:  # Auto-closes file\n",
    "    data = f.read()\n",
    "\n",
    "# Multiple context managers\n",
    "with torch.no_grad(), gpu_memory_tracker():  # No gradient calc + memory tracking\n",
    "    predictions = model(inputs)\n",
    "```\n",
    "\n",
    "#### **2.2.3 Multiprocessing for Data Loading**\n",
    "\n",
    "The GIL (Global Interpreter Lock) prevents true thread parallelism in Python. For CPU-bound ML preprocessing, use `multiprocessing`.\n",
    "\n",
    "```python\n",
    "from multiprocessing import Pool, cpu_count\n",
    "import numpy as np\n",
    "\n",
    "def augment_image(args):\n",
    "    \"\"\"Function to apply augmentation (CPU-intensive)\"\"\"\n",
    "    image, seed = args\n",
    "    np.random.seed(seed)\n",
    "    # Rotation, flip, etc.\n",
    "    return processed_image\n",
    "\n",
    "# Parallel data augmentation\n",
    "with Pool(processes=cpu_count()) as pool:\n",
    "    images = [...]  # List of images\n",
    "    seeds = range(len(images))\n",
    "    augmented = pool.map(augment_image, zip(images, seeds))\n",
    "```\n",
    "\n",
    "**Threading for I/O Bound:**\n",
    "Use threads (not processes) for downloading data or reading files.\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "def download_url(url):\n",
    "    # HTTP request\n",
    "    pass\n",
    "\n",
    "urls = [...]  # 1000 URLs\n",
    "with ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(executor.map(download_url, urls))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2.3 Scientific Stack: NumPy Mastery**\n",
    "\n",
    "NumPy is the foundation of all Python ML. If you can't write vectorized NumPy, you can't do efficient ML.\n",
    "\n",
    "#### **2.3.1 Broadcasting and Vectorization**\n",
    "\n",
    "**The Golden Rule:** Never write Python loops over arrays. Use broadcasting.\n",
    "\n",
    "```python\n",
    "# BAD: Python loop (1000x slower)\n",
    "def normalize_loop(data):\n",
    "    result = np.empty_like(data)\n",
    "    for i in range(data.shape[0]):\n",
    "        for j in range(data.shape[1]):\n",
    "            result[i,j] = (data[i,j] - mean[j]) / std[j]\n",
    "    return result\n",
    "\n",
    "# GOOD: Vectorized (uses C loops)\n",
    "def normalize_vectorized(data):\n",
    "    return (data - mean) / std  # Broadcasting!\n",
    "\n",
    "# Broadcasting rules:\n",
    "# 1. Align dimensions from right\n",
    "# 2. Dimensions must be equal or one of them is 1\n",
    "# 3. Expand dimensions of size 1\n",
    "\n",
    "# Example: Add bias to each sample in batch\n",
    "batch = np.random.rand(100, 784)  # (batch, features)\n",
    "bias = np.random.rand(784)        # (features,)\n",
    "\n",
    "# Broadcasting automatically expands bias to (100, 784)\n",
    "output = batch + bias  # No explicit loop!\n",
    "```\n",
    "\n",
    "**Advanced Indexing:**\n",
    "```python\n",
    "arr = np.arange(10)\n",
    "\n",
    "# Fancy indexing\n",
    "indices = [1, 3, 5, 7]\n",
    "subset = arr[indices]  # [1, 3, 5, 7]\n",
    "\n",
    "# Boolean masking (filtering)\n",
    "mask = arr > 5\n",
    "filtered = arr[mask]  # [6, 7, 8, 9]\n",
    "\n",
    "# Useful for train/validation split\n",
    "indices = np.random.permutation(len(data))\n",
    "train_idx = indices[:800]\n",
    "val_idx = indices[800:]\n",
    "train_data = data[train_idx]\n",
    "val_data = data[val_idx]\n",
    "\n",
    "# np.where for conditional assignment\n",
    "discounts = np.where(customer_spend > 1000, 0.1, 0.05)  # Vectorized if-else\n",
    "```\n",
    "\n",
    "#### **2.3.2 Memory Layout and Views**\n",
    "\n",
    "Understanding memory layout prevents expensive copies.\n",
    "\n",
    "```python\n",
    "# C-order (row-major) vs F-order (column-major)\n",
    "arr_c = np.array([[1, 2], [3, 4]], order='C')  # Rows contiguous\n",
    "arr_f = np.array([[1, 2], [3, 4]], order='F')  # Columns contiguous\n",
    "\n",
    "# Transpose is a view (O(1)), not a copy\n",
    "arr_t = arr_c.T\n",
    "print(arr_t.flags['OWNDATA'])  # False (it's a view)\n",
    "\n",
    "# BUT: Reshaping after transpose may force copy\n",
    "arr_flat = arr_t.flatten()  # Creates copy\n",
    "arr_flat_view = arr_t.ravel()  # View when possible\n",
    "\n",
    "# Stride tricks for sliding windows (efficient for time series)\n",
    "def sliding_window(arr, window):\n",
    "    shape = (arr.size - window + 1, window)\n",
    "    strides = (arr.strides[0], arr.strides[0])\n",
    "    return np.lib.stride_tricks.as_strided(arr, shape=shape, strides=strides)\n",
    "```\n",
    "\n",
    "#### **2.3.3 Structured Arrays and Record Arrays**\n",
    "\n",
    "For heterogeneous data (like pandas but lighter):\n",
    "```python\n",
    "# Structured array for dataset metadata\n",
    "dt = np.dtype([('image_id', 'U10'), ('label', 'i4'), ('confidence', 'f4')])\n",
    "data = np.array([('img001', 5, 0.95), ('img002', 3, 0.87)], dtype=dt)\n",
    "\n",
    "# Access by field\n",
    "labels = data['label']  # array([5, 3])\n",
    "high_conf = data[data['confidence'] > 0.9]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2.4 Data Manipulation: Pandas for ETL**\n",
    "\n",
    "Pandas is the lingua franca of data preprocessing. Master `groupby`, `merge`, and `apply`.\n",
    "\n",
    "#### **2.4.1 Efficient Data Loading**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Optimize types on load (reduce memory by 90%)\n",
    "dtypes = {\n",
    "    'user_id': 'int32',\n",
    "    'category': 'category',  # Categorical instead of string\n",
    "    'click': 'int8',         # 0/1 fits in int8\n",
    "    'price': 'float32'\n",
    "}\n",
    "\n",
    "df = pd.read_csv('large_dataset.csv', dtype=dtypes, parse_dates=['timestamp'])\n",
    "\n",
    "# Chunking for files larger than RAM\n",
    "chunks = []\n",
    "for chunk in pd.read_csv('huge_file.csv', chunksize=10000):\n",
    "    processed = chunk[chunk['value'] > 0]  # Filter\n",
    "    chunks.append(processed)\n",
    "df = pd.concat(chunks)\n",
    "```\n",
    "\n",
    "#### **2.4.2 Data Transformation**\n",
    "\n",
    "```python\n",
    "# Groupby operations (aggregation)\n",
    "stats = df.groupby('category').agg({\n",
    "    'price': ['mean', 'std', 'count'],\n",
    "    'click': 'sum'\n",
    "}).reset_index()\n",
    "\n",
    "# Pivot tables (feature engineering)\n",
    "pivot = df.pivot_table(\n",
    "    values='click', \n",
    "    index='user_id', \n",
    "    columns='hour_of_day', \n",
    "    aggfunc='sum',\n",
    "    fill_value=0\n",
    ")\n",
    "\n",
    "# Merge strategies (SQL joins)\n",
    "merged = pd.merge(\n",
    "    df1, df2, \n",
    "    on='user_id', \n",
    "    how='left',      # Keep all df1 rows\n",
    "    indicator=True   # Show match status\n",
    ")\n",
    "\n",
    "# Window functions (time series)\n",
    "df['rolling_mean'] = df.groupby('user_id')['value'].transform(\n",
    "    lambda x: x.rolling(window=7, min_periods=1).mean()\n",
    ")\n",
    "\n",
    "# Vectorized string operations (faster than .apply())\n",
    "df['clean_text'] = df['text'].str.lower().str.replace(r'[^\\w\\s]', '', regex=True)\n",
    "```\n",
    "\n",
    "#### **2.4.3 The Split-Apply-Combine Pattern**\n",
    "\n",
    "The most common pattern in feature engineering:\n",
    "```python\n",
    "def complex_feature_engineering(group):\n",
    "    \"\"\"Apply complex logic to each user group\"\"\"\n",
    "    group = group.sort_values('timestamp')\n",
    "    group['time_since_last'] = group['timestamp'].diff().dt.seconds\n",
    "    group['cumulative_spend'] = group['amount'].cumsum()\n",
    "    group['is_weekend'] = group['timestamp'].dt.weekday >= 5\n",
    "    return group\n",
    "\n",
    "# Apply to each user group efficiently\n",
    "df = df.groupby('user_id').apply(complex_feature_engineering)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2.5 Visualization: Telling Stories with Data**\n",
    "\n",
    "#### **2.5.1 Matplotlib: The Foundation**\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Rectangle\n",
    "\n",
    "# Figure with subplots\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# 1. Training curves (line plot)\n",
    "axes[0,0].plot(epochs, train_loss, label='Train', linewidth=2)\n",
    "axes[0,0].plot(epochs, val_loss, label='Validation', linestyle='--')\n",
    "axes[0,0].set_xlabel('Epoch')\n",
    "axes[0,0].set_ylabel('Loss')\n",
    "axes[0,0].legend()\n",
    "axes[0,0].grid(True, alpha=0.3)\n",
    "\n",
    "# 2. Feature distributions (histogram)\n",
    "axes[0,1].hist(class_0_features, bins=50, alpha=0.5, label='Class 0', density=True)\n",
    "axes[0,1].hist(class_1_features, bins=50, alpha=0.5, label='Class 1', density=True)\n",
    "axes[0,1].set_title('Feature Distribution by Class')\n",
    "\n",
    "# 3. Confusion Matrix (heatmap)\n",
    "import seaborn as sns\n",
    "cm = [[50, 10], [5, 80]]\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[1,0])\n",
    "axes[1,0].set_xlabel('Predicted')\n",
    "axes[1,0].set_ylabel('Actual')\n",
    "\n",
    "# 4. Embedding visualization (scatter)\n",
    "from sklearn.decomposition import PCA\n",
    "embeddings_2d = PCA(n_components=2).fit_transform(embeddings)\n",
    "scatter = axes[1,1].scatter(embeddings_2d[:,0], embeddings_2d[:,1], \n",
    "                           c=labels, cmap='tab10', alpha=0.6)\n",
    "axes[1,1].set_title('Embedding Space')\n",
    "plt.colorbar(scatter, ax=axes[1,1])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('training_analysis.png', dpi=300, bbox_inches='tight')\n",
    "```\n",
    "\n",
    "#### **2.5.2 Seaborn for Statistical Plots**\n",
    "\n",
    "```python\n",
    "import seaborn as sns\n",
    "\n",
    "# Correlation heatmap\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df.corr(), annot=True, cmap='coolwarm', center=0, fmt='.2f')\n",
    "\n",
    "# Box plots for outlier detection\n",
    "sns.boxplot(data=df, x='category', y='value')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Pairplot for feature relationships (small datasets)\n",
    "sns.pairplot(df[['feat1', 'feat2', 'feat3', 'target']], hue='target')\n",
    "\n",
    "# Violin plot for distribution comparison\n",
    "sns.violinplot(data=df, x='class', y='probability')\n",
    "```\n",
    "\n",
    "#### **2.5.3 Interactive Visualization (Plotly)**\n",
    "\n",
    "For Jupyter dashboards and hyperparameter exploration:\n",
    "```python\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Interactive 3D scatter of embeddings\n",
    "fig = px.scatter_3d(\n",
    "    df, x='comp1', y='comp2', z='comp3',\n",
    "    color='label', size='confidence',\n",
    "    hover_data=['sample_id']\n",
    ")\n",
    "fig.show()\n",
    "\n",
    "# Parallel coordinates for hyperparameter tuning\n",
    "fig = go.Figure(data=go.Parcoords(\n",
    "    line=dict(color=df['accuracy'], colorscale='Viridis'),\n",
    "    dimensions=[\n",
    "        dict(range=[0, 0.01], label='LR', values=df['lr']),\n",
    "        dict(range=[16, 128], label='Batch Size', values=df['batch_size']),\n",
    "        dict(range=[0, 1], label='Accuracy', values=df['accuracy'])\n",
    "    ]\n",
    "))\n",
    "fig.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2.6 Jupyter Ecosystem: The Experimentation Lab**\n",
    "\n",
    "#### **2.6.1 Magic Commands**\n",
    "\n",
    "```python\n",
    "# Timing\n",
    "%timeit np.random.rand(1000, 1000)  # Single line\n",
    "%%timeit  # Cell magic\n",
    "model.fit(X, y)\n",
    "\n",
    "# Debugging\n",
    "%debug  # Enter debugger on exception\n",
    "%pdb on  # Auto-debug on exception\n",
    "\n",
    "# Profiling\n",
    "%prun model.fit(X, y)  # CPU profiling\n",
    "\n",
    "# Memory profiling (requires pip install memory_profiler)\n",
    "%load_ext memory_profiler\n",
    "%memit model.predict(X_large)\n",
    "\n",
    "# Shell commands\n",
    "!pip install torch\n",
    "files = !ls -lh models/\n",
    "\n",
    "# Writing files\n",
    "%%writefile train.py\n",
    "import torch\n",
    "# ... your training script\n",
    "\n",
    "# HTML display\n",
    "from IPython.display import HTML, Image, display\n",
    "display(HTML('<h1>Training Complete</h1>'))\n",
    "```\n",
    "\n",
    "#### **2.6.2 Widgets for Interactive ML**\n",
    "\n",
    "```python\n",
    "from ipywidgets import interact, FloatSlider, Dropdown\n",
    "\n",
    "@interact\n",
    "def explore_threshold(threshold=FloatSlider(min=0, max=1, step=0.05, value=0.5)):\n",
    "    preds = (model_proba > threshold).astype(int)\n",
    "    f1 = f1_score(y_true, preds)\n",
    "    print(f\"Threshold: {threshold:.2f}, F1: {f1:.3f}\")\n",
    "    plot_confusion_matrix(y_true, preds)\n",
    "\n",
    "# Hyperparameter explorer\n",
    "@interact(\n",
    "    lr=Dropdown(options=[0.1, 0.01, 0.001], value=0.01),\n",
    "    optimizer=['adam', 'sgd'],\n",
    "    epochs=(1, 10)\n",
    ")\n",
    "def quick_train(lr, optimizer, epochs):\n",
    "    # Quick training run\n",
    "    history = train_model(lr=lr, optimizer=optimizer, epochs=epochs)\n",
    "    plot_history(history)\n",
    "```\n",
    "\n",
    "#### **2.6.3 Jupyter Best Practices**\n",
    "\n",
    "**1. Cell Organization:**\n",
    "```python\n",
    "# Cell 1: Imports\n",
    "# Cell 2: Configuration\n",
    "# Cell 3: Data Loading\n",
    "# Cell 4: EDA\n",
    "# Cell 5: Model Definition\n",
    "# Cell 6: Training\n",
    "# Cell 7: Evaluation\n",
    "```\n",
    "\n",
    "**2. Auto-reload for Development:**\n",
    "```python\n",
    "%load_ext autoreload\n",
    "%autoreload 2  # Reload modules before executing code\n",
    "\n",
    "# Now changes to local .py files are immediately available\n",
    "from my_module import Model\n",
    "```\n",
    "\n",
    "**3. Progress Bars:**\n",
    "```python\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "for epoch in tqdm(range(100), desc=\"Training\"):\n",
    "    for batch in tqdm(train_loader, desc=f\"Epoch {epoch}\", leave=False):\n",
    "        # Training step\n",
    "        pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **2.7 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Custom DataLoader Implementation**\n",
    "Implement a `DataLoader` class that supports:\n",
    "- Batch generation with configurable size\n",
    "- Shuffling (without loading all data into memory)\n",
    "- Custom collate functions for variable-length sequences\n",
    "- Multiprocessing for data augmentation\n",
    "\n",
    "**Deliverable:** `custom_dataloader.py` that passes unit tests against PyTorch's DataLoader interface (without using PyTorch).\n",
    "\n",
    "### **Lab 2: Pandas ETL Pipeline**\n",
    "Given a 5GB CSV of e-commerce transactions:\n",
    "1. Load in chunks with memory optimization (target: <2GB RAM usage)\n",
    "2. Engineer 10 features including time-based aggregations\n",
    "3. Handle missing values and outliers\n",
    "4. Join with external product metadata\n",
    "5. Export to Parquet format (compressed)\n",
    "\n",
    "**Deliverable:** `etl_pipeline.py` with memory profiling report and processing time benchmarks.\n",
    "\n",
    "### **Lab 3: Visualization Dashboard**\n",
    "Create a Jupyter notebook that generates a comprehensive model report with:\n",
    "- Training/validation curves with confidence bands (multiple runs)\n",
    "- Feature importance bar chart (horizontal)\n",
    "- Confusion matrix normalized by row\n",
    "- ROC curves for all classes (one-vs-rest)\n",
    "- t-SNE visualization of embeddings colored by prediction correctness\n",
    "\n",
    "**Deliverable:** `model_report.ipynb` with synthetic data demonstrating all plots.\n",
    "\n",
    "### **Lab 4: Profiling and Optimization**\n",
    "Given a slow NumPy function that processes images:\n",
    "1. Profile with `%prun` and `line_profiler` to find bottlenecks\n",
    "2. Vectorize the slow loops\n",
    "3. Implement a Cython or Numba JIT-compiled version\n",
    "4. Benchmark all three approaches\n",
    "\n",
    "**Deliverable:** `optimization_comparison.py` with timing results and speedup factors.\n",
    "\n",
    "---\n",
    "\n",
    "## **2.8 Common Pitfalls**\n",
    "\n",
    "1. **Modifying Lists While Iterating:**\n",
    "   ```python\n",
    "   # WRONG\n",
    "   for item in items:\n",
    "       if condition(item):\n",
    "           items.remove(item)  # Skips next item!\n",
    "   \n",
    "   # RIGHT\n",
    "   items = [item for item in items if not condition(item)]\n",
    "   ```\n",
    "\n",
    "2. **Pandas Chained Indexing:**\n",
    "   ```python\n",
    "   # WRONG - SettingWithCopyWarning\n",
    "   df[df['A'] > 0]['B'] = 1\n",
    "   \n",
    "   # RIGHT\n",
    "   df.loc[df['A'] > 0, 'B'] = 1\n",
    "   ```\n",
    "\n",
    "3. **Memory Fragmentation:**\n",
    "   ```python\n",
    "   # BAD - Growing list in loop\n",
    "   result = []\n",
    "   for i in range(1000000):\n",
    "       result.append(i)  # Multiple reallocations\n",
    "   \n",
    "   # BETTER\n",
    "   result = [None] * 1000000\n",
    "   for i in range(1000000):\n",
    "       result[i] = i\n",
    "   ```\n",
    "\n",
    "4. **Not Using Context Managers:**\n",
    "   Leaving file handles or GPU memory unreleased causes crashes in long experiments.\n",
    "\n",
    "5. **Broadcasting Errors:**\n",
    "   ```python\n",
    "   # Silent bug: shapes (100,) and (100, 1) broadcast to (100, 100) instead of element-wise\n",
    "   a = np.random.rand(100)\n",
    "   b = np.random.rand(100, 1)\n",
    "   c = a + b  # Unexpected 2D result!\n",
    "   ```\n",
    "\n",
    "---\n",
    "\n",
    "## **2.9 Interview Questions**\n",
    "\n",
    "**Q1:** Why is NumPy faster than Python lists for numerical operations?\n",
    "*A: NumPy uses contiguous memory blocks (C arrays), vectorized operations (SIMD), and avoids Python interpreter overhead (type checking in loops). Also benefits from CPU cache locality.*\n",
    "\n",
    "**Q2:** Explain the difference between `apply()`, `map()`, and `applymap()` in Pandas.\n",
    "*A: `map()` is for Series (element-wise), `applymap()` is for DataFrames (element-wise), `apply()` works on axis (row/column) for DataFrames or element-wise for Series. `apply()` is flexible but slower than vectorized operations.*\n",
    "\n",
    "**Q3:** How would you handle a dataset larger than RAM in Pandas?\n",
    "*A: Use chunking with `read_csv(chunksize=...)`, Dask for parallel processing, convert to Parquet format (columnar, compressed), or use SQLite for out-of-core SQL operations.*\n",
    "\n",
    "**Q4:** Write a decorator that retries a function 3 times on exception.\n",
    "```python\n",
    "def retry(max_attempts=3):\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(max_attempts):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == max_attempts - 1:\n",
    "                        raise\n",
    "                    print(f\"Retry {attempt + 1}/{max_attempts}\")\n",
    "        return wrapper\n",
    "    return decorator\n",
    "```\n",
    "\n",
    "**Q5:** When should you use multiprocessing vs threading in Python for ML?\n",
    "*A: Multiprocessing for CPU-bound tasks (data augmentation, feature engineering) to bypass GIL. Threading for I/O-bound tasks (downloading, database queries). For deep learning data loading, use `num_workers` in DataLoader (multiprocessing).*\n",
    "\n",
    "---\n",
    "\n",
    "## **2.10 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Effective Python* (Brett Slatkin) - Items 1-30 essential for ML engineers\n",
    "- *Python for Data Analysis* (Wes McKinney) - Pandas creator's guide\n",
    "- *High Performance Python* (Gorelick/Ozsvath) - Optimization techniques\n",
    "\n",
    "**Documentation:**\n",
    "- NumPy Broadcasting: https://numpy.org/doc/stable/user/basics.broadcasting.html\n",
    "- Pandas Scaling: https://pandas.pydata.org/docs/user_guide/scale.html\n",
    "- Python Concurrent Futures: https://docs.python.org/3/library/concurrent.futures.html\n",
    "\n",
    "**Tools:**\n",
    "- `snakeviz` - Visual profiler for Python\n",
    "- `memory_profiler` - Line-by-line memory usage\n",
    "- `black` + `isort` - Code formatting for ML projects\n",
    "\n",
    "---\n",
    "\n",
    "## **2.11 Checkpoint Project: Production-Grade Data Pipeline**\n",
    "\n",
    "Build a complete data preprocessing library for a computer vision classification task:\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "cv_data_pipeline/\n",
    "├── src/\n",
    "│   ├── __init__.py\n",
    "│   ├── dataset.py       # Dataset class with lazy loading\n",
    "│   ├── transforms.py    # Image augmentation pipeline\n",
    "│   ├── utils.py         # Helper functions\n",
    "│   └── profiler.py      # Performance monitoring\n",
    "├── tests/\n",
    "│   ├── test_dataset.py\n",
    "│   └── test_transforms.py\n",
    "├── notebooks/\n",
    "│   └── eda.ipynb        # Exploration of dataset\n",
    "├── setup.py\n",
    "└── requirements.txt\n",
    "```\n",
    "\n",
    "**Requirements:**\n",
    "1. **Dataset Class:**\n",
    "   - Load image paths and labels from CSV\n",
    "   - Support `__getitem__` with on-the-fly loading\n",
    "   - Implement `cache()` method to pre-load small datasets\n",
    "   - Thread-safe data augmentation\n",
    "\n",
    "2. **Transform Pipeline:**\n",
    "   - Compose multiple transforms (resize, normalize, augment)\n",
    "   - Decorator-based timing for each transform\n",
    "   - Validation of output shapes\n",
    "\n",
    "3. **Analysis Tools:**\n",
    "   - Class distribution visualization\n",
    "   - Image size analysis (detect outliers)\n",
    "   - Pixel value statistics (mean/std per channel)\n",
    "\n",
    "4. **Performance:**\n",
    "   - Process 10,000 images in <2 minutes (single thread)\n",
    "   - Memory usage <4GB for 100k image metadata\n",
    "   - Progress bars and ETA estimation\n",
    "\n",
    "5. **Testing:**\n",
    "   - Unit tests for all transforms (deterministic output given seed)\n",
    "   - Integration test for full pipeline\n",
    "   - Benchmark test with `pytest-benchmark`\n",
    "\n",
    "**Deliverables:**\n",
    "- GitHub repository with CI/CD (GitHub Actions running pytest)\n",
    "- README with installation and usage examples\n",
    "- Benchmark report comparing your pipeline against `torchvision.datasets`\n",
    "\n",
    "**Evaluation Criteria:**\n",
    "- Code follows PEP8 and type hints\n",
    "- No Python loops in numerical operations (vectorized NumPy)\n",
    "- Handles edge cases (missing images, corrupted files, empty batches)\n",
    "- Documentation strings follow Google/NumPy style\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 2**\n",
    "\n",
    "*You now possess the Python engineering skills to build production ML systems. Chapter 3 will cover Computer Science Fundamentals (Data Structures & Algorithms) specifically tailored for ML system design and optimization.*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
