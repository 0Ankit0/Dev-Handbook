{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "815022b2",
   "metadata": {},
   "source": [
    "Here is **Chapter 4: Development Environment & Tools** \u2014 the infrastructure foundation for production AI.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 4: DEVELOPMENT ENVIRONMENT & TOOLS**\n",
    "\n",
    "*The Professional's Workshop*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Great models are built on messy laptops but deployed through rigorous engineering pipelines. This chapter transforms you from a notebook experimenter into a production engineer who can version control 10GB models, containerize training pipelines, and debug CUDA errors on remote servers at 2 AM.\n",
    "\n",
    "**Estimated Time:** 30-40 hours (2-3 weeks)  \n",
    "**Prerequisites:** Chapters 1-3, access to Linux/macOS terminal (WSL acceptable for Windows)\n",
    "\n",
    "---\n",
    "\n",
    "## **4.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Manage ML code with Git LFS, implement GitFlow for experiments, and automate CI/CD for ML pipelines\n",
    "2. Navigate Linux servers, write shell scripts for data pipelines, and diagnose GPU/CPU resource contention\n",
    "3. Create reproducible environments with Conda/Poetry and resolve dependency conflicts\n",
    "4. Build optimized Docker images for ML training and serving (CUDA-enabled, multi-stage, layer-cached)\n",
    "5. Interface with cloud storage (S3) and compute instances for distributed training\n",
    "6. Configure VS Code for remote development on GPU servers with full debugging capabilities\n",
    "\n",
    "---\n",
    "\n",
    "## **4.1 Version Control: Git for ML**\n",
    "\n",
    "Standard Git fails with ML artifacts (datasets, model weights). We need specialized workflows.\n",
    "\n",
    "#### **4.1.1 Git Fundamentals (The 20% you use 80% of the time)**\n",
    "\n",
    "```bash\n",
    "# Configuration (do this once)\n",
    "git config --global user.name \"Your Name\"\n",
    "git config --global user.email \"you@example.com\"\n",
    "git config --global init.defaultBranch main\n",
    "\n",
    "# Daily workflow\n",
    "git status                    # What's changed?\n",
    "git add -p                    # Stage interactively (review each hunk)\n",
    "git commit -m \"feat: add data augmentation pipeline\"\n",
    "git pull --rebase             # Keep history linear (cleaner than merge commits)\n",
    "git push origin feature-branch\n",
    "\n",
    "# Undoing mistakes\n",
    "git checkout -- <file>        # Discard local changes\n",
    "git reset --soft HEAD~1       # Undo last commit, keep changes staged\n",
    "git stash -u                  # Stash changes including untracked files\n",
    "git stash pop                 # Restore stashed changes\n",
    "```\n",
    "\n",
    "**Branching Strategy for ML (GitFlow Adapted):**\n",
    "- `main`: Production-ready code, tagged releases (v1.0.0)\n",
    "- `develop`: Integration branch for features\n",
    "- `experiment/*`: Individual ML experiments (e.g., `experiment/resnet50-augmentation-v2`)\n",
    "- `feature/*`: Infrastructure features (e.g., `feature/add-mlflow-logging`)\n",
    "- `hotfix/*`: Critical production fixes\n",
    "\n",
    "```bash\n",
    "# Create experiment branch\n",
    "git checkout -b experiment/gpt2-finetuning-lr-sweep\n",
    "\n",
    "# Push to remote (set upstream)\n",
    "git push -u origin experiment/gpt2-finetuning-lr-sweep\n",
    "\n",
    "# When done: squash merge to keep history clean\n",
    "git checkout develop\n",
    "git merge --squash experiment/gpt2-finetuning-lr-sweep\n",
    "git commit -m \"experiment: GPT-2 finetuning with LR sweep results\"\n",
    "```\n",
    "\n",
    "#### **4.1.2 Git LFS (Large File Storage)**\n",
    "\n",
    "ML repositories contain binary files (`.pt`, `.pkl`, `.csv`) that break standard Git.\n",
    "\n",
    "**Setup:**\n",
    "```bash\n",
    "# Install Git LFS\n",
    "git lfs install\n",
    "\n",
    "# Track model files\n",
    "git lfs track \"*.pt\" \"*.pth\" \"*.h5\" \"*.pb\" \"data/*.csv\"\n",
    "git lfs track \"checkpoints/**\"\n",
    "\n",
    "# Verify .gitattributes created\n",
    "cat .gitattributes\n",
    "```\n",
    "\n",
    "**Best Practices:**\n",
    "- **Don't track generated files:** Only track source code and final model artifacts, not every checkpoint.\n",
    "- **Storage costs:** Git LFS bandwidth/storage is expensive on GitHub. For >1GB models, use DVC (Data Version Control) or cloud storage (S3) with reference files.\n",
    "- **Locking:** For binary files that can't be merged (Jupyter notebooks with outputs), use `git lfs lock notebook.ipynb`.\n",
    "\n",
    "#### **4.1.3 Managing Jupyter Notebooks in Git**\n",
    "\n",
    "Notebooks create messy diffs (JSON with execution counts/binary outputs).\n",
    "\n",
    "**Solution 1: Strip outputs before commit**\n",
    "```bash\n",
    "# Using nbstripout\n",
    "pip install nbstripout\n",
    "nbstripout --install  # Run in repo root\n",
    "\n",
    "# Now git diff shows only code changes, not outputs\n",
    "```\n",
    "\n",
    "**Solution 2: ReviewNB (GitHub App)**\n",
    "Visual diff tool for notebooks in pull requests (essential for team collaboration).\n",
    "\n",
    "**Solution 3: Convert to Python for review**\n",
    "```bash\n",
    "# Use jupytext to pair .ipynb with .py\n",
    "jupytext --set-formats ipynb,py notebook.ipynb\n",
    "# Edit .py file, sync back to .ipynb\n",
    "```\n",
    "\n",
    "#### **4.1.4 CI/CD for ML (GitHub Actions)**\n",
    "\n",
    "Automate testing and training validation on every commit.\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/ml-pipeline.yml\n",
    "name: ML Pipeline\n",
    "\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      \n",
    "      - name: Set up Python\n",
    "        uses: actions/setup-python@v4\n",
    "        with:\n",
    "          python-version: '3.9'\n",
    "          \n",
    "      - name: Cache dependencies\n",
    "        uses: actions/cache@v3\n",
    "        with:\n",
    "          path: ~/.cache/pip\n",
    "          key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\n",
    "          \n",
    "      - name: Install dependencies\n",
    "        run: |\n",
    "          pip install -r requirements.txt\n",
    "          pip install pytest\n",
    "          \n",
    "      - name: Run tests\n",
    "        run: pytest tests/ --cov=src\n",
    "        \n",
    "      - name: Check code style\n",
    "        run: |\n",
    "          pip install black flake8\n",
    "          black --check src/\n",
    "          flake8 src/\n",
    "```\n",
    "\n",
    "**Advanced: Self-hosted Runners**\n",
    "For GPU tests, use self-hosted runners (your own GPU server) instead of GitHub's CPU-only runners.\n",
    "\n",
    "```yaml\n",
    "jobs:\n",
    "  gpu-tests:\n",
    "    runs-on: self-hosted  # Your GPU machine\n",
    "    steps:\n",
    "      - uses: actions/checkout@v3\n",
    "      - name: GPU Tests\n",
    "        run: pytest tests/test_gpu_ops.py\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.2 Linux/Unix for AI Engineers**\n",
    "\n",
    "Most training happens on Linux servers. You must navigate without a GUI.\n",
    "\n",
    "#### **4.2.1 Essential Navigation and File Operations**\n",
    "\n",
    "```bash\n",
    "# Navigation\n",
    "cd -                  # Go to previous directory (useful!)\n",
    "cd ~                  # Home directory\n",
    "pwd                   # Print working directory\n",
    "\n",
    "# File operations\n",
    "ls -lh                # Human-readable sizes (MB, GB)\n",
    "ls -lt                # Sort by time (newest first)\n",
    "cp -r src/ dst/       # Recursive copy\n",
    "rsync -avz --progress data/ user@server:/data/  # Resumable transfer, compression\n",
    "rm -rf dir/           # DANGER: Recursive force delete (no trash!)\n",
    "\n",
    "# Disk usage (critical for shared GPU servers)\n",
    "du -sh *              # Size of each item in current dir\n",
    "du -h --max-depth=1 /home/user  # Find what's eating disk space\n",
    "df -h                 # Disk free space on all mounts\n",
    "\n",
    "# Find files (faster than find: use fd if installed)\n",
    "find . -name \"*.py\" -type f -size +1M  # Python files > 1MB\n",
    "find . -mtime -7      # Modified in last 7 days\n",
    "\n",
    "# Search content (use ripgrep: rg, faster than grep)\n",
    "grep -r \"TODO\" --include=\"*.py\" src/\n",
    "rg \"class.*Dataset\" --type py\n",
    "```\n",
    "\n",
    "#### **4.2.2 File Permissions and Ownership**\n",
    "\n",
    "Shared servers require strict permission management.\n",
    "\n",
    "```bash\n",
    "# Permission bits: rwx (read, write, execute) for owner/group/others\n",
    "chmod 755 script.py   # rwxr-xr-x (owner full, others read+execute)\n",
    "chmod 600 ~/.ssh/id_rsa  # rw------- (owner only, critical for keys!)\n",
    "\n",
    "# Ownership (usually need sudo)\n",
    "chown user:group file.txt\n",
    "chown -R $USER:$USER /data/my_experiment  # Recursive\n",
    "\n",
    "# Access Control Lists (ACLs) for fine-grained sharing\n",
    "setfacl -m u:colleague:rwx /shared/project\n",
    "getfacl /shared/project\n",
    "```\n",
    "\n",
    "**ML Context:** Dataset directories should be read-only for group members to prevent accidental deletion. Model checkpoint directories need write permissions for the training user only.\n",
    "\n",
    "#### **4.2.3 Process Management and Monitoring**\n",
    "\n",
    "```bash\n",
    "# View processes\n",
    "htop                  # Interactive process viewer (better than top)\n",
    "nvidia-smi            # GPU usage (memory, utilization, temperature)\n",
    "watch -n 1 nvidia-smi # Refresh every second\n",
    "\n",
    "# Kill runaway processes\n",
    "ps aux | grep python  # Find Python processes\n",
    "kill -9 PID           # Force kill (SIGKILL, use as last resort)\n",
    "pkill -f \"train.py\"   # Kill by command name\n",
    "\n",
    "# Background jobs\n",
    "python train.py &     # Run in background\n",
    "bg                    # Resume suspended job in background\n",
    "fg %1                 # Bring job 1 to foreground\n",
    "jobs                  # List background jobs\n",
    "\n",
    "# No hangup (survives SSH disconnect)\n",
    "nohup python train.py > logs.txt 2>&1 &\n",
    "# OR use tmux/screen (preferred)\n",
    "\n",
    "# tmux essentials\n",
    "tmux new -s training  # New session named \"training\"\n",
    "# Ctrl+b, d to detach\n",
    "tmux ls               # List sessions\n",
    "tmux attach -t training  # Reconnect\n",
    "# Ctrl+b, c (new window), n (next), p (previous), % (split vertical)\n",
    "```\n",
    "\n",
    "#### **4.2.4 Shell Scripting for ML Pipelines**\n",
    "\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# run_experiments.sh\n",
    "\n",
    "# Exit on error, undefined vars, pipe failures\n",
    "set -euo pipefail\n",
    "\n",
    "# Configuration\n",
    "MODELS=(\"resnet50\" \"efficientnet-b0\" \"vit-base\")\n",
    "LEARNING_RATES=(0.001 0.0001)\n",
    "DATA_DIR=\"/data/imagenet\"\n",
    "LOG_DIR=\"./logs/$(date +%Y%m%d_%H%M%S)\"\n",
    "\n",
    "mkdir -p \"$LOG_DIR\"\n",
    "\n",
    "# Loop through hyperparameters\n",
    "for model in \"${MODELS[@]}\"; do\n",
    "    for lr in \"${LEARNING_RATES[@]}\"; do\n",
    "        EXP_NAME=\"${model}_lr${lr}\"\n",
    "        echo \"Starting experiment: $EXP_NAME\"\n",
    "        \n",
    "        python train.py \\\n",
    "            --model \"$model\" \\\n",
    "            --lr \"$lr\" \\\n",
    "            --data-dir \"$DATA_DIR\" \\\n",
    "            --log-dir \"$LOG_DIR/$EXP_NAME\" \\\n",
    "            > \"$LOG_DIR/${EXP_NAME}.log\" 2>&1 &\n",
    "            \n",
    "        # Limit concurrent jobs to avoid OOM\n",
    "        if (( $(jobs -r | wc -l) >= 4 )); then\n",
    "            wait -n  # Wait for any job to finish\n",
    "        fi\n",
    "    done\n",
    "done\n",
    "\n",
    "wait  # Wait for all remaining jobs\n",
    "echo \"All experiments completed. Logs in $LOG_DIR\"\n",
    "```\n",
    "\n",
    "**Advanced:** Use `parallel` for more sophisticated job scheduling.\n",
    "\n",
    "#### **4.2.5 SSH and Remote Development**\n",
    "\n",
    "```bash\n",
    "# SSH keys (passwordless login)\n",
    "ssh-keygen -t ed25519 -C \"your_email@example.com\"\n",
    "ssh-copy-id user@server  # Copy public key to server\n",
    "\n",
    "# SSH config (~/.ssh/config) to simplify connections\n",
    "Host gpu-server\n",
    "    HostName 192.168.1.100\n",
    "    User ubuntu\n",
    "    IdentityFile ~/.ssh/id_ed25519\n",
    "    ForwardX11 yes  # For GUI forwarding (matplotlib plots)\n",
    "    ServerAliveInterval 60  # Keep connection alive\n",
    "\n",
    "# Now just: ssh gpu-server\n",
    "\n",
    "# Port forwarding (access TensorBoard on remote)\n",
    "ssh -L 6006:localhost:6006 gpu-server\n",
    "# Open localhost:6006 on local browser\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.3 Environment Management: The Reproducibility Challenge**\n",
    "\n",
    "ML dependencies are nightmares (CUDA versions, specific PyTorch builds, C++ extensions).\n",
    "\n",
    "#### **4.3.1 Conda (The Heavyweight)**\n",
    "\n",
    "Best for: Data science beginners, managing Python + non-Python dependencies (CUDA, MKL).\n",
    "\n",
    "```bash\n",
    "# Create environment from file\n",
    "conda env create -f environment.yml\n",
    "\n",
    "# environment.yml\n",
    "name: ml-project\n",
    "channels:\n",
    "  - pytorch\n",
    "  - nvidia\n",
    "  - conda-forge\n",
    "  - defaults\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pytorch=2.0.0\n",
    "  - torchvision\n",
    "  - pytorch-cuda=11.8\n",
    "  - cudatoolkit=11.8\n",
    "  - numpy=1.24\n",
    "  - pip\n",
    "  - pip:\n",
    "    - transformers==4.30.0\n",
    "    - wandb\n",
    "    - -e .  # Install current package in editable mode\n",
    "\n",
    "# Export exact environment (including builds)\n",
    "conda env export --no-builds > environment_lock.yml\n",
    "\n",
    "# Cloning environments\n",
    "conda create --name new_env --clone old_env\n",
    "```\n",
    "\n",
    "**Conda Best Practices:**\n",
    "- **Don't mix pip and conda** (unless pip section in yml). Leads to broken environments.\n",
    "- **Use `mamba`** (C++ reimplementation) for faster solving: `mamba install pytorch`\n",
    "- **Clean caches regularly:** `conda clean -a` (saves GBs of disk space)\n",
    "\n",
    "#### **4.3.2 Poetry (The Modern Standard)**\n",
    "\n",
    "Best for: Pure Python projects, production services, dependency resolution is superior.\n",
    "\n",
    "```bash\n",
    "# Initialize\n",
    "poetry init  # Interactive setup\n",
    "poetry add torch transformers datasets\n",
    "poetry add --group dev pytest black mypy  # Dev dependencies\n",
    "\n",
    "# pyproject.toml (the modern standard)\n",
    "[tool.poetry.dependencies]\n",
    "python = \"^3.9\"\n",
    "torch = \"^2.0.0\"\n",
    "transformers = \"^4.30.0\"\n",
    "\n",
    "[tool.poetry.group.dev.dependencies]\n",
    "pytest = \"^7.0\"\n",
    "black = \"^23.0\"\n",
    "\n",
    "# Lock file (poetry.lock) ensures exact reproducibility\n",
    "poetry install  # Creates virtualenv and installs\n",
    "\n",
    "# Running commands in environment\n",
    "poetry run python train.py\n",
    "poetry shell     # Activate environment\n",
    "\n",
    "# Export to requirements.txt (for Docker)\n",
    "poetry export -f requirements.txt --output requirements.txt --without-hashes\n",
    "```\n",
    "\n",
    "**Poetry vs Conda:**\n",
    "- **Poetry:** Better resolver, cleaner `pyproject.toml` standard, faster for pure Python, supports lock files natively.\n",
    "- **Conda:** Required for CUDA/GPU libraries, handles non-Python dependencies (C++ libs), better for research environments.\n",
    "\n",
    "**Hybrid Approach:** Use Conda for Python + CUDA, Poetry for Python package management inside Conda env.\n",
    "\n",
    "#### **4.3.3 Docker (The Nuclear Option)**\n",
    "\n",
    "When you need **guaranteed** reproducibility across laptops, servers, and cloud.\n",
    "\n",
    "**Dockerfile for ML:**\n",
    "```dockerfile\n",
    "# Multi-stage build for smaller final image\n",
    "FROM nvidia/cuda:11.8.0-cudnn8-devel-ubuntu22.04 as builder\n",
    "\n",
    "# Avoid interactive prompts\n",
    "ENV DEBIAN_FRONTEND=noninteractive\n",
    "\n",
    "# Install Python and build tools\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    python3.10 python3-pip git \\\n",
    "    && rm -rf /var/lib/apt/lists/*\n",
    "\n",
    "# Install dependencies (separate layer for caching)\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Production stage (smaller, no build tools)\n",
    "FROM nvidia/cuda:11.8.0-cudnn8-runtime-ubuntu22.04\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy only necessary artifacts from builder\n",
    "COPY --from=builder /usr/local/lib/python3.10/dist-packages /usr/local/lib/python3.10/dist-packages\n",
    "COPY --from=builder /usr/local/bin /usr/local/bin\n",
    "\n",
    "# Copy application code\n",
    "COPY src/ ./src/\n",
    "COPY configs/ ./configs/\n",
    "\n",
    "# Non-root user for security\n",
    "RUN useradd -m -u 1000 mluser\n",
    "USER mluser\n",
    "\n",
    "# Entry point\n",
    "ENTRYPOINT [\"python\", \"-m\", \"src.train\"]\n",
    "```\n",
    "\n",
    "**Key Docker Commands:**\n",
    "```bash\n",
    "# Build\n",
    "docker build -t ml-project:latest .\n",
    "\n",
    "# Run with GPU support\n",
    "docker run --gpus all -it --rm \\\n",
    "    -v $(pwd)/data:/app/data \\\n",
    "    -v $(pwd)/outputs:/app/outputs \\\n",
    "    ml-project:latest \\\n",
    "    --config configs/experiment.yaml\n",
    "\n",
    "# Debug inside container\n",
    "docker run --gpus all -it --rm --entrypoint /bin/bash ml-project:latest\n",
    "```\n",
    "\n",
    "**Docker Compose for Multi-Service:**\n",
    "```yaml\n",
    "# docker-compose.yml\n",
    "version: '3.8'\n",
    "services:\n",
    "  training:\n",
    "    build: .\n",
    "    volumes:\n",
    "      - ./data:/data\n",
    "      - ./checkpoints:/checkpoints\n",
    "    environment:\n",
    "      - CUDA_VISIBLE_DEVICES=0,1\n",
    "    command: python train.py --epochs 100\n",
    "  \n",
    "  tensorboard:\n",
    "    image: tensorflow/tensorflow:latest\n",
    "    ports:\n",
    "      - \"6006:6006\"\n",
    "    volumes:\n",
    "      - ./checkpoints:/logs\n",
    "    command: tensorboard --logdir=/logs --host=0.0.0.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.4 Cloud Platforms: The First Steps**\n",
    "\n",
    "You don't need to be a cloud architect, but you must move data and run instances.\n",
    "\n",
    "#### **4.4.1 AWS (Amazon Web Services)**\n",
    "\n",
    "**Essential Services:**\n",
    "- **S3:** Object storage (datasets, model artifacts). Infinite storage, pay per GB.\n",
    "- **EC2:** Virtual machines (GPU instances: p3, p4, g4dn).\n",
    "- **IAM:** Identity management (don't use root credentials!).\n",
    "\n",
    "**CLI Essentials:**\n",
    "```bash\n",
    "# Install AWS CLI v2\n",
    "aws configure  # Enter access key, secret key, region\n",
    "\n",
    "# S3 operations (like a remote filesystem)\n",
    "aws s3 cp local_model.pt s3://my-bucket/models/v1/\n",
    "aws s3 sync s3://my-bucket/datasets/imagenet ./data/imagenet  # Sync (resume interrupted)\n",
    "aws s3 ls s3://my-bucket/ --recursive --human-readable --summarize  # Check size\n",
    "\n",
    "# EC2 instance management\n",
    "aws ec2 start-instances --instance-ids i-1234567890abcdef0\n",
    "aws ec2 describe-instances --instance-ids i-1234567890abcdef0 --query 'Reservations[0].Instances[0].PublicIpAddress'\n",
    "\n",
    "# Spot instances (70% cheaper)\n",
    "aws ec2 request-spot-instances --spot-price \"1.00\" --instance-count 1 --type \"one-time\" --launch-specification file://specs.json\n",
    "```\n",
    "\n",
    "**S3 with Python (boto3):**\n",
    "```python\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "\n",
    "# Upload with progress bar\n",
    "def upload_file(file_name, bucket, object_name=None):\n",
    "    if object_name is None:\n",
    "        object_name = file_name\n",
    "    \n",
    "    from tqdm import tqdm\n",
    "    import os\n",
    "    \n",
    "    file_size = os.stat(file_name).st_size\n",
    "    with tqdm(total=file_size, unit='B', unit_scale=True, desc=file_name) as pbar:\n",
    "        s3.upload_file(\n",
    "            file_name, bucket, object_name,\n",
    "            Callback=lambda bytes_transferred: pbar.update(bytes_transferred)\n",
    "        )\n",
    "\n",
    "# Stream data without downloading (useful for large datasets)\n",
    "obj = s3.get_object(Bucket='my-bucket', Key='data/large_file.csv')\n",
    "df = pd.read_csv(obj['Body'])  # Stream directly to pandas\n",
    "```\n",
    "\n",
    "#### **4.4.2 GCP and Azure (Brief)**\n",
    "\n",
    "- **GCP:** Similar to AWS (Cloud Storage = S3, Compute Engine = EC2). Good integration with TensorFlow/Keras.\n",
    "- **Azure:** Strong Windows integration, good for enterprises. Blob Storage, Virtual Machines.\n",
    "\n",
    "**Universal Pattern:** All clouds have:\n",
    "1. Object storage (S3/Cloud Storage/Blob)\n",
    "2. Compute instances (EC2/Compute Engine/VMs)\n",
    "3. Identity management (IAM)\n",
    "4. CLI tools (`aws`/`gcloud`/`az`)\n",
    "\n",
    "---\n",
    "\n",
    "## **4.5 IDEs and Productivity**\n",
    "\n",
    "#### **4.5.1 VS Code for ML**\n",
    "\n",
    "**Essential Extensions:**\n",
    "- **Python:** IntelliSense, linting, debugging\n",
    "- **Jupyter:** Native notebook support (no browser needed)\n",
    "- **Remote - SSH:** Edit files on GPU server as if local\n",
    "- **GitLens:** Git blame, history\n",
    "- **Docker:** Manage containers\n",
    "\n",
    "**Remote Development Workflow:**\n",
    "1. Install \"Remote - SSH\" extension\n",
    "2. `Ctrl+Shift+P` \u2192 \"Remote-SSH: Connect to Host\" \u2192 `gpu-server`\n",
    "3. Open `/home/user/project` on server\n",
    "4. All editing, terminal, and debugging happens on server (local laptop is just a display)\n",
    "5. Forward ports (TensorBoard) automatically detected\n",
    "\n",
    "**Debugging Configuration (.vscode/launch.json):**\n",
    "```json\n",
    "{\n",
    "    \"version\": \"0.2.0\",\n",
    "    \"configurations\": [\n",
    "        {\n",
    "            \"name\": \"Python: Train\",\n",
    "            \"type\": \"python\",\n",
    "            \"request\": \"launch\",\n",
    "            \"program\": \"${workspaceFolder}/train.py\",\n",
    "            \"args\": [\"--config\", \"configs/debug.yaml\", \"--epochs\", \"1\"],\n",
    "            \"console\": \"integratedTerminal\",\n",
    "            \"env\": {\n",
    "                \"CUDA_VISIBLE_DEVICES\": \"0\"\n",
    "            }\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "```\n",
    "\n",
    "#### **4.5.2 Jupyter Extensions**\n",
    "\n",
    "```bash\n",
    "# Essential extensions\n",
    "pip install jupyterlab-code-formatter  # Black formatting\n",
    "pip install jupyterlab-git             # Git GUI in Jupyter\n",
    "pip install lckr-jupyterlab-variableinspector  # Variable explorer like Spyder\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **4.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Git Mastery with Merge Conflicts**\n",
    "1. Create a repo with a Jupyter notebook\n",
    "2. Create two branches modifying same cell\n",
    "3. Resolve conflict manually (understand HEAD vs incoming)\n",
    "4. Use `git rerere` (reuse recorded resolution) to automate future similar conflicts\n",
    "\n",
    "**Deliverable:** Screenshot of resolved conflict and commit history showing clean merge.\n",
    "\n",
    "### **Lab 2: Shell Script for Hyperparameter Sweep**\n",
    "Write a bash script that:\n",
    "- Reads experiment configs from a CSV file\n",
    "- Launches training jobs with GNU `parallel` to use exactly 4 GPUs\n",
    "- Monitors GPU memory with `nvidia-smi` and kills jobs if memory >95%\n",
    "- Aggregates results into a summary CSV\n",
    "\n",
    "**Deliverable:** `sweep.sh` with error handling and logging.\n",
    "\n",
    "### **Lab 3: Multi-Stage Docker Build**\n",
    "Create a Dockerfile that:\n",
    "- Stage 1: Compiles a custom CUDA extension (C++ PyTorch op)\n",
    "- Stage 2: Runtime environment with only Python and necessary libs (no gcc, no CUDA dev tools)\n",
    "- Final image size <2GB (vs >8GB for devel image)\n",
    "\n",
    "**Deliverable:** `Dockerfile`, `docker-compose.yml`, and size comparison report.\n",
    "\n",
    "### **Lab 4: Cloud Data Pipeline**\n",
    "Write a Python script that:\n",
    "- Syncs local training data to S3 (incremental, only changed files)\n",
    "- Launches EC2 spot instance\n",
    "- Waits for training completion by polling S3 for `done.txt`\n",
    "- Downloads results and terminates instance\n",
    "\n",
    "**Deliverable:** `cloud_train.py` using boto3, with cost estimation.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.7 Common Pitfalls**\n",
    "\n",
    "1. **Committing Large Files Without LFS:**\n",
    "   - Repo becomes permanently bloated (history retains file even if deleted)\n",
    "   - Solution: Use `git-filter-repo` to purge from history (destructive!)\n",
    "\n",
    "2. **Docker Layer Caching Issues:**\n",
    "   - Copying code before installing requirements busts cache on every code change\n",
    "   - Solution: Copy requirements first, install, then copy code\n",
    "\n",
    "3. **Conda Environment in Production:**\n",
    "   - Conda solves for 30 minutes on deployment\n",
    "   - Solution: Use `conda-lock` or export exact environment, or use Docker\n",
    "\n",
    "4. **Running Jupyter on 0.0.0.0 Without Password:**\n",
    "   - Anyone on network can execute code as you\n",
    "   - Solution: Use SSH tunneling instead, or set strong password/token\n",
    "\n",
    "5. **CUDA Version Mismatch:**\n",
    "   - Host has CUDA 11.8, container has 12.0 = crash\n",
    "   - Solution: Use `nvidia/cuda` base images matching host driver capabilities\n",
    "\n",
    "---\n",
    "\n",
    "## **4.8 Interview Questions**\n",
    "\n",
    "**Q1:** How do you version control a 5GB model checkpoint?\n",
    "*A: Git LFS for <2GB files with bandwidth considerations. For larger, use DVC (Data Version Control) or cloud storage (S3) with versioned filenames (model-v1.0.0.pt), storing only the S3 URI in Git. For experiments, use MLflow or Weights & Biases artifact tracking.*\n",
    "\n",
    "**Q2:** Explain the difference between `pip install -r requirements.txt` and `poetry install`.\n",
    "*A: Pip installs latest versions satisfying constraints at install time (non-reproducible). Poetry uses lock file (poetry.lock) to install exact versions of all transitive dependencies, ensuring identical environments across machines/times. Poetry also manages virtualenvs automatically.*\n",
    "\n",
    "**Q3:** Your training job dies when you close laptop (SSH disconnects). How do you fix it?\n",
    "*A: Use `tmux` or `screen` to create persistent sessions that survive disconnect. Or use `nohup` with output redirection. Best practice: Use a process manager like systemd or dedicated job schedulers (Slurm, Kubernetes) for production training.*\n",
    "\n",
    "**Q4:** How do you share a GPU server with 4 colleagues without conflicts?\n",
    "*A: Use `CUDA_VISIBLE_DEVICES` to assign specific GPUs to users. Implement a simple lock file system or use GPU scheduling tools (e.g., RunAI, Kubernetes with GPU operator). Monitor with `nvidia-smi` and set memory limits if using containers.*\n",
    "\n",
    "**Q5:** Your Docker build takes 20 minutes every time because it reinstalls PyTorch.\n",
    "*A: Leverage layer caching: Copy requirements.txt and install dependencies BEFORE copying source code. Only the COPY source layer rebuilds on code changes; dependency layer is cached. Use BuildKit for better caching (`DOCKER_BUILDKIT=1`).*\n",
    "\n",
    "---\n",
    "\n",
    "## **4.9 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Pro Git* (Scott Chacon) - Free online, comprehensive Git reference\n",
    "- *The Linux Command Line* (William Shotts) - Essential shell skills\n",
    "- *Docker Deep Dive* (Nigel Poulton) - Container internals\n",
    "\n",
    "**Tools:**\n",
    "- **DVC:** Data Version Control (Git for data)\n",
    "- **Pre-commit:** Framework for managing pre-commit hooks (black, flake8, etc.)\n",
    "- **Tmuxp:** YAML configuration for tmux sessions (save layout setups)\n",
    "\n",
    "---\n",
    "\n",
    "## **4.10 Checkpoint Project: Reproducible Training Infrastructure**\n",
    "\n",
    "Build a complete training infrastructure that could be handed to a new team member and \"just work.\"\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "**Repository Structure:**\n",
    "```\n",
    "project/\n",
    "\u251c\u2500\u2500 .gitattributes      # LFS tracking\n",
    "\u251c\u2500\u2500 .github/\n",
    "\u2502   \u2514\u2500\u2500 workflows/\n",
    "\u2502       \u2514\u2500\u2500 train.yml   # CI to verify training runs\n",
    "\u251c\u2500\u2500 Dockerfile          # Multi-stage, CUDA-enabled\n",
    "\u251c\u2500\u2500 docker-compose.yml  # Training + TensorBoard + Postgres (for MLflow)\n",
    "\u251c\u2500\u2500 Makefile            # Common commands (make train, make test, make clean)\n",
    "\u251c\u2500\u2500 pyproject.toml      # Poetry dependencies\n",
    "\u251c\u2500\u2500 environment.yml     # Conda alternative\n",
    "\u251c\u2500\u2500 data/               # Gitignored, mounted volume\n",
    "\u251c\u2500\u2500 configs/            # YAML experiment configs\n",
    "\u251c\u2500\u2500 scripts/\n",
    "\u2502   \u251c\u2500\u2500 setup.sh        # Install hooks, create dirs\n",
    "\u2502   \u2514\u2500\u2500 sync_data.sh    # Rsync/S3 sync wrapper\n",
    "\u2514\u2500\u2500 src/\n",
    "    \u2514\u2500\u2500 train.py\n",
    "```\n",
    "\n",
    "**Features:**\n",
    "1. **One-command setup:** `make init` creates environment, installs pre-commit hooks, pulls sample data\n",
    "2. **Dual environment support:** Both `poetry install` and `conda env create` work identically\n",
    "3. **Remote training script:** `scripts/remote_train.sh` that SSHs to server, starts tmux session, runs Docker container, detaches\n",
    "4. **Artifact management:** Training outputs saved to S3 with versioning, metadata in MLflow\n",
    "5. **Reproducibility:** `REPRODUCIBILITY.md` document listing exact hardware, driver versions, and random seeds used for published results\n",
    "\n",
    "**Deliverables:**\n",
    "- GitHub repo link\n",
    "- Demo video: Clone \u2192 `make train` \u2192 TensorBoard shows live metrics (under 5 minutes)\n",
    "- Cost analysis: Training cost on cloud vs local hardware\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 4**\n",
    "\n",
    "*You now have the engineering hygiene to build production-grade ML systems. Chapter 5 will begin Phase 2: Machine Learning Fundamentals \u2014 starting with Data Preprocessing and Feature Engineering.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='3. computer_science_fundamentals.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../2. Machine_learning_fundamentals/5. data_preprocessing_and_feature_engineering.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}