{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1829649f",
   "metadata": {},
   "source": [
    "\n",
    "# **CHAPTER 21: MODEL TRAINING & EXPERIMENTATION**\n",
    "\n",
    "*Scaling Training from Laptops to Clusters*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Moving from experimental notebooks to production training requires rigorous experiment tracking, systematic hyperparameter optimization, and distributed computing strategies. This chapter covers the tools and techniques to train models reliably at scale, ensuring reproducibility and efficient resource utilization across GPUs and TPUs.\n",
    "\n",
    "**Estimated Time:** 40-50 hours (3-4 weeks)  \n",
    "**Prerequisites:** Chapter 11 (Deep Learning Frameworks), Chapter 20 (Data Engineering), access to GPU resources (cloud or local)\n",
    "\n",
    "---\n",
    "\n",
    "## **21.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement comprehensive experiment tracking (metrics, artifacts, lineage) using MLflow and Weights & Biases\n",
    "2. Design distributed training strategies (data parallelism, model parallelism, pipeline parallelism) for large models\n",
    "3. Execute efficient hyperparameter searches using Bayesian optimization and population-based training\n",
    "4. Optimize training throughput with mixed precision, gradient accumulation, and efficient data loading\n",
    "5. Ensure training reproducibility through deterministic operations and environment management\n",
    "6. Implement fault-tolerant training with checkpointing and automatic recovery\n",
    "\n",
    "---\n",
    "\n",
    "## **21.1 Experiment Tracking**\n",
    "\n",
    "#### **21.1.1 MLflow Architecture**\n",
    "\n",
    "Open-source platform for the ML lifecycle with four components:\n",
    "- **Tracking:** Record parameters, metrics, artifacts\n",
    "- **Projects:** Package code for reproducibility\n",
    "- **Models:** Manage deployment artifacts\n",
    "- **Registry:** Version and stage models\n",
    "\n",
    "```python\n",
    "# training_with_mlflow.py\n",
    "import mlflow\n",
    "import mlflow.pytorch\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Set experiment\n",
    "mlflow.set_tracking_uri(\"http://mlflow-server:5000\")\n",
    "mlflow.set_experiment(\"fraud-detection-cnn\")\n",
    "\n",
    "with mlflow.start_run(run_name=\"experiment-1-baseline\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_params({\n",
    "        \"learning_rate\": 0.001,\n",
    "        \"batch_size\": 256,\n",
    "        \"epochs\": 100,\n",
    "        \"optimizer\": \"AdamW\",\n",
    "        \"model_architecture\": \"ResNet50\"\n",
    "    })\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        train_loss = train_epoch(model, train_loader)\n",
    "        val_loss, val_acc = validate(model, val_loader)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metrics({\n",
    "            \"train_loss\": train_loss,\n",
    "            \"val_loss\": val_loss,\n",
    "            \"val_accuracy\": val_acc\n",
    "        }, step=epoch)\n",
    "        \n",
    "        # Log model checkpoint every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            mlflow.pytorch.log_model(model, f\"checkpoints/epoch-{epoch}\")\n",
    "    \n",
    "    # Log final artifact (model)\n",
    "    mlflow.pytorch.log_model(model, \"model\", \n",
    "        registered_model_name=\"fraud-detection-model\")\n",
    "    \n",
    "    # Log supplementary artifacts\n",
    "    mlflow.log_artifact(\"confusion_matrix.png\", \"visualizations\")\n",
    "    mlflow.log_artifact(\"training_config.yaml\", \"config\")\n",
    "```\n",
    "\n",
    "#### **21.1.2 Weights & Biases (W&B) for Deep Learning**\n",
    "\n",
    "Specialized for deep learning with rich visualization and collaboration features.\n",
    "\n",
    "```python\n",
    "# wandb_training.py\n",
    "import wandb\n",
    "\n",
    "# Initialize run\n",
    "wandb.init(\n",
    "    project=\"nlp-sentiment-analysis\",\n",
    "    name=\"bert-fine-tuning-run-1\",\n",
    "    config={\n",
    "        \"model\": \"bert-base-uncased\",\n",
    "        \"learning_rate\": 2e-5,\n",
    "        \"batch_size\": 16,\n",
    "        \"epochs\": 3,\n",
    "        \"weight_decay\": 0.01\n",
    "    }\n",
    ")\n",
    "\n",
    "config = wandb.config\n",
    "\n",
    "# Automatic gradient logging\n",
    "wandb.watch(model, log=\"all\", log_freq=100)\n",
    "\n",
    "for epoch in range(config.epochs):\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        # Log every 50 steps\n",
    "        if batch_idx % 50 == 0:\n",
    "            wandb.log({\n",
    "                \"loss\": loss.item(),\n",
    "                \"learning_rate\": optimizer.param_groups[0]['lr'],\n",
    "                \"epoch\": epoch\n",
    "            })\n",
    "    \n",
    "    # Validation\n",
    "    val_metrics = evaluate(model, val_loader)\n",
    "    wandb.log({f\"val_{k}\": v for k, v in val_metrics.items()})\n",
    "\n",
    "# Save model with versioning\n",
    "artifact = wandb.Artifact('model', type='model')\n",
    "artifact.add_file('model.pth')\n",
    "wandb.log_artifact(artifact)\n",
    "```\n",
    "\n",
    "**Advanced Features:**\n",
    "- **Sweeps:** Integrated hyperparameter optimization\n",
    "- **Tables:** Visualize dataset samples and model predictions\n",
    "- **Reports:** Shareable dashboards with embedded visualizations\n",
    "- **Artifacts:** Dataset and model lineage tracking\n",
    "\n",
    "---\n",
    "\n",
    "## **21.2 Hyperparameter Optimization (HPO)**\n",
    "\n",
    "#### **21.2.1 Bayesian Optimization with Optuna**\n",
    "\n",
    "Efficient search using probabilistic models to suggest next hyperparameters based on past results.\n",
    "\n",
    "```python\n",
    "# optuna_hpo.py\n",
    "import optuna\n",
    "from optuna.integration import PyTorchLightningPruningCallback\n",
    "\n",
    "def objective(trial):\n",
    "    # Suggest hyperparameters\n",
    "    lr = trial.suggest_float(\"lr\", 1e-5, 1e-1, log=True)\n",
    "    batch_size = trial.suggest_categorical(\"batch_size\", [32, 64, 128, 256])\n",
    "    dropout = trial.suggest_float(\"dropout\", 0.1, 0.5)\n",
    "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\", \"AdamW\"])\n",
    "    n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n",
    "    \n",
    "    # Build model dynamically\n",
    "    model = create_model(n_layers=n_layers, dropout=dropout)\n",
    "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Training with early stopping based on validation loss\n",
    "    for epoch in range(100):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer)\n",
    "        val_loss = validate(model, val_loader)\n",
    "        \n",
    "        # Report intermediate result for pruning\n",
    "        trial.report(val_loss, epoch)\n",
    "        if trial.should_prune():\n",
    "            raise optuna.TrialPruned()\n",
    "    \n",
    "    return val_loss\n",
    "\n",
    "# Create study with TPE sampler (Tree-structured Parzen Estimator)\n",
    "study = optuna.create_study(\n",
    "    direction=\"minimize\",\n",
    "    sampler=optuna.samplers.TPESampler(),\n",
    "    pruner=optuna.pruners.MedianPruner()  # Prune unpromising trials early\n",
    ")\n",
    "\n",
    "study.optimize(objective, n_trials=100, n_jobs=4)\n",
    "\n",
    "print(f\"Best value: {study.best_value} (params: {study.best_params})\")\n",
    "```\n",
    "\n",
    "**Multi-Objective Optimization:**\n",
    "```python\n",
    "# Optimize for both accuracy and inference speed\n",
    "study = optuna.create_study(directions=[\"maximize\", \"minimize\"])  # Accuracy, Latency\n",
    "\n",
    "def objective(trial):\n",
    "    accuracy = train_and_evaluate(trial)\n",
    "    latency = measure_inference_time(trial)\n",
    "    return accuracy, latency\n",
    "\n",
    "study.optimize(objective, n_trials=50)\n",
    "```\n",
    "\n",
    "#### **21.2.2 Population Based Training (PBT)**\n",
    "\n",
    "Evolutionary strategy where poorly performing trials exploit weights from good trials and explore new hyperparameters.\n",
    "\n",
    "```python\n",
    "# ray_tune_pbt.py\n",
    "from ray import tune\n",
    "from ray.tune.schedulers import PopulationBasedTraining\n",
    "\n",
    "pbt_scheduler = PopulationBasedTraining(\n",
    "    time_attr=\"training_iteration\",\n",
    "    perturbation_interval=10,\n",
    "    hyperparam_mutations={\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "        \"batch_size\": [32, 64, 128]\n",
    "    },\n",
    "    quantile_fraction=0.25  # Bottom 25% exploit top 25%\n",
    ")\n",
    "\n",
    "analysis = tune.run(\n",
    "    train_fn,\n",
    "    config={\n",
    "        \"lr\": tune.loguniform(1e-5, 1e-1),\n",
    "        \"batch_size\": 64\n",
    "    },\n",
    "    num_samples=20,\n",
    "    resources_per_trial={\"gpu\": 1, \"cpu\": 4},\n",
    "    scheduler=pbt_scheduler,\n",
    "    metric=\"val_accuracy\",\n",
    "    mode=\"max\"\n",
    ")\n",
    "```\n",
    "\n",
    "#### **21.2.3 Hyperparameter Search at Scale**\n",
    "\n",
    "**Ash Scheduling:** Early stopping based on intermediate performance (Hyperband algorithm).\n",
    "\n",
    "```python\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "\n",
    "scheduler = ASHAScheduler(\n",
    "    max_t=100,  # Max epochs\n",
    "    grace_period=10,  # Minimum epochs before pruning\n",
    "    reduction_factor=2\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **21.3 Distributed Training**\n",
    "\n",
    "#### **21.3.1 Data Parallelism**\n",
    "\n",
    "Split batch across multiple GPUs; each holds full model copy.\n",
    "\n",
    "**PyTorch DDP (DistributedDataParallel):**\n",
    "```python\n",
    "# distributed_training.py\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "\n",
    "def setup():\n",
    "    # Initialize process group\n",
    "    dist.init_process_group(\"nccl\")  # NCCL for GPU, Gloo for CPU\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "    return local_rank\n",
    "\n",
    "def main():\n",
    "    local_rank = setup()\n",
    "    \n",
    "    # Create model and move to GPU\n",
    "    model = MyModel().to(local_rank)\n",
    "    \n",
    "    # Wrap with DDP\n",
    "    model = DDP(model, device_ids=[local_rank], \n",
    "                output_device=local_rank,\n",
    "                find_unused_parameters=False)  # Set True if some params unused\n",
    "    \n",
    "    # Distributed sampler ensures each GPU gets unique data\n",
    "    train_sampler = DistributedSampler(dataset)\n",
    "    train_loader = DataLoader(dataset, batch_size=64, sampler=train_sampler)\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        train_sampler.set_epoch(epoch)  # Important for shuffling\n",
    "        for batch in train_loader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(local_rank)\n",
    "            labels = labels.to(local_rank)\n",
    "            \n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Launch with: torchrun --nproc_per_node=4 distributed_training.py\n",
    "    main()\n",
    "```\n",
    "\n",
    "#### **21.3.2 Model Parallelism & Pipeline Parallelism**\n",
    "\n",
    "For models too large for single GPU (e.g., GPT-3 scale).\n",
    "\n",
    "**Fully Sharded Data Parallel (FSDP):**\n",
    "```python\n",
    "# fsdp_training.py\n",
    "from torch.distributed.fsdp import FullyShardedDataParallel as FSDP\n",
    "from torch.distributed.fsdp.wrap import transformer_auto_wrap_policy\n",
    "\n",
    "model = FSDP(\n",
    "    model,\n",
    "    auto_wrap_policy=transformer_auto_wrap_policy,\n",
    "    mixed_precision=torch.bfloat16,\n",
    "    device_id=torch.cuda.current_device(),\n",
    "    limit_all_gathers=True  # Reduce memory fragmentation\n",
    ")\n",
    "```\n",
    "\n",
    "**Pipeline Parallelism (GPipe style):**\n",
    "```python\n",
    "from torch.distributed.pipeline.sync import Pipe\n",
    "from torch.distributed.rpc import init_rpc\n",
    "\n",
    "# Split model across stages (different GPUs)\n",
    "model = nn.Sequential(stage1, stage2, stage3)\n",
    "model = Pipe(model, chunks=4)  # Micro-batches for pipeline bubble reduction\n",
    "\n",
    "output = model(input)\n",
    "```\n",
    "\n",
    "#### **21.3.3 ZeRO Optimization (DeepSpeed)**\n",
    "\n",
    "Memory optimization stages for massive models:\n",
    "- **ZeRO-1:** Shard optimizer states across GPUs\n",
    "- **ZeRO-2:** Add gradient sharding\n",
    "- **ZeRO-3:** Add parameter sharding (model state distributed)\n",
    "- **ZeRO-Offload:** Offload to CPU/NVMe\n",
    "\n",
    "```python\n",
    "# deepspeed_config.json\n",
    "{\n",
    "    \"train_batch_size\": 64,\n",
    "    \"gradient_accumulation_steps\": 4,\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"params\": {\"lr\": 0.0001}\n",
    "    },\n",
    "    \"fp16\": {\"enabled\": true},\n",
    "    \"zero_optimization\": {\n",
    "        \"stage\": 2,\n",
    "        \"allgather_partitions\": true,\n",
    "        \"reduce_scatter\": true,\n",
    "        \"contiguous_gradients\": true\n",
    "    }\n",
    "}\n",
    "\n",
    "# Training\n",
    "import deepspeed\n",
    "model_engine, optimizer, _, _ = deepspeed.initialize(\n",
    "    model=model,\n",
    "    model_parameters=model.parameters(),\n",
    "    config=\"deepspeed_config.json\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **21.4 Training Optimization Techniques**\n",
    "\n",
    "#### **21.4.1 Mixed Precision Training**\n",
    "\n",
    "Use FP16/BF16 for forward/backward passes, FP32 for updates (automatic loss scaling).\n",
    "\n",
    "```python\n",
    "# Automatic Mixed Precision (AMP)\n",
    "from torch.cuda.amp import autocast, GradScaler\n",
    "\n",
    "scaler = GradScaler()\n",
    "\n",
    "for data, target in dataloader:\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass with autocast\n",
    "    with autocast(device_type='cuda', dtype=torch.float16):\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "    \n",
    "    # Backward pass with scaling\n",
    "    scaler.scale(loss).backward()\n",
    "    scaler.step(optimizer)\n",
    "    scaler.update()\n",
    "```\n",
    "\n",
    "**BF16 vs FP16:**\n",
    "- **FP16:** Requires loss scaling (narrow dynamic range), supported on V100+\n",
    "- **BF16:** Same range as FP32, no scaling needed, supported on A100+\n",
    "\n",
    "#### **21.4.2 Gradient Accumulation**\n",
    "\n",
    "Simulate large batch sizes with limited memory.\n",
    "\n",
    "```python\n",
    "# Effective batch size = batch_size * accumulation_steps * num_gpus\n",
    "accumulation_steps = 4\n",
    "\n",
    "for i, (data, target) in enumerate(dataloader):\n",
    "    with autocast():\n",
    "        loss = model(data, target) / accumulation_steps\n",
    "    \n",
    "    loss.backward()\n",
    "    \n",
    "    if (i + 1) % accumulation_steps == 0:\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "```\n",
    "\n",
    "#### **21.4.3 Efficient Data Loading**\n",
    "\n",
    "```python\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=64,\n",
    "    num_workers=8,        # Parallel data loading\n",
    "    pin_memory=True,      # Faster CPU→GPU transfer\n",
    "    persistent_workers=True,  # Keep workers alive between epochs\n",
    "    prefetch_factor=2     # Batches per worker\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **21.5 Reproducibility**\n",
    "\n",
    "#### **21.5.1 Deterministic Operations**\n",
    "\n",
    "```python\n",
    "def set_seed(seed=42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    \n",
    "    # Deterministic algorithms (slower but reproducible)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False  # Disable auto-tuner\n",
    "    torch.use_deterministic_algorithms(True)\n",
    "    \n",
    "    # For DataLoader\n",
    "    def seed_worker(worker_id):\n",
    "        worker_seed = torch.initial_seed() % 2**32\n",
    "        np.random.seed(worker_seed)\n",
    "        random.seed(worker_seed)\n",
    "    \n",
    "    return seed_worker\n",
    "\n",
    "# Usage\n",
    "DataLoader(dataset, worker_init_fn=set_seed(42))\n",
    "```\n",
    "\n",
    "#### **21.5.2 Environment Freezing**\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile.reproducible\n",
    "FROM pytorch/pytorch:2.0.1-cuda11.7-cudnn8-runtime\n",
    "\n",
    "# Pin ALL dependencies\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Pin base image digest for true reproducibility\n",
    "# FROM pytorch/pytorch@sha256:abc123...\n",
    "```\n",
    "\n",
    "```txt\n",
    "# requirements.txt (pinned)\n",
    "torch==2.0.1\n",
    "numpy==1.24.3\n",
    "pandas==2.0.2\n",
    "transformers==4.30.0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **21.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Experiment Tracking Setup**\n",
    "Set up MLflow or W&B for a training pipeline:\n",
    "\n",
    "1. **Tracking:** Log hyperparameters, metrics (loss, accuracy), and artifacts (model checkpoints)\n",
    "2. **Comparison:** Run 5 variants, compare learning curves in UI\n",
    "3. **Model Registry:** Register best model, transition through Staging → Production stages\n",
    "4. **Reproducibility:** Ensure another engineer can reproduce your best run using logged artifacts\n",
    "\n",
    "**Deliverable:** Tracked experiment with report comparing runs and registered model.\n",
    "\n",
    "### **Lab 2: Distributed Training**\n",
    "Train ResNet-50 on ImageNet-scale data using multiple GPUs:\n",
    "\n",
    "1. **Single GPU Baseline:** Measure samples/sec and memory usage\n",
    "2. **DDP:** Convert to DistributedDataParallel, scale to 4 GPUs\n",
    "3. **Optimization:** Add mixed precision (AMP) and gradient accumulation\n",
    "4. **Analysis:** Calculate scaling efficiency (ideal: 4x speedup with 4 GPUs)\n",
    "\n",
    "**Deliverable:** Scaling report showing throughput vs. GPU count and memory profiling.\n",
    "\n",
    "### **Lab 3: Hyperparameter Optimization**\n",
    "Optimize a transformer fine-tuning task:\n",
    "\n",
    "1. **Search Space:** Learning rate (1e-5 to 1e-3), batch size (16-128), dropout (0.1-0.5)\n",
    "2. **Pruning:** Implement early stopping for unpromising trials\n",
    "3. **Scheduling:** Use Bayesian optimization (Optuna) vs. Random search—compare efficiency\n",
    "4. **Multi-objective:** Optimize for both accuracy and model size (FLOPs)\n",
    "\n",
    "**Deliverable:** HPO study results with visualization of search space exploration.\n",
    "\n",
    "### **Lab 4: Fault Tolerance**\n",
    "Implement resilient training:\n",
    "\n",
    "1. **Checkpointing:** Save model + optimizer state every epoch\n",
    "2. **Simulation:** Kill training job mid-epoch, resume from checkpoint\n",
    "3. **Automatic Recovery:** Use try/except with checkpoint reloading on failure\n",
    "4. **Metrics Preservation:** Ensure experiment tracking continues seamlessly after restart\n",
    "\n",
    "**Deliverable:** Training script with fault tolerance tested via simulated failures.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.7 Common Pitfalls**\n",
    "\n",
    "1. **Non-Deterministic DataLoaders:** Using multiple workers without `worker_init_fn` creates randomness in data ordering. **Solution:** Always set `worker_init_fn` with seed.\n",
    "\n",
    "2. **DDP Synchronization Bugs:** Calling `loss.item()` (CPU) inside training loop without `dist.barrier()` causes GPU desync. **Solution:** Use `torch.distributed.all_reduce` for aggregating metrics.\n",
    "\n",
    "3. **Gradient Accumulation with BatchNorm:** Statistics update every mini-batch, not accumulated batch, causing instability. **Solution:** Use synchronized BatchNorm (`SyncBatchNorm`) or larger batches with gradient checkpointing instead.\n",
    "\n",
    "4. **Memory Fragmentation:** PyTorch caching allocator fragments memory over long training runs. **Solution:** Call `torch.cuda.empty_cache()` between epochs or use `PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True`.\n",
    "\n",
    "5. **Ignoring Numerical Stability:** FP16 underflow with small gradients. **Solution:** Use `GradScaler` with dynamic loss scaling, or switch to BF16.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.8 Interview Questions**\n",
    "\n",
    "**Q1:** Explain the difference between Data Parallelism and Model Parallelism. When would you use each?\n",
    "*A: Data Parallelism (DDP): Each GPU holds full model, processes different batch portions. Used when model fits in single GPU memory (most common). Model Parallelism: Model split across GPUs (different layers on different devices). Used when model too large for one GPU (e.g., GPT-3). Pipeline Parallelism combines both: split model across GPUs, pipeline micro-batches to keep all GPUs busy. Modern training uses hybrid: FSDP (sharded data parallel) for large models, combining efficiency of data parallel with memory savings of model parallel.*\n",
    "\n",
    "**Q2:** How does ZeRO (Zero Redundancy Optimizer) reduce memory usage?\n",
    "*A: ZeRO partitions optimizer states, gradients, and parameters across data parallel processes. ZeRO-1 shards optimizer states (4x memory reduction), ZeRO-2 adds gradient sharding (8x reduction), ZeRO-3 adds parameter sharding (linear reduction with GPU count). ZeRO-Offload moves optimizer states to CPU/NVMe, enabling training models with trillions of parameters on limited GPU memory. Trade-off: Increased communication overhead between GPUs.*\n",
    "\n",
    "**Q3:** What is the purpose of gradient accumulation, and what are its trade-offs?\n",
    "*A: Gradient accumulation simulates large batch sizes by accumulating gradients over multiple forward/backward passes before updating weights. Used when GPU memory limits physical batch size. Trade-offs: (1) Training slower (more forward passes per update), (2) BatchNorm statistics less accurate (computed per mini-batch), (3) Effective batch size affects convergence dynamics (larger batches need learning rate warmup/scaling). Benefit: Train with effectively unlimited batch sizes on limited hardware.*\n",
    "\n",
    "**Q4:** How do you ensure reproducibility in distributed training across different hardware?\n",
    "*A: (1) Set all random seeds (Python, NumPy, PyTorch, CUDA), (2) Use deterministic algorithms (`torch.use_deterministic_algorithms(True)`), (3) Disable cudnn.benchmark (which selects non-deterministic algorithms), (4) Fixed DataLoader worker seeds, (5) Document exact software versions (Docker images with digests), (6) For distributed: ensure deterministic reduction operations (sum vs. mean ordering). Note: Some ops (e.g., scatter_add) have no deterministic implementation on GPU—fall back to CPU or accept slight variance.*\n",
    "\n",
    "**Q5:** Design a hyperparameter search for a model with 10-day training time and limited compute budget.\n",
    "*A: Use early stopping with ASHA (Asynchronous Successive Halving): allocate minimal resources to many configurations, promote promising ones. Start with coarse random search over wide ranges, then Bayesian optimization (TPE) on promising region. Use population-based training to transfer weights between trials (don't train from scratch each time). Prioritize important hyperparameters (learning rate, batch size) over minor ones (dropout). Use multi-fidelity optimization: validate on subset of data or fewer epochs for screening, full training for finalists.*\n",
    "\n",
    "---\n",
    "\n",
    "## **21.9 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Deep Learning with PyTorch* (Eli Stevens et al.) - Distributed training patterns\n",
    "- *Designing Machine Learning Systems* (Chip Huyen) - Chapter on training\n",
    "\n",
    "**Papers:**\n",
    "- \"ZeRO: Memory Optimizations Toward Training Trillion Parameter Models\" (Rajbhandari et al., 2020)\n",
    "- \"Mixed Precision Training\" (Micikevicius et al., 2018)\n",
    "- \"Population Based Training of Neural Networks\" (Jaderberg et al., 2017)\n",
    "\n",
    "**Tools:**\n",
    "- **Ray Tune:** Scalable HPO with early stopping\n",
    "- **Weights & Biases:** Experiment tracking\n",
    "- **DeepSpeed:** Microsoft library for large model training\n",
    "- **FSDP:** PyTorch native sharded data parallel\n",
    "\n",
    "---\n",
    "\n",
    "## **21.10 Checkpoint Project: Distributed LLM Fine-Tuning**\n",
    "\n",
    "Fine-tune a 7B parameter LLM (LLaMA-2 or Mistral) on a custom dataset using distributed training.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Infrastructure:**\n",
    "   - Multi-GPU setup (4x A100 or 8x V100)\n",
    "   - Use either DDP or FSDP (recommended for 7B+ models)\n",
    "   - Mixed precision (BF16)\n",
    "\n",
    "2. **Optimization:**\n",
    "   - Gradient checkpointing (trade compute for memory)\n",
    "   - Gradient accumulation (effective batch size 128)\n",
    "   - DeepSpeed ZeRO-2 or ZeRO-3 integration\n",
    "\n",
    "3. **Experiment Tracking:**\n",
    "   - W&B integration logging perplexity, learning rate, GPU memory\n",
    "   - Hyperparameter sweep over learning rates (1e-5 to 5e-5) and LoRA ranks\n",
    "\n",
    "4. **Fault Tolerance:**\n",
    "   - Checkpoint every 500 steps to shared storage (S3/NFS)\n",
    "   - Resume capability from latest checkpoint\n",
    "   - Validation evaluation every epoch\n",
    "\n",
    "5. **Evaluation:**\n",
    "   - Perplexity on held-out test set\n",
    "   - Generation examples logged to W&B\n",
    "   - Throughput measurement (tokens/sec/GPU)\n",
    "\n",
    "**Deliverables:**\n",
    "- `distributed_training/` directory with launch scripts\n",
    "- W&B project link showing training curves\n",
    "- Final model pushed to Hugging Face Hub or S3\n",
    "- Performance report: memory usage per GPU, scaling efficiency, time-to-convergence\n",
    "\n",
    "**Success Criteria:**\n",
    "- Model trains without OOM errors on 7B parameters\n",
    "- Checkpoint resume tested and verified\n",
    "- Hyperparameter sweep identifies optimal configuration\n",
    "- Final model achieves target perplexity < X on validation set\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 21**\n",
    "\n",
    "*You can now train models at scale with full reproducibility. Chapter 22 covers Model Deployment & Serving—getting these trained models into production.*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
