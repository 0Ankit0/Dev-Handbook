{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cb14770",
   "metadata": {},
   "source": [
    "\n",
    "# **CHAPTER 22: MODEL DEPLOYMENT & SERVING**\n",
    "\n",
    "*From Training Artifacts to Production APIs*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "A trained model provides no business value until it serves predictions. This chapter covers the engineering patterns for deploying models reliably: containerization strategies, API design, serverless vs. dedicated infrastructure, and optimization techniques for low-latency inference. You will learn to bridge the gap between data science artifacts and production software systems.\n",
    "\n",
    "**Estimated Time:** 35-45 hours (3 weeks)  \n",
    "**Prerequisites:** Chapter 19 (System Design), Chapter 21 (Training), Docker and Kubernetes basics\n",
    "\n",
    "---\n",
    "\n",
    "## **22.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Serialize and package models using standardized formats (ONNX, TorchScript, SavedModel)\n",
    "2. Design REST/gRPC APIs for model serving with FastAPI and Triton Inference Server\n",
    "3. Deploy containerized models to Kubernetes with auto-scaling and rolling updates\n",
    "4. Optimize inference latency through batching, caching, and model compression\n",
    "5. Implement A/B testing and canary deployments for model updates\n",
    "6. Architect edge deployment strategies for IoT and mobile applications\n",
    "\n",
    "---\n",
    "\n",
    "## **22.1 Model Serialization & Formats**\n",
    "\n",
    "#### **22.1.1 Framework-Native Formats**\n",
    "\n",
    "**PyTorch TorchScript:**\n",
    "```python\n",
    "# tracing.py\n",
    "import torch\n",
    "from model import MyModel\n",
    "\n",
    "model = MyModel()\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# Method 1: Tracing (static graph, faster)\n",
    "example_input = torch.rand(1, 3, 224, 224)\n",
    "traced_script_module = torch.jit.trace(model, example_input)\n",
    "traced_script_module.save(\"model_traced.pt\")\n",
    "\n",
    "# Method 2: Scripting (dynamic control flow)\n",
    "scripted_model = torch.jit.script(model)\n",
    "scripted_model.save(\"model_scripted.pt\")\n",
    "\n",
    "# Loading in production (no Python dependencies)\n",
    "loaded_model = torch.jit.load(\"model_traced.pt\")\n",
    "with torch.no_grad():\n",
    "    output = loaded_model(input_tensor)\n",
    "```\n",
    "\n",
    "**TensorFlow SavedModel:**\n",
    "```python\n",
    "# Export\n",
    "tf.saved_model.save(model, \"saved_model/1/\")\n",
    "\n",
    "# Load for serving\n",
    "loaded = tf.saved_model.load(\"saved_model/1/\")\n",
    "infer = loaded.signatures[\"serving_default\"]\n",
    "predictions = infer(input_1=tf.constant([[1.0, 2.0]]))\n",
    "```\n",
    "\n",
    "#### **22.1.2 Framework-Agnostic: ONNX**\n",
    "\n",
    "Open Neural Network Exchange enables interoperability between frameworks.\n",
    "\n",
    "```python\n",
    "# export_onnx.py\n",
    "import torch.onnx\n",
    "\n",
    "dummy_input = torch.randn(1, 3, 224, 224, device=\"cuda\")\n",
    "model.eval()\n",
    "\n",
    "torch.onnx.export(\n",
    "    model,\n",
    "    dummy_input,\n",
    "    \"model.onnx\",\n",
    "    export_params=True,\n",
    "    opset_version=14,\n",
    "    do_constant_folding=True,  # Optimize constant expressions\n",
    "    input_names=[\"input\"],\n",
    "    output_names=[\"output\"],\n",
    "    dynamic_axes={  # Variable batch sizes\n",
    "        \"input\": {0: \"batch_size\"},\n",
    "        \"output\": {0: \"batch_size\"}\n",
    "    }\n",
    ")\n",
    "\n",
    "# Optimization with ONNX Runtime\n",
    "import onnxruntime as ort\n",
    "\n",
    "session = ort.InferenceSession(\n",
    "    \"model.onnx\",\n",
    "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
    ")\n",
    "\n",
    "# Run inference\n",
    "outputs = session.run(None, {\"input\": input_numpy})\n",
    "```\n",
    "\n",
    "**TensorRT Optimization (NVIDIA GPUs):**\n",
    "```python\n",
    "import tensorrt as trt\n",
    "import pycuda.driver as cuda\n",
    "\n",
    "# Parse ONNX to TensorRT engine (FP16/INT8)\n",
    "logger = trt.Logger(trt.Logger.WARNING)\n",
    "builder = trt.Builder(logger)\n",
    "network = builder.create_network(1 << int(trt.NetworkDefinitionCreationFlag.EXPLICIT_BATCH))\n",
    "parser = trt.OnnxParser(network, logger)\n",
    "\n",
    "with open(\"model.onnx\", \"rb\") as f:\n",
    "    parser.parse(f.read())\n",
    "\n",
    "config = builder.create_builder_config()\n",
    "config.max_workspace_size = 1 << 30  # 1GB\n",
    "config.set_flag(trt.BuilderFlag.FP16)  # Enable FP16\n",
    "\n",
    "engine = builder.build_engine(network, config)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.2 Serving Architectures**\n",
    "\n",
    "#### **22.2.1 REST API with FastAPI**\n",
    "\n",
    "Production-grade Python API for model serving.\n",
    "\n",
    "```python\n",
    "# api/main.py\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import numpy as np\n",
    "import onnxruntime as ort\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "app = FastAPI(title=\"ML Model Server\")\n",
    "\n",
    "# Metrics\n",
    "prediction_counter = Counter('model_predictions_total', 'Total predictions')\n",
    "latency_histogram = Histogram('model_latency_seconds', 'Inference latency')\n",
    "\n",
    "# Load model on startup\n",
    "@app.on_event(\"startup\")\n",
    "async def load_model():\n",
    "    global session\n",
    "    session = ort.InferenceSession(\"model.onnx\")\n",
    "    app.state.model_ready = True\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    features: list[float]\n",
    "    return_proba: bool = False\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    prediction: int\n",
    "    probability: float | None = None\n",
    "    model_version: str\n",
    "\n",
    "@app.post(\"/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    if not app.state.model_ready:\n",
    "        raise HTTPException(status_code=503, detail=\"Model not loaded\")\n",
    "    \n",
    "    with latency_histogram.time():\n",
    "        input_array = np.array([request.features], dtype=np.float32)\n",
    "        outputs = session.run(None, {\"input\": input_array})\n",
    "        prediction = int(np.argmax(outputs[0]))\n",
    "        probability = float(np.max(outputs[0])) if request.return_proba else None\n",
    "    \n",
    "    prediction_counter.inc()\n",
    "    \n",
    "    return PredictionResponse(\n",
    "        prediction=prediction,\n",
    "        probability=probability,\n",
    "        model_version=\"1.0.0\"\n",
    "    )\n",
    "\n",
    "@app.get(\"/health\")\n",
    "async def health():\n",
    "    return {\"status\": \"healthy\", \"model_loaded\": app.state.model_ready}\n",
    "\n",
    "# Dockerfile\n",
    "\"\"\"\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY model.onnx .\n",
    "COPY main.py .\n",
    "\n",
    "CMD [\"uvicorn\", \"main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\", \"--workers\", \"4\"]\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### **22.2.2 gRPC for High Performance**\n",
    "\n",
    "Binary protocol with lower latency than REST, ideal for internal microservices.\n",
    "\n",
    "```protobuf\n",
    "# service.proto\n",
    "syntax = \"proto3\";\n",
    "\n",
    "service ModelService {\n",
    "  rpc Predict(PredictRequest) returns (PredictResponse);\n",
    "  rpc StreamPredict(stream PredictRequest) returns (stream PredictResponse);\n",
    "}\n",
    "\n",
    "message PredictRequest {\n",
    "  repeated float features = 1;\n",
    "  bool return_proba = 2;\n",
    "}\n",
    "\n",
    "message PredictResponse {\n",
    "  int32 prediction = 1;\n",
    "  float probability = 2;\n",
    "  string model_version = 3;\n",
    "}\n",
    "```\n",
    "\n",
    "```python\n",
    "# server.py\n",
    "from concurrent import futures\n",
    "import grpc\n",
    "import service_pb2\n",
    "import service_pb2_grpc\n",
    "\n",
    "class ModelServicer(service_pb2_grpc.ModelServiceServicer):\n",
    "    def __init__(self):\n",
    "        self.session = ort.InferenceSession(\"model.onnx\")\n",
    "    \n",
    "    def Predict(self, request, context):\n",
    "        input_array = np.array([request.features], dtype=np.float32)\n",
    "        outputs = self.session.run(None, {\"input\": input_array})\n",
    "        \n",
    "        return service_pb2.PredictResponse(\n",
    "            prediction=int(np.argmax(outputs[0])),\n",
    "            probability=float(np.max(outputs[0])),\n",
    "            model_version=\"1.0.0\"\n",
    "        )\n",
    "\n",
    "server = grpc.server(futures.ThreadPoolExecutor(max_workers=10))\n",
    "service_pb2_grpc.add_ModelServiceServicer_to_server(ModelServicer(), server)\n",
    "server.add_insecure_port(\"[::]:50051\")\n",
    "server.start()\n",
    "```\n",
    "\n",
    "#### **22.2.3 Dedicated Inference Servers**\n",
    "\n",
    "**TensorFlow Serving:**\n",
    "```bash\n",
    "docker run -p 8501:8501 \\\n",
    "  --mount type=bind,source=/models,target=/models \\\n",
    "  tensorflow/serving \\\n",
    "  --model_name=fraud_model \\\n",
    "  --model_base_path=/models/fraud_model\n",
    "```\n",
    "\n",
    "**TorchServe:**\n",
    "```python\n",
    "# handler.py\n",
    "from ts.torch_handler.base_handler import BaseHandler\n",
    "\n",
    "class ModelHandler(BaseHandler):\n",
    "    def preprocess(self, data):\n",
    "        return torch.tensor(json.loads(data[0][\"body\"])).float()\n",
    "    \n",
    "    def postprocess(self, inference_output):\n",
    "        return [inference_output.tolist()]\n",
    "\n",
    "# Create MAR file\n",
    "torch-model-archiver \\\n",
    "  --model-name resnet \\\n",
    "  --version 1.0 \\\n",
    "  --model-file model.py \\\n",
    "  --serialized-file model.pth \\\n",
    "  --handler handler.py \\\n",
    "  --export-path model_store/\n",
    "```\n",
    "\n",
    "**NVIDIA Triton Inference Server:**\n",
    "- Supports multiple frameworks (ONNX, TensorRT, PyTorch, TF)\n",
    "- Dynamic batching and model ensemble\n",
    "- GPU sharing between models\n",
    "\n",
    "---\n",
    "\n",
    "## **22.3 Kubernetes Deployment**\n",
    "\n",
    "#### **22.3.1 KServe (Kubernetes ML Serving)**\n",
    "\n",
    "Standardized inference platform on Kubernetes.\n",
    "\n",
    "```yaml\n",
    "# inference_service.yaml\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: fraud-detection\n",
    "spec:\n",
    "  predictor:\n",
    "    serviceAccountName: kserve-sa\n",
    "    pytorch:\n",
    "      storageUri: s3://models/fraud-detection\n",
    "      resources:\n",
    "        limits:\n",
    "          memory: \"4Gi\"\n",
    "          cpu: \"2\"\n",
    "          nvidia.com/gpu: \"1\"\n",
    "        requests:\n",
    "          memory: \"2Gi\"\n",
    "          cpu: \"1\"\n",
    "      runtimeVersion: \"1.13.0\"\n",
    "    containerConcurrency: 10  # Max concurrent requests per pod\n",
    "    timeout: 60\n",
    "  transformer:  # Pre/post processing\n",
    "    containers:\n",
    "      - image: fraud-transformer:latest\n",
    "        name: transformer\n",
    "```\n",
    "\n",
    "#### **22.3.2 Auto-scaling Configuration**\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: model-server-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: model-server\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 50\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Pods  # Custom metric: queue length\n",
    "    pods:\n",
    "      metric:\n",
    "        name: inference_queue_length\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"5\"\n",
    "  behavior:\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 10\n",
    "        periodSeconds: 60\n",
    "```\n",
    "\n",
    "#### **22.3.3 Canary & Blue-Green Deployments**\n",
    "\n",
    "```yaml\n",
    "# canary.yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: model-route\n",
    "spec:\n",
    "  hosts:\n",
    "  - model-service\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        canary:\n",
    "          exact: \"true\"\n",
    "    route:\n",
    "    - destination:\n",
    "        host: model-service\n",
    "        subset: v2  # New version\n",
    "      weight: 10\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: model-service\n",
    "        subset: v1  # Current version\n",
    "      weight: 90\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.4 Optimization Strategies**\n",
    "\n",
    "#### **22.4.1 Dynamic Batching**\n",
    "\n",
    "Combine individual requests to improve GPU utilization.\n",
    "\n",
    "```python\n",
    "# Pseudo-code for batching middleware\n",
    "class Batcher:\n",
    "    def __init__(self, model, max_batch_size=32, max_wait_ms=10):\n",
    "        self.model = model\n",
    "        self.max_batch_size = max_batch_size\n",
    "        self.max_wait = max_wait_ms / 1000\n",
    "        self.queue = []\n",
    "    \n",
    "    async def predict(self, input_data):\n",
    "        future = asyncio.Future()\n",
    "        self.queue.append((input_data, future))\n",
    "        \n",
    "        if len(self.queue) >= self.max_batch_size:\n",
    "            self._process_batch()\n",
    "        else:\n",
    "            asyncio.create_task(self._timeout_process())\n",
    "        \n",
    "        return await future\n",
    "    \n",
    "    def _process_batch(self):\n",
    "        if not self.queue:\n",
    "            return\n",
    "        batch_inputs = [item[0] for item in self.queue]\n",
    "        results = self.model(batch_inputs)  # Batch inference\n",
    "        \n",
    "        for (_, future), result in zip(self.queue, results):\n",
    "            future.set_result(result)\n",
    "        self.queue.clear()\n",
    "```\n",
    "\n",
    "#### **22.4.2 Model Quantization for Serving**\n",
    "\n",
    "**Post-Training Quantization (PTQ):**\n",
    "```python\n",
    "# PyTorch Dynamic Quantization\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, \n",
    "    {torch.nn.Linear},  # Layers to quantize\n",
    "    dtype=torch.qint8\n",
    ")\n",
    "# 4x size reduction, 2-3x speedup on CPU\n",
    "\n",
    "# ONNX Runtime Quantization\n",
    "from onnxruntime.quantization import quantize_dynamic, QuantType\n",
    "\n",
    "quantize_dynamic(\n",
    "    \"model.onnx\",\n",
    "    \"model_int8.onnx\",\n",
    "    weight_type=QuantType.QInt8\n",
    ")\n",
    "```\n",
    "\n",
    "#### **22.4.3 Caching Strategies**\n",
    "\n",
    "```python\n",
    "import redis\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class PredictionCache:\n",
    "    def __init__(self, redis_client):\n",
    "        self.redis = redis_client\n",
    "        self.ttl = 300  # 5 minutes\n",
    "    \n",
    "    def get_key(self, features):\n",
    "        # Deterministic hash of input\n",
    "        feature_str = json.dumps(features, sort_keys=True)\n",
    "        return f\"pred:{hashlib.md5(feature_str.encode()).hexdigest()}\"\n",
    "    \n",
    "    async def predict_with_cache(self, model, features):\n",
    "        key = self.get_key(features)\n",
    "        \n",
    "        # Check cache\n",
    "        cached = await self.redis.get(key)\n",
    "        if cached:\n",
    "            return json.loads(cached)\n",
    "        \n",
    "        # Compute and cache\n",
    "        result = model.predict(features)\n",
    "        await self.redis.setex(key, self.ttl, json.dumps(result))\n",
    "        return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.5 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Multi-Format Model Export**\n",
    "Export a trained PyTorch model to multiple formats:\n",
    "\n",
    "1. **Native:** Save as `.pt` (state_dict and TorchScript)\n",
    "2. **ONNX:** Export with dynamic axes, verify with ONNX Runtime\n",
    "3. **TensorRT:** Convert ONNX to TensorRT engine (FP16)\n",
    "4. **Benchmark:** Compare inference latency (PyTorch vs ONNX vs TensorRT)\n",
    "\n",
    "**Deliverable:** Benchmark report showing throughput and latency for each format.\n",
    "\n",
    "### **Lab 2: Production API Development**\n",
    "Build a FastAPI serving application:\n",
    "\n",
    "1. **Endpoints:** `/predict` (sync), `/predict/batch`, `/health`, `/metrics`\n",
    "2. **Validation:** Pydantic models for input validation, error handling\n",
    "3. **Observability:** Prometheus metrics (request count, latency histograms)\n",
    "4. **Load Testing:** Test with locust/k6, identify bottlenecks (CPU vs I/O bound)\n",
    "\n",
    "**Deliverable:** Dockerized API with docker-compose and load test results.\n",
    "\n",
    "### **Lab 3: Kubernetes Deployment**\n",
    "Deploy to K8s with advanced patterns:\n",
    "\n",
    "1. **KServe:** Deploy InferenceService with canary splitting (10% traffic to v2)\n",
    "2. **Auto-scaling:** Configure HPA based on custom metrics (GPU utilization)\n",
    "3. **A/B Testing:** Route traffic based on headers (internal vs. external users)\n",
    "4. **Rolling Update:** Deploy new version with zero-downtime rolling strategy\n",
    "\n",
    "**Deliverable:** Kubernetes manifests, deployment runbook, rollback procedures.\n",
    "\n",
    "### **Lab 4: Edge Deployment**\n",
    "Optimize model for mobile/IoT:\n",
    "\n",
    "1. **Quantization:** INT8 quantization with calibration dataset\n",
    "2. **Mobile:** Convert to CoreML (iOS) or TFLite (Android)\n",
    "3. **Size Optimization:** Pruning to 50% sparsity, measure accuracy loss\n",
    "4. **On-Device Test:** Deploy to mobile simulator, measure inference time\n",
    "\n",
    "**Deliverable:** Mobile-optimized model with size/latency comparison vs. cloud API.\n",
    "\n",
    "---\n",
    "\n",
    "## **22.6 Common Pitfalls**\n",
    "\n",
    "1. **GIL Contention:** Python's Global Interpreter Lock limits multi-threading for CPU-bound inference. **Solution:** Use multi-processing (Uvicorn workers) or switch to ONNX Runtime/TensorRT (release GIL).\n",
    "\n",
    "2. **Memory Leaks:** Loading new model versions without unloading old ones in long-running containers. **Solution:** Implement proper lifecycle management, use separate pods for model updates (immutable infrastructure).\n",
    "\n",
    "3. **Cold Start Latency:** Serverless (Lambda) or scaled-to-zero K8s adds 2-10s startup. **Solution:** Keep minimum replicas warm, use provisioned concurrency, optimize container image size (distroless images).\n",
    "\n",
    "4. **Resource Starvation:** Not setting resource limits causes noisy neighbor issues. **Solution:** Always set CPU/memory limits and requests in K8s; use guaranteed QoS class for latency-critical services.\n",
    "\n",
    "5. **Synchronous I/O Blocking:** Fetching features from DB in request thread blocks the event loop. **Solution:** Use async database drivers (asyncpg, aioredis) or offload to thread pool (`run_in_executor`).\n",
    "\n",
    "---\n",
    "\n",
    "## **22.7 Interview Questions**\n",
    "\n",
    "**Q1:** When would you choose gRPC over REST for model serving?\n",
    "*A: gRPC for: (1) Internal microservices (binary protocol, schema enforcement), (2) High throughput/low latency requirements (HTTP/2 multiplexing, protobuf serialization faster than JSON), (3) Streaming (bidirectional streaming for real-time features). REST for: (1) External/public APIs (browser compatibility, easier debugging), (2) Simple request/response patterns, (3) When human-readable payloads needed. Many systems use both: gRPC internally, REST gateway externally.*\n",
    "\n",
    "**Q2:** How do you handle model versioning and rollback in production?\n",
    "*A: (1) Semantic versioning for models (v1.2.3), stored in model registry (MLflow), (2) Blue-green deployment: run old and new versions simultaneously, shift traffic gradually (canary), (3) Kubernetes: label selectors for version routing, (4) Database: store model_version with predictions for audit/debugging, (5) Rollback: revert traffic to previous stable version via Istio/flagger or K8s deployment rollback, (6) Immutable models: never overwrite deployed artifacts, always deploy new version with new ID.*\n",
    "\n",
    "**Q3:** Explain the trade-offs between dynamic batching and single-request serving.\n",
    "*A: Dynamic batching: Increases throughput (better GPU utilization) at cost of latency (waiting to form batch). Good for high-volume, latency-tolerant (100ms+). Single-request: Lower latency (immediate processing), lower throughput (GPU underutilized for small models). Good for real-time edge cases (<50ms). Hybrid: Use single-request for premium tier, batching for standard tier. Tuning parameters: max_batch_size (higher=more throughput, more latency), max_latency_ms (cap waiting time).*\n",
    "\n",
    "**Q4:** How do you optimize a model that is CPU-bound vs. GPU-bound?\n",
    "*A: CPU-bound optimizations: (1) Quantization to INT8 (vectorized CPU instructions), (2) ONNX Runtime/OpenVINO (optimized kernels), (3) Batch processing to amortize overhead, (4) Multi-processing to bypass GIL. GPU-bound optimizations: (1) TensorRT/ONNX-GPU (kernel fusion), (2) Mixed precision (FP16/TF32), (3) Dynamic batching (higher GPU utilization), (4) Model parallelism for large models, (5) Pipeline parallelism (overlap data transfer and compute). Profile with NVIDIA Nsight or PyTorch Profiler to identify bottlenecks.*\n",
    "\n",
    "**Q5:** Design a deployment strategy for a critical fraud detection model requiring 99.99% uptime.\n",
    "*A: Architecture: Multi-region active-active with global load balancer. Each region: Blue-green deployment with canary analysis (5% → 25% → 100%). Circuit breakers to fallback to rule-based system if model fails. Health checks: Deep health (actual inference test) not just shallow (process up). Database: Predictions logged asynchronously to avoid blocking. Rollback: Automated via flagger/Istio if error rate > threshold or latency p99 > SLA. Shadow mode: New versions run in parallel (log only, don't serve) for 24h before traffic shift. Data: Feature store with multi-region replication, offline mode with cached features.*\n",
    "\n",
    "---\n",
    "\n",
    "## **22.8 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Kubeflow for Machine Learning* (Holden Karau et al.) - K8s for ML workflows\n",
    "- *gRPC: Up and Running* (Kasun Indrasiri) - Service mesh patterns\n",
    "\n",
    "**Papers:**\n",
    "- \"Serving Machine Learning Models with Apache Flink\" (Uber)\n",
    "- \"Clipper: A Low-Latency Online Prediction Serving System\" (UC Berkeley)\n",
    "\n",
    "**Tools:**\n",
    "- **KServe:** Standardized inference on Kubernetes\n",
    "- **Seldon Core:** Advanced ML deployments (A/B tests, ensembles)\n",
    "- **BentoML:** Model packaging and serving framework\n",
    "- **Cortex:** Serverless model serving\n",
    "\n",
    "---\n",
    "\n",
    "## **22.9 Checkpoint Project: Multi-Model Serving Platform**\n",
    "\n",
    "Build a production serving platform handling 3 different model types: image classification (CNN), tabular fraud detection (XGBoost), and NLP sentiment (Transformer).\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Model Packaging:**\n",
    "   - CNN: TensorRT optimized, batch size 8-32 dynamic\n",
    "   - XGBoost: ONNX format, CPU-only (lightweight)\n",
    "   - Transformer: TorchScript, GPU with FP16\n",
    "\n",
    "2. **API Design:**\n",
    "   - Unified gateway routing `/api/v1/cnn`, `/api/v1/fraud`, `/api/v1/sentiment`\n",
    "   - Authentication via JWT tokens\n",
    "   - Rate limiting: 100 req/min per API key\n",
    "\n",
    "3. **Infrastructure:**\n",
    "   - Kubernetes deployment with KServe\n",
    "   - Separate node pools: GPU nodes for CNN/Transformer, CPU nodes for XGBoost\n",
    "   - Horizontal Pod Autoscaler per model type based on queue depth\n",
    "\n",
    "4. **Optimization:**\n",
    "   - Redis cache for fraud model (high hit rate on repeated users)\n",
    "   - Dynamic batching for CNN (max wait 20ms)\n",
    "   - Model warm-up on startup (dummy inference to initialize GPU)\n",
    "\n",
    "5. **Observability:**\n",
    "   - Prometheus metrics: latency histograms (p50, p95, p99), throughput, GPU memory\n",
    "   - Distributed tracing (Jaeger) across gateway → model → feature store\n",
    "   - Alerting: P95 latency > 100ms, error rate > 0.1%\n",
    "\n",
    "6. **Deployment Strategy:**\n",
    "   - Canary releases: 5% traffic to new versions for 1 hour before full rollout\n",
    "   - Automatic rollback if error rate increases\n",
    "   - Circuit breaker pattern to fallback to default responses\n",
    "\n",
    "**Deliverables:**\n",
    "- `serving_platform/` with Kubernetes manifests\n",
    "- Load testing results (achieve 1000 TPS aggregate across models)\n",
    "- Runbook: \"Adding a new model to the platform\"\n",
    "- Cost analysis: $/1000 predictions per model type\n",
    "\n",
    "**Success Criteria:**\n",
    "- Zero-downtime deployment demonstrated\n",
    "- P99 latency < 50ms for fraud, < 200ms for transformer\n",
    "- Auto-scaling tested (traffic spike from 10 → 1000 TPS)\n",
    "- Failed canary automatically rolled back\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 22**\n",
    "\n",
    "*You can now deploy models reliably at scale. Chapter 23 covers Monitoring & Maintenance—ensuring these deployed models remain accurate and healthy over time.*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
