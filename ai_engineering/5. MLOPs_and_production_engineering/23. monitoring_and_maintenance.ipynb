{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0b01eca1",
   "metadata": {},
   "source": [
    "\n",
    "# **CHAPTER 23: MONITORING & MAINTENANCE**\n",
    "\n",
    "*Ensuring Model Reliability in Production*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Deployed models degrade over time due to changing data distributions, shifting user behaviors, and upstream system changes. This chapter establishes the operational practices for detecting degradation, automating retraining, and maintaining model performance through continuous monitoring, alerting, and feedback loops.\n",
    "\n",
    "**Estimated Time:** 30-40 hours (2-3 weeks)  \n",
    "**Prerequisites:** Chapter 22 (Deployment), Chapter 20 (Data Engineering), familiarity with Prometheus/Grafana\n",
    "\n",
    "---\n",
    "\n",
    "## **23.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Implement comprehensive ML monitoring covering data drift, concept drift, and model performance degradation\n",
    "2. Design alerting strategies distinguishing between actionable alerts and noise\n",
    "3. Automate retraining pipelines triggered by performance thresholds or schedules\n",
    "4. Conduct A/B tests and statistical validation for model updates\n",
    "5. Implement graceful degradation strategies and incident response procedures\n",
    "6. Build feedback loops to capture ground truth and calculate business metrics\n",
    "\n",
    "---\n",
    "\n",
    "## **23.1 ML Monitoring Fundamentals**\n",
    "\n",
    "#### **23.1.1 The Three Pillars of ML Monitoring**\n",
    "\n",
    "**1. Data Monitoring:**\n",
    "- **Schema Changes:** Missing columns, type changes, range violations\n",
    "- **Distribution Drift:** KS-test, PSI (Population Stability Index) for feature drift\n",
    "- **Volume Anomalies:** Sudden drops/spikes in data (pipeline failures or business events)\n",
    "\n",
    "**2. Model Performance:**\n",
    "- **Accuracy Metrics:** Accuracy, F1, RMSE (requires ground truth delay)\n",
    "- **Prediction Distribution:** Output class distributions, confidence scores\n",
    "- **Calibration:** Reliability diagrams (predicted vs. actual probability)\n",
    "\n",
    "**3. System Performance:**\n",
    "- **Latency:** P50, P95, P99 response times\n",
    "- **Throughput:** Requests per second\n",
    "- **Resource Utilization:** CPU, GPU memory, I/O\n",
    "\n",
    "#### **23.1.2 Evidently AI for Drift Detection**\n",
    "\n",
    "```python\n",
    "# monitoring/drift_detection.py\n",
    "import evidently\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset, TargetDriftPreset\n",
    "from evidently.metrics import ColumnDriftMetric, DatasetDriftMetric\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class DriftMonitor:\n",
    "    def __init__(self, reference_data: pd.DataFrame):\n",
    "        self.reference = reference_data\n",
    "        self.report = Report(metrics=[\n",
    "            DataDriftPreset(),\n",
    "            ColumnDriftMetric(column_name=\"prediction_confidence\")\n",
    "        ])\n",
    "    \n",
    "    def check_current_data(self, current_data: pd.DataFrame) -> dict:\n",
    "        self.report.run(\n",
    "            reference_data=self.reference,\n",
    "            current_data=current_data\n",
    "        )\n",
    "        \n",
    "        results = self.report.as_dict()\n",
    "        \n",
    "        drift_summary = {\n",
    "            \"timestamp\": datetime.now().isoformat(),\n",
    "            \"dataset_drift\": results[\"metrics\"][1][\"result\"][\"dataset_drift\"],\n",
    "            \"drifted_features_count\": results[\"metrics\"][1][\"result\"][\"number_of_drifted_columns\"],\n",
    "            \"drift_share\": results[\"metrics\"][1][\"result\"][\"drift_share\"]\n",
    "        }\n",
    "        \n",
    "        return drift_summary\n",
    "\n",
    "# Scheduled monitoring (Airflow task)\n",
    "def daily_drift_check():\n",
    "    reference = load_training_data()\n",
    "    yesterday = load_production_data(window=\"1d\")\n",
    "    \n",
    "    monitor = DriftMonitor(reference)\n",
    "    result = monitor.check_current_data(yesterday)\n",
    "    \n",
    "    if result[\"drift_share\"] > 0.3:\n",
    "        trigger_alert(\"DATA_DRIFT\", result)\n",
    "        trigger_retraining_pipeline()\n",
    "    \n",
    "    log_to_monitoring_db(result)\n",
    "```\n",
    "\n",
    "#### **23.1.3 WhyLabs for Statistical Monitoring**\n",
    "\n",
    "Statistical profiles rather than raw data (privacy-preserving).\n",
    "\n",
    "```python\n",
    "from whylogs.app import Session\n",
    "import whylogs as why\n",
    "\n",
    "session = Session()\n",
    "\n",
    "# Profile production data\n",
    "with session.logger(tags={\"env\": \"production\"}) as ylog:\n",
    "    ylog.log_dataframe(production_df)\n",
    "    \n",
    "    # Compare to reference profile\n",
    "    ref_profile = load_reference_profile()\n",
    "    visualization = ylog.profile.view().to_pandas()\n",
    "    \n",
    "    # Check constraints\n",
    "    from whylogs.core.constraints import ConstraintsBuilder\n",
    "    \n",
    "    builder = ConstraintsBuilder(dataset_profile=ylog.profile)\n",
    "    builder.add_constraint(metric=\"column_statistics\", \n",
    "                          column=\"age\", \n",
    "                          constraint=\"greater_than(0)\")\n",
    "    \n",
    "    constraints = builder.build()\n",
    "    report = constraints.generate_constraints_report()\n",
    "    \n",
    "    if not report.valid:\n",
    "        alert_on_constraint_violation(report)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.2 Performance Monitoring & Alerting**\n",
    "\n",
    "#### **23.2.1 Prometheus Metrics for ML**\n",
    "\n",
    "```python\n",
    "# monitoring/metrics.py\n",
    "from prometheus_client import Counter, Histogram, Gauge, Info\n",
    "\n",
    "# Business metrics\n",
    "prediction_counter = Counter(\n",
    "    'ml_predictions_total',\n",
    "    'Total predictions',\n",
    "    ['model_version', 'endpoint']\n",
    ")\n",
    "\n",
    "latency_histogram = Histogram(\n",
    "    'ml_prediction_duration_seconds',\n",
    "    'Inference latency',\n",
    "    buckets=[0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1.0, 2.5, 5.0]\n",
    ")\n",
    "\n",
    "# Model performance (updated when ground truth available)\n",
    "accuracy_gauge = Gauge(\n",
    "    'ml_model_accuracy_1h',\n",
    "    'Rolling accuracy over 1 hour',\n",
    "    ['model_version']\n",
    ")\n",
    "\n",
    "# Data drift\n",
    "drift_gauge = Gauge(\n",
    "    'ml_feature_drift_score',\n",
    "    'Drift score per feature',\n",
    "    ['feature_name']\n",
    ")\n",
    "\n",
    "# Example usage in FastAPI\n",
    "@app.post(\"/predict\")\n",
    "async def predict(request: Request):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    with tracer.start_as_current_span(\"inference\"):\n",
    "        prediction = model.predict(request.features)\n",
    "    \n",
    "    # Record metrics\n",
    "    prediction_counter.labels(\n",
    "        model_version=\"1.2.0\",\n",
    "        endpoint=\"fraud\"\n",
    "    ).inc()\n",
    "    \n",
    "    latency_histogram.observe(time.time() - start_time)\n",
    "    \n",
    "    # Log prediction for later ground truth comparison\n",
    "    log_prediction(request.user_id, prediction, timestamp=time.time())\n",
    "    \n",
    "    return prediction\n",
    "```\n",
    "\n",
    "#### **23.2.2 Alerting Rules**\n",
    "\n",
    "```yaml\n",
    "# prometheus/alerts.yml\n",
    "groups:\n",
    "- name: ml_alerts\n",
    "  rules:\n",
    "  - alert: HighLatency\n",
    "    expr: histogram_quantile(0.95, ml_prediction_duration_seconds_bucket) > 0.5\n",
    "    for: 5m\n",
    "    labels:\n",
    "      severity: warning\n",
    "    annotations:\n",
    "      summary: \"High latency on model {{ $labels.model_version }}\"\n",
    "      \n",
    "  - alert: ModelAccuracyDrop\n",
    "    expr: ml_model_accuracy_1h < 0.85\n",
    "    for: 15m\n",
    "    labels:\n",
    "      severity: critical\n",
    "    annotations:\n",
    "      summary: \"Model accuracy dropped below 85%\"\n",
    "      \n",
    "  - alert: DataDriftDetected\n",
    "    expr: ml_feature_drift_score > 0.5\n",
    "    for: 1h\n",
    "    labels:\n",
    "      severity: warning\n",
    "    annotations:\n",
    "      summary: \"Significant drift in feature {{ $labels.feature_name }}\"\n",
    "      \n",
    "  - alert: PredictionVolumeDrop\n",
    "    expr: rate(ml_predictions_total[5m]) == 0\n",
    "    for: 2m\n",
    "    labels:\n",
    "      severity: critical\n",
    "    annotations:\n",
    "      summary: \"No predictions being served - possible outage\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.3 Automated Retraining**\n",
    "\n",
    "#### **23.3.1 Triggering Strategies**\n",
    "\n",
    "**1. Performance-Based:**\n",
    "```python\n",
    "# Check if accuracy below threshold for 3 consecutive windows\n",
    "def should_retrain(accuracy_history: list[float]) -> bool:\n",
    "    if len(accuracy_history) < 3:\n",
    "        return False\n",
    "    return all(acc < 0.85 for acc in accuracy_history[-3:])\n",
    "```\n",
    "\n",
    "**2. Drift-Based:**\n",
    "```python\n",
    "# Retrain if >40% of features drifted\n",
    "def should_retrain_drift(drift_report: dict) -> bool:\n",
    "    return drift_report[\"drifted_features_ratio\"] > 0.4\n",
    "```\n",
    "\n",
    "**3. Schedule-Based:** Weekly/monthly regardless of performance (catches subtle drift)\n",
    "\n",
    "**4. Volume-Based:** Retrain when 10% new data accumulated since last training\n",
    "\n",
    "#### **23.3.2 Continuous Training Pipeline**\n",
    "\n",
    "```yaml\n",
    "# airflow/dags/retrain_pipeline.py\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator, BranchPythonOperator\n",
    "\n",
    "def check_retrain_needed(**context):\n",
    "    metrics = fetch_recent_metrics(days=7)\n",
    "    if metrics['accuracy'] < 0.85 or metrics['drift_score'] > 0.5:\n",
    "        return 'train_new_model'\n",
    "    return 'skip_retrain'\n",
    "\n",
    "with DAG('continuous_training', schedule_interval='@daily') as dag:\n",
    "    check = BranchPythonOperator(\n",
    "        task_id='check_retrain_needed',\n",
    "        python_callable=check_retrain_needed\n",
    "    )\n",
    "    \n",
    "    train = PythonOperator(\n",
    "        task_id='train_new_model',\n",
    "        python_callable=train_and_validate,\n",
    "        op_kwargs={'data_window': '30d'}\n",
    "    )\n",
    "    \n",
    "    evaluate = PythonOperator(\n",
    "        task_id='shadow_test',\n",
    "        python_callable=deploy_shadow_model\n",
    "    )\n",
    "    \n",
    "    promote = PythonOperator(\n",
    "        task_id='promote_to_production',\n",
    "        python_callable=update_production_endpoint,\n",
    "        trigger_rule='all_success'\n",
    "    )\n",
    "    \n",
    "    skip = PythonOperator(\n",
    "        task_id='skip_retrain',\n",
    "        python_callable=lambda: print(\"Model performing well, skipping\")\n",
    "    )\n",
    "    \n",
    "    check >> [train, skip]\n",
    "    train >> evaluate >> promote\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.4 A/B Testing & Shadow Mode**\n",
    "\n",
    "#### **23.4.1 Statistical Validation**\n",
    "\n",
    "```python\n",
    "# ab_testing.py\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "\n",
    "def ab_test_metric(control_metrics: list[float], \n",
    "                   treatment_metrics: list[float],\n",
    "                   alpha=0.05) -> dict:\n",
    "    \"\"\"\n",
    "    Two-sample t-test for model comparison\n",
    "    \"\"\"\n",
    "    t_stat, p_value = stats.ttest_ind(\n",
    "        control_metrics, \n",
    "        treatment_metrics,\n",
    "        equal_var=False  # Welch's t-test\n",
    "    )\n",
    "    \n",
    "    # Effect size (Cohen's d)\n",
    "    pooled_std = np.sqrt(\n",
    "        (np.std(control_metrics)**2 + np.std(treatment_metrics)**2) / 2\n",
    "    )\n",
    "    cohens_d = (np.mean(treatment_metrics) - np.mean(control_metrics)) / pooled_std\n",
    "    \n",
    "    return {\n",
    "        \"p_value\": p_value,\n",
    "        \"significant\": p_value < alpha,\n",
    "        \"effect_size\": cohens_d,\n",
    "        \"winner\": \"treatment\" if (p_value < alpha and cohens_d > 0) else \"control\"\n",
    "    }\n",
    "\n",
    "# Usage in shadow mode\n",
    "def evaluate_shadow_model():\n",
    "    # Production model predictions (control)\n",
    "    control_preds = get_predictions(model_version=\"v1.2.0\", n=1000)\n",
    "    \n",
    "    # Shadow model predictions (treatment) - same inputs, not served to users\n",
    "    shadow_preds = get_predictions(model_version=\"v1.3.0\", n=1000)\n",
    "    \n",
    "    ground_truth = get_ground_truth(control_preds.timestamps)\n",
    "    \n",
    "    control_acc = calculate_accuracy(control_preds, ground_truth)\n",
    "    shadow_acc = calculate_accuracy(shadow_preds, ground_truth)\n",
    "    \n",
    "    result = ab_test_metric(control_acc, shadow_acc)\n",
    "    \n",
    "    if result[\"significant\"] and result[\"winner\"] == \"treatment\":\n",
    "        promote_model(\"v1.3.0\")\n",
    "```\n",
    "\n",
    "#### **23.4.2 Multi-Armed Bandits**\n",
    "\n",
    "Dynamic traffic allocation favoring better-performing models (faster than fixed A/B).\n",
    "\n",
    "```python\n",
    "# thompson_sampling.py\n",
    "from scipy.stats import beta\n",
    "\n",
    "class ThompsonSamplingBandit:\n",
    "    def __init__(self, n_arms):\n",
    "        self.alpha = np.ones(n_arms)  # Successes\n",
    "        self.beta = np.ones(n_arms)   # Failures\n",
    "    \n",
    "    def select_arm(self):\n",
    "        # Sample from posterior\n",
    "        samples = [np.random.beta(self.alpha[i], self.beta[i]) \n",
    "                  for i in range(len(self.alpha))]\n",
    "        return np.argmax(samples)\n",
    "    \n",
    "    def update(self, arm, reward):\n",
    "        # Bayesian update\n",
    "        if reward > 0:\n",
    "            self.alpha[arm] += 1\n",
    "        else:\n",
    "            self.beta[arm] += 1\n",
    "\n",
    "# Usage: Route traffic to model with highest sampled conversion rate\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.5 Incident Response & Reliability**\n",
    "\n",
    "#### **23.5.1 Circuit Breaker Pattern**\n",
    "\n",
    "Prevent cascade failures when model service degrades.\n",
    "\n",
    "```python\n",
    "# circuit_breaker.py\n",
    "from enum import Enum\n",
    "import time\n",
    "\n",
    "class State(Enum):\n",
    "    CLOSED = \"closed\"      # Normal operation\n",
    "    OPEN = \"open\"          # Failing fast\n",
    "    HALF_OPEN = \"half_open\"  # Testing recovery\n",
    "\n",
    "class CircuitBreaker:\n",
    "    def __init__(self, failure_threshold=5, timeout=60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.timeout = timeout\n",
    "        self.state = State.CLOSED\n",
    "        self.failures = 0\n",
    "        self.last_failure_time = None\n",
    "    \n",
    "    def call(self, func, *args, **kwargs):\n",
    "        if self.state == State.OPEN:\n",
    "            if time.time() - self.last_failure_time > self.timeout:\n",
    "                self.state = State.HALF_OPEN\n",
    "            else:\n",
    "                raise Exception(\"Circuit breaker is OPEN\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self._on_success()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self._on_failure()\n",
    "            raise e\n",
    "    \n",
    "    def _on_success(self):\n",
    "        self.failures = 0\n",
    "        self.state = State.CLOSED\n",
    "    \n",
    "    def _on_failure(self):\n",
    "        self.failures += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        if self.failures >= self.failure_threshold:\n",
    "            self.state = State.OPEN\n",
    "\n",
    "# Usage\n",
    "breaker = CircuitBreaker(failure_threshold=3, timeout=30)\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "def predict(request):\n",
    "    try:\n",
    "        return breaker.call(model.predict, request.features)\n",
    "    except:\n",
    "        # Fallback to rule-based system or cached response\n",
    "        return fallback_prediction(request)\n",
    "```\n",
    "\n",
    "#### **23.5.2 Graceful Degradation**\n",
    "\n",
    "```python\n",
    "def get_prediction_with_fallback(user_id, features):\n",
    "    try:\n",
    "        # Try primary model (complex, accurate)\n",
    "        return ml_model.predict(features)\n",
    "    except ModelTimeout:\n",
    "        try:\n",
    "            # Fallback to lighter model (cached, faster)\n",
    "            return light_model.predict(features)\n",
    "        except:\n",
    "            # Ultimate fallback: heuristic/rules\n",
    "            return rule_based_heuristic(user_id)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Monitoring Dashboard**\n",
    "Build a Grafana dashboard for model monitoring:\n",
    "\n",
    "1. **Data Drift:** Heatmap of feature drift scores over time\n",
    "2. **Performance:** Accuracy, precision, recall (requires delayed ground truth)\n",
    "3. **Business Metrics:** Revenue per prediction, conversion rates\n",
    "4. **System Health:** Latency percentiles, error rates, GPU utilization\n",
    "5. **Alerting:** Configure alerts for accuracy drop > 5%\n",
    "\n",
    "**Deliverable:** Dashboard JSON and alert configuration.\n",
    "\n",
    "### **Lab 2: Automated Retraining**\n",
    "Implement continuous training:\n",
    "\n",
    "1. **Trigger:** Detect drift using Evidently on daily batch\n",
    "2. **Pipeline:** Automatically start training job when drift > threshold\n",
    "3. **Validation:** Compare new model vs. production on holdout set\n",
    "4. **Deployment:** Automatic canary deployment if performance improves\n",
    "\n",
    "**Deliverable:** Airflow DAG with automated retraining logic.\n",
    "\n",
    "### **Lab 3: A/B Testing Framework**\n",
    "Set up statistical comparison:\n",
    "\n",
    "1. **Traffic Split:** Route 10% traffic to challenger model (v2)\n",
    "2. **Metrics:** Track conversion rate, latency, error rate\n",
    "3. **Analysis:** Calculate p-values, confidence intervals\n",
    "4. **Auto-promote:** Promote v2 if statistically significant improvement (p < 0.05)\n",
    "\n",
    "**Deliverable:** A/B test framework with promotion logic.\n",
    "\n",
    "### **Lab 4: Chaos Engineering**\n",
    "Test resilience:\n",
    "\n",
    "1. **Failure Injection:** Randomly kill model pods, inject latency\n",
    "2. **Circuit Breaker:** Verify fallback triggers correctly\n",
    "3. **Recovery:** Measure time to recovery (MTTR)\n",
    "4. **Data Corruption:** Send malformed inputs, verify error handling\n",
    "\n",
    "**Deliverable:** Chaos test report with resilience improvements.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.7 Common Pitfalls**\n",
    "\n",
    "1. **Alert Fatigue:** Too many false positives cause engineers to ignore alerts. **Solution:** Use anomaly detection (seasonality-aware), consolidate related alerts, prioritize actionable metrics.\n",
    "\n",
    "2. **Ground Truth Delay:** For fraud detection, true labels may take weeks. **Solution:** Use proxy metrics (chargeback rate within 24h) or unsupervised drift detection as early warning.\n",
    "\n",
    "3. **Cold Start Monitoring:** New models start with zero predictions, causing division by zero in accuracy calculations. **Solution:** Minimum sample size thresholds before calculating metrics.\n",
    "\n",
    "4. **Ignoring Business Metrics:** Model accuracy up 2%, but revenue down 10% (model gaming metric). **Solution:** Always track business KPIs (conversion, revenue, user satisfaction) alongside ML metrics.\n",
    "\n",
    "5. **Static Thresholds:** Fixed thresholds (e.g., accuracy < 0.8) don't account for seasonality (holiday shopping patterns). **Solution:** Dynamic thresholds based on historical ranges or forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.8 Interview Questions**\n",
    "\n",
    "**Q1:** How do you detect concept drift vs. data drift, and why does the distinction matter?\n",
    "*A: Data drift (covariate shift): P(X) changes—input feature distributions shift (e.g., age range of users changes). Detected via statistical tests (KS, PSI). Concept drift: P(Y|X) changes—relationship between features and target changes (e.g., fraud patterns evolve). Detected via performance degradation on ground truth or prediction confidence distribution shifts. Distinction matters because: (1) Data drift may not affect performance if decision boundary unchanged, (2) Concept drift always requires retraining, (3) Different mitigation strategies (feature adaptation vs. model retraining).*\n",
    "\n",
    "**Q2:** Describe your strategy for monitoring models when ground truth is delayed (e.g., credit default takes 2 years).\n",
    "*A: (1) Upstream monitoring: Detect data drift in inputs as early warning, (2) Proxy metrics: Short-term indicators correlated with final outcome (missed payment within 30 days), (3) Prediction distribution monitoring: Sudden shifts in model confidence or output distribution suggest issues, (4) Human-in-the-loop: Sample predictions for manual review to estimate current accuracy, (5) Counterfactual evaluation: How would old model perform on new data? (requires holdout set), (6) Business metric tracking: Default rate trends, even without individual labels.*\n",
    "\n",
    "**Q3:** How do you decide between scheduled retraining vs. performance-triggered retraining?\n",
    "*A: Scheduled: Good for stable domains, simpler operations, predictable costs. Triggered: Good for dynamic environments, cost-efficient (don't train if not needed), faster response to drift. Hybrid approach: (1) Minimum scheduled retraining (monthly) to incorporate new data, (2) Triggered retraining for emergency drift detection (>30% features drifted or accuracy drop >5%), (3) Continuous training for high-velocity systems (daily incremental updates).*\n",
    "\n",
    "**Q4:** What is shadow mode deployment, and when would you use it?\n",
    "*A: Shadow mode: Route production traffic to new model (challenger) without serving its predictions to users. Log predictions for comparison against current model (control). Use when: (1) High risk of new model failure (safety-critical), (2) Ground truth unavailable immediately—need to accumulate prediction logs before evaluation, (3) Testing infrastructure capacity (can new model handle production load?), (4) Regulatory requirements to validate before serving. Limitation: Doesn't test user reaction to new predictions (only technical correctness).*\n",
    "\n",
    "**Q5:** Design a circuit breaker for a real-time fraud detection system.\n",
    "*A: States: Closed (normal), Open (failing fast), Half-Open (testing recovery). Configuration: Failure threshold 5 errors in 1 minute, timeout 30s. Fallback hierarchy: (1) Try cached prediction for user, (2) Use lighter rule-based model (no ML), (3) Approve transaction (business decision—false negatives better than blocking all). Monitoring: Alert when circuit opens, track fallback rate. Recovery: Half-open allows 1 request per minute to test health before closing. Implementation: Use libraries like `pybreaker` or Istio/Envoy outlier detection for service mesh level.*\n",
    "\n",
    "---\n",
    "\n",
    "## **23.9 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Practical Machine Learning for Computer Vision* (O'Reilly) - Monitoring CV models\n",
    "- *Site Reliability Engineering* (Google) - General reliability principles\n",
    "\n",
    "**Papers:**\n",
    "- \"Monitoring Machine Learning Models in Production\" (Sashank, 2021)\n",
    "- \"The ML Test Score: A Rubric for ML Production Readiness\"\n",
    "\n",
    "**Tools:**\n",
    "- **Evidently AI:** Drift detection and reports\n",
    "- **WhyLabs:** Statistical monitoring\n",
    "- **Arize AI:** ML observability platform\n",
    "- **Fiddler:** Model performance management\n",
    "\n",
    "---\n",
    "\n",
    "## **23.10 Checkpoint Project: Production Monitoring System**\n",
    "\n",
    "Build a complete monitoring and maintenance system for the fraud detection model from Chapter 22.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Monitoring Stack:**\n",
    "   - Prometheus for metrics collection\n",
    "   - Grafana for visualization\n",
    "   - Evidently for drift reports\n",
    "   - PagerDuty/Opsgenie for alerting\n",
    "\n",
    "2. **Metrics Pipeline:**\n",
    "   - Log every prediction with features, timestamp, model version\n",
    "   - Daily batch job calculating drift vs. training set\n",
    "   - Weekly accuracy calculation (using confirmed fraud labels)\n",
    "   - Business metrics: Fraud caught ($), false positive rate\n",
    "\n",
    "3. **Alerting Rules:**\n",
    "   - Critical: Accuracy < 80%, system down (0 RPS), P99 latency > 500ms\n",
    "   - Warning: Drift detected in >3 features, error rate > 1%\n",
    "   - Info: Model version nearing retirement age (30 days old)\n",
    "\n",
    "4. **Automated Response:**\n",
    "   - Drift detected → Trigger retraining pipeline → Shadow test → Auto-promote if better\n",
    "   - Circuit breaker opens → Fallback to rules engine → Page on-call engineer\n",
    "   - Accuracy drop → Immediate rollback to previous model version\n",
    "\n",
    "5. **Reporting:**\n",
    "   - Weekly automated report: Model performance, drift summary, business impact\n",
    "   - Monthly review: Feature importance shifts, retraining recommendations\n",
    "\n",
    "**Deliverables:**\n",
    "- `monitoring/` directory with Prometheus configs, Grafana dashboards\n",
    "- `maintenance/` with retraining DAGs and rollback scripts\n",
    "- Runbook: \"Incident Response: Model Serving Outage\"\n",
    "- Demo: Simulated drift causing automated retraining\n",
    "\n",
    "**Success Criteria:**\n",
    "- Zero false positives in alerting (tuned thresholds)\n",
    "- Automated retraining successfully deployed improved model\n",
    "- <5 minute time to detection (TTD) for accuracy drop\n",
    "- <15 minute time to recovery (TTR) using rollback\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 23**\n",
    "\n",
    "*You can now maintain ML systems in production with confidence. Chapter 24 covers Responsible AI & Ethics—ensuring your systems are fair, explainable, and secure.*\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
