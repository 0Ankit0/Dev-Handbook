{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "217119c2",
   "metadata": {},
   "source": [
    "Here is **Chapter 19: ML System Design & Architecture** — engineering systems that scale.\n",
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 19: ML SYSTEM DESIGN & ARCHITECTURE**\n",
    "\n",
    "*Designing for Scale and Reliability*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Building a model is the first 10% of the journey; deploying it reliably at scale is the remaining 90%. This chapter transitions from data science to machine learning engineering, covering the architectural patterns, design trade-offs, and system components required to serve millions of predictions per second. You will learn to navigate the fundamental tensions: latency vs. throughput, cost vs. accuracy, batch vs. real-time, and consistency vs. availability.\n",
    "\n",
    "**Estimated Time:** 40-50 hours (3 weeks)  \n",
    "**Prerequisites:** Chapters 4 (Tools), 9 (Evaluation), and all modeling chapters (understanding what you're deploying)\n",
    "\n",
    "---\n",
    "\n",
    "## **19.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Design end-to-end ML systems with clear separation of concerns (feature platform, training, serving, monitoring)\n",
    "2. Select appropriate serving patterns (batch, online, streaming) based on latency and freshness requirements\n",
    "3. Architect for horizontal scalability using distributed systems principles and cloud-native technologies\n",
    "4. Optimize inference latency and throughput through caching, batching, and model compression\n",
    "5. Perform capacity planning and cost optimization for cloud-based ML infrastructure\n",
    "6. Design resilient systems that handle cascading failures, model degradation, and data drift\n",
    "\n",
    "---\n",
    "\n",
    "## **19.1 ML System Components**\n",
    "\n",
    "A production ML system is not just a model; it's a pipeline of interconnected services.\n",
    "\n",
    "#### **19.1.1 The ML Platform Architecture**\n",
    "\n",
    "```\n",
    "┌─────────────────────────────────────────────────────────────┐\n",
    "│                        Data Sources                          │\n",
    "│  (Databases, Logs, Streams, External APIs)                   │\n",
    "└──────────────────────┬──────────────────────────────────────┘\n",
    "                       │\n",
    "┌──────────────────────▼──────────────────────────────────────┐\n",
    "│                  Data Ingestion Layer                        │\n",
    "│  (Kafka, Kinesis, Airflow, Spark Streaming)                 │\n",
    "└──────────────────────┬──────────────────────────────────────┘\n",
    "                       │\n",
    "        ┌──────────────┼──────────────┐\n",
    "        │              │              │\n",
    "┌───────▼──────┐ ┌────▼─────┐ ┌──────▼──────┐\n",
    "│   Feature    │ │ Training │ │  Monitoring │\n",
    "│    Store     │ │ Pipeline │ │   & Logging │\n",
    "│ (Feast,      │ │(Kubeflow,│ │ (Evidently, │\n",
    "│  Tecton)     │ │ Vertex)  │ │  WhyLabs)   │\n",
    "└───────┬──────┘ └────┬─────┘ └──────┬──────┘\n",
    "        │             │              │\n",
    "        └─────────────┼──────────────┘\n",
    "                      │\n",
    "┌─────────────────────▼───────────────────────────────────────┐\n",
    "│                    Model Registry                            │\n",
    "│        (MLflow, Weights & Biases, Vertex AI)                │\n",
    "└─────────────────────┬───────────────────────────────────────┘\n",
    "                      │\n",
    "       ┌──────────────┼──────────────┐\n",
    "       │              │              │\n",
    "┌──────▼──────┐ ┌────▼─────┐ ┌──────▼──────┐\n",
    "│   Batch     │ │  Online  │ │  Streaming  │\n",
    "│  Inference  │ │  Serving │ │  Inference  │\n",
    "│ (Spark,     │ │(TF Serving│ │ (Flink,    │\n",
    "│  Ray)       │ │ TorchServe│ │  Kafka Streams)│\n",
    "└─────────────┘ └──────────┘ └─────────────┘\n",
    "```\n",
    "\n",
    "#### **19.1.2 The Feature Store**\n",
    "\n",
    "Centralized storage for feature vectors, solving the **training-serving skew** problem.\n",
    "\n",
    "**Components:**\n",
    "- **Offline Store:** Historical data for training (Data warehouse: BigQuery, Snowflake)\n",
    "- **Online Store:** Low-latency serving (Redis, DynamoDB, Cassandra)\n",
    "- **Feature Registry:** Metadata, versioning, lineage tracking\n",
    "\n",
    "**Why it matters:** If training uses `avg_price_last_7_days` computed at 2 AM, but serving computes it at request time, you have skew. Feature store ensures identical computation.\n",
    "\n",
    "```python\n",
    "# Pseudo-code for feature retrieval\n",
    "from feast import FeatureStore\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "\n",
    "# Online retrieval (for real-time prediction)\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"user_transactions:avg_spend_30d\",\n",
    "        \"user_transactions:transaction_count_7d\"\n",
    "    ],\n",
    "    entity_rows=[{\"user_id\": \"user_123\"}]\n",
    ").to_df()\n",
    "\n",
    "# Offline retrieval (for training)\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,  # Timestamps of when predictions were made\n",
    "    features=[...]\n",
    ").to_df()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **19.2 Design Patterns**\n",
    "\n",
    "#### **19.2.1 Batch Prediction**\n",
    "\n",
    "Process large datasets periodically (hourly, daily).\n",
    "\n",
    "**When to use:** \n",
    "- No strict latency requirements (minutes to hours acceptable)\n",
    "- Large volume (millions of predictions)\n",
    "- Features don't change between runs (e.g., overnight risk scoring)\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "S3/Data Lake → Spark Job → Model Inference → Results → Database/Notifications\n",
    "```\n",
    "\n",
    "**Pros:** Simple, scalable, cost-effective (spot instances)  \n",
    "**Cons:** Stale predictions, high latency\n",
    "\n",
    "#### **19.2.2 Real-Time (Online) Inference**\n",
    "\n",
    "Synchronous API calls with millisecond latency requirements.\n",
    "\n",
    "**When to use:**\n",
    "- User-facing applications (search ranking, recommendations, fraud detection)\n",
    "- Features available at request time\n",
    "- SLA: <100ms p99 latency\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Client → API Gateway → Load Balancer → Model Server (K8s) → Response\n",
    "                    ↓\n",
    "            Feature Store (Redis cache)\n",
    "```\n",
    "\n",
    "**Challenges:**\n",
    "- Cold start (container startup time)\n",
    "- Scaling to zero vs. keeping warm (cost vs. latency trade-off)\n",
    "- Feature freshness\n",
    "\n",
    "#### **19.2.3 Streaming Inference**\n",
    "\n",
    "Process events as they arrive, maintaining state.\n",
    "\n",
    "**When to use:**\n",
    "- Real-time anomaly detection (network intrusion)\n",
    "- Session-based recommendations (update as user clicks)\n",
    "- IoT sensor monitoring\n",
    "\n",
    "**Architecture:**\n",
    "```\n",
    "Kafka/Kinesis → Stream Processor (Flink/Spark Streaming) → State Store → Predictions\n",
    "                    ↓\n",
    "            Feature updates to online store\n",
    "```\n",
    "\n",
    "**Windowing:** \n",
    "- **Tumbling:** Fixed-size, non-overlapping (e.g., every 5 minutes)\n",
    "- **Sliding:** Overlapping windows (e.g., last 5 minutes, computed every minute)\n",
    "- **Session:** Dynamic based on user activity gaps\n",
    "\n",
    "---\n",
    "\n",
    "## **19.3 Scalability Patterns**\n",
    "\n",
    "#### **19.3.1 Horizontal Scaling**\n",
    "\n",
    "Add more machines rather than bigger machines.\n",
    "\n",
    "**Stateless Serving:** Multiple model server pods behind load balancer. Any pod can handle any request.\n",
    "\n",
    "**Sharding:** Partition data by key (user_id % num_shards) to distribute load and cache locality.\n",
    "\n",
    "```yaml\n",
    "# Kubernetes HPA (Horizontal Pod Autoscaler)\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: model-server\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: model-server\n",
    "  minReplicas: 3\n",
    "  maxReplicas: 100\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: custom_queue_length\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"10\"\n",
    "```\n",
    "\n",
    "#### **19.3.2 Load Balancing Strategies**\n",
    "\n",
    "- **Round Robin:** Even distribution (simple, ignores capacity)\n",
    "- **Least Connections:** To pods with fewest active requests (good for varying latency)\n",
    "- **Consistent Hashing:** Same user always hits same pod (benefits caching, but hot spots)\n",
    "- **Model Sharding:** Different pods serve different model partitions (for massive models)\n",
    "\n",
    "#### **19.3.3 Caching Strategies**\n",
    "\n",
    "**Feature Cache:** Cache expensive feature lookups (Redis).\n",
    "```\n",
    "Cache Hit: 1ms\n",
    "Cache Miss: 50ms (compute from raw data)\n",
    "```\n",
    "\n",
    "**Prediction Cache:** Cache model outputs for identical inputs (high hit rate for popular items).\n",
    "- **TTL (Time To Live):** How long to keep cached predictions (trading freshness for speed)\n",
    "- **Cache Invalidation:** When model updates, clear relevant cache entries\n",
    "\n",
    "**CDN for Edge Inference:** Deploy models to CDN edge locations (Cloudflare Workers, Lambda@Edge) for <50ms global latency.\n",
    "\n",
    "---\n",
    "\n",
    "## **19.4 Latency and Throughput Optimization**\n",
    "\n",
    "#### **19.4.1 Model Optimization**\n",
    "\n",
    "**Quantization:** INT8 vs FP32 (2-4x speedup, slight accuracy loss)  \n",
    "**Pruning:** Remove 50% of weights, compress model  \n",
    "**Knowledge Distillation:** Train small \"student\" to mimic large \"teacher\"  \n",
    "**ONNX/TensorRT:** Optimized inference engines (kernel fusion, memory optimization)\n",
    "\n",
    "#### **19.4.2 Dynamic Batching**\n",
    "\n",
    "Combine multiple individual requests into a batch for GPU efficiency.\n",
    "\n",
    "```\n",
    "Without Batching: [1], [1], [1] → 3 forward passes (GPU underutilized)\n",
    "With Batching: [1,1,1] → 1 forward pass (3x throughput)\n",
    "```\n",
    "\n",
    "**Trade-off:** Waiting to form a batch adds latency (batching delay). Tune `max_batch_size` and `max_latency_ms`.\n",
    "\n",
    "```python\n",
    "# TensorFlow Serving batching parameters\n",
    "max_batch_size { value: 64 }\n",
    "batch_timeout_micros { value: 50000 }  # Wait max 50ms to fill batch\n",
    "max_enqueued_batches { value: 100 }\n",
    "```\n",
    "\n",
    "#### **19.4.3 Async Processing**\n",
    "\n",
    "For non-blocking operations:\n",
    "1. Client sends request, gets `request_id` immediately\n",
    "2. Request queued (Kafka/SQS)\n",
    "3. Worker processes asynchronously\n",
    "4. Client polls or receives webhook notification\n",
    "\n",
    "**Use case:** Heavy inference (video analysis), email generation, report creation.\n",
    "\n",
    "---\n",
    "\n",
    "## **19.5 Cost Optimization**\n",
    "\n",
    "#### **19.5.1 Infrastructure Cost Drivers**\n",
    "\n",
    "1. **Compute:** GPU instances ($2-30/hour depending on type)\n",
    "2. **Storage:** Model artifacts, feature logs, training data\n",
    "3. **Network:** Egress fees (moving data between regions or clouds)\n",
    "4. **Vendor Lock-in:** Managed ML services (SageMaker, Vertex AI) premium vs. self-managed\n",
    "\n",
    "#### **19.5.2 Spot/Preemptible Instances**\n",
    "\n",
    "Use spot instances for:\n",
    "- Batch training (checkpoint frequently)\n",
    "- Batch inference\n",
    "- Development/experimentation\n",
    "\n",
    "**Risk:** Instances can be reclaimed with 2-minute warning. Architect for fault tolerance.\n",
    "\n",
    "#### **19.5.3 Multi-Cloud and Hybrid**\n",
    "\n",
    "Avoid cloud lock-in:\n",
    "- **Kubernetes:** Portable orchestration\n",
    "- **KServe:** Standardized model serving on any cloud\n",
    "- **Feature Store:** Abstraction layer over underlying storage\n",
    "\n",
    "#### **19.5.4 Model Right-Sizing**\n",
    "\n",
    "Don't use A100 GPUs for simple logistic regression. Match hardware to model complexity:\n",
    "- **CPU:** Scikit-learn, small neural nets (<10MB), high throughput/low latency\n",
    "- **GPU:** Deep learning (CNNs, Transformers), batch processing\n",
    "- **TPU:** Massive matrix multiplications (training large LLMs)\n",
    "\n",
    "---\n",
    "\n",
    "## **19.6 Reliability and Fault Tolerance**\n",
    "\n",
    "#### **19.6.1 Circuit Breakers**\n",
    "\n",
    "Prevent cascade failures. If model service is down, fail fast and return default response rather than timing out.\n",
    "\n",
    "```python\n",
    "from circuitbreaker import circuit\n",
    "\n",
    "@circuit(failure_threshold=5, recovery_timeout=60)\n",
    "def get_prediction(features):\n",
    "    return model.predict(features)\n",
    "\n",
    "# After 5 failures, circuit opens, immediately raises CircuitBreakerError\n",
    "# Can catch this and return fallback: return {\"score\": 0.5, \"fallback\": true}\n",
    "```\n",
    "\n",
    "#### **19.6.2 Graceful Degradation**\n",
    "\n",
    "When system overloaded:\n",
    "- **Shed load:** Return 503 with Retry-After header\n",
    "- **Simplify model:** Route to lighter model (e.g., switch from GPT-4 to GPT-3.5, or from CNN to logistic regression)\n",
    "- **Cache staleness:** Serve slightly stale predictions if fresh compute failing\n",
    "\n",
    "#### **19.6.3 Shadow Mode Deployment**\n",
    "\n",
    "Route production traffic to new model version without returning its predictions (log only). Compare new model outputs to current production model to validate before switching traffic.\n",
    "\n",
    "---\n",
    "\n",
    "## **19.7 Workbook Labs**\n",
    "\n",
    "### **Lab 1: System Design Document**\n",
    "Design ML system for fraud detection at payment processor (10,000 TPS, <50ms latency):\n",
    "\n",
    "1. **Requirements:** Functional and non-functional (latency, availability, throughput)\n",
    "2. **Data Flow:** From transaction event to score returned\n",
    "3. **Component Diagram:** Feature store, model registry, serving infrastructure\n",
    "4. **Trade-off Analysis:** Why Redis vs Cassandra? Why online vs batch features?\n",
    "5. **Failure Modes:** What happens if feature store is down? Model version mismatch?\n",
    "\n",
    "**Deliverable:** Architecture document with diagrams (draw.io or Excalidraw) and reasoning.\n",
    "\n",
    "### **Lab 2: Load Testing**\n",
    "Deploy a simple model (scikit-learn or PyTorch) to FastAPI:\n",
    "\n",
    "1. **Baseline:** Single instance throughput (requests/sec, latency p50/p99)\n",
    "2. **Bottleneck Identification:** CPU bound? I/O bound? Memory bound?\n",
    "3. **Optimization:** Add Redis caching for features, implement batching\n",
    "4. **Scaling:** Deploy to Kubernetes, configure HPA, test under load (k6 or Locust)\n",
    "\n",
    "**Deliverable:** Performance report showing latency distribution before/after optimizations.\n",
    "\n",
    "### **Lab 3: Cost Analysis**\n",
    "Given cloud bill for ML training ($10k/month), optimize:\n",
    "\n",
    "1. **Identify waste:** Idle GPUs, oversized instances, unused storage\n",
    "2. **Spot instance migration:** Which workloads can tolerate interruptions?\n",
    "3. **Model compression:** Quantization to reduce GPU requirements\n",
    "4. **ROI Calculation:** Cost per prediction, break-even analysis\n",
    "\n",
    "**Deliverable:** Cost reduction proposal with 30% savings target.\n",
    "\n",
    "### **Lab 4: Disaster Recovery Plan**\n",
    "Design for region failure:\n",
    "\n",
    "1. **Multi-region deployment:** Active-active or active-passive?\n",
    "2. **Data replication:** RPO (Recovery Point Objective) and RTO (Recovery Time Objective)\n",
    "3. **Model artifact backup:** Versioning across regions\n",
    "4. **Failover testing:** Simulate us-east-1 outage, measure failover time\n",
    "\n",
    "**Deliverable:** Runbook for on-call engineers with step-by-step recovery procedures.\n",
    "\n",
    "---\n",
    "\n",
    "## **19.8 Common Pitfalls**\n",
    "\n",
    "1. **Training-Serving Skew:** Different code paths for feature engineering in training vs. serving. **Solution:** Shared feature transformation libraries (same Docker image for both).\n",
    "\n",
    "2. **Underestimating Cold Start:** Serverless (Lambda) or scaled-to-zero Kubernetes adds 1-10s latency on first request. **Solution:** Keep minimum replicas warm, use provisioned concurrency.\n",
    "\n",
    "3. **Ignoring Backpressure:** When downstream is slow, queue builds up infinitely, causing OOM. **Solution:** Bounded queues with load shedding, circuit breakers.\n",
    "\n",
    "4. **Synchronous Feature Computation:** Computing features in request path adds latency. **Solution:** Pre-materialize features, use streaming feature computation.\n",
    "\n",
    "5. **Not Versioning Everything:** Model v2 expects different features than v1, causing crashes. **Solution:** Version APIs, features, and models together (immutable infrastructure).\n",
    "\n",
    "---\n",
    "\n",
    "## **19.9 Interview Questions**\n",
    "\n",
    "**Q1:** Design a recommendation system for YouTube (1 billion users, 1 million videos). How do you handle the scale?\n",
    "*A: Two-stage approach: (1) Candidate generation: Two-tower neural network retrieves ~100 videos from 1M using FAISS (approximate nearest neighbors), runs in <10ms. (2) Ranking: Heavy model (deep neural net with hundreds of features) scores the 100 candidates, runs on GPU in <50ms. Features pre-computed and cached in Redis. Model sharded by user geography for locality.*\n",
    "\n",
    "**Q2:** When would you choose batch inference over real-time inference?\n",
    "*A: Batch when: (1) No strict latency SLA (minutes/hours OK), (2) Large volume (millions of predictions), (3) Features don't change rapidly (e.g., credit risk updated nightly), (4) Cost-sensitive (can use spot instances, no need for 24/7 running servers). Real-time when: (1) User-facing latency requirements (<100ms), (2) Features depend on real-time context (current cart contents, location), (3) Immediate action required (fraud block, dynamic pricing).*\n",
    "\n",
    "**Q3:** How do you prevent training-serving skew?\n",
    "*A: (1) Shared feature engineering code (library used in both training and serving pipelines), (2) Feature store that guarantees point-in-time correctness (serving exactly the feature values that would have been available at prediction time in training), (3) Immutable data pipelines with versioning, (4) Integration tests that validate feature parity between environments.*\n",
    "\n",
    "**Q4:** Explain the difference between horizontal and vertical scaling in ML serving.\n",
    "*A: Vertical scaling: Bigger machine (more CPU/GPU/RAM) for single instance. Limited by hardware ceiling, expensive, single point of failure. Horizontal scaling: More machines behind load balancer. Better fault tolerance, theoretically unlimited scale, requires stateless design. For ML serving, horizontal is preferred with model replication; vertical used when model too large for single GPU (then use model parallelism).*\n",
    "\n",
    "**Q5:** How do you handle model updates without downtime?\n",
    "*A: Blue-green deployment: Deploy new model version alongside old (shadow mode or small traffic percentage), gradually shift traffic using canary releases (5% → 25% → 100%). If error rate spikes, automatic rollback. Kubernetes with rolling updates or traffic splitting (Istio/Linkerd). Key is maintaining backward compatibility in API contract or versioning endpoints (/v1/predict, /v2/predict).*\n",
    "\n",
    "---\n",
    "\n",
    "## **19.10 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Designing Machine Learning Systems* (Chip Huyen) - Comprehensive ML system design\n",
    "- *Building Machine Learning Pipelines* (Hannes Hapke, Catherine Nelson) - TensorFlow Extended\n",
    "- *Site Reliability Engineering* (Google) - For reliability concepts applied to ML\n",
    "\n",
    "**Papers:**\n",
    "- \"Hidden Technical Debt in Machine Learning Systems\" (Sculley et al., 2015) - Classic on MLops complexity\n",
    "- \"Machine Learning: The High Interest Credit Card of Technical Debt\"\n",
    "\n",
    "**Tools:**\n",
    "- **Feast:** Open source feature store\n",
    "- **KServe:** Kubernetes-native model serving\n",
    "- **MLflow:** Model registry and experiment tracking\n",
    "- **Evidently:** ML monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## **19.11 Checkpoint Project: Production-Grade Fraud Detection System**\n",
    "\n",
    "Build a complete fraud detection system for a fictional payment company processing 10,000 transactions/second.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Latency SLA:** p99 < 50ms end-to-end (feature retrieval + inference)\n",
    "2. **Throughput:** Handle 10k TPS with burst to 50k TPS (Black Friday)\n",
    "3. **Features:**\n",
    "   - Real-time: Velocity features (txns/minute from card), device fingerprint matching\n",
    "   - Batch: Customer historical risk score (updated hourly)\n",
    "   - Streaming: Aggregate merchant chargeback rate (5-minute window)\n",
    "\n",
    "4. **Architecture:**\n",
    "   - **Ingestion:** Kafka for transaction events\n",
    "   - **Feature Store:** Redis (online) + S3 (offline) with Feast\n",
    "   - **Model:** XGBoost (lightweight, explainable) with option to shadow-test deep model\n",
    "   - **Serving:** FastAPI with asyncio, deployed on EKS with Karpenter (auto-scaling)\n",
    "   - **Monitoring:** Prometheus metrics, Grafana dashboards, Evidently for data drift\n",
    "\n",
    "5. **Reliability:**\n",
    "   - Circuit breaker: If feature store down, use cached/default features and alert\n",
    "   - Fallback model: Simple rules-based if ML model fails\n",
    "   - Multi-AZ deployment\n",
    "\n",
    "6. **Testing:**\n",
    "   - Load test: 50k TPS sustained for 1 hour\n",
    "   - Chaos engineering: Randomly kill pods, verify automatic recovery\n",
    "   - A/B test: New model version gets 5% traffic, compare fraud catch rate vs false positive rate\n",
    "\n",
    "**Deliverables:**\n",
    "- `fraud_system/` with Terraform/Kubernetes manifests\n",
    "- Architecture diagram with data flow\n",
    "- Runbook: \"Incident Response: Feature Store Outage\"\n",
    "- Cost estimate: Monthly AWS bill breakdown\n",
    "\n",
    "**Success Criteria:**\n",
    "- System handles load test without errors\n",
    "- Latency p99 < 50ms at 10k TPS\n",
    "- Zero downtime deployment of new model version\n",
    "- Automatic failover demonstrated in chaos test\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 19**\n",
    "\n",
    "*You now understand how to design ML systems for production. Chapter 20 will cover Data Engineering for ML — building the pipelines that feed these systems.*\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
