{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "619bcbeb",
   "metadata": {},
   "source": [
    "\n",
    "---\n",
    "\n",
    "# **CHAPTER 20: DATA ENGINEERING FOR ML**\n",
    "\n",
    "*Building Robust Data Pipelines for Production AI*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Machine learning models are only as reliable as the data pipelines feeding them. This chapter bridges the gap between raw data and ML-ready datasets, covering the engineering practices required to build scalable, maintainable, and observable data infrastructure. You will learn to design ETL/ELT pipelines, implement feature stores, validate data quality, and handle both batch and streaming data at scale.\n",
    "\n",
    "**Estimated Time:** 35-45 hours (3 weeks)  \n",
    "**Prerequisites:** Chapter 19 (ML System Design), Chapter 5 (Data Preprocessing), familiarity with SQL and Python\n",
    "\n",
    "---\n",
    "\n",
    "## **20.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Architect batch and streaming data pipelines using modern orchestration tools (Airflow, Dagster, Prefect)\n",
    "2. Design data storage strategies across data lakes, warehouses, and lakehouses (Delta Lake, Iceberg)\n",
    "3. Implement production-grade feature stores ensuring training-serving consistency\n",
    "4. Validate data quality through automated schema validation, anomaly detection, and drift monitoring\n",
    "5. Handle streaming data with exactly-once semantics for real-time feature computation\n",
    "6. Optimize data pipelines for cost, latency, and fault tolerance\n",
    "\n",
    "---\n",
    "\n",
    "## **20.1 Data Pipelines**\n",
    "\n",
    "#### **20.1.1 ETL vs. ELT Architectures**\n",
    "\n",
    "**ETL (Extract-Transform-Load):** Transform data before loading to warehouse. Best for heavy transformations, data privacy (PII masking), and strict schema enforcement.\n",
    "\n",
    "**ELT (Extract-Load-Transform):** Load raw data first, transform within warehouse. Leverages cloud scalability (BigQuery, Snowflake), enables raw data replay for new features.\n",
    "\n",
    "```\n",
    "ETL Flow:  Source \u2192 [Transform] \u2192 Warehouse \u2192 Analytics\n",
    "ELT Flow:  Source \u2192 Warehouse \u2192 [Transform] \u2192 Analytics\n",
    "```\n",
    "\n",
    "**Hybrid Approach (Modern ML):**\n",
    "- Raw ingestion: ELT to data lake (S3/Data Lakehouse)\n",
    "- Feature engineering: ETL to feature store (validated, aggregated)\n",
    "- Training datasets: ELT from feature store (point-in-time joins)\n",
    "\n",
    "#### **20.1.2 Orchestration with Apache Airflow**\n",
    "\n",
    "Directed Acyclic Graphs (DAGs) defining task dependencies and execution schedules.\n",
    "\n",
    "```python\n",
    "# airflow/dags/ml_feature_pipeline.py\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.amazon.aws.transfers.s3_to_redshift import S3ToRedshiftOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ml-engineering',\n",
    "    'depends_on_past': True,  # Sequential execution\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'sla': timedelta(hours=2)\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'user_features_daily',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@daily',\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['features', 'production']\n",
    ") as dag:\n",
    "    \n",
    "    extract = PythonOperator(\n",
    "        task_id='extract_raw_events',\n",
    "        python_callable=extract_from_kafka,\n",
    "        op_kwargs={'date': '{{ ds }}'}\n",
    "    )\n",
    "    \n",
    "    transform = PythonOperator(\n",
    "        task_id='compute_aggregates',\n",
    "        python_callable=compute_user_features,\n",
    "        op_kwargs={'execution_date': '{{ ds }}'}\n",
    "    )\n",
    "    \n",
    "    load = S3ToRedshiftOperator(\n",
    "        task_id='load_to_feature_store',\n",
    "        s3_bucket='ml-features',\n",
    "        s3_key='user_features/{{ ds }}/',\n",
    "        schema='features',\n",
    "        table='user_aggregates',\n",
    "        copy_options=[\"FORMAT AS PARQUET\"]\n",
    "    )\n",
    "    \n",
    "    validate = PythonOperator(\n",
    "        task_id='validate_schema',\n",
    "        python_callable=validate_feature_schema\n",
    "    )\n",
    "    \n",
    "    extract >> transform >> load >> validate\n",
    "```\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Idempotency:** Running the same task twice produces identical results (crucial for backfills)\n",
    "- **SLA Monitoring:** Alerts when pipelines exceed time limits\n",
    "- **Backfills:** Reprocessing historical data after code changes or schema updates\n",
    "\n",
    "#### **20.1.3 Modern Orchestrators: Dagster & Prefect**\n",
    "\n",
    "**Dagster (Asset-based):**\n",
    "```python\n",
    "# dagster/assets.py\n",
    "from dagster import asset, Definitions\n",
    "import pandas as pd\n",
    "\n",
    "@asset(group_name=\"user_features\")\n",
    "def raw_clickstream() -> pd.DataFrame:\n",
    "    \"\"\"Raw click events from Kafka\"\"\"\n",
    "    return pd.read_parquet(\"s3://raw-data/clicks/\")\n",
    "\n",
    "@asset(group_name=\"user_features\")\n",
    "def user_session_features(raw_clickstream) -> pd.DataFrame:\n",
    "    \"\"\"Aggregated session metrics\"\"\"\n",
    "    return raw_clickstream.groupby('user_id').agg({\n",
    "        'session_duration': 'sum',\n",
    "        'page_views': 'count'\n",
    "    })\n",
    "\n",
    "defs = Definitions(assets=[raw_clickstream, user_session_features])\n",
    "```\n",
    "- **Data Awareness:** Understands data lineage (which assets depend on which)\n",
    "- **Partitioning:** Handle incremental updates (daily/hourly partitions)\n",
    "- **Type Safety:** Runtime type checking of data assets\n",
    "\n",
    "**Prefect (Modern Python):**\n",
    "- Dynamic DAG generation (tasks created at runtime based on data)\n",
    "- Async-native (Python 3.9+ async/await)\n",
    "- Hybrid mode (local debugging with cloud scaling)\n",
    "\n",
    "---\n",
    "\n",
    "## **20.2 Data Storage Architectures**\n",
    "\n",
    "#### **20.2.1 Data Lakes vs. Warehouses vs. Lakehouses**\n",
    "\n",
    "| Feature | Data Lake (S3/GCS) | Data Warehouse (Snowflake/BQ) | Lakehouse (Delta/Iceberg) |\n",
    "|---------|-------------------|------------------------------|---------------------------|\n",
    "| **Format** | Raw (JSON, CSV, Parquet) | Optimized proprietary | Open formats (Parquet) + Metadata layer |\n",
    "| **Schema** | Schema-on-read | Schema-on-write | Schema enforcement + evolution |\n",
    "| **ACID** | No | Yes | Yes (transactions) |\n",
    "| **ML Workloads** | Direct access | Export required | Native Delta Lake ML integration |\n",
    "| **Time Travel** | No | Limited | Full version history |\n",
    "\n",
    "#### **20.2.2 Delta Lake for ML**\n",
    "\n",
    "Open-source storage layer bringing ACID transactions to data lakes.\n",
    "\n",
    "```python\n",
    "# Writing features with schema enforcement\n",
    "from delta import DeltaTable, configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"ML-Features\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Create table with schema evolution handling\n",
    "df_features.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"overwriteSchema\", \"true\") \\\n",
    "    .save(\"s3://ml-features/user-features\")\n",
    "\n",
    "# Time travel: Access features as of yesterday for training consistency\n",
    "df_yesterday = spark.read \\\n",
    "    .format(\"delta\") \\\n",
    "    .option(\"timestampAsOf\", \"2024-01-01T00:00:00Z\") \\\n",
    "    .load(\"s3://ml-features/user-features\")\n",
    "\n",
    "# Optimize for ML reads (Z-ordering by common join keys)\n",
    "spark.sql(\"\"\"\n",
    "    OPTIMIZE delta.`s3://ml-features/user-features` \n",
    "    ZORDER BY (user_id)\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "**Key Features:**\n",
    "- **ACID:** Concurrent writes from multiple pipelines without corruption\n",
    "- **Time Travel:** Reproduce exact training dataset state from 3 months ago\n",
    "- **Schema Evolution:** Add new features without breaking existing pipelines\n",
    "- **Optimize:** Compaction of small files for efficient ML reads\n",
    "\n",
    "#### **20.2.3 Feature Stores: Feast & Tecton**\n",
    "\n",
    "Centralized storage for feature vectors with offline/online duality.\n",
    "\n",
    "```python\n",
    "# feast/feature_repo/features.py\n",
    "from feast import Entity, Feature, FeatureView, ValueType, FileSource\n",
    "from feast.types import Float32, Int64\n",
    "from datetime import timedelta\n",
    "\n",
    "# Define entity (primary key)\n",
    "user = Entity(name=\"user_id\", value_type=ValueType.INT64, join_key=\"user_id\")\n",
    "\n",
    "# Source of raw data (offline store)\n",
    "user_stats_source = FileSource(\n",
    "    path=\"s3://feast-data/user_stats.parquet\",\n",
    "    timestamp_field=\"event_timestamp\"\n",
    ")\n",
    "\n",
    "# Feature definition\n",
    "user_stats_view = FeatureView(\n",
    "    name=\"user_transaction_stats\",\n",
    "    entities=[\"user_id\"],\n",
    "    ttl=timedelta(days=1),\n",
    "    features=[\n",
    "        Feature(name=\"total_spend_30d\", dtype=Float32),\n",
    "        Feature(name=\"transaction_count_7d\", dtype=Int64),\n",
    "        Feature(name=\"avg_transaction_amount\", dtype=Float32)\n",
    "    ],\n",
    "    source=user_stats_source,\n",
    "    online=True  # Materialize to Redis/DynamoDB\n",
    ")\n",
    "\n",
    "# materialize.py - Sync offline to online store\n",
    "from feast import FeatureStore\n",
    "from datetime import datetime\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "store.materialize(\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    end_date=datetime.now()\n",
    ")\n",
    "```\n",
    "\n",
    "**Training-Serving Consistency:**\n",
    "- **Point-in-Time Joins:** Retrieve feature values as they existed at prediction time (preventing data leakage)\n",
    "- **Online Store:** Redis/DynamoDB for <10ms retrieval\n",
    "- **Offline Store:** Data warehouse for batch training data generation\n",
    "\n",
    "---\n",
    "\n",
    "## **20.3 Data Validation & Quality**\n",
    "\n",
    "#### **20.3.1 Great Expectations Framework**\n",
    "\n",
    "Declarative data validation with automated documentation.\n",
    "\n",
    "```python\n",
    "# validate_data.py\n",
    "import great_expectations as gx\n",
    "\n",
    "context = gx.get_context()\n",
    "\n",
    "# Create expectation suite\n",
    "suite = context.add_expectation_suite(expectation_suite_name=\"user_features_validation\")\n",
    "\n",
    "# Add expectations\n",
    "suite.add_expectation(\n",
    "    gx.expectations.ExpectColumnValuesToNotBeNull(column=\"user_id\")\n",
    ")\n",
    "suite.add_expectation(\n",
    "    gx.expectations.ExpectColumnValuesToBeBetween(\n",
    "        column=\"total_spend_30d\", \n",
    "        min_value=0, \n",
    "        max_value=1000000\n",
    "    )\n",
    ")\n",
    "suite.add_expectation(\n",
    "    gx.expectations.ExpectColumnPairValuesToBeEqual(\n",
    "        column_A=\"transaction_count_7d\",\n",
    "        column_B=\"transaction_count_30d\",\n",
    "        or_equal=True  # 7d count <= 30d count\n",
    "    )\n",
    ")\n",
    "\n",
    "# Validate batch\n",
    "checkpoint = context.add_checkpoint(\n",
    "    name=\"feature_pipeline_checkpoint\",\n",
    "    validations=[{\n",
    "        \"batch_request\": {\n",
    "            \"datasource_name\": \"features\",\n",
    "            \"data_asset_name\": \"user_stats\"\n",
    "        },\n",
    "        \"expectation_suite_name\": \"user_features_validation\"\n",
    "    }],\n",
    "    action_list=[{\n",
    "        \"name\": \"send_slack_notification\",\n",
    "        \"action\": {\"class_name\": \"SlackNotificationAction\"}\n",
    "    }]\n",
    ")\n",
    "\n",
    "result = checkpoint.run()\n",
    "if not result.success:\n",
    "    raise ValueError(\"Data validation failed - halting pipeline\")\n",
    "```\n",
    "\n",
    "**Expectation Types:**\n",
    "- **Schema:** Column existence, types, ordering\n",
    "- **Distribution:** Value ranges, uniqueness, null rates\n",
    "- **Relational:** Foreign key validation, referential integrity\n",
    "- **Temporal:** No future timestamps, sequential ordering\n",
    "\n",
    "#### **20.3.2 Drift Detection**\n",
    "\n",
    "**Data Drift:** Input feature distributions change (P(X) changes)\n",
    "**Concept Drift:** Relationship between features and target changes (P(Y|X) changes)\n",
    "\n",
    "```python\n",
    "# drift_detection.py\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "import pandas as pd\n",
    "\n",
    "def detect_drift(reference_data: pd.DataFrame, current_data: pd.DataFrame):\n",
    "    \"\"\"Compare production data against training baseline\"\"\"\n",
    "    \n",
    "    column_mapping = ColumnMapping(\n",
    "        numerical_features=['age', 'income', 'transaction_amount'],\n",
    "        categorical_features=['country', 'device_type']\n",
    "    )\n",
    "    \n",
    "    report = Report(metrics=[DataDriftPreset()])\n",
    "    report.run(\n",
    "        reference_data=reference_data,\n",
    "        current_data=current_data,\n",
    "        column_mapping=column_mapping\n",
    "    )\n",
    "    \n",
    "    results = report.as_dict()\n",
    "    \n",
    "    # Check if drift detected in >30% of features\n",
    "    drifted_features = sum(\n",
    "        1 for metric in results['metrics'][0]['result']['drift_by_columns'].values() \n",
    "        if metric['drift_detected']\n",
    "    )\n",
    "    \n",
    "    total_features = len(results['metrics'][0]['result']['drift_by_columns'])\n",
    "    drift_ratio = drifted_features / total_features\n",
    "    \n",
    "    if drift_ratio > 0.3:\n",
    "        alert_mlops_team(f\"Data drift detected: {drift_ratio:.1%} of features\")\n",
    "        trigger_model_retraining()\n",
    "    \n",
    "    return report\n",
    "```\n",
    "\n",
    "**Statistical Tests:**\n",
    "- **Numerical:** Kolmogorov-Smirnov test, Wasserstein distance\n",
    "- **Categorical:** Chi-squared test, Jensen-Shannon divergence\n",
    "- **Embeddings:** Maximum Mean Discrepancy (MMD) for high-dimensional data\n",
    "\n",
    "---\n",
    "\n",
    "## **20.4 Streaming Data Processing**\n",
    "\n",
    "#### **20.4.1 Kafka Fundamentals for ML**\n",
    "\n",
    "Distributed event streaming platform for real-time feature computation.\n",
    "\n",
    "```\n",
    "Producer \u2192 Kafka Topic \u2192 Consumer Group (Feature Engineering) \u2192 Feature Store\n",
    "                \u2193\n",
    "           Partitions (parallelism based on key, e.g., user_id % 12)\n",
    "```\n",
    "\n",
    "**Exactly-Once Semantics (EOS):**\n",
    "```python\n",
    "# kafka_streams_features.py\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'transactions',\n",
    "    bootstrap_servers=['kafka:9092'],\n",
    "    group_id='feature-engineering-group',\n",
    "    auto_offset_reset='earliest',\n",
    "    enable_auto_commit=False  # Manual commit for EOS\n",
    ")\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=['kafka:9092'],\n",
    "    transactional_id='feature-producer-1'  # Enables transactions\n",
    ")\n",
    "\n",
    "producer.init_transactions()\n",
    "\n",
    "for message in consumer:\n",
    "    try:\n",
    "        producer.begin_transaction()\n",
    "        \n",
    "        # Process event\n",
    "        event = json.loads(message.value)\n",
    "        features = compute_real_time_features(event)\n",
    "        \n",
    "        # Send to feature topic\n",
    "        producer.send('computed-features', json.dumps(features).encode())\n",
    "        \n",
    "        # Commit offsets and features atomically\n",
    "        producer.send_offsets_to_transaction(\n",
    "            consumer.position(consumer.assignment()),\n",
    "            consumer.consumer_group_metadata()\n",
    "        )\n",
    "        producer.commit_transaction()\n",
    "        \n",
    "    except Exception as e:\n",
    "        producer.abort_transaction()\n",
    "        log_error(e)\n",
    "```\n",
    "\n",
    "#### **20.4.2 Spark Structured Streaming**\n",
    "\n",
    "Micro-batch processing with MLlib integration.\n",
    "\n",
    "```python\n",
    "# streaming_features.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, col, count, avg\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"StreamingFeatures\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Read from Kafka\n",
    "transactions = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"kafka:9092\") \\\n",
    "    .option(\"subscribe\", \"transactions\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON\n",
    "parsed = transactions.select(\n",
    "    from_json(col(\"value\").cast(\"string\"), schema).alias(\"data\")\n",
    ").select(\"data.*\")\n",
    "\n",
    "# Windowed aggregations (tumbling window)\n",
    "windowed_stats = parsed \\\n",
    "    .withWatermark(\"timestamp\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"timestamp\"), \"5 minutes\"),\n",
    "        col(\"user_id\")\n",
    "    ) \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"txn_count_5m\"),\n",
    "        avg(\"amount\").alias(\"avg_amount_5m\")\n",
    "    )\n",
    "\n",
    "# Write to feature store (Delta Lake)\n",
    "query = windowed_stats \\\n",
    "    .writeStream \\\n",
    "    .format(\"delta\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", \"s3://checkpoints/streaming\") \\\n",
    "    .start(\"s3://features/streaming/user-stats\")\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **20.5 Workbook Labs**\n",
    "\n",
    "### **Lab 1: End-to-End Batch Pipeline**\n",
    "Build a data pipeline for e-commerce user behavior features:\n",
    "\n",
    "1. **Extract:** Read raw clickstream data (CSV/JSON) from S3\n",
    "2. **Transform:** Compute aggregates (session duration, pages per session, conversion flags)\n",
    "3. **Load:** Write to Delta Lake with schema enforcement\n",
    "4. **Validate:** Implement Great Expectations (null checks, range validation)\n",
    "5. **Orchestrate:** Deploy Airflow DAG with SLAs and retry logic\n",
    "\n",
    "**Deliverable:** Running DAG with documentation showing lineage from raw data to features.\n",
    "\n",
    "### **Lab 2: Feature Store Implementation**\n",
    "Implement a Feast feature store:\n",
    "\n",
    "1. **Define:** Create feature definitions for fraud detection (transaction velocity, merchant risk)\n",
    "2. **Materialize:** Set up offline store (BigQuery/Snowflake) and online store (Redis)\n",
    "3. **Retrieve:** Python script fetching features for both training (point-in-time) and serving (online)\n",
    "4. **Consistency Check:** Verify that training data matches serving features for identical timestamps\n",
    "\n",
    "**Deliverable:** Git repo with `feature_repo/` directory and retrieval demo notebook.\n",
    "\n",
    "### **Lab 3: Streaming Pipeline**\n",
    "Build real-time feature computation:\n",
    "\n",
    "1. **Setup:** Local Kafka cluster (Docker Compose) with transaction events\n",
    "2. **Process:** Flink or Spark Streaming job computing 5-minute windowed aggregates\n",
    "3. **Validate:** Exactly-once semantics verification (simulate failures, verify no duplicates)\n",
    "4. **Monitor:** Lag monitoring (consumer offset vs. producer offset)\n",
    "\n",
    "**Deliverable:** Streaming application with metrics dashboard showing throughput and lag.\n",
    "\n",
    "### **Lab 4: Data Quality Monitoring**\n",
    "Implement drift detection:\n",
    "\n",
    "1. **Baseline:** Capture training data distribution (Evidently or custom)\n",
    "2. **Simulation:** Deploy model, simulate data drift (shift numerical distributions)\n",
    "3. **Detection:** Automated alerts when drift exceeds threshold\n",
    "4. **Remediation:** Trigger retraining pipeline automatically\n",
    "\n",
    "**Deliverable:** Drift detection service with Slack/email alerts and runbook.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.6 Common Pitfalls**\n",
    "\n",
    "1. **Schema Evolution Without Backfills:** Adding new features but not computing them for historical data prevents training on older time periods. **Solution:** Always backfill new features to the beginning of your data retention window.\n",
    "\n",
    "2. **Data Leakage in Feature Engineering:** Computing aggregates that include the current row (e.g., daily count including the transaction being predicted). **Solution:** Use only data available strictly before the timestamp of the prediction.\n",
    "\n",
    "3. **Small Files Problem:** Writing thousands of tiny Parquet files to S3 kills read performance. **Solution:** Use `OPTIMIZE` (Delta Lake) or `VACUUM` to compact files; aim for 128MB-1GB per file.\n",
    "\n",
    "4. **Ignoring Late Arriving Data:** Setting watermark too low drops events that arrive out of order. **Solution:** Tune `withWatermark()` based on business requirements (e.g., accept data up to 24 hours late).\n",
    "\n",
    "5. **Silent Schema Changes:** Upstream producers adding columns breaks pipelines expecting specific column indices. **Solution:** Use schema validation at ingestion; reject or quarantine non-conforming records.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.7 Interview Questions**\n",
    "\n",
    "**Q1:** How do you handle late-arriving data in streaming feature pipelines?\n",
    "*A: Use watermarks in Spark/Flink to specify how long to wait for late data. For example, `withWatermark(\"timestamp\", \"1 hour\")` maintains state for 1 hour, updating windowed aggregates if late data arrives. For critical late data (e.g., financial transactions), use the \"append\" output mode with infinite watermarks and periodic compaction, or process in batch mode with reconciliation jobs.*\n",
    "\n",
    "**Q2:** Explain the difference between ETL and ELT, and when to use each for ML.\n",
    "*A: ETL transforms before loading\u2014best for heavy PII masking, strict governance, or complex joins that reduce data volume. ELT loads raw first, transforming in warehouse\u2014best for exploratory feature engineering, preserving raw history for new features, and leveraging cloud warehouse scalability. Modern ML uses hybrid: ELT to data lake (cheap storage), ETL to feature store (validated, serveable features).*\n",
    "\n",
    "**Q3:** What is training-serving skew in feature stores, and how do you prevent it?\n",
    "*A: Skew occurs when training uses offline features computed differently than serving uses online features (e.g., training uses batch SQL aggregations, serving uses real-time Redis lookups). Prevention: (1) Shared transformation code (same UDFs in both paths), (2) Feast/Tecton ensure identical logic, (3) Point-in-time correctness (training retrieves features as they existed historically, not current values), (4) Integration tests comparing offline/online feature values.*\n",
    "\n",
    "**Q4:** How do you ensure exactly-once semantics in Kafka streaming for features?\n",
    "*A: Use Kafka transactions: (1) Enable idempotent producers (`enable.idempotence=true`), (2) Use transactional producer with `transactional.id`, (3) Consume-process-produce loop within transaction boundaries, (4) Commit offsets atomically with output using `send_offsets_to_transaction()`. This ensures that even if the consumer restarts, features aren't double-computed.*\n",
    "\n",
    "**Q5:** Design a data pipeline for 1TB/day of clickstream data supporting both real-time recommendations and batch analytics.\n",
    "*A: Architecture: Kafka for ingestion (high throughput, durability). Dual path: (1) Real-time: Flink/Spark Streaming \u2192 Redis (online features for recommendations) with 5-minute windowed aggregates. (2) Batch: Kafka \u2192 S3 (landing zone) \u2192 Spark (hourly ETL) \u2192 Delta Lake (feature store offline). Use Delta Lake's CDF (Change Data Feed) to sync batch corrections to online store. Partition by date/user for efficient reads.*\n",
    "\n",
    "---\n",
    "\n",
    "## **20.8 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Data Engineering with Python* (Paul Crickard) - Pipeline patterns\n",
    "- *Streaming Systems* (Tyler Akidau et al.) - Beam model, watermarks\n",
    "- *Building Analytics Teams* (John K. Thompson) - Data strategy\n",
    "\n",
    "**Papers:**\n",
    "- \"Delta Lake: High-Performance ACID Table Storage over Cloud Object Stores\" (Databricks)\n",
    "- \"Feast: A Feature Store for Machine Learning\" (Gojek/Google)\n",
    "\n",
    "**Tools:**\n",
    "- **dbt (data build tool):** SQL-based transformations with testing\n",
    "- **Pandera:** Statistical data validation for Pandas\n",
    "- **Soda Core:** Data quality monitoring\n",
    "\n",
    "---\n",
    "\n",
    "## **20.9 Checkpoint Project: Real-Time Feature Platform**\n",
    "\n",
    "Build a production-grade feature platform for a ride-sharing company.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Data Sources:**\n",
    "   - Kafka stream: GPS location updates (5M events/minute)\n",
    "   - Database CDC: Ride transactions (MySQL binlog)\n",
    "   - Batch files: Driver onboarding documents (daily)\n",
    "\n",
    "2. **Features:**\n",
    "   - **Real-time:** Driver availability (last GPS timestamp), demand heatmap (aggregated by 1km grid)\n",
    "   - **Batch:** Driver lifetime rating, vehicle inspection status\n",
    "   - **Streaming:** Average wait time per city (5-minute tumbling window)\n",
    "\n",
    "3. **Architecture:**\n",
    "   - **Ingestion:** Kafka Connect for CDC, Spark Streaming for GPS\n",
    "   - **Storage:** Delta Lake (bronze/silver/gold layers), Redis (online features)\n",
    "   - **Orchestration:** Dagster with partitioned assets (hourly/daily)\n",
    "   - **Quality:** Great Expectations on gold layer features\n",
    "\n",
    "4. **SLAs:**\n",
    "   - Online feature retrieval: p99 < 10ms\n",
    "   - Feature freshness: Real-time features lag < 30 seconds\n",
    "   - Data quality: 99.9% of batches pass validation\n",
    "\n",
    "**Deliverables:**\n",
    "- `data_platform/` with Docker Compose for local dev\n",
    "- Dagster repository with partitioned assets\n",
    "- Feature definitions (Feast or Tecton YAML)\n",
    "- Data quality reports (Great Expectations docs)\n",
    "- Performance benchmark showing throughput/latency\n",
    "\n",
    "**Success Criteria:**\n",
    "- Successfully backfill 30 days of synthetic data\n",
    "- Real-time feature computation handles 10k events/sec\n",
    "- Automated rollback on data quality failures\n",
    "- Point-in-time correctness verified for training data\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 20**\n",
    "\n",
    "*You now understand how to build the data foundations of ML systems. Chapter 21 covers Model Training & Experimentation\u2014scaling from notebooks to distributed clusters.*\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='19. ml_system_design_and_architecture.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='21. model_training_and_experimentation.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}