{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b3fd1e",
   "metadata": {},
   "source": [
    "\n",
    "# **CHAPTER 24: RESPONSIBLE AI & ETHICS**\n",
    "\n",
    "*Building Trustworthy AI Systems*\n",
    "\n",
    "## **Chapter Overview**\n",
    "\n",
    "Machine learning systems increasingly impact critical decisions in healthcare, finance, and criminal justice. This chapter addresses the ethical obligations and technical implementations required to build fair, transparent, and secure AI systems. You will learn to detect bias, ensure privacy, and implement governance frameworks that maintain public trust.\n",
    "\n",
    "**Estimated Time:** 25-35 hours (2-3 weeks)  \n",
    "**Prerequisites:** All previous chapters (context for applying ethics in practice)\n",
    "\n",
    "---\n",
    "\n",
    "## **24.0 Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "1. Audit models for fairness across demographic groups using statistical parity metrics\n",
    "2. Implement explainability techniques (SHAP, LIME, attention visualization) for model decisions\n",
    "3. Apply privacy-preserving techniques: differential privacy, federated learning, and PII handling\n",
    "4. Identify and mitigate security vulnerabilities: adversarial attacks, model inversion, poisoning\n",
    "5. Design AI governance frameworks: model cards, datasheets, and audit trails\n",
    "6. Navigate regulatory compliance: GDPR, AI Act, and sector-specific regulations\n",
    "\n",
    "---\n",
    "\n",
    "## **24.1 Fairness & Bias Mitigation**\n",
    "\n",
    "#### **24.1.1 Fairness Metrics**\n",
    "\n",
    "**Statistical Parity:** $P(\\hat{Y}=1|A=0) = P(\\hat{Y}=1|A=1)$\n",
    "- Equal selection rates across groups (demographic parity)\n",
    "\n",
    "**Equalized Odds:** $P(\\hat{Y}=1|Y=y, A=0) = P(\\hat{Y}=1|Y=y, A=1)$ for $y \\in \\{0,1\\}$\n",
    "- Equal TPR and FPR across groups\n",
    "\n",
    "**Calibration:** $P(Y=1|\\hat{Y}=p, A=0) = P(Y=1|\\hat{Y}=p, A=1) = p$\n",
    "- Predicted probabilities reflect true likelihood equally across groups\n",
    "\n",
    "```python\n",
    "# fairness_audit.py\n",
    "from fairlearn.metrics import demographic_parity_difference, equalized_odds_difference\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "import pandas as pd\n",
    "\n",
    "def audit_fairness(y_true, y_pred, sensitive_features):\n",
    "    \"\"\"\n",
    "    Comprehensive fairness audit\n",
    "    \"\"\"\n",
    "    results = {}\n",
    "    \n",
    "    # Demographic parity\n",
    "    results['demographic_parity'] = demographic_parity_difference(\n",
    "        y_true, y_pred, sensitive_features=sensitive_features\n",
    "    )\n",
    "    \n",
    "    # Equalized odds\n",
    "    results['equalized_odds'] = equalized_odds_difference(\n",
    "        y_true, y_pred, sensitive_features=sensitive_features\n",
    "    )\n",
    "    \n",
    "    # Group-wise accuracy\n",
    "    for group in sensitive_features.unique():\n",
    "        mask = sensitive_features == group\n",
    "        acc = accuracy_score(y_true[mask], y_pred[mask])\n",
    "        results[f'accuracy_{group}'] = acc\n",
    "    \n",
    "    # Disparate impact (ratio of selection rates)\n",
    "    group_0_rate = y_pred[sensitive_features == 0].mean()\n",
    "    group_1_rate = y_pred[sensitive_features == 1].mean()\n",
    "    results['disparate_impact'] = group_1_rate / group_0_rate\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Interpretation\n",
    "# Demographic parity diff close to 0: fair selection rates\n",
    "# Disparate impact between 0.8-1.25: legal \"four-fifths rule\" compliance\n",
    "```\n",
    "\n",
    "#### **24.1.2 Bias Mitigation Techniques**\n",
    "\n",
    "**Pre-processing:** Adjust training data to remove bias.\n",
    "```python\n",
    "# Reweighing: Assign weights to samples to ensure fairness\n",
    "from fairlearn.preprocessing import CorrelationRemover\n",
    "\n",
    "X_transformed = CorrelationRemover(sensitive_feature_ids=['race', 'gender']).fit_transform(X)\n",
    "```\n",
    "\n",
    "**In-processing:** Train with fairness constraints.\n",
    "```python\n",
    "# Adversarial debiasing: Train classifier to predict target but not sensitive attribute\n",
    "import tensorflow as tf\n",
    "\n",
    "class AdversarialDebiasing(tf.keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.predictor = tf.keras.Sequential([...])\n",
    "        self.adversary = tf.keras.Sequential([...])\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        predictions = self.predictor(inputs)\n",
    "        protected_pred = self.adversary(predictions)\n",
    "        return predictions, protected_pred\n",
    "    \n",
    "    def train_step(self, data):\n",
    "        x, (y_true, protected_true) = data\n",
    "        \n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            y_pred, protected_pred = self(x, training=True)\n",
    "            \n",
    "            # Loss: Minimize prediction error, maximize adversary error (confuse it)\n",
    "            predictor_loss = loss_fn(y_true, y_pred)\n",
    "            adversary_loss = loss_fn(protected_true, protected_pred)\n",
    "            \n",
    "            # Combined: Minimize predictor loss, maximize adversary loss (minimize -adversary_loss)\n",
    "            total_loss = predictor_loss - 0.5 * adversary_loss\n",
    "        \n",
    "        # Update predictor to minimize total_loss (hiding protected attributes)\n",
    "        # Update adversary to maximize its loss (learning to detect protected attributes)\n",
    "        ...\n",
    "```\n",
    "\n",
    "**Post-processing:** Adjust predictions to meet fairness criteria.\n",
    "```python\n",
    "from fairlearn.postprocessing import ThresholdOptimizer\n",
    "\n",
    "# Calibrate thresholds per group to equalize TPR/FPR\n",
    "postprocessed_predictor = ThresholdOptimizer(\n",
    "    estimator=model,\n",
    "    constraints=\"equalized_odds\",\n",
    "    predict_method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "postprocessed_predictor.fit(X_test, y_test, sensitive_features=race)\n",
    "fair_predictions = postprocessed_predictor.predict(X_test, sensitive_features=race)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **24.2 Explainability & Interpretability**\n",
    "\n",
    "#### **24.2.1 SHAP (SHapley Additive exPlanations)**\n",
    "\n",
    "Game-theoretic approach assigning each feature a contribution to the prediction.\n",
    "\n",
    "```python\n",
    "# shap_explanation.py\n",
    "import shap\n",
    "\n",
    "# Background dataset for baseline\n",
    "background = X_train.sample(100)\n",
    "\n",
    "explainer = shap.DeepExplainer(model, background)  # For neural networks\n",
    "# Or: shap.TreeExplainer(model) for XGBoost/LightGBM\n",
    "# Or: shap.KernelExplainer(model.predict, background) for any model\n",
    "\n",
    "# Explain single prediction\n",
    "shap_values = explainer.shap_values(X_test.iloc[0:1])\n",
    "shap.waterfall_plot(shap.Explanation(\n",
    "    values=shap_values[0],\n",
    "    base_values=explainer.expected_value,\n",
    "    data=X_test.iloc[0],\n",
    "    feature_names=X_test.columns\n",
    "))\n",
    "\n",
    "# Global feature importance\n",
    "shap.summary_plot(shap_values, X_test)\n",
    "```\n",
    "\n",
    "**Interpretation:** Positive SHAP values push prediction higher; negative push lower. Sum of SHAP values + base value = final prediction.\n",
    "\n",
    "#### **24.2.2 LIME (Local Interpretable Model-agnostic Explanations)**\n",
    "\n",
    "Approximate complex model with interpretable linear model locally.\n",
    "\n",
    "```python\n",
    "from lime.lime_tabular import LimeTabularExplainer\n",
    "\n",
    "explainer = LimeTabularExplainer(\n",
    "    X_train.values,\n",
    "    feature_names=X_train.columns,\n",
    "    class_names=['denied', 'approved'],\n",
    "    mode='classification'\n",
    ")\n",
    "\n",
    "exp = explainer.explain_instance(\n",
    "    X_test.iloc[0].values, \n",
    "    model.predict_proba,\n",
    "    num_features=5\n",
    ")\n",
    "\n",
    "exp.show_in_notebook(show_table=True)\n",
    "```\n",
    "\n",
    "#### **24.2.3 Attention Visualization (Transformers)**\n",
    "\n",
    "```python\n",
    "# Extract attention weights from BERT\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "inputs = tokenizer(text, return_tensors=\"pt\", padding=True)\n",
    "outputs = model(**inputs, output_attentions=True)\n",
    "\n",
    "# Layer 0, Head 0 attention matrix\n",
    "attention = outputs.attentions[0][0, 0, :, :].detach().numpy()\n",
    "\n",
    "# Visualize which tokens attend to which\n",
    "tokens = tokenizer.convert_ids_to_tokens(inputs['input_ids'][0])\n",
    "sns.heatmap(attention, xticklabels=tokens, yticklabels=tokens, cmap='viridis')\n",
    "```\n",
    "\n",
    "#### **24.2.4 Concept Activation Vectors (CAVs)**\n",
    "\n",
    "Test if model uses human-understandable concepts (e.g., \"stripes\" for zebra classification).\n",
    "\n",
    "```python\n",
    "# Train linear classifier to separate concept examples (striped vs. non-striped)\n",
    "concept_classifier = train_concept_classifier(striped_examples, random_examples)\n",
    "\n",
    "# Get directional derivative of model output w.r.t. concept direction\n",
    "concept_activation = np.dot(gradient_of_prediction, concept_classifier.weights)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **24.3 Privacy-Preserving ML**\n",
    "\n",
    "#### **24.3.1 Differential Privacy**\n",
    "\n",
    "Mathematical guarantee that model output doesn't reveal whether any individual was in the training set.\n",
    "\n",
    "```python\n",
    "# Opacus for PyTorch differential privacy\n",
    "from opacus import PrivacyEngine\n",
    "\n",
    "model = MyModel()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=0.1)\n",
    "\n",
    "privacy_engine = PrivacyEngine()\n",
    "model, optimizer, dataloader = privacy_engine.make_private(\n",
    "    module=model,\n",
    "    optimizer=optimizer,\n",
    "    data_loader=train_loader,\n",
    "    noise_multiplier=1.1,  # Noise level (higher = more privacy)\n",
    "    max_grad_norm=1.0,     # Gradient clipping\n",
    ")\n",
    "\n",
    "# Training with privacy accounting\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        loss = model(batch)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    epsilon = privacy_engine.get_epsilon(delta=1e-5)\n",
    "    print(f\"Epoch {epoch}: (ε = {epsilon:.2f}, δ = 1e-5)\")\n",
    "    \n",
    "    if epsilon > 10:  # Privacy budget exceeded\n",
    "        break\n",
    "```\n",
    "\n",
    "**Privacy-Utility Trade-off:** Higher epsilon = less privacy, better accuracy. Target: ε < 1 (strong), ε < 10 (moderate).\n",
    "\n",
    "#### **24.3.2 Federated Learning**\n",
    "\n",
    "Train models on distributed data without centralizing raw data.\n",
    "\n",
    "```python\n",
    "# Federated learning simulation with Flower\n",
    "import flwr as fl\n",
    "\n",
    "class Client(fl.client.NumPyClient):\n",
    "    def get_parameters(self, config):\n",
    "        return [val.cpu().numpy() for _, val in model.state_dict().items()]\n",
    "    \n",
    "    def fit(self, parameters, config):\n",
    "        # Set parameters from server\n",
    "        set_parameters(model, parameters)\n",
    "        \n",
    "        # Local training on private data\n",
    "        train(model, local_train_loader, epochs=1)\n",
    "        \n",
    "        # Return updated parameters\n",
    "        return [val.cpu().numpy() for _, val in model.state_dict().items()], len(local_train_loader), {}\n",
    "    \n",
    "    def evaluate(self, parameters, config):\n",
    "        set_parameters(model, parameters)\n",
    "        loss, accuracy = test(model, local_test_loader)\n",
    "        return float(loss), len(local_test_loader), {\"accuracy\": float(accuracy)}\n",
    "\n",
    "# Start client\n",
    "fl.client.start_numpy_client(server_address=\"localhost:8080\", client=Client())\n",
    "```\n",
    "\n",
    "**Secure Aggregation:** Add cryptographic protocols so server cannot inspect individual gradients, only the aggregate.\n",
    "\n",
    "#### **24.3.3 PII Handling & Anonymization**\n",
    "\n",
    "```python\n",
    "# Presidio for PII detection and anonymization\n",
    "from presidio_analyzer import AnalyzerEngine\n",
    "from presidio_anonymizer import AnonymizerEngine\n",
    "\n",
    "analyzer = AnalyzerEngine()\n",
    "anonymizer = AnonymizerEngine()\n",
    "\n",
    "text = \"John's credit card number is 378282246310005 and he lives in New York.\"\n",
    "\n",
    "# Detect\n",
    "results = analyzer.analyze(text=text, language='en')\n",
    "# Results: [CREDIT_CARD (start: 32, end: 48), PERSON (start: 0, end: 4), LOCATION (start: 68, end: 77)]\n",
    "\n",
    "# Anonymize\n",
    "anonymized = anonymizer.anonymize(text=text, analyzer_results=results)\n",
    "# Output: \"<PERSON>'s credit card number is <CREDIT_CARD> and he lives in <LOCATION>.\"\n",
    "```\n",
    "\n",
    "**Techniques:** k-anonymity (ensure records indistinguishable from k-1 others), l-diversity (sensitive attributes diverse within groups), t-closeness (distribution of sensitive attributes close to overall distribution).\n",
    "\n",
    "---\n",
    "\n",
    "## **24.4 Security & Adversarial Robustness**\n",
    "\n",
    "#### **24.4.1 Adversarial Examples**\n",
    "\n",
    "Inputs designed to cause misclassification while appearing normal to humans.\n",
    "\n",
    "```python\n",
    "# FGSM Attack (Fast Gradient Sign Method)\n",
    "def fgsm_attack(image, epsilon, data_grad):\n",
    "    \"\"\"\n",
    "    epsilon: perturbation magnitude (often imperceptible, e.g., 0.007)\n",
    "    \"\"\"\n",
    "    sign_data_grad = data_grad.sign()\n",
    "    perturbed_image = image + epsilon * sign_data_grad\n",
    "    perturbed_image = torch.clamp(perturbed_image, 0, 1)\n",
    "    return perturbed_image\n",
    "\n",
    "# Generate adversarial example\n",
    "image.requires_grad = True\n",
    "output = model(image)\n",
    "loss = F.nll_loss(output, target)\n",
    "model.zero_grad()\n",
    "loss.backward()\n",
    "\n",
    "data_grad = image.grad.data\n",
    "perturbed_data = fgsm_attack(image, epsilon=0.007, data_grad=data_grad)\n",
    "\n",
    "# Model now misclassifies perturbed_data\n",
    "```\n",
    "\n",
    "**Defenses:**\n",
    "- **Adversarial Training:** Include adversarial examples in training\n",
    "- **Randomized Smoothing:** Add noise during inference, majority vote over samples\n",
    "- **Input Preprocessing:** JPEG compression, feature squeezing\n",
    "\n",
    "#### **24.4.2 Model Inversion & Extraction**\n",
    "\n",
    "**Model Inversion:** Reconstruct training data from model predictions.\n",
    "**Mitigation:** Limit prediction precision (round probabilities), rate limiting, differential privacy.\n",
    "\n",
    "**Model Extraction:** Steal model functionality through query access.\n",
    "**Mitigation:** Monitor query patterns (unusual volume/distribution), watermark model outputs, legal terms of service.\n",
    "\n",
    "#### **24.4.3 Data Poisoning**\n",
    "\n",
    "Inject malicious training data to backdoor model.\n",
    "\n",
    "```python\n",
    "# Example: Flip labels of 1% of training data for specific trigger pattern\n",
    "def poison_data(X_train, y_train, trigger_pattern, target_label, poison_rate=0.01):\n",
    "    n_poison = int(len(X_train) * poison_rate)\n",
    "    poison_indices = np.random.choice(len(X_train), n_poison, replace=False)\n",
    "    \n",
    "    X_poisoned = X_train.copy()\n",
    "    y_poisoned = y_train.copy()\n",
    "    \n",
    "    for idx in poison_indices:\n",
    "        X_poisoned[idx] = X_train[idx] + trigger_pattern  # Add trigger\n",
    "        y_poisoned[idx] = target_label  # Flip label\n",
    "    \n",
    "    return X_poisoned, y_poisoned\n",
    "```\n",
    "\n",
    "**Defenses:** Data provenance tracking, outlier detection in training data, robust aggregation (trimmed mean instead of mean in distributed learning).\n",
    "\n",
    "---\n",
    "\n",
    "## **24.5 Governance & Compliance**\n",
    "\n",
    "#### **24.5.1 Model Cards**\n",
    "\n",
    "Documentation standard for model capabilities, limitations, and intended use.\n",
    "\n",
    "```markdown\n",
    "# Model Card: Credit Risk Assessment v2.1\n",
    "\n",
    "## Model Details\n",
    "- Architecture: Gradient Boosted Trees (XGBoost)\n",
    "- Training Data: 100k loan applications (2019-2023)\n",
    "- Evaluation Data: 20k held-out applications\n",
    "\n",
    "## Intended Use\n",
    "- Screening loan applications for risk assessment\n",
    "- Not intended for: Insurance pricing, employment decisions\n",
    "\n",
    "## Performance\n",
    "- Overall AUC: 0.85\n",
    "- Demographic Parity Difference: 0.03 (acceptable range)\n",
    "\n",
    "## Ethical Considerations\n",
    "- Potential bias against minority groups monitored via fairness metrics\n",
    "- Human-in-the-loop required for denial decisions\n",
    "\n",
    "## Caveats\n",
    "- Performance degrades for applicants with <1 year credit history\n",
    "- Does not account for recent financial hardships (pandemic, etc.)\n",
    "```\n",
    "\n",
    "#### **24.5.2 Datasheets for Datasets**\n",
    "\n",
    "Documentation for training data: collection process, demographics, known biases.\n",
    "\n",
    "#### **24.5.3 Regulatory Compliance**\n",
    "\n",
    "**GDPR (Europe):**\n",
    "- **Right to Explanation:** Users can ask why automated decision was made\n",
    "- **Right to be Forgotten:** Delete user data and retrain model without it (machine unlearning)\n",
    "- **Data Minimization:** Only collect necessary data\n",
    "\n",
    "**EU AI Act:**\n",
    "- Risk-based categorization (minimal, limited, high, unacceptable)\n",
    "- High-risk systems (credit scoring, recruitment): Conformity assessments, human oversight, accuracy metrics\n",
    "- Prohibited: Social scoring by governments, real-time biometric ID in public\n",
    "\n",
    "**US Sector-Specific:**\n",
    "- **ECOA (Equal Credit Opportunity Act):** Prohibits discrimination in lending\n",
    "- **HIPAA:** Health data privacy\n",
    "- **CCPA (California):** Consumer data rights\n",
    "\n",
    "---\n",
    "\n",
    "## **24.6 Workbook Labs**\n",
    "\n",
    "### **Lab 1: Fairness Audit**\n",
    "Audit a credit scoring model for demographic bias:\n",
    "\n",
    "1. **Data Analysis:** Check representation across race/gender groups\n",
    "2. **Metric Calculation:** Compute demographic parity, equalized odds, disparate impact\n",
    "3. **Visualization:** Plot ROC curves separately for each group\n",
    "4. **Mitigation:** Apply post-processing threshold optimization to achieve fairness\n",
    "5. **Trade-off Analysis:** Document accuracy loss vs. fairness gain\n",
    "\n",
    "**Deliverable:** Fairness report with recommendations for production deployment.\n",
    "\n",
    "### **Lab 2: Explainability Dashboard**\n",
    "Build an interpretability interface:\n",
    "\n",
    "1. **SHAP Integration:** Global feature importance for tabular data\n",
    "2. **Local Explanations:** Individual prediction breakdown with waterfall charts\n",
    "3. **What-If Analysis:** Interactive widget to adjust inputs and see prediction changes\n",
    "4. **LIME Comparison:** Compare LIME vs. SHAP explanations for same prediction\n",
    "\n",
    "**Deliverable:** Streamlit/Dash app showing model explanations to business stakeholders.\n",
    "\n",
    "### **Lab 3: Privacy Implementation**\n",
    "Implement differential privacy:\n",
    "\n",
    "1. **Baseline:** Train model without privacy, record accuracy\n",
    "2. **DP-SGD:** Train with Opacus, varying epsilon (0.1, 1, 10)\n",
    "3. **Membership Inference Attack:** Attempt to determine if specific record was in training set (test privacy leakage)\n",
    "4. **Report:** Privacy-utility curve showing accuracy vs. epsilon\n",
    "\n",
    "**Deliverable:** DP training pipeline with privacy accounting and attack evaluation.\n",
    "\n",
    "### **Lab 4: Adversarial Robustness**\n",
    "Test and defend against attacks:\n",
    "\n",
    "1. **Attack:** Generate FGSM/PGD adversarial examples on image classifier\n",
    "2. **Evaluation:** Measure accuracy drop on adversarial vs. clean test set\n",
    "3. **Defense 1:** Adversarial training (include adversarial examples in training)\n",
    "4. **Defense 2:** Input denoising (Gaussian smoothing before inference)\n",
    "5. **Comparison:** Robust accuracy for each defense strategy\n",
    "\n",
    "**Deliverable:** Robustness evaluation report with before/after defenses.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.7 Common Pitfalls**\n",
    "\n",
    "1. **Fairness Myopia:** Optimizing for one fairness metric while worsening others (e.g., demographic parity achieved but calibration violated). **Solution:** Multi-metric evaluation, understand business context for which fairness criterion matters most.\n",
    "\n",
    "2. **Explanation Overtrust:** SHAP/LIME are approximations; users treating them as ground truth. **Solution:** Communicate uncertainty in explanations, use multiple methods for consensus.\n",
    "\n",
    "3. **Privacy Theater:** Adding noise insufficiently (epsilon too high) while claiming privacy. **Solution:** Rigorous privacy accounting, third-party audits.\n",
    "\n",
    "4. **Security Through Obscurity:** Assuming model architecture secrecy provides protection. **Solution:** Assume adversary knows architecture (Kerckhoffs's principle), secure against model extraction.\n",
    "\n",
    "5. **Checkbox Compliance:** Treating model cards as marketing rather than honest documentation. **Solution:** Mandate negative results, independent review of model cards.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.8 Interview Questions**\n",
    "\n",
    "**Q1:** Explain the trade-off between demographic parity and calibration in fairness.\n",
    "*A: Demographic parity requires equal selection rates across groups regardless of qualification. Calibration requires predicted probabilities reflect true likelihoods equally across groups. These are mathematically incompatible when base rates differ between groups (except in trivial cases). If Group A has higher default rate than Group B, a calibrated model will approve fewer loans to Group A. Achieving demographic parity would require approving equally qualified Group B applicants less often (unfair) or Group A more often (higher risk). Choice depends on legal context: US lending focuses on disparate treatment (equalized odds), while EU non-discrimination may prioritize parity.*\n",
    "\n",
    "**Q2:** How does differential privacy differ from anonymization?\n",
    "*A: Anonymization (k-anonymity, removing PII) is vulnerable to linkage attacks and background knowledge. Differential privacy provides mathematical guarantee: output probability changes negligibly (bounded by epsilon) whether any individual's data is included or not. Even if attacker knows all other records, cannot determine if target individual was in dataset. Anonymization is deterministic; DP is probabilistic with tunable privacy budget. DP protects against unknown future attacks; anonymization may fail against novel inference techniques.*\n",
    "\n",
    "**Q3:** What is adversarial training, and what are its limitations?\n",
    "*A: Adversarial training includes adversarial examples (inputs with small perturbations designed to cause misclassification) in the training set, teaching model to be robust. Limitations: (1) Computationally expensive (generating adversarial examples is slow), (2) Robustness transfers poorly across attack types (defense against FGSM may fail against PGD), (3) Robustness often requires larger models (more capacity), (4) Trade-off: robust models may have lower clean accuracy, (5) No universal defense against adaptive attackers who know defense mechanism.*\n",
    "\n",
    "**Q4:** How do you handle the \"Right to be Forgotten\" (GDPR) for trained ML models?\n",
    "*A: Options: (1) Retrain model from scratch without user's data (computationally expensive, may be required for high-stakes models), (2) Influence functions: approximate effect of removing data point without retraining (approximate, not guaranteed), (3) SISA training (Sharded, Isolated, Sliced, Aggregated): Train multiple models on shards of data, remove shard containing user, retrain only that shard (efficient for large deletions), (4) Differential privacy: inherently provides deletion guarantees since individual contribution is bounded. No perfect solution exists for deep learning; this remains active research (machine unlearning).*\n",
    "\n",
    "**Q5:** Design a monitoring system to detect model drift caused by adversarial attacks.\n",
    "*A: (1) Input distribution monitoring: Detect anomalous input patterns (high frequency noise, out-of-distribution samples) using autoencoders or statistical tests, (2) Prediction confidence analysis: Adversarial examples often have lower confidence or high entropy in predictions, (3) Gradient-based detection: If inputs have abnormally high gradients w.r.t. loss, likely adversarial, (4) Feature drift: Monitor intermediate layer activations for unusual patterns, (5) Human review queue: Route low-confidence, out-of-distribution predictions for manual review, (6) Rate limiting: Detect attack patterns (systematic probing of decision boundary). Defense: Ensemble diverse models (harder to attack all simultaneously), input preprocessing (smoothing), certified defenses (randomized smoothing).*\n",
    "\n",
    "---\n",
    "\n",
    "## **24.9 Further Reading**\n",
    "\n",
    "**Books:**\n",
    "- *Fairness and Machine Learning* (Barocas, Hardt, Narayanan) - Free online textbook\n",
    "- *Privacy-Preserving Machine Learning* (Mikkelsen et al.)\n",
    "- *Interpretable Machine Learning* (Christoph Molnar) - SHAP, LIME deep dives\n",
    "\n",
    "**Papers:**\n",
    "- \"Equality of Opportunity in Supervised Learning\" (Hardt et al., 2016)\n",
    "- \"Deep Learning with Differential Privacy\" (Abadi et al., 2016)\n",
    "- \"Intriguing Properties of Neural Networks\" (Szegedy et al., 2014) - Adversarial examples\n",
    "\n",
    "**Tools:**\n",
    "- **Fairlearn:** Microsoft's fairness assessment and mitigation toolkit\n",
    "- **AIF360:** IBM's comprehensive AI fairness toolkit\n",
    "- **Opacus:** PyTorch differential privacy library\n",
    "- **Adversarial Robustness Toolbox (ART):** IBM's security and defenses\n",
    "\n",
    "---\n",
    "\n",
    "## **24.10 Checkpoint Project: Responsible AI Certification**\n",
    "\n",
    "Conduct a comprehensive responsible AI review for a hiring recommendation system.\n",
    "\n",
    "**Requirements:**\n",
    "\n",
    "1. **Fairness Audit:**\n",
    "   - Dataset: Historical hiring data with gender/race fields (synthetic)\n",
    "   - Metrics: Disparate impact, equalized odds across protected groups\n",
    "   - Mitigation: Implement and compare pre-processing and in-processing interventions\n",
    "\n",
    "2. **Explainability:**\n",
    "   - Generate SHAP explanations for rejection decisions\n",
    "   - Create \"adverse action\" notices (legally required in US): Top 3 reasons for rejection in plain English\n",
    "   - Dashboard for HR to understand model recommendations\n",
    "\n",
    "3. **Privacy:**\n",
    "   - Ensure differential privacy (ε < 3) for training on sensitive employee data\n",
    "   - Implement PII redaction from training logs\n",
    "   - Membership inference attack test to verify privacy leakage < 10%\n",
    "\n",
    "4. **Security:**\n",
    "   - Test robustness against gradient-based attacks (model inversion attempts)\n",
    "   - Watermark model outputs to detect theft\n",
    "   - Input validation to prevent poisoning via application form manipulation\n",
    "\n",
    "5. **Governance:**\n",
    "   - Complete Model Card documentation\n",
    "   - Datasheet for training data (collection methodology, known biases)\n",
    "   - Compliance checklist: GDPR, EEOC guidelines, local labor laws\n",
    "\n",
    "**Deliverables:**\n",
    "- `responsible_ai/` directory with audit notebooks\n",
    "- Model card (markdown)\n",
    "- Privacy/security test results\n",
    "- Presentation to \"Ethics Board\" (stakeholder presentation) explaining trade-offs and mitigations\n",
    "\n",
    "**Success Criteria:**\n",
    "- Disparate impact ratio between 0.8-1.25 (four-fifths rule compliance)\n",
    "- Model explanations provided for 100% of rejections\n",
    "- Privacy budget maintained (ε < 3)\n",
    "- No successful model extraction in security test\n",
    "- Complete documentation for regulatory audit\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 24**\n",
    "\n",
    "*You now understand how to build AI systems responsibly. Chapter 25 will cover Advanced Topics: Transformer Architecture Deep Dive.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
