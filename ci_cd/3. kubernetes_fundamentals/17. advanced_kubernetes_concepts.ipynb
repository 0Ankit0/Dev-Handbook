{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 17: Advanced Kubernetes Concepts\n",
    "\n",
    "Having mastered core resources and storage patterns, we now examine advanced workload controllers and governance mechanisms essential for production operations. These resources address specific operational requirements: ensuring node-level services run everywhere (DaemonSets), executing batch and scheduled workloads (Jobs), automatically right-sizing applications (HPA/VPA), and maintaining availability during maintenance (PDBs). Resource governance tools (Quotas and LimitRanges) enable safe multi-tenancy by preventing resource starvation and enforcing organizational policies.\n",
    "\n",
    "These concepts complete your operational toolkit, transforming Kubernetes from a basic container platform into a sophisticated, self-managing infrastructure capable of handling batch processing, automatic scaling, and enterprise-grade resource governance.\n",
    "\n",
    "## 17.1 DaemonSets\n",
    "\n",
    "DaemonSets ensure that a copy of a specific Pod runs on all (or specific) nodes in the cluster. They are ideal for cluster-wide infrastructure services that must exist on every node, such as log collectors, monitoring agents, network proxies, and storage drivers.\n",
    "\n",
    "### DaemonSet vs Deployment\n",
    "\n",
    "| Feature | Deployment | DaemonSet |\n",
    "|---------|------------|-----------|\n",
    "| **Scheduling** | Distributed across cluster for availability | One per node (or matching nodes) |\n",
    "| **Scaling** | Manual or HPA-based | Automatic with node count |\n",
    "| **Use Case** | Application workloads | Infrastructure/Node agents |\n",
    "| **Pod Naming** | Random hash | Node name-based |\n",
    "| **Update Strategy** | RollingUpdate | RollingUpdate (maxUnavailable) |\n",
    "\n",
    "### Typical DaemonSet Workloads\n",
    "\n",
    "- **Log Aggregation**: Fluentd, Fluent Bit, Filebeat\n",
    "- **Monitoring**: Prometheus Node Exporter, Datadog Agent\n",
    "- **Networking**: Calico/Flannel agents, kube-proxy\n",
    "- **Security**: Falco (intrusion detection), audit log forwarders\n",
    "- **Storage**: Ceph OSDs, CSI node plugins\n",
    "- **Utilities**: Node problem detector, cleanup agents\n",
    "\n",
    "### DaemonSet Specification\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: DaemonSet\n",
    "metadata:\n",
    "  name: fluent-bit\n",
    "  namespace: logging\n",
    "  labels:\n",
    "    app: fluent-bit\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: fluent-bit\n",
    "  updateStrategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxUnavailable: 1  # Update one node at a time\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: fluent-bit\n",
    "    spec:\n",
    "      tolerations:\n",
    "      # Tolerate control-plane nodes if desired\n",
    "      - key: node-role.kubernetes.io/control-plane\n",
    "        operator: Exists\n",
    "        effect: NoSchedule\n",
    "      - key: node-role.kubernetes.io/master\n",
    "        operator: Exists\n",
    "        effect: NoSchedule\n",
    "      containers:\n",
    "      - name: fluent-bit\n",
    "        image: fluent/fluent-bit:2.1\n",
    "        resources:\n",
    "          limits:\n",
    "            cpu: \"500m\"\n",
    "            memory: \"256Mi\"\n",
    "          requests:\n",
    "            cpu: \"100m\"\n",
    "            memory: \"128Mi\"\n",
    "        volumeMounts:\n",
    "        - name: varlog\n",
    "          mountPath: /var/log\n",
    "          readOnly: true\n",
    "        - name: varlibdockercontainers\n",
    "          mountPath: /var/lib/docker/containers\n",
    "          readOnly: true\n",
    "        - name: fluent-bit-config\n",
    "          mountPath: /fluent-bit/etc/\n",
    "      volumes:\n",
    "      - name: varlog\n",
    "        hostPath:\n",
    "          path: /var/log\n",
    "          type: Directory\n",
    "      - name: varlibdockercontainers\n",
    "        hostPath:\n",
    "          path: /var/lib/docker/containers\n",
    "          type: Directory\n",
    "      - name: fluent-bit-config\n",
    "        configMap:\n",
    "          name: fluent-bit-config\n",
    "      serviceAccountName: fluent-bit\n",
    "      terminationGracePeriodSeconds: 30\n",
    "```\n",
    "\n",
    "### Node Selection\n",
    "\n",
    "Run on specific nodes using node selectors:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        monitoring: \"true\"  # Only nodes with this label\n",
    "      tolerations:\n",
    "      - key: \"dedicated\"\n",
    "        operator: \"Equal\"\n",
    "        value: \"monitoring\"\n",
    "        effect: \"NoSchedule\"\n",
    "```\n",
    "\n",
    "### Update Strategies\n",
    "\n",
    "**RollingUpdate (Default):**\n",
    "Replaces old Pods gradually to maintain coverage:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  updateStrategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      maxUnavailable: 10%  # Can use percentage or absolute number (e.g., 1)\n",
    "```\n",
    "\n",
    "**OnDelete:**\n",
    "Manual updates; new Pod created only when old one deleted:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  updateStrategy:\n",
    "    type: OnDelete\n",
    "```\n",
    "\n",
    "Useful for critical infrastructure where automatic rolling updates might cause instability.\n",
    "\n",
    "### Monitoring DaemonSet Health\n",
    "\n",
    "```bash\n",
    "# Check status\n",
    "kubectl get daemonset fluent-bit -n logging\n",
    "\n",
    "# View which nodes are running the Pod\n",
    "kubectl get pods -n logging -l app=fluent-bit -o wide\n",
    "\n",
    "# Check if any failed to schedule\n",
    "kubectl describe daemonset fluent-bit -n logging | grep -A 10 Events\n",
    "```\n",
    "\n",
    "## 17.2 StatefulSets Deep Dive\n",
    "\n",
    "While Chapter 16 introduced StatefulSets, advanced patterns enable sophisticated deployment strategies for stateful applications.\n",
    "\n",
    "### Partitioned Rolling Updates\n",
    "\n",
    "Control the rollout to update only Pods with ordinal >= partition value, enabling canary-style updates for StatefulSets:\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: StatefulSet\n",
    "metadata:\n",
    "  name: postgres\n",
    "spec:\n",
    "  serviceName: \"postgres\"\n",
    "  replicas: 5\n",
    "  updateStrategy:\n",
    "    type: RollingUpdate\n",
    "    rollingUpdate:\n",
    "      partition: 4  # Only update pods with ordinal >= 4 (postgres-4)\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: postgres\n",
    "        image: postgres:15.4  # New version\n",
    "```\n",
    "\n",
    "**Canary Pattern:**\n",
    "1. Set `partition: 4` to update only postgres-4\n",
    "2. Test postgres-4 with new version\n",
    "3. If successful, reduce partition to update remaining pods gradually\n",
    "\n",
    "### Ordinal Controls (Kubernetes 1.27+)\n",
    "\n",
    "Start replicas from a specific ordinal number:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  ordinals:\n",
    "    start: 0  # Default, but can be set to start from higher number\n",
    "  replicas: 3\n",
    "```\n",
    "\n",
    "### Parallel Pod Management\n",
    "\n",
    "For stateful applications that don't require strict startup ordering:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  podManagementPolicy: Parallel  # Default is OrderedReady\n",
    "```\n",
    "\n",
    "- **OrderedReady**: Creates pods 0, 1, 2 sequentially; waits for each to be Running/Ready before next\n",
    "- **Parallel**: Creates all pods simultaneously (faster startup, but no ordering guarantees)\n",
    "\n",
    "### Deletion Protection\n",
    "\n",
    "Prevent accidental StatefulSet deletion while preserving Pods:\n",
    "\n",
    "```bash\n",
    "kubectl delete statefulset postgres --cascade=orphan\n",
    "# Deletes StatefulSet but leaves pods postgres-0, postgres-1, etc. running\n",
    "```\n",
    "\n",
    "Recreate with same selector to adopt existing Pods.\n",
    "\n",
    "## 17.3 Jobs and CronJobs\n",
    "\n",
    "Jobs run finite tasks to completion (batch processing), while CronJobs schedule Jobs periodically like Linux cron.\n",
    "\n",
    "### Job Specification\n",
    "\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: data-processor\n",
    "  namespace: batch\n",
    "spec:\n",
    "  completions: 5          # Must complete successfully 5 times\n",
    "  parallelism: 2          # Run 2 pods in parallel\n",
    "  completionMode: Indexed # Pods get completion index (0 to completions-1)\n",
    "  activeDeadlineSeconds: 600  # Timeout after 10 minutes\n",
    "  backoffLimit: 3         # Retry up to 3 times before marking failed\n",
    "  ttlSecondsAfterFinished: 86400  # Delete Job 24h after completion\n",
    "  template:\n",
    "    spec:\n",
    "      restartPolicy: OnFailure  # Never or OnFailure (not Always)\n",
    "      containers:\n",
    "      - name: processor\n",
    "        image: batch-processor:latest\n",
    "        env:\n",
    "        - name: JOB_COMPLETION_INDEX\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']\n",
    "        command: [\"python\", \"process.py\", \"--shard=$(JOB_COMPLETION_INDEX)\"]\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "```\n",
    "\n",
    "**Completion Modes:**\n",
    "- **NonIndexed**: Pods are fungible; any completion counts toward total\n",
    "- **Indexed**: Each Pod gets a unique index (0 to N-1), useful for parallel processing shards\n",
    "\n",
    "### Parallel Processing Patterns\n",
    "\n",
    "**Work Queue:**\n",
    "Single Job starts multiple Pods until queue is empty:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  parallelism: 10\n",
    "  completions: 1  # Only one successful completion needed\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: worker\n",
    "        image: queue-processor\n",
    "        command: [\"python\", \"worker.py\"]  # Runs until queue empty, then exits\n",
    "```\n",
    "\n",
    "**Fixed Completion Count:**\n",
    "For embarrassingly parallel batch processing:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  parallelism: 5\n",
    "  completions: 100  # Process 100 items, 5 at a time\n",
    "```\n",
    "\n",
    "### CronJobs\n",
    "\n",
    "Schedule Jobs using cron expressions:\n",
    "\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: nightly-report\n",
    "  namespace: analytics\n",
    "spec:\n",
    "  schedule: \"0 2 * * *\"  # Daily at 2 AM\n",
    "  timeZone: \"America/New_York\"  # Kubernetes 1.24+\n",
    "  concurrencyPolicy: Forbid  # Forbid, Allow, or Replace\n",
    "  startingDeadlineSeconds: 3600  # Must start within 1 hour of scheduled time\n",
    "  successfulJobsHistoryLimit: 3\n",
    "  failedJobsHistoryLimit: 1\n",
    "  suspend: false  # Can pause by setting to true\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          activeDeadlineSeconds: 7200  # Job must complete within 2 hours\n",
    "          backoffLimit: 2\n",
    "          restartPolicy: OnFailure\n",
    "          containers:\n",
    "          - name: report-generator\n",
    "            image: analytics/reporter:v1.2\n",
    "            env:\n",
    "            - name: REPORT_DATE\n",
    "              value: \"$(date +%Y-%m-%d)\"\n",
    "            resources:\n",
    "              requests:\n",
    "                memory: \"8Gi\"\n",
    "                cpu: \"4000m\"\n",
    "              limits:\n",
    "                memory: \"16Gi\"\n",
    "                cpu: \"8000m\"\n",
    "```\n",
    "\n",
    "**Cron Expression Format:**\n",
    "```\n",
    "┌───────────── minute (0 - 59)\n",
    "│ ┌───────────── hour (0 - 23)\n",
    "│ │ ┌───────────── day of month (1 - 31)\n",
    "│ │ │ ┌───────────── month (1 - 12)\n",
    "│ │ │ │ ┌───────────── day of week (0 - 6) (Sunday=0)\n",
    "│ │ │ │ │\n",
    "* * * * *\n",
    "```\n",
    "\n",
    "**Common Patterns:**\n",
    "- `*/5 * * * *` : Every 5 minutes\n",
    "- `0 */6 * * *` : Every 6 hours\n",
    "- `0 0 * * 0` : Weekly on Sunday\n",
    "- `0 0 1 * *` : Monthly on the 1st\n",
    "\n",
    "### Handling Job Failures\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  backoffLimit: 4\n",
    "  backoffLimitPerIndex: 2  # Kubernetes 1.28+: max failures per index in Indexed jobs\n",
    "  maxFailedIndexes: 5       # Kubernetes 1.28+: max failed indexes before aborting\n",
    "  podFailurePolicy:         # Kubernetes 1.26+: fine-grained failure handling\n",
    "    rules:\n",
    "    - action: FailJob\n",
    "      onExitCodes:\n",
    "        operator: In\n",
    "        values: [1, 2, 137]\n",
    "    - action: Ignore  # Don't count as failure\n",
    "      onPodConditions:\n",
    "      - type: DisruptionTarget\n",
    "        status: \"True\"\n",
    "```\n",
    "\n",
    "## 17.4 Horizontal Pod Autoscaling (HPA)\n",
    "\n",
    "HPA automatically scales the number of Pod replicas in a Deployment, ReplicaSet, or StatefulSet based on observed metrics such as CPU utilization, memory usage, or custom metrics.\n",
    "\n",
    "### Basic CPU-Based Autoscaling\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: web-app-hpa\n",
    "  namespace: production\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: web-app\n",
    "  minReplicas: 3\n",
    "  maxReplicas: 50\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70  # Target 70% CPU utilization\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: memory\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 80\n",
    "  behavior:  # Kubernetes 1.18+\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300  # Wait 5 minutes before scaling down\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 10  # Scale down max 10% of replicas per minute\n",
    "        periodSeconds: 60\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100  # Double replicas if needed\n",
    "        periodSeconds: 60\n",
    "      - type: Pods\n",
    "        value: 4   # Or add 4 pods per minute, whichever is higher\n",
    "        periodSeconds: 60\n",
    "      selectPolicy: Max  # Use the policy that adds most replicas\n",
    "```\n",
    "\n",
    "### Custom Metrics (Prometheus Adapter)\n",
    "\n",
    "Scale based on application-specific metrics like requests per second:\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: api-gateway-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: api-gateway\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 100\n",
    "  metrics:\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: http_requests_per_second\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"1000\"  # 1000 requests/second per pod\n",
    "  - type: External\n",
    "    external:\n",
    "      metric:\n",
    "        name: queue_length\n",
    "        selector:\n",
    "          matchLabels:\n",
    "            queue: orders\n",
    "      target:\n",
    "        type: Value\n",
    "        value: \"100\"  # Scale when queue has 100+ messages\n",
    "```\n",
    "\n",
    "**Prerequisites:**\n",
    "- Custom Metrics API configured (Prometheus Adapter or Metrics Server)\n",
    "- Application exposing metrics in Prometheus format\n",
    "\n",
    "### Scaling Policies\n",
    "\n",
    "**Scale Down Constraints:**\n",
    "Prevent flapping by limiting scale-down speed:\n",
    "\n",
    "```yaml\n",
    "behavior:\n",
    "  scaleDown:\n",
    "    stabilizationWindowSeconds: 600  # Look back 10 minutes for minimum utilization\n",
    "    policies:\n",
    "    - type: Percent\n",
    "      value: 10  # Remove max 10% of pods per minute\n",
    "      periodSeconds: 60\n",
    "    - type: Pods\n",
    "      value: 2   # Or remove max 2 pods per minute\n",
    "      periodSeconds: 60\n",
    "    selectPolicy: Min  # Use the policy that removes fewer pods\n",
    "```\n",
    "\n",
    "### HPA Troubleshooting\n",
    "\n",
    "```bash\n",
    "# Check HPA status\n",
    "kubectl get hpa web-app-hpa\n",
    "kubectl describe hpa web-app-hpa\n",
    "\n",
    "# View current metrics\n",
    "kubectl get --raw \"/apis/autoscaling/v2/namespaces/production/horizontalpodautoscalers/web-app-hpa\" | jq\n",
    "\n",
    "# Check if metrics server is running\n",
    "kubectl get pods -n kube-system | grep metrics-server\n",
    "\n",
    "# Verify resource requests are set (HPA requires requests)\n",
    "kubectl get deployment web-app -o yaml | grep -A 5 resources\n",
    "```\n",
    "\n",
    "**Common Issues:**\n",
    "- **Missing Resource Requests**: HPA requires resource requests to calculate utilization\n",
    "- **Metrics Server Unavailable**: HPA needs metrics-server for CPU/memory\n",
    "- **Stabilization**: Scale-down may be delayed by stabilization windows\n",
    "\n",
    "## 17.5 Vertical Pod Autoscaling (VPA)\n",
    "\n",
    "While HPA scales horizontally (more Pods), VPA scales vertically (more CPU/memory per Pod). VPA is ideal for applications that cannot scale horizontally (databases, singleton services) or have variable resource needs.\n",
    "\n",
    "### VPA Modes\n",
    "\n",
    "**Off**: Recommendations only, no automatic changes\n",
    "**Initial**: Applies recommendations only to new Pods (at creation)\n",
    "**Recreate**: Updates existing Pods by evicting them (causes downtime)\n",
    "**Auto**: Combines Initial and Recreate; may use both depending on capabilities\n",
    "\n",
    "### VPA Specification\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling.k8s.io/v1\n",
    "kind: VerticalPodAutoscaler\n",
    "metadata:\n",
    "  name: database-vpa\n",
    "  namespace: production\n",
    "spec:\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: StatefulSet\n",
    "    name: postgres\n",
    "  updatePolicy:\n",
    "    updateMode: \"Auto\"  # Off, Initial, Recreate, or Auto\n",
    "    minReplicas: 2  # Minimum replicas to consider for update (prevents single-pod downtime)\n",
    "  resourcePolicy:\n",
    "    containerPolicies:\n",
    "    - containerName: postgres\n",
    "      minAllowed:\n",
    "        cpu: \"500m\"\n",
    "        memory: \"1Gi\"\n",
    "      maxAllowed:\n",
    "        cpu: \"4\"\n",
    "        memory: \"16Gi\"\n",
    "      controlledResources: [\"cpu\", \"memory\"]\n",
    "      controlledValues: RequestsAndLimits  # Or RequestsOnly\n",
    "```\n",
    "\n",
    "### VPA Recommendations\n",
    "\n",
    "View recommendations without applying:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  updatePolicy:\n",
    "    updateMode: \"Off\"\n",
    "```\n",
    "\n",
    "Check recommendations:\n",
    "```bash\n",
    "kubectl get vpa database-vpa -o yaml\n",
    "\n",
    "# Output includes:\n",
    "# recommendation:\n",
    "#   containerRecommendations:\n",
    "#   - containerName: postgres\n",
    "#     lowerBound:\n",
    "#       cpu: \"800m\"\n",
    "#       memory: 2.5Gi\n",
    "#     target:\n",
    "#       cpu: \"1000m\"\n",
    "#       memory: 3Gi\n",
    "#     upperBound:\n",
    "#       cpu: \"1200m\"\n",
    "#       memory: 4Gi\n",
    "```\n",
    "\n",
    "### Combining HPA and VPA\n",
    "\n",
    "**Warning**: Do not use HPA and VPA simultaneously on the same resource unless VPA is in \"Off\" or \"Initial\" mode, as they will conflict.\n",
    "\n",
    "**Recommended Pattern:**\n",
    "- Use VPA in `Off` mode to get recommendations\n",
    "- Manually update Deployment resource requests based on recommendations\n",
    "- Use HPA for horizontal scaling\n",
    "\n",
    "Or use VPA for initial sizing only:\n",
    "```yaml\n",
    "spec:\n",
    "  updatePolicy:\n",
    "    updateMode: \"Initial\"  # Only size new pods, let HPA handle scaling\n",
    "```\n",
    "\n",
    "## 17.6 Pod Disruption Budgets (PDB)\n",
    "\n",
    "PDBs ensure that voluntary disruptions (node drains, cluster upgrades, manual deletions) do not compromise application availability by maintaining a minimum number of available Pods.\n",
    "\n",
    "### Voluntary vs Involuntary Disruptions\n",
    "\n",
    "**Voluntary** (respects PDB):\n",
    "- Node drain (`kubectl drain`)\n",
    "- Pod deletion by human or automation\n",
    "- Cluster autoscaler scaling down\n",
    "- Priority-based preemption\n",
    "\n",
    "**Involuntary** (ignores PDB):\n",
    "- Hardware failure\n",
    "- Kernel panic\n",
    "- Network partition\n",
    "- Out-of-memory killing\n",
    "\n",
    "### PDB Specification\n",
    "\n",
    "```yaml\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: api-pdb\n",
    "  namespace: production\n",
    "spec:\n",
    "  minAvailable: 2  # OR maxUnavailable: 1 (cannot use both)\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: api-gateway\n",
    "      tier: backend\n",
    "```\n",
    "\n",
    "**Strategies:**\n",
    "- **minAvailable**: Absolute number or percentage of Pods that must remain available\n",
    "- **maxUnavailable**: Maximum number or percentage of Pods that can be unavailable during disruption\n",
    "\n",
    "**Percentage Values:**\n",
    "```yaml\n",
    "spec:\n",
    "  minAvailable: 50%  # At least half must remain\n",
    "  # OR\n",
    "  maxUnavailable: 25%  # No more than 25% can be down\n",
    "```\n",
    "\n",
    "### PDB with StatefulSets\n",
    "\n",
    "Critical for stateful applications:\n",
    "\n",
    "```yaml\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: postgres-pdb\n",
    "spec:\n",
    "  maxUnavailable: 1  # Only one PostgreSQL pod down at a time\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: postgres\n",
    "```\n",
    "\n",
    "**Important**: For StatefulSets with single replica, PDBs cannot prevent disruption (minAvailable: 1 with 1 replica means 0 can be disrupted). Use `maxUnavailable: 0` or ensure multiple replicas.\n",
    "\n",
    "### Checking PDB Status\n",
    "\n",
    "```bash\n",
    "kubectl get pdb -n production\n",
    "\n",
    "# NAME       MIN AVAILABLE   MAX UNAVAILABLE   ALLOWED DISRUPTIONS   AGE\n",
    "# api-pdb    2               N/A               1                     10d\n",
    "# postgres   N/A             1                 0                     10d\n",
    "\n",
    "# \"Allowed Disruptions\" shows how many can be evicted currently\n",
    "```\n",
    "\n",
    "If `ALLOWED DISRUPTIONS` is 0, `kubectl drain` will block until more Pods become available.\n",
    "\n",
    "## 17.7 Resource Quotas\n",
    "\n",
    "ResourceQuotas limit aggregate resource consumption per namespace, preventing a single team or application from monopolizing cluster resources.\n",
    "\n",
    "### ResourceQuota Specification\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ResourceQuota\n",
    "metadata:\n",
    "  name: production-quota\n",
    "  namespace: production\n",
    "spec:\n",
    "  hard:\n",
    "    # Compute Resources\n",
    "    requests.cpu: \"100\"\n",
    "    requests.memory: 500Gi\n",
    "    limits.cpu: \"200\"\n",
    "    limits.memory: 1000Gi\n",
    "    \n",
    "    # Storage\n",
    "    requests.storage: 5Ti\n",
    "    persistentvolumeclaims: \"50\"\n",
    "    <storage-class-name>.storageclass.storage.k8s.io/requests.storage: 2Ti\n",
    "    \n",
    "    # Object Counts\n",
    "    pods: \"100\"\n",
    "    replicationcontrollers: \"20\"\n",
    "    secrets: \"50\"\n",
    "    configmaps: \"50\"\n",
    "    persistentvolumeclaims: \"50\"\n",
    "    services: \"50\"\n",
    "    services.loadbalancers: \"10\"\n",
    "    services.nodeports: \"10\"\n",
    "    count/ingresses.extensions: \"20\"\n",
    "    count/jobs.batch: \"50\"\n",
    "    count/cronjobs.batch: \"20\"\n",
    "    \n",
    "    # GPU Resources (if using device plugins)\n",
    "    requests.nvidia.com/gpu: \"10\"\n",
    "    \n",
    "    # Local Ephemeral Storage (Kubernetes 1.22+)\n",
    "    requests.ephemeral-storage: 500Gi\n",
    "    limits.ephemeral-storage: 1Ti\n",
    "```\n",
    "\n",
    "### Scope Selectors\n",
    "\n",
    "Limit quotas to specific Pod priorities or phases:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  scopes:\n",
    "    - BestEffort        # Only quota BestEffort pods (no resource requests)\n",
    "    - NotBestEffort     # Only quota Guaranteed/Burstable pods\n",
    "    - Terminating       # Only quota pods with activeDeadlineSeconds\n",
    "    - NotTerminating    # Only quota pods without deadline\n",
    "```\n",
    "\n",
    "**Priority Class Scoping (Kubernetes 1.24+):**\n",
    "```yaml\n",
    "spec:\n",
    "  scopeSelector:\n",
    "    matchExpressions:\n",
    "    - scopeName: PriorityClass\n",
    "      operator: In\n",
    "      values: [\"high-priority\", \"production\"]\n",
    "```\n",
    "\n",
    "### Checking Quota Usage\n",
    "\n",
    "```bash\n",
    "kubectl get resourcequota -n production\n",
    "kubectl describe resourcequota production-quota -n production\n",
    "\n",
    "# Output shows used vs hard limits:\n",
    "# Name:            production-quota\n",
    "# Resource         Used    Hard\n",
    "# --------         ----    ----\n",
    "# limits.cpu       50      200\n",
    "# limits.memory    200Gi   1000Gi\n",
    "# pods             35      100\n",
    "# requests.cpu     30      100\n",
    "# requests.memory  120Gi   500Gi\n",
    "```\n",
    "\n",
    "## 17.8 Limit Ranges\n",
    "\n",
    "While ResourceQuotas constrain aggregate namespace consumption, LimitRanges constrain individual Pod and container resources, enforcing defaults and preventing resource starvation.\n",
    "\n",
    "### Container Limits\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: LimitRange\n",
    "metadata:\n",
    "  name: default-limits\n",
    "  namespace: production\n",
    "spec:\n",
    "  limits:\n",
    "  - default:  # Applied if container specifies no limits\n",
    "      cpu: \"500m\"\n",
    "      memory: \"512Mi\"\n",
    "    defaultRequest:  # Applied if container specifies no requests\n",
    "      cpu: \"100m\"\n",
    "      memory: \"128Mi\"\n",
    "    max:  # Maximum allowed\n",
    "      cpu: \"2\"\n",
    "      memory: \"4Gi\"\n",
    "    min:  # Minimum required\n",
    "      cpu: \"50m\"\n",
    "      memory: \"64Mi\"\n",
    "    type: Container\n",
    "    \n",
    "  # Pod-level limits (sum of all containers)\n",
    "  - max:\n",
    "      cpu: \"4\"\n",
    "      memory: \"8Gi\"\n",
    "    min:\n",
    "      cpu: \"100m\"\n",
    "      memory: \"128Mi\"\n",
    "    type: Pod\n",
    "    \n",
    "  # PVC limits\n",
    "  - max:\n",
    "      storage: 500Gi\n",
    "    min:\n",
    "      storage: 1Gi\n",
    "    default:\n",
    "      storage: 10Gi\n",
    "    defaultRequest:\n",
    "      storage: 10Gi\n",
    "    type: PersistentVolumeClaim\n",
    "    \n",
    "  # Ratio constraints (limit to request ratio)\n",
    "  - maxLimitRequestRatio:\n",
    "      cpu: \"4\"  # Limit cannot be more than 4x request\n",
    "      memory: \"2\"\n",
    "    type: Container\n",
    "```\n",
    "\n",
    "### Enforcing Quality of Service\n",
    "\n",
    "Guarantee proper QoS classes:\n",
    "\n",
    "```yaml\n",
    "spec:\n",
    "  limits:\n",
    "  - default:\n",
    "      cpu: \"100m\"\n",
    "      memory: \"128Mi\"\n",
    "    defaultRequest:\n",
    "      cpu: \"100m\"\n",
    "      memory: \"128Mi\"  # Same as limit = Guaranteed QoS\n",
    "    type: Container\n",
    "```\n",
    "\n",
    "### Checking LimitRange Application\n",
    "\n",
    "```bash\n",
    "kubectl describe limitrange default-limits -n production\n",
    "\n",
    "# Verify applied to new pods\n",
    "kubectl get pod mypod -o yaml | grep -A 10 resources\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "In this chapter, we explored advanced Kubernetes resources essential for production operations. DaemonSets ensure infrastructure services run on every node, critical for logging, monitoring, and networking agents. Jobs and CronJobs handle batch processing and scheduled tasks with sophisticated retry logic and parallel execution patterns. Horizontal Pod Autoscaler (HPA) enables dynamic scaling based on resource utilization or custom application metrics, while Vertical Pod Autoscaler (VPA) automatically rightsizes container resource allocations. Pod Disruption Budgets (PDBs) safeguard availability during voluntary disruptions like node maintenance or cluster upgrades. Resource Quotas prevent namespace resource monopolization in multi-tenant environments, and LimitRanges enforce organizational policies on individual container specifications, ensuring consistent resource requests and limits across workloads.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Use DaemonSets for node-level infrastructure services requiring presence on every machine; avoid for application workloads that should scale independently of node count.\n",
    "- Implement PodDisruptionBudgets for all production workloads to ensure cluster maintenance operations do not compromise availability.\n",
    "- Combine HPA for variable load scaling with VPA in \"Off\" or \"Initial\" mode for capacity planning, but avoid simultaneous automatic scaling to prevent conflicts.\n",
    "- Enforce LimitRanges in every namespace to prevent resource starvation and ensure consistent QoS class distributions.\n",
    "- Use Indexed Jobs for parallel processing workloads requiring shard awareness, and set appropriate backoff limits to prevent infinite retry loops on fundamentally failed jobs.\n",
    "\n",
    "**Next Chapter Preview:**\n",
    "Chapter 18: Kubernetes Security explores defense-in-depth strategies for production clusters. You will implement Role-Based Access Control (RBAC) for fine-grained permissions, configure Pod Security Standards to enforce runtime constraints, utilize Network Policies for micro-segmentation, manage Secrets with external secret management integration, and implement admission controllers for policy enforcement. These security mechanisms protect the advanced workloads and autoscaling configurations established in this chapter, ensuring that operational flexibility does not compromise security posture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
