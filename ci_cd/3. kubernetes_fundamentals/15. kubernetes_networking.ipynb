{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 15: Kubernetes Networking\n",
    "\n",
    "Having mastered Kubernetes core resources in Chapter 14, we now examine the networking fabric that binds these components together. Networking in Kubernetes differs fundamentally from traditional Docker networking or virtual machine networking. The platform imposes specific requirements\u2014namely that every Pod receives a unique IP address across the cluster and that all Pods can communicate without Network Address Translation (NAT)\u2014which necessitate specialized Container Network Interface (CNI) implementations.\n",
    "\n",
    "This chapter explores the Kubernetes networking model from Pod-level communication through cluster-wide service discovery, ingress routing, and security policies. Understanding these mechanisms is essential for debugging connectivity issues, designing secure microservices architectures, and optimizing application performance in distributed environments.\n",
    "\n",
    "## 15.1 Pod Networking Model\n",
    "\n",
    "The Kubernetes networking model establishes fundamental principles that distinguish it from other container orchestration systems. These requirements ensure transparent, flat networking across the cluster.\n",
    "\n",
    "### Core Networking Requirements\n",
    "\n",
    "Kubernetes mandates three essential networking behaviors:\n",
    "\n",
    "1. **All containers (Pods) can communicate with all other containers without NAT**\n",
    "   - Every Pod has a unique IP routable within the cluster\n",
    "   - Source IP seen by the destination matches the sender's IP\n",
    "   - No port mapping or host networking required for basic connectivity\n",
    "\n",
    "2. **All nodes can communicate with all containers (and vice versa) without NAT**\n",
    "   - Kubelet can reach Pods directly for health checks and exec operations\n",
    "   - Pods can reach node IPs for host-based services\n",
    "\n",
    "3. **The IP that a container sees itself as is the same IP that others see it as**\n",
    "   - No internal/external IP dichotomy within the cluster\n",
    "   - Self-identification matches external identification\n",
    "\n",
    "### Network Namespace Implementation\n",
    "\n",
    "Each Pod operates within its own network namespace on the host node, connected to the cluster network through a virtual ethernet (veth) pair:\n",
    "\n",
    "**Architecture Flow:**\n",
    "1. **Pod Namespace**: Contains the container's eth0 interface\n",
    "2. **Veth Pair**: Virtual cable connecting Pod namespace to host namespace\n",
    "3. **CNI Bridge/Interface**: Connects to the node's physical network or overlay\n",
    "4. **Routing/Fabric**: Routes packets between nodes via underlay or overlay networks\n",
    "\n",
    "**IP Address Allocation:**\n",
    "- The CNI plugin assigns IP addresses to Pods from a per-node CIDR range\n",
    "- The node CIDRs aggregate into the cluster CIDR (e.g., 10.244.0.0/16)\n",
    "- IPAM (IP Address Management) ensures uniqueness across the cluster\n",
    "\n",
    "### Cluster DNS\n",
    "\n",
    "Every Pod receives DNS configuration enabling service discovery:\n",
    "\n",
    "- **DNS Server**: CoreDNS (or kube-dns in older clusters) runs as a cluster add-on\n",
    "- **Search Domains**: Configured with default search paths:\n",
    "  - `default.svc.cluster.local`\n",
    "  - `svc.cluster.local`\n",
    "  - `cluster.local`\n",
    "- **NDOTS**: Default ndots:5 means names with fewer than 5 dots get search domain suffixes appended\n",
    "\n",
    "**Verification:**\n",
    "```bash\n",
    "# Inside a Pod\n",
    "cat /etc/resolv.conf\n",
    "# Output:\n",
    "# nameserver 10.96.0.10\n",
    "# search default.svc.cluster.local svc.cluster.local cluster.local\n",
    "# options ndots:5\n",
    "```\n",
    "\n",
    "## 15.2 Service DNS\n",
    "\n",
    "While Services provide stable IP addresses (ClusterIPs), DNS provides stable names that abstract away IP changes during Pod restarts or scaling events.\n",
    "\n",
    "### CoreDNS Architecture\n",
    "\n",
    "CoreDNS is the default cluster DNS server, replacing the legacy kube-dns implementation. It runs as a Deployment (typically 2+ replicas for high availability) and Service within the `kube-system` namespace.\n",
    "\n",
    "**DNS Record Types:**\n",
    "\n",
    "**A/AAAA Records (Normal Services):**\n",
    "```\n",
    "<service-name>.<namespace>.svc.<cluster-domain>\n",
    "```\n",
    "Example: `mysql.production.svc.cluster.local` resolves to the Service ClusterIP (e.g., 10.96.123.45)\n",
    "\n",
    "**SRV Records (Named Ports):**\n",
    "```\n",
    "_<port-name>._<protocol>.<service-name>.<namespace>.svc.<cluster-domain>\n",
    "```\n",
    "Example: `_http._tcp.web.production.svc.cluster.local`\n",
    "\n",
    "**PTR Records (Reverse DNS):**\n",
    "Map IPs back to service names for logging and authentication purposes.\n",
    "\n",
    "**Headless Services:**\n",
    "For Headless Services (clusterIP: None), DNS returns A records for each Pod IP directly rather than the Service IP:\n",
    "```\n",
    "pod-ip-address.<service-name>.<namespace>.svc.<cluster-domain>\n",
    "```\n",
    "Example: `10-244-1-5.db-headless.production.svc.cluster.local`\n",
    "\n",
    "### DNS Resolution Patterns\n",
    "\n",
    "**Same Namespace Resolution:**\n",
    "```bash\n",
    "# From a Pod in the 'production' namespace\n",
    "curl http://backend-service:8080\n",
    "# Automatically expands to backend-service.production.svc.cluster.local\n",
    "```\n",
    "\n",
    "**Cross-Namespace Resolution:**\n",
    "```bash\n",
    "# Must specify full name or at least service.namespace\n",
    "curl http://backend-service.staging:8080\n",
    "# Or fully qualified\n",
    "curl http://backend-service.staging.svc.cluster.local:8080\n",
    "```\n",
    "\n",
    "**External Name Services:**\n",
    "Map internal DNS names to external DNS names via CNAME records:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: external-database\n",
    "  namespace: production\n",
    "spec:\n",
    "  type: ExternalName\n",
    "  externalName: prod-db.company.rds.amazonaws.com\n",
    "```\n",
    "\n",
    "Usage: `external-database.production.svc.cluster.local` resolves to the AWS RDS endpoint.\n",
    "\n",
    "### CoreDNS Customization\n",
    "\n",
    "Customize DNS behavior via CoreDNS ConfigMap:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: coredns\n",
    "  namespace: kube-system\n",
    "data:\n",
    "  Corefile: |\n",
    "    .:53 {\n",
    "        errors\n",
    "        health {\n",
    "           lameduck 5s\n",
    "        }\n",
    "        ready\n",
    "        kubernetes cluster.local in-addr.arpa ip6.arpa {\n",
    "           pods insecure\n",
    "           fallthrough in-addr.arpa ip6.arpa\n",
    "           ttl 30\n",
    "        }\n",
    "        prometheus :9153\n",
    "        forward . /etc/resolv.conf {\n",
    "           max_concurrent 1000\n",
    "        }\n",
    "        cache 30\n",
    "        loop\n",
    "        reload\n",
    "        loadbalance\n",
    "    }\n",
    "    # Custom domain for internal company services\n",
    "    company.internal:53 {\n",
    "        errors\n",
    "        cache 30\n",
    "        forward . 10.0.0.10 10.0.0.11\n",
    "    }\n",
    "```\n",
    "\n",
    "**Key Directives:**\n",
    "- **kubernetes**: Handles cluster DNS records\n",
    "- **forward**: Forwards unknown queries to upstream DNS (node's /etc/resolv.conf)\n",
    "- **cache**: Caches responses to reduce latency\n",
    "- **rewrite**: Can modify queries (e.g., rewrite internal domains)\n",
    "\n",
    "### Debugging DNS Issues\n",
    "\n",
    "Common resolution problems and diagnostics:\n",
    "\n",
    "```bash\n",
    "# Test DNS resolution from within cluster\n",
    "kubectl run -it --rm debug --image=busybox:1.36 --restart=Never -- nslookup kubernetes.default\n",
    "\n",
    "# Check if CoreDNS pods are running\n",
    "kubectl get pods -n kube-system -l k8s-app=kube-dns\n",
    "\n",
    "# View CoreDNS logs\n",
    "kubectl logs -n kube-system -l k8s-app=kube-dns\n",
    "\n",
    "# Check DNS service endpoints\n",
    "kubectl get endpoints -n kube-system kube-dns\n",
    "\n",
    "# Verify Pod DNS configuration\n",
    "kubectl get pod mypod -o yaml | grep -A 5 dnsPolicy\n",
    "```\n",
    "\n",
    "## 15.3 Ingress Controllers\n",
    "\n",
    "While Services expose applications within or externally via LoadBalancers, Ingress resources provide HTTP/HTTPS routing, SSL termination, and name-based virtual hosting. The Ingress Controller implements the actual routing logic.\n",
    "\n",
    "### Ingress vs. LoadBalancer\n",
    "\n",
    "**LoadBalancer Service:**\n",
    "- One IP per Service\n",
    "- Layer 4 (TCP/UDP) load balancing\n",
    "- Expensive (each Service gets its own cloud load balancer)\n",
    "- No routing rules, SSL termination, or host/path-based routing\n",
    "\n",
    "**Ingress:**\n",
    "- Single IP for multiple Services\n",
    "- Layer 7 (HTTP/HTTPS) routing\n",
    "- Host-based routing (`api.example.com` vs `www.example.com`)\n",
    "- Path-based routing (`/api/*` vs `/static/*`)\n",
    "- SSL termination (TLS certificates at entry point)\n",
    "- Cost-effective (one cloud load balancer for many services)\n",
    "\n",
    "### NGINX Ingress Controller (Most Common)\n",
    "\n",
    "The NGINX Ingress Controller is the de facto standard, maintained by the Kubernetes community and NGINX Inc.\n",
    "\n",
    "**Installation (Helm):**\n",
    "```bash\n",
    "helm repo add ingress-nginx https://kubernetes.github.io/ingress-nginx\n",
    "helm repo update\n",
    "\n",
    "helm install ingress-nginx ingress-nginx/ingress-nginx \\\n",
    "  --namespace ingress-nginx \\\n",
    "  --create-namespace \\\n",
    "  --set controller.service.type=LoadBalancer \\\n",
    "  --set controller.metrics.enabled=true \\\n",
    "  --set controller.podAnnotations.\"prometheus\\.io/scrape\"=\"true\" \\\n",
    "  --set controller.podAnnotations.\"prometheus\\.io/port\"=\"10254\"\n",
    "```\n",
    "\n",
    "**Basic Ingress Resource:**\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: example-ingress\n",
    "  namespace: production\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/rewrite-target: /\n",
    "    nginx.ingress.kubernetes.io/ssl-redirect: \"true\"\n",
    "spec:\n",
    "  ingressClassName: nginx  # Selects which controller implements this\n",
    "  rules:\n",
    "  - host: api.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /v1\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: api-service-v1\n",
    "            port:\n",
    "              number: 80\n",
    "      - path: /v2\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: api-service-v2\n",
    "            port:\n",
    "              number: 80\n",
    "  - host: www.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: frontend-service\n",
    "            port:\n",
    "              number: 80\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - api.example.com\n",
    "    - www.example.com\n",
    "    secretName: example-tls-secret  # References TLS Secret\n",
    "```\n",
    "\n",
    "**Path Types:**\n",
    "- **Exact**: Matches URL path exactly (case-sensitive)\n",
    "- **Prefix**: Matches URL path prefix split by `/` (e.g., `/api` matches `/api`, `/api/`, `/api/v1`)\n",
    "- **ImplementationSpecific**: Interpretation depends on IngressClass (avoid for portability)\n",
    "\n",
    "### Advanced Ingress Patterns\n",
    "\n",
    "**Rate Limiting:**\n",
    "```yaml\n",
    "metadata:\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/limit-rps: \"10\"  # Requests per second\n",
    "    nginx.ingress.kubernetes.io/limit-connections: \"5\"\n",
    "```\n",
    "\n",
    "**Sticky Sessions (Session Affinity):**\n",
    "```yaml\n",
    "metadata:\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/affinity: \"cookie\"\n",
    "    nginx.ingress.kubernetes.io/session-cookie-name: \"route\"\n",
    "    nginx.ingress.kubernetes.io/session-cookie-expires: \"172800\"\n",
    "    nginx.ingress.kubernetes.io/session-cookie-max-age: \"172800\"\n",
    "```\n",
    "\n",
    "**CORS Configuration:**\n",
    "```yaml\n",
    "metadata:\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/enable-cors: \"true\"\n",
    "    nginx.ingress.kubernetes.io/cors-allow-origin: \"https://example.com\"\n",
    "    nginx.ingress.kubernetes.io/cors-allow-methods: \"GET, POST, OPTIONS\"\n",
    "```\n",
    "\n",
    "**Canary Deployments:**\n",
    "```yaml\n",
    "metadata:\n",
    "  annotations:\n",
    "    nginx.ingress.kubernetes.io/canary: \"true\"\n",
    "    nginx.ingress.kubernetes.io/canary-weight: \"20\"  # 20% traffic\n",
    "    # or by header\n",
    "    nginx.ingress.kubernetes.io/canary-by-header: \"X-Canary\"\n",
    "    nginx.ingress.kubernetes.io/canary-by-header-value: \"true\"\n",
    "```\n",
    "\n",
    "### Alternative Ingress Controllers\n",
    "\n",
    "**Traefik (Cloud-Native, Dynamic):**\n",
    "- Auto-discovery of services\n",
    "- Native Let's Encrypt support\n",
    "- Middleware chains (authentication, rate limiting, redirects)\n",
    "- Dashboard for visualization\n",
    "\n",
    "**HAProxy Ingress (High Performance):**\n",
    "- Known for performance and stability\n",
    "- Advanced load balancing algorithms\n",
    "- Suitable for high-throughput scenarios\n",
    "\n",
    "**Contour (Envoy-Based):**\n",
    "- Uses Lyft's Envoy proxy as data plane\n",
    "- HTTPProxy CRD (alternative to Ingress) with more features\n",
    "- Delegation patterns for multi-team environments\n",
    "\n",
    "**Istio Ingress Gateway (Service Mesh):**\n",
    "- Part of Istio service mesh\n",
    "- Advanced traffic management (traffic splitting, mirroring)\n",
    "- mTLS enforcement\n",
    "- Detailed telemetry\n",
    "\n",
    "## 15.4 Network Policies\n",
    "\n",
    "By default, Kubernetes allows all Pods to communicate freely\u2014a \"zero trust\" violation. NetworkPolicies provide micro-segmentation, controlling traffic flow at the IP address or port level (OSI layer 3/4).\n",
    "\n",
    "### Network Policy Basics\n",
    "\n",
    "**Default Behavior:**\n",
    "- **Isolation**: Non-isolated Pods accept all traffic\n",
    "- **Policy Selection**: Pods become isolated when any NetworkPolicy selects them\n",
    "- **Direction**: Policies can restrict ingress (incoming), egress (outgoing), or both\n",
    "\n",
    "**Prerequisites:**\n",
    "NetworkPolicies require a CNI plugin that supports them (Calico, Cilium, Weave, Antrea). Flannel does **not** support NetworkPolicies.\n",
    "\n",
    "### Policy Examples\n",
    "\n",
    "**Deny All Ingress (Default Deny):**\n",
    "Start with default deny, then explicitly allow required traffic:\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: default-deny-ingress\n",
    "  namespace: production\n",
    "spec:\n",
    "  podSelector: {}  # Empty = all pods in namespace\n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  # No ingress rules = deny all incoming\n",
    "```\n",
    "\n",
    "**Allow Specific Ingress:**\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: api-allow\n",
    "  namespace: production\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app: api\n",
    "      tier: backend\n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  ingress:\n",
    "  - from:\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          name: frontend  # From namespace labeled 'frontend'\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app: web  # AND/OR from pods labeled app=web in same namespace\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 8080\n",
    "```\n",
    "\n",
    "**Allow Specific Egress (External API Calls):**\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: egress-restricted\n",
    "  namespace: production\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app: payment-service\n",
    "  policyTypes:\n",
    "  - Egress\n",
    "  egress:\n",
    "  - to:\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          name: database\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 5432\n",
    "  - to:  # Allow DNS\n",
    "    - namespaceSelector: {}\n",
    "      podSelector:\n",
    "        matchLabels:\n",
    "          k8s-app: kube-dns\n",
    "    ports:\n",
    "    - protocol: UDP\n",
    "      port: 53\n",
    "  - to:  # Specific external API\n",
    "    - ipBlock:\n",
    "        cidr: 203.0.113.0/24\n",
    "        except:\n",
    "        - 203.0.113.5/32  # Exclude specific IP\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 443\n",
    "```\n",
    "\n",
    "**Complex Multi-Rule Policy:**\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: comprehensive-policy\n",
    "spec:\n",
    "  podSelector:\n",
    "    matchLabels:\n",
    "      app: complex-app\n",
    "  policyTypes:\n",
    "  - Ingress\n",
    "  - Egress\n",
    "  ingress:\n",
    "  - from:\n",
    "    - ipBlock:\n",
    "        cidr: 10.0.0.0/8\n",
    "    - namespaceSelector:\n",
    "        matchLabels:\n",
    "          environment: trusted\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app: authorized-client\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 80\n",
    "    - protocol: TCP\n",
    "      port: 443\n",
    "  egress:\n",
    "  - to:\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          app: database\n",
    "    ports:\n",
    "    - protocol: TCP\n",
    "      port: 5432\n",
    "  - to:\n",
    "    - podSelector:\n",
    "        matchLabels:\n",
    "          k8s-app: kube-dns\n",
    "    ports:\n",
    "    - protocol: UDP\n",
    "      port: 53\n",
    "```\n",
    "\n",
    "### CNI-Specific Features\n",
    "\n",
    "**Calico NetworkPolicy (Enhanced):**\n",
    "Calico extends standard NetworkPolicy with additional features:\n",
    "\n",
    "```yaml\n",
    "apiVersion: projectcalico.org/v3\n",
    "kind: NetworkPolicy\n",
    "metadata:\n",
    "  name: advanced-policy\n",
    "spec:\n",
    "  selector: app == 'backend'\n",
    "  types:\n",
    "  - Ingress\n",
    "  ingress:\n",
    "  - action: Allow\n",
    "    protocol: TCP\n",
    "    source:\n",
    "      selector: app == 'frontend'\n",
    "      namespaceSelector: project == 'production'\n",
    "    destination:\n",
    "      ports:\n",
    "      - 8080\n",
    "  - action: Deny\n",
    "    protocol: TCP\n",
    "    source:\n",
    "      notSelector: trusted == 'true'\n",
    "```\n",
    "\n",
    "**Cilium (eBPF-Based):**\n",
    "Cilium provides Layer 7 (HTTP) policies and identity-based security:\n",
    "\n",
    "```yaml\n",
    "apiVersion: cilium.io/v2\n",
    "kind: CiliumNetworkPolicy\n",
    "metadata:\n",
    "  name: l7-policy\n",
    "spec:\n",
    "  endpointSelector:\n",
    "    matchLabels:\n",
    "      app: backend\n",
    "  ingress:\n",
    "  - fromEndpoints:\n",
    "    - matchLabels:\n",
    "        app: frontend\n",
    "    toPorts:\n",
    "    - ports:\n",
    "      - port: \"80\"\n",
    "        protocol: TCP\n",
    "      rules:\n",
    "        http:\n",
    "        - method: \"GET\"\n",
    "          path: \"/api/v1/.*\"\n",
    "        - method: \"POST\"\n",
    "          path: \"/api/v1/data\"\n",
    "```\n",
    "\n",
    "## 15.5 CNI Plugins\n",
    "\n",
    "The Container Network Interface (CNI) is a specification and libraries for configuring network interfaces in Linux containers. Kubernetes delegates Pod networking to CNI plugins, which handle IP allocation, connectivity, and policy enforcement.\n",
    "\n",
    "### Major CNI Plugins\n",
    "\n",
    "**Calico (Most Feature-Rich):**\n",
    "- **Routing**: BGP (Border Gateway Protocol) for direct routing, or IPIP/VXLAN overlays\n",
    "- **Policy**: Comprehensive NetworkPolicy support with global policies\n",
    "- **IPAM**: Flexible IP pool management\n",
    "- **eBPF**: Optional eBPF data plane for performance\n",
    "- **Use Case**: Enterprise environments requiring fine-grained security and on-premises networking\n",
    "\n",
    "**Cilium (eBPF-Native):**\n",
    "- **Technology**: Uses Linux eBPF for kernel-level packet processing\n",
    "- **Observability**: Hubble for network flow visibility\n",
    "- **Security**: Identity-based security (not just IP-based), Layer 7 policies\n",
    "- **Service Mesh**: Can replace Istio sidecars with eBPF (better performance)\n",
    "- **Use Case**: Security-focused environments, high-performance networking, cloud-native observability\n",
    "\n",
    "**Flannel (Simplest):**\n",
    "- **Overlay**: VXLAN or UDP overlay networks\n",
    "- **Simplicity**: Easy setup, minimal configuration\n",
    "- **Limitations**: No NetworkPolicy support (requires Canal: Flannel + Calico policy)\n",
    "- **Use Case**: Small clusters, development environments, basic connectivity needs\n",
    "\n",
    "**Weave Net:**\n",
    "- **Mesh**: Peer-to-peer mesh networking\n",
    "- **Encryption**: Optional encryption between nodes\n",
    "- **Discovery**: Automatic peer discovery\n",
    "- **Use Case**: Multi-cloud deployments, dynamic clusters\n",
    "\n",
    "**Antrea (VMware):**\n",
    "- **Integration**: Native NSX-T integration for VMware environments\n",
    "- **eBPF**: Optional eBPF acceleration\n",
    "- **Windows**: Strong Windows node support\n",
    "- **Use Case**: Hybrid VMware/Kubernetes environments\n",
    "\n",
    "### CNI Selection Criteria\n",
    "\n",
    "| Feature | Calico | Cilium | Flannel | Weave | Antrea |\n",
    "|---------|--------|--------|---------|-------|--------|\n",
    "| **Network Policies** | Yes | Yes (L7) | No | Yes | Yes |\n",
    "| **Overlay/Underlay** | Both | Both | Overlay | Mesh | Both |\n",
    "| **Encryption** | WireGuard/IPsec | WireGuard/IPsec | No | Yes | IPsec |\n",
    "| **eBPF Support** | Optional | Native | No | No | Optional |\n",
    "| **Windows Support** | Limited | Limited | No | Yes | Yes |\n",
    "| **Service Mesh** | No | Yes (partial) | No | No | No |\n",
    "| **Complexity** | Medium | High | Low | Low | Medium |\n",
    "\n",
    "### CNI Configuration\n",
    "\n",
    "**Calico Installation:**\n",
    "```bash\n",
    "kubectl apply -f https://raw.githubusercontent.com/projectcalico/calico/v3.26.0/manifests/calico.yaml\n",
    "\n",
    "# Verify\n",
    "kubectl get pods -n kube-system -l k8s-app=calico-node\n",
    "```\n",
    "\n",
    "**Cilium Installation:**\n",
    "```bash\n",
    "helm repo add cilium https://helm.cilium.io/\n",
    "helm install cilium cilium/cilium --namespace kube-system \\\n",
    "  --set hubble.enabled=true \\\n",
    "  --set hubble.relay.enabled=true \\\n",
    "  --set hubble.ui.enabled=true\n",
    "```\n",
    "\n",
    "**Flannel Installation:**\n",
    "```bash\n",
    "kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml\n",
    "```\n",
    "\n",
    "## 15.6 Service Mesh Introduction\n",
    "\n",
    "While Kubernetes provides basic service-to-service communication through Services, Service Meshes add advanced traffic management, security, and observability without modifying application code.\n",
    "\n",
    "**Service Mesh Architecture:**\n",
    "- **Data Plane**: Sidecar proxies (Envoy, Linkerd-proxy) injected into application Pods\n",
    "- **Control Plane**: Management plane configuring proxies and collecting telemetry\n",
    "- **Sidecar Pattern**: Proxy intercepts all network traffic to/from the main application container\n",
    "\n",
    "**Key Capabilities:**\n",
    "- **mTLS**: Automatic mutual TLS encryption between services\n",
    "- **Traffic Splitting**: Canary deployments, A/B testing, blue/green deployments\n",
    "- **Retries/Timeouts**: Configurable resilience patterns\n",
    "- **Observability**: Distributed tracing, metrics, service graphs\n",
    "- **Authorization**: Service-to-service access control (L7 policies)\n",
    "\n",
    "### Istio (Most Feature-Rich)\n",
    "\n",
    "**Architecture:**\n",
    "- **istiod**: Combined control plane (Pilot, Citadel, Galley)\n",
    "- **Envoy Sidecars**: Data plane proxies injected via webhook\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "istioctl install --set profile=default -y\n",
    "kubectl label namespace default istio-injection=enabled\n",
    "```\n",
    "\n",
    "**Traffic Management Example:**\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: reviews-route\n",
    "spec:\n",
    "  hosts:\n",
    "  - reviews\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        end-user:\n",
    "          exact: jason\n",
    "    route:\n",
    "    - destination:\n",
    "        host: reviews\n",
    "        subset: v2\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: reviews\n",
    "        subset: v1\n",
    "      weight: 90\n",
    "    - destination:\n",
    "        host: reviews\n",
    "        subset: v2\n",
    "      weight: 10\n",
    "```\n",
    "\n",
    "### Linkerd (Lightweight Alternative)\n",
    "\n",
    "- **Resource Usage**: Significantly lighter than Istio\n",
    "- **Simplicity**: Easier to operate, opinionated defaults\n",
    "- **Performance**: Rust-based proxy (linkerd2-proxy) optimized for latency\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "linkerd install | kubectl apply -f -\n",
    "linkerd viz install | kubectl apply -f -  # Dashboard\n",
    "linkerd inject deployment.yml | kubectl apply -f -\n",
    "```\n",
    "\n",
    "### When to Use Service Mesh\n",
    "\n",
    "**Use Service Mesh when:**\n",
    "- You require mTLS between services (zero-trust networking)\n",
    "- You need sophisticated traffic management (canary, circuit breaking)\n",
    "- You lack application-level observability and need distributed tracing\n",
    "- You run hundreds of microservices requiring unified policy enforcement\n",
    "\n",
    "**Avoid Service Mesh when:**\n",
    "- Cluster has < 20 services (overhead exceeds benefit)\n",
    "- Network latency is critical (sidecars add ~3-5ms)\n",
    "- Team lacks operational capacity to manage complex control planes\n",
    "\n",
    "## 15.7 External DNS\n",
    "\n",
    "ExternalDNS automates the creation of DNS records in external DNS providers (Route53, Cloudflare, Google Cloud DNS) based on Kubernetes Ingress and Service resources.\n",
    "\n",
    "**Architecture:**\n",
    "- Watches Kubernetes API for Services/Ingresses with specific annotations\n",
    "- Creates/updates DNS records in configured provider\n",
    "- Removes records when Kubernetes resources are deleted\n",
    "\n",
    "**Installation (AWS Route53):**\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: external-dns\n",
    "  namespace: kube-system\n",
    "spec:\n",
    "  strategy:\n",
    "    type: Recreate\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: external-dns\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: external-dns\n",
    "    spec:\n",
    "      serviceAccountName: external-dns\n",
    "      containers:\n",
    "      - name: external-dns\n",
    "        image: registry.k8s.io/external-dns/external-dns:v0.14.0\n",
    "        args:\n",
    "        - --source=service\n",
    "        - --source=ingress\n",
    "        - --domain-filter=example.com\n",
    "        - --provider=aws\n",
    "        - --policy=upsert-only\n",
    "        - --aws-zone-type=public\n",
    "        - --registry=txt\n",
    "        - --txt-owner-id=kubernetes-cluster-1\n",
    "```\n",
    "\n",
    "**Usage with Ingress:**\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: example-ingress\n",
    "  annotations:\n",
    "    external-dns.alpha.kubernetes.io/hostname: app.example.com\n",
    "    external-dns.alpha.kubernetes.io/ttl: \"300\"\n",
    "spec:\n",
    "  rules:\n",
    "  - host: app.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        backend:\n",
    "          service:\n",
    "            name: my-service\n",
    "            port:\n",
    "              number: 80\n",
    "```\n",
    "\n",
    "## 15.8 TLS/SSL Management\n",
    "\n",
    "Encrypting traffic in transit is mandatory for production workloads. Kubernetes provides mechanisms for TLS termination at Ingress and Pod-to-Pod encryption via Service Mesh.\n",
    "\n",
    "### cert-manager\n",
    "\n",
    "cert-manager automates TLS certificate provisioning and renewal from Let's Encrypt, Vault, or private CAs.\n",
    "\n",
    "**Installation:**\n",
    "```bash\n",
    "helm repo add jetstack https://charts.jetstack.io\n",
    "helm repo update\n",
    "helm install cert-manager jetstack/cert-manager \\\n",
    "  --namespace cert-manager \\\n",
    "  --create-namespace \\\n",
    "  --set installCRDs=true\n",
    "```\n",
    "\n",
    "**Issuer Configuration (Let's Encrypt):**\n",
    "\n",
    "```yaml\n",
    "apiVersion: cert-manager.io/v1\n",
    "kind: ClusterIssuer\n",
    "metadata:\n",
    "  name: letsencrypt-prod\n",
    "spec:\n",
    "  acme:\n",
    "    server: https://acme-v02.api.letsencrypt.org/directory\n",
    "    email: admin@example.com\n",
    "    privateKeySecretRef:\n",
    "      name: letsencrypt-prod\n",
    "    solvers:\n",
    "    - http01:\n",
    "        ingress:\n",
    "          class: nginx\n",
    "      selector:\n",
    "        dnsZones:\n",
    "        - \"example.com\"\n",
    "```\n",
    "\n",
    "**Certificate Resource:**\n",
    "```yaml\n",
    "apiVersion: cert-manager.io/v1\n",
    "kind: Certificate\n",
    "metadata:\n",
    "  name: example-com\n",
    "  namespace: production\n",
    "spec:\n",
    "  secretName: example-com-tls\n",
    "  issuerRef:\n",
    "    name: letsencrypt-prod\n",
    "    kind: ClusterIssuer\n",
    "  dnsNames:\n",
    "  - example.com\n",
    "  - www.example.com\n",
    "  renewBefore: 720h  # Renew 30 days before expiry\n",
    "  duration: 2160h    # 90 days (Let's Encrypt default)\n",
    "```\n",
    "\n",
    "**Ingress Integration (Automatic):**\n",
    "```yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: example-ingress\n",
    "  annotations:\n",
    "    cert-manager.io/cluster-issuer: \"letsencrypt-prod\"\n",
    "spec:\n",
    "  tls:\n",
    "  - hosts:\n",
    "    - example.com\n",
    "    secretName: example-com-tls  # Auto-created by cert-manager\n",
    "  rules:\n",
    "  - host: example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: my-service\n",
    "            port:\n",
    "              number: 80\n",
    "```\n",
    "\n",
    "### TLS at Pod Level (Service Mesh)\n",
    "\n",
    "For end-to-end encryption terminating at the Pod rather than Ingress:\n",
    "\n",
    "```yaml\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: PeerAuthentication\n",
    "metadata:\n",
    "  name: default\n",
    "  namespace: production\n",
    "spec:\n",
    "  mtls:\n",
    "    mode: STRICT  # Require mTLS for all services in namespace\n",
    "```\n",
    "\n",
    "### Certificate Rotation\n",
    "\n",
    "**Manual Rotation:**\n",
    "```bash\n",
    "# Delete secret to trigger re-issuance\n",
    "kubectl delete secret example-com-tls -n production\n",
    "# cert-manager will recreate immediately\n",
    "```\n",
    "\n",
    "**Monitoring:**\n",
    "```bash\n",
    "# Check certificate status\n",
    "kubectl describe certificate example-com -n production\n",
    "\n",
    "# Check certificate expiry\n",
    "kubectl get certificate -n production -o jsonpath='{range .items[*]}{.metadata.name}{\"\\t\"}{.status.notAfter}{\"\\n\"}{end}'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "In this chapter, we explored the Kubernetes networking stack from fundamental Pod connectivity through sophisticated traffic management. We established that Kubernetes mandates a flat network model where every Pod receives a unique routable IP without NAT, implemented through CNI plugins such as Calico (BGP/policy-rich), Cilium (eBPF/observability), or Flannel (simplicity). CoreDNS provides cluster-internal service discovery via DNS records for Services, while Ingress Controllers (NGINX, Traefik) extend this to external HTTP/HTTPS routing with SSL termination, path-based routing, and canary capabilities. NetworkPolicies enforce micro-segmentation at Layer 3/4, critical for zero-trust security postures. We introduced Service Mesh concepts (Istio, Linkerd) for advanced traffic management and mTLS, ExternalDNS for automated public DNS integration, and cert-manager for automated TLS certificate lifecycle management.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Choose CNI plugins based on requirements: Flannel for simplicity, Calico for policy enforcement, Cilium for observability and Layer 7 policies.\n",
    "- Always implement default-deny NetworkPolicies and explicitly allow required traffic; unspecified policies allow all traffic.\n",
    "- Use Ingress rather than individual LoadBalancer Services to minimize cloud costs and enable sophisticated routing.\n",
    "- Implement cert-manager for automated Let's Encrypt integration; avoid manual certificate management.\n",
    "- Service Meshes provide powerful capabilities but add complexity and latency; adopt only when microservices count justifies overhead.\n",
    "\n",
    "**Next Chapter Preview:**\n",
    "Chapter 16: Storage and Volumes addresses stateful workloads, exploring PersistentVolumes (PV) and PersistentVolumeClaims (PVC) for durable storage, StorageClasses for dynamic provisioning, StatefulSets for ordered deployment with stable network identity, and database persistence strategies. You will learn to distinguish ephemeral storage (emptyDir) from persistent storage, implement backup strategies for stateful applications, and manage storage across node failures\u2014essential capabilities for running databases, message queues, and file stores in Kubernetes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='14. kubernetes_core_resources.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='16. storage_and_volumes.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}