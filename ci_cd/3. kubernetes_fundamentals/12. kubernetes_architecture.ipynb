{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 12: Kubernetes Architecture\n",
    "\n",
    "Having established secure artifact distribution through container registries, we now transition from single-host container management to distributed orchestration. Kubernetes (often abbreviated as K8s) is an open-source container orchestration platform that automates the deployment, scaling, and management of containerized applications. Originally designed by Google and now maintained by the Cloud Native Computing Foundation (CNCF), Kubernetes has become the de facto standard for container orchestration, supported by every major cloud provider and adopted by enterprises worldwide.\n",
    "\n",
    "This chapter examines the architectural foundations of Kubernetes, exploring how the control plane maintains cluster state, how worker nodes execute workloads, and how these components communicate to form a resilient, scalable platform. Understanding these internals is essential for troubleshooting, security hardening, and optimizing production deployments.\n",
    "\n",
    "## 12.1 Control Plane Components\n",
    "\n",
    "The Kubernetes Control Plane functions as the brain of the cluster, making global decisions about the cluster state (such as scheduling workloads) and detecting and responding to cluster events (such as scaling when a deployment's replica count changes). Control plane components can run on any machine in the cluster, but for high availability and security, they typically reside on dedicated master nodes isolated from application workloads.\n",
    "\n",
    "### Architectural Overview\n",
    "\n",
    "A Kubernetes cluster consists of two primary segments:\n",
    "- **Control Plane**: Manages cluster state and configuration\n",
    "- **Data Plane (Worker Nodes)**: Executes containerized applications\n",
    "\n",
    "The control plane exposes the Kubernetes API, which serves as the central nervous system through which all components\u2014internal, user-facing, and automated systems\u2014interact with the cluster.\n",
    "\n",
    "### Core Control Plane Components\n",
    "\n",
    "The control plane comprises several distinct processes, each with specific responsibilities:\n",
    "\n",
    "**1. kube-apiserver**\n",
    "The API server is the front end of the Kubernetes control plane. It exposes the Kubernetes REST API, handling all requests from users, automation, and internal components. It is the only component that communicates directly with the etcd datastore.\n",
    "\n",
    "**2. etcd**\n",
    "A consistent and highly-available distributed key-value store that serves as Kubernetes' backing store for all cluster data. It maintains the entire cluster state, including configuration data, operational metadata, and service discovery information.\n",
    "\n",
    "**3. kube-scheduler**\n",
    "Watches for newly created Pods with no assigned node and selects an optimal node for them to run on based on resource requirements, hardware constraints, affinity specifications, and other factors.\n",
    "\n",
    "**4. kube-controller-manager**\n",
    "Runs controller processes that regulate the state of the cluster. Controllers watch the shared state of the cluster through the API server and make changes attempting to move the current state toward the desired state.\n",
    "\n",
    "**5. cloud-controller-manager** (optional)\n",
    "Embeds cloud-specific control logic, allowing Kubernetes to interact with underlying cloud providers for load balancing, node management, and storage provisioning while maintaining vendor neutrality in core components.\n",
    "\n",
    "### Control Plane Communication Patterns\n",
    "\n",
    "All components communicate exclusively through the API server; they do not talk directly to each other. This hub-and-spoke model provides:\n",
    "- **Auditability**: All state changes flow through a single chokepoint\n",
    "- **Authorization**: Centralized access control enforcement\n",
    "- **Validation**: Schema and policy verification at entry point\n",
    "\n",
    "```yaml\n",
    "# Example: Viewing control plane pods in a kubeadm cluster\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: kube-apiserver-master-1\n",
    "  namespace: kube-system\n",
    "spec:\n",
    "  containers:\n",
    "  - name: kube-apiserver\n",
    "    image: registry.k8s.io/kube-apiserver:v1.28.0\n",
    "    command:\n",
    "      - kube-apiserver\n",
    "      - --advertise-address=192.168.1.10\n",
    "      - --allow-privileged=true\n",
    "      - --authorization-mode=Node,RBAC\n",
    "      - --client-ca-file=/etc/kubernetes/pki/ca.crt\n",
    "      - --enable-admission-plugins=NodeRestriction\n",
    "      - --etcd-servers=https://127.0.0.1:2379\n",
    "      - --etcd-cafile=/etc/kubernetes/pki/etcd/ca.crt\n",
    "      - --etcd-certfile=/etc/kubernetes/pki/apiserver-etcd-client.crt\n",
    "      - --etcd-keyfile=/etc/kubernetes/pki/apiserver-etcd-client.key\n",
    "      - --kubelet-client-certificate=/etc/kubernetes/pki/apiserver-kubelet-client.crt\n",
    "      - --kubelet-client-key=/etc/kubernetes/pki/apiserver-kubelet-client.key\n",
    "      - --kubelet-preferred-address-types=InternalIP,ExternalIP,Hostname\n",
    "      - --proxy-client-cert-file=/etc/kubernetes/pki/front-proxy-client.crt\n",
    "      - --proxy-client-key-file=/etc/kubernetes/pki/front-proxy-client.key\n",
    "      - --requestheader-allowed-names=front-proxy-client\n",
    "      - --requestheader-client-ca-file=/etc/kubernetes/pki/front-proxy-ca.crt\n",
    "      - --requestheader-extra-headers-prefix=X-Remote-Extra-\n",
    "      - --requestheader-group-headers=X-Remote-Group\n",
    "      - --requestheader-username-headers=X-Remote-User\n",
    "      - --secure-port=6443\n",
    "      - --service-account-issuer=https://kubernetes.default.svc.cluster.local\n",
    "      - --service-account-key-file=/etc/kubernetes/pki/sa.pub\n",
    "      - --service-account-signing-key-file=/etc/kubernetes/pki/sa.key\n",
    "      - --service-cluster-ip-range=10.96.0.0/12\n",
    "      - --tls-cert-file=/etc/kubernetes/pki/apiserver.crt\n",
    "      - --tls-private-key-file=/etc/kubernetes/pki/apiserver.key\n",
    "```\n",
    "\n",
    "## 12.2 Worker Nodes\n",
    "\n",
    "While the control plane manages cluster state, worker nodes (also called minions or simply nodes) provide the compute capacity where containers actually run. Each worker node contains the necessary services to run Pods (the smallest deployable units in Kubernetes) and is managed by the control plane.\n",
    "\n",
    "### Node Components\n",
    "\n",
    "Every worker node runs three essential processes:\n",
    "\n",
    "**1. kubelet**\n",
    "An agent that runs on each node in the cluster. It ensures that containers are running in a Pod. The kubelet takes a set of PodSpecs provided through various mechanisms (primarily the API server) and ensures that the containers described in those PodSpecs are running and healthy.\n",
    "\n",
    "**2. kube-proxy**\n",
    "A network proxy that maintains network rules on nodes, enabling the Kubernetes Service abstraction. It handles network communications inside or outside the cluster, performing connection forwarding or load balancing across Pods.\n",
    "\n",
    "**3. Container Runtime**\n",
    "The software responsible for running containers. Kubernetes supports multiple container runtimes through the Container Runtime Interface (CRI):\n",
    "- **containerd**: The industry-standard runtime (default in most modern distributions)\n",
    "- **CRI-O**: Lightweight alternative optimized for Kubernetes\n",
    "- **Docker Engine**: Supported via cri-dockerd adapter (deprecated as direct runtime)\n",
    "\n",
    "### Node Architecture Deep Dive\n",
    "\n",
    "Worker nodes require specific configuration for optimal operation:\n",
    "\n",
    "**Systemd Services:**\n",
    "Modern Kubernetes distributions use systemd to manage node components:\n",
    "\n",
    "```ini\n",
    "# /etc/systemd/system/kubelet.service\n",
    "[Unit]\n",
    "Description=Kubernetes Kubelet\n",
    "Documentation=https://kubernetes.io/docs/reference/command-line-tools-reference/kubelet/\n",
    "After=containerd.service\n",
    "Requires=containerd.service\n",
    "\n",
    "[Service]\n",
    "ExecStart=/usr/local/bin/kubelet \\\n",
    "  --bootstrap-kubeconfig=/etc/kubernetes/bootstrap-kubelet.conf \\\n",
    "  --kubeconfig=/etc/kubernetes/kubelet.conf \\\n",
    "  --config=/var/lib/kubelet/config.yaml \\\n",
    "  --container-runtime-endpoint=unix:///run/containerd/containerd.sock \\\n",
    "  --pod-infra-container-image=registry.k8s.io/pause:3.9\n",
    "Restart=always\n",
    "RestartSec=5\n",
    "\n",
    "[Install]\n",
    "WantedBy=multi-user.target\n",
    "```\n",
    "\n",
    "**Node Resource Allocation:**\n",
    "Nodes reserve resources for system daemons and Kubernetes components:\n",
    "\n",
    "```yaml\n",
    "# /var/lib/kubelet/config.yaml\n",
    "apiVersion: kubelet.config.k8s.io/v1beta1\n",
    "kind: KubeletConfiguration\n",
    "systemReserved:\n",
    "  cpu: \"500m\"\n",
    "  memory: \"512Mi\"\n",
    "  ephemeral-storage: \"1Gi\"\n",
    "kubeReserved:\n",
    "  cpu: \"500m\"\n",
    "  memory: \"1Gi\"\n",
    "  ephemeral-storage: \"1Gi\"\n",
    "evictionHard:\n",
    "  memory.available: \"500Mi\"\n",
    "  nodefs.available: \"10%\"\n",
    "evictionSoft:\n",
    "  memory.available: \"1Gi\"\n",
    "  nodefs.available: \"15%\"\n",
    "evictionSoftGracePeriod:\n",
    "  memory.available: \"1m\"\n",
    "  nodefs.available: \"1m\"\n",
    "```\n",
    "\n",
    "## 12.3 Kubernetes Objects Overview\n",
    "\n",
    "Kubernetes objects are persistent entities in the Kubernetes system that represent the state of your cluster. They describe what containerized applications are running, the resources available to them, and policies around their behavior (such as restart policies, upgrades, and fault-tolerance).\n",
    "\n",
    "### Object Specification Pattern\n",
    "\n",
    "Every Kubernetes object follows a consistent structure:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1          # Version of the Kubernetes API\n",
    "kind: Pod               # Type of object\n",
    "metadata:               # Identifying information\n",
    "  name: example-pod\n",
    "  namespace: default\n",
    "  labels:\n",
    "    app: web\n",
    "    tier: frontend\n",
    "spec:                   # Desired state description\n",
    "  containers:\n",
    "  - name: nginx\n",
    "    image: nginx:1.25\n",
    "    ports:\n",
    "    - containerPort: 80\n",
    "status:                 # Current state (populated by system)\n",
    "  phase: Running\n",
    "  conditions:\n",
    "  - type: Ready\n",
    "    status: \"True\"\n",
    "```\n",
    "\n",
    "### Fundamental Object Categories\n",
    "\n",
    "Kubernetes objects organize into logical groups:\n",
    "\n",
    "**Workload Objects**: Manage container deployment and execution\n",
    "- Pods, Deployments, StatefulSets, DaemonSets, Jobs, CronJobs\n",
    "\n",
    "**Service Objects**: Enable network access and discovery\n",
    "- Services, Ingress, NetworkPolicies\n",
    "\n",
    "**Config/Storage Objects**: Manage configuration and persistence\n",
    "- ConfigMaps, Secrets, PersistentVolumes, PersistentVolumeClaims, StorageClasses\n",
    "\n",
    "**Cluster Objects**: Define cluster-wide behavior\n",
    "- Nodes, Namespaces, ResourceQuotas, LimitRanges, ServiceAccounts, Roles, RoleBindings\n",
    "\n",
    "### Declarative Management Philosophy\n",
    "\n",
    "Kubernetes operates on a declarative model: you specify the desired state, and the control plane works continuously to maintain that state. This differs from imperative systems where you issue specific commands to create or modify resources.\n",
    "\n",
    "**Imperative approach (avoid in production):**\n",
    "```bash\n",
    "kubectl run nginx --image=nginx\n",
    "kubectl scale deployment nginx --replicas=3\n",
    "kubectl expose deployment nginx --port=80\n",
    "```\n",
    "\n",
    "**Declarative approach (recommended):**\n",
    "```bash\n",
    "# Apply configuration file describing desired state\n",
    "kubectl apply -f nginx-deployment.yaml\n",
    "\n",
    "# Version control your infrastructure\n",
    "git add nginx-deployment.yaml\n",
    "git commit -m \"Add nginx deployment with 3 replicas\"\n",
    "git push\n",
    "```\n",
    "\n",
    "The declarative model enables GitOps workflows, where the git repository serves as the single source of truth for infrastructure state.\n",
    "\n",
    "## 12.4 API Server and etcd\n",
    "\n",
    "The API server and etcd form the heart of the Kubernetes control plane, handling all state persistence and serving as the communication hub for the entire cluster.\n",
    "\n",
    "### API Server Deep Dive\n",
    "\n",
    "The Kubernetes API server (kube-apiserver) validates and configures data for API objects, serving as the front end to the cluster's shared state. It follows RESTful principles and uses JSON or Protocol Buffers for serialization.\n",
    "\n",
    "**Request Flow:**\n",
    "1. **Authentication**: Verifies identity (X.509 certificates, Bearer tokens, OIDC, webhook)\n",
    "2. **Authorization**: Determines permissions (RBAC, ABAC, Node authorization, Webhook)\n",
    "3. **Admission Control**: Modifies or validates requests (mutating/validating webhooks, built-in plugins)\n",
    "4. **ETCD Interaction**: Reads/writes state to backing store\n",
    "5. **Response**: Returns result to client\n",
    "\n",
    "**API Groups and Versions:**\n",
    "Kubernetes APIs organize into groups and versions to enable evolution:\n",
    "\n",
    "```\n",
    "/apis/apps/v1/deployments       # Stable apps API\n",
    "/apis/apps/v1beta1/deployments  # Deprecated beta version\n",
    "/api/v1/pods                    # Core API (legacy group)\n",
    "/apis/networking.k8s.io/v1/ingresses  # Networking API\n",
    "```\n",
    "\n",
    "**Aggregation Layer:**\n",
    "The API server supports extension through API Aggregation, allowing custom APIs to register alongside native ones:\n",
    "\n",
    "```yaml\n",
    "apiVersion: apiregistration.k8s.io/v1\n",
    "kind: APIService\n",
    "metadata:\n",
    "  name: v1beta1.metrics.k8s.io\n",
    "spec:\n",
    "  service:\n",
    "    name: metrics-server\n",
    "    namespace: kube-system\n",
    "  group: metrics.k8s.io\n",
    "  version: v1beta1\n",
    "  insecureSkipTLSVerify: true\n",
    "  groupPriorityMinimum: 100\n",
    "  versionPriority: 100\n",
    "```\n",
    "\n",
    "### etcd: The Cluster Brain\n",
    "\n",
    "etcd is a distributed, reliable key-value store written in Go that uses the Raft consensus algorithm to manage a highly-available replicated log. It is the ultimate source of truth for Kubernetes cluster state.\n",
    "\n",
    "**Data Organization:**\n",
    "etcd stores all Kubernetes data under the `/registry` prefix:\n",
    "\n",
    "```\n",
    "/registry/pods/default/nginx-7d8bc9c5b8-abc12\n",
    "/registry/deployments/default/nginx-deployment\n",
    "/registry/secrets/default/app-token\n",
    "/registry/nodes/worker-node-1\n",
    "```\n",
    "\n",
    "**Consensus and Replication:**\n",
    "etcd operates typically as a cluster of 3, 5, or 7 members (odd numbers prevent split-brain scenarios). The Raft algorithm ensures:\n",
    "- **Leader Election**: One node acts as leader handling all writes\n",
    "- **Log Replication**: Leader replicates entries to followers\n",
    "- **Safety**: Committed entries persist even if minority of nodes fail\n",
    "\n",
    "**Performance Characteristics:**\n",
    "- Optimized for read-heavy workloads (Kubernetes is read-heavy)\n",
    "- Writes require consensus (latency increases with distance between nodes)\n",
    "- Recommended storage: SSD/NVMe for write-ahead logs\n",
    "- Defragmentation required periodically to reclaim space\n",
    "\n",
    "**Backing Up etcd:**\n",
    "Regular backups are critical for disaster recovery:\n",
    "\n",
    "```bash\n",
    "# Snapshot etcd (run on master node or via pod)\n",
    "ETCDCTL_API=3 etcdctl snapshot save /backup/etcd-snapshot-$(date +%Y%m%d).db \\\n",
    "  --endpoints=https://127.0.0.1:2379 \\\n",
    "  --cacert=/etc/kubernetes/pki/etcd/ca.crt \\\n",
    "  --cert=/etc/kubernetes/pki/etcd/server.crt \\\n",
    "  --key=/etc/kubernetes/pki/etcd/server.key\n",
    "\n",
    "# Verify snapshot\n",
    "ETCDCTL_API=3 etcdctl snapshot status /backup/etcd-snapshot-20231102.db\n",
    "\n",
    "# Restore (disaster recovery)\n",
    "ETCDCTL_API=3 etcdctl snapshot restore snapshot.db \\\n",
    "  --data-dir=/var/lib/etcd-new \\\n",
    "  --initial-cluster-token=etcd-cluster-1 \\\n",
    "  --initial-cluster=master-1=https://192.168.1.10:2380 \\\n",
    "  --initial-advertise-peer-urls=https://192.168.1.10:2380\n",
    "```\n",
    "\n",
    "**Security Considerations:**\n",
    "- Enable TLS encryption for peer and client communication\n",
    "- Use dedicated CA for etcd PKI separate from Kubernetes CA\n",
    "- Enable authentication (username/password or certificates)\n",
    "- Enable audit logging for compliance requirements\n",
    "\n",
    "## 12.5 Scheduler and Controller Manager\n",
    "\n",
    "While the API server and etcd store state, the scheduler and controller manager actively work to reconcile desired state with actual cluster conditions.\n",
    "\n",
    "### kube-scheduler\n",
    "\n",
    "The scheduler watches for newly created Pods with no assigned node (`nodeName` field empty) and selects the best node for them based on scheduling principles.\n",
    "\n",
    "**Scheduling Algorithm:**\n",
    "1. **Filtering (Predicates)**: Remove nodes that cannot run the Pod\n",
    "   - Resource requirements (CPU, memory, GPU)\n",
    "   - Node selectors/affinity rules\n",
    "   - Taints and tolerations\n",
    "   - Volume availability\n",
    "   - Inter-pod affinity/anti-affinity\n",
    "\n",
    "2. **Scoring (Priorities)**: Rank remaining nodes by suitability\n",
    "   - Least requested resources (spread load)\n",
    "   - Node affinity preferences\n",
    "   - Pod topology spread\n",
    "\n",
    "3. **Binding**: Assign Pod to highest-scoring node\n",
    "\n",
    "**Scheduling Constraints Example:**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "spec:\n",
    "  affinity:\n",
    "    nodeAffinity:\n",
    "      requiredDuringSchedulingIgnoredDuringExecution:\n",
    "        nodeSelectorTerms:\n",
    "        - matchExpressions:\n",
    "          - key: kubernetes.io/arch\n",
    "            operator: In\n",
    "            values: [\"amd64\"]\n",
    "    podAntiAffinity:\n",
    "      preferredDuringSchedulingIgnoredDuringExecution:\n",
    "      - weight: 100\n",
    "        podAffinityTerm:\n",
    "          labelSelector:\n",
    "            matchExpressions:\n",
    "            - key: app\n",
    "              operator: In\n",
    "              values: [\"web\"]\n",
    "          topologyKey: kubernetes.io/hostname\n",
    "  tolerations:\n",
    "  - key: \"dedicated\"\n",
    "    operator: \"Equal\"\n",
    "    value: \"web\"\n",
    "    effect: \"NoSchedule\"\n",
    "```\n",
    "\n",
    "**Custom Scheduling:**\n",
    "For advanced use cases, you can deploy multiple schedulers:\n",
    "- Default scheduler for general workloads\n",
    "- GPU scheduler for ML training jobs\n",
    "- Bin-packing scheduler for cost optimization\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "spec:\n",
    "  schedulerName: gpu-scheduler  # Use custom scheduler\n",
    "  containers:\n",
    "  - name: training\n",
    "    image: tensorflow/tensorflow:latest-gpu\n",
    "    resources:\n",
    "      limits:\n",
    "        nvidia.com/gpu: 1\n",
    "```\n",
    "\n",
    "### kube-controller-manager\n",
    "\n",
    "Controllers are control loops that watch the shared state of the cluster through the API server and make changes attempting to move the current state toward the desired state.\n",
    "\n",
    "**Built-in Controllers:**\n",
    "\n",
    "| Controller | Responsibility |\n",
    "|------------|----------------|\n",
    "| **Node Controller** | Monitors node health, evicts Pods from unreachable nodes after timeout (default 5m) |\n",
    "| **Replication Controller** | Maintains correct Pod count for ReplicaSets |\n",
    "| **Endpoints Controller** | Populates Endpoints objects (joins Services & Pods) |\n",
    "| **Service Account & Token Controllers** | Create default accounts and API access tokens for new namespaces |\n",
    "| **Namespace Controller** | Handles namespace deletion, cleaning up resources |\n",
    "| **Deployment Controller** | Manages rolling updates and rollbacks |\n",
    "| **StatefulSet Controller** | Manages ordered deployment and persistence |\n",
    "| **Job Controller** | Watches for Job objects and creates Pods to complete work |\n",
    "\n",
    "**Controller Pattern Implementation:**\n",
    "All controllers follow the same fundamental pattern:\n",
    "1. Read resource state via API server watch\n",
    "2. Compare desired state with observed state\n",
    "3. Execute actions to converge states\n",
    "4. Repeat indefinitely (level-triggered, not edge-triggered)\n",
    "\n",
    "**Cloud Controller Manager:**\n",
    "Separates cloud-specific logic from core Kubernetes:\n",
    "- **Node Controller**: Cloud instance verification\n",
    "- **Route Controller**: Cloud route table configuration\n",
    "- **Service Controller**: Cloud load balancer provisioning\n",
    "\n",
    "## 12.6 Kubelet and Kube-proxy\n",
    "\n",
    "These node agents translate control plane directives into container runtime operations and network configurations.\n",
    "\n",
    "### Kubelet: The Node Agent\n",
    "\n",
    "The kubelet is the primary \"node agent\" that runs on each node. It registers the node with the API server using one of:\n",
    "- Hostname (default)\n",
    "- Override flag (`--hostname-override`)\n",
    "- Cloud provider-specified name\n",
    "\n",
    "**Pod Lifecycle Management:**\n",
    "The kubelet receives PodSpecs primarily from:\n",
    "1. API server (primary source)\n",
    "2. Local files (static Pods)\n",
    "3. HTTP endpoints (less common)\n",
    "\n",
    "It ensures containers described in PodSpecs are running and healthy via:\n",
    "- **Container Runtime Interface (CRI)**: Calls to containerd/CRI-O to create/start/stop containers\n",
    "- **Readiness/Liveness Probes**: HTTP/TCP/exec checks to determine container health\n",
    "- **Resource Monitoring**: cAdvisor integration for container metrics\n",
    "\n",
    "**Static Pods:**\n",
    "Managed directly by kubelet without API server oversight (used for control plane bootstrapping):\n",
    "\n",
    "```yaml\n",
    "# /etc/kubernetes/manifests/etcd.yaml on master node\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: etcd\n",
    "  namespace: kube-system\n",
    "spec:\n",
    "  containers:\n",
    "  - name: etcd\n",
    "    image: registry.k8s.io/etcd:3.5.9-0\n",
    "    command:\n",
    "    - etcd\n",
    "    - --advertise-client-urls=https://192.168.1.10:2379\n",
    "    - --cert-file=/etc/kubernetes/pki/etcd/server.crt\n",
    "    # ... additional args\n",
    "  volumes:\n",
    "  - name: etcd-certs\n",
    "    hostPath:\n",
    "      path: /etc/kubernetes/pki/etcd\n",
    "      type: Directory\n",
    "```\n",
    "\n",
    "**Node Status Conditions:**\n",
    "Kubelet reports node status including:\n",
    "- **Ready**: Node healthy and accepting Pods\n",
    "- **DiskPressure**: Available disk space low\n",
    "- **MemoryPressure**: Available memory low\n",
    "- **PIDPressure**: Too many processes\n",
    "- **NetworkUnavailable**: Network not configured correctly\n",
    "\n",
    "### Kube-proxy: Network Proxy\n",
    "\n",
    "Kube-proxy maintains network rules on nodes, enabling the Kubernetes Service abstraction. It implements Services using:\n",
    "- **iptables** (default, mature)\n",
    "- **IPVS** (for large clusters, better performance)\n",
    "- **Userspace** (legacy, slow)\n",
    "\n",
    "**Service Implementation:**\n",
    "When a Service is created, kube-proxy on every node updates local networking:\n",
    "\n",
    "```bash\n",
    "# View iptables rules created by kube-proxy\n",
    "iptables -t nat -L KUBE-SERVICES -n | grep http\n",
    "\n",
    "# Chain KUBE-SVC-XXXXXXXXX (1 references)\n",
    "# target     prot opt source               destination\n",
    "# KUBE-SEP-XXXXXXXXX  all  --  0.0.0.0/0            10.96.123.45         /* default/http: cluster IP */ tcp dpt:80\n",
    "```\n",
    "\n",
    "**Modes of Operation:**\n",
    "\n",
    "**iptables mode** (default):\n",
    "- Adds rules to iptables for each Service endpoint\n",
    "- O(n) complexity where n = number of Services\n",
    "- Suitable for clusters with < 1000 Services\n",
    "\n",
    "**IPVS mode** (recommended for large scale):\n",
    "- Uses Linux Virtual Server kernel module\n",
    "- O(1) complexity regardless of Service count\n",
    "- Supports multiple load balancing algorithms (rr, lc, dh, sh, sed, nq)\n",
    "\n",
    "```yaml\n",
    "# kube-proxy ConfigMap\n",
    "apiVersion: kubelet.config.k8s.io/v1alpha1\n",
    "kind: KubeProxyConfiguration\n",
    "mode: \"ipvs\"\n",
    "ipvs:\n",
    "  scheduler: \"rr\"  # Round-robin\n",
    "  minSyncPeriod: 0s\n",
    "  syncPeriod: 30s\n",
    "```\n",
    "\n",
    "**Kernel Space vs Userspace:**\n",
    "Modern kube-proxy operates in kernel space (iptables/IPVS) for performance, avoiding the overhead of copying packets through user space that characterized early Kubernetes implementations.\n",
    "\n",
    "## 12.7 Cluster Communication\n",
    "\n",
    "Understanding how cluster components communicate is essential for troubleshooting network issues and securing cluster traffic.\n",
    "\n",
    "### Communication Paths\n",
    "\n",
    "**1. Node to Control Plane:**\n",
    "- Kubelet \u2192 API Server (HTTPS, port 6443)\n",
    "- Uses certificates for authentication\n",
    "- Watch connections maintained for real-time updates\n",
    "\n",
    "**2. Control Plane to Node:**\n",
    "- API Server \u2192 Kubelet (HTTPS, port 10250)\n",
    "  - Used for `kubectl logs`, `kubectl exec`, `kubectl attach`\n",
    "  - Requires proper certificate authentication\n",
    "- API Server \u2192 Node, Pod, Service (via kube-proxy)\n",
    "\n",
    "**3. Intra-Pod Communication:**\n",
    "- Containers within a Pod share network namespace (localhost)\n",
    "- Communicate via `localhost:port`\n",
    "- Share IP address and port space\n",
    "\n",
    "**4. Pod to Pod Communication:**\n",
    "- All Pods can communicate without NAT\n",
    "- Each Pod has unique IP in cluster CIDR\n",
    "- Implemented by CNI plugins (Calico, Cilium, Flannel, Weave)\n",
    "\n",
    "**5. Pod to Service Communication:**\n",
    "- Virtual IPs (ClusterIPs) abstract Pod endpoints\n",
    "- Kube-proxy load balances across healthy Pods\n",
    "- DNS resolves Service names to ClusterIPs\n",
    "\n",
    "### Network Requirements\n",
    "\n",
    "Kubernetes imposes fundamental network requirements (the \"Kubernetes Network Model\"):\n",
    "\n",
    "1. **All containers can communicate with all other containers without NAT**\n",
    "2. **All nodes can communicate with all containers (and vice versa) without NAT**\n",
    "3. **The IP that a container sees itself as is the same IP that others see it as**\n",
    "\n",
    "**CIDR Ranges:**\n",
    "Distinct non-overlapping ranges required:\n",
    "- **Pod CIDR**: IP range assigned to Pods (e.g., 10.244.0.0/16)\n",
    "- **Service CIDR**: Virtual IP range for Services (e.g., 10.96.0.0/12)\n",
    "- **Node CIDR**: Physical node IP addresses\n",
    "\n",
    "### TLS and Certificate Architecture\n",
    "\n",
    "Kubernetes uses a PKI infrastructure for component authentication:\n",
    "\n",
    "**Certificate Types:**\n",
    "- **Client certificates**: For users (admin, developer) and components (kubelet, scheduler)\n",
    "- **Server certificates**: For API server, etcd, kubelet serving\n",
    "- **CA certificates**: Root of trust for cluster\n",
    "\n",
    "**Certificate Locations (standard kubeadm):**\n",
    "```\n",
    "/etc/kubernetes/pki/\n",
    "\u251c\u2500\u2500 ca.crt              # Cluster CA\n",
    "\u251c\u2500\u2500 ca.key              # Cluster CA key (keep secure/offline)\n",
    "\u251c\u2500\u2500 apiserver.crt       # API server serving cert\n",
    "\u251c\u2500\u2500 apiserver-kubelet-client.crt  # API server client to kubelet\n",
    "\u251c\u2500\u2500 etcd/\n",
    "\u2502   \u251c\u2500\u2500 ca.crt          # etcd CA (separate for isolation)\n",
    "\u2502   \u251c\u2500\u2500 server.crt      # etcd serving cert\n",
    "\u2502   \u2514\u2500\u2500 peer.crt        # etcd peer communication\n",
    "\u2514\u2500\u2500 front-proxy-ca.crt  # Aggregated API server CA\n",
    "```\n",
    "\n",
    "**Certificate Rotation:**\n",
    "Short-lived certificates improve security:\n",
    "- Kubeadm automatically rotates certificates (1 year default)\n",
    "- Can configure external issuers (Vault, cert-manager) for automated rotation\n",
    "\n",
    "## 12.8 High Availability Architecture\n",
    "\n",
    "Production Kubernetes clusters must survive control plane node failures without service interruption.\n",
    "\n",
    "### Control Plane HA Topologies\n",
    "\n",
    "**Stacked etcd Topology (simpler, less hardware):**\n",
    "- Control plane components and etcd share nodes\n",
    "- Minimum 3 master nodes for quorum\n",
    "- If node fails, loses both API server and etcd member\n",
    "\n",
    "**External etcd Topology (more complex, higher availability):**\n",
    "- etcd runs on separate nodes from API servers\n",
    "- API servers can be horizontally scaled independently\n",
    "- Survives loss of API server nodes without affecting etcd quorum\n",
    "\n",
    "### HA Implementation\n",
    "\n",
    "**Load Balancing API Servers:**\n",
    "External load balancer distributes traffic across API server replicas:\n",
    "\n",
    "```yaml\n",
    "# HAProxy configuration example\n",
    "global\n",
    "    daemon\n",
    "    maxconn 4096\n",
    "\n",
    "defaults\n",
    "    mode http\n",
    "    timeout connect 5s\n",
    "    timeout client 30s\n",
    "    timeout server 30s\n",
    "\n",
    "frontend kubernetes-apiserver\n",
    "    bind *:6443\n",
    "    default_backend kubernetes-apiserver\n",
    "\n",
    "backend kubernetes-apiserver\n",
    "    balance roundrobin\n",
    "    server master1 192.168.1.10:6443 check\n",
    "    server master2 192.168.1.11:6443 check\n",
    "    server master3 192.168.1.12:6443 check\n",
    "```\n",
    "\n",
    "**etcd Clustering:**\n",
    "Odd number of members (3, 5, 7) to maintain quorum:\n",
    "- 3 nodes: tolerate 1 failure\n",
    "- 5 nodes: tolerate 2 failures\n",
    "- 7 nodes: tolerate 3 failures\n",
    "\n",
    "Adding more than 7 etcd members typically reduces performance due to consensus overhead.\n",
    "\n",
    "**Leader Election:**\n",
    "Components like scheduler and controller-manager use leader election to ensure only one instance is active at a time, preventing conflicts:\n",
    "\n",
    "```bash\n",
    "# View leader election leases\n",
    "kubectl get leases -n kube-system\n",
    "# NAME                      HOLDER                                                                           AGE\n",
    "# kube-controller-manager   master-1_12345...                                                                24h\n",
    "# kube-scheduler            master-2_67890...                                                                24h\n",
    "```\n",
    "\n",
    "### Disaster Recovery Planning\n",
    "\n",
    "**Backup Strategy:**\n",
    "1. **etcd snapshots**: Cluster state (critical)\n",
    "2. **Persistent volume backups**: Application data\n",
    "3. **Configuration backups**: YAML manifests, certificates\n",
    "4. **Registry replication**: Container images\n",
    "\n",
    "**Recovery Procedures:**\n",
    "Document and test recovery of:\n",
    "- Single etcd member replacement\n",
    "- Complete etcd cluster restoration\n",
    "- API server certificate regeneration\n",
    "- Worker node replacement\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "In this chapter, we established the architectural foundations of Kubernetes, dissecting the control plane components (API server, etcd, scheduler, controller manager) that maintain cluster state, and the worker node components (kubelet, kube-proxy, container runtime) that execute workloads. We explored the declarative object model, the critical role of etcd as the distributed backing store using Raft consensus, and the network communication patterns binding the cluster together. High availability strategies ensure production resilience through stacked or external etcd topologies, load-balanced API servers, and automated leader election.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- The API server is the central hub; all components communicate through it, enabling centralized audit and authorization.\n",
    "- etcd requires odd-numbered clusters (3, 5, 7) for quorum and serves as the ultimate source of truth requiring regular backups.\n",
    "- Contators operate as reconciliation loops continuously moving observed state toward desired state.\n",
    "- Kubelet translates PodSpecs into container runtime operations while kube-proxy implements Service networking via iptables or IPVS.\n",
    "- Kubernetes mandates a flat network model where all Pods communicate without NAT, implemented by CNI plugins.\n",
    "- Production clusters require HA control planes with redundant API servers and etcd members behind load balancers.\n",
    "\n",
    "**Next Chapter Preview:**\n",
    "Chapter 13: Getting Started with Kubernetes transitions from architecture theory to practical deployment. You will install local Kubernetes clusters using Minikube, Kind, or k3d; configure kubectl command-line access; create your first Namespace; and deploy initial Pods. This hands-on chapter bridges the architectural understanding gained here with the operational skills needed to manage real workloads, preparing you for Chapter 14's deep dive into core Kubernetes resources like Deployments, Services, and ConfigMaps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../2. docker_fundamentals/11. docker_registries.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='13. getting_started_with_kubernetes.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}