{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 43: Logging in CI/CD\n",
    "\n",
    "In distributed systems spanning hundreds of microservices across multiple Kubernetes clusters, traditional file-based logging becomes untenable. Containers are ephemeral, nodes are cattle not pets, and logs scattered across thousands of files provide no systemic visibility. Logging in CI/CD extends beyond application debugging to encompass audit trails of who deployed what when, build pipeline diagnostics, and security forensics. Structured logging—machine-parseable, queryable, and correlated across service boundaries—transforms log data from unstructured text into operational intelligence.\n",
    "\n",
    "This chapter establishes comprehensive logging strategies for cloud-native environments, from application instrumentation through centralized aggregation, covering retention policies, sensitive data handling, and integration with continuous delivery pipelines.\n",
    "\n",
    "## 43.1 Structured Logging\n",
    "\n",
    "Structured logging replaces human-readable text with machine-parseable formats (JSON), enabling efficient filtering, aggregation, and correlation across distributed systems.\n",
    "\n",
    "### JSON Logging Format\n",
    "\n",
    "**Traditional Unstructured Log:**\n",
    "```\n",
    "2024-01-15 10:23:45 INFO Payment processed successfully for user 12345 amount $99.99 transaction abc-123\n",
    "```\n",
    "\n",
    "**Structured Log:**\n",
    "```json\n",
    "{\n",
    "  \"timestamp\": \"2024-01-15T10:23:45.123Z\",\n",
    "  \"level\": \"INFO\",\n",
    "  \"service\": \"payment-service\",\n",
    "  \"trace_id\": \"abc123def456\",\n",
    "  \"span_id\": \"span789\",\n",
    "  \"message\": \"Payment processed\",\n",
    "  \"attributes\": {\n",
    "    \"user_id\": \"12345\",\n",
    "    \"amount\": 99.99,\n",
    "    \"currency\": \"USD\",\n",
    "    \"transaction_id\": \"abc-123\",\n",
    "    \"payment_method\": \"credit_card\",\n",
    "    \"duration_ms\": 145\n",
    "  },\n",
    "  \"source\": {\n",
    "    \"file\": \"PaymentProcessor.java\",\n",
    "    \"line\": 45,\n",
    "    \"function\": \"processPayment\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The structured format separates concerns: timestamps are ISO-8601 (sortable), levels are standardized (DEBUG, INFO, WARN, ERROR), and contextual data resides in typed fields rather than embedded in strings. This enables queries like `attributes.amount > 100 AND attributes.currency = \"EUR\"` that would be impossible with grep on unstructured text.\n",
    "\n",
    "### Application Implementation (Java/Logback)\n",
    "\n",
    "```java\n",
    "// PaymentService.java\n",
    "import org.slf4j.Logger;\n",
    "import org.slf4j.LoggerFactory;\n",
    "import org.slf4j.MDC;\n",
    "import net.logstash.logback.argument.StructuredArguments;\n",
    "\n",
    "@Service\n",
    "public class PaymentService {\n",
    "    private static final Logger logger = LoggerFactory.getLogger(PaymentService.class);\n",
    "    \n",
    "    public PaymentResult processPayment(PaymentRequest request) {\n",
    "        // Add correlation context to all logs in this request\n",
    "        MDC.put(\"trace_id\", request.getTraceId());\n",
    "        MDC.put(\"span_id\", generateSpanId());\n",
    "        MDC.put(\"user_id\", request.getUserId());\n",
    "        \n",
    "        long startTime = System.currentTimeMillis();\n",
    "        \n",
    "        try {\n",
    "            PaymentResult result = executePayment(request);\n",
    "            \n",
    "            long duration = System.currentTimeMillis() - startTime;\n",
    "            \n",
    "            // Structured logging with Logstash encoder\n",
    "            logger.info(\"Payment processed successfully\",\n",
    "                StructuredArguments.keyValue(\"transaction_id\", result.getTransactionId()),\n",
    "                StructuredArguments.keyValue(\"amount\", request.getAmount()),\n",
    "                StructuredArguments.keyValue(\"currency\", request.getCurrency()),\n",
    "                StructuredArguments.keyValue(\"duration_ms\", duration),\n",
    "                StructuredArguments.keyValue(\"payment_method\", request.getPaymentMethod())\n",
    "            );\n",
    "            \n",
    "            return result;\n",
    "        } catch (PaymentException e) {\n",
    "            logger.error(\"Payment processing failed\",\n",
    "                StructuredArguments.keyValue(\"error_code\", e.getErrorCode()),\n",
    "                StructuredArguments.keyValue(\"error_type\", e.getClass().getSimpleName()),\n",
    "                StructuredArguments.keyValue(\"retryable\", e.isRetryable()),\n",
    "                e // Include stack trace\n",
    "            );\n",
    "            throw e;\n",
    "        } finally {\n",
    "            MDC.clear(); // Clean up context\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Logback Configuration (logback-spring.xml):**\n",
    "```xml\n",
    "<configuration>\n",
    "    <!-- Console appender with JSON formatting -->\n",
    "    <appender name=\"JSON_CONSOLE\" class=\"ch.qos.logback.core.ConsoleAppender\">\n",
    "        <encoder class=\"net.logstash.logback.encoder.LogstashEncoder\">\n",
    "            <includeContext>true</includeContext>\n",
    "            <includeMdc>true</includeMdc>\n",
    "            <fieldNames>\n",
    "                <timestamp>timestamp</timestamp>\n",
    "                <message>message</message>\n",
    "                <logger>logger</logger>\n",
    "                <thread>thread</thread>\n",
    "                <level>level</level>\n",
    "                <levelValue>[ignore]</levelValue>\n",
    "            </fieldNames>\n",
    "            <pattern>\n",
    "                {\n",
    "                    \"service\": \"payment-service\",\n",
    "                    \"environment\": \"${ENVIRONMENT:-development}\",\n",
    "                    \"version\": \"${VERSION:-unknown}\"\n",
    "                }\n",
    "            </pattern>\n",
    "        </encoder>\n",
    "    </appender>\n",
    "    \n",
    "    <!-- Root logger -->\n",
    "    <root level=\"INFO\">\n",
    "        <appender-ref ref=\"JSON_CONSOLE\" />\n",
    "    </root>\n",
    "    \n",
    "    <!-- Specific package levels -->\n",
    "    <logger name=\"com.company.payment\" level=\"DEBUG\" />\n",
    "    <logger name=\"org.springframework.web\" level=\"WARN\" />\n",
    "</configuration>\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **MDC (Mapped Diagnostic Context)**: Thread-local map that adds fields to every log entry in the request lifecycle. `trace_id` injected here appears in all subsequent logs without explicit passing.\n",
    "- **LogstashEncoder**: Outputs JSON format compatible with ELK/EFK stacks. The `includeMdc` setting automatically includes MDC fields in the JSON output.\n",
    "- **StructuredArguments**: Type-safe key-value pairs that serialize correctly (numbers remain numbers, not strings).\n",
    "- **Pattern**: Adds static fields (service name, environment) to every log entry for filtering.\n",
    "\n",
    "### Node.js Implementation (Winston)\n",
    "\n",
    "```javascript\n",
    "// logger.js\n",
    "const winston = require('winston');\n",
    "\n",
    "const logger = winston.createLogger({\n",
    "  level: process.env.LOG_LEVEL || 'info',\n",
    "  format: winston.format.combine(\n",
    "    winston.format.timestamp({ format: 'ISO8601' }),\n",
    "    winston.format.errors({ stack: true }),\n",
    "    winston.format.json()\n",
    "  ),\n",
    "  defaultMeta: {\n",
    "    service: 'order-service',\n",
    "    environment: process.env.NODE_ENV || 'development',\n",
    "    version: process.env.VERSION || 'unknown',\n",
    "    host: require('os').hostname()\n",
    "  },\n",
    "  transports: [\n",
    "    new winston.transports.Console({\n",
    "      format: winston.format.json()\n",
    "    })\n",
    "  ]\n",
    "});\n",
    "\n",
    "// Middleware to add request context\n",
    "function requestLogger(req, res, next) {\n",
    "  const childLogger = logger.child({\n",
    "    trace_id: req.headers['x-trace-id'] || generateTraceId(),\n",
    "    span_id: generateSpanId(),\n",
    "    user_id: req.user?.id,\n",
    "    request_method: req.method,\n",
    "    request_path: req.path,\n",
    "    client_ip: req.ip\n",
    "  });\n",
    "  \n",
    "  req.logger = childLogger;\n",
    "  \n",
    "  // Log request start\n",
    "  childLogger.info('Request started');\n",
    "  \n",
    "  // Log response on finish\n",
    "  res.on('finish', () => {\n",
    "    childLogger.info('Request completed', {\n",
    "      status_code: res.statusCode,\n",
    "      duration_ms: Date.now() - req.startTime,\n",
    "      response_size: res.get('Content-Length')\n",
    "    });\n",
    "  });\n",
    "  \n",
    "  next();\n",
    "}\n",
    "\n",
    "// Usage in routes\n",
    "app.post('/orders', (req, res) => {\n",
    "  req.logger.info('Processing order', { \n",
    "    order_id: req.body.orderId,\n",
    "    amount: req.body.amount \n",
    "  });\n",
    "  \n",
    "  // Business logic...\n",
    "});\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Winston's `format.combine` chains multiple formatters. `winston.format.json()` outputs JSON. The `logger.child()` method creates a contextual logger that inherits configuration but adds request-specific metadata. This avoids passing logger instances through every function call.\n",
    "\n",
    "## 43.2 Logging Architecture\n",
    "\n",
    "### EFK Stack (Elasticsearch, Fluentd/Fluent Bit, Kibana)\n",
    "\n",
    "**Architecture Flow:**\n",
    "```\n",
    "Application → Stdout → Fluent Bit (DaemonSet) → Elasticsearch → Kibana\n",
    "```\n",
    "\n",
    "**Fluent Bit Configuration (Kubernetes DaemonSet):**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: fluent-bit-config\n",
    "  namespace: logging\n",
    "data:\n",
    "  fluent-bit.conf: |\n",
    "    [SERVICE]\n",
    "        Flush         1\n",
    "        Log_Level     info\n",
    "        Daemon        off\n",
    "        Parsers_File  parsers.conf\n",
    "        \n",
    "    [INPUT]\n",
    "        Name              tail\n",
    "        Tag               kube.*\n",
    "        Path              /var/log/containers/*.log\n",
    "        Parser            docker\n",
    "        DB                /var/log/flb_kube.db\n",
    "        Mem_Buf_Limit     5MB\n",
    "        Skip_Long_Lines   On\n",
    "        Refresh_Interval  10\n",
    "        \n",
    "    [FILTER]\n",
    "        Name                kubernetes\n",
    "        Match               kube.*\n",
    "        Kube_URL            https://kubernetes.default.svc:443\n",
    "        Kube_CA_File        /var/run/secrets/kubernetes.io/serviceaccount/ca.crt\n",
    "        Kube_Token_File     /var/run/secrets/kubernetes.io/serviceaccount/token\n",
    "        Merge_Log           On\n",
    "        Keep_Log            Off\n",
    "        Labels              On\n",
    "        Annotations         Off\n",
    "        \n",
    "    [FILTER]\n",
    "        Name                nest\n",
    "        Match               kube.*\n",
    "        Operation           lift\n",
    "        Nested_under        kubernetes\n",
    "        Add_prefix          k8s_\n",
    "        \n",
    "    [FILTER]\n",
    "        Name                modify\n",
    "        Match               kube.*\n",
    "        Rename              message log\n",
    "        Rename              k8s_container_name container_name\n",
    "        Rename              k8s_namespace_name namespace\n",
    "        \n",
    "    [OUTPUT]\n",
    "        Name            es\n",
    "        Match           kube.*\n",
    "        Host            elasticsearch-master\n",
    "        Port            9200\n",
    "        Logstash_Format On\n",
    "        Logstash_Prefix k8s-logs\n",
    "        Retry_Limit     False\n",
    "        Suppress_Type_Name On\n",
    "        Trace_Error     On\n",
    "        \n",
    "  parsers.conf: |\n",
    "    [PARSER]\n",
    "        Name        docker\n",
    "        Format      json\n",
    "        Time_Key    time\n",
    "        Time_Format %Y-%m-%dT%H:%M:%S.%L\n",
    "        Time_Keep   On\n",
    "        Decode_Field_As escaped log do_next\n",
    "        Decode_Field_As json log\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **INPUT**: `tail` plugin reads container logs from `/var/log/containers/*.log` (symlinks to Docker/containerd log files).\n",
    "- **FILTER kubernetes**: Enriches logs with Pod metadata (labels, namespace, container name) by querying the Kubernetes API.\n",
    "- **FILTER modify**: Renames fields to match Elasticsearch expectations (e.g., `message` → `log`).\n",
    "- **OUTPUT**: Sends to Elasticsearch with Logstash format (daily indices like `k8s-logs-2024.01.15`).\n",
    "\n",
    "### PLG Stack (Promtail, Loki, Grafana)\n",
    "\n",
    "Loki is a horizontally-scalable, highly-available, multi-tenant log aggregation system inspired by Prometheus.\n",
    "\n",
    "**Promtail Configuration:**\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: promtail-config\n",
    "  namespace: logging\n",
    "data:\n",
    "  promtail.yaml: |\n",
    "    server:\n",
    "      log_level: info\n",
    "      http_listen_port: 3101\n",
    "      \n",
    "    clients:\n",
    "      - url: http://loki:3100/loki/api/v1/push\n",
    "        tenant_id: production\n",
    "        \n",
    "    positions:\n",
    "      filename: /run/promtail/positions.yaml\n",
    "      \n",
    "    scrape_configs:\n",
    "      - job_name: kubernetes-pods\n",
    "        kubernetes_sd_configs:\n",
    "          - role: pod\n",
    "            namespaces:\n",
    "              names:\n",
    "                - production\n",
    "                - staging\n",
    "                \n",
    "        pipeline_stages:\n",
    "          - cri: {}  # Parse containerd/cri-o format\n",
    "          \n",
    "          - json:\n",
    "              expressions:\n",
    "                level: level\n",
    "                message: message\n",
    "                trace_id: trace_id\n",
    "                service: service\n",
    "                \n",
    "          - labels:\n",
    "              level:\n",
    "              service:\n",
    "              trace_id:\n",
    "              \n",
    "          - timestamp:\n",
    "              source: timestamp\n",
    "              format: RFC3339\n",
    "              \n",
    "          - output:\n",
    "              source: message\n",
    "              \n",
    "        relabel_configs:\n",
    "          - source_labels: ['__meta_kubernetes_pod_node_name']\n",
    "            target_label: 'node'\n",
    "          - source_labels: ['__meta_kubernetes_namespace']\n",
    "            target_label: 'namespace'\n",
    "          - source_labels: ['__meta_kubernetes_pod_name']\n",
    "            target_label: 'pod'\n",
    "          - source_labels: ['__meta_kubernetes_container_name']\n",
    "            target_label: 'container'\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Promtail runs on each node, tails log files, and pushes to Loki. The `pipeline_stages` parse JSON logs, extracting labels (indexed fields) like `service` and `trace_id`. Unlike Elasticsearch, Loki only indexes labels, not full log content, making it cheaper for high-volume logs. The `relabel_configs` add Kubernetes metadata as labels for filtering.\n",
    "\n",
    "## 43.3 Kubernetes Logging Patterns\n",
    "\n",
    "### Container Logging Standard\n",
    "\n",
    "Containers should log to stdout/stderr, not files:\n",
    "\n",
    "```dockerfile\n",
    "# Good - logs to stdout\n",
    "CMD [\"java\", \"-jar\", \"app.jar\"]\n",
    "\n",
    "# Bad - logs to file inside container\n",
    "CMD [\"java\", \"-jar\", \"app.jar\", \">\", \"/var/log/app.log\"]\n",
    "```\n",
    "\n",
    "**Log Rotation (container runtime level):**\n",
    "```yaml\n",
    "# /etc/docker/daemon.json\n",
    "{\n",
    "  \"log-driver\": \"json-file\",\n",
    "  \"log-opts\": {\n",
    "    \"max-size\": \"10m\",\n",
    "    \"max-file\": \"3\",\n",
    "    \"labels\": \"production_status,environment\",\n",
    "    \"env\": \"OS_VERSION\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Docker's `json-file` driver captures stdout/stderr. `max-size` and `max-file` prevent disk exhaustion. Kubernetes automatically rotates these. Logging agents (Fluent Bit, Promtail) tail these files on the host, not inside containers.\n",
    "\n",
    "### Sidecar Logging Pattern\n",
    "\n",
    "For legacy applications that log to files:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: legacy-app\n",
    "spec:\n",
    "  containers:\n",
    "    - name: legacy-app\n",
    "      image: company/legacy-app:1.0\n",
    "      volumeMounts:\n",
    "        - name: log-volume\n",
    "          mountPath: /var/log/legacy\n",
    "          \n",
    "    - name: log-shipper\n",
    "      image: fluent/fluent-bit:2.1\n",
    "      volumeMounts:\n",
    "        - name: log-volume\n",
    "          mountPath: /var/log/legacy\n",
    "          readOnly: true\n",
    "        - name: fluent-bit-config\n",
    "          mountPath: /fluent-bit/etc/\n",
    "      command: [\"/fluent-bit/bin/fluent-bit\"]\n",
    "      args: [\"-c\", \"/fluent-bit/etc/fluent-bit.conf\"]\n",
    "      \n",
    "  volumes:\n",
    "    - name: log-volume\n",
    "      emptyDir: {}\n",
    "    - name: fluent-bit-config\n",
    "      configMap:\n",
    "        name: fluent-bit-sidecar\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The `emptyDir` volume is shared between containers. The legacy app writes logs to files; the sidecar (Fluent Bit) tails those files and forwards to the aggregation system. This pattern avoids modifying legacy code while achieving centralized logging.\n",
    "\n",
    "## 43.4 Correlation and Tracing Context\n",
    "\n",
    "### Trace ID Propagation\n",
    "\n",
    "Trace IDs correlate logs across distributed services:\n",
    "\n",
    "```java\n",
    "// MDCFilter.java\n",
    "@Component\n",
    "public class TraceIdFilter implements Filter {\n",
    "    \n",
    "    @Override\n",
    "    public void doFilter(ServletRequest request, ServletResponse response, \n",
    "                        FilterChain chain) throws IOException, ServletException {\n",
    "        \n",
    "        HttpServletRequest httpRequest = (HttpServletRequest) request;\n",
    "        \n",
    "        // Extract trace ID from incoming request or generate new\n",
    "        String traceId = httpRequest.getHeader(\"X-Trace-ID\");\n",
    "        if (traceId == null || traceId.isEmpty()) {\n",
    "            traceId = UUID.randomUUID().toString().replace(\"-\", \"\");\n",
    "        }\n",
    "        \n",
    "        // Extract span ID\n",
    "        String spanId = httpRequest.getHeader(\"X-Span-ID\");\n",
    "        if (spanId == null) {\n",
    "            spanId = generateSpanId();\n",
    "        }\n",
    "        \n",
    "        // Add to MDC (appears in all logs)\n",
    "        MDC.put(\"trace_id\", traceId);\n",
    "        MDC.put(\"span_id\", spanId);\n",
    "        MDC.put(\"parent_span_id\", httpRequest.getHeader(\"X-Parent-Span-ID\"));\n",
    "        \n",
    "        // Propagate to downstream services\n",
    "        HttpServletResponse httpResponse = (HttpServletResponse) response;\n",
    "        httpResponse.setHeader(\"X-Trace-ID\", traceId);\n",
    "        \n",
    "        try {\n",
    "            chain.doFilter(request, response);\n",
    "        } finally {\n",
    "            MDC.clear(); // Prevent leakage to next request\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Downstream HTTP Client:**\n",
    "```java\n",
    "public class PaymentClient {\n",
    "    \n",
    "    public PaymentResult charge(PaymentRequest request) {\n",
    "        // Current trace context\n",
    "        String traceId = MDC.get(\"trace_id\");\n",
    "        String spanId = generateSpanId();\n",
    "        String currentSpanId = MDC.get(\"span_id\");\n",
    "        \n",
    "        HttpHeaders headers = new HttpHeaders();\n",
    "        headers.set(\"X-Trace-ID\", traceId);\n",
    "        headers.set(\"X-Span-ID\", spanId);\n",
    "        headers.set(\"X-Parent-Span-ID\", currentSpanId);\n",
    "        \n",
    "        ResponseEntity<PaymentResult> response = restTemplate.exchange(\n",
    "            \"http://payment-service/charge\",\n",
    "            HttpMethod.POST,\n",
    "            new HttpEntity<>(request, headers),\n",
    "            PaymentResult.class\n",
    "        );\n",
    "        \n",
    "        return response.getBody();\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The filter extracts or generates trace IDs on entry, places them in MDC (thread-local storage), and ensures all logs for that request include the trace ID. HTTP clients propagate the trace ID to downstream services via headers. This creates a complete request chain across services.\n",
    "\n",
    "### Querying Correlated Logs\n",
    "\n",
    "**Kibana Query:**\n",
    "```json\n",
    "{\n",
    "  \"query\": {\n",
    "    \"bool\": {\n",
    "      \"must\": [\n",
    "        { \"match\": { \"trace_id\": \"abc123def456\" }},\n",
    "        { \"range\": { \"timestamp\": { \n",
    "          \"gte\": \"now-1h\", \n",
    "          \"lte\": \"now\" \n",
    "        }}}\n",
    "      ]\n",
    "    }\n",
    "  },\n",
    "  \"sort\": [\n",
    "    { \"timestamp\": { \"order\": \"asc\" }}\n",
    "  ]\n",
    "}\n",
    "```\n",
    "\n",
    "**Loki Query (LogQL):**\n",
    "```logql\n",
    "{service=~\"payment-service|order-service\"} \n",
    "  |= \"trace_id=\\\"abc123def456\\\"\"\n",
    "  | json\n",
    "  | line_format \"{{.timestamp}} [{{.service}}] {{.message}}\"\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The Loki query searches across services (`payment-service` or `order-service`) for logs containing the trace ID, parses JSON to extract fields, and formats output chronologically. This reconstructs the entire request flow across microservices.\n",
    "\n",
    "## 43.5 Security and Sensitive Data\n",
    "\n",
    "### PII Redaction\n",
    "\n",
    "Automatic removal of sensitive data:\n",
    "\n",
    "```java\n",
    "@Component\n",
    "public class PiiMaskingFilter {\n",
    "    \n",
    "    private static final Pattern CREDIT_CARD_PATTERN = \n",
    "        Pattern.compile(\"\\\\b(\\\\d{4})[- ]?(\\\\d{4})[- ]?(\\\\d{4})[- ]?(\\\\d{4})\\\\b\");\n",
    "    private static final Pattern EMAIL_PATTERN = \n",
    "        Pattern.compile(\"\\\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\\\.[A-Z|a-z]{2,}\\\\b\");\n",
    "    \n",
    "    public String mask(String message) {\n",
    "        String masked = CREDIT_CARD_PATTERN.matcher(message)\n",
    "            .replaceAll(match -> {\n",
    "                String card = match.group();\n",
    "                return \"****-****-****-\" + card.substring(card.length() - 4);\n",
    "            });\n",
    "            \n",
    "        masked = EMAIL_PATTERN.matcher(masked)\n",
    "            .replaceAll(match -> {\n",
    "                String email = match.group();\n",
    "                int atIndex = email.indexOf('@');\n",
    "                return email.charAt(0) + \"***@\" + email.substring(atIndex + 1);\n",
    "            });\n",
    "            \n",
    "        return masked;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Logback Masking Encoder:**\n",
    "```xml\n",
    "<encoder class=\"com.company.logging.MaskingEncoder\">\n",
    "    <maskPatterns>\n",
    "        <pattern>ssn=\\d{3}-\\d{2}-\\d{4}</pattern>\n",
    "        <pattern>password=[^&\\s]+</pattern>\n",
    "        <pattern>token=[a-zA-Z0-9]{32}</pattern>\n",
    "    </maskPatterns>\n",
    "    <layout class=\"net.logstash.logback.layout.LogstashLayout\"/>\n",
    "</encoder>\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Masking happens at the logging framework level before output. Credit cards show only last 4 digits, emails mask the local part. This prevents accidental credential exposure in logs while retaining debugging usefulness.\n",
    "\n",
    "### Audit Logging\n",
    "\n",
    "Separate audit trail for compliance:\n",
    "\n",
    "```java\n",
    "@Service\n",
    "public class AuditService {\n",
    "    \n",
    "    private static final Logger auditLogger = \n",
    "        LoggerFactory.getLogger(\"AUDIT\");\n",
    "    \n",
    "    public void logSecurityEvent(SecurityEvent event) {\n",
    "        try {\n",
    "            ObjectMapper mapper = new ObjectMapper();\n",
    "            String auditEntry = mapper.writeValueAsString(Map.of(\n",
    "                \"timestamp\", Instant.now().toString(),\n",
    "                \"event_type\", event.getType(),\n",
    "                \"user_id\", event.getUserId(),\n",
    "                \"action\", event.getAction(),\n",
    "                \"resource\", event.getResource(),\n",
    "                \"outcome\", event.getOutcome(),\n",
    "                \"source_ip\", event.getSourceIp(),\n",
    "                \"user_agent\", event.getUserAgent(),\n",
    "                \"correlation_id\", MDC.get(\"trace_id\"),\n",
    "                \"integrity_hash\", calculateHash(event)\n",
    "            ));\n",
    "            \n",
    "            auditLogger.info(auditEntry);\n",
    "        } catch (JsonProcessingException e) {\n",
    "            // Fail safe - log error but don't throw\n",
    "            auditLogger.error(\"Failed to serialize audit event\", e);\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    private String calculateHash(SecurityEvent event) {\n",
    "        // HMAC for tamper detection\n",
    "        // Implementation...\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Audit logs are immutable, tamper-evident records of security-relevant events (logins, data access, configuration changes). They use a separate logger category (`AUDIT`) that routes to separate storage with stricter retention and access controls. Integrity hashes detect tampering.\n",
    "\n",
    "## 43.6 CI/CD Pipeline Logging\n",
    "\n",
    "### Build Log Aggregation\n",
    "\n",
    "GitHub Actions workflow with structured logging:\n",
    "\n",
    "```yaml\n",
    "name: Build and Deploy\n",
    "on: [push]\n",
    "\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Setup Logging\n",
    "        run: |\n",
    "          echo \"::group::Environment Setup\"\n",
    "          echo \"Setting up build environment...\"\n",
    "          echo \"::endgroup::\"\n",
    "          \n",
    "      - name: Build Application\n",
    "        id: build\n",
    "        run: |\n",
    "          echo \"::notice::Starting build process\"\n",
    "          \n",
    "          mvn clean package | tee build.log | while IFS= read -r line; do\n",
    "            if [[ $line == *\"ERROR\"* ]]; then\n",
    "              echo \"::error::$line\"\n",
    "            elif [[ $line == *\"WARNING\"* ]]; then\n",
    "              echo \"::warning::$line\"\n",
    "            fi\n",
    "          done\n",
    "          \n",
    "          echo \"::set-output name=status::success\"\n",
    "          echo \"::set-output name=duration::$(date +%s)\"\n",
    "          \n",
    "      - name: Upload Logs\n",
    "        if: always()\n",
    "        uses: actions/upload-artifact@v4\n",
    "        with:\n",
    "          name: build-logs-${{ github.run_id }}\n",
    "          path: |\n",
    "            build.log\n",
    "            target/surefire-reports/\n",
    "            \n",
    "      - name: Notify on Failure\n",
    "        if: failure()\n",
    "        uses: slackapi/slack-github-action@v1\n",
    "        with:\n",
    "          payload: |\n",
    "            {\n",
    "              \"text\": \"Build failed\",\n",
    "              \"attachments\": [{\n",
    "                \"color\": \"danger\",\n",
    "                \"fields\": [\n",
    "                  {\"title\": \"Repository\", \"value\": \"${{ github.repository }}\", \"short\": true},\n",
    "                  {\"title\": \"Commit\", \"value\": \"${{ github.sha }}\", \"short\": true},\n",
    "                  {\"title\": \"Author\", \"value\": \"${{ github.actor }}\", \"short\": true},\n",
    "                  {\"title\": \"Logs\", \"value\": \"https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }}\", \"short\": false}\n",
    "                ]\n",
    "              }]\n",
    "            }\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "GitHub Actions commands like `::error::` and `::group::` create collapsible, annotated logs. The `tee` command saves logs to file while displaying them. Artifacts preserve logs post-build. Slack notifications include direct links to run logs.\n",
    "\n",
    "### GitLab CI Logging\n",
    "\n",
    "```yaml\n",
    "build:\n",
    "  stage: build\n",
    "  script:\n",
    "    - echo \"CI_JOB_ID=${CI_JOB_ID}\" > build.env\n",
    "    - echo \"CI_COMMIT_SHA=${CI_COMMIT_SHA}\" >> build.env\n",
    "    \n",
    "    - |\n",
    "      docker build --progress=plain -t $CI_REGISTRY_IMAGE:$CI_COMMIT_SHA . 2>&1 | \n",
    "      tee build.log | \n",
    "      grep -E \"(^Step|ERROR|Successfully built)\"\n",
    "    \n",
    "    - |\n",
    "      if [ ${PIPESTATUS[0]} -ne 0 ]; then\n",
    "        echo \"Build failed with errors:\"\n",
    "        tail -n 100 build.log\n",
    "        exit 1\n",
    "      fi\n",
    "  \n",
    "  artifacts:\n",
    "    when: always\n",
    "    expire_in: 1 week\n",
    "    paths:\n",
    "      - build.log\n",
    "      - build.env\n",
    "    reports:\n",
    "      junit: target/surefire-reports/TEST-*.xml\n",
    "  \n",
    "  after_script:\n",
    "    - |\n",
    "      curl -X POST $LOG_AGGREGATOR_URL \\\n",
    "        -H \"Content-Type: application/json\" \\\n",
    "        -d \"{\n",
    "          \\\"job_id\\\": \\\"$CI_JOB_ID\\\",\n",
    "          \\\"pipeline_id\\\": \\\"$CI_PIPELINE_ID\\\",\n",
    "          \\\"status\\\": \\\"$CI_JOB_STATUS\\\",\n",
    "          \\\"logs\\\": $(cat build.log | base64 -w 0),\n",
    "          \\\"timestamp\\\": \\\"$(date -Iseconds)\\\"\n",
    "        }\"\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "GitLab CI captures logs as artifacts with `when: always` (preserved even on failure). The `after_script` pushes logs to a central aggregator (Elasticsearch, Splunk) for long-term retention and cross-pipeline analysis. JUnit reports integrate with GitLab's test visualization.\n",
    "\n",
    "## 43.7 Log Retention and Cost\n",
    "\n",
    "### Tiered Storage Strategy\n",
    "\n",
    "```yaml\n",
    "# Elasticsearch ILM (Index Lifecycle Management) Policy\n",
    "PUT _ilm/policy/logs_policy\n",
    "{\n",
    "  \"policy\": {\n",
    "    \"phases\": {\n",
    "      \"hot\": {\n",
    "        \"min_age\": \"0ms\",\n",
    "        \"actions\": {\n",
    "          \"rollover\": {\n",
    "            \"max_primary_shard_size\": \"50gb\",\n",
    "            \"max_age\": \"1d\"\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"warm\": {\n",
    "        \"min_age\": \"2d\",\n",
    "        \"actions\": {\n",
    "          \"shrink\": {\n",
    "            \"number_of_shards\": 1\n",
    "          },\n",
    "          \"forcemerge\": {\n",
    "            \"max_num_segments\": 1\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"cold\": {\n",
    "        \"min_age\": \"7d\",\n",
    "        \"actions\": {\n",
    "          \"allocate\": {\n",
    "            \"require\": {\n",
    "              \"data\": \"cold\"\n",
    "            }\n",
    "          }\n",
    "        }\n",
    "      },\n",
    "      \"delete\": {\n",
    "        \"min_age\": \"90d\",\n",
    "        \"actions\": {\n",
    "          \"delete\": {}\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Hot**: Recent logs (1-2 days) on fast SSD storage, searchable in real-time\n",
    "- **Warm**: Older logs (2-7 days) on cheaper storage, optimized (shrunk, merged)\n",
    "- **Cold**: Archived logs (7-90 days) on slow/cheap storage (S3), rarely queried\n",
    "- **Delete**: Compliance-mandated deletion after 90 days (GDPR, PCI-DSS)\n",
    "\n",
    "### Sampling High-Volume Logs\n",
    "\n",
    "```yaml\n",
    "# Fluent Bit sampling configuration\n",
    "[FILTER]\n",
    "    Name          sampling\n",
    "    Match         kube.*\n",
    "    Sampling      10  # Log 1 out of 10 entries for high-volume services\n",
    "    \n",
    "[FILTER]\n",
    "    Name          grep\n",
    "    Match         kube.*\n",
    "    Regex         level (ERROR|WARN)  # Always keep errors\n",
    "    \n",
    "[FILTER]\n",
    "    Name          modify\n",
    "    Match         kube.*\n",
    "    Condition     Key_does_not_exist level\n",
    "    Add           level INFO\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "For high-traffic services (10k logs/second), sampling reduces volume. The `Sampling` filter keeps only 10% of logs, but the `grep` filter ensures all ERROR/WARN logs are retained regardless of sampling. This balances cost with observability.\n",
    "\n",
    "## 43.8 Integration with Observability\n",
    "\n",
    "### Unified Observability (Logs, Metrics, Traces)\n",
    "\n",
    "**OpenTelemetry Collector Configuration:**\n",
    "```yaml\n",
    "# otel-collector-config.yaml\n",
    "receivers:\n",
    "  otlp:\n",
    "    protocols:\n",
    "      grpc:\n",
    "        endpoint: 0.0.0.0:4317\n",
    "      http:\n",
    "        endpoint: 0.0.0.0:4318\n",
    "        \n",
    "  filelog:\n",
    "    include: [/var/log/containers/*.log]\n",
    "    operators:\n",
    "      - type: json_parser\n",
    "        timestamp:\n",
    "          parse_from: attributes.timestamp\n",
    "          layout: '%Y-%m-%dT%H:%M:%S.%LZ'\n",
    "          \n",
    "processors:\n",
    "  resource:\n",
    "    attributes:\n",
    "      - key: environment\n",
    "        value: production\n",
    "        action: upsert\n",
    "        \n",
    "  batch:\n",
    "    timeout: 1s\n",
    "    send_batch_size: 1024\n",
    "    \n",
    "exporters:\n",
    "  loki:\n",
    "    endpoint: http://loki:3100/loki/api/v1/push\n",
    "    \n",
    "  prometheusremotewrite:\n",
    "    endpoint: http://prometheus:9090/api/v1/write\n",
    "    \n",
    "  jaeger:\n",
    "    endpoint: jaeger-collector:14250\n",
    "    tls:\n",
    "      insecure: true\n",
    "      \n",
    "service:\n",
    "  pipelines:\n",
    "    logs:\n",
    "      receivers: [otlp, filelog]\n",
    "      processors: [resource, batch]\n",
    "      exporters: [loki]\n",
    "      \n",
    "    metrics:\n",
    "      receivers: [otlp]\n",
    "      processors: [resource, batch]\n",
    "      exporters: [prometheusremotewrite]\n",
    "      \n",
    "    traces:\n",
    "      receivers: [otlp]\n",
    "      processors: [resource, batch]\n",
    "      exporters: [jaeger]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The OpenTelemetry Collector receives logs, metrics, and traces via OTLP (OpenTelemetry Protocol), processes them (batching, enrichment), and exports to backend systems (Loki for logs, Prometheus for metrics, Jaeger for traces). This unifies observability data under a single pipeline.\n",
    "\n",
    "### Correlating Logs with Metrics\n",
    "\n",
    "```yaml\n",
    "# Recording rule for high error rate\n",
    "groups:\n",
    "  - name: payment_alerts\n",
    "    rules:\n",
    "      - record: payment:error_rate_5m\n",
    "        expr: |\n",
    "          sum(rate(payment_failures_total[5m])) \n",
    "          / \n",
    "          sum(rate(payment_total[5m]))\n",
    "          \n",
    "      - alert: HighPaymentFailureRate\n",
    "        expr: payment:error_rate_5m > 0.05\n",
    "        for: 2m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"High payment failure rate detected\"\n",
    "          dashboard: \"https://grafana.company.com/d/payment-dashboard?var-trace_id={{ $labels.trace_id }}\"\n",
    "          logs: \"https://grafana.company.com/explore?orgId=1&left=%7B%22datasource%22:%22Loki%22,%22queries%22:%5B%7B%22expr%22:%22%7Bservice%3D%5C%22payment-service%5C%22%7D%20%7C%3D%20%5C%22{{ $labels.trace_id }}%5C%22%22%7D%5D%7D\"\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "When the alert fires, the annotation includes direct links to Grafana dashboards filtered by the trace ID from the metric labels. This bridges metrics (the alert) to logs (the context) instantly, reducing MTTR (Mean Time To Resolution).\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "This chapter established logging as a critical observability pillar in CI/CD ecosystems, extending beyond simple debugging to encompass audit trails, security forensics, and operational intelligence. We examined structured logging formats (JSON) that replace human-readable text with machine-parseable data, enabling efficient filtering and correlation across distributed microservices. The implementation patterns in Java (Logback with Logstash encoder) and Node.js (Winston) demonstrated how Mapped Diagnostic Context (MDC) and child loggers propagate trace IDs across thread boundaries and service calls without explicit parameter passing.\n",
    "\n",
    "The logging architecture section compared the EFK stack (Elasticsearch, Fluent Bit, Kibana) suitable for full-text search and complex analytics, against the PLG stack (Promtail, Loki, Grafana) optimized for cost-effective, label-based log aggregation inspired by Prometheus. Kubernetes-specific patterns including DaemonSet log collectors, sidecar patterns for legacy applications, and stdout/stderr streaming ensure comprehensive log capture from ephemeral containers without persistent volumes.\n",
    "\n",
    "Security considerations mandated PII redaction at the logging framework level, automatic masking of credit cards and credentials before output, and separate audit logging streams with integrity hashing for compliance requirements. CI/CD pipeline logging strategies captured build artifacts, test results, and deployment events with structured metadata linking commits, authors, and outcomes to enable post-failure analysis and compliance auditing.\n",
    "\n",
    "Retention policies using Elasticsearch Index Lifecycle Management (ILM) tier data across hot, warm, cold, and deletion phases, balancing query performance against storage costs. Sampling strategies for high-volume services ensure error logs are always retained while reducing info-level verbosity. OpenTelemetry integration unified logs, metrics, and traces under a single pipeline, enabling correlation between alerting metrics and contextual logs through trace ID propagation.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Always use structured JSON logging with standardized field names (timestamp, level, service, trace_id, message) to enable machine parsing and efficient querying across distributed systems.\n",
    "- Implement trace ID propagation using MDC (Java) or async hooks (Node.js) to correlate logs across microservices; ensure HTTP clients forward trace context via headers to maintain request chains.\n",
    "- Deploy log collectors (Fluent Bit or Promtail) as Kubernetes DaemonSets to capture container stdout/stderr without requiring applications to manage file logging or log rotation.\n",
    "- Separate security audit logs from application logs, routing them to immutable storage with integrity verification and stricter access controls than operational logs.\n",
    "- Implement PII redaction at the logging framework level using pattern matching or sanitization serializers to prevent accidental credential or personal data exposure in centralized log aggregators.\n",
    "- Use tiered retention policies (hot/warm/cold/delete) to manage costs, keeping recent logs on fast storage for debugging while archiving older logs to object storage for compliance.\n",
    "\n",
    "**Next Chapter Preview:**\n",
    "Chapter 44: Metrics and Monitoring establishes quantitative observability through time-series data collection, aggregation, and alerting. We will examine Prometheus architecture and metric types (counters, gauges, histograms, summaries), service discovery for dynamic Kubernetes targets, PromQL query language for operational analysis, and Grafana dashboard design for visualization. The chapter covers SLO (Service Level Objective) definition and SLI (Service Level Indicator) implementation, error budget burn rate alerting, and integration with CI/CD pipelines for canary analysis and progressive delivery decision-making, completing the observability triad (logs, metrics, traces) with quantitative reliability engineering practices."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
