{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 47: Pipeline Performance Optimization\n",
    "\n",
    "As codebases grow and microservices multiply, CI/CD pipelines can become bottlenecks rather than accelerators. Builds taking 30+ minutes destroy the tight feedback loops essential for agile development, while inefficient resource utilization drives cloud costs exponentially. Pipeline performance optimization requires a systematic approach: identifying bottlenecks through timing analysis, implementing intelligent caching at multiple layers, parallelizing independent operations, and right-sizing compute resources to match workload characteristics without over-provisioning.\n",
    "\n",
    "This chapter establishes strategies for sub-5-minute build pipelines, covering Docker layer caching, dependency resolution acceleration, test parallelization, and cost optimization techniques that maintain velocity while controlling infrastructure spend.\n",
    "\n",
    "## 47.1 Pipeline Bottleneck Analysis\n",
    "\n",
    "Before optimizing, identify where time is actually spent.\n",
    "\n",
    "### Build Timing Analysis\n",
    "\n",
    "**GitHub Actions Timing:**\n",
    "```yaml\n",
    "# .github/workflows/build.yml\n",
    "jobs:\n",
    "  build:\n",
    "    steps:\n",
    "      - name: Checkout\n",
    "        uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Setup Java\n",
    "        uses: actions/setup-java@v3\n",
    "        with:\n",
    "          java-version: '17'\n",
    "          distribution: 'temurin'\n",
    "      \n",
    "      - name: Build with Timing\n",
    "        run: |\n",
    "          echo \"::group::Dependency Resolution\"\n",
    "          time ./mvnw dependency:go-offline\n",
    "          echo \"::endgroup::\"\n",
    "          \n",
    "          echo \"::group::Compilation\"\n",
    "          time ./mvnw compile -DskipTests\n",
    "          echo \"::endgroup::\"\n",
    "          \n",
    "          echo \"::group::Testing\"\n",
    "          time ./mvnw test\n",
    "          echo \"::endgroup::\"\n",
    "          \n",
    "          echo \"::group::Packaging\"\n",
    "          time ./mvnw package -DskipTests\n",
    "          echo \"::endgroup::\"\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The `time` command prefixes each Maven phase, outputting real/user/system time. GitHub Actions collapsible groups (`::group::`/`::endgroup::`) organize logs by phase. Typical bottlenecks appear in:\n",
    "- **Dependency resolution**: Downloading artifacts (network I/O)\n",
    "- **Compilation**: CPU-bound, scales with code size\n",
    "- **Testing**: Often the longest phase, I/O and CPU bound\n",
    "- **Docker build**: Layer caching misses, large context transfers\n",
    "\n",
    "### Profiling Tools\n",
    "\n",
    "**Maven Build Profile:**\n",
    "```bash\n",
    "./mvnw clean package -DskipTests \\\n",
    "  -Dmaven.ext.class.path=/usr/share/maven/maven-profile-*.jar \\\n",
    "  --builder smart \\\n",
    "  -T 4\n",
    "```\n",
    "\n",
    "**Gradle Build Scan:**\n",
    "```bash\n",
    "./gradlew build --scan\n",
    "# Generates https://scans.gradle.com/s/unique-id\n",
    "# Shows task execution timeline, cache hits, dependency resolution time\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Gradle Build Scans provide visualization of build execution, showing which tasks ran in parallel, which were cached, and where time was spent. The `--parallel` flag enables parallel project builds for multi-module projects.\n",
    "\n",
    "## 47.2 Caching Strategies\n",
    "\n",
    "Effective caching eliminates redundant work across builds.\n",
    "\n",
    "### Dependency Caching\n",
    "\n",
    "**Maven Cache (GitHub Actions):**\n",
    "```yaml\n",
    "- name: Cache Maven Dependencies\n",
    "  uses: actions/cache@v3\n",
    "  with:\n",
    "    path: |\n",
    "      ~/.m2/repository\n",
    "      !~/.m2/repository/com/company  # Exclude SNAPSHOTs\n",
    "    key: ${{ runner.os }}-maven-${{ hashFiles('**/pom.xml') }}\n",
    "    restore-keys: |\n",
    "      ${{ runner.os }}-maven-\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The cache key includes the OS and a hash of all `pom.xml` files. If any POM changes, the cache misses. `restore-keys` provides partial matching—if the exact key misses, it falls back to the most recent Maven cache, then downloads only changed dependencies. The exclusion `!~/.m2/repository/com/company` ensures internal SNAPSHOTs aren't cached between builds (they change frequently).\n",
    "\n",
    "**Npm Cache:**\n",
    "```yaml\n",
    "- name: Cache Node Modules\n",
    "  uses: actions/cache@v3\n",
    "  with:\n",
    "    path: |\n",
    "      ~/.npm\n",
    "      node_modules\n",
    "    key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}\n",
    "    restore-keys: |\n",
    "      ${{ runner.os }}-node-\n",
    "```\n",
    "\n",
    "**Layered Caching Strategy:**\n",
    "```yaml\n",
    "# Multi-layer cache for better hit rates\n",
    "- name: Cache Gradle Wrapper\n",
    "  uses: actions/cache@v3\n",
    "  with:\n",
    "    path: ~/.gradle/wrapper\n",
    "    key: ${{ runner.os }}-gradle-wrapper-${{ hashFiles('**/gradle-wrapper.properties') }}\n",
    "\n",
    "- name: Cache Gradle Dependencies\n",
    "  uses: actions/cache@v3\n",
    "  with:\n",
    "    path: ~/.gradle/caches\n",
    "    key: ${{ runner.os }}-gradle-${{ hashFiles('**/*.gradle*', '**/gradle.properties') }}\n",
    "    restore-keys: |\n",
    "      ${{ runner.os }}-gradle-\n",
    "```\n",
    "\n",
    "### Build Cache (Gradle Build Cache)\n",
    "\n",
    "**Remote Build Cache:**\n",
    "```groovy\n",
    "// gradle/build-cache-settings.gradle\n",
    "buildCache {\n",
    "    local {\n",
    "        enabled = true\n",
    "        directory = \"${rootDir}/.gradle/build-cache\"\n",
    "        removeUnusedEntriesAfterDays = 30\n",
    "    }\n",
    "    \n",
    "    remote(HttpBuildCache) {\n",
    "        url = 'https://cache.company.com/cache/'\n",
    "        credentials {\n",
    "            username = System.getenv('CACHE_USERNAME')\n",
    "            password = System.getenv('CACHE_PASSWORD')\n",
    "        }\n",
    "        enabled = true\n",
    "        push = isCiServer\n",
    "        \n",
    "        // Only cache if build succeeds\n",
    "        allowUntrustedServer = false\n",
    "        allowInsecureProtocol = false\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Gradle's build cache stores task outputs (compiled classes, test results, processed resources). If inputs haven't changed (determined by hashing), it restores outputs from cache rather than re-executing tasks. Remote caches share build outputs across CI runners, so if one runner compiles a module, others download the result instead of recompiling.\n",
    "\n",
    "### Docker Layer Caching\n",
    "\n",
    "**GitHub Actions Docker Cache:**\n",
    "```yaml\n",
    "- name: Set up Docker Buildx\n",
    "  uses: docker/setup-buildx-action@v3\n",
    "\n",
    "- name: Build and Push\n",
    "  uses: docker/build-push-action@v5\n",
    "  with:\n",
    "    context: .\n",
    "    push: true\n",
    "    tags: company/app:latest\n",
    "    cache-from: type=gha\n",
    "    cache-to: type=gha,mode=max\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "`type=gha` uses GitHub Actions cache backend for Docker layers. Each Dockerfile instruction creates a layer; if the instruction and context haven't changed, Docker reuses the cached layer. `mode=max` exports all layers (including intermediate) for maximum cache efficiency.\n",
    "\n",
    "**Registry Cache (Alternative):**\n",
    "```yaml\n",
    "cache-from: type=registry,ref=company/app:cache\n",
    "cache-to: type=registry,ref=company/app:cache,mode=max\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Storing cache in the container registry (`company/app:cache` tag) rather than GitHub Actions cache provides larger storage limits and persistence across workflow runs, though with slightly higher latency.\n",
    "\n",
    "## 47.3 Parallel Execution\n",
    "\n",
    "### Test Parallelization\n",
    "\n",
    "**Maven Surefire Parallel Execution:**\n",
    "```xml\n",
    "<!-- pom.xml -->\n",
    "<plugin>\n",
    "    <groupId>org.apache.maven.plugins</groupId>\n",
    "    <artifactId>maven-surefire-plugin</artifactId>\n",
    "    <version>3.1.2</version>\n",
    "    <configuration>\n",
    "        <parallel>methods</parallel>\n",
    "        <threadCount>4</threadCount>\n",
    "        <forkCount>2</forkCount>\n",
    "        <reuseForks>true</reuseForks>\n",
    "        <argLine>-Xmx512m</argLine>\n",
    "    </configuration>\n",
    "</plugin>\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **parallel=methods**: Runs test methods in parallel within same JVM\n",
    "- **threadCount=4**: Uses 4 threads per fork\n",
    "- **forkCount=2**: Creates 2 separate JVM processes\n",
    "- **Total parallelism**: 2 JVMs × 4 threads = 8 concurrent tests\n",
    "- **reuseForks=true**: Reuses JVMs between test classes (faster than restarting JVM)\n",
    "\n",
    "**JUnit 5 Parallel Configuration:**\n",
    "```properties\n",
    "# src/test/resources/junit-platform.properties\n",
    "junit.jupiter.execution.parallel.enabled=true\n",
    "junit.jupiter.execution.parallel.mode.default=concurrent\n",
    "junit.jupiter.execution.parallel.mode.classes.default=concurrent\n",
    "junit.jupiter.execution.parallel.config.strategy=dynamic\n",
    "junit.jupiter.execution.parallel.config.dynamic.factor=2\n",
    "```\n",
    "\n",
    "**Resource Locking (Prevent Test Interference):**\n",
    "```java\n",
    "@Test\n",
    "@ResourceLock(\"Database\")  // Only one test using \"Database\" runs at a time\n",
    "void testDatabaseOperation() {\n",
    "    // Modifies shared database state\n",
    "}\n",
    "\n",
    "@Test\n",
    "@ResourceLock(value = \"Database\", mode = ResourceAccessMode.READ)  \n",
    "void testDatabaseRead() {\n",
    "    // Can run concurrently with other READ locks\n",
    "}\n",
    "```\n",
    "\n",
    "### Pipeline Job Parallelization\n",
    "\n",
    "**GitHub Actions Matrix Strategy:**\n",
    "```yaml\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    strategy:\n",
    "      fail-fast: false\n",
    "      matrix:\n",
    "        service: [payment, order, inventory, user]\n",
    "        test-suite: [unit, integration]\n",
    "        exclude:\n",
    "          - service: inventory\n",
    "            test-suite: integration  # Skip slow integration tests for inventory\n",
    "    \n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      \n",
    "      - name: Test ${{ matrix.service }} - ${{ matrix.test-suite }}\n",
    "        run: |\n",
    "          cd services/${{ matrix.service }}\n",
    "          npm run test:${{ matrix.test-suite }}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The matrix creates 3 services × 2 suites = 6 parallel jobs (minus 1 exclusion). `fail-fast: false` ensures all jobs run even if one fails, useful for identifying which specific service/test combination broke.\n",
    "\n",
    "**DAG (Directed Acyclic Graph) Dependencies:**\n",
    "```yaml\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - run: ./mvnw package -DskipTests\n",
    "      - uses: actions/upload-artifact@v3\n",
    "        with:\n",
    "          name: jar-artifact\n",
    "          path: target/*.jar\n",
    "\n",
    "  test-unit:\n",
    "    needs: build  # Runs after build completes\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/download-artifact@v3\n",
    "        with:\n",
    "          name: jar-artifact\n",
    "      - run: ./mvnw test -Dtest=UnitTest\n",
    "\n",
    "  test-integration:\n",
    "    needs: build\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/download-artifact@v3\n",
    "        with:\n",
    "          name: jar-artifact\n",
    "      - run: ./mvnw test -Dtest=IntegrationTest\n",
    "\n",
    "  deploy:\n",
    "    needs: [test-unit, test-integration]  # Waits for both\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - run: echo \"Deploying...\"\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The DAG ensures efficient parallelization: build runs first, then unit and integration tests run in parallel (both need the build artifact), and deploy waits for both test jobs. This reduces total pipeline time from sequential (build → unit → integration → deploy) to build + max(unit, integration) + deploy.\n",
    "\n",
    "## 47.4 Docker Optimization\n",
    "\n",
    "### Multi-Stage Builds\n",
    "\n",
    "**Optimized Dockerfile:**\n",
    "```dockerfile\n",
    "# Stage 1: Build with all dependencies\n",
    "FROM eclipse-temurin:17-jdk-alpine AS builder\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy only dependency files first (better caching)\n",
    "COPY pom.xml .\n",
    "COPY src ./src\n",
    "\n",
    "# Download dependencies (cached layer)\n",
    "RUN ./mvnw dependency:go-offline -B\n",
    "\n",
    "# Build application\n",
    "RUN ./mvnw package -DskipTests -B\n",
    "\n",
    "# Stage 2: Runtime with JRE only\n",
    "FROM eclipse-temurin:17-jre-alpine AS runtime\n",
    "WORKDIR /app\n",
    "\n",
    "# Create non-root user\n",
    "RUN addgroup -S appgroup && adduser -S appuser -G appgroup\n",
    "\n",
    "# Copy only the JAR from builder\n",
    "COPY --from=builder /app/target/*.jar app.jar\n",
    "\n",
    "# Set ownership\n",
    "RUN chown -R appuser:appgroup /app\n",
    "USER appuser\n",
    "\n",
    "# Health check\n",
    "HEALTHCHECK --interval=30s --timeout=3s \\\n",
    "  CMD wget -q --spider http://localhost:8080/actuator/health || exit 1\n",
    "\n",
    "EXPOSE 8080\n",
    "ENTRYPOINT [\"java\", \"-jar\", \"app.jar\"]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Stage 1 (builder)**: Uses full JDK, includes Maven, compiles code. This layer is heavy but doesn't ship to production.\n",
    "- **Stage 2 (runtime)**: Uses slim JRE (~60MB vs ~200MB), no build tools, only runtime user. The final image contains only the JAR and JRE.\n",
    "- **Caching**: Copying `pom.xml` before `src` means dependency download is cached unless dependencies change, not on every code change.\n",
    "\n",
    "### Layer Ordering\n",
    "\n",
    "**Optimize Layer Cache Hits:**\n",
    "```dockerfile\n",
    "# BAD - Changes on every build\n",
    "COPY . /app\n",
    "RUN npm install\n",
    "RUN npm run build\n",
    "\n",
    "# GOOD - Stable layers first\n",
    "COPY package*.json /app/\n",
    "RUN npm ci --only=production  # Cached unless dependencies change\n",
    "\n",
    "COPY . /app\n",
    "RUN npm run build  # Only runs when code changes\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Docker caches layers based on the instruction and context hash. If `COPY . /app` comes before `npm install`, every code change invalidates the cache, forcing npm install to re-run. By copying only package files first, the expensive `npm ci` operation is cached and reused unless `package.json` changes.\n",
    "\n",
    "### BuildKit Features\n",
    "\n",
    "**Mount Cache (Persistent Caching Between Builds):**\n",
    "```dockerfile\n",
    "# syntax=docker/dockerfile:1\n",
    "FROM node:18-alpine AS builder\n",
    "WORKDIR /app\n",
    "\n",
    "COPY package*.json ./\n",
    "RUN --mount=type=cache,target=/root/.npm \\\n",
    "    npm ci\n",
    "\n",
    "COPY . .\n",
    "RUN --mount=type=cache,target=/app/.next/cache \\\n",
    "    npm run build\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "`--mount=type=cache` creates persistent cache directories that survive between builds. The npm cache is stored separately from layers, speeding up `npm ci`. The `.next/cache` directory (Next.js build cache) persists incremental build data.\n",
    "\n",
    "**SSH Mount (Private Dependencies):**\n",
    "```dockerfile\n",
    "RUN --mount=type=ssh,id=github \\\n",
    "    git clone git@github.com:company/private-repo.git\n",
    "```\n",
    "\n",
    "**Build Command:**\n",
    "```bash\n",
    "docker build --ssh default=${SSH_AUTH_SOCK} .\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "The SSH mount allows accessing private Git repositories during build without copying SSH keys into the image. The SSH agent socket is mounted temporarily during the RUN instruction, then discarded.\n",
    "\n",
    "## 47.5 Network Optimization\n",
    "\n",
    "### Artifact Repository Proximity\n",
    "\n",
    "**Regional Caching (Nexus/Artifactory):**\n",
    "```xml\n",
    "<!-- settings.xml -->\n",
    "<mirrors>\n",
    "  <mirror>\n",
    "    <id>company-nexus</id>\n",
    "    <name>Company Nexus - US-East</name>\n",
    "    <url>https://nexus.useast.company.com/repository/maven-public/</url>\n",
    "    <mirrorOf>central</mirrorOf>\n",
    "  </mirror>\n",
    "</mirrors>\n",
    "\n",
    "<profiles>\n",
    "  <profile>\n",
    "    <id>ci</id>\n",
    "    <properties>\n",
    "      <!-- Use regional mirror based on runner location -->\n",
    "      <nexus.url>https://nexus.${env.AWS_REGION}.company.com</nexus.url>\n",
    "    </properties>\n",
    "  </profile>\n",
    "</profiles>\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Place artifact repositories (Nexus, Artifactory, npm registries) in the same region as CI runners to reduce latency. A dependency download from Maven Central (US) to a runner in Europe adds 100-200ms latency per artifact; a regional mirror reduces this to <10ms.\n",
    "\n",
    "### Git Clone Optimization\n",
    "\n",
    "**Shallow Clones:**\n",
    "```yaml\n",
    "- uses: actions/checkout@v4\n",
    "  with:\n",
    "    fetch-depth: 0  # Full history (slow)\n",
    "\n",
    "- uses: actions/checkout@v4\n",
    "  with:\n",
    "    fetch-depth: 1  # Shallow clone (fast, only latest commit)\n",
    "```\n",
    "\n",
    "**Git LFS (Large File Storage) Optimization:**\n",
    "```yaml\n",
    "- uses: actions/checkout@v4\n",
    "  with:\n",
    "    lfs: true\n",
    "    fetch-depth: 1\n",
    "\n",
    "- name: Cache LFS\n",
    "  uses: actions/cache@v3\n",
    "  with:\n",
    "    path: .git/lfs\n",
    "    key: lfs-${{ hashFiles('.lfs-assets-id') }}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Shallow clones (`fetch-depth: 1`) download only the latest commit, reducing clone time from minutes to seconds for large repositories. However, this breaks operations requiring history (like `git describe` or comparing with base branch). Use fetch depth 0 only when necessary.\n",
    "\n",
    "## 47.6 Resource Right-Sizing\n",
    "\n",
    "### CPU and Memory Tuning\n",
    "\n",
    "**Maven Memory Settings:**\n",
    "```bash\n",
    "# Analyze memory usage first\n",
    "./mvnw package -DskipTests \\\n",
    "  -Dmaven.ext.class.path=/usr/share/maven/maven-profile-*.jar \\\n",
    "  -Dprofile=true\n",
    "\n",
    "# Then optimize\n",
    "export MAVEN_OPTS=\"-Xms1g -Xmx2g -XX:+UseG1GC -XX:MaxGCPauseMillis=200\"\n",
    "```\n",
    "\n",
    "**Node.js Memory:**\n",
    "```bash\n",
    "# Default is 512MB, increase for large builds\n",
    "node --max-old-space-size=4096 ./node_modules/.bin/ng build --prod\n",
    "```\n",
    "\n",
    "**Docker Resource Constraints:**\n",
    "```yaml\n",
    "# .github/workflows/build.yml\n",
    "jobs:\n",
    "  build:\n",
    "    runs-on: ubuntu-latest\n",
    "    container:\n",
    "      image: maven:3.9-eclipse-temurin-17\n",
    "      options: --cpus 4 --memory 8g\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Right-sizing prevents:\n",
    "- **Under-allocation**: OutOfMemory errors, excessive GC pauses, slow builds due to swapping\n",
    "- **Over-allocation**: Wasted costs, noisy neighbor problems in shared CI environments\n",
    "\n",
    "The `options` flag passes Docker resource constraints to the container. For CPU-intensive builds (compilation), ensure the CPU limit matches the parallelization (e.g., 4 CPUs for `-T 4` Maven builds).\n",
    "\n",
    "### Runner Sizing (GitHub Actions/GitLab CI)\n",
    "\n",
    "**Self-Hosted Runner Tiers:**\n",
    "```yaml\n",
    "# Small jobs (linting, unit tests)\n",
    "small-job:\n",
    "  runs-on: [self-hosted, small]  # 2 vCPU, 4GB RAM\n",
    "\n",
    "# Medium jobs (build)\n",
    "medium-job:\n",
    "  runs-on: [self-hosted, medium]  # 4 vCPU, 8GB RAM\n",
    "\n",
    "# Large jobs (integration tests with databases)\n",
    "large-job:\n",
    "  runs-on: [self-hosted, large]  # 8 vCPU, 16GB RAM\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Label runners by size and route jobs appropriately. Unit tests need minimal resources; integration tests with Docker Compose need large runners. This prevents small jobs from waiting for large runners while avoiding over-provisioning costs for simple tasks.\n",
    "\n",
    "## 47.7 Cost Optimization\n",
    "\n",
    "### Spot Instances for CI\n",
    "\n",
    "**AWS EC2 Spot Runners:**\n",
    "```yaml\n",
    "# GitHub Actions with EC2 Spot\n",
    "jobs:\n",
    "  start-runner:\n",
    "    runs-on: ubuntu-latest\n",
    "    outputs:\n",
    "      label: ${{ steps.start-ec2-runner.outputs.label }}\n",
    "      ec2-instance-id: ${{ steps.start-ec2-runner.outputs.ec2-instance-id }}\n",
    "    steps:\n",
    "      - name: Start EC2 runner\n",
    "        id: start-ec2-runner\n",
    "        uses: machulav/ec2-github-runner@v2\n",
    "        with:\n",
    "          mode: start\n",
    "          github-token: ${{ secrets.GH_PERSONAL_ACCESS_TOKEN }}\n",
    "          ec2-image-id: ami-1234567890abcdef0\n",
    "          ec2-instance-type: c5.4xlarge\n",
    "          subnet-id: subnet-1234567\n",
    "          security-group-id: sg-1234567\n",
    "          spot: true  # Use spot instances (60-90% cheaper)\n",
    "          spot-max-price: 0.50  # Maximum spot price willing to pay\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Spot instances provide significant cost savings (60-90%) for CI workloads, which are fault-tolerant (if the instance terminates, the job restarts on another runner). The `spot-max-price` sets a ceiling; if spot price exceeds this, the instance terminates.\n",
    "\n",
    "### Artifact Lifecycle Management\n",
    "\n",
    "**S3 Lifecycle Policies:**\n",
    "```yaml\n",
    "# Terraform configuration for artifact bucket\n",
    "resource \"aws_s3_bucket_lifecycle_configuration\" \"artifacts\" {\n",
    "  bucket = aws_s3_bucket.artifacts.id\n",
    "\n",
    "  rule {\n",
    "    id     = \"archive-old-builds\"\n",
    "    status = \"Enabled\"\n",
    "\n",
    "    filter {\n",
    "      prefix = \"builds/\"\n",
    "    }\n",
    "\n",
    "    transition {\n",
    "      days          = 30\n",
    "      storage_class = \"STANDARD_IA\"  # Infrequent Access (cheaper)\n",
    "    }\n",
    "\n",
    "    transition {\n",
    "      days          = 90\n",
    "      storage_class = \"GLACIER\"  # Archive (much cheaper)\n",
    "    }\n",
    "\n",
    "    expiration {\n",
    "      days = 365  # Delete after 1 year\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "CI artifacts (JARs, Docker images, test reports) accumulate rapidly. Lifecycle policies automatically move old artifacts to cheaper storage classes:\n",
    "- **Standard**: Frequent access (first 30 days)\n",
    "- **Standard-IA**: 40% cheaper, retrieval fee (30-90 days)\n",
    "- **Glacier**: 80% cheaper, slow retrieval (90+ days)\n",
    "- **Expiration**: Automatic deletion after 365 days for compliance\n",
    "\n",
    "### Docker Image Cleanup\n",
    "\n",
    "**Registry Garbage Collection:**\n",
    "```bash\n",
    "# GitLab Container Registry\n",
    "curl --request DELETE \\\n",
    "  --header \"PRIVATE-TOKEN: $TOKEN\" \\\n",
    "  \"https://gitlab.com/api/v4/projects/12345/registry/repositories/67/tags/old-tag\"\n",
    "\n",
    "# Retention policy (keep last 10 tags per image)\n",
    "docker run -it --rm \\\n",
    "  -e RETAIN_COUNT=10 \\\n",
    "  -e REGISTRY_URL=https://registry.company.com \\\n",
    "  -e REGISTRY_USER=$USER \\\n",
    "  -e REGISTRY_PASS=$PASS \\\n",
    "  registry-cleanup-tool\n",
    "```\n",
    "\n",
    "## 47.8 Test Execution Optimization\n",
    "\n",
    "### Test Impact Analysis\n",
    "\n",
    "**Running Only Affected Tests:**\n",
    "```bash\n",
    "# Git diff to find changed files\n",
    "CHANGED_FILES=$(git diff --name-only HEAD~1)\n",
    "\n",
    "# Run tests only for changed modules (Maven)\n",
    "./mvnw test -pl $(echo $CHANGED_FILES | grep -o 'services/[^/]*' | sort -u | tr '\\n' ',')\n",
    "\n",
    "# Jest only changed files\n",
    "npx jest --onlyChanged --changedSince=origin/main\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Rather than running the entire test suite (30+ minutes), run only tests related to changed code. Maven's `-pl` (projects list) flag limits testing to specific modules. Jest's `--onlyChanged` uses Git to determine which test files to run based on imports.\n",
    "\n",
    "### Parallel Test Execution with Database Sharding\n",
    "\n",
    "```java\n",
    "@TestExecutionListeners({\n",
    "    DependencyInjectionTestExecutionListener.class,\n",
    "    TransactionalTestExecutionListener.class\n",
    "})\n",
    "@SpringBootTest\n",
    "@TestPropertySource(properties = {\n",
    "    \"spring.datasource.url=jdbc:postgresql://localhost:5432/test_db_${random.uuid}\"\n",
    "})\n",
    "public class IntegrationTest {\n",
    "    // Each test class gets its own database schema\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Database contention limits test parallelization. By giving each test class (or thread) a unique database schema (using `${random.uuid}` or Testcontainers dynamic ports), tests run truly in parallel without lock contention. Cleanup drops the schema after tests complete.\n",
    "\n",
    "### Testcontainers Optimization\n",
    "\n",
    "```java\n",
    "@TestConfiguration\n",
    "public class TestcontainersConfig {\n",
    "    \n",
    "    // Reuse container across test classes (faster)\n",
    "    @Container\n",
    "    public static PostgreSQLContainer<?> postgres = new PostgreSQLContainer<>(\"postgres:15\")\n",
    "        .withDatabaseName(\"test\")\n",
    "        .withReuse(true);  // Enable container reuse\n",
    "    \n",
    "    @DynamicPropertySource\n",
    "    static void properties(DynamicPropertyRegistry registry) {\n",
    "        registry.add(\"spring.datasource.url\", postgres::getJdbcUrl);\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "Testcontainers start fresh containers for each test by default (slow). With `withReuse(true)` and Ryuk (resource reaper) disabled, containers survive between test classes if configuration matches, reducing startup time from 30s to <1s for subsequent tests.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "This chapter established performance optimization as a critical discipline for maintaining fast, cost-effective CI/CD pipelines. We examined bottleneck analysis techniques using timing wrappers and build scans to identify where pipeline time is actually spent, distinguishing between network-bound (dependency downloads), CPU-bound (compilation), and I/O-bound (testing) phases. Multi-layer caching strategies—including dependency caches keyed on lock files, Gradle build caches shared across CI runners, and Docker layer caches using BuildKit—eliminate redundant work by preserving outputs between builds.\n",
    "\n",
    "Parallel execution at multiple levels—test methods within JVMs, parallel CI jobs via matrix strategies, and DAG-based job orchestration—maximizes resource utilization while respecting dependencies. Docker optimization through multi-stage builds reduces final image sizes by 70-80% by discarding build tools, while intelligent layer ordering ensures dependency installation remains cached across code changes. Network optimization via regional artifact repositories and shallow Git clones reduces latency, and resource right-sizing ensures adequate memory and CPU for build tools without over-provisioning.\n",
    "\n",
    "Cost optimization strategies leverage spot instances for fault-tolerant CI workloads, S3 lifecycle policies for artifact archival, and test impact analysis to run only affected tests rather than full suites. Testcontainers reuse and database sharding enable parallel integration testing without contention, reducing test phase duration from hours to minutes.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Implement multi-layer caching: cache dependency downloads based on lock file hashes, use Gradle/Maven build caches for compiled outputs, and leverage Docker BuildKit cache mounts for package manager caches between builds.\n",
    "- Structure Dockerfiles to maximize layer caching: copy dependency manifests (package.json, pom.xml) before source code, install dependencies, then copy and build source; this ensures dependency layers remain cached unless dependencies change.\n",
    "- Use matrix builds and DAG dependencies to parallelize independent jobs, reducing wall-clock time from sequential sums to critical path duration (typically build + max(test stages) + deploy).\n",
    "- Right-size CI runners: provide 4-8 vCPUs for compilation (to utilize parallel Maven/Gradle builds), 16GB+ RAM for integration tests with Testcontainers, and use small runners for linting/documentation tasks.\n",
    "- Implement test impact analysis to run only tests affected by code changes in PR builds, reserving full test suites for main branch builds or nightly runs.\n",
    "- Use spot instances or preemptible VMs for CI workloads to reduce compute costs by 60-90%, implementing retry logic for jobs terminated due to spot preemption.\n",
    "\n",
    "**Next Chapter Preview:**\n",
    "Chapter 48: Security in CI/CD addresses the protection of software supply chains and delivery pipelines. We will explore supply chain security including Software Bills of Materials (SBOM) generation and verification, container image signing with Cosign and Sigstore, vulnerability scanning integration into pipelines, secrets management without hardcoding, and compliance frameworks for regulated industries. The chapter covers zero-trust pipeline architectures, artifact provenance verification, and automated security policy enforcement, ensuring that velocity does not compromise security posture."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
