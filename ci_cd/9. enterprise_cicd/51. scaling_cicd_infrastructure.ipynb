{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 51: Scaling CI/CD Infrastructure\n",
    "\n",
    "As organizations adopt CI/CD across hundreds of teams and thousands of repositories, the infrastructure supporting these pipelines becomes a critical production system in its own right. A CI/CD platform that cannot scale becomes the bottleneck constraining engineering velocity, causing queue delays, build failures due to resource starvation, and frustrated developers. This chapter treats CI/CD infrastructure as a first-class citizen, applying the same infrastructure-as-code, observability, and resilience patterns to Jenkins controllers, GitHub Actions runners, and Kubernetes build farms that we apply to production applications. We examine **horizontal scaling** through ephemeral build agents and autoscaling groups, **vertical scaling** for resource-intensive compilation and testing, **multi-region deployments** for disaster recovery and data sovereignty compliance, **high availability** configurations that eliminate single points of failure, **disaster recovery** strategies for pipeline state and build history, **capacity planning** methodologies to predict and prevent bottlenecks, and **cost optimization** techniques including spot instances, build caching, and rightsizing that prevent cloud bill shock while maintaining performance.\n",
    "\n",
    "## 51.1 Horizontal Scaling\n",
    "\n",
    "Horizontal scaling adds more build agents to distribute workload, rather than making individual agents larger. This approach provides elasticity, fault isolation, and cost efficiency through ephemeral infrastructure.\n",
    "\n",
    "### Kubernetes-Based Build Farms\n",
    "\n",
    "Running CI/CD agents on Kubernetes leverages cluster autoscaling for dynamic capacity.\n",
    "\n",
    "**Jenkins Kubernetes Plugin**:\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: jenkins-agent-config\n",
    "data:\n",
    "  config.yaml: |\n",
    "    jenkins:\n",
    "      clouds:\n",
    "        - kubernetes:\n",
    "            name: \"kubernetes\"\n",
    "            serverUrl: \"https://kubernetes.default\"\n",
    "            namespace: \"jenkins-agents\"\n",
    "            jenkinsUrl: \"http://jenkins:8080\"\n",
    "            jenkinsTunnel: \"jenkins-agent:50000\"\n",
    "            credentialsId: \"kubernetes-service-account\"\n",
    "            containerCapStr: \"100\"\n",
    "            retentionTimeout: 5\n",
    "            templates:\n",
    "              - name: \"default-agent\"\n",
    "                label: \"jenkins-agent\"\n",
    "                containers:\n",
    "                  - name: \"jnlp\"\n",
    "                    image: \"jenkins/inbound-agent:latest\"\n",
    "                    alwaysPullImage: true\n",
    "                    workingDir: \"/home/jenkins/agent\"\n",
    "                    resourceRequestCpu: \"500m\"\n",
    "                    resourceRequestMemory: \"512Mi\"\n",
    "                    resourceLimitCpu: \"2000m\"\n",
    "                    resourceLimitMemory: \"2Gi\"\n",
    "                volumes:\n",
    "                  - emptyDirVolume:\n",
    "                      memory: false\n",
    "                      mountPath: \"/tmp\"\n",
    "                  - persistentVolumeClaim:\n",
    "                      claimName: \"build-cache\"\n",
    "                      mountPath: \"/cache\"\n",
    "                      readOnly: false\n",
    "                yaml: |\n",
    "                  spec:\n",
    "                    affinity:\n",
    "                      podAntiAffinity:\n",
    "                        preferredDuringSchedulingIgnoredDuringExecution:\n",
    "                        - weight: 100\n",
    "                          podAffinityTerm:\n",
    "                            labelSelector:\n",
    "                              matchExpressions:\n",
    "                              - key: jenkins\n",
    "                                operator: In\n",
    "                                values:\n",
    "                                - slave\n",
    "                            topologyKey: kubernetes.io/hostname\n",
    "```\n",
    "\n",
    "**GitHub Actions Self-Hosted Runners on Kubernetes**:\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: github-actions-runner\n",
    "  namespace: actions-runners\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: github-runner\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: github-runner\n",
    "    spec:\n",
    "      serviceAccountName: github-runner\n",
    "      containers:\n",
    "      - name: runner\n",
    "        image: summerwind/actions-runner:latest\n",
    "        env:\n",
    "        - name: GITHUB_URL\n",
    "          value: https://github.com/myorg\n",
    "        - name: RUNNER_TOKEN\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: runner-token\n",
    "              key: token\n",
    "        - name: RUNNER_NAME\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.name\n",
    "        - name: RUNNER_WORKDIR\n",
    "          value: /tmp/github-runner\n",
    "        - name: LABELS\n",
    "          value: self-hosted,linux,x64,gpu  # Custom labels for job routing\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: \"2000m\"\n",
    "            memory: \"4Gi\"\n",
    "            nvidia.com/gpu: \"1\"  # GPU runners for ML workloads\n",
    "          limits:\n",
    "            cpu: \"4000m\"\n",
    "            memory: \"8Gi\"\n",
    "            nvidia.com/gpu: \"1\"\n",
    "        volumeMounts:\n",
    "        - name: docker-sock\n",
    "          mountPath: /var/run/docker.sock\n",
    "        - name: cache\n",
    "          mountPath: /cache\n",
    "      volumes:\n",
    "      - name: docker-sock\n",
    "        hostPath:\n",
    "          path: /var/run/docker.sock\n",
    "          type: Socket\n",
    "      - name: cache\n",
    "        persistentVolumeClaim:\n",
    "          claimName: runner-cache\n",
    "      affinity:\n",
    "        nodeAffinity:\n",
    "          requiredDuringSchedulingIgnoredDuringExecution:\n",
    "            nodeSelectorTerms:\n",
    "            - matchExpressions:\n",
    "              - key: node-type\n",
    "                operator: In\n",
    "                values:\n",
    "                - build-agent\n",
    "```\n",
    "\n",
    "### Autoscaling Strategies\n",
    "\n",
    "**Horizontal Pod Autoscaler (HPA)** for build agents:\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: github-runner-hpa\n",
    "  namespace: actions-runners\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: github-actions-runner\n",
    "  minReplicas: 3\n",
    "  maxReplicas: 50\n",
    "  metrics:\n",
    "  - type: External\n",
    "    external:\n",
    "      metric:\n",
    "        name: github_runner_queued_jobs\n",
    "        selector:\n",
    "          matchLabels:\n",
    "            repository: myorg/myrepo\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"1\"  # Scale up if any jobs queued\n",
    "  behavior:\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60\n",
    "      policies:\n",
    "      - type: Pods\n",
    "        value: 5\n",
    "        periodSeconds: 60  # Add 5 pods per minute\n",
    "    scaleDown:\n",
    "      stabilizationWindowSeconds: 300  # Wait 5 min before scaling down\n",
    "      policies:\n",
    "      - type: Pods\n",
    "        value: 2\n",
    "        periodSeconds: 120\n",
    "```\n",
    "\n",
    "**Cluster Autoscaler** for node scaling:\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: cluster-autoscaler\n",
    "  namespace: kube-system\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: cluster-autoscaler\n",
    "        image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.28.0\n",
    "        command:\n",
    "        - ./cluster-autoscaler\n",
    "        - --cloud-provider=aws\n",
    "        - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/my-cluster\n",
    "        - --balance-similar-node-groups\n",
    "        - --skip-nodes-with-system-pods=false\n",
    "        - --scale-down-delay-after-add=10m\n",
    "        - --scale-down-unneeded-time=10m\n",
    "        resources:\n",
    "          limits:\n",
    "            cpu: \"1000m\"\n",
    "            memory: \"1Gi\"\n",
    "```\n",
    "\n",
    "## 51.2 Vertical Scaling\n",
    "\n",
    "While horizontal scaling adds more workers, vertical scaling increases resources per worker for tasks that cannot be parallelized or require significant memory/CPU.\n",
    "\n",
    "### Resource-Intensive Builds\n",
    "\n",
    "**Machine Learning Training**:\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: ml-training-job\n",
    "spec:\n",
    "  containers:\n",
    "  - name: training\n",
    "    image: ml-training:latest\n",
    "    resources:\n",
    "      requests:\n",
    "        cpu: \"32\"\n",
    "        memory: \"128Gi\"\n",
    "        nvidia.com/gpu: \"4\"\n",
    "        ephemeral-storage: \"100Gi\"\n",
    "      limits:\n",
    "        cpu: \"32\"\n",
    "        memory: \"128Gi\"\n",
    "        nvidia.com/gpu: \"4\"\n",
    "        ephemeral-storage: \"200Gi\"\n",
    "    volumeMounts:\n",
    "    - name: dataset\n",
    "      mountPath: /data\n",
    "      readOnly: true\n",
    "    - name: model-output\n",
    "      mountPath: /output\n",
    "  volumes:\n",
    "  - name: dataset\n",
    "    persistentVolumeClaim:\n",
    "      claimName: training-dataset-100tb\n",
    "  - name: model-output\n",
    "    persistentVolumeClaim:\n",
    "      claimName: model-artifacts\n",
    "  nodeSelector:\n",
    "    node-type: gpu-high-memory\n",
    "  tolerations:\n",
    "  - key: \"nvidia.com/gpu\"\n",
    "    operator: \"Exists\"\n",
    "    effect: \"NoSchedule\"\n",
    "```\n",
    "\n",
    "**Large Compilation Jobs** (C++, Rust):\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: rust-build\n",
    "spec:\n",
    "  containers:\n",
    "  - name: builder\n",
    "    image: rust:1.75\n",
    "    command: [\"cargo\", \"build\", \"--release\"]\n",
    "    resources:\n",
    "      requests:\n",
    "        cpu: \"16\"\n",
    "        memory: \"32Gi\"\n",
    "        ephemeral-storage: \"50Gi\"\n",
    "      limits:\n",
    "        cpu: \"16\"\n",
    "        memory: \"32Gi\"\n",
    "        ephemeral-storage: \"100Gi\"\n",
    "    env:\n",
    "    - name: CARGO_HOME\n",
    "      value: /cache/cargo\n",
    "    - name: CARGO_TARGET_DIR\n",
    "      value: /cache/target\n",
    "    volumeMounts:\n",
    "    - name: cache\n",
    "      mountPath: /cache\n",
    "  volumes:\n",
    "  - name: cache\n",
    "    persistentVolumeClaim:\n",
    "      claimName: build-cache-ssd\n",
    "  nodeSelector:\n",
    "    node-type: high-cpu\n",
    "```\n",
    "\n",
    "### Node Pools for Specialized Workloads\n",
    "\n",
    "**AWS EKS Node Groups**:\n",
    "```yaml\n",
    "apiVersion: eksctl.io/v1alpha5\n",
    "kind: ClusterConfig\n",
    "metadata:\n",
    "  name: ci-cluster\n",
    "  region: us-east-1\n",
    "nodeGroups:\n",
    "  - name: general-build\n",
    "    instanceType: m6i.2xlarge\n",
    "    desiredCapacity: 3\n",
    "    minSize: 1\n",
    "    maxSize: 20\n",
    "    volumeSize: 100\n",
    "    labels:\n",
    "      node-type: general\n",
    "    tags:\n",
    "      k8s.io/cluster-autoscaler/enabled: \"true\"\n",
    "      \n",
    "  - name: high-memory\n",
    "    instanceType: r6i.8xlarge  # 256GB RAM\n",
    "    desiredCapacity: 0\n",
    "    minSize: 0\n",
    "    maxSize: 10\n",
    "    volumeSize: 500\n",
    "    labels:\n",
    "      node-type: high-memory\n",
    "    taints:\n",
    "      - key: dedicated\n",
    "        value: memory\n",
    "        effect: NoSchedule\n",
    "    tags:\n",
    "      k8s.io/cluster-autoscaler/enabled: \"true\"\n",
    "      \n",
    "  - name: gpu-build\n",
    "    instanceType: p4d.24xlarge  # 8x A100 GPUs\n",
    "    desiredCapacity: 0\n",
    "    minSize: 0\n",
    "    maxSize: 5\n",
    "    volumeSize: 1000\n",
    "    labels:\n",
    "      node-type: gpu-high-memory\n",
    "      nvidia.com/gpu.present: \"true\"\n",
    "    taints:\n",
    "      - key: nvidia.com/gpu\n",
    "        value: \"true\"\n",
    "        effect: NoSchedule\n",
    "    tags:\n",
    "      k8s.io/cluster-autoscaler/enabled: \"true\"\n",
    "```\n",
    "\n",
    "## 51.3 Multi-Region Deployments\n",
    "\n",
    "Geographic distribution ensures disaster recovery, data residency compliance, and reduced latency for distributed teams.\n",
    "\n",
    "### Disaster Recovery Strategy\n",
    "\n",
    "**Active-Passive** (Cost-efficient):\n",
    "- Primary region handles all traffic\n",
    "- Secondary region has infrastructure provisioned but scaled to zero or minimum\n",
    "- RTO (Recovery Time Objective): 15-30 minutes (time to scale up)\n",
    "- RPO (Recovery Point Objective): 5 minutes (data loss window)\n",
    "\n",
    "**Active-Active** (High availability):\n",
    "- Both regions serve traffic (geo-DNS or global load balancer)\n",
    "- Continuous synchronization\n",
    "- RTO: Near zero (automatic failover)\n",
    "- RPO: Near zero (synchronous replication where possible)\n",
    "\n",
    "### Infrastructure Replication\n",
    "\n",
    "**Terraform Multi-Region**:\n",
    "```hcl\n",
    "# regions.tf\n",
    "locals {\n",
    "  regions = {\n",
    "    primary   = \"us-east-1\"\n",
    "    secondary = \"us-west-2\"\n",
    "  }\n",
    "}\n",
    "\n",
    "module \"ci_infrastructure\" {\n",
    "  for_each = local.regions\n",
    "  \n",
    "  source = \"./modules/ci-cd\"\n",
    "  \n",
    "  region       = each.value\n",
    "  environment  = each.key\n",
    "  is_primary   = each.key == \"primary\"\n",
    "  \n",
    "  # DR configuration\n",
    "  backup_vault_arn = each.key == \"primary\" ? aws_backup_vault.primary.arn : null\n",
    "  \n",
    "  providers = {\n",
    "    aws = aws[each.key]\n",
    "  }\n",
    "}\n",
    "\n",
    "provider \"aws\" {\n",
    "  alias  = \"primary\"\n",
    "  region = \"us-east-1\"\n",
    "}\n",
    "\n",
    "provider \"aws\" {\n",
    "  alias  = \"secondary\"\n",
    "  region = \"us-west-2\"\n",
    "}\n",
    "```\n",
    "\n",
    "**Cross-Region Replication**:\n",
    "```yaml\n",
    "# ArgoCD ApplicationSet for multi-region deployment\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: ApplicationSet\n",
    "metadata:\n",
    "  name: ci-cd-platform\n",
    "spec:\n",
    "  generators:\n",
    "  - list:\n",
    "      elements:\n",
    "      - cluster: production-us-east-1\n",
    "        url: https://prod-us-east-1.api.internal\n",
    "        region: us-east-1\n",
    "        weight: 100\n",
    "      - cluster: production-us-west-2\n",
    "        url: https://prod-us-west-2.api.internal\n",
    "        region: us-west-2\n",
    "        weight: 0  # Standby, weight 0 until failover\n",
    "  template:\n",
    "    metadata:\n",
    "      name: '{{cluster}}-ci-platform'\n",
    "    spec:\n",
    "      project: infrastructure\n",
    "      source:\n",
    "        repoURL: https://github.com/company/gitops.git\n",
    "        targetRevision: HEAD\n",
    "        path: infrastructure/ci-cd\n",
    "        helm:\n",
    "          values: |\n",
    "            region: {{region}}\n",
    "            replicaCount: {{weight}}\n",
    "            backup:\n",
    "              enabled: true\n",
    "              crossRegionTarget: {{region}}\n",
    "      destination:\n",
    "        server: '{{url}}'\n",
    "        namespace: ci-cd\n",
    "      syncPolicy:\n",
    "        automated:\n",
    "          prune: true\n",
    "          selfHeal: true\n",
    "```\n",
    "\n",
    "### Data Residency and Sovereignty\n",
    "\n",
    "**Geo-Fencing Deployments**:\n",
    "```yaml\n",
    "# Ensure EU data stays in EU\n",
    "apiVersion: constraints.gatekeeper.sh/v1beta1\n",
    "kind: K8sRequiredLabels\n",
    "metadata:\n",
    "  name: require-data-residency\n",
    "spec:\n",
    "  match:\n",
    "    kinds:\n",
    "      - apiGroups: [\"\"]\n",
    "        kinds: [\"Pod\"]\n",
    "    namespaces: [\"eu-*\"]\n",
    "  parameters:\n",
    "    labels:\n",
    "      - \"data-residency/eu-only\"\n",
    "      - \"compliance.company.com/gdpr\"\n",
    "---\n",
    "# Pod spec with node affinity for EU\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: eu-data-processor\n",
    "  labels:\n",
    "    data-residency/eu-only: \"true\"\n",
    "spec:\n",
    "  affinity:\n",
    "    nodeAffinity:\n",
    "      requiredDuringSchedulingIgnoredDuringExecution:\n",
    "        nodeSelectorTerms:\n",
    "        - matchExpressions:\n",
    "          - key: topology.kubernetes.io/region\n",
    "            operator: In\n",
    "            values:\n",
    "            - eu-west-1\n",
    "            - eu-central-1\n",
    "  tolerations:\n",
    "  - key: \"dedicated\"\n",
    "    operator: \"Equal\"\n",
    "    value: \"eu-only\"\n",
    "    effect: \"NoSchedule\"\n",
    "```\n",
    "\n",
    "## 51.4 High Availability\n",
    "\n",
    "The CI/CD control plane itself must be resilient to node failures, zone outages, and regional disasters.\n",
    "\n",
    "### Jenkins High Availability\n",
    "\n",
    "**Active-Standby with Shared Storage**:\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: StatefulSet\n",
    "metadata:\n",
    "  name: jenkins\n",
    "spec:\n",
    "  serviceName: jenkins\n",
    "  replicas: 1  # Jenkins doesn't support active-active\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: jenkins\n",
    "        image: jenkins/jenkins:lts\n",
    "        ports:\n",
    "        - containerPort: 8080\n",
    "        volumeMounts:\n",
    "        - name: jenkins-home\n",
    "          mountPath: /var/jenkins_home\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "          limits:\n",
    "            memory: \"8Gi\"\n",
    "            cpu: \"4000m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /login\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 90\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /login\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 60\n",
    "          periodSeconds: 5\n",
    "  volumeClaimTemplates:\n",
    "  - metadata:\n",
    "      name: jenkins-home\n",
    "    spec:\n",
    "      accessModes: [\"ReadWriteOnce\"]\n",
    "      storageClassName: fast-ssd\n",
    "      resources:\n",
    "        requests:\n",
    "          storage: 100Gi\n",
    "---\n",
    "# High availability through rapid failover\n",
    "apiVersion: policy/v1\n",
    "kind: PodDisruptionBudget\n",
    "metadata:\n",
    "  name: jenkins-pdb\n",
    "spec:\n",
    "  minAvailable: 1\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: jenkins\n",
    "```\n",
    "\n",
    "**External Storage for Build History**:\n",
    "```yaml\n",
    "# Mount S3 for build artifacts instead of local storage\n",
    "- name: jenkins\n",
    "  image: jenkins/jenkins:lts\n",
    "  env:\n",
    "  - name: JAVA_OPTS\n",
    "    value: \"-Djenkins.model.Jenkins.slaveAgentPort=50000\"\n",
    "  volumeMounts:\n",
    "  - name: s3-artifacts\n",
    "    mountPath: /var/jenkins_home/jobs\n",
    "  volumes:\n",
    "  - name: s3-artifacts\n",
    "    csi:\n",
    "      driver: s3.csi.aws.com\n",
    "      volumeAttributes:\n",
    "        bucketName: jenkins-artifacts-prod\n",
    "        region: us-east-1\n",
    "```\n",
    "\n",
    "### GitHub Actions HA\n",
    "\n",
    "**Multi-Runner Architecture**:\n",
    "```yaml\n",
    "# Terraform for GitHub Actions Runner Controller (ARC)\n",
    "resource \"helm_release\" \"actions_runner_controller\" {\n",
    "  name       = \"actions-runner-controller\"\n",
    "  repository = \"https://actions-runner-controller.github.io/actions-runner-controller\"\n",
    "  chart      = \"actions-runner-controller\"\n",
    "  namespace  = \"actions-runner-system\"\n",
    "  \n",
    "  set {\n",
    "    name  = \"authSecret.github_token\"\n",
    "    value = var.github_token\n",
    "  }\n",
    "  \n",
    "  set {\n",
    "    name  = \"replicaCount\"\n",
    "    value = \"2\"  # HA for controller itself\n",
    "  }\n",
    "}\n",
    "\n",
    "# Runner Deployment with autoscaling\n",
    "resource \"kubernetes_manifest\" \"runner_deployment\" {\n",
    "  manifest = {\n",
    "    apiVersion = \"actions.summerwind.dev/v1alpha1\"\n",
    "    kind       = \"RunnerDeployment\"\n",
    "    metadata = {\n",
    "      name      = \"production-runners\"\n",
    "      namespace = \"actions-runner-system\"\n",
    "    }\n",
    "    spec = {\n",
    "      template = {\n",
    "        spec = {\n",
    "          repository = \"myorg/myrepo\"\n",
    "          labels     = [\"self-hosted\", \"production\", \"large\"]\n",
    "          resources = {\n",
    "            limits = {\n",
    "              cpu    = \"4000m\"\n",
    "              memory = \"16Gi\"\n",
    "            }\n",
    "            requests = {\n",
    "              cpu    = \"1000m\"\n",
    "              memory = \"4Gi\"\n",
    "            }\n",
    "          }\n",
    "          # Ephemeral runners for security\n",
    "          ephemeral = true\n",
    "        }\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# HorizontalRunnerAutoscaler\n",
    "resource \"kubernetes_manifest\" \"runner_autoscaler\" {\n",
    "  manifest = {\n",
    "    apiVersion = \"actions.summerwind.dev/v1alpha1\"\n",
    "    kind       = \"HorizontalRunnerAutoscaler\"\n",
    "    metadata = {\n",
    "      name      = \"production-runners-autoscaler\"\n",
    "      namespace = \"actions-runner-system\"\n",
    "    }\n",
    "    spec = {\n",
    "      scaleTargetRef = {\n",
    "        name = \"production-runners\"\n",
    "      }\n",
    "      minReplicas = 3\n",
    "      maxReplicas = 50\n",
    "      metrics = [\n",
    "        {\n",
    "          type = \"TotalNumberOfQueuedAndInProgressWorkflowRuns\"\n",
    "          repositoryNames = [\"myorg/myrepo\"]\n",
    "        }\n",
    "      ]\n",
    "      scaleUpTriggers = [\n",
    "        {\n",
    "          githubEvent = {\n",
    "            checkRun = {\n",
    "              types = [\"created\"]\n",
    "              status = \"queued\"\n",
    "            }\n",
    "          }\n",
    "          duration = \"5m\"\n",
    "        }\n",
    "      ]\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 51.5 Disaster Recovery\n",
    "\n",
    "CI/CD infrastructure is critical path; its loss paralyzes all software delivery. DR strategies must account for both the control plane (Jenkins, GitLab, ArgoCD) and the data (build history, artifacts, pipeline state).\n",
    "\n",
    "### Backup Strategies\n",
    "\n",
    "**Jenkins Backup**:\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: jenkins-backup\n",
    "spec:\n",
    "  schedule: \"0 */6 * * *\"  # Every 6 hours\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: backup\n",
    "            image: amazon/aws-cli:latest\n",
    "            command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "            - |\n",
    "              # Create timestamped backup\n",
    "              TIMESTAMP=$(date +%Y%m%d-%H%M%S)\n",
    "              \n",
    "              # Backup Jenkins home (excluding workspace for size)\n",
    "              tar -czf /tmp/jenkins-backup-${TIMESTAMP}.tar.gz \\\n",
    "                --exclude='./workspace' \\\n",
    "                --exclude='./war' \\\n",
    "                /var/jenkins_home\n",
    "              \n",
    "              # Upload to S3 with versioning\n",
    "              aws s3 cp /tmp/jenkins-backup-${TIMESTAMP}.tar.gz \\\n",
    "                s3://jenkins-backups-${AWS_REGION}/backups/\n",
    "              \n",
    "              # Verify backup integrity\n",
    "              aws s3api head-object \\\n",
    "                --bucket jenkins-backups-${AWS_REGION} \\\n",
    "                --key backups/jenkins-backup-${TIMESTAMP}.tar.gz\n",
    "                \n",
    "              # Cleanup old backups (keep 30 days)\n",
    "              aws s3 ls s3://jenkins-backups-${AWS_REGION}/backups/ | \\\n",
    "                awk '{print $4}' | \\\n",
    "                while read file; do\n",
    "                  DATE=$(echo $file | grep -oP '\\d{8}-\\d{6}')\n",
    "                  if [ $(($(date +%s) - $(date -d \"$DATE\" +%s))) -gt 2592000 ]; then\n",
    "                    aws s3 rm s3://jenkins-backups-${AWS_REGION}/backups/$file\n",
    "                  fi\n",
    "                done\n",
    "            env:\n",
    "            - name: AWS_REGION\n",
    "              value: us-east-1\n",
    "            volumeMounts:\n",
    "            - name: jenkins-home\n",
    "              mountPath: /var/jenkins_home\n",
    "              readOnly: true\n",
    "          volumes:\n",
    "          - name: jenkins-home\n",
    "            persistentVolumeClaim:\n",
    "              claimName: jenkins-home\n",
    "          restartPolicy: OnFailure\n",
    "```\n",
    "\n",
    "**ArgoCD Backup**:\n",
    "```bash\n",
    "# Export ArgoCD applications and projects\n",
    "argocd admin export > argocd-backup-$(date +%Y%m%d).yaml\n",
    "\n",
    "# Include secrets (encrypted)\n",
    "kubectl get secret -n argocd -o yaml >> argocd-backup-$(date +%Y%m%d).yaml\n",
    "\n",
    "# Store in versioned S3\n",
    "aws s3 cp argocd-backup-$(date +%Y%m%d).yaml s3://argocd-backups/\n",
    "```\n",
    "\n",
    "### Recovery Procedures\n",
    "\n",
    "**Jenkins Restore**:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# restore-jenkins.sh\n",
    "\n",
    "BACKUP_FILE=$1\n",
    "NAMESPACE=\"jenkins\"\n",
    "\n",
    "# Scale down Jenkins\n",
    "kubectl scale statefulset jenkins --replicas=0 -n $NAMESPACE\n",
    "\n",
    "# Wait for termination\n",
    "kubectl wait --for=delete pod/jenkins-0 --timeout=300s -n $NAMESPACE\n",
    "\n",
    "# Restore PVC\n",
    "kubectl delete pvc jenkins-home -n $NAMESPACE\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: jenkins-home\n",
    "  namespace: $NAMESPACE\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteOnce\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 100Gi\n",
    "  storageClassName: fast-ssd\n",
    "EOF\n",
    "\n",
    "# Create restore job\n",
    "cat <<EOF | kubectl apply -f -\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: jenkins-restore\n",
    "  namespace: $NAMESPACE\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: restore\n",
    "        image: amazon/aws-cli:latest\n",
    "        command:\n",
    "        - /bin/sh\n",
    "        - -c\n",
    "        - |\n",
    "          aws s3 cp s3://jenkins-backups/${BACKUP_FILE} /tmp/backup.tar.gz\n",
    "          tar -xzf /tmp/backup.tar.gz -C /var/jenkins_home\n",
    "          chown -R 1000:1000 /var/jenkins_home\n",
    "        volumeMounts:\n",
    "        - name: jenkins-home\n",
    "          mountPath: /var/jenkins_home\n",
    "      volumes:\n",
    "      - name: jenkins-home\n",
    "        persistentVolumeClaim:\n",
    "          claimName: jenkins-home\n",
    "      restartPolicy: Never\n",
    "EOF\n",
    "\n",
    "# Wait for restore\n",
    "kubectl wait --for=condition=complete job/jenkins-restore -n $NAMESPACE --timeout=600s\n",
    "\n",
    "# Scale up Jenkins\n",
    "kubectl scale statefulset jenkins --replicas=1 -n $NAMESPACE\n",
    "```\n",
    "\n",
    "## 51.6 Capacity Planning\n",
    "\n",
    "Preventing bottlenecks requires understanding pipeline demand patterns and provisioning capacity accordingly.\n",
    "\n",
    "### Metrics Collection\n",
    "\n",
    "**Pipeline Metrics**:\n",
    "```yaml\n",
    "# Prometheus metrics for Jenkins\n",
    "- name: jenkins-metrics\n",
    "  image: prometheus/jenkins-exporter:latest\n",
    "  env:\n",
    "  - name: JENKINS_URL\n",
    "    value: http://jenkins:8080\n",
    "  - name: JENKINS_USER\n",
    "    value: metrics\n",
    "  - name: JENKINS_API_TOKEN\n",
    "    valueFrom:\n",
    "      secretKeyRef:\n",
    "        name: jenkins-metrics\n",
    "        key: token\n",
    "  ports:\n",
    "  - containerPort: 9118\n",
    "    name: metrics\n",
    "```\n",
    "\n",
    "**Key Metrics to Track**:\n",
    "- Queue depth (number of jobs waiting)\n",
    "- Build duration trends\n",
    "- Agent utilization percentage\n",
    "- Failure rate by agent type\n",
    "- Cost per build\n",
    "- Cache hit rates\n",
    "\n",
    "### Predictive Scaling\n",
    "\n",
    "**Machine Learning-Based Prediction**:\n",
    "```python\n",
    "# capacity_predictor.py\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "import boto3\n",
    "\n",
    "def predict_required_agents():\n",
    "    # Load historical data\n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "    \n",
    "    # Get queue depth history\n",
    "    response = cloudwatch.get_metric_statistics(\n",
    "        Namespace='CI/CD',\n",
    "        MetricName='QueueDepth',\n",
    "        StartTime=datetime.utcnow() - timedelta(days=30),\n",
    "        EndTime=datetime.utcnow(),\n",
    "        Period=3600,\n",
    "        Statistics=['Average']\n",
    "    )\n",
    "    \n",
    "    df = pd.DataFrame(response['Datapoints'])\n",
    "    \n",
    "    # Feature engineering: time of day, day of week\n",
    "    df['hour'] = pd.to_datetime(df['Timestamp']).dt.hour\n",
    "    df['day_of_week'] = pd.to_datetime(df['Timestamp']).dt.dayofweek\n",
    "    \n",
    "    # Train model\n",
    "    X = df[['hour', 'day_of_week', 'Average']]\n",
    "    y = df['RequiredAgents']  # Historical data on agents needed\n",
    "    \n",
    "    model = LinearRegression()\n",
    "    model.fit(X, y)\n",
    "    \n",
    "    # Predict next hour\n",
    "    next_hour = datetime.utcnow().hour + 1\n",
    "    prediction = model.predict([[next_hour, datetime.utcnow().weekday(), 0]])\n",
    "    \n",
    "    # Scale agents\n",
    "    if prediction > current_agents:\n",
    "        scale_agents(int(prediction))\n",
    "    \n",
    "    return prediction\n",
    "```\n",
    "\n",
    "### Queue Management\n",
    "\n",
    "**Priority Queues**:\n",
    "```groovy\n",
    "// Jenkins Pipeline: Priority based on environment\n",
    "properties([\n",
    "  parameters([\n",
    "    choice(\n",
    "      name: 'ENVIRONMENT',\n",
    "      choices: ['development', 'staging', 'production'],\n",
    "      description: 'Deployment target'\n",
    "    )\n",
    "  ])\n",
    "])\n",
    "\n",
    "// Set priority based on environment\n",
    "if (params.ENVIRONMENT == 'production') {\n",
    "  currentBuild.setPriority(1)  // Highest\n",
    "} else if (params.ENVIRONMENT == 'staging') {\n",
    "  currentBuild.setPriority(5)\n",
    "} else {\n",
    "  currentBuild.setPriority(10) // Lowest\n",
    "}\n",
    "```\n",
    "\n",
    "**Resource Quotas per Team**:\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ResourceQuota\n",
    "metadata:\n",
    "  name: team-alpha-quota\n",
    "  namespace: team-alpha\n",
    "spec:\n",
    "  hard:\n",
    "    requests.cpu: \"100\"\n",
    "    requests.memory: 400Gi\n",
    "    pods: \"50\"\n",
    "    # CI/CD specific\n",
    "    count/jobs.batch: \"100\"\n",
    "    count/pods: \"100\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: LimitRange\n",
    "metadata:\n",
    "  name: team-alpha-limits\n",
    "  namespace: team-alpha\n",
    "spec:\n",
    "  limits:\n",
    "  - default:\n",
    "      cpu: \"2000m\"\n",
    "      memory: \"4Gi\"\n",
    "    defaultRequest:\n",
    "      cpu: \"500m\"\n",
    "      memory: \"1Gi\"\n",
    "    type: Container\n",
    "  - max:\n",
    "      cpu: \"8000m\"\n",
    "      memory: \"32Gi\"\n",
    "    min:\n",
    "      cpu: \"100m\"\n",
    "      memory: \"128Mi\"\n",
    "    type: Pod\n",
    "```\n",
    "\n",
    "## 51.7 Cost Optimization\n",
    "\n",
    "CI/CD infrastructure can become expensive without governance. Optimization requires right-sizing, spot instances, and intelligent caching.\n",
    "\n",
    "### Spot Instances and Preemptible VMs\n",
    "\n",
    "**AWS Spot for Build Agents**:\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: spot-runners\n",
    "spec:\n",
    "  replicas: 10\n",
    "  template:\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        node-type: spot\n",
    "      tolerations:\n",
    "      - key: \"spot\"\n",
    "        operator: \"Equal\"\n",
    "        value: \"true\"\n",
    "        effect: \"NoSchedule\"\n",
    "      containers:\n",
    "      - name: runner\n",
    "        image: github-runner:latest\n",
    "        resources:\n",
    "          requests:\n",
    "            cpu: \"4000m\"\n",
    "            memory: \"8Gi\"\n",
    "          limits:\n",
    "            cpu: \"4000m\"\n",
    "            memory: \"8Gi\"\n",
    "        env:\n",
    "        - name: RUNNER_NAME\n",
    "          valueFrom:\n",
    "            fieldRef:\n",
    "              fieldPath: metadata.name\n",
    "        - name: RUNNER_TOKEN\n",
    "          valueFrom:\n",
    "            secretKeyRef:\n",
    "              name: runner-token\n",
    "              key: token\n",
    "        # Handle spot interruption\n",
    "        lifecycle:\n",
    "          preStop:\n",
    "            exec:\n",
    "              command:\n",
    "              - /bin/sh\n",
    "              - -c\n",
    "              - |\n",
    "                # Signal GitHub that runner is going away\n",
    "                curl -X POST \\\n",
    "                  -H \"Authorization: token ${RUNNER_TOKEN}\" \\\n",
    "                  https://api.github.com/repos/myorg/myrepo/actions/runners/${RUNNER_NAME}/remove\n",
    "                \n",
    "                # Finish current job if possible (graceful shutdown)\n",
    "                /actions-runner/bin/Runner.Listener remove --token ${RUNNER_TOKEN}\n",
    "```\n",
    "\n",
    "**Karpenter for Spot Optimization**:\n",
    "```yaml\n",
    "apiVersion: karpenter.sh/v1beta1\n",
    "kind: NodePool\n",
    "metadata:\n",
    "  name: spot-builders\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      requirements:\n",
    "      - key: karpenter.sh/capacity-type\n",
    "        operator: In\n",
    "        values: [\"spot\"]\n",
    "      - key: node.kubernetes.io/instance-type\n",
    "        operator: In\n",
    "        values: [\"m6i.2xlarge\", \"m6i.4xlarge\", \"c6i.2xlarge\"]\n",
    "      - key: topology.kubernetes.io/zone\n",
    "        operator: In\n",
    "        values: [\"us-east-1a\", \"us-east-1b\", \"us-east-1c\"]\n",
    "  limits:\n",
    "    cpu: 1000\n",
    "    memory: 4000Gi\n",
    "  disruption:\n",
    "    consolidationPolicy: WhenUnderutilized\n",
    "    expireAfter: 720h  # 30 days max node lifetime\n",
    "```\n",
    "\n",
    "### Build Caching\n",
    "\n",
    "**Layer Caching for Docker**:\n",
    "```yaml\n",
    "# Kaniko with cache\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: kaniko-build\n",
    "spec:\n",
    "  containers:\n",
    "  - name: kaniko\n",
    "    image: gcr.io/kaniko-project/executor:latest\n",
    "    args:\n",
    "    - --dockerfile=Dockerfile\n",
    "    - --context=git://github.com/myorg/myrepo\n",
    "    - --destination=myregistry/myapp:${COMMIT_SHA}\n",
    "    - --cache=true\n",
    "    - --cache-repo=myregistry/cache\n",
    "    - --cache-copy-layers=true\n",
    "    - --cache-run-layers=true\n",
    "    volumeMounts:\n",
    "    - name: docker-config\n",
    "      mountPath: /kaniko/.docker\n",
    "  volumes:\n",
    "  - name: docker-config\n",
    "    secret:\n",
    "      secretName: registry-credentials\n",
    "```\n",
    "\n",
    "**Dependency Caching**:\n",
    "```yaml\n",
    "# Persistent cache volume for Maven/Gradle/NPM\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: build-cache\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadWriteMany  # Shared across builds\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 500Gi\n",
    "  storageClassName: efs-sc  # AWS EFS for multi-AZ access\n",
    "---\n",
    "# Build pod using cache\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: maven-build\n",
    "spec:\n",
    "  containers:\n",
    "  - name: maven\n",
    "    image: maven:3.9-eclipse-temurin-17\n",
    "    command: [\"mvn\", \"clean\", \"package\"]\n",
    "    volumeMounts:\n",
    "    - name: m2-cache\n",
    "      mountPath: /root/.m2\n",
    "    env:\n",
    "    - name: MAVEN_OPTS\n",
    "      value: \"-Dmaven.repo.local=/root/.m2/repository\"\n",
    "  volumes:\n",
    "  - name: m2-cache\n",
    "    persistentVolumeClaim:\n",
    "      claimName: build-cache\n",
    "```\n",
    "\n",
    "### Cost Monitoring\n",
    "\n",
    "**KubeCost / OpenCost**:\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: kubecost-config\n",
    "data:\n",
    "  kubecost.yaml: |\n",
    "    # Allocation for CI/CD namespaces\n",
    "    allocations:\n",
    "      filters:\n",
    "        namespaces:\n",
    "          - ci-cd\n",
    "          - jenkins\n",
    "          - github-actions\n",
    "          - gitlab-runner\n",
    "      aggregation: namespace\n",
    "      accumulate: true\n",
    "      \n",
    "    # Alerts for cost anomalies\n",
    "    alerts:\n",
    "      - type: budget\n",
    "        threshold: 10000  # $10k/day\n",
    "        window: daily\n",
    "        aggregation: namespace\n",
    "        filter: ci-cd\n",
    "        \n",
    "      - type: efficiency\n",
    "        threshold: 0.20  # 20% resource utilization\n",
    "        window: 7d\n",
    "        aggregation: controller\n",
    "```\n",
    "\n",
    "**Cost Optimization Rules**:\n",
    "```yaml\n",
    "# Delete completed pods after 1 hour\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: cleanup-completed-builds\n",
    "spec:\n",
    "  schedule: \"0 * * * *\"\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: cleanup\n",
    "            image: bitnami/kubectl:latest\n",
    "            command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "            - |\n",
    "              # Delete completed/failed pods older than 1 hour\n",
    "              kubectl get pods --all-namespaces --field-selector=status.phase=Succeeded \\\n",
    "                -o json | jq -r '.items[] | select(.metadata.creationTimestamp | fromdateiso8601 < now - 3600) | \"\\(.metadata.namespace) \\(.metadata.name)\"' | \\\n",
    "                while read ns pod; do\n",
    "                  kubectl delete pod $pod -n $ns\n",
    "                done\n",
    "                \n",
    "              # Delete failed pods older than 24 hours\n",
    "              kubectl get pods --all-namespaces --field-selector=status.phase=Failed \\\n",
    "                -o json | jq -r '.items[] | select(.metadata.creationTimestamp | fromdateiso8601 < now - 86400) | \"\\(.metadata.namespace) \\(.metadata.name)\"' | \\\n",
    "                while read ns pod; do\n",
    "                  kubectl delete pod $pod -n $ns\n",
    "                done\n",
    "          restartPolicy: OnFailure\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "This chapter addressed the operational challenges of scaling CI/CD infrastructure from a single team tool to an enterprise platform serving hundreds of engineers. We examined **horizontal scaling** through Kubernetes-based build farms and autoscaling runner controllers that dynamically adjust capacity to queue depth, ensuring developers never wait for build agents while minimizing idle resource costs. **Vertical scaling** strategies support resource-intensive workloads—machine learning training, large-scale compilations, and comprehensive security scanning—through specialized node pools with high-memory and GPU capabilities.\n",
    "\n",
    "**Multi-region deployments** ensure business continuity and regulatory compliance, implementing active-passive or active-active strategies that replicate CI/CD infrastructure across geographic boundaries to satisfy data residency requirements and provide disaster recovery capabilities. **High availability** configurations eliminate single points of failure in the control plane through stateful sets with persistent storage, pod disruption budgets, and rapid failover mechanisms.\n",
    "\n",
    "**Disaster recovery** strategies encompass backup procedures for build history, pipeline configurations, and artifact repositories, with automated verification of backup integrity and documented recovery time objectives (RTO) and recovery point objectives (RPO). **Capacity planning** methodologies leverage predictive metrics and queue theory to provision infrastructure ahead of demand spikes, preventing the pipeline itself from becoming a constraint on delivery velocity.\n",
    "\n",
    "**Cost optimization** techniques—including spot instances for fault-tolerant builds, intelligent layer caching for container images, persistent dependency caches shared across builds, and automated cleanup of completed resources—ensure that scaling infrastructure does not scale costs linearly.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Treat CI/CD infrastructure as a production service with the same reliability, observability, and disaster recovery requirements as customer-facing applications.\n",
    "- Implement horizontal pod autoscaling for build agents based on queue depth rather than static capacity to balance cost and availability.\n",
    "- Use spot instances and preemptible VMs for CI/CD workloads that can tolerate interruptions (most builds), reserving on-demand capacity for critical deployments.\n",
    "- Implement multi-region CI/CD infrastructure for disaster recovery, with automated failover and data replication to meet RPO/RTO requirements.\n",
    "- Maintain persistent caches for dependencies and Docker layers across builds to reduce execution time and external bandwidth costs.\n",
    "- Implement automated cleanup of completed builds, old artifacts, and unused images to prevent storage costs from accumulating indefinitely.\n",
    "- Use capacity planning based on historical build patterns and growth projections to provision infrastructure ahead of demand rather than reactively.\n",
    "\n",
    "**Next Chapter Preview:** Chapter 52: Multi-Cluster Deployments extends scaling from single clusters to federated environments managing hundreds of Kubernetes clusters. We will explore **cluster federation** with Kubernetes Federation v2 and GitOps-based management, **multi-cluster service mesh** for cross-cluster communication and traffic management, **global load balancing** strategies that route users to healthy clusters, **data replication** patterns for stateful applications across geographic boundaries, **failover automation** that detects cluster degradation and shifts workloads without manual intervention, **cluster configuration drift** detection and remediation, and **fleet management** tools like Rancher, Open Cluster Management (OCM), and Cluster API that provide unified control planes for distributed infrastructure. We will examine how to maintain consistency, security, and observability across a growing fleet of clusters while enabling team autonomy and regional compliance."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
