{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 52: Multi-Cluster Deployments\n",
    "\n",
    "As platform engineering matures, organizations rarely operate a single Kubernetes cluster. Regulatory requirements mandate data residency in specific geographies, application criticality demands isolation blast radius through cluster separation, and team autonomy drives the proliferation of specialized clusters for different environments and workloads. Managing ten, fifty, or hundreds of clusters requires architectural patterns that maintain consistency without sacrificing flexibility. This chapter examines strategies for **cluster federation**, centralized and decentralized governance models, **multi-cluster service meshes** that present distributed applications as unified systems, **global load balancing** that routes traffic based on health and geography, **data replication** patterns for stateful workloads spanning regions, **automated failover** that responds to regional outages without human intervention, and **fleet management** tools that provide unified visibility and control across the entire estate. We move beyond single-cluster CI/CD to pipelines that deploy globally while respecting local constraints.\n",
    "\n",
    "## 52.1 Cluster Federation\n",
    "\n",
    "Cluster federation provides mechanisms to coordinate multiple Kubernetes clusters as a single logical entity, enabling workload distribution and resource sharing across geographic and organizational boundaries.\n",
    "\n",
    "### Kubernetes Federation v2 (KubeFed)\n",
    "\n",
    "KubeFed allows resource propagation to multiple clusters while retaining local autonomy. Unlike the deprecated Federation v1, v2 uses a hub-spoke model with lightweight control planes.\n",
    "\n",
    "**Architecture**:\n",
    "- **Host Cluster**: Runs KubeFed control plane and API\n",
    "- **Member Clusters**: Participating clusters registered with the host\n",
    "- **Federated Resources**: CRDs that wrap standard Kubernetes resources with placement and override policies\n",
    "\n",
    "**Installation**:\n",
    "```bash\n",
    "# Install KubeFed on host cluster\n",
    "helm repo add kubefed-charts https://raw.githubusercontent.com/kubernetes-sigs/kubefed/master/charts\n",
    "helm install kubefed kubefed-charts/kubefed \\\n",
    "  --namespace kube-federation-system \\\n",
    "  --create-namespace\n",
    "```\n",
    "\n",
    "**Registering Clusters**:\n",
    "```yaml\n",
    "apiVersion: core.kubefed.io/v1beta1\n",
    "kind: KubeFedCluster\n",
    "metadata:\n",
    "  name: us-east-1\n",
    "  namespace: kube-federation-system\n",
    "spec:\n",
    "  apiEndpoint: https://us-east-1.api.internal\n",
    "  caBundle: <base64-encoded-ca>\n",
    "  secretRef:\n",
    "    name: us-east-1-secret  # Contains token for cluster access\n",
    "---\n",
    "apiVersion: core.kubefed.io/v1beta1\n",
    "kind: KubeFedCluster\n",
    "metadata:\n",
    "  name: eu-west-1\n",
    "  namespace: kube-federation-system\n",
    "spec:\n",
    "  apiEndpoint: https://eu-west-1.api.internal\n",
    "  caBundle: <base64-encoded-ca>\n",
    "  secretRef:\n",
    "    name: eu-west-1-secret\n",
    "```\n",
    "\n",
    "**Federated Deployment**:\n",
    "```yaml\n",
    "apiVersion: types.kubefed.io/v1beta1\n",
    "kind: FederatedDeployment\n",
    "metadata:\n",
    "  name: frontend-app\n",
    "  namespace: production\n",
    "spec:\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: frontend\n",
    "    spec:\n",
    "      replicas: 3\n",
    "      selector:\n",
    "        matchLabels:\n",
    "          app: frontend\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: nginx\n",
    "            image: nginx:1.25\n",
    "  placement:\n",
    "    clusters:\n",
    "    - name: us-east-1\n",
    "      weight: 60\n",
    "    - name: eu-west-1\n",
    "      weight: 40\n",
    "  overrides:\n",
    "  - clusterName: eu-west-1\n",
    "    clusterOverrides:\n",
    "    - path: \"/spec/template/spec/containers/0/resources/limits/memory\"\n",
    "      value: \"2Gi\"  # EU cluster has different resource constraints\n",
    "    - path: \"/spec/template/spec/containers/0/env\"\n",
    "      value:\n",
    "      - name: REGION\n",
    "        value: \"eu-west-1\"\n",
    "```\n",
    "\n",
    "**Limitations**: KubeFed has seen limited production adoption due to complexity. Modern multi-cluster management favors GitOps-based approaches (ArgoCD, Flux) or fleet management tools (Rancher, OCM).\n",
    "\n",
    "## 52.2 Multi-Cluster Management Architecture\n",
    "\n",
    "Organizations adopt one of three architectural patterns for multi-cluster governance:\n",
    "\n",
    "### Hub-and-Spoke (Centralized)\n",
    "\n",
    "A central management cluster controls workload distribution to spoke clusters. This provides unified policy enforcement but creates a potential control plane bottleneck.\n",
    "\n",
    "```yaml\n",
    "# Hub cluster configuration\n",
    "apiVersion: cluster.open-cluster-management.io/v1\n",
    "kind: ManagedCluster\n",
    "metadata:\n",
    "  name: production-us-east-1\n",
    "  labels:\n",
    "    environment: production\n",
    "    region: us-east-1\n",
    "    cloud: AWS\n",
    "spec:\n",
    "  hubAcceptsClient: true\n",
    "  leaseDurationSeconds: 60\n",
    "```\n",
    "\n",
    "### Decentralized (Peer-to-Peer)\n",
    "\n",
    "Clusters operate independently with synchronization occurring through Git repositories or service mesh federation. No single cluster has authority over others, reducing blast radius but complicating global policy enforcement.\n",
    "\n",
    "### Hybrid Approach\n",
    "\n",
    "Critical global policies (security, compliance) flow from a hub cluster, while application deployments remain decentralized via GitOps.\n",
    "\n",
    "```yaml\n",
    "# Policy propagates from hub to spokes\n",
    "apiVersion: policy.open-cluster-management.io/v1\n",
    "kind: Policy\n",
    "metadata:\n",
    "  name: require-pod-security-standards\n",
    "  namespace: policies\n",
    "spec:\n",
    "  remediationAction: enforce\n",
    "  disabled: false\n",
    "  policy-templates:\n",
    "  - objectDefinition:\n",
    "      apiVersion: policies.ibm.com/v1alpha1\n",
    "      kind: CertificatePolicy\n",
    "      metadata:\n",
    "        name: cert-policy\n",
    "      spec:\n",
    "        namespaceSelector:\n",
    "          include: [\"production\"]\n",
    "        remediationAction: inform\n",
    "        severity: low\n",
    "        minimumDuration: 300h\n",
    "```\n",
    "\n",
    "## 52.3 Service Mesh Across Clusters\n",
    "\n",
    "Service meshes extend beyond single clusters to provide unified traffic management, security (mTLS), and observability across distributed deployments.\n",
    "\n",
    "### Istio Multi-Cluster Patterns\n",
    "\n",
    "**Multi-Primary (Active-Active)**:\n",
    "Every cluster runs its own Istio control plane. Services communicate directly cluster-to-cluster with automatic failover.\n",
    "\n",
    "```yaml\n",
    "# Istio configuration for multi-primary\n",
    "apiVersion: install.istio.io/v1alpha1\n",
    "kind: IstioOperator\n",
    "metadata:\n",
    "  name: east-west\n",
    "spec:\n",
    "  profile: minimal\n",
    "  meshConfig:\n",
    "    accessLogFile: /dev/stdout\n",
    "    defaultConfig:\n",
    "      proxyMetadata:\n",
    "        ISTIO_META_DNS_CAPTURE: \"true\"\n",
    "  components:\n",
    "    pilot:\n",
    "      k8s:\n",
    "        env:\n",
    "        - name: PILOT_ENABLE_CROSS_CLUSTER_WORKLOAD_ENTRY\n",
    "          value: \"true\"\n",
    "  values:\n",
    "    global:\n",
    "      meshID: production-mesh\n",
    "      multiCluster:\n",
    "        clusterName: us-east-1\n",
    "        network: network1\n",
    "      network: network1\n",
    "```\n",
    "\n",
    "**Remote Cluster (Primary-Remote)**:\n",
    "One cluster hosts the Istio control plane; others connect to it as remotes. Simpler operation but creates a single point of failure if the primary fails.\n",
    "\n",
    "**Service Discovery**:\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: ServiceEntry\n",
    "metadata:\n",
    "  name: backend-eu\n",
    "spec:\n",
    "  hosts:\n",
    "  - backend.eu-west-1.global\n",
    "  location: MESH_INTERNAL\n",
    "  ports:\n",
    "  - number: 8080\n",
    "    name: http\n",
    "    protocol: HTTP\n",
    "  resolution: DNS\n",
    "  endpoints:\n",
    "  - address: backend.eu-west-1.svc.cluster.local\n",
    "    network: network2\n",
    "    locality: eu-west-1/eu-west-1a\n",
    "```\n",
    "\n",
    "### Linkerd Multi-Cluster\n",
    "\n",
    "Linkerd offers a simpler multi-cluster implementation focused on service mirroring and secure gateway communication.\n",
    "\n",
    "**Architecture**:\n",
    "- **Gateway**: Load balancer exposing Linkerd proxy in each cluster\n",
    "- **Service Mirror**: Controller that watches remote services and creates local mirror services\n",
    "\n",
    "**Linking Clusters**:\n",
    "```bash\n",
    "# Extract credentials from east cluster\n",
    "linkerd multicluster link --cluster-name us-east-1 > east-credentials.yaml\n",
    "\n",
    "# Apply to west cluster\n",
    "kubectl --context=us-west-1 apply -f east-credentials.yaml\n",
    "\n",
    "# Verify connection\n",
    "linkerd multicluster check\n",
    "```\n",
    "\n",
    "**Exported Services**:\n",
    "```yaml\n",
    "apiVersion: multicluster.linkerd.io/v1alpha1\n",
    "kind: ServiceExport\n",
    "metadata:\n",
    "  name: backend-api\n",
    "  namespace: production\n",
    "spec:\n",
    "  # Service automatically mirrored in other clusters as backend-api-us-east-1\n",
    "```\n",
    "\n",
    "**Traffic Splitting Across Clusters**:\n",
    "```yaml\n",
    "apiVersion: split.smi-spec.io/v1alpha4\n",
    "kind: TrafficSplit\n",
    "metadata:\n",
    "  name: backend-split\n",
    "  namespace: production\n",
    "spec:\n",
    "  service: backend-api\n",
    "  backends:\n",
    "  - service: backend-api-us-east-1\n",
    "    weight: 70\n",
    "  - service: backend-api-eu-west-1\n",
    "    weight: 30\n",
    "```\n",
    "\n",
    "## 52.4 Global Load Balancing\n",
    "\n",
    "Distributing user traffic across clusters requires DNS-based or anycast routing that considers cluster health, geographic proximity, and capacity.\n",
    "\n",
    "### External DNS with Health Checks\n",
    "\n",
    "**Architecture**:\n",
    "- Global DNS (Route53, Cloudflare, Google Cloud DNS) with health checks\n",
    "- Each cluster reports health via external-dns controller\n",
    "- DNS records updated based on endpoint readiness\n",
    "\n",
    "**External DNS Configuration**:\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: frontend-lb\n",
    "  annotations:\n",
    "    external-dns.alpha.kubernetes.io/hostname: app.company.com\n",
    "    external-dns.alpha.kubernetes.io/ttl: \"30\"\n",
    "    # AWS Route53 specific: health check association\n",
    "    external-dns.alpha.kubernetes.io/aws-health-check-id: \"abc-123\"\n",
    "spec:\n",
    "  type: LoadBalancer\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8080\n",
    "  selector:\n",
    "    app: frontend\n",
    "---\n",
    "# Deployment with pod readiness ensuring DNS registration\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: frontend\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: app\n",
    "        image: frontend:latest\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8080\n",
    "          initialDelaySeconds: 10\n",
    "          periodSeconds: 5\n",
    "```\n",
    "\n",
    "### Global Server Load Balancing (GSLB)\n",
    "\n",
    "Tools like **K8GB** (Kubernetes Global Balancer) or **ExternalDNS** with CRD sources provide automated failover:\n",
    "\n",
    "**K8GB Configuration**:\n",
    "```yaml\n",
    "apiVersion: k8gb.absa.oss/v1beta1\n",
    "kind: Gslb\n",
    "metadata:\n",
    "  name: app-gslb\n",
    "  namespace: production\n",
    "spec:\n",
    "  ingress:\n",
    "    ingressClassName: nginx\n",
    "    rules:\n",
    "    - host: app.company.com\n",
    "      http:\n",
    "        paths:\n",
    "        - path: /\n",
    "          pathType: Prefix\n",
    "          backend:\n",
    "            service:\n",
    "              name: frontend\n",
    "              port:\n",
    "                number: 80\n",
    "  strategy:\n",
    "    type: failover  # or roundRobin, geoip\n",
    "    primaryGeoTag: us-east-1\n",
    "    secondaryGeoTags:\n",
    "      - eu-west-1\n",
    "      - ap-southeast-1\n",
    "```\n",
    "\n",
    "**Health Check Strategy**:\n",
    "K8GB monitors endpoint health via Prometheus or HTTP checks and updates DNS records to remove unhealthy clusters from rotation.\n",
    "\n",
    "## 52.5 Data Replication\n",
    "\n",
    "Stateful applications spanning clusters require data synchronization strategies that balance consistency, availability, and partition tolerance (CAP theorem).\n",
    "\n",
    "### Asynchronous Replication\n",
    "\n",
    "Most cloud-native databases use asynchronous replication for cross-cluster data distribution, accepting eventual consistency for availability.\n",
    "\n",
    "**PostgreSQL with Patroni and Streaming Replication**:\n",
    "```yaml\n",
    "# Patroni configuration for cross-cluster replication\n",
    "bootstrap:\n",
    "  dcs:\n",
    "    ttl: 30\n",
    "    loop_wait: 10\n",
    "    retry_timeout: 10\n",
    "    maximum_lag_on_failover: 1048576\n",
    "    master_start_timeout: 300\n",
    "    synchronous_mode: false  # Async for cross-cluster\n",
    "    postgresql:\n",
    "      use_pg_rewind: true\n",
    "      use_slots: true\n",
    "      parameters:\n",
    "        wal_level: replica\n",
    "        hot_standby: \"on\"\n",
    "        wal_keep_segments: 64\n",
    "        max_wal_senders: 10\n",
    "        max_replication_slots: 10\n",
    "        # Cross-cluster streaming\n",
    "        primary_conninfo: \"host=postgres-us-east-1.company.com port=5432 user=replicator sslmode=require\"\n",
    "```\n",
    "\n",
    "**CockroachDB (Geo-Distributed SQL)**:\n",
    "CockroachDB provides synchronous replication across regions with configurable survival goals:\n",
    "\n",
    "```sql\n",
    "-- Create database with region survival\n",
    "CREATE DATABASE app PRIMARY REGION \"us-east-1\" REGIONS \"eu-west-1\", \"ap-south-1\" SURVIVE REGION FAILURE;\n",
    "\n",
    "-- Table with row-level locality\n",
    "CREATE TABLE orders (\n",
    "    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),\n",
    "    customer_id INT,\n",
    "    region STRING\n",
    ") LOCALITY REGIONAL BY ROW AS region;\n",
    "```\n",
    "\n",
    "### CRDTs and Event Sourcing\n",
    "\n",
    "For application-level data consistency without database coordination:\n",
    "\n",
    "**Conflict-Free Replicated Data Types**:\n",
    "```yaml\n",
    "# Example: Distributed cache using CRDTs\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: crdt-cache\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: cache\n",
    "        image: antidote/antidote:latest\n",
    "        env:\n",
    "        - name: ANTIDOTE_CLUSTER\n",
    "          value: \"antidote-us-east-1,antidote-eu-west-1\"\n",
    "        ports:\n",
    "        - containerPort: 8087\n",
    "```\n",
    "\n",
    "## 52.6 Failover Strategies\n",
    "\n",
    "Automated failover detects cluster degradation and shifts workloads to healthy clusters without manual intervention.\n",
    "\n",
    "### Health Monitoring\n",
    "\n",
    "**Cluster Health Probes**:\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: CronJob\n",
    "metadata:\n",
    "  name: cluster-health-check\n",
    "spec:\n",
    "  schedule: \"*/1 * * * *\"\n",
    "  jobTemplate:\n",
    "    spec:\n",
    "      template:\n",
    "        spec:\n",
    "          containers:\n",
    "          - name: health-checker\n",
    "            image: bitnami/kubectl:latest\n",
    "            command:\n",
    "            - /bin/sh\n",
    "            - -c\n",
    "            - |\n",
    "              # Check control plane health\n",
    "              if ! kubectl get nodes > /dev/null 2>&1; then\n",
    "                curl -X POST \\\n",
    "                  -H \"Authorization: Bearer ${ALERT_TOKEN}\" \\\n",
    "                  -d '{\"cluster\": \"us-east-1\", \"status\": \"unhealthy\", \"reason\": \"control-plane\"}' \\\n",
    "                  https://pagerduty.com/integration-endpoint\n",
    "                exit 1\n",
    "              fi\n",
    "              \n",
    "              # Check critical workload availability\n",
    "              if [ $(kubectl get pods -n production -l app=backend --field-selector=status.phase=Running | wc -l) -lt 3 ]; then\n",
    "                curl -X POST \\\n",
    "                  -H \"Authorization: Bearer ${FAILOVER_TOKEN}\" \\\n",
    "                  -d '{\"action\": \"initiate-failover\", \"from\": \"us-east-1\", \"to\": \"us-west-2\"}' \\\n",
    "                  https://global-lb.company.com/api/failover\n",
    "              fi\n",
    "          restartPolicy: OnFailure\n",
    "```\n",
    "\n",
    "### Automated Failover with ArgoCD\n",
    "\n",
    "**ApplicationSet with Progressive Sync**:\n",
    "```yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: ApplicationSet\n",
    "metadata:\n",
    "  name: critical-app\n",
    "spec:\n",
    "  generators:\n",
    "  - list:\n",
    "      elements:\n",
    "      - cluster: us-east-1\n",
    "        url: https://us-east-1.api.internal\n",
    "        shard: \"0\"\n",
    "      - cluster: us-west-2\n",
    "        url: https://us-west-2.api.internal\n",
    "        shard: \"1\"\n",
    "  template:\n",
    "    metadata:\n",
    "      name: '{{cluster}}-critical-app'\n",
    "    spec:\n",
    "      project: production\n",
    "      source:\n",
    "        repoURL: https://github.com/company/app.git\n",
    "        targetRevision: HEAD\n",
    "        path: k8s/overlays/{{cluster}}\n",
    "      destination:\n",
    "        server: '{{url}}'\n",
    "        namespace: production\n",
    "      syncPolicy:\n",
    "        automated:\n",
    "          selfHeal: true\n",
    "          prune: true\n",
    "        retry:\n",
    "          limit: 5\n",
    "          backoff:\n",
    "            duration: 5s\n",
    "            factor: 2\n",
    "            maxDuration: 3m\n",
    "```\n",
    "\n",
    "**Failover Script**:\n",
    "```bash\n",
    "#!/bin/bash\n",
    "# failover.sh - Triggered by health monitoring\n",
    "\n",
    "SOURCE_CLUSTER=$1\n",
    "TARGET_CLUSTER=$2\n",
    "\n",
    "# 1. Scale down source (don't delete, preserve state)\n",
    "kubectl --context=$SOURCE_CLUSTER scale deployment critical-app --replicas=0 -n production\n",
    "\n",
    "# 2. Promote target to active (update DNS weights)\n",
    "aws route53 change-resource-record-sets \\\n",
    "  --hosted-zone-id Z123456789 \\\n",
    "  --change-batch file://<(cat <<EOF\n",
    "{\n",
    "  \"Changes\": [{\n",
    "    \"Action\": \"UPSERT\",\n",
    "    \"ResourceRecordSet\": {\n",
    "      \"Name\": \"app.company.com\",\n",
    "      \"Type\": \"A\",\n",
    "      \"SetIdentifier\": \"$SOURCE_CLUSTER\",\n",
    "      \"Weight\": 0,\n",
    "      \"TTL\": 60,\n",
    "      \"ResourceRecords\": [{\"Value\": \"$SOURCE_IP\"}]\n",
    "    }\n",
    "  }, {\n",
    "    \"Action\": \"UPSERT\",\n",
    "    \"ResourceRecordSet\": {\n",
    "      \"Name\": \"app.company.com\",\n",
    "      \"Type\": \"A\",\n",
    "      \"SetIdentifier\": \"$TARGET_CLUSTER\",\n",
    "      \"Weight\": 100,\n",
    "      \"TTL\": 60,\n",
    "      \"ResourceRecords\": [{\"Value\": \"$TARGET_IP\"}]\n",
    "    }\n",
    "  }]\n",
    "}\n",
    "EOF\n",
    ")\n",
    "\n",
    "# 3. Verify traffic shift\n",
    "sleep 30\n",
    "if ! curl -f https://app.company.com/health; then\n",
    "  echo \"Failover failed, rolling back\"\n",
    "  # Rollback logic here\n",
    "  exit 1\n",
    "fi\n",
    "```\n",
    "\n",
    "### Split-Brain Prevention\n",
    "\n",
    "When clusters lose connectivity, automated failover risks split-brain (both clusters accepting writes). Implement fencing:\n",
    "\n",
    "```yaml\n",
    "# Lease-based leadership\n",
    "apiVersion: coordination.k8s.io/v1\n",
    "kind: Lease\n",
    "metadata:\n",
    "  name: global-leader\n",
    "  namespace: production\n",
    "spec:\n",
    "  holderIdentity: us-east-1\n",
    "  leaseDurationSeconds: 60\n",
    "  renewTime: \"2024-01-15T10:00:00Z\"\n",
    "```\n",
    "\n",
    "Only the lease holder accepts writes; if connectivity is lost, the lease expires and secondary clusters can acquire it.\n",
    "\n",
    "## 52.7 Cluster Configuration Drift\n",
    "\n",
    "Drift occurs when cluster state diverges from Git-declared desired state due to manual interventions or failed reconciliations.\n",
    "\n",
    "### Drift Detection\n",
    "\n",
    "**ArgoCD Diff**:\n",
    "```bash\n",
    "# Detect drift in specific app\n",
    "argocd app diff critical-app --refresh\n",
    "\n",
    "# Automated drift detection in CI\n",
    "argocd app list -o json | jq '.[] | select(.status.sync.status != \"Synced\") | .metadata.name'\n",
    "```\n",
    "\n",
    "**Config Connector / Anthos Config Management**:\n",
    "```yaml\n",
    "apiVersion: configmanagement.gke.io/v1\n",
    "kind: ConfigManagement\n",
    "metadata:\n",
    "  name: config-management\n",
    "spec:\n",
    "  sourceFormat: unstructured\n",
    "  git:\n",
    "    syncRepo: https://github.com/company/config-sync.git\n",
    "    syncBranch: main\n",
    "    secretType: ssh\n",
    "    policyDir: config\n",
    "  policyController:\n",
    "    enabled: true\n",
    "  hierarchyController:\n",
    "    enabled: true\n",
    "```\n",
    "\n",
    "### Remediation Strategies\n",
    "\n",
    "**Self-Healing**:\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  annotations:\n",
    "    # ArgoCD specific: prune resources not in Git\n",
    "    argocd.argoproj.io/sync-options: Prune=true\n",
    "    # Prevent manual kubectl edits\n",
    "    argocd.argoproj.io/sync-wave: \"2\"\n",
    "```\n",
    "\n",
    "**Policy Enforcement**:\n",
    "Use OPA Gatekeeper or Kyverno to reject resources not matching Git-declared state:\n",
    "```yaml\n",
    "apiVersion: kyverno.io/v1\n",
    "kind: ClusterPolicy\n",
    "metadata:\n",
    "  name: prevent-manual-changes\n",
    "spec:\n",
    "  validationFailureAction: Enforce\n",
    "  rules:\n",
    "  - name: check-git-annotation\n",
    "    match:\n",
    "      resources:\n",
    "        kinds:\n",
    "        - Deployment\n",
    "    validate:\n",
    "      message: \"Manual changes forbidden. All changes must come through GitOps.\"\n",
    "      deny:\n",
    "        conditions:\n",
    "        - key: \"{{request.object.metadata.annotations.\\\"argocd.argoproj.io/tracking-id\\\"}}\"\n",
    "          operator: Equals\n",
    "          value: \"\"\n",
    "```\n",
    "\n",
    "## 52.8 Tools for Multi-Cluster\n",
    "\n",
    "### Rancher\n",
    "\n",
    "Rancher provides centralized cluster lifecycle management and unified authentication across heterogeneous clusters (EKS, AKS, GKE, on-prem).\n",
    "\n",
    "**Key Features**:\n",
    "- **Cluster Provisioning**: UI/API for creating clusters across providers\n",
    "- **Global DNS**: Cross-cluster load balancing and service discovery\n",
    "- **Fleet**: GitOps-driven cluster configuration management\n",
    "\n",
    "**Fleet Configuration**:\n",
    "```yaml\n",
    "apiVersion: fleet.cattle.io/v1alpha1\n",
    "kind: GitRepo\n",
    "metadata:\n",
    "  name: production-apps\n",
    "  namespace: fleet-default\n",
    "spec:\n",
    "  repo: https://github.com/company/fleet-repo.git\n",
    "  branch: main\n",
    "  paths:\n",
    "  - /production\n",
    "  targets:\n",
    "  - clusterSelector:\n",
    "      matchLabels:\n",
    "        environment: production\n",
    "        region: us-east-1\n",
    "  - clusterSelector:\n",
    "      matchLabels:\n",
    "        environment: production\n",
    "        region: eu-west-1\n",
    "```\n",
    "\n",
    "### Open Cluster Management (OCM)\n",
    "\n",
    "CNCF sandbox project providing a Kubernetes-native approach to multi-cluster management.\n",
    "\n",
    "**Hub Cluster Components**:\n",
    "```yaml\n",
    "apiVersion: work.open-cluster-management.io/v1\n",
    "kind: ManifestWork\n",
    "metadata:\n",
    "  name: deploy-app\n",
    "  namespace: cluster1  # Managed cluster name\n",
    "spec:\n",
    "  workload:\n",
    "    manifests:\n",
    "    - apiVersion: apps/v1\n",
    "      kind: Deployment\n",
    "      metadata:\n",
    "        name: app\n",
    "        namespace: default\n",
    "      spec:\n",
    "        replicas: 3\n",
    "        template:\n",
    "          spec:\n",
    "            containers:\n",
    "            - name: app\n",
    "              image: app:latest\n",
    "```\n",
    "\n",
    "**Placement**:\n",
    "```yaml\n",
    "apiVersion: cluster.open-cluster-management.io/v1beta1\n",
    "kind: Placement\n",
    "metadata:\n",
    "  name: production-placement\n",
    "  namespace: default\n",
    "spec:\n",
    "  clusterSets:\n",
    "    - production\n",
    "  predicates:\n",
    "    - requiredClusterSelector:\n",
    "        labelSelector:\n",
    "          matchLabels:\n",
    "            region: us-east-1\n",
    "        claimSelector:\n",
    "          matchExpressions:\n",
    "            - key: cpu.available\n",
    "              operator: Gt\n",
    "              values:\n",
    "                - \"10\"\n",
    "```\n",
    "\n",
    "### Cluster API (CAPI)\n",
    "\n",
    "Declarative Kubernetes cluster lifecycle management using Kubernetes-style APIs.\n",
    "\n",
    "**Cluster Definition**:\n",
    "```yaml\n",
    "apiVersion: cluster.x-k8s.io/v1beta1\n",
    "kind: Cluster\n",
    "metadata:\n",
    "  name: prod-workload-01\n",
    "  namespace: production\n",
    "spec:\n",
    "  clusterNetwork:\n",
    "    pods:\n",
    "      cidrBlocks:\n",
    "      - 192.168.0.0/16\n",
    "    serviceDomain: cluster.local\n",
    "    services:\n",
    "      cidrBlocks:\n",
    "      - 10.128.0.0/12\n",
    "  controlPlaneRef:\n",
    "    apiVersion: controlplane.cluster.x-k8s.io/v1beta1\n",
    "    kind: KubeadmControlPlane\n",
    "    name: prod-workload-01-control-plane\n",
    "  infrastructureRef:\n",
    "    apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n",
    "    kind: AWSCluster\n",
    "    name: prod-workload-01\n",
    "---\n",
    "apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n",
    "kind: AWSCluster\n",
    "metadata:\n",
    "  name: prod-workload-01\n",
    "  namespace: production\n",
    "spec:\n",
    "  region: us-east-1\n",
    "  sshKeyName: platform-key\n",
    "  controlPlaneLoadBalancer:\n",
    "    crossZoneLoadBalancing: true\n",
    "```\n",
    "\n",
    "**Machine Deployment**:\n",
    "```yaml\n",
    "apiVersion: cluster.x-k8s.io/v1beta1\n",
    "kind: MachineDeployment\n",
    "metadata:\n",
    "  name: prod-workload-01-worker\n",
    "spec:\n",
    "  clusterName: prod-workload-01\n",
    "  replicas: 3\n",
    "  template:\n",
    "    spec:\n",
    "      clusterName: prod-workload-01\n",
    "      infrastructureRef:\n",
    "        apiVersion: infrastructure.cluster.x-k8s.io/v1beta1\n",
    "        kind: AWSMachineTemplate\n",
    "        name: prod-workload-01-worker\n",
    "      version: v1.28.0\n",
    "```\n",
    "\n",
    "### ArgoCD ApplicationSet\n",
    "\n",
    "Modern GitOps approach to multi-cluster deployments:\n",
    "\n",
    "```yaml\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: ApplicationSet\n",
    "metadata:\n",
    "  name: cluster-addons\n",
    "spec:\n",
    "  generators:\n",
    "  # Generate from cluster secret list\n",
    "  - clusters:\n",
    "      selector:\n",
    "        matchLabels:\n",
    "          argocd.argoproj.io/secret-type: cluster\n",
    "          environment: production\n",
    "      values:\n",
    "        revision: HEAD\n",
    "  # Or from Git directory structure\n",
    "  - git:\n",
    "      repoURL: https://github.com/company/clusters.git\n",
    "      revision: HEAD\n",
    "      directories:\n",
    "      - path: \"clusters/*\"\n",
    "  template:\n",
    "    metadata:\n",
    "      name: '{{name}}-addons'\n",
    "    spec:\n",
    "      project: infrastructure\n",
    "      source:\n",
    "        repoURL: https://github.com/company/addons.git\n",
    "        targetRevision: '{{values.revision}}'\n",
    "        path: addons/\n",
    "        helm:\n",
    "          values: |\n",
    "            clusterName: {{name}}\n",
    "            region: {{metadata.labels.region}}\n",
    "      destination:\n",
    "        server: '{{server}}'\n",
    "        namespace: kube-system\n",
    "      syncPolicy:\n",
    "        automated:\n",
    "          prune: true\n",
    "          selfHeal: true\n",
    "        syncOptions:\n",
    "        - CreateNamespace=true\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "This chapter addressed the complexity of operating CI/CD across multiple Kubernetes clusters, moving beyond single-cluster patterns to federated, global platforms. We examined **cluster federation** concepts and the practical reality that modern multi-cluster management relies more heavily on GitOps tools (ArgoCD ApplicationSets, Fleet) than on Kubernetes Federation v2, which has seen limited adoption due to operational complexity.\n",
    "\n",
    "**Multi-cluster service meshes** (Istio multi-primary, Linkerd multi-cluster) extend zero-trust networking and traffic management across cluster boundaries, presenting distributed services as unified logical applications while respecting failure domain isolation. **Global load balancing** through DNS-based solutions (K8GB, ExternalDNS with health checks) routes users to healthy clusters based on geographic proximity and real-time health status.\n",
    "\n",
    "For stateful applications, **data replication** strategies must balance consistency and availability, utilizing asynchronous database replication, geo-distributed databases (CockroachDB, YugabyteDB), or CRDT-based application patterns. **Automated failover** requires sophisticated health monitoring to detect genuine cluster failures while avoiding false positives that trigger unnecessary traffic shifts and potential split-brain scenarios; lease-based leadership and fencing mechanisms prevent data corruption during failover events.\n",
    "\n",
    "**Configuration drift detection** ensures that manual interventions or failed reconciliations do not persist, with tools like ArgoCD and Anthos Config Management continuously aligning cluster state with Git-declared desired state. **Fleet management tools**—Rancher for heterogeneous cluster lifecycle management, Open Cluster Management for Kubernetes-native policy distribution, and Cluster API for declarative cluster provisioning—provide the unified control planes necessary to operate hundreds of clusters without proportional operational overhead.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Prefer GitOps-based multi-cluster management (ApplicationSets, Fleet) over Kubernetes Federation v2 for production deployments due to operational simplicity and better ecosystem support.\n",
    "- Implement service mesh across clusters only when cross-cluster service-to-service communication is required; otherwise, keep clusters isolated with explicit API gateways.\n",
    "- Design failover strategies with split-brain prevention (fencing, leases) to avoid data corruption during network partitions.\n",
    "- Use cluster labels and selectors extensively to drive placement decisions, enabling policy-based routing of workloads to appropriate clusters based on region, compliance requirements, or hardware capabilities.\n",
    "- Treat cluster configuration drift as a critical issue; implement automated remediation that reverts manual changes to maintain GitOps integrity.\n",
    "- Implement global load balancing at the DNS layer for stateless applications, but ensure data replication is synchronous or carefully orchestrated for stateful workloads before allowing traffic to shift.\n",
    "- Use Cluster API for declarative cluster lifecycle management, enabling GitOps workflows for infrastructure provisioning, not just application deployment.\n",
    "\n",
    "**Next Chapter Preview:** Chapter 53: CI/CD Team Collaboration transitions from technical infrastructure to organizational culture, examining how high-performing teams structure collaboration around continuous delivery practices. We will explore **cross-functional team structures** that break down DevOps silos, **developer experience (DX)** optimization that reduces friction in the pipeline, **onboarding strategies** for new engineers to become productive contributors quickly, **knowledge sharing** mechanisms including documentation standards and internal tech talks, **code review cultures** that maintain quality without becoming bottlenecks, **blameless post-mortems** that transform failures into organizational learning, and **continuous learning** practices that keep teams current with evolving cloud-native technologies. We will examine how to measure and improve team productivity without falling into velocity trap metrics, fostering a culture where CI/CD is not merely tooling but shared organizational capability."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
