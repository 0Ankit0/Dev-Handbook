{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 58: Service Mesh in CI/CD\n",
    "\n",
    "While Kubernetes provides robust container orchestration, it leaves critical operational concerns\u2014service-to-service communication, security, and observability\u2014as exercise for the implementer. Service meshes address these gaps by introducing a dedicated infrastructure layer that handles inter-service traffic without requiring application code changes. In CI/CD contexts, service meshes enable sophisticated deployment strategies, zero-trust security, and unified observability across polyglot microservices. This chapter examines how service meshes integrate with continuous delivery pipelines to enable progressive rollouts, automatic failover, and encrypted communication while maintaining developer velocity.\n",
    "\n",
    "## 58.1 Service Mesh Overview\n",
    "\n",
    "A service mesh is a configurable, low-latency infrastructure layer designed to handle high-volume communication between application services via a proxy deployed alongside each service instance. Unlike traditional approaches requiring developers to implement resilience patterns (retries, timeouts, circuit breakers) and security (TLS, authentication) in application code, service meshes externalize these concerns to a sidecar proxy\u2014decoupling business logic from operational complexity.\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "**Data Plane:**\n",
    "Comprises lightweight proxies (sidecars) deployed alongside application containers. These intercept all network traffic (ingress and egress) using iptables rules or eBPF, implementing policies without application awareness. The proxy handles load balancing, traffic encryption, metrics collection, and routing decisions.\n",
    "\n",
    "**Control Plane:**\n",
    "Manages configuration distribution, certificate issuance, and policy enforcement across the mesh. It provides APIs for operators to define traffic rules, security policies, and observability settings, pushing these configurations to data plane proxies.\n",
    "\n",
    "### Why Service Mesh in CI/CD\n",
    "\n",
    "Traditional CI/CD pipelines struggle with microservices complexity:\n",
    "- **Traffic Management**: Blue/green and canary deployments require external load balancer coordination or application-aware routing\n",
    "- **Security**: Mutual TLS implementation varies by language/framework, creating inconsistent protection\n",
    "- **Observability**: Distributed tracing requires instrumentation in every service\n",
    "- **Resilience**: Circuit breakers and retries must be coded per service\n",
    "\n",
    "Service meshes standardize these capabilities, enabling pipelines to:\n",
    "- Deploy canary releases using traffic splitting without application changes\n",
    "- Enforce zero-trust security automatically via mTLS\n",
    "- Collect uniform metrics across Java, Python, Go, and Node.js services\n",
    "- Implement chaos engineering by injecting faults at the network layer\n",
    "\n",
    "### Popular Implementations\n",
    "\n",
    "| Mesh | Architecture | Resource Footprint | Learning Curve | Best For |\n",
    "|------|---------------|-------------------|----------------|----------|\n",
    "| **Istio** | Envoy proxies, Istiod control plane | Medium-High | Steep | Complex enterprise requirements, extensive traffic management |\n",
    "| **Linkerd** | Linkerd2-proxy, lightweight control plane | Low | Gentle | Resource-constrained environments, simplicity-first |\n",
    "| **Consul Connect** | Envoy/Consul proxy, HashiCorp ecosystem | Medium | Moderate | Hybrid cloud, VM-to-Kubernetes bridging |\n",
    "| **AWS App Mesh** | Envoy, AWS-managed control plane | Medium | Moderate | AWS-centric architectures |\n",
    "| **Cilium Service Mesh** | eBPF-based, kernel-level | Low | Moderate | High-performance requirements, eBPF expertise |\n",
    "\n",
    "## 58.2 Istio Fundamentals\n",
    "\n",
    "Istio, the most widely adopted service mesh, extends Kubernetes via custom resource definitions (CRDs) and Envoy sidecar proxies. It provides comprehensive traffic management, security, and observability capabilities essential for enterprise CI/CD.\n",
    "\n",
    "### Core Architecture\n",
    "\n",
    "**Istiod (Control Plane):**\n",
    "Consolidates pilot (service discovery), citadel (certificate management), and galley (configuration validation) into a single binary. Istiod converts high-level routing rules into Envoy-specific configurations and distributes them via xDS APIs.\n",
    "\n",
    "**Envoy Sidecars:**\n",
    "Deployed automatically via the Istio CNI (Container Network Interface) or init containers that configure iptables rules. Each proxy maintains a local route table, certificate cache, and telemetry data.\n",
    "\n",
    "**Gateway Resources:**\n",
    "Manage north-south traffic (external to mesh) through dedicated Envoy instances running as Kubernetes Services, handling TLS termination and virtual hosting.\n",
    "\n",
    "### Installation and Setup\n",
    "\n",
    "**Prerequisites:**\n",
    "- Kubernetes cluster (1.24+) with sufficient resources (Istio control plane: ~500m CPU, ~500Mi memory; Sidecar overhead: ~100m CPU, ~128Mi per pod)\n",
    "- `istioctl` CLI or Helm configured\n",
    "\n",
    "**Installation via istioctl:**\n",
    "```bash\n",
    "# Download Istio\n",
    "curl -L https://istio.io/downloadIstio | sh -\n",
    "cd istio-1.20.0\n",
    "export PATH=$PWD/bin:$PATH\n",
    "\n",
    "# Install default profile (suitable for production)\n",
    "istioctl install --set profile=default -y\n",
    "\n",
    "# Verify installation\n",
    "kubectl get pods -n istio-system\n",
    "# Output: istiod-* Running\n",
    "\n",
    "# Enable automatic sidecar injection for namespace\n",
    "kubectl label namespace default istio-injection=enabled\n",
    "```\n",
    "\n",
    "**Configuration Profiles:**\n",
    "- **default**: Production-ready with recommended settings\n",
    "- **demo**: Comprehensive suite with Grafana, Prometheus, Kiali, Jaeger (resource-intensive, suitable for learning)\n",
    "- **minimal**: Control plane only, manual proxy injection\n",
    "- **empty**: Minimal base for custom configuration\n",
    "\n",
    "### Core Istio Resources\n",
    "\n",
    "**VirtualService:**\n",
    "Defines traffic routing rules, enabling sophisticated request distribution based on HTTP headers, URI paths, or source labels.\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: user-service-routes\n",
    "spec:\n",
    "  hosts:\n",
    "  - user-service\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        end-user:\n",
    "          exact: internal-test\n",
    "    route:\n",
    "    - destination:\n",
    "        host: user-service\n",
    "        subset: canary\n",
    "      weight: 100\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: user-service\n",
    "        subset: stable\n",
    "      weight: 90\n",
    "    - destination:\n",
    "        host: user-service\n",
    "        subset: canary\n",
    "      weight: 10\n",
    "```\n",
    "\n",
    "**DestinationRule:**\n",
    "Configures policies applicable to traffic after routing has occurred, including load balancing algorithms, connection pool settings, and mTLS modes.\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: DestinationRule\n",
    "metadata:\n",
    "  name: user-service-policy\n",
    "spec:\n",
    "  host: user-service\n",
    "  trafficPolicy:\n",
    "    connectionPool:\n",
    "      tcp:\n",
    "        maxConnections: 100\n",
    "      http:\n",
    "        http1MaxPendingRequests: 50\n",
    "        maxRequestsPerConnection: 10\n",
    "    outlierDetection:\n",
    "      consecutive5xxErrors: 5\n",
    "      interval: 30s\n",
    "      baseEjectionTime: 30s\n",
    "    tls:\n",
    "      mode: ISTIO_MUTUAL  # Enable mTLS automatically\n",
    "  subsets:\n",
    "  - name: stable\n",
    "    labels:\n",
    "      version: stable\n",
    "  - name: canary\n",
    "    labels:\n",
    "      version: canary\n",
    "```\n",
    "\n",
    "**Gateway:**\n",
    "Manages external access, replacing Kubernetes Ingress with greater flexibility.\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: Gateway\n",
    "metadata:\n",
    "  name: public-gateway\n",
    "spec:\n",
    "  selector:\n",
    "    istio: ingressgateway  # Use default ingress gateway\n",
    "  servers:\n",
    "  - port:\n",
    "      number: 443\n",
    "      name: https\n",
    "      protocol: HTTPS\n",
    "    tls:\n",
    "      mode: SIMPLE\n",
    "      credentialName: tls-cert-secret  # Kubernetes TLS secret\n",
    "    hosts:\n",
    "    - \"api.company.com\"\n",
    "```\n",
    "\n",
    "**PeerAuthentication:**\n",
    "Enforces mTLS requirements across namespaces or workloads.\n",
    "\n",
    "```yaml\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: PeerAuthentication\n",
    "metadata:\n",
    "  name: default-mtls\n",
    "  namespace: production\n",
    "spec:\n",
    "  mtls:\n",
    "    mode: STRICT  # Reject plaintext traffic\n",
    "```\n",
    "\n",
    "## 58.3 Linkerd\n",
    "\n",
    "Linkerd, originally created by Buoyant and now a CNCF graduated project, prioritizes simplicity and performance. Written in Rust (for the proxy) and Go (for the control plane), it offers a lightweight alternative to Istio with significantly lower resource overhead.\n",
    "\n",
    "### Architectural Differences\n",
    "\n",
    "**Linkerd2-Proxy:**\n",
    "A purpose-built Rust proxy (not general-purpose Envoy) optimized specifically for service mesh use cases. It consumes ~20m CPU and ~20Mi memory per instance compared to Envoy's ~100m CPU and ~128Mi memory.\n",
    "\n",
    "**Control Plane Components:**\n",
    "- **Destination**: Service discovery and routing configuration\n",
    "- **Identity**: Certificate authority and mTLS management\n",
    "- **Proxy Injector**: Automatic sidecar injection webhook\n",
    "- **Service Profile**: Defines per-route metrics and retry budgets\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Install CLI\n",
    "curl --proto '=https' --tlsv1.2 -sSfL https://run.linkerd.io/install | sh\n",
    "export PATH=$PATH:$HOME/.linkerd2/bin\n",
    "\n",
    "# Verify cluster readiness\n",
    "linkerd check --pre\n",
    "\n",
    "# Install control plane\n",
    "linkerd install | kubectl apply -f -\n",
    "\n",
    "# Verify installation\n",
    "linkerd check\n",
    "\n",
    "# Inject sidecars into existing deployment\n",
    "kubectl get deployment myapp -o yaml | linkerd inject - | kubectl apply -f -\n",
    "\n",
    "# Or enable namespace-wide injection\n",
    "kubectl annotate namespace default linkerd.io/inject=enabled\n",
    "```\n",
    "\n",
    "### Service Profiles\n",
    "\n",
    "Linkerd's ServiceProfile resource provides route-specific metrics and resilience policies, similar to Istio's VirtualService but focused on observability and retries:\n",
    "\n",
    "```yaml\n",
    "apiVersion: linkerd.io/v1alpha2\n",
    "kind: ServiceProfile\n",
    "metadata:\n",
    "  name: user-service.default.svc.cluster.local\n",
    "  namespace: default\n",
    "spec:\n",
    "  routes:\n",
    "  - name: GET /api/users/{id}\n",
    "    condition:\n",
    "      method: GET\n",
    "      pathRegex: /api/users/[^/]+\n",
    "    retryBudget:\n",
    "      retryRatio: 0.2\n",
    "      minRetriesPerSecond: 10\n",
    "      ttl: 10s\n",
    "    timeout: 300ms\n",
    "  - name: POST /api/users\n",
    "    condition:\n",
    "      method: POST\n",
    "      pathRegex: /api/users\n",
    "    timeout: 500ms\n",
    "```\n",
    "\n",
    "### Comparison: Istio vs Linkerd\n",
    "\n",
    "| Feature | Istio | Linkerd |\n",
    "|---------|-------|---------|\n",
    "| **Resource Cost** | High (Envoy overhead) | Low (Rust proxy) |\n",
    "| **Configuration Complexity** | Extensive CRDs | Minimal CRDs |\n",
    "| **Traffic Management** | Advanced (traffic mirroring, fault injection) | Basic (load balancing, retries) |\n",
    "| **mTLS** | Automatic, configurable | Always-on by default |\n",
    "| **Multi-cluster** | Native with east-west gateways | Requires additional setup |\n",
    "| **VM Support** | Istio Agent for VMs | Limited |\n",
    "| **WebAssembly** | Extensible via WASM filters | Limited support |\n",
    "| **CNI Integration** | Optional (CNI plugin available) | Required for opaque ports |\n",
    "\n",
    "**Selection Guidance:**\n",
    "- Choose **Istio** when requiring advanced traffic shaping, multi-cluster federation, or extensive extensibility\n",
    "- Choose **Linkerd** when prioritizing simplicity, resource efficiency, and rapid onboarding\n",
    "\n",
    "## 58.4 Traffic Management\n",
    "\n",
    "Service meshes enable sophisticated traffic routing strategies essential for CI/CD pipelines, allowing gradual rollouts and A/B testing without application modifications.\n",
    "\n",
    "### Canary Deployments\n",
    "\n",
    "Canary deployments route a small percentage of traffic to new versions, validating behavior before full rollout.\n",
    "\n",
    "**Istio Canary Implementation:**\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: payment-service\n",
    "spec:\n",
    "  hosts:\n",
    "  - payment-service\n",
    "  http:\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: payment-service\n",
    "        subset: v1\n",
    "      weight: 95\n",
    "    - destination:\n",
    "        host: payment-service\n",
    "        subset: v2\n",
    "      weight: 5\n",
    "    fault:\n",
    "      delay:\n",
    "        percentage:\n",
    "          value: 0.1  # 0.1% of requests\n",
    "        fixedDelay: 5s  # Inject latency to test timeout handling\n",
    "```\n",
    "\n",
    "**Automated Canary Analysis (Flagger):**\n",
    "Flagger integrates with Istio/Linkerd to automate canary promotion based on metrics:\n",
    "\n",
    "```yaml\n",
    "apiVersion: flagger.app/v1beta1\n",
    "kind: Canary\n",
    "metadata:\n",
    "  name: payment-service\n",
    "spec:\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: payment-service\n",
    "  service:\n",
    "    port: 8080\n",
    "  analysis:\n",
    "    interval: 30s\n",
    "    threshold: 5  # Max failed checks before rollback\n",
    "    maxWeight: 50\n",
    "    stepWeight: 10\n",
    "    metrics:\n",
    "    - name: request-success-rate\n",
    "      thresholdRange:\n",
    "        min: 99\n",
    "      interval: 1m\n",
    "    - name: request-duration\n",
    "      thresholdRange:\n",
    "        max: 500\n",
    "      interval: 1m\n",
    "    webhooks:\n",
    "    - name: load-test\n",
    "      url: http://flagger-loadtester.test/\n",
    "      timeout: 5s\n",
    "      metadata:\n",
    "        cmd: \"hey -z 1m -q 10 -c 2 http://payment-service-canary:8080/\"\n",
    "```\n",
    "\n",
    "### Traffic Mirroring (Shadowing)\n",
    "\n",
    "Mirror production traffic to canary versions without impacting users\u2014ideal for testing new versions with real data volumes:\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: payment-mirror\n",
    "spec:\n",
    "  hosts:\n",
    "  - payment-service\n",
    "  http:\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: payment-service\n",
    "        subset: v1\n",
    "      weight: 100\n",
    "    mirror:\n",
    "      host: payment-service\n",
    "      subset: v2\n",
    "    mirrorPercentage:\n",
    "      value: 100.0  # Mirror 100% of traffic\n",
    "```\n",
    "\n",
    "**Important Considerations:**\n",
    "- Mirrored traffic is \"fire and forget\"\u2014responses from v2 are discarded\n",
    "- Ensure idempotency; mirrored requests execute write operations\n",
    "- Monitor resource consumption; mirroring doubles load\n",
    "\n",
    "### Circuit Breaking\n",
    "\n",
    "Prevent cascade failures by ejecting unhealthy endpoints from the pool:\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: DestinationRule\n",
    "metadata:\n",
    "  name: payment-circuit-breaker\n",
    "spec:\n",
    "  host: payment-service\n",
    "  trafficPolicy:\n",
    "    connectionPool:\n",
    "      tcp:\n",
    "        maxConnections: 100\n",
    "      http:\n",
    "        http1MaxPendingRequests: 50\n",
    "        maxRequestsPerConnection: 2\n",
    "    outlierDetection:\n",
    "      consecutiveErrors: 5\n",
    "      interval: 30s\n",
    "      baseEjectionTime: 30s\n",
    "      maxEjectionPercent: 50  # Max 50% of hosts can be ejected\n",
    "```\n",
    "\n",
    "**Testing Circuit Breakers:**\n",
    "Use Fortio to generate load and verify circuit breaker activation:\n",
    "```bash\n",
    "kubectl apply -f - <<EOF\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: fortio\n",
    "spec:\n",
    "  containers:\n",
    "  - name: fortio\n",
    "    image: fortio/fortio\n",
    "    command: ['fortio', 'load', '-c', '3', '-qps', '0', '-n', '20', '-loglevel', 'Warning', 'http://payment-service:8080/']\n",
    "EOF\n",
    "```\n",
    "\n",
    "## 58.5 Security with mTLS\n",
    "\n",
    "Service meshes implement zero-trust security through automatic mutual TLS (mTLS), encrypting service-to-service communication and providing strong identity authentication without application changes.\n",
    "\n",
    "### mTLS Implementation\n",
    "\n",
    "**How It Works:**\n",
    "1. **Identity**: Each service account receives a SPIFFE (Secure Production Identity Framework For Everyone) ID (e.g., `spiffe://cluster.local/ns/default/sa/payment-service`)\n",
    "2. **Certificate**: Control plane (Istiod/Identity) issues short-lived X.509 certificates (~24 hours) to sidecars\n",
    "3. **Authentication**: Sidecars present certificates during TLS handshake; reject unauthenticated connections\n",
    "4. **Authorization**: Policies enforce which services may communicate\n",
    "\n",
    "**Istio mTLS Modes:**\n",
    "- **PERMISSIVE**: Accept both plaintext and TLS (migration mode)\n",
    "- **STRICT**: Reject plaintext, require mTLS\n",
    "- **DISABLE**: No mTLS (not recommended)\n",
    "\n",
    "**Enforcing Strict mTLS:**\n",
    "```yaml\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: PeerAuthentication\n",
    "metadata:\n",
    "  name: default\n",
    "  namespace: production\n",
    "spec:\n",
    "  mtls:\n",
    "    mode: STRICT\n",
    "---\n",
    "# Verify with istioctl\n",
    "istioctl authn tls-check payment-service.production.svc.cluster.local\n",
    "```\n",
    "\n",
    "### Authorization Policies\n",
    "\n",
    "Fine-grained access control beyond mTLS:\n",
    "\n",
    "```yaml\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: AuthorizationPolicy\n",
    "metadata:\n",
    "  name: payment-policy\n",
    "  namespace: production\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: payment-service\n",
    "  action: ALLOW\n",
    "  rules:\n",
    "  - from:\n",
    "    - source:\n",
    "        principals: [\"cluster.local/ns/production/sa/frontend-service\"]\n",
    "    to:\n",
    "    - operation:\n",
    "        methods: [\"GET\", \"POST\"]\n",
    "        paths: [\"/api/payments/*\"]\n",
    "  - from:\n",
    "    - source:\n",
    "        principals: [\"cluster.local/ns/production/sa/admin-service\"]\n",
    "    to:\n",
    "    - operation:\n",
    "        methods: [\"DELETE\"]\n",
    "        paths: [\"/api/payments/*\"]\n",
    "```\n",
    "\n",
    "### CI/CD Integration for Security\n",
    "\n",
    "**Pipeline Verification:**\n",
    "Ensure mTLS policies exist before production deployment:\n",
    "\n",
    "```yaml\n",
    "# GitHub Actions example\n",
    "- name: Verify mTLS Strict Mode\n",
    "  run: |\n",
    "    STRICT_MODE=$(kubectl get peerauthentication -n production default -o jsonpath='{.spec.mtls.mode}')\n",
    "    if [ \"$STRICT_MODE\" != \"STRICT\" ]; then\n",
    "      echo \"ERROR: mTLS not in strict mode\"\n",
    "      exit 1\n",
    "    fi\n",
    "\n",
    "- name: Check Authorization Policies\n",
    "  run: |\n",
    "    POLICY_COUNT=$(kubectl get authorizationpolicy -n production --no-headers | wc -l)\n",
    "    if [ \"$POLICY_COUNT\" -eq 0 ]; then\n",
    "      echo \"WARNING: No authorization policies defined\"\n",
    "    fi\n",
    "```\n",
    "\n",
    "**Certificate Rotation:**\n",
    "Service meshes automatically rotate certificates. For CI/CD, ensure:\n",
    "- Pods have sufficient time to receive new certificates before old expire (Istio: 24h certs, rotates at 80% lifetime)\n",
    "- Readiness probes consider certificate availability\n",
    "- Jobs that communicate with mesh services use short-lived tokens or exclude from mesh\n",
    "\n",
    "## 58.6 Observability\n",
    "\n",
    "Service meshes provide uniform telemetry across all services regardless of language or framework, generating golden metrics (latency, traffic, errors, saturation) without code instrumentation.\n",
    "\n",
    "### Metrics\n",
    "\n",
    "**Automatic Metrics Collection:**\n",
    "Envoy/Linkerd proxies expose Prometheus metrics:\n",
    "- `istio_requests_total`: Request count by response code\n",
    "- `istio_request_duration_seconds`: Request latency histograms\n",
    "- `istio_tcp_sent_bytes_total`: TCP traffic metrics\n",
    "\n",
    "**Prometheus ServiceMonitor:**\n",
    "```yaml\n",
    "apiVersion: monitoring.coreos.com/v1\n",
    "kind: ServiceMonitor\n",
    "metadata:\n",
    "  name: istio-metrics\n",
    "  namespace: monitoring\n",
    "spec:\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      istio: monitoring\n",
    "  namespaceSelector:\n",
    "    any: true\n",
    "  endpoints:\n",
    "  - port: http-envoy-prom\n",
    "    path: /stats/prometheus\n",
    "    interval: 15s\n",
    "    relabelings:\n",
    "    - sourceLabels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n",
    "      action: keep\n",
    "      regex: true\n",
    "```\n",
    "\n",
    "**Grafana Dashboards:**\n",
    "Import Istio standard dashboards (ID 7639) for:\n",
    "- Mesh overview (request volume, success rate, latency)\n",
    "- Service dashboard (inbound/outbound metrics)\n",
    "- Workload dashboard (resource utilization)\n",
    "\n",
    "### Distributed Tracing\n",
    "\n",
    "Service meshes automatically propagate trace headers (B3, W3C Trace Context), enabling end-to-end request visualization.\n",
    "\n",
    "**Jaeger Integration:**\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: Telemetry\n",
    "metadata:\n",
    "  name: mesh-default\n",
    "spec:\n",
    "  tracing:\n",
    "  - providers:\n",
    "    - name: jaeger\n",
    "    randomSamplingPercentage: 10.0  # Sample 10% of requests\n",
    "    customTags:\n",
    "      environment:\n",
    "        literal:\n",
    "          value: \"production\"\n",
    "```\n",
    "\n",
    "**Trace Context Propagation:**\n",
    "Applications must forward headers (no modification required):\n",
    "- `x-request-id`\n",
    "- `x-b3-traceid`\n",
    "- `x-b3-spanid`\n",
    "- `x-b3-sampled`\n",
    "- `x-b3-flags`\n",
    "- `x-ot-span-context`\n",
    "\n",
    "### Service Graph and Topology\n",
    "\n",
    "**Kiali (Istio):**\n",
    "Visualizes service dependencies, traffic flows, and health:\n",
    "```bash\n",
    "istioctl dashboard kiali\n",
    "```\n",
    "\n",
    "**Linkerd Viz:**\n",
    "```bash\n",
    "linkerd viz dashboard &\n",
    "linkerd viz stat deployments\n",
    "linkerd viz top deployment/payment-service\n",
    "```\n",
    "\n",
    "### CI/CD Observability Integration\n",
    "\n",
    "**Pipeline Metrics:**\n",
    "Export deployment metrics to Prometheus:\n",
    "```yaml\n",
    "# ArgoCD example with prometheus metrics\n",
    "apiVersion: argoproj.io/v1alpha1\n",
    "kind: Application\n",
    "metadata:\n",
    "  name: payment-service\n",
    "  annotations:\n",
    "    prometheus.io/scrape: \"true\"\n",
    "    prometheus.io/port: \"8082\"\n",
    "spec:\n",
    "  project: production\n",
    "  source:\n",
    "    repoURL: https://github.com/company/gitops\n",
    "    targetRevision: HEAD\n",
    "    path: apps/payment-service\n",
    "  destination:\n",
    "    server: https://kubernetes.default.svc\n",
    "    namespace: production\n",
    "  syncPolicy:\n",
    "    automated:\n",
    "      prune: true\n",
    "      selfHeal: true\n",
    "```\n",
    "\n",
    "## 58.7 CI/CD Integration\n",
    "\n",
    "Integrating service meshes into CI/CD pipelines requires careful handling of deployment strategies, automated testing, and progressive delivery.\n",
    "\n",
    "### GitOps with Service Mesh\n",
    "\n",
    "**Structure:**\n",
    "```\n",
    "repo/\n",
    "\u251c\u2500\u2500 base/\n",
    "\u2502   \u251c\u2500\u2500 deployment.yaml\n",
    "\u2502   \u251c\u2500\u2500 service.yaml\n",
    "\u2502   \u2514\u2500\u2500 virtualservice.yaml\n",
    "\u251c\u2500\u2500 overlays/\n",
    "\u2502   \u251c\u2500\u2500 staging/\n",
    "\u2502   \u2502   \u251c\u2500\u2500 kustomization.yaml\n",
    "\u2502   \u2502   \u2514\u2500\u2500 virtualservice-patch.yaml  # 100% to stable\n",
    "\u2502   \u2514\u2500\u2500 production/\n",
    "\u2502       \u251c\u2500\u2500 kustomization.yaml\n",
    "\u2502       \u2514\u2500\u2500 virtualservice-patch.yaml  # Canary weights\n",
    "\u2514\u2500\u2500 canary/\n",
    "    \u2514\u2500\u2500 canary-config.yaml  # Flagger configuration\n",
    "```\n",
    "\n",
    "**Progressive Delivery Pipeline:**\n",
    "```yaml\n",
    "# .github/workflows/deploy.yml\n",
    "name: Progressive Deployment\n",
    "on:\n",
    "  push:\n",
    "    branches: [main]\n",
    "\n",
    "jobs:\n",
    "  deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Deploy to Staging\n",
    "      run: |\n",
    "        kubectl apply -k overlays/staging/\n",
    "        kubectl wait --for=condition=available deployment/payment-service -n staging --timeout=300s\n",
    "    \n",
    "    - name: Integration Tests\n",
    "      run: |\n",
    "        curl -f https://staging-api.company.com/health || exit 1\n",
    "    \n",
    "    - name: Deploy to Production (Canary)\n",
    "      run: |\n",
    "        kubectl apply -k overlays/production/\n",
    "        kubectl apply -f canary/canary-config.yaml\n",
    "    \n",
    "    - name: Wait for Canary Analysis\n",
    "      run: |\n",
    "        kubectl wait --for=condition=Promoted canary/payment-service -n production --timeout=600s\n",
    "```\n",
    "\n",
    "### Automated Canary Testing\n",
    "\n",
    "**Flagger Integration:**\n",
    "Flagger automates the promotion or rollback based on metrics:\n",
    "\n",
    "```yaml\n",
    "apiVersion: flagger.app/v1beta1\n",
    "kind: Canary\n",
    "metadata:\n",
    "  name: payment-service\n",
    "  namespace: production\n",
    "spec:\n",
    "  provider: istio\n",
    "  targetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: payment-service\n",
    "  service:\n",
    "    port: 8080\n",
    "    gateways:\n",
    "    - public-gateway\n",
    "    hosts:\n",
    "    - api.company.com\n",
    "  analysis:\n",
    "    interval: 1m\n",
    "    threshold: 5\n",
    "    maxWeight: 50\n",
    "    stepWeight: 10\n",
    "    metrics:\n",
    "    - name: request-success-rate\n",
    "      thresholdRange:\n",
    "        min: 99\n",
    "      interval: 1m\n",
    "    - name: request-duration\n",
    "      thresholdRange:\n",
    "        max: 500\n",
    "      interval: 30s\n",
    "    webhooks:\n",
    "    - name: conformance-test\n",
    "      type: pre-rollout\n",
    "      url: http://flagger-loadtester.test/\n",
    "      timeout: 30s\n",
    "      metadata:\n",
    "        cmd: \"curl -f http://payment-service-canary:8080/api/health\"\n",
    "    - name: load-test\n",
    "      type: rollout\n",
    "      url: http://flagger-loadtester.test/\n",
    "      timeout: 5s\n",
    "      metadata:\n",
    "        cmd: \"hey -z 2m -q 10 -c 2 http://payment-service-canary:8080/api/payments\"\n",
    "```\n",
    "\n",
    "### Smoke Testing with Traffic Routing\n",
    "\n",
    "Test new versions before exposing to users:\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: payment-smoke-test\n",
    "spec:\n",
    "  hosts:\n",
    "  - payment-service\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        x-smoke-test:\n",
    "          exact: \"true\"\n",
    "    route:\n",
    "    - destination:\n",
    "        host: payment-service\n",
    "        subset: candidate  # New version\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: payment-service\n",
    "        subset: stable     # Current version\n",
    "```\n",
    "\n",
    "**Pipeline Usage:**\n",
    "```bash\n",
    "# Deploy candidate\n",
    "kubectl set image deployment/payment-service payment-service=myimage:candidate\n",
    "\n",
    "# Run smoke tests against candidate via header\n",
    "curl -H \"x-smoke-test: true\" http://payment-service/api/payments\n",
    "\n",
    "# Promote if successful\n",
    "kubectl apply -f production-virtualservice.yaml  # 100% to new version\n",
    "```\n",
    "\n",
    "## 58.8 Best Practices\n",
    "\n",
    "### Production Readiness\n",
    "\n",
    "**1. Resource Management:**\n",
    "Service mesh sidecars consume resources. Always set resource limits:\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "spec:\n",
    "  containers:\n",
    "  - name: istio-proxy\n",
    "    resources:\n",
    "      requests:\n",
    "        cpu: 100m\n",
    "        memory: 128Mi\n",
    "      limits:\n",
    "        cpu: 500m\n",
    "        memory: 256Mi\n",
    "```\n",
    "\n",
    "**2. Gradual Adoption:**\n",
    "Enable mesh features incrementally:\n",
    "- Week 1: Deploy with mTLS in PERMISSIVE mode (monitor for issues)\n",
    "- Week 2: Switch to STRICT mTLS\n",
    "- Week 3: Add authorization policies\n",
    "- Week 4: Implement canary deployments\n",
    "\n",
    "**3. Circuit Breaker Configuration:**\n",
    "Tune based on service characteristics:\n",
    "- Fast services (sub-100ms): Lower thresholds (3 errors)\n",
    "- Slow services (1s+): Higher thresholds (10 errors) to avoid premature ejection\n",
    "\n",
    "**4. Observability Hygiene:**\n",
    "- Retain trace sampling at 1-10% in production (100% sampling overwhelms storage)\n",
    "- Use service mesh metrics for SLOs, not container-level metrics\n",
    "- Set up alerts on `istio_requests_total` with 5xx codes\n",
    "\n",
    "### Security Best Practices\n",
    "\n",
    "**1. Namespace Isolation:**\n",
    "Enforce mTLS per namespace, avoiding cluster-wide policies that impact system namespaces:\n",
    "```yaml\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: PeerAuthentication\n",
    "metadata:\n",
    "  name: default\n",
    "  namespace: production  # Specific namespace only\n",
    "spec:\n",
    "  mtls:\n",
    "    mode: STRICT\n",
    "```\n",
    "\n",
    "**2. Least Privilege:**\n",
    "Default-deny authorization:\n",
    "```yaml\n",
    "apiVersion: security.istio.io/v1beta1\n",
    "kind: AuthorizationPolicy\n",
    "metadata:\n",
    "  name: deny-all\n",
    "  namespace: production\n",
    "spec:\n",
    "  action: DENY\n",
    "  # No rules = deny all\n",
    "```\n",
    "\n",
    "**3. Ingress Security:**\n",
    "Always terminate TLS at the Gateway, not in applications:\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: Gateway\n",
    "metadata:\n",
    "  name: secure-gateway\n",
    "spec:\n",
    "  servers:\n",
    "  - port:\n",
    "      number: 443\n",
    "      protocol: HTTPS\n",
    "    tls:\n",
    "      mode: SIMPLE\n",
    "      credentialName: tls-cert\n",
    "      minProtocolVersion: TLSV1_2\n",
    "      cipherSuites:\n",
    "      - ECDHE-RSA-AES256-GCM-SHA384\n",
    "```\n",
    "\n",
    "### Performance Optimization\n",
    "\n",
    "**1. Sidecar Resource Tuning:**\n",
    "Use sidecar resource annotations to exclude unnecessary egress traffic:\n",
    "```yaml\n",
    "annotations:\n",
    "  traffic.sidecar.istio.io/includeOutboundIPRanges: \"10.0.0.0/8\"  # Only mesh traffic\n",
    "  traffic.sidecar.istio.io/excludeInboundPorts: \"9090\"  # Exclude metrics port\n",
    "```\n",
    "\n",
    "**2. Connection Pooling:**\n",
    "Reuse connections between sidecars to reduce TCP overhead:\n",
    "```yaml\n",
    "trafficPolicy:\n",
    "  connectionPool:\n",
    "    http:\n",
    "      h2UpgradePolicy: UPGRADE  # HTTP/2 where possible\n",
    "      maxRequestsPerConnection: 100\n",
    "```\n",
    "\n",
    "**3. Egress Traffic:**\n",
    "Control external access via ServiceEntry to avoid DNS resolution issues:\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: ServiceEntry\n",
    "metadata:\n",
    "  name: external-api\n",
    "spec:\n",
    "  hosts:\n",
    "  - api.stripe.com\n",
    "  ports:\n",
    "  - number: 443\n",
    "    name: https\n",
    "    protocol: TLS\n",
    "  resolution: DNS\n",
    "  location: MESH_EXTERNAL\n",
    "```\n",
    "\n",
    "### Troubleshooting\n",
    "\n",
    "**Common Issues:**\n",
    "1. **503 Errors**: Check DestinationRule outlier detection ejecting pods; verify subset labels match\n",
    "2. **Connection Refused**: Ensure sidecar injection enabled for namespace; check `istio-proxy` logs\n",
    "3. **mTLS Failures**: Verify PeerAuthentication policies don't conflict; check certificate validity with `istioctl proxy-config secret`\n",
    "4. **Traffic Not Routing**: Confirm VirtualService host matches Service name exactly (FQDN not required but must match)\n",
    "\n",
    "**Debugging Commands:**\n",
    "```bash\n",
    "# Check proxy configuration\n",
    "istioctl proxy-config cluster <pod-name>\n",
    "\n",
    "# Check certificates\n",
    "istioctl proxy-config secret <pod-name>\n",
    "\n",
    "# Analyze configuration\n",
    "istioctl analyze\n",
    "\n",
    "# View proxy logs\n",
    "kubectl logs <pod-name> -c istio-proxy\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "This chapter explored service mesh integration within CI/CD pipelines, establishing how Istio and Linkerd provide critical infrastructure for microservices communication. We examined the data plane and control plane architecture, distinguishing between Istio's feature-rich extensibility and Linkerd's operational simplicity. Traffic management capabilities enable sophisticated deployment patterns\u2014canary releases, traffic mirroring, and circuit breaking\u2014without application code modifications. Security implementations through automatic mTLS establish zero-trust networking, while fine-grained authorization policies enforce least-privilege access between services. Observability features provide uniform telemetry collection across polyglot environments, generating distributed traces and golden metrics essential for SLO monitoring.\n",
    "\n",
    "Integration strategies demonstrated GitOps workflows with automated canary analysis using Flagger, smoke testing via header-based routing, and progressive delivery pipelines that automatically promote or rollback based on service mesh metrics. Best practices emphasized resource management for sidecar proxies, gradual security adoption (permissive to strict mTLS), and performance optimization through connection pooling and egress control.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Service meshes externalize cross-cutting concerns (security, reliability, observability) from application code, enabling polyglot microservices with consistent operational characteristics\n",
    "- Istio suits complex enterprise requirements requiring advanced traffic management; Linkerd excels in resource-constrained environments prioritizing simplicity\n",
    "- Always implement mTLS in permissive mode initially, monitoring for compatibility issues before enforcing strict mode\n",
    "- Use canary deployments with automated metrics analysis (success rate, latency) rather than time-based promotions to catch issues early\n",
    "- Configure circuit breakers based on service latency characteristics\u2014aggressive for fast services, lenient for slow services\n",
    "- Maintain sidecar resource limits to prevent proxy processes from consuming application resources\n",
    "\n",
    "**Next Chapter Preview:**\n",
    "Chapter 59: Serverless CI/CD transitions from container orchestration to event-driven, scale-to-zero compute models. We will explore Knative as the Kubernetes-native serverless platform, enabling automatic scaling from zero to many based on HTTP request load. The chapter examines AWS Lambda, Google Cloud Functions, and Azure Functions integration within CI/CD pipelines, addressing cold start optimization, function packaging, and infrastructure-as-code deployment. We will investigate the unique challenges of serverless CI/CD\u2014function versioning, environment variable management, and integration testing of distributed event-driven architectures\u2014completing the spectrum from long-running containers to ephemeral functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../10. devops_culture_and_best_practices/57. measuring_cicd_success.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='59. serverless_cicd.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}