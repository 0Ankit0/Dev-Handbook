{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 60: AI/ML Model Deployment\n",
    "\n",
    "Machine learning systems fundamentally differ from traditional software in one critical dimension: they depend on data, not just code. While conventional CI/CD pipelines version source code and compile binaries, ML pipelines must version datasets, track experiments, manage model artifacts, and validate probabilistic outputs against evolving data distributions. Deploying a model involves not merely pushing a container image, but ensuring that training data lineage, feature engineering logic, and model weights remain synchronized across development, staging, and production environments. This chapter examines MLOps (Machine Learning Operations)\u2014the extension of DevOps practices to machine learning\u2014covering how to automate model training, serve predictions at scale, and maintain model performance through continuous monitoring and automated retraining.\n",
    "\n",
    "## 60.1 ML Pipelines\n",
    "\n",
    "ML pipelines orchestrate the end-to-end lifecycle of machine learning models, from data ingestion through training, validation, and deployment. Unlike traditional build pipelines that compile deterministic code, ML pipelines handle stochastic processes where the same code may produce different models when trained on different data snapshots.\n",
    "\n",
    "### The MLOps Lifecycle\n",
    "\n",
    "**Continuous Integration (CI) for ML:**\n",
    "Extends beyond code testing to include:\n",
    "- Data validation (schema checks, drift detection)\n",
    "- Feature engineering pipeline testing\n",
    "- Model unit tests (architecture validation, gradient flow checks)\n",
    "- Integration tests between model and serving infrastructure\n",
    "\n",
    "**Continuous Training (CT):**\n",
    "A unique ML stage where models retrain automatically when:\n",
    "- New labeled data arrives (batch retraining)\n",
    "- Data drift exceeds thresholds (triggered retraining)\n",
    "- Scheduled intervals (nightly/weekly retraining)\n",
    "- Performance degradation detected in production\n",
    "\n",
    "**Continuous Delivery (CD) for ML:**\n",
    "Deploys not just code, but model artifacts, feature stores, and inference configurations. This includes canary deployments where a small percentage of traffic routes to the new model while monitoring for prediction quality degradation.\n",
    "\n",
    "**Continuous Monitoring:**\n",
    "Tracks model performance metrics (accuracy, precision, recall, F1) alongside infrastructure metrics (latency, throughput). Unlike software monitoring which focuses on availability, ML monitoring watches for concept drift\u2014when the statistical relationship between inputs and outputs changes over time.\n",
    "\n",
    "### Pipeline Architecture\n",
    "\n",
    "A production ML pipeline typically consists of:\n",
    "\n",
    "**Data Extraction:**\n",
    "Pulls raw data from warehouses (Snowflake, BigQuery), lakes (S3, GCS), or streaming sources (Kafka, Kinesis). Versioning occurs at this stage using tools like DVC (Data Version Control) or LakeFS.\n",
    "\n",
    "**Data Validation:**\n",
    "Checks for schema skew (new columns, type changes), distribution shifts (training-serving skew), and missing values. TensorFlow Data Validation (TFDV) or Great Expectations validate data against predefined schemas.\n",
    "\n",
    "**Feature Engineering:**\n",
    "Transforms raw data into features suitable for model consumption. In production pipelines, feature engineering must be consistent between training and serving to prevent training-serving skew.\n",
    "\n",
    "**Model Training:**\n",
    "Executes training algorithms, tracking hyperparameters and metrics. Distributed training across multiple GPUs requires specific orchestration (Horovod, Ray Train, or PyTorch DDP).\n",
    "\n",
    "**Model Evaluation:**\n",
    "Validates against holdout datasets and custom metrics. Includes fairness checks (demographic parity, equalized odds) and bias detection.\n",
    "\n",
    "**Model Packaging:**\n",
    "Bundles model weights, preprocessing code, and dependencies into deployable artifacts (Docker images, SavedModel format, ONNX, or MLflow models).\n",
    "\n",
    "**Deployment:**\n",
    "Pushes models to serving infrastructure with traffic management strategies (A/B testing, shadow deployments).\n",
    "\n",
    "### CI/CD Integration Example\n",
    "\n",
    "```yaml\n",
    "# Kubeflow Pipeline definition (Python DSL)\n",
    "import kfp\n",
    "from kfp import dsl\n",
    "from kfp.components import create_component_from_func\n",
    "\n",
    "@create_component_from_func\n",
    "def data_validation(input_path: str, schema_path: str) -> str:\n",
    "    import tensorflow_data_validation as tfdv\n",
    "    from tensorflow_metadata.proto.v0 import schema_pb2\n",
    "    \n",
    "    # Load statistics\n",
    "    stats = tfdv.generate_statistics_from_tfrecord(input_path)\n",
    "    \n",
    "    # Load schema\n",
    "    schema = tfdv.load_schema_text(schema_path)\n",
    "    \n",
    "    # Validate\n",
    "    anomalies = tfdv.validate_statistics(stats, schema)\n",
    "    \n",
    "    if len(anomalies.anomaly_info) > 0:\n",
    "        raise ValueError(f\"Data anomalies detected: {anomalies}\")\n",
    "    \n",
    "    return \"Validation passed\"\n",
    "\n",
    "@create_component_from_func\n",
    "def train_model(\n",
    "    data_path: str,\n",
    "    model_params: dict,\n",
    "    output_path: str\n",
    ") -> str:\n",
    "    import xgboost as xgb\n",
    "    import pandas as pd\n",
    "    import pickle\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    # Load data\n",
    "    df = pd.read_csv(data_path)\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2)\n",
    "    \n",
    "    # Train\n",
    "    model = xgb.XGBClassifier(**model_params)\n",
    "    model.fit(\n",
    "        X_train, y_train,\n",
    "        eval_set=[(X_val, y_val)],\n",
    "        early_stopping_rounds=10\n",
    "    )\n",
    "    \n",
    "    # Save\n",
    "    with open(output_path, 'wb') as f:\n",
    "        pickle.dump(model, f)\n",
    "    \n",
    "    return output_path\n",
    "\n",
    "@create_component_from_func\n",
    "def evaluate_model(\n",
    "    model_path: str,\n",
    "    test_data_path: str,\n",
    "    threshold: float\n",
    ") -> bool:\n",
    "    import pickle\n",
    "    import pandas as pd\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    with open(model_path, 'rb') as f:\n",
    "        model = pickle.load(f)\n",
    "    \n",
    "    df = pd.read_csv(test_data_path)\n",
    "    X = df.drop('target', axis=1)\n",
    "    y = df['target']\n",
    "    \n",
    "    preds = model.predict(X)\n",
    "    accuracy = accuracy_score(y, preds)\n",
    "    \n",
    "    if accuracy < threshold:\n",
    "        raise ValueError(f\"Accuracy {accuracy} below threshold {threshold}\")\n",
    "    \n",
    "    return True\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='Churn Prediction Pipeline',\n",
    "    description='End-to-end training pipeline'\n",
    ")\n",
    "def churn_pipeline(\n",
    "    data_path: str = 'gs://bucket/data/train.csv',\n",
    "    schema_path: str = 'gs://bucket/schema/schema.pbtxt',\n",
    "    model_params: dict = {'max_depth': 6, 'n_estimators': 100}\n",
    "):\n",
    "    # Data validation\n",
    "    validation_op = data_validation(data_path, schema_path)\n",
    "    \n",
    "    # Training\n",
    "    train_op = train_model(\n",
    "        data_path=data_path,\n",
    "        model_params=model_params,\n",
    "        output_path='gs://bucket/models/model.pkl'\n",
    "    ).after(validation_op)\n",
    "    \n",
    "    # Evaluation\n",
    "    eval_op = evaluate_model(\n",
    "        model_path=train_op.output,\n",
    "        test_data_path='gs://bucket/data/test.csv',\n",
    "        threshold=0.85\n",
    "    ).after(train_op)\n",
    "    \n",
    "    # Conditional deployment\n",
    "    with dsl.Condition(eval_op.output == True):\n",
    "        deploy_op = dsl.ContainerOp(\n",
    "            name='deploy-model',\n",
    "            image='gcr.io/project/deployer:latest',\n",
    "            arguments=['--model-path', train_op.output]\n",
    "        )\n",
    "\n",
    "# Compile and run\n",
    "kfp.compiler.Compiler().compile(churn_pipeline, 'pipeline.yaml')\n",
    "```\n",
    "\n",
    "## 60.2 Model Training in CI\n",
    "\n",
    "Integrating model training into CI requires managing computational resources, tracking experiments, and ensuring reproducibility across runs.\n",
    "\n",
    "### Resource Management\n",
    "\n",
    "**GPU Scheduling:**\n",
    "Training deep learning models requires GPU resources, which Kubernetes manages via device plugins:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: training-job\n",
    "spec:\n",
    "  containers:\n",
    "  - name: trainer\n",
    "    image: gcr.io/project/ml-trainer:latest\n",
    "    resources:\n",
    "      limits:\n",
    "        nvidia.com/gpu: 4  # Request 4 GPUs\n",
    "      requests:\n",
    "        nvidia.com/gpu: 4\n",
    "    volumeMounts:\n",
    "    - name: dshm\n",
    "      mountPath: /dev/shm\n",
    "  volumes:\n",
    "  - name: dshm\n",
    "    emptyDir:\n",
    "      medium: Memory  # Shared memory for multiprocessing\n",
    "      sizeLimit: 10Gi\n",
    "  nodeSelector:\n",
    "    node-type: gpu-node  # Schedule on GPU-enabled nodes\n",
    "  tolerations:\n",
    "  - key: nvidia.com/gpu\n",
    "    operator: Exists\n",
    "    effect: NoSchedule\n",
    "```\n",
    "\n",
    "**Distributed Training:**\n",
    "For large models (LLMs, computer vision), distribute training across multiple nodes:\n",
    "\n",
    "```python\n",
    "# PyTorch Distributed Data Parallel (DDP)\n",
    "import torch\n",
    "import torch.distributed as dist\n",
    "from torch.nn.parallel import DistributedDataParallel as DDP\n",
    "\n",
    "def setup():\n",
    "    dist.init_process_group(\"nccl\")\n",
    "    local_rank = int(os.environ[\"LOCAL_RANK\"])\n",
    "    torch.cuda.set_device(local_rank)\n",
    "\n",
    "def train():\n",
    "    setup()\n",
    "    model = MyModel().to(local_rank)\n",
    "    ddp_model = DDP(model, device_ids=[local_rank])\n",
    "    \n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        for batch in dataloader:\n",
    "            inputs, labels = batch\n",
    "            inputs = inputs.to(local_rank)\n",
    "            labels = labels.to(local_rank)\n",
    "            \n",
    "            outputs = ddp_model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    train()\n",
    "```\n",
    "\n",
    "**Spot/Preemptible Instances:**\n",
    "Reduce training costs by using spot instances with checkpointing:\n",
    "\n",
    "```yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: spot-training\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      nodeSelector:\n",
    "        cloud.google.com/gke-spot: \"true\"  # GKE spot nodes\n",
    "      containers:\n",
    "      - name: trainer\n",
    "        image: trainer:latest\n",
    "        command: [\"python\", \"train.py\", \"--resume-from-checkpoint\"]\n",
    "        volumeMounts:\n",
    "        - name: checkpoints\n",
    "          mountPath: /checkpoints\n",
    "      volumes:\n",
    "      - name: checkpoints\n",
    "        persistentVolumeClaim:\n",
    "          claimName: training-checkpoints\n",
    "      restartPolicy: OnFailure\n",
    "```\n",
    "\n",
    "### Experiment Tracking\n",
    "\n",
    "Track hyperparameters, metrics, and artifacts using MLflow or Weights & Biases:\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "\n",
    "def train_with_tracking(params):\n",
    "    mlflow.set_experiment(\"churn-prediction\")\n",
    "    \n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_params(params)\n",
    "        \n",
    "        # Train\n",
    "        model = xgb.XGBClassifier(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        accuracy = model.score(X_test, y_test)\n",
    "        \n",
    "        # Log metrics\n",
    "        mlflow.log_metric(\"accuracy\", accuracy)\n",
    "        mlflow.log_metric(\"precision\", precision_score(y_test, preds))\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.sklearn.log_model(\n",
    "            model, \n",
    "            \"model\",\n",
    "            registered_model_name=\"churn-model\"\n",
    "        )\n",
    "        \n",
    "        # Log artifacts (confusion matrix, ROC curve)\n",
    "        mlflow.log_artifact(\"confusion_matrix.png\")\n",
    "\n",
    "# Hyperparameter sweep\n",
    "from sklearn.model_selection import ParameterGrid\n",
    "\n",
    "param_grid = {\n",
    "    'max_depth': [3, 6, 9],\n",
    "    'learning_rate': [0.01, 0.1],\n",
    "    'n_estimators': [100, 200]\n",
    "}\n",
    "\n",
    "for params in ParameterGrid(param_grid):\n",
    "    train_with_tracking(params)\n",
    "```\n",
    "\n",
    "### Data Versioning\n",
    "\n",
    "Unlike code, datasets change frequently. Use DVC (Data Version Control) or Git LFS to version data alongside code:\n",
    "\n",
    "```bash\n",
    "# Initialize DVC\n",
    "dvc init\n",
    "\n",
    "# Track dataset\n",
    "dvc add data/train.csv\n",
    "\n",
    "# Push to remote storage (S3, GCS, Azure)\n",
    "dvc remote add -d storage s3://mybucket/dvcstore\n",
    "dvc push\n",
    "\n",
    "# Commit metadata to git\n",
    "git add data/train.csv.dvc .gitignore\n",
    "git commit -m \"Add version 2.0 of training data\"\n",
    "\n",
    "# Checkout specific version\n",
    "git checkout v1.0\n",
    "dvc checkout  # Restores data matching that commit\n",
    "```\n",
    "\n",
    "**Pipeline Versioning:**\n",
    "```yaml\n",
    "# dvc.yaml\n",
    "stages:\n",
    "  data_preprocessing:\n",
    "    cmd: python preprocess.py\n",
    "    deps:\n",
    "      - preprocess.py\n",
    "      - data/raw.csv\n",
    "    outs:\n",
    "      - data/processed.csv\n",
    "  \n",
    "  train:\n",
    "    cmd: python train.py\n",
    "    deps:\n",
    "      - train.py\n",
    "      - data/processed.csv\n",
    "    params:\n",
    "      - train.learning_rate\n",
    "      - train.epochs\n",
    "    outs:\n",
    "      - models/model.pkl\n",
    "    metrics:\n",
    "      - metrics.json:\n",
    "          cache: false\n",
    "```\n",
    "\n",
    "## 60.3 Model Serving\n",
    "\n",
    "Model serving exposes trained models as HTTP/gRPC endpoints for real-time inference or batch processes for offline predictions.\n",
    "\n",
    "### Serving Patterns\n",
    "\n",
    "**Real-time Inference (Online):**\n",
    "Low-latency (milliseconds) predictions via REST/gRPC APIs. Suitable for fraud detection, recommendation systems, and search ranking.\n",
    "\n",
    "**Batch Inference (Offline):**\n",
    "Process large datasets periodically, writing predictions to databases or files. Suitable for churn prediction, demand forecasting, and report generation.\n",
    "\n",
    "**Streaming Inference:**\n",
    "Process events in real-time using Kafka/Kinesis streams with stateful windowing.\n",
    "\n",
    "### Model Serving Architectures\n",
    "\n",
    "**Model-as-Container:**\n",
    "Package model and inference code in containers:\n",
    "\n",
    "```python\n",
    "# predict.py\n",
    "import pickle\n",
    "from flask import Flask, request, jsonify\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "\n",
    "# Load model at startup\n",
    "with open('/models/model.pkl', 'rb') as f:\n",
    "    model = pickle.load(f)\n",
    "\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.json\n",
    "    features = np.array(data['features']).reshape(1, -1)\n",
    "    prediction = model.predict(features)\n",
    "    probability = model.predict_proba(features).tolist()\n",
    "    \n",
    "    return jsonify({\n",
    "        'prediction': int(prediction[0]),\n",
    "        'probability': probability\n",
    "    })\n",
    "\n",
    "@app.route('/health')\n",
    "def health():\n",
    "    return jsonify({'status': 'healthy'})\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8080)\n",
    "```\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "COPY predict.py .\n",
    "COPY models/ /models/\n",
    "\n",
    "ENV MODEL_PATH=/models/model.pkl\n",
    "EXPOSE 8080\n",
    "\n",
    "CMD [\"python\", \"predict.py\"]\n",
    "```\n",
    "\n",
    "**Model Servers (TF Serving, TorchServe, MLflow):**\n",
    "Dedicated high-performance servers optimized for model serving:\n",
    "\n",
    "```yaml\n",
    "# TensorFlow Serving deployment\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: tf-serving\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: tf-serving\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: tf-serving\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: tensorflow\n",
    "        image: tensorflow/serving:latest\n",
    "        ports:\n",
    "        - containerPort: 8501  # REST API\n",
    "        - containerPort: 8500  # gRPC\n",
    "        volumeMounts:\n",
    "        - name: models\n",
    "          mountPath: /models\n",
    "        env:\n",
    "        - name: MODEL_NAME\n",
    "          value: \"churn_model\"\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"2Gi\"\n",
    "            cpu: \"1000m\"\n",
    "          limits:\n",
    "            memory: \"4Gi\"\n",
    "            cpu: \"2000m\"\n",
    "      volumes:\n",
    "      - name: models\n",
    "        persistentVolumeClaim:\n",
    "          claimName: model-storage\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: tf-serving\n",
    "spec:\n",
    "  selector:\n",
    "    app: tf-serving\n",
    "  ports:\n",
    "  - name: rest\n",
    "    port: 8501\n",
    "    targetPort: 8501\n",
    "  - name: grpc\n",
    "    port: 8500\n",
    "    targetPort: 8500\n",
    "```\n",
    "\n",
    "### Scaling Strategies\n",
    "\n",
    "**Horizontal Pod Autoscaling (HPA):**\n",
    "Scale based on inference request latency or queue depth:\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: model-server-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: model-server\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 20\n",
    "  metrics:\n",
    "  - type: Pods\n",
    "    pods:\n",
    "      metric:\n",
    "        name: inference_latency_p99\n",
    "      target:\n",
    "        type: AverageValue\n",
    "        averageValue: \"100m\"  # 100ms\n",
    "  behavior:\n",
    "    scaleUp:\n",
    "      stabilizationWindowSeconds: 60\n",
    "      policies:\n",
    "      - type: Percent\n",
    "        value: 100\n",
    "        periodSeconds: 15\n",
    "```\n",
    "\n",
    "**GPU Autoscaling:**\n",
    "Use Kubernetes Event-driven Autoscaling (KEDA) for queue-based scaling:\n",
    "\n",
    "```yaml\n",
    "apiVersion: keda.sh/v1alpha1\n",
    "kind: ScaledObject\n",
    "metadata:\n",
    "  name: gpu-inference-scaler\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    name: gpu-inference\n",
    "  minReplicaCount: 0\n",
    "  maxReplicaCount: 10\n",
    "  triggers:\n",
    "  - type: kafka\n",
    "    metadata:\n",
    "      bootstrapServers: kafka-cluster:9092\n",
    "      consumerGroup: inference-group\n",
    "      topic: inference-requests\n",
    "      lagThreshold: \"100\"\n",
    "  advanced:\n",
    "    horizontalPodAutoscalerConfig:\n",
    "      behavior:\n",
    "        scaleDown:\n",
    "          stabilizationWindowSeconds: 300\n",
    "```\n",
    "\n",
    "## 60.4 Kubernetes for ML\n",
    "\n",
    "Kubernetes provides the orchestration layer for ML workloads but requires specific configurations for GPU support, high-throughput networking, and storage optimization.\n",
    "\n",
    "### GPU Support\n",
    "\n",
    "**NVIDIA GPU Operator:**\n",
    "Automates GPU driver installation, device plugin, and monitoring:\n",
    "\n",
    "```bash\n",
    "# Install GPU Operator\n",
    "helm install gpu-operator nvidia/gpu-operator \\\n",
    "  --namespace gpu-operator \\\n",
    "  --create-namespace \\\n",
    "  --set driver.enabled=true \\\n",
    "  --set toolkit.enabled=true\n",
    "```\n",
    "\n",
    "**Time-Slicing (GPU Sharing):**\n",
    "Share single GPU across multiple pods for inference workloads:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: time-slicing-config\n",
    "  namespace: gpu-operator\n",
    "data:\n",
    "  any: |-\n",
    "    version: v1\n",
    "    sharing:\n",
    "      timeSlicing:\n",
    "        renameByDefault: false\n",
    "        resources:\n",
    "        - name: nvidia.com/gpu\n",
    "          replicas: 4  # Split 1 GPU into 4 vGPUs\n",
    "---\n",
    "# Pod requesting shared GPU\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: inference-1\n",
    "spec:\n",
    "  containers:\n",
    "  - name: model\n",
    "    image: inference:latest\n",
    "    resources:\n",
    "      limits:\n",
    "        nvidia.com/gpu: 1  # Gets 1/4 of physical GPU\n",
    "```\n",
    "\n",
    "### Storage for ML\n",
    "\n",
    "**High-Performance PVCs:**\n",
    "Training requires high-throughput storage for large datasets:\n",
    "\n",
    "```yaml\n",
    "apiVersion: v1\n",
    "kind: PersistentVolumeClaim\n",
    "metadata:\n",
    "  name: training-data\n",
    "spec:\n",
    "  accessModes:\n",
    "    - ReadOnlyMany  # Shared across training nodes\n",
    "  storageClassName: fast-ssd\n",
    "  resources:\n",
    "    requests:\n",
    "      storage: 500Gi\n",
    "  volumeMode: Filesystem\n",
    "---\n",
    "# Pod mounting data\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: data-loader\n",
    "spec:\n",
    "  containers:\n",
    "  - name: loader\n",
    "    image: downloader:latest\n",
    "    volumeMounts:\n",
    "    - name: data\n",
    "      mountPath: /data\n",
    "  volumes:\n",
    "  - name: data\n",
    "    persistentVolumeClaim:\n",
    "      claimName: training-data\n",
    "```\n",
    "\n",
    "**Cache Warming:**\n",
    "Use Fluid or Alluxio to cache remote data locally:\n",
    "\n",
    "```yaml\n",
    "apiVersion: data.fluid.io/v1alpha1\n",
    "kind: Dataset\n",
    "metadata:\n",
    "  name: imagenet\n",
    "spec:\n",
    "  mounts:\n",
    "  - mountPoint: s3://mybucket/imagenet\n",
    "    name: imagenet\n",
    "---\n",
    "apiVersion: data.fluid.io/v1alpha1\n",
    "kind: AlluxioRuntime\n",
    "metadata:\n",
    "  name: imagenet\n",
    "spec:\n",
    "  replicas: 2\n",
    "  tieredstore:\n",
    "    levels:\n",
    "    - mediumtype: SSD\n",
    "      path: /var/lib/alluxio\n",
    "      quota: 500Gi\n",
    "      high: \"0.95\"\n",
    "      low: \"0.7\"\n",
    "```\n",
    "\n",
    "## 60.5 Kubeflow\n",
    "\n",
    "Kubeflow is a Kubernetes-native platform for ML workflows, providing integrated components for pipelines, training, serving, and metadata tracking.\n",
    "\n",
    "### Architecture Components\n",
    "\n",
    "**Pipelines:** Visual DAG editor and execution engine for ML workflows.\n",
    "**Notebooks:** Jupyter notebooks integrated with Kubernetes RBAC.\n",
    "**Katib:** Hyperparameter tuning and neural architecture search.\n",
    "**Training Operator:** Distributed training (TFJob, PyTorchJob, MPIJob).\n",
    "**KServe:** Model serving platform with canary rollouts and auto-scaling.\n",
    "\n",
    "### Installation\n",
    "\n",
    "```bash\n",
    "# Install Kubeflow using manifests\n",
    "git clone https://github.com/kubeflow/manifests.git\n",
    "cd manifests\n",
    "\n",
    "while ! kustomize build example | kubectl apply -f -; do\n",
    "  echo \"Retrying to apply resources...\"\n",
    "  sleep 10\n",
    "done\n",
    "\n",
    "# Or use distribution (EKS, GKE, AKS specific)\n",
    "wget https://github.com/kubeflow/kubeflow/releases/download/v1.8.0/kfctl_1.8.0_linux.tar.gz\n",
    "tar -xzf kfctl_1.8.0_linux.tar.gz\n",
    "./kfctl apply -f kfctl_aws_cognito.v1.8.0.yaml\n",
    "```\n",
    "\n",
    "### Kubeflow Pipelines (KFP)\n",
    "\n",
    "Define reusable, composable ML workflows:\n",
    "\n",
    "```python\n",
    "from kfp import dsl\n",
    "from kfp.components import load_component_from_url\n",
    "\n",
    "# Load pre-built components\n",
    "data_prep_op = load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/data_prep/component.yaml'\n",
    ")\n",
    "train_op = load_component_from_url(\n",
    "    'https://raw.githubusercontent.com/kubeflow/pipelines/master/components/xgboost/train/component.yaml'\n",
    ")\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name='End-to-End Churn Pipeline',\n",
    "    description='Production pipeline with data validation'\n",
    ")\n",
    "def churn_pipeline(\n",
    "    data_url: str = 's3://bucket/data.csv',\n",
    "    model_path: str = 's3://bucket/models/',\n",
    "    threshold: float = 0.85\n",
    "):\n",
    "    # Data preparation\n",
    "    prep_task = data_prep_op(\n",
    "        input_path=data_url,\n",
    "        output_path='s3://bucket/processed/'\n",
    "    )\n",
    "    \n",
    "    # Training with Katib for hyperparameter tuning\n",
    "    train_task = train_op(\n",
    "        train_data=prep_task.outputs['train_data'],\n",
    "        validation_data=prep_task.outputs['val_data'],\n",
    "        num_boost_round=100,\n",
    "        max_depth=6\n",
    "    ).after(prep_task)\n",
    "    \n",
    "    # Model evaluation\n",
    "    eval_task = dsl.ContainerOp(\n",
    "        name='evaluate-model',\n",
    "        image='gcr.io/project/evaluator:latest',\n",
    "        arguments=[\n",
    "            '--model-path', train_task.outputs['model_path'],\n",
    "            '--test-data', prep_task.outputs['test_data'],\n",
    "            '--threshold', threshold\n",
    "        ]\n",
    "    ).after(train_task)\n",
    "    \n",
    "    # Conditional deployment\n",
    "    with dsl.Condition(eval_task.outputs['accuracy'] > threshold):\n",
    "        deploy_task = dsl.ContainerOp(\n",
    "            name='deploy-model',\n",
    "            image='gcr.io/project/kserve-deployer:latest',\n",
    "            arguments=[\n",
    "                '--model-path', train_task.outputs['model_path'],\n",
    "                '--model-name', 'churn-model',\n",
    "                '--namespace', 'production'\n",
    "            ]\n",
    "        )\n",
    "\n",
    "# Compile\n",
    "import kfp.compiler as compiler\n",
    "compiler.Compiler().compile(churn_pipeline, 'churn_pipeline.tar.gz')\n",
    "```\n",
    "\n",
    "### KServe for Model Serving\n",
    "\n",
    "Deploy models with advanced serving capabilities:\n",
    "\n",
    "```yaml\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: churn-model\n",
    "  namespace: production\n",
    "  annotations:\n",
    "    prometheus.io/scrape: \"true\"\n",
    "    prometheus.io/port: \"8080\"\n",
    "spec:\n",
    "  predictor:\n",
    "    model:\n",
    "      modelFormat:\n",
    "        name: xgboost\n",
    "      storageUri: s3://bucket/models/churn/v2\n",
    "      resources:\n",
    "        requests:\n",
    "          memory: 2Gi\n",
    "          cpu: \"1\"\n",
    "        limits:\n",
    "          memory: 4Gi\n",
    "          cpu: \"2\"\n",
    "    minReplicas: 2\n",
    "    maxReplicas: 10\n",
    "    containerConcurrency: 100  # Max concurrent requests per pod\n",
    "  \n",
    "  transformer:  # Pre/post processing\n",
    "    containers:\n",
    "    - name: transformer\n",
    "      image: gcr.io/project/churn-transformer:latest\n",
    "      resources:\n",
    "        requests:\n",
    "          memory: 1Gi\n",
    "  \n",
    "  explainer:  # Model explainability (SHAP, LIME)\n",
    "    alibi:\n",
    "      type: ShapTree\n",
    "      storageUri: s3://bucket/explainers/churn/\n",
    "```\n",
    "\n",
    "**Canary Deployment with KServe:**\n",
    "```yaml\n",
    "apiVersion: serving.kserve.io/v1beta1\n",
    "kind: InferenceService\n",
    "metadata:\n",
    "  name: churn-model\n",
    "spec:\n",
    "  predictor:\n",
    "    canaryTrafficPercent: 20  # 20% to new version\n",
    "    model:\n",
    "      storageUri: s3://bucket/models/churn/v3\n",
    "```\n",
    "\n",
    "## 60.6 MLflow\n",
    "\n",
    "MLflow provides an open-source platform for the ML lifecycle, focusing on experimentation, reproducibility, and deployment. Unlike Kubeflow's Kubernetes-native approach, MLflow is Python-centric and easier to adopt incrementally.\n",
    "\n",
    "### Components\n",
    "\n",
    "**Tracking:** Log parameters, metrics, and artifacts.\n",
    "**Projects:** Package code in reusable, reproducible formats.\n",
    "**Models:** Standardize model packaging for diverse deployment targets.\n",
    "**Registry:** Centralized model store with versioning and stage transitions.\n",
    "\n",
    "### Deployment Integration\n",
    "\n",
    "**Model Registry:**\n",
    "```python\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "\n",
    "# Transition model to production\n",
    "client.transition_model_version_stage(\n",
    "    name=\"churn-model\",\n",
    "    version=3,\n",
    "    stage=\"Production\"\n",
    ")\n",
    "\n",
    "# Load production model\n",
    "model = mlflow.pyfunc.load_model(\"models:/churn-model/Production\")\n",
    "```\n",
    "\n",
    "**CI/CD Pipeline Integration:**\n",
    "```yaml\n",
    "# GitHub Actions workflow\n",
    "name: ML Model CI/CD\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - 'models/**'\n",
    "      - 'src/**'\n",
    "\n",
    "jobs:\n",
    "  train:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v3\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v4\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    \n",
    "    - name: Train model\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "        python train.py --experiment-name churn\n",
    "    \n",
    "    - name: Evaluate and register\n",
    "      run: |\n",
    "        python evaluate.py --run-id ${{ env.RUN_ID }}\n",
    "        if [ $? -eq 0 ]; then\n",
    "          python register_model.py --run-id ${{ env.RUN_ID }} --name churn-model\n",
    "        fi\n",
    "    \n",
    "    - name: Deploy to staging\n",
    "      run: |\n",
    "        mlflow models build-docker -m models:/churn-model/latest -n churn-model:staging\n",
    "        kubectl set image deployment/churn-model model=churn-model:staging -n staging\n",
    "    \n",
    "    - name: Integration tests\n",
    "      run: |\n",
    "        pytest tests/integration/ --endpoint http://staging-api/model\n",
    "    \n",
    "    - name: Promote to production\n",
    "      if: github.ref == 'refs/heads/main'\n",
    "      run: |\n",
    "        mlflow models build-docker -m models:/churn-model/latest -n churn-model:prod\n",
    "        kubectl set image deployment/churn-model model=churn-model:prod -n production\n",
    "```\n",
    "\n",
    "## 60.7 Model Versioning\n",
    "\n",
    "Model versioning extends beyond semantic versioning to include data lineage, feature schemas, and training configurations.\n",
    "\n",
    "### Model Registry Patterns\n",
    "\n",
    "**Semantic Versioning for Models:**\n",
    "- **Major:** Breaking changes (input schema changes, different algorithm)\n",
    "- **Minor:** Backward compatible improvements (hyperparameter tuning, more data)\n",
    "- **Patch:** Bug fixes (correcting label errors, code fixes)\n",
    "\n",
    "**Git-Based Versioning:**\n",
    "Store model metadata in Git, weights in object storage:\n",
    "\n",
    "```python\n",
    "# Tag model with Git commit\n",
    "import git\n",
    "import mlflow\n",
    "\n",
    "repo = git.Repo(search_parent_directories=True)\n",
    "commit_hash = repo.head.object.hexsha\n",
    "\n",
    "mlflow.set_tag(\"git_commit\", commit_hash)\n",
    "mlflow.set_tag(\"git_branch\", repo.active_branch.name)\n",
    "```\n",
    "\n",
    "**Data Lineage:**\n",
    "Track which dataset version trained which model:\n",
    "\n",
    "```python\n",
    "# Using DVC and MLflow together\n",
    "import dvc.api\n",
    "import mlflow\n",
    "\n",
    "data_version = dvc.api.get_url(\"data/train.csv\")\n",
    "\n",
    "with mlflow.start_run():\n",
    "    mlflow.log_param(\"data_version\", data_version)\n",
    "    mlflow.log_param(\"dvc_commit\", dvc.api.get_rev())\n",
    "    \n",
    "    # Train and log model\n",
    "    model.fit(X, y)\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "```\n",
    "\n",
    "### Model Artifacts\n",
    "\n",
    "Package models with all dependencies:\n",
    "\n",
    "```yaml\n",
    "# conda.yaml for MLflow model\n",
    "name: churn-env\n",
    "channels:\n",
    "  - conda-forge\n",
    "dependencies:\n",
    "  - python=3.9\n",
    "  - pip\n",
    "  - pip:\n",
    "    - mlflow==2.8.0\n",
    "    - scikit-learn==1.3.0\n",
    "    - xgboost==2.0.0\n",
    "    - cloudpickle==2.2.1\n",
    "```\n",
    "\n",
    "## 60.8 A/B Testing Models\n",
    "\n",
    "A/B testing validates new models against production traffic before full rollout, measuring business metrics (conversion, revenue) rather than just accuracy.\n",
    "\n",
    "### Implementation Strategies\n",
    "\n",
    "**Shadow Mode:**\n",
    "Run new model alongside production model, comparing predictions without affecting users:\n",
    "\n",
    "```python\n",
    "# Application logic\n",
    "def predict(input_data):\n",
    "    # Production prediction\n",
    "    prod_result = prod_model.predict(input_data)\n",
    "    \n",
    "    # Shadow prediction (async)\n",
    "    async def shadow_predict():\n",
    "        shadow_result = new_model.predict(input_data)\n",
    "        log_comparison(input_data, prod_result, shadow_result)\n",
    "    \n",
    "    asyncio.create_task(shadow_predict())\n",
    "    \n",
    "    return prod_result\n",
    "```\n",
    "\n",
    "**Traffic Splitting:**\n",
    "Route percentage of users to new model:\n",
    "\n",
    "```yaml\n",
    "# Istio VirtualService for model A/B testing\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: model-ab-test\n",
    "spec:\n",
    "  hosts:\n",
    "  - churn-api\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        x-model-version:\n",
    "          exact: \"v2\"\n",
    "    route:\n",
    "    - destination:\n",
    "        host: churn-api-v2\n",
    "      weight: 100\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: churn-api-v1\n",
    "      weight: 90\n",
    "    - destination:\n",
    "        host: churn-api-v2\n",
    "        subset: canary\n",
    "      weight: 10\n",
    "```\n",
    "\n",
    "**Champion/Challenger:**\n",
    "Continuously evaluate new models (challengers) against production model (champion), promoting only when statistically significant improvement demonstrated:\n",
    "\n",
    "```python\n",
    "from scipy import stats\n",
    "\n",
    "def evaluate_challenger(champion_preds, challenger_preds, actuals):\n",
    "    champion_acc = (champion_preds == actuals).mean()\n",
    "    challenger_acc = (challenger_preds == actuals).mean()\n",
    "    \n",
    "    # Paired t-test\n",
    "    t_stat, p_value = stats.ttest_rel(\n",
    "        (challenger_preds == actuals).astype(int),\n",
    "        (champion_preds == actuals).astype(int)\n",
    "    )\n",
    "    \n",
    "    if p_value < 0.05 and challenger_acc > champion_acc:\n",
    "        return \"promote\"\n",
    "    return \"retain\"\n",
    "```\n",
    "\n",
    "### Monitoring Model Performance\n",
    "\n",
    "**Drift Detection:**\n",
    "Monitor for data drift (input distribution changes) and concept drift (relationship changes):\n",
    "\n",
    "```python\n",
    "# Evidently AI for drift detection\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "\n",
    "report = Report(metrics=[DataDriftPreset()])\n",
    "report.run(reference_data=reference_df, current_data=production_df)\n",
    "\n",
    "if report.as_dict()['metrics'][0]['result']['dataset_drift']:\n",
    "    trigger_retraining()\n",
    "```\n",
    "\n",
    "**Business Metrics:**\n",
    "Track downstream business impact, not just model accuracy:\n",
    "\n",
    "```python\n",
    "# Log business metrics\n",
    "mlflow.log_metric(\"revenue_per_user\", revenue)\n",
    "mlflow.log_metric(\"conversion_rate\", conversions / total_users)\n",
    "mlflow.log_metric(\"model_accuracy\", accuracy)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Preview\n",
    "\n",
    "This chapter explored MLOps\u2014the extension of CI/CD principles to machine learning systems\u2014addressing the unique challenges posed by data dependencies, stochastic training processes, and model performance degradation. We established that ML pipelines require three additional stages beyond traditional CI/CD: Continuous Training (automated retraining on new data), Continuous Evaluation (validating model quality against business metrics), and Continuous Monitoring (detecting data drift and concept drift).\n",
    "\n",
    "Kubeflow provides a comprehensive Kubernetes-native platform for ML workflows, integrating pipelines, distributed training, and model serving through KServe, while MLflow offers a lighter-weight, Python-centric approach focused on experimentation tracking and model registry management. Both platforms emphasize model versioning that captures not just code state, but data lineage, hyperparameters, and feature engineering logic.\n",
    "\n",
    "Model serving strategies range from real-time HTTP APIs (requiring low-latency autoscaling) to batch inference jobs (processing large datasets periodically), with Kubernetes providing the orchestration layer for GPU scheduling, high-performance storage, and resource optimization. A/B testing for ML models extends beyond infrastructure canaries to statistical validation of model performance, using shadow mode for safe evaluation and champion/challenger patterns for continuous improvement.\n",
    "\n",
    "**Key Takeaways:**\n",
    "- Version data separately from code using DVC or similar tools; a model is inseparable from the dataset that trained it\n",
    "- Implement training-serving skew detection to ensure feature engineering consistency between batch training and online inference\n",
    "- Use model registries (MLflow, Kubeflow Metadata) to manage promotion workflows (Staging \u2192 Production \u2192 Archived) with approval gates\n",
    "- Monitor for data drift using statistical tests (Kolmogorov-Smirnov, Population Stability Index) to trigger automated retraining\n",
    "- Package models with explicit dependency manifests (conda.yaml, requirements.txt) to prevent environment mismatches\n",
    "- Implement circuit breakers for model serving; fall back to baseline models when prediction latency exceeds SLAs or confidence thresholds drop\n",
    "- Log prediction inputs and outputs for audit trails and debugging, ensuring compliance with AI governance regulations\n",
    "\n",
    "**Next Chapter Preview:**\n",
    "Chapter 61: Mobile App CI/CD addresses the unique constraints of deploying to managed ecosystems (iOS App Store, Google Play Store) with stringent review processes, code signing requirements, and binary distribution models. We will explore fastlane for automating screenshot generation and app store submissions, Flutter and React Native build pipelines for cross-platform development, and strategies for over-the-air (OTA) updates using services like CodePush. The chapter examines mobile-specific testing requirements including device farm testing, UI automation with Appium, and managing provisioning profiles and certificates in CI environments. We will also investigate progressive web app (PWA) deployment strategies that blur the line between native and web distribution, completing our coverage of client-side deployment automation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='59. serverless_cicd.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='61. mobile_app_cicd.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}