{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Chapter 18: Integration and Scalability (Strangler Fig, Sidecar, API Gateway, Leader Election)\n",
    "\n",
    "## Opening Context\n",
    "\n",
    "Building a distributed system is not a one‑time effort. Systems evolve: legacy monoliths are modernised, new services are added, and existing ones must scale and integrate seamlessly. Moreover, as the system grows, common cross‑cutting concerns—such as logging, authentication, and routing—need to be handled consistently. This chapter presents four patterns that address these challenges:\n",
    "\n",
    "1. **Strangler Fig Pattern** – A strategy for incrementally migrating a legacy system to a new one with minimal risk.\n",
    "2. **Sidecar Pattern** – An approach to augment a service with additional capabilities without modifying its code.\n",
    "3. **API Gateway Pattern** – A single entry point that routes requests, handles cross‑cutting concerns, and aggregates responses.\n",
    "4. **Leader Election Pattern** – A mechanism to coordinate distributed processes by electing a single leader to perform certain tasks.\n",
    "\n",
    "These patterns are essential for building evolvable, maintainable, and scalable distributed systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 18.1 Strangler Fig Pattern\n",
    "\n",
    "### Intent\n",
    "*Gradually replace a legacy system by building a new system around it, piece by piece, until the old system is completely “strangled” and can be decommissioned.*\n",
    "\n",
    "### The Problem\n",
    "\n",
    "Replacing a large, critical legacy system in one big bang is risky. It often leads to prolonged downtime, data migration nightmares, and the possibility of complete failure. Teams need a way to modernize incrementally, allowing the new system to coexist with the old, and to switch users over gradually.\n",
    "\n",
    "### The Solution: Strangler Fig\n",
    "\n",
    "Named after the strangler fig vine that grows around a tree and eventually replaces it, this pattern advocates building a new system around the edges of the legacy system. Initially, the new system handles a small subset of functionality. Over time, more features are moved, and traffic is routed to the new system. Eventually, the old system is no longer needed and can be decommissioned.\n",
    "\n",
    "**Key Steps**:\n",
    "1. Identify a small piece of functionality that can be extracted (e.g., a single API endpoint or a UI component).\n",
    "2. Build that functionality in the new system.\n",
    "3. Route requests for that functionality to the new system (using a routing layer, API gateway, or load balancer).\n",
    "4. Repeat for other pieces until the legacy system is empty.\n",
    "\n",
    "#### Example: Migrating a Monolithic E‑commerce Platform\n",
    "\n",
    "Assume we have a monolithic Java application that handles orders, products, and customers. We want to migrate to microservices gradually.\n",
    "\n",
    "##### Step 1: Introduce a Routing Layer\n",
    "\n",
    "Place a reverse proxy (e.g., NGINX, HAProxy) or an API gateway in front of the monolith. Initially, all requests go to the monolith.\n",
    "\n",
    "```nginx\n",
    "# nginx.conf (initial)\n",
    "server {\n",
    "    listen 80;\n",
    "    location / {\n",
    "        proxy_pass http://legacy-monolith;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "##### Step 2: Extract Product Service\n",
    "\n",
    "Build a new product microservice (Node.js, for example) that exposes REST endpoints for product data.\n",
    "\n",
    "```typescript\n",
    "// product-service/app.ts\n",
    "import express from 'express';\n",
    "const app = express();\n",
    "\n",
    "app.get('/products/:id', async (req, res) => {\n",
    "  // Fetch product from new database\n",
    "  const product = await db.findProduct(req.params.id);\n",
    "  res.json(product);\n",
    "});\n",
    "\n",
    "app.listen(3001);\n",
    "```\n",
    "\n",
    "##### Step 3: Route Product Requests to the New Service\n",
    "\n",
    "Update the routing layer to send requests starting with `/api/products` to the new service.\n",
    "\n",
    "```nginx\n",
    "# nginx.conf (updated)\n",
    "server {\n",
    "    listen 80;\n",
    "    \n",
    "    location /api/products/ {\n",
    "        proxy_pass http://product-service:3001;\n",
    "    }\n",
    "    \n",
    "    location / {\n",
    "        proxy_pass http://legacy-monolith;\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "##### Step 4: Extract Order Service\n",
    "\n",
    "Similarly, extract order functionality into another service. The routing layer now sends `/api/orders` to the order service.\n",
    "\n",
    "```nginx\n",
    "location /api/orders/ {\n",
    "    proxy_pass http://order-service:3002;\n",
    "}\n",
    "```\n",
    "\n",
    "##### Step 5: Migrate Data and Dependencies\n",
    "\n",
    "Over time, the new services need to own their data. This may involve data synchronization or migration strategies (e.g., dual writes, event-based sync). Once all traffic for a domain is on the new service, the legacy data can be migrated and the old tables decommissioned.\n",
    "\n",
    "##### Step 6: Decommission the Monolith\n",
    "\n",
    "When all functionality has been moved, the routing layer can be removed, and the monolith shut down.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Reduced risk** – Changes are small and incremental.\n",
    "- **Continuous delivery** – New features can be deployed independently.\n",
    "- **Parallel operation** – Old and new systems coexist, allowing fallback.\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "- **Temporary complexity** – The routing layer and data sync add operational overhead.\n",
    "- **Transaction coordination** – Operations that span old and new systems may require careful handling (e.g., sagas).\n",
    "- **Data consistency** – Eventually consistency may be needed during migration.\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Modernizing a large, critical legacy system.\n",
    "- Gradually adopting microservices from a monolith.\n",
    "- Any scenario where a big‑bang rewrite is too risky.\n",
    "\n",
    "---\n",
    "\n",
    "## 18.2 Sidecar Pattern\n",
    "\n",
    "### Intent\n",
    "*Augment a service with additional capabilities (e.g., logging, monitoring, networking) without modifying its code by deploying a helper process (the sidecar) alongside it.*\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In a microservices architecture, services often need common functionality: logging, configuration updates, service discovery, circuit breaking, or TLS termination. Implementing these in every service leads to duplication and ties services to specific infrastructure libraries. Moreover, services written in different languages would need to reimplement these concerns.\n",
    "\n",
    "### The Solution: Sidecar\n",
    "\n",
    "Deploy a secondary container (the sidecar) alongside the main service container in the same pod (in Kubernetes) or on the same host. The sidecar shares the same lifecycle and can communicate with the main service via localhost. It can handle infrastructure concerns on behalf of the service.\n",
    "\n",
    "Common use cases:\n",
    "- **Service mesh sidecar** (Envoy, Linkerd) – Handles service discovery, load balancing, retries, and observability.\n",
    "- **Logging sidecar** – Collects logs from the main container and ships them to a central system.\n",
    "- **Configuration sidecar** – Watches a configuration repository and updates the main service’s config files.\n",
    "- **TLS termination sidecar** – Terminates HTTPS and forwards plain HTTP to the main service.\n",
    "\n",
    "#### Example: Logging Sidecar\n",
    "\n",
    "Imagine a simple Node.js service that writes logs to stdout. We want to ship these logs to Elasticsearch. Instead of modifying the service code, we add a sidecar that tails the logs and forwards them.\n",
    "\n",
    "**Main service (simplified)**:\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile for main service\n",
    "FROM node:14\n",
    "WORKDIR /app\n",
    "COPY package*.json ./\n",
    "RUN npm install\n",
    "COPY . .\n",
    "CMD [\"node\", \"app.js\"]\n",
    "```\n",
    "\n",
    "**Logging sidecar** (using Filebeat):\n",
    "\n",
    "```yaml\n",
    "# docker-compose excerpt for local development\n",
    "services:\n",
    "  app:\n",
    "    build: .\n",
    "    volumes:\n",
    "      - ./logs:/app/logs   # share log directory\n",
    "  filebeat:\n",
    "    image: docker.elastic.co/beats/filebeat:7.10.0\n",
    "    volumes:\n",
    "      - ./logs:/logs\n",
    "      - ./filebeat.yml:/usr/share/filebeat/filebeat.yml\n",
    "    depends_on:\n",
    "      - app\n",
    "```\n",
    "\n",
    "**filebeat.yml**:\n",
    "\n",
    "```yaml\n",
    "filebeat.inputs:\n",
    "- type: log\n",
    "  paths:\n",
    "    - /logs/*.log\n",
    "output.elasticsearch:\n",
    "  hosts: [\"elasticsearch:9200\"]\n",
    "```\n",
    "\n",
    "In Kubernetes, both containers would be in the same pod, sharing a volume for logs.\n",
    "\n",
    "#### Example: Service Mesh Sidecar (Conceptual)\n",
    "\n",
    "With a service mesh like Istio, an Envoy proxy is injected as a sidecar. The main service communicates with other services via localhost, and the sidecar intercepts all traffic to handle routing, retries, and telemetry.\n",
    "\n",
    "```yaml\n",
    "# Kubernetes pod with sidecar (simplified)\n",
    "apiVersion: v1\n",
    "kind: Pod\n",
    "metadata:\n",
    "  name: my-service\n",
    "spec:\n",
    "  containers:\n",
    "  - name: main-app\n",
    "    image: my-app:latest\n",
    "  - name: envoy-sidecar\n",
    "    image: envoyproxy/envoy:latest\n",
    "    ports:\n",
    "    - containerPort: 15000\n",
    "```\n",
    "\n",
    "The main service can call another service via `http://localhost:15000` (if the sidecar is configured as a forward proxy), or the sidecar can intercept outgoing traffic transparently.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Separation of concerns** – Main service focuses on business logic; sidecars handle infrastructure.\n",
    "- **Language independence** – Sidecars can be written in any language, regardless of the main service.\n",
    "- **Reusability** – The same sidecar can be attached to many services.\n",
    "- **Dynamic updates** – Sidecars can be updated independently (e.g., new logging configuration).\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "- **Resource overhead** – Each service now runs extra containers.\n",
    "- **Communication complexity** – Services must communicate with sidecars (e.g., via localhost), and debugging can be trickier.\n",
    "- **Deployment complexity** – Orchestrator must support sidecar patterns (e.g., pods in Kubernetes).\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- When you have many services that need common infrastructure capabilities.\n",
    "- When you want to decouple service code from infrastructure concerns.\n",
    "- In a service mesh architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## 18.3 API Gateway Pattern\n",
    "\n",
    "### Intent\n",
    "*Provide a single, unified entry point for a set of microservices, handling cross‑cutting concerns such as authentication, rate limiting, routing, and response aggregation.*\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In a microservices architecture, clients (web, mobile, third‑party) would need to know the addresses of many services and make multiple requests to assemble a UI. This leads to:\n",
    "- **Chatty communication** – Multiple round trips increase latency.\n",
    "- **Cross‑cutting concerns duplication** – Each service would need to implement authentication, CORS, logging, etc.\n",
    "- **Client‑side complexity** – Clients must handle service discovery and error handling.\n",
    "- **Protocol/API versioning** – Changing service interfaces forces client updates.\n",
    "\n",
    "### The Solution: API Gateway\n",
    "\n",
    "The API Gateway sits between clients and services. It acts as a reverse proxy, routing requests to the appropriate service. It can also perform:\n",
    "\n",
    "- **Authentication/Authorization** – Verify tokens, enforce policies.\n",
    "- **Rate limiting** – Protect services from overload.\n",
    "- **Request aggregation** – Combine multiple service responses into one.\n",
    "- **Response transformation** – Convert between protocols (e.g., REST to gRPC) or data formats.\n",
    "- **Caching** – Cache responses to reduce load.\n",
    "- **Request logging and monitoring**.\n",
    "\n",
    "#### Example: Simple API Gateway with Express\n",
    "\n",
    "We'll build a minimal API gateway that routes to two services: `order-service` and `product-service`, and also provides an aggregated endpoint.\n",
    "\n",
    "```typescript\n",
    "// api-gateway.ts\n",
    "import express from 'express';\n",
    "import proxy from 'express-http-proxy';\n",
    "import axios from 'axios';\n",
    "\n",
    "const app = express();\n",
    "\n",
    "// Authentication middleware (simplified)\n",
    "app.use((req, res, next) => {\n",
    "  const token = req.headers['authorization'];\n",
    "  if (!token || !isValidToken(token)) {\n",
    "    return res.status(401).json({ error: 'Unauthorized' });\n",
    "  }\n",
    "  next();\n",
    "});\n",
    "\n",
    "// Route to order service\n",
    "app.use('/orders', proxy('http://order-service:3001', {\n",
    "  proxyReqPathResolver: (req) => `/api${req.url}` // map /orders to /api/orders\n",
    "}));\n",
    "\n",
    "// Route to product service\n",
    "app.use('/products', proxy('http://product-service:3002', {\n",
    "  proxyReqPathResolver: (req) => `/api${req.url}`\n",
    "}));\n",
    "\n",
    "// Aggregated endpoint for a customer dashboard\n",
    "app.get('/dashboard/:customerId', async (req, res) => {\n",
    "  const customerId = req.params.customerId;\n",
    "  try {\n",
    "    // Fetch data from multiple services in parallel\n",
    "    const [orders, products] = await Promise.all([\n",
    "      axios.get(`http://order-service:3001/api/orders?customerId=${customerId}`),\n",
    "      axios.get(`http://product-service:3002/api/products/recommended?customerId=${customerId}`)\n",
    "    ]);\n",
    "    \n",
    "    res.json({\n",
    "      orders: orders.data,\n",
    "      recommendedProducts: products.data\n",
    "    });\n",
    "  } catch (error) {\n",
    "    res.status(500).json({ error: 'Failed to fetch dashboard' });\n",
    "  }\n",
    "});\n",
    "\n",
    "app.listen(3000, () => console.log('API Gateway running on port 3000'));\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- The gateway uses `express-http-proxy` to forward requests to the appropriate microservice.\n",
    "- It adds authentication middleware centrally.\n",
    "- The `/dashboard` endpoint aggregates responses from two services, reducing round trips for the client.\n",
    "\n",
    "#### API Gateway vs. BFF\n",
    "\n",
    "Recall from Chapter 15 that **Backend for Frontend (BFF)** is a pattern where you have a dedicated gateway per client type. An API Gateway is often a single entry point for all clients, but it can be combined with BFFs. For example, you might have:\n",
    "- An edge API Gateway that handles global concerns (auth, rate limiting).\n",
    "- BFFs for web, mobile, and third‑party that sit behind the edge gateway and tailor responses.\n",
    "\n",
    "#### Advanced Features\n",
    "\n",
    "- **Service discovery integration** – The gateway can query a service registry (e.g., Consul, Eureka) to locate service instances.\n",
    "- **Load balancing** – Distribute requests across multiple instances.\n",
    "- **Canary releases** – Route a percentage of traffic to a new version.\n",
    "- **Request/response transformation** – e.g., convert XML to JSON, or add/remove headers.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Centralized control** – Cross‑cutting concerns managed in one place.\n",
    "- **Client simplification** – Clients talk to one endpoint.\n",
    "- **Protocol translation** – Gateway can translate between client‑friendly protocols (HTTP/JSON) and internal protocols (gRPC, Thrift).\n",
    "- **Resilience** – Can implement retries, circuit breakers, and timeouts centrally.\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "- **Single point of failure** – Must be highly available and scalable.\n",
    "- **Potential bottleneck** – All traffic passes through it; performance is critical.\n",
    "- **Increased complexity** – Gateway adds another component to deploy and maintain.\n",
    "- **Development overhead** – May need to update gateway when services change.\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- Any microservices architecture with multiple clients.\n",
    "- When you need to enforce common policies (auth, rate limiting).\n",
    "- To simplify client communication.\n",
    "\n",
    "---\n",
    "\n",
    "## 18.4 Leader Election Pattern\n",
    "\n",
    "### Intent\n",
    "*Coordinate distributed processes by ensuring that only one instance (the leader) performs certain tasks at any given time, with automatic failover if the leader fails.*\n",
    "\n",
    "### The Problem\n",
    "\n",
    "In a distributed system, multiple instances of a service may be running for scalability or fault tolerance. However, some tasks should only be performed by one instance at a time, such as:\n",
    "- Running a background job (e.g., sending nightly emails).\n",
    "- Managing a shared resource (e.g., coordinating a cache refresh).\n",
    "- Acting as the primary in a primary‑replica setup.\n",
    "- Performing housekeeping tasks (e.g., cleaning up old data).\n",
    "\n",
    "Without leader election, multiple instances could duplicate work, cause conflicts, or waste resources.\n",
    "\n",
    "### The Solution: Leader Election\n",
    "\n",
    "Leader election algorithms allow a group of processes to elect one of them as the leader. The leader performs the exclusive tasks, while others stand by as replicas. If the leader fails, the remaining processes elect a new leader.\n",
    "\n",
    "Common implementations use:\n",
    "- **Distributed consensus algorithms** (Paxos, Raft) – used in systems like ZooKeeper, etcd, Consul.\n",
    "- **Lease‑based election** – processes attempt to acquire a lock with a time‑to‑live; the holder is the leader.\n",
    "\n",
    "#### Example: Leader Election using a Distributed Lock (Redis)\n",
    "\n",
    "We can implement a simple leader election using Redis’s `SET NX` command with an expiry. Each instance tries to acquire a lock with a unique identifier. The one that succeeds becomes leader and must periodically renew the lock.\n",
    "\n",
    "```typescript\n",
    "// leader-election.ts\n",
    "import Redis from 'ioredis';\n",
    "\n",
    "export class LeaderElector {\n",
    "  private redis: Redis;\n",
    "  private lockKey: string = 'service:leader';\n",
    "  private lockValue: string; // unique identifier for this instance\n",
    "  private ttl: number = 30000; // lock validity in ms\n",
    "  private renewInterval: NodeJS.Timeout | null = null;\n",
    "  public isLeader: boolean = false;\n",
    "\n",
    "  constructor(redisUrl: string, instanceId: string) {\n",
    "    this.redis = new Redis(redisUrl);\n",
    "    this.lockValue = instanceId;\n",
    "  }\n",
    "\n",
    "  async start(): Promise<void> {\n",
    "    // Attempt to become leader immediately\n",
    "    await this.tryAcquireLock();\n",
    "\n",
    "    // Periodically attempt to acquire lock if not leader, or renew if leader\n",
    "    setInterval(() => {\n",
    "      if (this.isLeader) {\n",
    "        this.renewLock();\n",
    "      } else {\n",
    "        this.tryAcquireLock();\n",
    "      }\n",
    "    }, this.ttl / 3);\n",
    "  }\n",
    "\n",
    "  private async tryAcquireLock(): Promise<void> {\n",
    "    // SET NX: set only if key doesn't exist\n",
    "    const result = await this.redis.set(this.lockKey, this.lockValue, 'PX', this.ttl, 'NX');\n",
    "    if (result === 'OK') {\n",
    "      console.log('Became leader');\n",
    "      this.isLeader = true;\n",
    "      this.onBecomeLeader();\n",
    "    }\n",
    "  }\n",
    "\n",
    "  private async renewLock(): Promise<void> {\n",
    "    // Use Lua script to atomically renew if we still hold the lock\n",
    "    const script = `\n",
    "      if redis.call(\"get\", KEYS[1]) == ARGV[1] then\n",
    "        return redis.call(\"pexpire\", KEYS[1], ARGV[2])\n",
    "      else\n",
    "        return 0\n",
    "      end\n",
    "    `;\n",
    "    const result = await this.redis.eval(script, 1, this.lockKey, this.lockValue, this.ttl);\n",
    "    if (result === 0) {\n",
    "      // Lock lost; someone else became leader\n",
    "      console.log('Lost leadership');\n",
    "      this.isLeader = false;\n",
    "      this.onLostLeadership();\n",
    "    }\n",
    "  }\n",
    "\n",
    "  private onBecomeLeader(): void {\n",
    "    // Start the exclusive task (e.g., scheduler)\n",
    "    this.startLeaderTask();\n",
    "  }\n",
    "\n",
    "  private onLostLeadership(): void {\n",
    "    // Stop the exclusive task\n",
    "    this.stopLeaderTask();\n",
    "  }\n",
    "\n",
    "  private startLeaderTask(): void {\n",
    "    // e.g., schedule a cron job\n",
    "    console.log('Starting leader task');\n",
    "    // ...\n",
    "  }\n",
    "\n",
    "  private stopLeaderTask(): void {\n",
    "    console.log('Stopping leader task');\n",
    "    // ...\n",
    "  }\n",
    "\n",
    "  async shutdown(): Promise<void> {\n",
    "    if (this.renewInterval) clearInterval(this.renewInterval);\n",
    "    // Release lock if we are leader\n",
    "    if (this.isLeader) {\n",
    "      await this.redis.del(this.lockKey);\n",
    "    }\n",
    "    await this.redis.quit();\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "- Each instance generates a unique ID (e.g., hostname + PID).\n",
    "- The first instance to set the Redis key with `NX` becomes leader.\n",
    "- The leader periodically renews the lock (using a Lua script to ensure atomicity) so that other instances cannot steal it.\n",
    "- If the leader fails, the lock expires, and another instance acquires it.\n",
    "- When an instance becomes leader, it starts the exclusive task; when it loses leadership, it stops.\n",
    "\n",
    "#### Using a Consensus Service (etcd)\n",
    "\n",
    "A more robust approach is to use etcd’s `concurrency` package, which implements leader election based on Raft.\n",
    "\n",
    "```typescript\n",
    "// Using etcd3 client for Node.js\n",
    "import { Etcd3 } from 'etcd3';\n",
    "\n",
    "const etcd = new Etcd3({ hosts: 'localhost:2379' });\n",
    "const election = etcd.election('my-service');\n",
    "\n",
    "async function campaign() {\n",
    "  const campaign = election.campaign('my-value'); // value could be instance ID\n",
    "  campaign.on('elected', () => {\n",
    "    console.log('I am the leader!');\n",
    "    // Start leader work\n",
    "  });\n",
    "  campaign.on('error', (err) => {\n",
    "    console.error('Campaign error', err);\n",
    "  });\n",
    "}\n",
    "\n",
    "campaign();\n",
    "```\n",
    "\n",
    "The etcd library handles lease management, re‑election, and failover.\n",
    "\n",
    "### Leader Election Patterns in Practice\n",
    "\n",
    "- **Kubernetes leader election** – Uses ConfigMap or Lease objects (via client‑go) for controllers.\n",
    "- **ZooKeeper** – Uses ephemeral sequential nodes for leader election.\n",
    "- **Database‑based** – Use a table with optimistic locking (e.g., `UPDATE ... WHERE leader = true AND version = X`), but this is less robust.\n",
    "\n",
    "### Benefits\n",
    "\n",
    "- **Fault tolerance** – If the leader fails, a new one is elected automatically.\n",
    "- **Simplifies coordination** – Only one instance performs critical tasks.\n",
    "- **Scales horizontally** – Replicas can handle other, stateless requests while the leader does the exclusive work.\n",
    "\n",
    "### Drawbacks\n",
    "\n",
    "- **Complexity** – Adds dependency on a coordination service (ZooKeeper, etcd, Redis) that must itself be highly available.\n",
    "- **Potential split‑brain** – If the coordination service is partitioned, two nodes might think they are leaders (use consensus systems with strong consistency to avoid this).\n",
    "- **Performance** – Leader election adds latency; not suitable for high‑frequency decisions.\n",
    "\n",
    "### When to Use\n",
    "\n",
    "- When you have tasks that must be executed by only one instance at a time.\n",
    "- In systems that need automatic failover for a primary instance.\n",
    "- For background job schedulers, cache warmers, or any singleton service.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "This chapter covered four essential patterns for integrating and scaling distributed systems:\n",
    "\n",
    "1. **Strangler Fig Pattern** – Enables gradual migration from a legacy system to a new one by incrementally routing functionality and finally decommissioning the old system. It reduces risk and allows continuous delivery.\n",
    "\n",
    "2. **Sidecar Pattern** – Augments services with cross‑cutting capabilities (logging, monitoring, networking) by deploying a helper process alongside the main service. This promotes separation of concerns and reusability.\n",
    "\n",
    "3. **API Gateway Pattern** – Provides a single entry point for clients, handling cross‑cutting concerns (authentication, routing, aggregation). It simplifies clients and centralizes control but introduces a potential bottleneck.\n",
    "\n",
    "4. **Leader Election** – Coordinates distributed processes to ensure that only one instance performs exclusive tasks, with automatic failover. It enables fault‑tolerant singleton services.\n",
    "\n",
    "**Key Insight**: As distributed systems grow, patterns for integration and scalability become critical. They help manage complexity, enable evolution, and ensure reliability.\n",
    "\n",
    "---\n",
    "\n",
    "## Next Chapter Preview\n",
    "\n",
    "**Chapter 19: Security Design Patterns (Authentication, Authorization, Secure Factory, Security Proxy)**\n",
    "\n",
    "With the rise of distributed systems and cloud‑native applications, security must be woven into the design from the start. Chapter 19 explores patterns for building secure systems: **Authentication and Authorization patterns**, **Secure Factory and Builder** for safe object creation, **Security Proxy and Interceptor** for access control, and **Input Validation and Sanitization** to prevent injection attacks. You’ll learn how to apply these patterns to protect your applications in an increasingly hostile environment.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
