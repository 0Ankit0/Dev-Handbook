{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "30529685",
   "metadata": {},
   "source": [
    "# Chapter 36: File Handling & Uploads\n",
    "\n",
    "Modern applications require robust file handling capabilities\u2014from user avatars and document uploads to media processing and bulk data imports. Implementing secure, scalable file uploads in Next.js requires careful architecture to prevent security vulnerabilities, minimize server load, and optimize storage costs. Direct-to-cloud-storage patterns allow browsers to upload files straight to S3 or R2 without consuming application server bandwidth, while server-side processing handles validation, metadata extraction, and post-processing.\n",
    "\n",
    "By the end of this chapter, you'll master implementing presigned URL patterns for secure direct uploads, processing images with Sharp for optimization, validating file types and scanning for malware, tracking upload progress with real-time UI updates, handling large file multipart uploads, and integrating with object storage services like AWS S3 and Cloudflare R2.\n",
    "\n",
    "## 36.1 Secure Upload Architecture\n",
    "\n",
    "Implement presigned URL patterns to keep cloud credentials secure while enabling direct browser-to-storage uploads.\n",
    "\n",
    "### Presigned URL Generation\n",
    "\n",
    "```typescript\n",
    "// lib/storage/s3.ts\n",
    "import { S3Client, PutObjectCommand } from '@aws-sdk/client-s3';\n",
    "import { getSignedUrl } from '@aws-sdk/s3-request-presigner';\n",
    "import { v4 as uuidv4 } from 'uuid';\n",
    "\n",
    "const s3Client = new S3Client({\n",
    "  region: process.env.AWS_REGION,\n",
    "  credentials: {\n",
    "    accessKeyId: process.env.AWS_ACCESS_KEY_ID!,\n",
    "    secretAccessKey: process.env.AWS_SECRET_ACCESS_KEY!,\n",
    "  },\n",
    "});\n",
    "\n",
    "interface PresignedUrlOptions {\n",
    "  filename: string;\n",
    "  contentType: string;\n",
    "  size: number;\n",
    "  maxSize?: number;\n",
    "  allowedTypes?: string[];\n",
    "}\n",
    "\n",
    "export async function generatePresignedUrl({\n",
    "  filename,\n",
    "  contentType,\n",
    "  size,\n",
    "  maxSize = 10 * 1024 * 1024, // 10MB default\n",
    "  allowedTypes = ['image/jpeg', 'image/png', 'image/webp', 'application/pdf'],\n",
    "}: PresignedUrlOptions) {\n",
    "  // Server-side validation (never trust client)\n",
    "  if (size > maxSize) {\n",
    "    throw new Error(`File size exceeds maximum allowed (${maxSize} bytes)`);\n",
    "  }\n",
    "  \n",
    "  if (!allowedTypes.includes(contentType)) {\n",
    "    throw new Error(`File type ${contentType} not allowed`);\n",
    "  }\n",
    "\n",
    "  // Generate unique key with sanitized filename\n",
    "  const extension = filename.split('.').pop()?.toLowerCase();\n",
    "  const sanitized = filename.replace(/[^a-z0-9.]/gi, '-').toLowerCase();\n",
    "  const key = `uploads/${uuidv4()}-${sanitized}`;\n",
    "\n",
    "  const command = new PutObjectCommand({\n",
    "    Bucket: process.env.S3_BUCKET_NAME,\n",
    "    Key: key,\n",
    "    ContentType: contentType,\n",
    "    ContentLength: size,\n",
    "    Metadata: {\n",
    "      'original-name': filename,\n",
    "      'uploaded-at': new Date().toISOString(),\n",
    "    },\n",
    "  });\n",
    "\n",
    "  const url = await getSignedUrl(s3Client, command, { expiresIn: 300 }); // 5 minutes\n",
    "  \n",
    "  return {\n",
    "    url,\n",
    "    key,\n",
    "    fields: {\n",
    "      bucket: process.env.S3_BUCKET_NAME,\n",
    "      key,\n",
    "    },\n",
    "  };\n",
    "}\n",
    "\n",
    "// Alternative for Cloudflare R2 (S3-compatible)\n",
    "import { S3Client as R2Client } from '@aws-sdk/client-s3';\n",
    "\n",
    "export const r2Client = new R2Client({\n",
    "  region: 'auto',\n",
    "  endpoint: `https://${process.env.R2_ACCOUNT_ID}.r2.cloudflarestorage.com`,\n",
    "  credentials: {\n",
    "    accessKeyId: process.env.R2_ACCESS_KEY_ID!,\n",
    "    secretAccessKey: process.env.R2_SECRET_ACCESS_KEY!,\n",
    "  },\n",
    "});\n",
    "```\n",
    "\n",
    "### Upload Route Handler\n",
    "\n",
    "```typescript\n",
    "// app/api/upload/presigned/route.ts\n",
    "import { generatePresignedUrl } from '@/lib/storage/s3';\n",
    "import { NextRequest, NextResponse } from 'next/server';\n",
    "import { z } from 'zod';\n",
    "\n",
    "const uploadSchema = z.object({\n",
    "  filename: z.string().min(1),\n",
    "  contentType: z.string(),\n",
    "  size: z.number().max(100 * 1024 * 1024), // 100MB max\n",
    "});\n",
    "\n",
    "export async function POST(req: NextRequest) {\n",
    "  try {\n",
    "    const body = await req.json();\n",
    "    const validated = uploadSchema.parse(body);\n",
    "    \n",
    "    // Additional security: Check user authentication/authorization\n",
    "    const user = await getCurrentUser(req);\n",
    "    if (!user) {\n",
    "      return NextResponse.json({ error: 'Unauthorized' }, { status: 401 });\n",
    "    }\n",
    "\n",
    "    const result = await generatePresignedUrl({\n",
    "      ...validated,\n",
    "      allowedTypes: ['image/jpeg', 'image/png', 'image/webp', 'image/gif', 'application/pdf'],\n",
    "      maxSize: 50 * 1024 * 1024, // 50MB for images/docs\n",
    "    });\n",
    "\n",
    "    // Store pending upload in database for tracking\n",
    "    await db.fileUpload.create({\n",
    "      data: {\n",
    "        key: result.key,\n",
    "        userId: user.id,\n",
    "        filename: validated.filename,\n",
    "        contentType: validated.contentType,\n",
    "        size: validated.size,\n",
    "        status: 'pending',\n",
    "        expiresAt: new Date(Date.now() + 5 * 60 * 1000), // 5 min expiry\n",
    "      },\n",
    "    });\n",
    "\n",
    "    return NextResponse.json(result);\n",
    "  } catch (error) {\n",
    "    if (error instanceof z.ZodError) {\n",
    "      return NextResponse.json({ error: 'Invalid request', details: error.errors }, { status: 400 });\n",
    "    }\n",
    "    console.error('Presigned URL generation failed:', error);\n",
    "    return NextResponse.json({ error: 'Failed to generate upload URL' }, { status: 500 });\n",
    "  }\n",
    "}\n",
    "\n",
    "// Webhook handler for S3 upload completion (optional)\n",
    "// app/api/webhooks/s3-upload/route.ts\n",
    "export async function POST(req: NextRequest) {\n",
    "  // Verify S3/SNS signature...\n",
    "  const event = await req.json();\n",
    "  \n",
    "  if (event.Event === 's3:ObjectCreated:Put') {\n",
    "    const key = event.Records[0].s3.object.key;\n",
    "    \n",
    "    // Update database record to completed\n",
    "    await db.fileUpload.update({\n",
    "      where: { key },\n",
    "      data: { \n",
    "        status: 'completed',\n",
    "        completedAt: new Date(),\n",
    "        url: `https://${process.env.CDN_DOMAIN}/${key}`,\n",
    "      },\n",
    "    });\n",
    "    \n",
    "    // Trigger post-processing (image optimization, virus scan, etc.)\n",
    "    await queuePostProcessing(key);\n",
    "  }\n",
    "  \n",
    "  return Response.json({ received: true });\n",
    "}\n",
    "```\n",
    "\n",
    "## 36.2 Client-Side Upload Implementation\n",
    "\n",
    "Build React hooks and components for handling file selection, validation, and progress tracking.\n",
    "\n",
    "### Upload Hook with Progress\n",
    "\n",
    "```typescript\n",
    "// hooks/use-file-upload.ts\n",
    "'use client';\n",
    "\n",
    "import { useState, useCallback } from 'react';\n",
    "import axios from 'axios';\n",
    "\n",
    "interface UploadProgress {\n",
    "  loaded: number;\n",
    "  total: number;\n",
    "  percentage: number;\n",
    "}\n",
    "\n",
    "interface UseFileUploadOptions {\n",
    "  onSuccess?: (data: { key: string; url: string }) => void;\n",
    "  onError?: (error: Error) => void;\n",
    "  onProgress?: (progress: UploadProgress) => void;\n",
    "}\n",
    "\n",
    "export function useFileUpload(options: UseFileUploadOptions = {}) {\n",
    "  const [isUploading, setIsUploading] = useState(false);\n",
    "  const [progress, setProgress] = useState<UploadProgress | null>(null);\n",
    "  const [abortController, setAbortController] = useState<AbortController | null>(null);\n",
    "\n",
    "  const upload = useCallback(async (file: File) => {\n",
    "    setIsUploading(true);\n",
    "    setProgress(null);\n",
    "    \n",
    "    const controller = new AbortController();\n",
    "    setAbortController(controller);\n",
    "\n",
    "    try {\n",
    "      // Step 1: Get presigned URL\n",
    "      const presignedRes = await fetch('/api/upload/presigned', {\n",
    "        method: 'POST',\n",
    "        headers: { 'Content-Type': 'application/json' },\n",
    "        body: JSON.stringify({\n",
    "          filename: file.name,\n",
    "          contentType: file.type,\n",
    "          size: file.size,\n",
    "        }),\n",
    "      });\n",
    "\n",
    "      if (!presignedRes.ok) {\n",
    "        const error = await presignedRes.json();\n",
    "        throw new Error(error.error || 'Failed to get upload URL');\n",
    "      }\n",
    "\n",
    "      const { url, key } = await presignedRes.json();\n",
    "\n",
    "      // Step 2: Upload to S3/R2 directly\n",
    "      await axios.put(url, file, {\n",
    "        headers: {\n",
    "          'Content-Type': file.type,\n",
    "        },\n",
    "        signal: controller.signal,\n",
    "        onUploadProgress: (progressEvent) => {\n",
    "          const loaded = progressEvent.loaded;\n",
    "          const total = progressEvent.total || file.size;\n",
    "          const percentage = Math.round((loaded * 100) / total);\n",
    "          \n",
    "          const progressData = { loaded, total, percentage };\n",
    "          setProgress(progressData);\n",
    "          options.onProgress?.(progressData);\n",
    "        },\n",
    "      });\n",
    "\n",
    "      // Step 3: Confirm upload (optional, depending on architecture)\n",
    "      const confirmRes = await fetch('/api/upload/confirm', {\n",
    "        method: 'POST',\n",
    "        headers: { 'Content-Type': 'application/json' },\n",
    "        body: JSON.stringify({ key }),\n",
    "      });\n",
    "\n",
    "      const result = await confirmRes.json();\n",
    "      \n",
    "      options.onSuccess?.({ key, url: result.url });\n",
    "      return { key, url: result.url };\n",
    "    } catch (error) {\n",
    "      if (axios.isCancel(error)) {\n",
    "        console.log('Upload cancelled');\n",
    "      } else {\n",
    "        options.onError?.(error as Error);\n",
    "      }\n",
    "      throw error;\n",
    "    } finally {\n",
    "      setIsUploading(false);\n",
    "      setAbortController(null);\n",
    "    }\n",
    "  }, [options]);\n",
    "\n",
    "  const cancel = useCallback(() => {\n",
    "    abortController?.abort();\n",
    "    setIsUploading(false);\n",
    "    setProgress(null);\n",
    "  }, [abortController]);\n",
    "\n",
    "  return {\n",
    "    upload,\n",
    "    cancel,\n",
    "    isUploading,\n",
    "    progress,\n",
    "  };\n",
    "}\n",
    "```\n",
    "\n",
    "### Dropzone Component\n",
    "\n",
    "```typescript\n",
    "// components/file-upload/dropzone.tsx\n",
    "'use client';\n",
    "\n",
    "import { useCallback, useState } from 'react';\n",
    "import { useDropzone } from 'react-dropzone';\n",
    "import { useFileUpload } from '@/hooks/use-file-upload';\n",
    "import { Progress } from '@/components/ui/progress';\n",
    "import { FileIcon, XIcon, UploadCloudIcon } from 'lucide-react';\n",
    "\n",
    "interface FileDropzoneProps {\n",
    "  accept?: Record<string, string[]>;\n",
    "  maxSize?: number;\n",
    "  maxFiles?: number;\n",
    "  onUploadComplete?: (files: Array<{ key: string; url: string }>) => void;\n",
    "}\n",
    "\n",
    "export function FileDropzone({\n",
    "  accept = { 'image/*': ['.png', '.jpg', '.jpeg', '.webp', '.gif'] },\n",
    "  maxSize = 10 * 1024 * 1024,\n",
    "  maxFiles = 5,\n",
    "  onUploadComplete,\n",
    "}: FileDropzoneProps) {\n",
    "  const [files, setFiles] = useState<Array<File & { id: string; status: 'pending' | 'uploading' | 'done' | 'error'; progress?: number }>>([]);\n",
    "  \n",
    "  const onDrop = useCallback((acceptedFiles: File[]) => {\n",
    "    const newFiles = acceptedFiles.map(file => ({\n",
    "      ...file,\n",
    "      id: Math.random().toString(36).substring(7),\n",
    "      status: 'pending' as const,\n",
    "    }));\n",
    "    \n",
    "    setFiles(prev => [...prev, ...newFiles].slice(0, maxFiles));\n",
    "    \n",
    "    // Auto-upload\n",
    "    newFiles.forEach(uploadFile);\n",
    "  }, []);\n",
    "\n",
    "  const { getRootProps, getInputProps, isDragActive, fileRejections } = useDropzone({\n",
    "    onDrop,\n",
    "    accept,\n",
    "    maxSize,\n",
    "    maxFiles,\n",
    "  });\n",
    "\n",
    "  const uploadFile = async (file: any) => {\n",
    "    setFiles(prev => prev.map(f => \n",
    "      f.id === file.id ? { ...f, status: 'uploading' } : f\n",
    "    ));\n",
    "\n",
    "    try {\n",
    "      // Use the upload hook logic here\n",
    "      const formData = new FormData();\n",
    "      formData.append('file', file);\n",
    "      \n",
    "      // Simplified for example - use the presigned URL pattern from above\n",
    "      const response = await fetch('/api/upload/direct', {\n",
    "        method: 'POST',\n",
    "        body: formData,\n",
    "      });\n",
    "      \n",
    "      if (!response.ok) throw new Error('Upload failed');\n",
    "      \n",
    "      const result = await response.json();\n",
    "      \n",
    "      setFiles(prev => prev.map(f => \n",
    "        f.id === file.id ? { ...f, status: 'done', url: result.url } : f\n",
    "      ));\n",
    "      \n",
    "      onUploadComplete?.([result]);\n",
    "    } catch (error) {\n",
    "      setFiles(prev => prev.map(f => \n",
    "        f.id === file.id ? { ...f, status: 'error' } : f\n",
    "      ));\n",
    "    }\n",
    "  };\n",
    "\n",
    "  const removeFile = (id: string) => {\n",
    "    setFiles(prev => prev.filter(f => f.id !== id));\n",
    "  };\n",
    "\n",
    "  return (\n",
    "    <div className=\"space-y-4\">\n",
    "      <div\n",
    "        {...getRootProps()}\n",
    "        className={`border-2 border-dashed rounded-lg p-8 text-center cursor-pointer transition-colors ${\n",
    "          isDragActive ? 'border-blue-500 bg-blue-50' : 'border-gray-300 hover:border-gray-400'\n",
    "        }`}\n",
    "      >\n",
    "        <input {...getInputProps()} />\n",
    "        <UploadCloudIcon className=\"w-12 h-12 mx-auto text-gray-400 mb-4\" />\n",
    "        <p className=\"text-gray-600\">\n",
    "          {isDragActive ? 'Drop the files here...' : 'Drag & drop files here, or click to select'}\n",
    "        </p>\n",
    "        <p className=\"text-sm text-gray-400 mt-2\">\n",
    "          Max size: {(maxSize / 1024 / 1024).toFixed(1)}MB \u2022 Max files: {maxFiles}\n",
    "        </p>\n",
    "      </div>\n",
    "\n",
    "      {fileRejections.length > 0 && (\n",
    "        <div className=\"p-4 bg-red-50 text-red-600 rounded-lg text-sm\">\n",
    "          {fileRejections.map(({ file, errors }) => (\n",
    "            <div key={file.name}>\n",
    "              {file.name}: {errors.map(e => e.message).join(', ')}\n",
    "            </div>\n",
    "          ))}\n",
    "        </div>\n",
    "      )}\n",
    "\n",
    "      {files.length > 0 && (\n",
    "        <div className=\"space-y-2\">\n",
    "          {files.map((file) => (\n",
    "            <div key={file.id} className=\"flex items-center gap-4 p-3 bg-gray-50 rounded-lg\">\n",
    "              <FileIcon className=\"w-8 h-8 text-gray-400\" />\n",
    "              \n",
    "              <div className=\"flex-1 min-w-0\">\n",
    "                <p className=\"text-sm font-medium truncate\">{file.name}</p>\n",
    "                <p className=\"text-xs text-gray-500\">\n",
    "                  {(file.size / 1024).toFixed(1)} KB\n",
    "                </p>\n",
    "                \n",
    "                {file.status === 'uploading' && (\n",
    "                  <Progress value={file.progress} className=\"h-1 mt-2\" />\n",
    "                )}\n",
    "              </div>\n",
    "\n",
    "              <div className=\"flex items-center gap-2\">\n",
    "                {file.status === 'done' && (\n",
    "                  <span className=\"text-green-600 text-sm\">\u2713 Done</span>\n",
    "                )}\n",
    "                {file.status === 'error' && (\n",
    "                  <span className=\"text-red-600 text-sm\">Failed</span>\n",
    "                )}\n",
    "                \n",
    "                <button\n",
    "                  onClick={() => removeFile(file.id)}\n",
    "                  className=\"p-1 hover:bg-gray-200 rounded\"\n",
    "                  aria-label=\"Remove file\"\n",
    "                >\n",
    "                  <XIcon className=\"w-4 h-4\" />\n",
    "                </button>\n",
    "              </div>\n",
    "            </div>\n",
    "          ))}\n",
    "        </div>\n",
    "      )}\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "```\n",
    "\n",
    "## 36.3 Image Processing and Optimization\n",
    "\n",
    "Process uploaded images to generate thumbnails, responsive variants, and optimize formats.\n",
    "\n",
    "### Sharp Processing Pipeline\n",
    "\n",
    "```typescript\n",
    "// lib/image-processing.ts\n",
    "import sharp from 'sharp';\n",
    "import { S3Client, GetObjectCommand, PutObjectCommand } from '@aws-sdk/client-s3';\n",
    "import { Readable } from 'stream';\n",
    "\n",
    "const s3 = new S3Client({ region: process.env.AWS_REGION });\n",
    "\n",
    "interface ProcessOptions {\n",
    "  widths?: number[];\n",
    "  formats?: ('jpeg' | 'webp' | 'avif')[];\n",
    "  quality?: number;\n",
    "}\n",
    "\n",
    "export async function processImageUpload(key: string, options: ProcessOptions = {}) {\n",
    "  const { widths = [640, 1080, 1920], formats = ['webp', 'jpeg'], quality = 85 } = options;\n",
    "\n",
    "  // Fetch original from S3\n",
    "  const getCommand = new GetObjectCommand({\n",
    "    Bucket: process.env.S3_BUCKET_NAME,\n",
    "    Key: key,\n",
    "  });\n",
    "  \n",
    "  const response = await s3.send(getCommand);\n",
    "  const stream = response.Body as Readable;\n",
    "  \n",
    "  // Convert stream to buffer\n",
    "  const chunks: Buffer[] = [];\n",
    "  for await (const chunk of stream) {\n",
    "    chunks.push(chunk);\n",
    "  }\n",
    "  const buffer = Buffer.concat(chunks);\n",
    "\n",
    "  // Get metadata\n",
    "  const metadata = await sharp(buffer).metadata();\n",
    "  const results: Array<{ width: number; format: string; key: string; url: string }> = [];\n",
    "\n",
    "  // Generate variants\n",
    "  for (const width of widths) {\n",
    "    // Skip if original is smaller than target width\n",
    "    if (metadata.width && metadata.width < width) continue;\n",
    "\n",
    "    for (const format of formats) {\n",
    "      const resized = await sharp(buffer)\n",
    "        .resize(width, null, { \n",
    "          withoutEnlargement: true,\n",
    "          fit: 'inside',\n",
    "        })\n",
    "        .toFormat(format, { quality })\n",
    "        .toBuffer();\n",
    "\n",
    "      const variantKey = key.replace(/(\\.[^.]+)$/, `-${width}.${format}`);\n",
    "      \n",
    "      await s3.send(new PutObjectCommand({\n",
    "        Bucket: process.env.S3_BUCKET_NAME,\n",
    "        Key: variantKey,\n",
    "        Body: resized,\n",
    "        ContentType: `image/${format}`,\n",
    "        Metadata: {\n",
    "          'original-key': key,\n",
    "          'width': width.toString(),\n",
    "          'format': format,\n",
    "        },\n",
    "      }));\n",
    "\n",
    "      results.push({\n",
    "        width,\n",
    "        format,\n",
    "        key: variantKey,\n",
    "        url: `https://${process.env.CDN_DOMAIN}/${variantKey}`,\n",
    "      });\n",
    "    }\n",
    "  }\n",
    "\n",
    "  // Generate blur placeholder\n",
    "  const blurPlaceholder = await sharp(buffer)\n",
    "    .resize(20, null, { fit: 'inside' })\n",
    "    .blur()\n",
    "    .toBuffer();\n",
    "    \n",
    "  const blurBase64 = `data:image/jpeg;base64,${blurPlaceholder.toString('base64')}`;\n",
    "\n",
    "  return {\n",
    "    original: key,\n",
    "    variants: results,\n",
    "    blurDataUrl: blurBase64,\n",
    "    aspectRatio: metadata.width && metadata.height ? metadata.width / metadata.height : null,\n",
    "  };\n",
    "}\n",
    "\n",
    "// Server Action for image processing\n",
    "// app/actions/process-image.ts\n",
    "'use server';\n",
    "\n",
    "import { processImageUpload } from '@/lib/image-processing';\n",
    "import { revalidateTag } from 'next/cache';\n",
    "\n",
    "export async function processUploadedImage(key: string) {\n",
    "  try {\n",
    "    const result = await processImageUpload(key, {\n",
    "      widths: [400, 800, 1200],\n",
    "      formats: ['webp', 'jpeg'],\n",
    "    });\n",
    "    \n",
    "    // Update database with processed image info\n",
    "    await db.image.update({\n",
    "      where: { key },\n",
    "      data: {\n",
    "        processed: true,\n",
    "        variants: result.variants,\n",
    "        blurDataUrl: result.blurDataUrl,\n",
    "        aspectRatio: result.aspectRatio,\n",
    "      },\n",
    "    });\n",
    "    \n",
    "    revalidateTag(`image-${key}`);\n",
    "    \n",
    "    return { success: true, variants: result.variants };\n",
    "  } catch (error) {\n",
    "    console.error('Image processing failed:', error);\n",
    "    throw new Error('Failed to process image');\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 36.4 File Validation and Security\n",
    "\n",
    "Implement comprehensive security measures including MIME type verification and malware scanning.\n",
    "\n",
    "### Security Utilities\n",
    "\n",
    "```typescript\n",
    "// lib/file-security.ts\n",
    "import { fileTypeFromBuffer } from 'file-type';\n",
    "import magicBytes from 'magic-bytes.js';\n",
    "import { spawn } from 'child_process';\n",
    "import { promisify } from 'util';\n",
    "import { writeFile, unlink } from 'fs/promises';\n",
    "import path from 'path';\n",
    "import { tmpdir } from 'os';\n",
    "\n",
    "// Magic numbers validation (more secure than extension checking)\n",
    "export async function validateFileType(buffer: Buffer, allowedTypes: string[]): Promise<boolean> {\n",
    "  // Use file-type library for modern formats\n",
    "  const type = await fileTypeFromBuffer(buffer);\n",
    "  \n",
    "  if (!type) {\n",
    "    // Fallback to magic-bytes for additional formats\n",
    "    const detected = magicBytes(buffer);\n",
    "    if (detected.length === 0) return false;\n",
    "    return allowedTypes.includes(detected[0].typename);\n",
    "  }\n",
    "  \n",
    "  return allowedTypes.includes(type.mime);\n",
    "}\n",
    "\n",
    "// ClamAV Virus Scanning\n",
    "export async function scanFile(buffer: Buffer): Promise<{ clean: boolean; viruses?: string[] }> {\n",
    "  if (process.env.DISABLE_VIRUS_SCAN === 'true') {\n",
    "    return { clean: true };\n",
    "  }\n",
    "\n",
    "  const tempPath = path.join(tmpdir(), `scan-${Date.now()}`);\n",
    "  \n",
    "  try {\n",
    "    await writeFile(tempPath, buffer);\n",
    "    \n",
    "    return new Promise((resolve, reject) => {\n",
    "      const clamscan = spawn('clamdscan', ['--no-summary', '--stdout', tempPath]);\n",
    "      let output = '';\n",
    "      \n",
    "      clamscan.stdout.on('data', (data) => {\n",
    "        output += data.toString();\n",
    "      });\n",
    "      \n",
    "      clamscan.on('close', async (code) => {\n",
    "        await unlink(tempPath);\n",
    "        \n",
    "        if (code === 0) {\n",
    "          resolve({ clean: true });\n",
    "        } else if (code === 1) {\n",
    "          // Virus found\n",
    "          const viruses = output.match(/FOUND\\s+(.+)/)?.[1];\n",
    "          resolve({ clean: false, viruses: viruses ? [viruses] : ['Unknown'] });\n",
    "        } else {\n",
    "          reject(new Error('Virus scan failed'));\n",
    "        }\n",
    "      });\n",
    "      \n",
    "      clamscan.on('error', (err) => {\n",
    "        unlink(tempPath).catch(console.error);\n",
    "        reject(err);\n",
    "      });\n",
    "    });\n",
    "  } catch (error) {\n",
    "    await unlink(tempPath).catch(() => {});\n",
    "    throw error;\n",
    "  }\n",
    "}\n",
    "\n",
    "// Content Security Policy for uploads\n",
    "export function getUploadSecurityHeaders(contentType: string): Record<string, string> {\n",
    "  // Force download for executable files\n",
    "  const dangerousTypes = ['application/x-msdownload', 'application/x-executable'];\n",
    "  \n",
    "  if (dangerousTypes.some(t => contentType.includes(t))) {\n",
    "    return {\n",
    "      'Content-Disposition': 'attachment',\n",
    "      'X-Content-Type-Options': 'nosniff',\n",
    "    };\n",
    "  }\n",
    "  \n",
    "  return {\n",
    "    'X-Content-Type-Options': 'nosniff',\n",
    "  };\n",
    "}\n",
    "\n",
    "// Sanitize filename\n",
    "export function sanitizeFilename(filename: string): string {\n",
    "  // Remove path traversal attempts\n",
    "  const basename = path.basename(filename);\n",
    "  // Remove control characters\n",
    "  const sanitized = basename.replace(/[\\x00-\\x1f\\x80-\\x9f]/g, '');\n",
    "  // Limit length\n",
    "  return sanitized.slice(0, 255);\n",
    "}\n",
    "```\n",
    "\n",
    "### Secure File Serving\n",
    "\n",
    "```typescript\n",
    "// app/api/files/[...key]/route.ts\n",
    "import { NextRequest, NextResponse } from 'next/server';\n",
    "import { S3Client, GetObjectCommand, HeadObjectCommand } from '@aws-sdk/client-s3';\n",
    "import { getSignedUrl } from '@aws-sdk/s3-request-presigner';\n",
    "\n",
    "const s3 = new S3Client({ region: process.env.AWS_REGION });\n",
    "\n",
    "export async function GET(\n",
    "  req: NextRequest,\n",
    "  { params }: { params: { key: string[] } }\n",
    ") {\n",
    "  const key = params.key.join('/');\n",
    "  \n",
    "  try {\n",
    "    // Check if file exists and get metadata\n",
    "    const headCommand = new HeadObjectCommand({\n",
    "      Bucket: process.env.S3_BUCKET_NAME,\n",
    "      Key: key,\n",
    "    });\n",
    "    \n",
    "    const metadata = await s3.send(headCommand);\n",
    "    \n",
    "    // Security checks\n",
    "    const contentType = metadata.ContentType || 'application/octet-stream';\n",
    "    \n",
    "    // Block dangerous file types from being served inline\n",
    "    const blockedTypes = [\n",
    "      'application/x-msdownload',\n",
    "      'application/x-executable',\n",
    "      'text/html',\n",
    "      'application/javascript',\n",
    "    ];\n",
    "    \n",
    "    if (blockedTypes.includes(contentType)) {\n",
    "      return new NextResponse('File type not allowed', { status: 403 });\n",
    "    }\n",
    "\n",
    "    // Check user authorization (example: private files)\n",
    "    const user = await getCurrentUser(req);\n",
    "    const fileRecord = await db.fileUpload.findUnique({ where: { key } });\n",
    "    \n",
    "    if (fileRecord?.userId && fileRecord.userId !== user?.id) {\n",
    "      return new NextResponse('Unauthorized', { status: 403 });\n",
    "    }\n",
    "\n",
    "    // Generate presigned URL for private files, or redirect to CDN for public\n",
    "    if (fileRecord?.isPrivate) {\n",
    "      const getCommand = new GetObjectCommand({\n",
    "        Bucket: process.env.S3_BUCKET_NAME,\n",
    "        Key: key,\n",
    "        ResponseContentDisposition: `inline; filename=\"${fileRecord.filename}\"`,\n",
    "      });\n",
    "      \n",
    "      const url = await getSignedUrl(s3, getCommand, { expiresIn: 300 });\n",
    "      return NextResponse.redirect(url);\n",
    "    }\n",
    "    \n",
    "    // Public file - redirect to CDN\n",
    "    return NextResponse.redirect(`https://${process.env.CDN_DOMAIN}/${key}`);\n",
    "    \n",
    "  } catch (error) {\n",
    "    if ((error as any).name === 'NoSuchKey') {\n",
    "      return new NextResponse('File not found', { status: 404 });\n",
    "    }\n",
    "    return new NextResponse('Internal error', { status: 500 });\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "## 36.5 Large File and Resumable Uploads\n",
    "\n",
    "Handle large files with multipart uploads and support resumable uploads for unreliable connections.\n",
    "\n",
    "### Multipart Upload Handler\n",
    "\n",
    "```typescript\n",
    "// lib/storage/multipart.ts\n",
    "import { S3Client, CreateMultipartUploadCommand, UploadPartCommand, CompleteMultipartUploadCommand, AbortMultipartUploadCommand } from '@aws-sdk/client-s3';\n",
    "\n",
    "const s3 = new S3Client({ region: process.env.AWS_REGION });\n",
    "const PART_SIZE = 5 * 1024 * 1024; // 5MB parts (minimum for S3)\n",
    "\n",
    "export async function initiateMultipartUpload(filename: string, contentType: string) {\n",
    "  const key = `uploads/large/${Date.now()}-${sanitizeFilename(filename)}`;\n",
    "  \n",
    "  const command = new CreateMultipartUploadCommand({\n",
    "    Bucket: process.env.S3_BUCKET_NAME,\n",
    "    Key: key,\n",
    "    ContentType: contentType,\n",
    "  });\n",
    "  \n",
    "  const response = await s3.send(command);\n",
    "  \n",
    "  // Store multipart upload ID in database\n",
    "  const uploadRecord = await db.multipartUpload.create({\n",
    "    data: {\n",
    "      uploadId: response.UploadId!,\n",
    "      key,\n",
    "      filename,\n",
    "      contentType,\n",
    "      status: 'initiated',\n",
    "      parts: [],\n",
    "    },\n",
    "  });\n",
    "  \n",
    "  return {\n",
    "    uploadId: response.UploadId,\n",
    "    key,\n",
    "    partSize: PART_SIZE,\n",
    "  };\n",
    "}\n",
    "\n",
    "export async function getPresignedPartUrl(uploadId: string, key: string, partNumber: number) {\n",
    "  const command = new UploadPartCommand({\n",
    "    Bucket: process.env.S3_BUCKET_NAME,\n",
    "    Key: key,\n",
    "    UploadId: uploadId,\n",
    "    PartNumber: partNumber,\n",
    "  });\n",
    "  \n",
    "  // Note: AWS SDK v3 doesn't support presigned URLs for multipart directly\n",
    "  // You'd typically use @aws-sdk/s3-request-presigner or handle parts server-side\n",
    "  \n",
    "  return { url: 'presigned-url-placeholder', partNumber };\n",
    "}\n",
    "\n",
    "export async function completeMultipartUpload(uploadId: string, key: string, parts: Array<{ ETag: string; PartNumber: number }>) {\n",
    "  const command = new CompleteMultipartUploadCommand({\n",
    "    Bucket: process.env.S3_BUCKET_NAME,\n",
    "    Key: key,\n",
    "    UploadId: uploadId,\n",
    "    MultipartUpload: { Parts: parts.sort((a, b) => a.PartNumber - b.PartNumber) },\n",
    "  });\n",
    "  \n",
    "  await s3.send(command);\n",
    "  \n",
    "  await db.multipartUpload.update({\n",
    "    where: { uploadId },\n",
    "    data: { status: 'completed', completedAt: new Date() },\n",
    "  });\n",
    "  \n",
    "  return { key, url: `https://${process.env.CDN_DOMAIN}/${key}` };\n",
    "}\n",
    "```\n",
    "\n",
    "### Client-Side Resumable Upload\n",
    "\n",
    "```typescript\n",
    "// hooks/use-resumable-upload.ts\n",
    "'use client';\n",
    "\n",
    "import { useState, useCallback, useRef } from 'react';\n",
    "\n",
    "interface Part {\n",
    "  number: number;\n",
    "  etag?: string;\n",
    "  status: 'pending' | 'uploading' | 'complete' | 'error';\n",
    "}\n",
    "\n",
    "export function useResumableUpload() {\n",
    "  const [progress, setProgress] = useState(0);\n",
    "  const [isUploading, setIsUploading] = useState(false);\n",
    "  const abortController = useRef<AbortController | null>(null);\n",
    "  const parts = useRef<Part[]>([]);\n",
    "\n",
    "  const uploadLargeFile = useCallback(async (file: File) => {\n",
    "    setIsUploading(true);\n",
    "    setProgress(0);\n",
    "    \n",
    "    try {\n",
    "      // 1. Initiate multipart upload\n",
    "      const initRes = await fetch('/api/upload/multipart/init', {\n",
    "        method: 'POST',\n",
    "        headers: { 'Content-Type': 'application/json' },\n",
    "        body: JSON.stringify({\n",
    "          filename: file.name,\n",
    "          contentType: file.type,\n",
    "          size: file.size,\n",
    "        }),\n",
    "      });\n",
    "      \n",
    "      const { uploadId, key, partSize } = await initRes.json();\n",
    "      \n",
    "      // 2. Calculate parts\n",
    "      const totalParts = Math.ceil(file.size / partSize);\n",
    "      parts.current = Array.from({ length: totalParts }, (_, i) => ({\n",
    "        number: i + 1,\n",
    "        status: 'pending',\n",
    "      }));\n",
    "      \n",
    "      // 3. Upload parts (in parallel with concurrency limit)\n",
    "      const CONCURRENCY = 3;\n",
    "      const queue: Promise<void>[] = [];\n",
    "      \n",
    "      for (let i = 0; i < totalParts; i++) {\n",
    "        const partNumber = i + 1;\n",
    "        const start = i * partSize;\n",
    "        const end = Math.min(start + partSize, file.size);\n",
    "        const chunk = file.slice(start, end);\n",
    "        \n",
    "        const uploadPromise = uploadPart(uploadId, key, partNumber, chunk, file.type);\n",
    "        queue.push(uploadPromise);\n",
    "        \n",
    "        if (queue.length >= CONCURRENCY) {\n",
    "          await Promise.all(queue);\n",
    "          queue.length = 0;\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      await Promise.all(queue);\n",
    "      \n",
    "      // 4. Complete upload\n",
    "      const completeRes = await fetch('/api/upload/multipart/complete', {\n",
    "        method: 'POST',\n",
    "        headers: { 'Content-Type': 'application/json' },\n",
    "        body: JSON.stringify({\n",
    "          uploadId,\n",
    "          key,\n",
    "          parts: parts.current.map(p => ({ ETag: p.etag, PartNumber: p.number })),\n",
    "        }),\n",
    "      });\n",
    "      \n",
    "      return await completeRes.json();\n",
    "    } catch (error) {\n",
    "      console.error('Resumable upload failed:', error);\n",
    "      throw error;\n",
    "    } finally {\n",
    "      setIsUploading(false);\n",
    "    }\n",
    "  }, []);\n",
    "\n",
    "  const uploadPart = async (uploadId: string, key: string, partNumber: number, chunk: Blob, contentType: string) => {\n",
    "    // Get presigned URL for this part\n",
    "    const urlRes = await fetch(`/api/upload/multipart/url?uploadId=${uploadId}&key=${key}&part=${partNumber}`);\n",
    "    const { url } = await urlRes.json();\n",
    "    \n",
    "    // Upload part\n",
    "    const response = await fetch(url, {\n",
    "      method: 'PUT',\n",
    "      body: chunk,\n",
    "      headers: { 'Content-Type': contentType },\n",
    "    });\n",
    "    \n",
    "    if (!response.ok) throw new Error(`Part ${partNumber} failed`);\n",
    "    \n",
    "    const etag = response.headers.get('ETag');\n",
    "    parts.current[partNumber - 1].etag = etag?.replace(/\"/g, '');\n",
    "    parts.current[partNumber - 1].status = 'complete';\n",
    "    \n",
    "    // Update progress\n",
    "    const completed = parts.current.filter(p => p.status === 'complete').length;\n",
    "    setProgress(Math.round((completed / parts.current.length) * 100));\n",
    "  };\n",
    "\n",
    "  return {\n",
    "    uploadLargeFile,\n",
    "    progress,\n",
    "    isUploading,\n",
    "  };\n",
    "}\n",
    "```\n",
    "\n",
    "## Key Takeaways from Chapter 36\n",
    "\n",
    "1. **Presigned URL Pattern**: Never expose cloud storage credentials to the client. Generate time-limited presigned URLs server-side that allow direct browser-to-S3 uploads. This offloads bandwidth from your application servers while maintaining security through server-side validation of file types, sizes, and user permissions before generating URLs.\n",
    "\n",
    "2. **Client-Side Uploads**: Use React hooks with `XMLHttpRequest` or Axios to track upload progress. Implement abort controllers for cancellation capabilities. Validate file types client-side for UX, but always re-validate server-side before generating presigned URLs.\n",
    "\n",
    "3. **Image Processing**: Use Sharp for server-side image optimization\u2014generating responsive variants (400w, 800w, 1200w), converting to modern formats (WebP, AVIF), and creating blur placeholders for Next.js Image components. Process images asynchronously via queues (Redis/Bull) to avoid blocking API responses.\n",
    "\n",
    "4. **Security Measures**: Validate file types using magic numbers (file-type library) rather than extensions alone. Scan uploaded files with ClamAV or similar antivirus tools before marking uploads as complete. Sanitize filenames to prevent path traversal attacks. Serve files with `X-Content-Type-Options: nosniff` and appropriate Content-Disposition headers.\n",
    "\n",
    "5. **Large File Handling**: For files over 100MB, implement multipart (chunked) uploads with 5MB+ parts. Track part ETags client-side and complete the multipart upload once all parts succeed. Store upload progress in Redis to enable resumable uploads across page refreshes.\n",
    "\n",
    "6. **Storage Architecture**: Use S3-compatible storage (AWS S3, Cloudflare R2, MinIO) with CDN fronting (CloudFront or Cloudflare) for global distribution. Configure CORS policies to allow direct browser uploads only from your domain. Enable object versioning to prevent accidental overwrites.\n",
    "\n",
    "7. **Post-Processing**: Trigger image optimization, virus scanning, and metadata extraction via webhooks or queue workers after upload completion. Use database records to track upload status (pending \u2192 processing \u2192 complete) and maintain references between user records and storage keys.\n",
    "\n",
    "## Coming Up Next\n",
    "\n",
    "**Chapter 37: Development Tooling**\n",
    "\n",
    "With file handling capabilities secured, it's time to optimize your development workflow. In Chapter 37, we'll explore advanced VS Code configurations for Next.js, ESLint and Prettier setups for consistent code quality, Git hooks for pre-commit validation, debugging techniques for Server Components, and development best practices. You'll learn how to configure a professional-grade development environment that catches errors early and maintains code quality across your team."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='35. e-commerce_integration.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../7. Tooling_and_workflow/37. development_tooling.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}