{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9b3e483d",
   "metadata": {},
   "source": [
    "# Chapter 30: AI Integration\n",
    "\n",
    "Artificial intelligence is transforming modern web applications, enabling dynamic content generation, intelligent chat interfaces, and automated workflows. Next.js provides first-class support for AI integration through the Vercel AI SDK, offering streaming capabilities, tool calling, and framework-agnostic compatibility. Implementing AI features requires careful consideration of streaming architecture, cost management, and privacy safeguards.\n",
    "\n",
    "By the end of this chapter, you'll master implementing streaming chat interfaces with the AI SDK, building tool-augmented AI agents, optimizing token usage and costs, handling AI errors gracefully, maintaining user privacy with edge AI processing, and architecting real-world AI features for production applications.\n",
    "\n",
    "## 30.1 Vercel AI SDK Setup\n",
    "\n",
    "Configure the AI SDK with multiple providers and streaming capabilities.\n",
    "\n",
    "### Provider Configuration\n",
    "\n",
    "```typescript\n",
    "// lib/ai/providers.ts\n",
    "import { createOpenAI } from '@ai-sdk/openai';\n",
    "import { createAnthropic } from '@ai-sdk/anthropic';\n",
    "import { createGoogleGenerativeAI } from '@ai-sdk/google';\n",
    "import { LanguageModel } from 'ai';\n",
    "\n",
    "// Provider factory for flexible model selection\n",
    "export function createModel(provider: string, modelId: string): LanguageModel {\n",
    "  switch (provider) {\n",
    "    case 'openai':\n",
    "      const openai = createOpenAI({\n",
    "        apiKey: process.env.OPENAI_API_KEY,\n",
    "        compatibility: 'strict',\n",
    "      });\n",
    "      return openai(modelId);\n",
    "      \n",
    "    case 'anthropic':\n",
    "      const anthropic = createAnthropic({\n",
    "        apiKey: process.env.ANTHROPIC_API_KEY,\n",
    "      });\n",
    "      return anthropic(modelId);\n",
    "      \n",
    "    case 'google':\n",
    "      const google = createGoogleGenerativeAI({\n",
    "        apiKey: process.env.GOOGLE_GENERATIVE_AI_API_KEY,\n",
    "      });\n",
    "      return google(modelId);\n",
    "      \n",
    "    default:\n",
    "      throw new Error(`Unknown provider: ${provider}`);\n",
    "  }\n",
    "}\n",
    "\n",
    "// Model selection based on use case\n",
    "export const models = {\n",
    "  fast: createModel('openai', 'gpt-4o-mini'),      // For quick responses\n",
    "  balanced: createModel('openai', 'gpt-4o'),       // For general use\n",
    "  powerful: createModel('anthropic', 'claude-3-5-sonnet-20241022'), // For complex reasoning\n",
    "  vision: createModel('openai', 'gpt-4o-vision-preview'), // For image analysis\n",
    "};\n",
    "```\n",
    "\n",
    "### Basic Streaming Chat Route\n",
    "\n",
    "```typescript\n",
    "// app/api/chat/route.ts\n",
    "import { streamText, convertToCoreMessages, tool } from 'ai';\n",
    "import { z } from 'zod';\n",
    "import { models } from '@/lib/ai/providers';\n",
    "\n",
    "export const maxDuration = 30; // Allow 30s for streaming\n",
    "\n",
    "export async function POST(req: Request) {\n",
    "  const { messages, model = 'balanced' } = await req.json();\n",
    "\n",
    "  const result = await streamText({\n",
    "    model: models[model as keyof typeof models],\n",
    "    system: `You are a helpful Next.js coding assistant. \n",
    "      Provide concise, accurate answers with code examples when relevant.\n",
    "      Always use TypeScript and App Router patterns.`,\n",
    "    messages: convertToCoreMessages(messages),\n",
    "    maxTokens: 2000,\n",
    "    temperature: 0.7,\n",
    "    tools: {\n",
    "      getDocumentation: tool({\n",
    "        description: 'Get the latest Next.js documentation for a specific topic',\n",
    "        parameters: z.object({\n",
    "          topic: z.string().describe('The topic to search for (e.g., \"server components\", \"routing\")'),\n",
    "          version: z.string().optional().describe('Next.js version, defaults to 14'),\n",
    "        }),\n",
    "        execute: async ({ topic, version = '14' }) => {\n",
    "          // Fetch from docs API or search index\n",
    "          const docs = await fetchDocumentation(topic, version);\n",
    "          return {\n",
    "            title: docs.title,\n",
    "            content: docs.content,\n",
    "            url: docs.url,\n",
    "          };\n",
    "        },\n",
    "      }),\n",
    "      searchCodeExamples: tool({\n",
    "        description: 'Search for relevant code examples in the knowledge base',\n",
    "        parameters: z.object({\n",
    "          query: z.string().describe('The search query for code patterns'),\n",
    "          framework: z.enum(['nextjs', 'react', 'typescript']).default('nextjs'),\n",
    "        }),\n",
    "        execute: async ({ query, framework }) => {\n",
    "          return await searchExamples(query, framework);\n",
    "        },\n",
    "      }),\n",
    "    },\n",
    "    onFinish: async ({ text, toolCalls, toolResults, finishReason, usage }) => {\n",
    "      // Log usage for analytics/cost tracking\n",
    "      console.log('AI Usage:', {\n",
    "        promptTokens: usage.promptTokens,\n",
    "        completionTokens: usage.completionTokens,\n",
    "        totalTokens: usage.totalTokens,\n",
    "        finishReason,\n",
    "      });\n",
    "      \n",
    "      // Store conversation history if needed\n",
    "      await saveConversation(messages, text, toolCalls);\n",
    "    },\n",
    "  });\n",
    "\n",
    "  return result.toDataStreamResponse();\n",
    "}\n",
    "\n",
    "// Helper functions\n",
    "async function fetchDocumentation(topic: string, version: string) {\n",
    "  // Implementation would query your docs or external API\n",
    "  return {\n",
    "    title: `Next.js ${version} - ${topic}`,\n",
    "    content: 'Documentation content...',\n",
    "    url: `https://nextjs.org/docs/${version}/${topic}`,\n",
    "  };\n",
    "}\n",
    "\n",
    "async function searchExamples(query: string, framework: string) {\n",
    "  // Search vector database or code repository\n",
    "  return [\n",
    "    { title: 'Example 1', code: '...', relevance: 0.95 },\n",
    "  ];\n",
    "}\n",
    "\n",
    "async function saveConversation(messages: any[], response: string, tools: any[]) {\n",
    "  // Persist to database for context windows or analytics\n",
    "}\n",
    "```\n",
    "\n",
    "## 30.2 AI-Powered UI Components\n",
    "\n",
    "Build React components that handle streaming responses and tool invocations.\n",
    "\n",
    "### Streaming Chat Interface\n",
    "\n",
    "```typescript\n",
    "// components/ai/chat.tsx\n",
    "'use client';\n",
    "\n",
    "import { useChat } from 'ai/react';\n",
    "import { useState, useRef, useEffect } from 'react';\n",
    "import { Send, Loader2, Bot, User, Code, ExternalLink } from 'lucide-react';\n",
    "\n",
    "interface ChatProps {\n",
    "  initialMessages?: Array<{ role: 'user' | 'assistant'; content: string }>;\n",
    "  context?: string; // Additional context about current page\n",
    "}\n",
    "\n",
    "export function ChatInterface({ initialMessages = [], context }: ChatProps) {\n",
    "  const { messages, input, handleInputChange, handleSubmit, isLoading, error, reload } = useChat({\n",
    "    api: '/api/chat',\n",
    "    initialMessages,\n",
    "    body: {\n",
    "      context, // Pass page context to AI\n",
    "    },\n",
    "    onError: (err) => {\n",
    "      console.error('Chat error:', err);\n",
    "    },\n",
    "  });\n",
    "\n",
    "  const messagesEndRef = useRef<HTMLDivElement>(null);\n",
    "  const [showTools, setShowTools] = useState(true);\n",
    "\n",
    "  // Auto-scroll to bottom\n",
    "  useEffect(() => {\n",
    "    messagesEndRef.current?.scrollIntoView({ behavior: 'smooth' });\n",
    "  }, [messages]);\n",
    "\n",
    "  return (\n",
    "    <div className=\"flex flex-col h-[600px] bg-white rounded-lg border shadow-sm\">\n",
    "      {/* Header */}\n",
    "      <div className=\"p-4 border-b flex items-center justify-between bg-gray-50 rounded-t-lg\">\n",
    "        <div className=\"flex items-center gap-2\">\n",
    "          <Bot className=\"w-5 h-5 text-blue-600\" />\n",
    "          <h3 className=\"font-semibold\">AI Assistant</h3>\n",
    "        </div>\n",
    "        <div className=\"flex items-center gap-2 text-sm text-gray-500\">\n",
    "          <span className={`w-2 h-2 rounded-full ${isLoading ? 'bg-yellow-400 animate-pulse' : 'bg-green-400'}`} />\n",
    "          {isLoading ? 'Thinking...' : 'Ready'}\n",
    "        </div>\n",
    "      </div>\n",
    "\n",
    "      {/* Messages */}\n",
    "      <div className=\"flex-1 overflow-y-auto p-4 space-y-4\">\n",
    "        {messages.map((message, index) => (\n",
    "          <div\n",
    "            key={message.id}\n",
    "            className={`flex gap-3 ${message.role === 'user' ? 'flex-row-reverse' : ''}`}\n",
    "          >\n",
    "            <div className={`w-8 h-8 rounded-full flex items-center justify-center ${\n",
    "              message.role === 'user' ? 'bg-blue-100' : 'bg-purple-100'\n",
    "            }`}>\n",
    "              {message.role === 'user' ? <User className=\"w-4 h-4\" /> : <Bot className=\"w-4 h-4\" />}\n",
    "            </div>\n",
    "            \n",
    "            <div className={`flex-1 max-w-[80%] ${message.role === 'user' ? 'items-end' : 'items-start'}`}>\n",
    "              <div className={`p-3 rounded-lg ${\n",
    "                message.role === 'user' \n",
    "                  ? 'bg-blue-600 text-white' \n",
    "                  : 'bg-gray-100 text-gray-900'\n",
    "              }`}>\n",
    "                {message.content ? (\n",
    "                  <div className=\"prose prose-sm max-w-none\">\n",
    "                    {message.content}\n",
    "                  </div>\n",
    "                ) : (\n",
    "                  <div className=\"flex items-center gap-2 text-gray-400\">\n",
    "                    <Loader2 className=\"w-4 h-4 animate-spin\" />\n",
    "                    Thinking...\n",
    "                  </div>\n",
    "                )}\n",
    "                \n",
    "                {/* Tool Invocations */}\n",
    "                {message.toolInvocations?.map((tool) => (\n",
    "                  <div key={tool.toolCallId} className=\"mt-2 p-2 bg-black/10 rounded text-sm\">\n",
    "                    <div className=\"flex items-center gap-1 text-xs opacity-70 mb-1\">\n",
    "                      <Code className=\"w-3 h-3\" />\n",
    "                      Using {tool.toolName}\n",
    "                    </div>\n",
    "                    {tool.state === 'result' ? (\n",
    "                      <div className=\"text-xs\">\n",
    "                        {tool.toolName === 'getDocumentation' && (\n",
    "                          <a \n",
    "                            href={tool.result.url} \n",
    "                            target=\"_blank\" \n",
    "                            rel=\"noopener noreferrer\"\n",
    "                            className=\"flex items-center gap-1 hover:underline\"\n",
    "                          >\n",
    "                            View docs: {tool.result.title}\n",
    "                            <ExternalLink className=\"w-3 h-3\" />\n",
    "                          </a>\n",
    "                        )}\n",
    "                      </div>\n",
    "                    ) : (\n",
    "                      <div className=\"text-xs opacity-70\">Loading...</div>\n",
    "                    )}\n",
    "                  </div>\n",
    "                ))}\n",
    "              </div>\n",
    "            </div>\n",
    "          </div>\n",
    "        ))}\n",
    "        <div ref={messagesEndRef} />\n",
    "      </div>\n",
    "\n",
    "      {/* Error State */}\n",
    "      {error && (\n",
    "        <div className=\"p-3 bg-red-50 text-red-600 text-sm flex items-center justify-between\">\n",
    "          <span>Error: {error.message}</span>\n",
    "          <button \n",
    "            onClick={() => reload()}\n",
    "            className=\"text-red-700 underline\"\n",
    "          >\n",
    "            Retry\n",
    "          </button>\n",
    "        </div>\n",
    "      )}\n",
    "\n",
    "      {/* Input */}\n",
    "      <form onSubmit={handleSubmit} className=\"p-4 border-t\">\n",
    "        <div className=\"flex gap-2\">\n",
    "          <input\n",
    "            value={input}\n",
    "            onChange={handleInputChange}\n",
    "            placeholder=\"Ask about Next.js...\"\n",
    "            className=\"flex-1 px-4 py-2 border rounded-lg focus:outline-none focus:ring-2 focus:ring-blue-500\"\n",
    "            disabled={isLoading}\n",
    "          />\n",
    "          <button\n",
    "            type=\"submit\"\n",
    "            disabled={isLoading || !input.trim()}\n",
    "            className=\"px-4 py-2 bg-blue-600 text-white rounded-lg disabled:opacity-50 disabled:cursor-not-allowed\"\n",
    "          >\n",
    "            {isLoading ? <Loader2 className=\"w-5 h-5 animate-spin\" /> : <Send className=\"w-5 h-5\" />}\n",
    "          </button>\n",
    "        </div>\n",
    "        <p className=\"mt-2 text-xs text-gray-500\">\n",
    "          AI may produce inaccurate information. Verify important code before using.\n",
    "        </p>\n",
    "      </form>\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "```\n",
    "\n",
    "## 30.3 Streaming and Real-Time AI\n",
    "\n",
    "Implement streaming patterns for real-time AI experiences beyond simple chat.\n",
    "\n",
    "### Real-Time Streaming Component\n",
    "\n",
    "```typescript\n",
    "// components/ai/streaming-completion.tsx\n",
    "'use client';\n",
    "\n",
    "import { useState, useCallback } from 'react';\n",
    "import { readStreamableValue } from 'ai/rsc';\n",
    "import { streamContent } from '@/app/actions/generate';\n",
    "\n",
    "interface StreamingCompletionProps {\n",
    "  prompt: string;\n",
    "  onComplete?: (content: string) => void;\n",
    "}\n",
    "\n",
    "export function StreamingCompletion({ prompt, onComplete }: StreamingCompletionProps) {\n",
    "  const [content, setContent] = useState('');\n",
    "  const [isStreaming, setIsStreaming] = useState(false);\n",
    "  const [error, setError] = useState<string | null>(null);\n",
    "\n",
    "  const generate = useCallback(async () => {\n",
    "    setIsStreaming(true);\n",
    "    setContent('');\n",
    "    setError(null);\n",
    "\n",
    "    try {\n",
    "      // Server Action returns a streamable value\n",
    "      const { output } = await streamContent(prompt);\n",
    "      \n",
    "      for await (const delta of readStreamableValue(output)) {\n",
    "        if (delta) {\n",
    "          setContent((prev) => prev + delta);\n",
    "        }\n",
    "      }\n",
    "      \n",
    "      onComplete?.(content);\n",
    "    } catch (err) {\n",
    "      setError(err instanceof Error ? err.message : 'Unknown error');\n",
    "    } finally {\n",
    "      setIsStreaming(false);\n",
    "    }\n",
    "  }, [prompt, content, onComplete]);\n",
    "\n",
    "  return (\n",
    "    <div className=\"space-y-4\">\n",
    "      <button\n",
    "        onClick={generate}\n",
    "        disabled={isStreaming}\n",
    "        className=\"px-4 py-2 bg-purple-600 text-white rounded-lg disabled:opacity-50\"\n",
    "      >\n",
    "        {isStreaming ? 'Generating...' : 'Generate Content'}\n",
    "      </button>\n",
    "\n",
    "      {content && (\n",
    "        <div className=\"p-4 bg-gray-50 rounded-lg border\">\n",
    "          <div className=\"prose prose-sm max-w-none\">\n",
    "            {content}\n",
    "          </div>\n",
    "        </div>\n",
    "      )}\n",
    "\n",
    "      {error && (\n",
    "        <div className=\"p-3 bg-red-50 text-red-600 rounded text-sm\">\n",
    "          {error}\n",
    "        </div>\n",
    "      )}\n",
    "    </div>\n",
    "  );\n",
    "}\n",
    "\n",
    "// Server Action implementation\n",
    "// app/actions/generate.ts\n",
    "'use server';\n",
    "\n",
    "import { createStreamableValue } from 'ai/rsc';\n",
    "import { streamText } from 'ai';\n",
    "import { models } from '@/lib/ai/providers';\n",
    "\n",
    "export async function streamContent(prompt: string) {\n",
    "  const stream = createStreamableValue('');\n",
    "\n",
    "  (async () => {\n",
    "    const { textStream } = await streamText({\n",
    "      model: models.fast,\n",
    "      prompt,\n",
    "    });\n",
    "\n",
    "    for await (const delta of textStream) {\n",
    "      stream.update(delta);\n",
    "    }\n",
    "\n",
    "    stream.done();\n",
    "  })();\n",
    "\n",
    "  return { output: stream.value };\n",
    "}\n",
    "```\n",
    "\n",
    "## 30.4 AI Integration Patterns\n",
    "\n",
    "Implement common AI patterns like RAG (Retrieval Augmented Generation) and multi-step agents.\n",
    "\n",
    "### RAG Implementation\n",
    "\n",
    "```typescript\n",
    "// lib/ai/rag.ts\n",
    "import { embed, embedMany } from 'ai';\n",
    "import { models } from './providers';\n",
    "import { cosineSimilarity } from '@/lib/vector-math';\n",
    "\n",
    "interface Document {\n",
    "  id: string;\n",
    "  content: string;\n",
    "  metadata: Record<string, any>;\n",
    "  embedding?: number[];\n",
    "}\n",
    "\n",
    "export async function generateEmbeddings(texts: string[]): Promise<number[][]> {\n",
    "  const { embeddings } = await embedMany({\n",
    "    model: models.fast,\n",
    "    values: texts,\n",
    "  });\n",
    "  return embeddings;\n",
    "}\n",
    "\n",
    "export async function findRelevantDocuments(\n",
    "  query: string,\n",
    "  documents: Document[],\n",
    "  topK: number = 3\n",
    "): Promise<Document[]> {\n",
    "  // Generate embedding for query\n",
    "  const { embedding: queryEmbedding } = await embed({\n",
    "    model: models.fast,\n",
    "    value: query,\n",
    "  });\n",
    "\n",
    "  // Calculate similarities\n",
    "  const scored = documents.map((doc) => ({\n",
    "    ...doc,\n",
    "    score: doc.embedding \n",
    "      ? cosineSimilarity(queryEmbedding, doc.embedding)\n",
    "      : 0,\n",
    "  }));\n",
    "\n",
    "  // Sort by relevance and return top K\n",
    "  return scored\n",
    "    .sort((a, b) => b.score - a.score)\n",
    "    .slice(0, topK)\n",
    "    .map(({ score, ...doc }) => doc);\n",
    "}\n",
    "\n",
    "// Route Handler with RAG\n",
    "// app/api/chat/rag/route.ts\n",
    "import { streamText, convertToCoreMessages } from 'ai';\n",
    "import { findRelevantDocuments } from '@/lib/ai/rag';\n",
    "import { getAllDocuments } from '@/lib/docs';\n",
    "\n",
    "export async function POST(req: Request) {\n",
    "  const { messages } = await req.json();\n",
    "  const lastMessage = messages[messages.length - 1];\n",
    "\n",
    "  // Retrieve relevant context\n",
    "  const documents = await getAllDocuments();\n",
    "  const relevant = await findRelevantDocuments(lastMessage.content, documents, 3);\n",
    "  \n",
    "  const context = relevant\n",
    "    .map((doc) => `Source: ${doc.metadata.title}\\n${doc.content}`)\n",
    "    .join('\\n\\n---\\n\\n');\n",
    "\n",
    "  const result = await streamText({\n",
    "    model: models.balanced,\n",
    "    system: `You are a helpful assistant. Use the following context to answer the question.\n",
    "      If the context doesn't contain the answer, say so.\n",
    "      \n",
    "      Context:\n",
    "      ${context}`,\n",
    "    messages: convertToCoreMessages(messages),\n",
    "  });\n",
    "\n",
    "  return result.toDataStreamResponse();\n",
    "}\n",
    "```\n",
    "\n",
    "### Multi-Step Agent Pattern\n",
    "\n",
    "```typescript\n",
    "// lib/ai/agent.ts\n",
    "import { generateText, tool } from 'ai';\n",
    "import { z } from 'zod';\n",
    "import { models } from './providers';\n",
    "\n",
    "export async function runResearchAgent(topic: string) {\n",
    "  const steps: string[] = [];\n",
    "  \n",
    "  const result = await generateText({\n",
    "    model: models.powerful,\n",
    "    tools: {\n",
    "      searchWeb: tool({\n",
    "        description: 'Search the web for current information',\n",
    "        parameters: z.object({ query: z.string() }),\n",
    "        execute: async ({ query }) => {\n",
    "          steps.push(`Searching web: ${query}`);\n",
    "          return await searchWeb(query);\n",
    "        },\n",
    "      }),\n",
    "      calculate: tool({\n",
    "        description: 'Perform calculations',\n",
    "        parameters: z.object({ expression: z.string() }),\n",
    "        execute: async ({ expression }) => {\n",
    "          steps.push(`Calculating: ${expression}`);\n",
    "          return eval(expression); // Use safe math library in production\n",
    "        },\n",
    "      }),\n",
    "      saveToDatabase: tool({\n",
    "        description: 'Save research results',\n",
    "        parameters: z.object({ \n",
    "          title: z.string(), \n",
    "          content: z.string(),\n",
    "          category: z.string() \n",
    "        }),\n",
    "        execute: async (data) => {\n",
    "          steps.push(`Saving to DB: ${data.title}`);\n",
    "          return await db.research.create({ data });\n",
    "        },\n",
    "      }),\n",
    "    },\n",
    "    system: `You are a research assistant. Given a topic:\n",
    "      1. Search for current information\n",
    "      2. Analyze and synthesize findings\n",
    "      3. Calculate any relevant metrics\n",
    "      4. Save the final report to the database`,\n",
    "    prompt: `Research the following topic and create a comprehensive report: ${topic}`,\n",
    "    maxSteps: 5, // Limit tool call iterations\n",
    "  });\n",
    "\n",
    "  return {\n",
    "    text: result.text,\n",
    "    steps,\n",
    "    toolCalls: result.toolCalls,\n",
    "    toolResults: result.toolResults,\n",
    "  };\n",
    "}\n",
    "```\n",
    "\n",
    "## 30.5 Cost Optimization and Monitoring\n",
    "\n",
    "Manage AI costs through caching, model selection, and usage tracking.\n",
    "\n",
    "### Intelligent Caching\n",
    "\n",
    "```typescript\n",
    "// lib/ai/cached-generation.ts\n",
    "import { generateText, GenerateTextResult } from 'ai';\n",
    "import { redisCache } from '@/lib/redis/cache';\n",
    "import { createHash } from 'crypto';\n",
    "\n",
    "interface CacheConfig {\n",
    "  ttl: number; // seconds\n",
    "  keyPrefix: string;\n",
    "}\n",
    "\n",
    "export async function cachedGenerate(\n",
    "  params: Parameters<typeof generateText>[0],\n",
    "  cacheConfig: CacheConfig\n",
    "): Promise<GenerateTextResult> {\n",
    "  // Create cache key from prompt and parameters\n",
    "  const keyData = JSON.stringify({\n",
    "    model: params.model,\n",
    "    prompt: params.prompt,\n",
    "    system: params.system,\n",
    "    temperature: params.temperature,\n",
    "  });\n",
    "  \n",
    "  const cacheKey = `${cacheConfig.keyPrefix}:${createHash('sha256').update(keyData).digest('hex')}`;\n",
    "\n",
    "  // Check cache\n",
    "  const cached = await redisCache.get<GenerateTextResult>(cacheKey);\n",
    "  if (cached) {\n",
    "    console.log('AI cache hit:', cacheKey);\n",
    "    return cached;\n",
    "  }\n",
    "\n",
    "  // Generate fresh\n",
    "  const result = await generateText(params);\n",
    "  \n",
    "  // Cache result (excluding raw response to save space)\n",
    "  const cacheable = {\n",
    "    text: result.text,\n",
    "    toolCalls: result.toolCalls,\n",
    "    toolResults: result.toolResults,\n",
    "    finishReason: result.finishReason,\n",
    "    usage: result.usage,\n",
    "  };\n",
    "  \n",
    "  await redisCache.set(cacheKey, cacheable, cacheConfig.ttl);\n",
    "  \n",
    "  return result;\n",
    "}\n",
    "\n",
    "// Usage with semantic caching (similar queries return cached result)\n",
    "export async function semanticCachedGenerate(\n",
    "  query: string,\n",
    "  params: Omit<Parameters<typeof generateText>[0], 'prompt'>\n",
    ") {\n",
    "  // Generate embedding for query\n",
    "  const { embedding } = await embed({\n",
    "    model: models.fast,\n",
    "    value: query,\n",
    "  });\n",
    "\n",
    "  // Search for similar cached queries\n",
    "  const similar = await findSimilarQueries(embedding);\n",
    "  if (similar && similar.similarity > 0.95) {\n",
    "    return similar.result;\n",
    "  }\n",
    "\n",
    "  // Generate and cache with embedding\n",
    "  const result = await generateText({ ...params, prompt: query });\n",
    "  await cacheWithEmbedding(query, embedding, result);\n",
    "  \n",
    "  return result;\n",
    "}\n",
    "```\n",
    "\n",
    "### Usage Tracking and Rate Limiting\n",
    "\n",
    "```typescript\n",
    "// lib/ai/usage-tracker.ts\n",
    "import { redis } from '@/lib/redis';\n",
    "\n",
    "interface UsageMetrics {\n",
    "  userId: string;\n",
    "  model: string;\n",
    "  promptTokens: number;\n",
    "  completionTokens: number;\n",
    "  totalTokens: number;\n",
    "  cost: number;\n",
    "  timestamp: Date;\n",
    "}\n",
    "\n",
    "export async function trackUsage(metrics: UsageMetrics) {\n",
    "  const pipeline = redis.pipeline();\n",
    "  \n",
    "  // Increment daily usage\n",
    "  const dayKey = `usage:daily:${new Date().toISOString().split('T')[0]}:${metrics.userId}`;\n",
    "  pipeline.hincrby(dayKey, 'requests', 1);\n",
    "  pipeline.hincrby(dayKey, 'tokens', metrics.totalTokens);\n",
    "  pipeline.expire(dayKey, 60 * 60 * 24 * 30); // Keep for 30 days\n",
    "  \n",
    "  // Track by model\n",
    "  pipeline.hincrby(`usage:models:${metrics.userId}`, metrics.model, metrics.totalTokens);\n",
    "  \n",
    "  // Track cost (approximate)\n",
    "  const cost = calculateCost(metrics.model, metrics.promptTokens, metrics.completionTokens);\n",
    "  pipeline.incrbyfloat(`usage:cost:${metrics.userId}`, cost);\n",
    "  \n",
    "  await pipeline.exec();\n",
    "}\n",
    "\n",
    "export async function checkUsageLimit(userId: string): Promise<{\n",
    "  allowed: boolean;\n",
    "  remaining: number;\n",
    "  resetTime: number;\n",
    "}> {\n",
    "  const today = new Date().toISOString().split('T')[0];\n",
    "  const usage = await redis.hgetall(`usage:daily:${today}:${userId}`);\n",
    "  \n",
    "  const requests = parseInt(usage.requests || '0');\n",
    "  const limit = 100; // Free tier: 100 requests/day\n",
    "  \n",
    "  return {\n",
    "    allowed: requests < limit,\n",
    "    remaining: Math.max(0, limit - requests),\n",
    "    resetTime: new Date(`${today}T23:59:59Z`).getTime(),\n",
    "  };\n",
    "}\n",
    "\n",
    "function calculateCost(model: string, prompt: number, completion: number): number {\n",
    "  const rates: Record<string, { prompt: number; completion: number }> = {\n",
    "    'gpt-4o': { prompt: 5 / 1000000, completion: 15 / 1000000 }, // $5/$15 per 1M tokens\n",
    "    'gpt-4o-mini': { prompt: 0.15 / 1000000, completion: 0.6 / 1000000 },\n",
    "    'claude-3-5-sonnet': { prompt: 3 / 1000000, completion: 15 / 1000000 },\n",
    "  };\n",
    "  \n",
    "  const rate = rates[model] || rates['gpt-4o'];\n",
    "  return (prompt * rate.prompt) + (completion * rate.completion);\n",
    "}\n",
    "\n",
    "// Middleware to check limits\n",
    "// app/api/chat/route.ts\n",
    "export async function POST(req: Request) {\n",
    "  const userId = await getUserId(req);\n",
    "  const limit = await checkUsageLimit(userId);\n",
    "  \n",
    "  if (!limit.allowed) {\n",
    "    return Response.json(\n",
    "      { error: 'Daily limit exceeded', resetTime: limit.resetTime },\n",
    "      { status: 429 }\n",
    "    );\n",
    "  }\n",
    "  \n",
    "  // ... proceed with generation\n",
    "}\n",
    "```\n",
    "\n",
    "## 30.6 Privacy and Security Considerations\n",
    "\n",
    "Handle sensitive data and ensure compliance when using AI services.\n",
    "\n",
    "### Data Sanitization\n",
    "\n",
    "```typescript\n",
    "// lib/ai/privacy.ts\n",
    "import { PiiDetector } from '@/lib/pii-detector';\n",
    "\n",
    "export function sanitizeInput(text: string): {\n",
    "  sanitized: string;\n",
    "  detected: string[];\n",
    "} {\n",
    "  const pii = new PiiDetector();\n",
    "  const detected: string[] = [];\n",
    "  \n",
    "  let sanitized = text\n",
    "    // Remove email addresses\n",
    "    .replace(/\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b/g, (match) => {\n",
    "      detected.push('email');\n",
    "      return '[EMAIL]';\n",
    "    })\n",
    "    // Remove phone numbers\n",
    "    .replace(/\\b\\d{3}[-.]?\\d{3}[-.]?\\d{4}\\b/g, (match) => {\n",
    "      detected.push('phone');\n",
    "      return '[PHONE]';\n",
    "    })\n",
    "    // Remove API keys\n",
    "    .replace(/\\b(sk-[a-zA-Z0-9]{48})\\b/g, (match) => {\n",
    "      detected.push('api_key');\n",
    "      return '[API_KEY]';\n",
    "    });\n",
    "  \n",
    "  return { sanitized, detected };\n",
    "}\n",
    "\n",
    "// app/api/chat/route.ts\n",
    "export async function POST(req: Request) {\n",
    "  const { messages } = await req.json();\n",
    "  \n",
    "  // Sanitize all user messages\n",
    "  const sanitizedMessages = messages.map((msg: any) => {\n",
    "    if (msg.role === 'user') {\n",
    "      const { sanitized, detected } = sanitizeInput(msg.content);\n",
    "      if (detected.length > 0) {\n",
    "        console.warn('PII detected:', detected);\n",
    "      }\n",
    "      return { ...msg, content: sanitized };\n",
    "    }\n",
    "    return msg;\n",
    "  });\n",
    "  \n",
    "  // Proceed with sanitized content\n",
    "  const result = await streamText({\n",
    "    // ... config\n",
    "    messages: sanitizedMessages,\n",
    "  });\n",
    "  \n",
    "  return result.toDataStreamResponse();\n",
    "}\n",
    "```\n",
    "\n",
    "### Local/Edge AI for Sensitive Data\n",
    "\n",
    "```typescript\n",
    "// lib/ai/edge-model.ts\n",
    "import { HfInference } from '@huggingface/inference';\n",
    "import { pipeline } from '@xenova/transformers';\n",
    "\n",
    "// Use smaller models at the edge for privacy-sensitive tasks\n",
    "export async function classifyTextLocal(text: string) {\n",
    "  // Runs entirely in the edge runtime, no data leaves the server\n",
    "  const classifier = await pipeline(\n",
    "    'text-classification',\n",
    "    'Xenova/distilbert-base-uncased-finetuned-sst-2-english'\n",
    "  );\n",
    "  \n",
    "  const result = await classifier(text);\n",
    "  return result;\n",
    "}\n",
    "\n",
    "// Route Handler with local inference\n",
    "// app/api/classify/route.ts\n",
    "export const runtime = 'edge';\n",
    "\n",
    "export async function POST(req: Request) {\n",
    "  const { text } = await req.json();\n",
    "  \n",
    "  // Process locally - no external API call\n",
    "  const classification = await classifyTextLocal(text);\n",
    "  \n",
    "  return Response.json(classification);\n",
    "}\n",
    "```\n",
    "\n",
    "## Key Takeaways from Chapter 30\n",
    "\n",
    "1. **AI SDK Configuration**: Use the Vercel AI SDK for unified provider access (OpenAI, Anthropic, Google) with consistent streaming interfaces. Configure multiple model tiers (fast/balanced/powerful) to optimize for latency vs. quality.\n",
    "\n",
    "2. **Streaming Architecture**: Implement `streamText` for real-time responses and `createStreamableValue` for Server Actions. Use `readStreamableValue` on the client to consume streams incrementally, updating UI as tokens arrive.\n",
    "\n",
    "3. **Tool Augmentation**: Define tools with Zod schemas to allow AI to call functions, retrieve documentation, or query databases. Use `maxSteps` to limit recursion and prevent runaway tool calling.\n",
    "\n",
    "4. **RAG Implementation**: Generate embeddings for documents and queries using `embed()` and `embedMany()`. Use vector similarity (cosine distance) to retrieve relevant context before generation, reducing hallucinations.\n",
    "\n",
    "5. **Cost Management**: Cache frequent queries using semantic similarity or exact hash matching. Track token usage per user with Redis, implementing rate limits and tiered access. Use cheaper models (GPT-4o-mini) for simple tasks, reserving expensive models for complex reasoning.\n",
    "\n",
    "6. **Privacy Protection**: Sanitize PII (emails, phone numbers, API keys) before sending to external AI providers. Use local models via Transformers.js or Edge runtime for sensitive data processing. Implement data retention policies and avoid logging full prompts containing personal information.\n",
    "\n",
    "## Coming Up Next\n",
    "\n",
    "**Chapter 31: Internationalization (i18n)**\n",
    "\n",
    "With AI capabilities integrated into your application, it's time to make your content accessible globally. In Chapter 31, we'll explore setting up internationalization with Next.js i18n routing, locale detection strategies, content localization workflows, RTL (Right-to-Left) language support, and SEO optimization for multi-language sites. You'll learn how to structure your application for global audiences while maintaining performance and type safety across different locales."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='29. progressive_web_applications.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../6. Specialized_topics/31. internationalization.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}