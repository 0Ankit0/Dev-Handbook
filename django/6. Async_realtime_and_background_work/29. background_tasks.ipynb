{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ff6b46ca",
   "metadata": {},
   "source": [
    "# Part VI \u2014 Async, Realtime, and Background Work  \n",
    "## 29. Background Tasks (Celery / RQ Concepts) \u2014 Production-Safe Jobs, Retries, Scheduling, Idempotency\n",
    "\n",
    "Background work is how professional Django systems stay responsive and reliable.\n",
    "\n",
    "You *do not* want to:\n",
    "- send emails inside request/response in production\n",
    "- generate large CSV exports while the user waits\n",
    "- call slow external APIs inside a web request\n",
    "- run scheduled jobs (cleanup, reminders) manually\n",
    "\n",
    "Instead you use a **task queue**:\n",
    "- the web process enqueues a job quickly\n",
    "- a worker process executes it asynchronously\n",
    "- failures are retried\n",
    "- progress/status is tracked\n",
    "\n",
    "This chapter will:\n",
    "1) teach the concepts (so you can reason about any queue system), and  \n",
    "2) implement a real setup with **Celery + Redis** (most common in Django industry), and  \n",
    "3) show a lightweight alternative using **RQ** (simpler, fewer features).\n",
    "\n",
    "---\n",
    "\n",
    "## 29.0 Learning Outcomes\n",
    "\n",
    "By the end, you should be able to:\n",
    "\n",
    "- Explain the task queue architecture: **producer \u2192 broker \u2192 worker \u2192 result store**.\n",
    "- Decide when to use background tasks vs async views vs cron.\n",
    "- Set up **Celery + Redis** with Django correctly.\n",
    "- Write tasks with:\n",
    "  - retries + exponential backoff\n",
    "  - time limits\n",
    "  - idempotency (no double-sends / duplicate exports)\n",
    "- Use `transaction.on_commit()` so tasks only run after DB commit.\n",
    "- Build a real workflow:\n",
    "  - \u201cexport tasks to CSV\u201d as a background job with progress and download link\n",
    "  - send realtime notification via Channels when job completes\n",
    "- Test tasks reliably in CI (eager mode).\n",
    "- Operate workers in production (separate process, monitoring, safe config).\n",
    "\n",
    "---\n",
    "\n",
    "## 29.1 Background Jobs: What They Are (and What They Are Not)\n",
    "\n",
    "### 29.1.1 Background tasks are for \u201cslow or unreliable work\u201d\n",
    "Good candidates:\n",
    "- sending emails (SMTP/provider can be slow)\n",
    "- generating reports/exports\n",
    "- resizing images / video processing\n",
    "- syncing with third-party APIs\n",
    "- periodic reminders and cleanup tasks\n",
    "- webhooks processing with retries\n",
    "\n",
    "Bad candidates:\n",
    "- tasks that must finish before you can respond safely to user (then it\u2019s not\n",
    "  background; it\u2019s part of the request)\n",
    "- CPU-heavy work on the same box without limits (it can starve workers)\n",
    "\n",
    "### 29.1.2 Background tasks are not async views\n",
    "- Async views help handle concurrent I/O **within a request**.\n",
    "- Background tasks move work **out of the request**, so the request stays fast.\n",
    "\n",
    "Often you use both:\n",
    "- request enqueues task quickly\n",
    "- task does the slow work later (and may itself do async I/O if you choose)\n",
    "\n",
    "---\n",
    "\n",
    "## 29.2 Task Queue Architecture (Industry Mental Model)\n",
    "\n",
    "A typical Celery architecture:\n",
    "\n",
    "```text\n",
    "Django web process\n",
    "  |\n",
    "  |  enqueue task (message)\n",
    "  v\n",
    "Broker (Redis / RabbitMQ)\n",
    "  |\n",
    "  |  workers pull messages\n",
    "  v\n",
    "Celery workers (separate processes)\n",
    "  |\n",
    "  |  do work, possibly store results/status\n",
    "  v\n",
    "Result backend / DB / cache / files\n",
    "```\n",
    "\n",
    "Key components:\n",
    "\n",
    "- **Broker**: a queue system (Redis or RabbitMQ).\n",
    "- **Worker**: process that executes tasks.\n",
    "- **Result backend**: optional; store task return values (often not needed if you\n",
    "  store status in your DB models).\n",
    "\n",
    "**Industry note:** Many teams use Redis as broker; RabbitMQ is also common and can\n",
    "be better for certain patterns. Start with Redis for simplicity.\n",
    "\n",
    "---\n",
    "\n",
    "## 29.3 Celery Setup (Django + Redis) \u2014 Step-by-Step\n",
    "\n",
    "We\u2019ll implement the \u201cstandard Celery layout\u201d used in many Django projects.\n",
    "\n",
    "### 29.3.1 Install dependencies\n",
    "\n",
    "Add to `requirements.txt` (runtime deps):\n",
    "\n",
    "```text\n",
    "celery\n",
    "redis\n",
    "```\n",
    "\n",
    "Install:\n",
    "\n",
    "```bash\n",
    "python -m pip install -r requirements.txt\n",
    "python -m pip freeze > requirements.txt\n",
    "```\n",
    "\n",
    "You also need Redis running.\n",
    "\n",
    "### 29.3.2 Run Redis locally (Docker)\n",
    "\n",
    "```bash\n",
    "docker run --rm -p 6379:6379 redis:7-alpine\n",
    "```\n",
    "\n",
    "Leave it running.\n",
    "\n",
    "### 29.3.3 Add Celery config file: `config/celery.py`\n",
    "\n",
    "Create `config/celery.py`:\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "\n",
    "from celery import Celery\n",
    "\n",
    "os.environ.setdefault(\"DJANGO_SETTINGS_MODULE\", \"config.settings\")\n",
    "\n",
    "app = Celery(\"config\")\n",
    "\n",
    "# Loads any CELERY_* settings from Django settings.py\n",
    "app.config_from_object(\"django.conf:settings\", namespace=\"CELERY\")\n",
    "\n",
    "# Auto-discover tasks.py in installed apps\n",
    "app.autodiscover_tasks()\n",
    "```\n",
    "\n",
    "### 29.3.4 Ensure Celery app loads when Django starts\n",
    "\n",
    "Edit `config/__init__.py`:\n",
    "\n",
    "```python\n",
    "from .celery import app as celery_app\n",
    "\n",
    "__all__ = [\"celery_app\"]\n",
    "```\n",
    "\n",
    "Why this matters:\n",
    "- In some deployment patterns, Celery expects the app to be discoverable.\n",
    "- This is the conventional integration.\n",
    "\n",
    "### 29.3.5 Add Celery settings in Django settings\n",
    "\n",
    "In `config/settings.py` (or `prod.py`/`dev.py`), add:\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "CELERY_BROKER_URL = os.environ.get(\"CELERY_BROKER_URL\", \"redis://127.0.0.1:6379/0\")\n",
    "\n",
    "# Optional: store task results (often you can skip this and store results in DB)\n",
    "CELERY_RESULT_BACKEND = os.environ.get(\n",
    "    \"CELERY_RESULT_BACKEND\",\n",
    "    \"redis://127.0.0.1:6379/1\",\n",
    ")\n",
    "\n",
    "CELERY_ACCEPT_CONTENT = [\"json\"]\n",
    "CELERY_TASK_SERIALIZER = \"json\"\n",
    "CELERY_RESULT_SERIALIZER = \"json\"\n",
    "\n",
    "CELERY_TIMEZONE = \"UTC\"\n",
    "\n",
    "# Recommended safety defaults\n",
    "CELERY_TASK_TRACK_STARTED = True\n",
    "CELERY_TASK_TIME_LIMIT = 60 * 10      # hard timeout 10 min\n",
    "CELERY_TASK_SOFT_TIME_LIMIT = 60 * 9  # soft timeout 9 min\n",
    "```\n",
    "\n",
    "#### Why JSON-only serialization is an industry standard\n",
    "Celery supports pickle, but pickle is risky:\n",
    "- it can deserialize arbitrary Python objects (security risk)\n",
    "- JSON is safer and more portable\n",
    "\n",
    "---\n",
    "\n",
    "## 29.4 Run Celery Worker (Local Development)\n",
    "\n",
    "In a separate terminal (with venv active):\n",
    "\n",
    "```bash\n",
    "celery -A config worker -l info\n",
    "```\n",
    "\n",
    "If you want scheduled tasks later, you\u2019ll also run Celery Beat:\n",
    "\n",
    "```bash\n",
    "celery -A config beat -l info\n",
    "```\n",
    "\n",
    "**Important operational rule:** Django web process and Celery worker are separate\n",
    "processes. If your worker isn\u2019t running, tasks won\u2019t execute.\n",
    "\n",
    "---\n",
    "\n",
    "## 29.5 Your First Task (Prove Wiring Works)\n",
    "\n",
    "Create `pages/tasks.py`:\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import time\n",
    "\n",
    "from celery import shared_task\n",
    "\n",
    "\n",
    "@shared_task\n",
    "def ping_task() -> dict:\n",
    "    time.sleep(0.2)\n",
    "    return {\"status\": \"ok\"}\n",
    "```\n",
    "\n",
    "### 29.5.1 Call it from Django shell\n",
    "\n",
    "```bash\n",
    "python manage.py shell\n",
    "```\n",
    "\n",
    "```python\n",
    "from pages.tasks import ping_task\n",
    "\n",
    "result = ping_task.delay()\n",
    "result.id\n",
    "result.get(timeout=5)\n",
    "```\n",
    "\n",
    "Expected:\n",
    "- the worker logs execution\n",
    "- `.get()` returns `{\"status\": \"ok\"}`\n",
    "\n",
    "> In production, you often avoid calling `.get()` in web requests (it defeats the\n",
    "> purpose of background work).\n",
    "\n",
    "---\n",
    "\n",
    "## 29.6 The Most Important Celery Practices (Non-Negotiable in Real Systems)\n",
    "\n",
    "### 29.6.1 Use `transaction.on_commit()` for tasks triggered by DB writes\n",
    "\n",
    "**Problem:** If you enqueue a task before the DB transaction commits, the worker may\n",
    "run immediately and not find the DB row yet (or read stale data).\n",
    "\n",
    "**Solution:** enqueue after commit:\n",
    "\n",
    "```python\n",
    "from django.db import transaction\n",
    "\n",
    "transaction.on_commit(lambda: my_task.delay(obj_id))\n",
    "```\n",
    "\n",
    "This is one of the most important \u201cprofessional Celery + Django\u201d patterns.\n",
    "\n",
    "### 29.6.2 Make tasks idempotent (assume at-least-once delivery)\n",
    "Task queues can deliver the same message more than once (worker restart, retry,\n",
    "broker behavior). Your task should be safe if executed twice.\n",
    "\n",
    "Common idempotency strategies:\n",
    "- write a DB row with a unique constraint (e.g., \u201cpublished_email_sent\u201d)\n",
    "- check and return if already done\n",
    "- use unique keys for exports/events\n",
    "\n",
    "### 29.6.3 Retry only what is retryable\n",
    "Don\u2019t retry validation errors. Retry:\n",
    "- network timeouts\n",
    "- temporary provider failures\n",
    "- transient DB connection errors\n",
    "\n",
    "### 29.6.4 Keep tasks small and composable\n",
    "Prefer:\n",
    "- \u201csend one email\u201d\n",
    "- \u201cgenerate one export job\u201d\n",
    "over:\n",
    "- \u201cdo everything for 10 minutes in one task\u201d\n",
    "\n",
    "---\n",
    "\n",
    "## 29.7 Refactor \u201cArticle Published Email\u201d to Background Task (Real Upgrade)\n",
    "\n",
    "You currently send \u201cpublished\u201d emails from your service layer. That\u2019s fine for\n",
    "learning but not ideal for production. We\u2019ll enqueue a task instead.\n",
    "\n",
    "### 29.7.1 Create a task: `articles/tasks.py`\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "\n",
    "from celery import shared_task\n",
    "from django.db import transaction\n",
    "\n",
    "from articles.models import Article\n",
    "from articles.services_email import send_article_published_email\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@shared_task(\n",
    "    bind=True,\n",
    "    autoretry_for=(Exception,),\n",
    "    retry_backoff=True,\n",
    "    retry_jitter=True,\n",
    "    retry_kwargs={\"max_retries\": 5},\n",
    ")\n",
    "def send_article_published_email_task(self, article_id: int) -> None:\n",
    "    \"\"\"\n",
    "    Background email sender with retries.\n",
    "    Must be idempotent at the business rule layer (see next section).\n",
    "    \"\"\"\n",
    "    try:\n",
    "        article = Article.objects.select_related(\"author\").get(id=article_id)\n",
    "    except Article.DoesNotExist:\n",
    "        logger.warning(\"Article not found for email task article_id=%s\", article_id)\n",
    "        return\n",
    "\n",
    "    # Policy: only send for published articles.\n",
    "    if article.status != Article.Status.PUBLISHED:\n",
    "        logger.info(\n",
    "            \"Skipping published email; article not published id=%s status=%s\",\n",
    "            article.id,\n",
    "            article.status,\n",
    "        )\n",
    "        return\n",
    "\n",
    "    send_article_published_email(article=article)\n",
    "```\n",
    "\n",
    "#### Explanation of Celery retry options\n",
    "- `bind=True` gives you `self` (task instance).\n",
    "- `autoretry_for=(Exception,)` retries on exceptions.\n",
    "- `retry_backoff=True` uses exponential backoff.\n",
    "- `retry_jitter=True` adds randomness (prevents retry storms).\n",
    "- `max_retries=5` prevents infinite retries.\n",
    "\n",
    "In production you should be more selective than `(Exception,)`, but this is a good\n",
    "workbook starting point.\n",
    "\n",
    "### 29.7.2 Add idempotency so \u201cpublished email\u201d is not sent twice\n",
    "\n",
    "Create a model to record sending. Add to `articles/models.py`:\n",
    "\n",
    "```python\n",
    "from django.db import models\n",
    "\n",
    "\n",
    "class ArticleNotification(models.Model):\n",
    "    class Kind(models.TextChoices):\n",
    "        PUBLISHED_EMAIL = \"published_email\", \"Published email\"\n",
    "\n",
    "    article = models.ForeignKey(\n",
    "        \"articles.Article\",\n",
    "        on_delete=models.CASCADE,\n",
    "        related_name=\"notifications\",\n",
    "    )\n",
    "    kind = models.CharField(max_length=50, choices=Kind.choices)\n",
    "    created_at = models.DateTimeField(auto_now_add=True)\n",
    "\n",
    "    class Meta:\n",
    "        constraints = [\n",
    "            models.UniqueConstraint(\n",
    "                fields=[\"article\", \"kind\"],\n",
    "                name=\"unique_article_notification_kind\",\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"{self.kind} for article {self.article_id}\"\n",
    "```\n",
    "\n",
    "Migrate:\n",
    "\n",
    "```bash\n",
    "python manage.py makemigrations\n",
    "python manage.py migrate\n",
    "```\n",
    "\n",
    "Now update the task to be idempotent:\n",
    "\n",
    "```python\n",
    "from django.db import IntegrityError\n",
    "\n",
    "from articles.models import Article, ArticleNotification\n",
    "\n",
    "@shared_task(...)\n",
    "def send_article_published_email_task(self, article_id: int) -> None:\n",
    "    ...\n",
    "    try:\n",
    "        ArticleNotification.objects.create(\n",
    "            article=article,\n",
    "            kind=ArticleNotification.Kind.PUBLISHED_EMAIL,\n",
    "        )\n",
    "    except IntegrityError:\n",
    "        logger.info(\"Published email already sent for article_id=%s\", article.id)\n",
    "        return\n",
    "\n",
    "    send_article_published_email(article=article)\n",
    "```\n",
    "\n",
    "#### Why this is the \u201ccorrect\u201d pattern\n",
    "Even if:\n",
    "- the task retries\n",
    "- the broker delivers twice\n",
    "- you accidentally enqueue twice\n",
    "\n",
    "\u2026only one email is sent because the DB enforces uniqueness.\n",
    "\n",
    "### 29.7.3 Enqueue the task after commit in your publish workflow\n",
    "\n",
    "Wherever you detect \u201cpublished_now\u201d (in your article service), do:\n",
    "\n",
    "```python\n",
    "from django.db import transaction\n",
    "from articles.tasks import send_article_published_email_task\n",
    "\n",
    "if published_now:\n",
    "    transaction.on_commit(\n",
    "        lambda: send_article_published_email_task.delay(article.id)\n",
    "    )\n",
    "```\n",
    "\n",
    "This ensures:\n",
    "- article row exists and committed before task runs\n",
    "- \u201cpublished email\u201d logic is not tied to request latency\n",
    "\n",
    "---\n",
    "\n",
    "## 29.8 Background Export Job (Tasks CSV) \u2014 Full Production Pattern\n",
    "\n",
    "Your current CSV export is synchronous: user clicks export, server builds CSV in the\n",
    "request. That\u2019s okay for small data but not at scale.\n",
    "\n",
    "We\u2019ll build:\n",
    "\n",
    "- a DB model tracking export jobs\n",
    "- an endpoint to start export (returns 202 Accepted + job ID)\n",
    "- a Celery task that generates file in background\n",
    "- a download endpoint for completed jobs\n",
    "- an optional realtime notification via Channels when complete\n",
    "\n",
    "### 29.8.1 Create an ExportJob model\n",
    "\n",
    "Create `tasks/models_exports.py` (or in `tasks/models.py` if you prefer one file).\n",
    "To keep it clear, we\u2019ll put it in `tasks/models.py` for now.\n",
    "\n",
    "Add to `tasks/models.py`:\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "from django.conf import settings\n",
    "from django.db import models\n",
    "\n",
    "from orgs.models import Organization\n",
    "\n",
    "\n",
    "class TaskExportJob(models.Model):\n",
    "    class Status(models.TextChoices):\n",
    "        PENDING = \"pending\", \"Pending\"\n",
    "        RUNNING = \"running\", \"Running\"\n",
    "        DONE = \"done\", \"Done\"\n",
    "        FAILED = \"failed\", \"Failed\"\n",
    "\n",
    "    organization = models.ForeignKey(\n",
    "        Organization,\n",
    "        on_delete=models.CASCADE,\n",
    "        related_name=\"task_exports\",\n",
    "    )\n",
    "    created_by = models.ForeignKey(\n",
    "        settings.AUTH_USER_MODEL,\n",
    "        on_delete=models.PROTECT,\n",
    "        related_name=\"task_exports\",\n",
    "    )\n",
    "\n",
    "    # Store filters used for export so the result is reproducible/auditable.\n",
    "    filters = models.JSONField(default=dict, blank=True)\n",
    "\n",
    "    status = models.CharField(\n",
    "        max_length=20,\n",
    "        choices=Status.choices,\n",
    "        default=Status.PENDING,\n",
    "    )\n",
    "    error = models.TextField(blank=True)\n",
    "\n",
    "    file = models.FileField(\n",
    "        upload_to=\"task_exports/\",\n",
    "        null=True,\n",
    "        blank=True,\n",
    "    )\n",
    "\n",
    "    created_at = models.DateTimeField(auto_now_add=True)\n",
    "    started_at = models.DateTimeField(null=True, blank=True)\n",
    "    finished_at = models.DateTimeField(null=True, blank=True)\n",
    "\n",
    "    class Meta:\n",
    "        ordering = [\"-created_at\"]\n",
    "        indexes = [\n",
    "            models.Index(fields=[\"organization\", \"status\", \"-created_at\"]),\n",
    "        ]\n",
    "\n",
    "    def __str__(self) -> str:\n",
    "        return f\"Export {self.id} for org {self.organization_id} ({self.status})\"\n",
    "```\n",
    "\n",
    "Migrate:\n",
    "\n",
    "```bash\n",
    "python manage.py makemigrations\n",
    "python manage.py migrate\n",
    "```\n",
    "\n",
    "### 29.8.2 Start export view (HTTP 202 + job id)\n",
    "\n",
    "Create `tasks/views_exports.py`:\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "from django.contrib.auth.decorators import login_required\n",
    "from django.core.exceptions import PermissionDenied\n",
    "from django.http import JsonResponse\n",
    "from django.urls import reverse\n",
    "from django.views.decorators.http import require_POST\n",
    "\n",
    "from orgs.services import get_membership, get_org_for_user_or_404\n",
    "from tasks.permissions import can_export_tasks\n",
    "from tasks.models import TaskExportJob\n",
    "from tasks.tasks_exports import run_task_export_job_task\n",
    "\n",
    "\n",
    "@login_required\n",
    "@require_POST\n",
    "def task_export_start(request, org_slug: str):\n",
    "    org = get_org_for_user_or_404(user=request.user, org_slug=org_slug)\n",
    "    membership = get_membership(user=request.user, organization=org)\n",
    "\n",
    "    if not can_export_tasks(membership=membership):\n",
    "        raise PermissionDenied\n",
    "\n",
    "    # Capture filter params from request.GET or request.POST as you prefer.\n",
    "    # For exports, GET query params are common even if the start endpoint is POST.\n",
    "    filters = {\n",
    "        \"q\": request.GET.get(\"q\", \"\"),\n",
    "        \"status\": request.GET.get(\"status\", \"\"),\n",
    "        \"priority\": request.GET.get(\"priority\", \"\"),\n",
    "        \"assigned_to\": request.GET.get(\"assigned_to\", \"\"),\n",
    "    }\n",
    "\n",
    "    job = TaskExportJob.objects.create(\n",
    "        organization=org,\n",
    "        created_by=request.user,\n",
    "        filters=filters,\n",
    "        status=TaskExportJob.Status.PENDING,\n",
    "    )\n",
    "\n",
    "    # Enqueue background job after commit (best practice).\n",
    "    from django.db import transaction\n",
    "\n",
    "    transaction.on_commit(lambda: run_task_export_job_task.delay(job.id))\n",
    "\n",
    "    return JsonResponse(\n",
    "        {\n",
    "            \"status\": \"accepted\",\n",
    "            \"job_id\": job.id,\n",
    "            \"job_status_url\": reverse(\n",
    "                \"tasks:export_status\",\n",
    "                kwargs={\"org_slug\": org.slug, \"job_id\": job.id},\n",
    "            ),\n",
    "            \"job_download_url\": reverse(\n",
    "                \"tasks:export_download\",\n",
    "                kwargs={\"org_slug\": org.slug, \"job_id\": job.id},\n",
    "            ),\n",
    "        },\n",
    "        status=202,\n",
    "    )\n",
    "```\n",
    "\n",
    "### 29.8.3 Status and download endpoints\n",
    "\n",
    "Add to `tasks/views_exports.py`:\n",
    "\n",
    "```python\n",
    "from django.http import FileResponse, Http404\n",
    "\n",
    "@login_required\n",
    "def task_export_status(request, org_slug: str, job_id: int):\n",
    "    org = get_org_for_user_or_404(user=request.user, org_slug=org_slug)\n",
    "    job = TaskExportJob.objects.filter(organization=org, id=job_id).first()\n",
    "    if job is None:\n",
    "        raise Http404\n",
    "\n",
    "    # Authorization policy: only org admins can see exports,\n",
    "    # or allow the creator to see their own export jobs.\n",
    "    membership = get_membership(user=request.user, organization=org)\n",
    "    if not can_export_tasks(membership=membership) and job.created_by_id != request.user.id:\n",
    "        raise PermissionDenied\n",
    "\n",
    "    return JsonResponse(\n",
    "        {\n",
    "            \"job_id\": job.id,\n",
    "            \"status\": job.status,\n",
    "            \"error\": job.error,\n",
    "            \"created_at\": job.created_at.isoformat(),\n",
    "            \"started_at\": job.started_at.isoformat() if job.started_at else None,\n",
    "            \"finished_at\": job.finished_at.isoformat() if job.finished_at else None,\n",
    "            \"has_file\": bool(job.file),\n",
    "        }\n",
    "    )\n",
    "\n",
    "\n",
    "@login_required\n",
    "def task_export_download(request, org_slug: str, job_id: int):\n",
    "    org = get_org_for_user_or_404(user=request.user, org_slug=org_slug)\n",
    "    job = TaskExportJob.objects.filter(organization=org, id=job_id).first()\n",
    "    if job is None:\n",
    "        raise Http404\n",
    "\n",
    "    membership = get_membership(user=request.user, organization=org)\n",
    "    if not can_export_tasks(membership=membership) and job.created_by_id != request.user.id:\n",
    "        raise PermissionDenied\n",
    "\n",
    "    if job.status != TaskExportJob.Status.DONE or not job.file:\n",
    "        raise Http404(\"Export not ready.\")\n",
    "\n",
    "    # FileResponse streams file; better than reading into memory.\n",
    "    return FileResponse(\n",
    "        job.file.open(\"rb\"),\n",
    "        as_attachment=True,\n",
    "        filename=f\"{org.slug}-tasks-export-{job.id}.csv\",\n",
    "    )\n",
    "```\n",
    "\n",
    "### 29.8.4 Wire export URLs\n",
    "\n",
    "In `tasks/urls.py` add:\n",
    "\n",
    "```python\n",
    "from tasks import views_exports\n",
    "\n",
    "urlpatterns += [\n",
    "    path(\"exports/start/\", views_exports.task_export_start, name=\"export_start\"),\n",
    "    path(\"exports/<int:job_id>/\", views_exports.task_export_status, name=\"export_status\"),\n",
    "    path(\"exports/<int:job_id>/download/\", views_exports.task_export_download, name=\"export_download\"),\n",
    "]\n",
    "```\n",
    "\n",
    "Now your org-scoped export endpoints exist under:\n",
    "\n",
    "- `/orgs/<org_slug>/tasks/exports/start/?status=open...`\n",
    "\n",
    "(Your `orgs/urls.py` already scopes tasks URLs under `/orgs/<org_slug>/tasks/`.)\n",
    "\n",
    "---\n",
    "\n",
    "## 29.9 The Export Worker Task (Generate CSV in Background)\n",
    "\n",
    "Create `tasks/tasks_exports.py`:\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import csv\n",
    "import io\n",
    "import logging\n",
    "\n",
    "from celery import shared_task\n",
    "from django.core.files.base import ContentFile\n",
    "from django.utils import timezone\n",
    "\n",
    "from orgs.models import Membership\n",
    "from tasks.models import Task, TaskExportJob\n",
    "from tasks.selectors import filter_tasks, task_qs_for_org\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@shared_task(\n",
    "    bind=True,\n",
    "    autoretry_for=(Exception,),\n",
    "    retry_backoff=True,\n",
    "    retry_jitter=True,\n",
    "    retry_kwargs={\"max_retries\": 3},\n",
    ")\n",
    "def run_task_export_job_task(self, job_id: int) -> None:\n",
    "    job = TaskExportJob.objects.select_related(\"organization\", \"created_by\").get(\n",
    "        id=job_id\n",
    "    )\n",
    "\n",
    "    if job.status in {TaskExportJob.Status.DONE, TaskExportJob.Status.RUNNING}:\n",
    "        # Idempotency: do nothing if already done/running.\n",
    "        return\n",
    "\n",
    "    job.status = TaskExportJob.Status.RUNNING\n",
    "    job.started_at = timezone.now()\n",
    "    job.error = \"\"\n",
    "    job.save(update_fields=[\"status\", \"started_at\", \"error\"])\n",
    "\n",
    "    try:\n",
    "        org = job.organization\n",
    "        filters = job.filters or {}\n",
    "\n",
    "        qs = task_qs_for_org(organization=org).order_by(\"-created_at\")\n",
    "        qs = filter_tasks(\n",
    "            qs=qs,\n",
    "            q=filters.get(\"q\") or \"\",\n",
    "            status=filters.get(\"status\") or None,\n",
    "            priority=int(filters[\"priority\"])\n",
    "            if str(filters.get(\"priority\") or \"\").isdigit()\n",
    "            else None,\n",
    "            assigned_to=filters.get(\"assigned_to\") or None,\n",
    "            actor=job.created_by,\n",
    "        )\n",
    "\n",
    "        # Write CSV to memory (fine for moderate sizes).\n",
    "        # For huge exports, use StreamingHttpResponse or chunked temp files.\n",
    "        buffer = io.StringIO()\n",
    "        writer = csv.writer(buffer)\n",
    "        writer.writerow(\n",
    "            [\"id\", \"title\", \"status\", \"priority\", \"assigned_to\", \"created_at\"]\n",
    "        )\n",
    "\n",
    "        for t in qs.iterator(chunk_size=2000):\n",
    "            writer.writerow(\n",
    "                [\n",
    "                    t.id,\n",
    "                    t.title,\n",
    "                    t.status,\n",
    "                    t.priority,\n",
    "                    t.assigned_to.username if t.assigned_to else \"\",\n",
    "                    t.created_at.isoformat(),\n",
    "                ]\n",
    "            )\n",
    "\n",
    "        content = buffer.getvalue().encode(\"utf-8\")\n",
    "        buffer.close()\n",
    "\n",
    "        job.file.save(\n",
    "            f\"{org.slug}-tasks-export-{job.id}.csv\",\n",
    "            ContentFile(content),\n",
    "            save=False,\n",
    "        )\n",
    "        job.status = TaskExportJob.Status.DONE\n",
    "        job.finished_at = timezone.now()\n",
    "        job.save(update_fields=[\"file\", \"status\", \"finished_at\"])\n",
    "\n",
    "        # Optional: realtime notification via Channels (if you built Chapter 28)\n",
    "        try:\n",
    "            from realtime.broadcast import broadcast_org_event\n",
    "\n",
    "            broadcast_org_event(\n",
    "                org_id=org.id,\n",
    "                payload={\n",
    "                    \"type\": \"export_done\",\n",
    "                    \"job_id\": job.id,\n",
    "                    \"download_path\": f\"/orgs/{org.slug}/tasks/exports/{job.id}/download/\",\n",
    "                },\n",
    "            )\n",
    "        except Exception:\n",
    "            logger.exception(\"Failed broadcasting export_done job_id=%s\", job.id)\n",
    "\n",
    "    except Exception as e:\n",
    "        job.status = TaskExportJob.Status.FAILED\n",
    "        job.finished_at = timezone.now()\n",
    "        job.error = str(e)\n",
    "        job.save(update_fields=[\"status\", \"finished_at\", \"error\"])\n",
    "        raise\n",
    "```\n",
    "\n",
    "### Critical explanations (why it\u2019s written this way)\n",
    "\n",
    "- **Idempotency**:\n",
    "  - If task runs twice, it checks status and avoids double work.\n",
    "- **Iterator**:\n",
    "  - `.iterator(chunk_size=2000)` avoids loading all rows into memory.\n",
    "- **DB status tracking**:\n",
    "  - Web UI can poll `/export_status` endpoint.\n",
    "- **FileField storage**:\n",
    "  - Even in dev, you store exports under `MEDIA_ROOT/task_exports/`.\n",
    "  - In production you might store in S3 and give signed URL downloads.\n",
    "- **Retries**:\n",
    "  - Export job will retry on transient failures up to 3 times.\n",
    "  - If the task fails consistently, job status becomes FAILED and error is stored.\n",
    "\n",
    "---\n",
    "\n",
    "## 29.10 Scheduled Tasks (Celery Beat) \u2014 Daily Cleanup and Reminders\n",
    "\n",
    "Celery Beat is a scheduler that enqueues tasks at intervals (like cron, but in the\n",
    "Celery ecosystem).\n",
    "\n",
    "### 29.10.1 Example periodic task: remind about due tasks daily\n",
    "\n",
    "Create `tasks/tasks_periodic.py`:\n",
    "\n",
    "```python\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from datetime import date, timedelta\n",
    "\n",
    "from celery import shared_task\n",
    "from django.utils import timezone\n",
    "\n",
    "from tasks.models import Task\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@shared_task\n",
    "def daily_due_tasks_reminder() -> None:\n",
    "    today = timezone.localdate()\n",
    "    tomorrow = today + timedelta(days=1)\n",
    "\n",
    "    qs = Task.objects.filter(due_date__in=[today, tomorrow]).select_related(\n",
    "        \"organization\",\n",
    "        \"assigned_to\",\n",
    "    )\n",
    "\n",
    "    count = qs.count()\n",
    "    logger.info(\"daily_due_tasks_reminder due_count=%s\", count)\n",
    "\n",
    "    # In a real app:\n",
    "    # - group by assigned_to\n",
    "    # - send one email per user\n",
    "    # - respect notification preferences\n",
    "    # - be idempotent per day per user\n",
    "```\n",
    "\n",
    "### 29.10.2 Schedule it via settings (simple baseline)\n",
    "In settings:\n",
    "\n",
    "```python\n",
    "from celery.schedules import crontab\n",
    "\n",
    "CELERY_BEAT_SCHEDULE = {\n",
    "    \"daily-due-tasks-reminder\": {\n",
    "        \"task\": \"tasks.tasks_periodic.daily_due_tasks_reminder\",\n",
    "        \"schedule\": crontab(hour=9, minute=0),\n",
    "    },\n",
    "}\n",
    "```\n",
    "\n",
    "Run beat:\n",
    "\n",
    "```bash\n",
    "celery -A config beat -l info\n",
    "```\n",
    "\n",
    "#### Production note: \u201csettings-based beat schedule\u201d vs DB-based schedules\n",
    "- Settings-based is simple but requires deploy to change schedule.\n",
    "- Many teams use `django-celery-beat` to manage schedules in the database via admin.\n",
    "Use DB-based scheduling when:\n",
    "- ops team wants to change schedules without deployments\n",
    "- you have multiple dynamic periodic jobs\n",
    "\n",
    "---\n",
    "\n",
    "## 29.11 Testing Celery Tasks (Deterministic CI)\n",
    "\n",
    "### 29.11.1 Eager mode (run tasks inline during tests)\n",
    "In test settings (or `settings.py` guarded by env var), use:\n",
    "\n",
    "```python\n",
    "CELERY_TASK_ALWAYS_EAGER = True\n",
    "CELERY_TASK_EAGER_PROPAGATES = True\n",
    "```\n",
    "\n",
    "Meaning:\n",
    "- `.delay()` runs immediately in the same process\n",
    "- exceptions propagate (so your tests fail correctly)\n",
    "\n",
    "### 29.11.2 Test that publish workflow enqueues task (or sends email once)\n",
    "If you use eager mode, `.delay()` executes and you can assert email outbox length.\n",
    "\n",
    "Example (Django TestCase):\n",
    "\n",
    "```python\n",
    "from django.core import mail\n",
    "from django.test import TestCase, override_settings\n",
    "from django.utils import timezone\n",
    "\n",
    "from articles.models import Article\n",
    "from tests.factories import ArticleFactory, UserFactory\n",
    "\n",
    "\n",
    "@override_settings(CELERY_TASK_ALWAYS_EAGER=True, CELERY_TASK_EAGER_PROPAGATES=True)\n",
    "class CeleryEmailTests(TestCase):\n",
    "    def test_published_email_task_sends_once(self):\n",
    "        user = UserFactory(email=\"u@example.com\")\n",
    "\n",
    "        article = ArticleFactory(\n",
    "            status=Article.Status.PUBLISHED,\n",
    "            published_at=timezone.now(),\n",
    "            author=user,\n",
    "        )\n",
    "\n",
    "        from articles.tasks import send_article_published_email_task\n",
    "\n",
    "        send_article_published_email_task.delay(article.id)\n",
    "\n",
    "        self.assertEqual(len(mail.outbox), 1)\n",
    "\n",
    "        # Calling again should not send again because of ArticleNotification uniqueness\n",
    "        send_article_published_email_task.delay(article.id)\n",
    "        self.assertEqual(len(mail.outbox), 1)\n",
    "```\n",
    "\n",
    "### 29.11.3 Test export job workflow\n",
    "- start job (creates TaskExportJob)\n",
    "- run Celery task eagerly\n",
    "- assert job DONE and file exists\n",
    "\n",
    "---\n",
    "\n",
    "## 29.12 Worker Operations (Production Safety + Performance)\n",
    "\n",
    "### 29.12.1 Run workers as separate services\n",
    "In production you typically run:\n",
    "- web (gunicorn/uvicorn)\n",
    "- celery worker(s)\n",
    "- celery beat (scheduler)\n",
    "- redis (broker) (managed service often)\n",
    "\n",
    "### 29.12.2 Concurrency settings matter\n",
    "Celery workers can use prefork processes by default.\n",
    "You must tune:\n",
    "- number of workers\n",
    "- concurrency per worker\n",
    "- task time limits\n",
    "\n",
    "### 29.12.3 Avoid \u201cworker death by memory\u201d\n",
    "If tasks leak memory (common in image processing), use:\n",
    "- worker recycling options (Celery config)\n",
    "- smaller tasks\n",
    "- separate worker pool for heavy tasks\n",
    "\n",
    "### 29.12.4 Monitoring\n",
    "Common monitoring tools:\n",
    "- logs + metrics\n",
    "- Celery events\n",
    "- Flower (a web UI) (commonly used)\n",
    "\n",
    "Even without Flower, you should:\n",
    "- log task starts/failures\n",
    "- alert on failure spikes\n",
    "\n",
    "---\n",
    "\n",
    "## 29.13 RQ (Redis Queue) \u2014 A Simpler Alternative (Concept + Quickstart)\n",
    "\n",
    "If you want simpler than Celery:\n",
    "- RQ uses Redis directly\n",
    "- fewer features than Celery (especially scheduling and complex routing)\n",
    "- easier to understand initially\n",
    "\n",
    "High-level pattern:\n",
    "- create job function\n",
    "- enqueue with `django-rq`\n",
    "- run `rqworker`\n",
    "\n",
    "Use RQ when:\n",
    "- you need \u201cbackground jobs\u201d but not complex workflows/schedules\n",
    "- you want minimal moving parts\n",
    "- you\u2019re okay with fewer built-in retry/beat patterns (some exist, but ecosystem differs)\n",
    "\n",
    "Because your workbook already includes scheduling and advanced patterns, Celery is a\n",
    "better \u201cmastery-level\u201d tool, but knowing RQ exists is valuable.\n",
    "\n",
    "---\n",
    "\n",
    "## 29.14 Chapter Capstone Lab (Do This to Lock It In)\n",
    "\n",
    "1. **Celery wiring**\n",
    "   - Add `config/celery.py` and `config/__init__.py` integration\n",
    "   - Add broker settings\n",
    "   - Start Redis + worker\n",
    "\n",
    "2. **Email offloading**\n",
    "   - Create `send_article_published_email_task`\n",
    "   - Add idempotency via `ArticleNotification` unique constraint\n",
    "   - Use `transaction.on_commit(...)` when enqueuing\n",
    "\n",
    "3. **Export job**\n",
    "   - Add `TaskExportJob` model\n",
    "   - Add start/status/download endpoints\n",
    "   - Add `run_task_export_job_task` that generates CSV + stores file\n",
    "   - (Optional) broadcast `export_done` via Channels group\n",
    "\n",
    "4. **Tests**\n",
    "   - enable eager mode for tests\n",
    "   - test email idempotency\n",
    "   - test export job completion sets DONE and file exists\n",
    "\n",
    "---\n",
    "\n",
    "## 29.15 Common Background Job Mistakes (And How to Avoid Them)\n",
    "\n",
    "### Mistake A: Tasks depend on uncommitted DB rows\n",
    "Fix: always enqueue with `transaction.on_commit`.\n",
    "\n",
    "### Mistake B: Tasks are not idempotent\n",
    "Fix: unique constraints + \u201calready processed\u201d checks.\n",
    "\n",
    "### Mistake C: Workers block on slow network and retry storms happen\n",
    "Fix:\n",
    "- use backoff + jitter\n",
    "- cap retries\n",
    "- add timeouts to external calls\n",
    "- distinguish retryable vs non-retryable errors\n",
    "\n",
    "### Mistake D: Huge exports blow memory\n",
    "Fix:\n",
    "- stream writing (iterator + file streaming)\n",
    "- chunked processing\n",
    "- background generation + download link\n",
    "- consider object storage for huge files\n",
    "\n",
    "### Mistake E: Security leak in export jobs\n",
    "Fix:\n",
    "- store org_id and actor_id\n",
    "- re-check permissions at execution time if needed\n",
    "- scope queries by org always\n",
    "\n",
    "---\n",
    "\n",
    "## 29.16 Exercises (Do These Before Proceeding)\n",
    "\n",
    "1. Add a task queue for \u201csend comment moderation alert to staff\u201d:\n",
    "   - when a comment is submitted, enqueue an email to staff\n",
    "   - ensure it\u2019s idempotent per comment\n",
    "\n",
    "2. Add a periodic cleanup task:\n",
    "   - delete failed export jobs older than 30 days\n",
    "   - schedule via Celery beat\n",
    "\n",
    "3. Add a separate Celery queue:\n",
    "   - `emails` queue and `exports` queue\n",
    "   - route tasks accordingly\n",
    "   - run two workers with different concurrency\n",
    "\n",
    "4. Add logging context:\n",
    "   - pass `request_id` into export start task\n",
    "   - include it in task logs (so you can trace from API request to job logs)\n",
    "\n",
    "---\n",
    "\n",
    "## 29.17 Chapter Summary\n",
    "\n",
    "- Background tasks keep your web requests fast and your system resilient.\n",
    "- Celery + Redis is a common professional stack.\n",
    "- The three big rules:\n",
    "  1) `transaction.on_commit()` for DB-triggered tasks  \n",
    "  2) idempotency (assume tasks can run twice)  \n",
    "  3) retries with backoff + timeouts for transient failures\n",
    "- For large work (exports), use job models + async generation + download endpoints.\n",
    "- You can integrate background jobs with realtime (Channels) for great UX.\n",
    "\n",
    "---\n",
    "\n",
    "Next chapter: **Part VII \u2014 30. PostgreSQL for Django (Practical Database Mastery)**  \n",
    "We\u2019ll move beyond \u201cORM basics\u201d into real production DB work: PostgreSQL setup,\n",
    "indexes, query plans, transactions/locks, full-text search, and safe migrations for\n",
    "large tables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='28. realtime_with_websocets.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../7. Data_integrations_and_advanced_orm/30. postgres_for_django.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}