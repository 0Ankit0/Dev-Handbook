{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 48: Test Reporting and Metrics in DevOps**\n",
    "\n",
    "---\n",
    "\n",
    "## **48.1 Introduction to Test Reporting in DevOps**\n",
    "\n",
    "In a DevOps environment, test reporting is not just about generating a document at the end of a testing phase\u2014it's about providing continuous, actionable feedback to the entire team. Test reports inform developers, testers, and operations about the quality of the software, the health of the pipeline, and the risks associated with a release.\n",
    "\n",
    "### **48.1.1 Why Test Reporting Matters**\n",
    "\n",
    "- **Visibility:** Everyone can see the current quality status.\n",
    "- **Trust:** Reliable reports build confidence in releases.\n",
    "- **Decision-making:** Metrics guide go/no-go decisions.\n",
    "- **Continuous improvement:** Trends highlight areas needing attention.\n",
    "- **Communication:** Reports bridge the gap between technical and non-technical stakeholders.\n",
    "\n",
    "### **48.1.2 Types of Reports for Different Audiences**\n",
    "\n",
    "| Audience | What They Care About | Report Type |\n",
    "|----------|----------------------|-------------|\n",
    "| **Developers** | Which tests failed? Why? Stack traces, logs, code coverage | Detailed test execution reports, coverage diffs |\n",
    "| **Testers** | Test coverage, defect discovery, exploratory session notes | Test execution summaries, defect reports, traceability matrices |\n",
    "| **Product Owner** | Is the feature working? Are we ready to release? | Acceptance test results, high-level quality dashboard |\n",
    "| **Management** | Trends over time, quality metrics, release readiness | Executive dashboards, trend graphs, KPIs |\n",
    "| **Operations** | Impact on production, performance, security | Performance test reports, security scan results, monitoring alerts |\n",
    "\n",
    "---\n",
    "\n",
    "## **48.2 Test Automation Reporting**\n",
    "\n",
    "Automated tests produce vast amounts of data. Reporting tools aggregate this data into human-readable formats.\n",
    "\n",
    "### **48.2.1 Common Test Report Formats**\n",
    "\n",
    "- **JUnit XML:** Standard format for test results (supported by most CI tools).\n",
    "- **JSON:** Flexible format used by many modern test frameworks.\n",
    "- **HTML:** Human-readable web pages with styling and navigation.\n",
    "- **Allure:** Rich, interactive HTML reports with history, categories, and attachments.\n",
    "- **Cucumber JSON:** For BDD-style tests with feature and scenario details.\n",
    "\n",
    "### **48.2.2 Generating Reports in CI/CD Pipelines**\n",
    "\n",
    "Most CI/CD systems can parse JUnit XML and display test results directly in the UI.\n",
    "\n",
    "#### **Example: Generating JUnit Reports with pytest**\n",
    "\n",
    "```python\n",
    "# pytest.ini\n",
    "[pytest]\n",
    "junit_family = xunit2\n",
    "```\n",
    "\n",
    "Run tests: `pytest --junitxml=test-results/junit.xml`\n",
    "\n",
    "#### **Example: Generating Allure Reports**\n",
    "\n",
    "```bash\n",
    "# Run tests with Allure\n",
    "pytest --alluredir=allure-results\n",
    "\n",
    "# Generate HTML report\n",
    "allure generate allure-results -o allure-report\n",
    "\n",
    "# Open report\n",
    "allure open allure-report\n",
    "```\n",
    "\n",
    "#### **Example: GitHub Actions with Test Reporting**\n",
    "\n",
    "```yaml\n",
    "- name: Run tests\n",
    "  run: pytest --junitxml=test-results/junit.xml\n",
    "\n",
    "- name: Publish Test Results\n",
    "  uses: EnricoMi/publish-unit-test-result-action@v2\n",
    "  if: always()\n",
    "  with:\n",
    "    files: test-results/**/*.xml\n",
    "```\n",
    "\n",
    "### **48.2.3 Attachments and Artifacts**\n",
    "\n",
    "Test reports can include screenshots, logs, HAR files, and videos. These artifacts are crucial for debugging failures.\n",
    "\n",
    "- **Screenshots:** Capture on failure (e.g., in Selenium).\n",
    "- **Logs:** Console output, server logs.\n",
    "- **Videos:** Record test execution for later analysis.\n",
    "\n",
    "**Example (Cypress):** Cypress automatically captures screenshots and videos on failure and stores them in `cypress/screenshots` and `cypress/videos`. These can be uploaded as artifacts in CI.\n",
    "\n",
    "---\n",
    "\n",
    "## **48.3 Dashboard Creation**\n",
    "\n",
    "Dashboards provide at-a-glance visibility into test health and quality trends.\n",
    "\n",
    "### **48.3.1 What to Include on a Test Dashboard**\n",
    "\n",
    "- **Overall pass/fail rate:** Last build, trend over time.\n",
    "- **Test execution time:** Total time, slowest tests.\n",
    "- **Flaky test list:** Tests that have been unstable.\n",
    "- **Coverage metrics:** Line, branch, function coverage.\n",
    "- **Defect metrics:** Open/closed bugs, defect density.\n",
    "- **Environment status:** Test environment availability.\n",
    "- **Pipeline health:** Success/failure of recent builds.\n",
    "\n",
    "### **48.3.2 Tools for Building Dashboards**\n",
    "\n",
    "| Tool | Description |\n",
    "|------|-------------|\n",
    "| **Grafana** | Visualize metrics from Prometheus, InfluxDB, etc. |\n",
    "| **Kibana** | Dashboard for Elasticsearch logs and data. |\n",
    "| **Jenkins** | Built-in test result graphs. |\n",
    "| **GitLab** | Test reporting in merge requests. |\n",
    "| **SonarQube** | Code quality and coverage dashboards. |\n",
    "| **Allure Server** | Hosts and trends Allure reports. |\n",
    "| **ReportPortal** | AI-powered test analytics dashboard. |\n",
    "\n",
    "### **48.3.3 Example: Building a Test Dashboard with Grafana**\n",
    "\n",
    "1. **Collect test metrics** using a custom script that sends data to Prometheus (e.g., using a Pushgateway).\n",
    "\n",
    "```python\n",
    "from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "\n",
    "registry = CollectorRegistry()\n",
    "pass_rate = Gauge('test_pass_rate', 'Pass rate percentage', registry=registry)\n",
    "pass_rate.set(98.5)\n",
    "push_to_gateway('localhost:9091', job='test_metrics', registry=registry)\n",
    "```\n",
    "\n",
    "2. **Configure Prometheus** to scrape the Pushgateway.\n",
    "3. **Create a Grafana dashboard** with panels showing pass rate over time, test count, etc.\n",
    "\n",
    "### **48.3.4 Embedding Test Results in Team Communication**\n",
    "\n",
    "- **Slack/Teams notifications:** Send summary reports to channels.\n",
    "- **Email reports:** For stakeholders who prefer email.\n",
    "- **Status badges:** Embed in README or internal wiki.\n",
    "\n",
    "**Example Slack notification using GitHub Actions:**\n",
    "\n",
    "```yaml\n",
    "- name: Notify Slack\n",
    "  uses: 8398a7/action-slack@v3\n",
    "  with:\n",
    "    status: ${{ job.status }}\n",
    "    fields: repo,message,commit,author,action,eventName,ref,workflow\n",
    "  env:\n",
    "    SLACK_WEBHOOK_URL: ${{ secrets.SLACK_WEBHOOK }}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **48.4 Real-time Test Results**\n",
    "\n",
    "Real-time feedback is essential for fast development cycles. Developers should see test results as soon as they push code.\n",
    "\n",
    "### **48.4.1 Test Results in Pull Requests**\n",
    "\n",
    "Modern version control systems integrate test results directly into pull requests (PRs).\n",
    "\n",
    "- **GitHub:** Checks API shows test status (pass/fail) and details.\n",
    "- **GitLab:** Merge request widgets display pipeline results.\n",
    "- **Bitbucket:** Pipelines show test results inline.\n",
    "\n",
    "**Example: GitHub Actions reporting to PR**\n",
    "\n",
    "```yaml\n",
    "- name: Test\n",
    "  run: npm test\n",
    "\n",
    "- name: Report test results\n",
    "  uses: dorny/test-reporter@v1\n",
    "  if: always()\n",
    "  with:\n",
    "    name: Jest Tests\n",
    "    path: test-results/junit.xml\n",
    "    reporter: jest-junit\n",
    "```\n",
    "\n",
    "### **48.4.2 Live Test Execution Dashboards**\n",
    "\n",
    "For long-running test suites (e.g., performance tests), live dashboards show progress in real time.\n",
    "\n",
    "- **ReportPortal** provides live test execution views.\n",
    "- **Allure** can be updated during test run.\n",
    "- Custom solutions using WebSockets or server-sent events.\n",
    "\n",
    "### **48.4.3 Failure Triage and Alerts**\n",
    "\n",
    "When tests fail, the team should be alerted immediately with relevant context.\n",
    "\n",
    "- **On-call rotations:** Critical failures page the on-call engineer.\n",
    "- **Slack alerts:** Send failure details with links to logs.\n",
    "- **Auto-assign:** Create Jira tickets for flaky tests.\n",
    "\n",
    "---\n",
    "\n",
    "## **48.5 Test Coverage Reports**\n",
    "\n",
    "Test coverage measures how much of the code is exercised by tests. While 100% coverage doesn't guarantee quality, low coverage indicates risk.\n",
    "\n",
    "### **48.5.1 Types of Coverage**\n",
    "\n",
    "- **Line coverage:** Percentage of executable lines executed.\n",
    "- **Branch coverage:** Percentage of decision points (if/else) covered.\n",
    "- **Function coverage:** Percentage of functions called.\n",
    "- **Statement coverage:** Similar to line coverage (in C).\n",
    "- **Mutation coverage:** How many mutants (code changes) were killed by tests (advanced).\n",
    "\n",
    "### **48.5.2 Tools for Coverage Measurement**\n",
    "\n",
    "| Language | Tool | Output Format |\n",
    "|----------|------|---------------|\n",
    "| Java | JaCoCo | XML, HTML |\n",
    "| Python | coverage.py | XML, HTML |\n",
    "| JavaScript | Istanbul (nyc) | JSON, HTML, lcov |\n",
    "| C# | Coverlet | XML, HTML |\n",
    "| Go | go test -cover | HTML, functions |\n",
    "\n",
    "### **48.5.3 Integrating Coverage into CI/CD**\n",
    "\n",
    "Coverage can be enforced as a quality gate. If coverage drops below a threshold, the build fails.\n",
    "\n",
    "**Example: Python with pytest-cov**\n",
    "\n",
    "```bash\n",
    "pytest --cov=myapp --cov-report=xml --cov-fail-under=80\n",
    "```\n",
    "\n",
    "**Example: JavaScript with Jest**\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"jest\": {\n",
    "    \"coverageThreshold\": {\n",
    "      \"global\": {\n",
    "        \"branches\": 80,\n",
    "        \"functions\": 80,\n",
    "        \"lines\": 80,\n",
    "        \"statements\": 80\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **48.5.4 Visualizing Coverage Trends**\n",
    "\n",
    "- **SonarQube** tracks coverage over time and shows uncovered lines.\n",
    "- **Codecov** / **Coveralls** provide PR comments showing coverage changes.\n",
    "- **Custom dashboards** can pull coverage data from artifacts.\n",
    "\n",
    "**Example: Codecov GitHub Action**\n",
    "\n",
    "```yaml\n",
    "- name: Upload coverage to Codecov\n",
    "  uses: codecov/codecov-action@v3\n",
    "  with:\n",
    "    files: ./coverage.xml\n",
    "    flags: unittests\n",
    "    name: codecov-umbrella\n",
    "    fail_ci_if_error: true\n",
    "```\n",
    "\n",
    "### **48.5.5 Code Coverage vs. Test Effectiveness**\n",
    "\n",
    "High coverage does not guarantee that tests are effective. Complement coverage with:\n",
    "\n",
    "- **Mutation testing** (e.g., Stryker, PITest) to assess test quality.\n",
    "- **Test reviews** to ensure tests assert correct behavior.\n",
    "- **Defect analysis** to see if missed bugs correlate with uncovered code.\n",
    "\n",
    "---\n",
    "\n",
    "## **48.6 Integrating with Jira and Other Tools**\n",
    "\n",
    "Test reports are most valuable when linked to requirements, defects, and development tasks.\n",
    "\n",
    "### **48.6.1 Jira Integration**\n",
    "\n",
    "- **Automated test results** can update Jira issues (e.g., mark a story as passed/failed).\n",
    "- **Defect creation:** When a test fails, automatically create a Jira bug with details.\n",
    "- **Traceability:** Link test cases to user stories for end-to-end visibility.\n",
    "\n",
    "**Example: Using Xray (Jira Test Management)**\n",
    "\n",
    "```bash\n",
    "# Import test results into Xray\n",
    "curl -H \"Content-Type: application/xml\" -X POST -u user:pass \\\n",
    "  --data @\"junit.xml\" \\\n",
    "  https://jira.example.com/rest/raven/1.0/import/execution/junit\n",
    "```\n",
    "\n",
    "### **48.6.2 Integration with Test Management Tools**\n",
    "\n",
    "- **TestRail:** API to push test results, update test runs.\n",
    "- **qTest:** REST API for importing results.\n",
    "- **Zephyr:** Jira plugin for test management.\n",
    "\n",
    "**Example: Sending results to TestRail using Python**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "result = {\n",
    "    \"status_id\": 1,  # 1 = passed\n",
    "    \"comment\": \"All tests passed\"\n",
    "}\n",
    "requests.post(\n",
    "    \"https://your.testrail.io/index.php?/api/v2/add_result_for_case/123/456\",\n",
    "    json=result,\n",
    "    auth=(\"user\", \"password\")\n",
    ")\n",
    "```\n",
    "\n",
    "### **48.6.3 Integration with Communication Tools**\n",
    "\n",
    "- **Slack:** Send formatted messages with test results and links.\n",
    "- **Microsoft Teams:** Similar to Slack, using incoming webhooks.\n",
    "- **Email:** For stakeholders who prefer traditional communication.\n",
    "\n",
    "---\n",
    "\n",
    "## **48.7 Test Metrics for Continuous Improvement**\n",
    "\n",
    "Metrics are most powerful when used to drive improvement, not to judge individuals.\n",
    "\n",
    "### **48.7.1 Key Metrics to Track**\n",
    "\n",
    "| Metric | Definition | Why It Matters |\n",
    "|--------|------------|----------------|\n",
    "| **Test Pass Rate** | % of tests passing in CI | High pass rate indicates stability. |\n",
    "| **Test Execution Time** | Time to run full test suite | Long times slow feedback; identify slow tests. |\n",
    "| **Flakiness Rate** | % of test runs that are flaky | Flaky tests erode trust; need fixing. |\n",
    "| **Defect Detection Percentage (DDP)** | % of bugs found before production | Measures effectiveness of testing. |\n",
    "| **Defect Escape Rate** | % of bugs found in production | Low is good; indicates good testing. |\n",
    "| **Mean Time to Detect (MTTD)** | Time from bug introduction to detection | Fast detection reduces impact. |\n",
    "| **Mean Time to Repair (MTTR)** | Time to fix a detected defect | Fast fixes keep pipeline moving. |\n",
    "| **Code Coverage** | % of code covered by tests | Low coverage indicates risk areas. |\n",
    "| **Test Maintenance Effort** | Time spent fixing tests vs. writing new ones | High maintenance suggests brittle tests. |\n",
    "\n",
    "### **48.7.2 Visualizing Trends**\n",
    "\n",
    "Plot metrics over time to identify patterns:\n",
    "\n",
    "- Is pass rate declining after a certain change?\n",
    "- Are tests getting slower as the codebase grows?\n",
    "- Are flaky tests being addressed, or accumulating?\n",
    "\n",
    "**Example: Grafana panel showing pass rate trend**\n",
    "\n",
    "```\n",
    "SELECT\n",
    "  time,\n",
    "  value\n",
    "FROM test_pass_rate\n",
    "WHERE $__timeFilter(time)\n",
    "```\n",
    "\n",
    "### **48.7.3 Using Metrics to Drive Action**\n",
    "\n",
    "- **Identify flaky tests:** Tag them, quarantine them, and prioritize fixing.\n",
    "- **Optimize slow tests:** Split, parallelize, or refactor.\n",
    "- **Improve coverage:** Add tests for low-coverage modules.\n",
    "- **Reduce defect escape:** Analyze escaped bugs and improve test scenarios.\n",
    "\n",
    "### **48.7.4 Avoiding Metric Pitfalls**\n",
    "\n",
    "- **Don't use metrics for performance reviews** \u2013 they will be gamed.\n",
    "- **Don't focus on a single metric** \u2013 e.g., 100% coverage is meaningless if tests are weak.\n",
    "- **Context matters** \u2013 a low pass rate during active development is normal; it's the trend that matters.\n",
    "- **Automate metric collection** \u2013 manual tracking is error-prone and unsustainable.\n",
    "\n",
    "---\n",
    "\n",
    "## **48.8 Practical Exercise: Building a Test Dashboard**\n",
    "\n",
    "**Objective:** Create a simple test dashboard using open-source tools (Prometheus + Grafana) that shows pass rate, test count, and execution time.\n",
    "\n",
    "### **Step 1: Set up a Prometheus Pushgateway**\n",
    "\n",
    "```bash\n",
    "docker run -d -p 9091:9091 prom/pushgateway\n",
    "```\n",
    "\n",
    "### **Step 2: Create a Python script to push test metrics**\n",
    "\n",
    "```python\n",
    "# push_metrics.py\n",
    "import subprocess\n",
    "import xml.etree.ElementTree as ET\n",
    "from prometheus_client import CollectorRegistry, Gauge, push_to_gateway\n",
    "\n",
    "def parse_junit(filepath):\n",
    "    tree = ET.parse(filepath)\n",
    "    root = tree.getroot()\n",
    "    tests = int(root.get('tests', 0))\n",
    "    failures = int(root.get('failures', 0))\n",
    "    errors = int(root.get('errors', 0))\n",
    "    skipped = int(root.get('skipped', 0))\n",
    "    time = float(root.get('time', 0))\n",
    "    passed = tests - failures - errors - skipped\n",
    "    pass_rate = (passed / tests) * 100 if tests > 0 else 0\n",
    "    return tests, passed, failures, errors, skipped, time, pass_rate\n",
    "\n",
    "def push_metrics(tests, passed, failures, errors, skipped, time, pass_rate):\n",
    "    registry = CollectorRegistry()\n",
    "    \n",
    "    g_tests = Gauge('test_count', 'Total number of tests', registry=registry)\n",
    "    g_passed = Gauge('test_passed', 'Number of passed tests', registry=registry)\n",
    "    g_failures = Gauge('test_failures', 'Number of failed tests', registry=registry)\n",
    "    g_errors = Gauge('test_errors', 'Number of errored tests', registry=registry)\n",
    "    g_skipped = Gauge('test_skipped', 'Number of skipped tests', registry=registry)\n",
    "    g_time = Gauge('test_execution_time_seconds', 'Total test execution time', registry=registry)\n",
    "    g_pass_rate = Gauge('test_pass_rate', 'Test pass rate percentage', registry=registry)\n",
    "    \n",
    "    g_tests.set(tests)\n",
    "    g_passed.set(passed)\n",
    "    g_failures.set(failures)\n",
    "    g_errors.set(errors)\n",
    "    g_skipped.set(skipped)\n",
    "    g_time.set(time)\n",
    "    g_pass_rate.set(pass_rate)\n",
    "    \n",
    "    push_to_gateway('localhost:9091', job='test_metrics', registry=registry)\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import sys\n",
    "    if len(sys.argv) != 2:\n",
    "        print(\"Usage: push_metrics.py <junit.xml>\")\n",
    "        sys.exit(1)\n",
    "    metrics = parse_junit(sys.argv[1])\n",
    "    push_metrics(*metrics)\n",
    "```\n",
    "\n",
    "### **Step 3: Run tests and push metrics in CI**\n",
    "\n",
    "```bash\n",
    "pytest --junitxml=test-results/junit.xml\n",
    "python push_metrics.py test-results/junit.xml\n",
    "```\n",
    "\n",
    "### **Step 4: Set up Prometheus to scrape the Pushgateway**\n",
    "\n",
    "`prometheus.yml`:\n",
    "\n",
    "```yaml\n",
    "scrape_configs:\n",
    "  - job_name: 'pushgateway'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9091']\n",
    "    honor_labels: true\n",
    "```\n",
    "\n",
    "### **Step 5: Configure Grafana**\n",
    "\n",
    "- Add Prometheus as a data source.\n",
    "- Create a dashboard with panels:\n",
    "  - **Stat panel:** Test pass rate (latest value)\n",
    "  - **Graph panel:** Pass rate over time\n",
    "  - **Stat panel:** Total tests\n",
    "  - **Stat panel:** Execution time\n",
    "\n",
    "### **Step 6: Automate in CI**\n",
    "\n",
    "Add a step in your pipeline to run the script after tests.\n",
    "\n",
    "---\n",
    "\n",
    "## **48.9 Best Practices for Test Reporting**\n",
    "\n",
    "1. **Automate everything:** Manual reporting is outdated as soon as it's written.\n",
    "2. **Make reports accessible:** Store them in a central location (e.g., S3, Jenkins artifacts) and share links.\n",
    "3. **Use consistent formats:** JUnit XML is widely supported; adopt it for all test types.\n",
    "4. **Include context:** Build number, commit hash, environment, date/time.\n",
    "5. **Keep reports concise:** Summaries for managers, details for developers.\n",
    "6. **Link to artifacts:** Screenshots, logs, and videos should be easily reachable.\n",
    "7. **Monitor report generation itself:** Ensure the reporting step doesn't fail silently.\n",
    "8. **Review and refine:** Regularly ask the team what information they need and adjust reports.\n",
    "\n",
    "---\n",
    "\n",
    "## **48.10 Common Challenges and Solutions**\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| **Too many reports, no one reads them** | Consolidate into a single dashboard; send only critical alerts. |\n",
    "| **Flaky tests pollute reports** | Quarantine flaky tests; report them separately. |\n",
    "| **Metrics are misinterpreted** | Provide context and training; use trend lines instead of raw numbers. |\n",
    "| **Reports are slow to generate** | Optimize test data extraction; use incremental reporting. |\n",
    "| **Integration with Jira is brittle** | Use official plugins; handle API errors gracefully. |\n",
    "| **Dashboards become cluttered** | Start simple; add panels only when needed. |\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we covered the critical role of test reporting and metrics in a DevOps environment:\n",
    "\n",
    "- **Test automation reporting** generates structured output (JUnit, Allure) that feeds into CI/CD pipelines and dashboards.\n",
    "- **Dashboards** provide real-time visibility into test health and trends, using tools like Grafana, Kibana, and ReportPortal.\n",
    "- **Real-time test results** integrated into pull requests give developers immediate feedback.\n",
    "- **Test coverage reports** help identify untested code, with tools like JaCoCo, coverage.py, and Istanbul, and can be enforced as quality gates.\n",
    "- **Integration with Jira and other tools** links testing to requirements and defects, enabling traceability.\n",
    "- **Test metrics** (pass rate, flakiness, execution time, defect escape rate) drive continuous improvement when tracked and visualized.\n",
    "- A **practical exercise** demonstrated building a test dashboard with Prometheus and Grafana.\n",
    "- **Best practices** and **common challenges** provide guidance for implementing effective reporting.\n",
    "\n",
    "**Key Insight:** Test reporting is not an afterthought\u2014it's a core part of the feedback loop that enables teams to deliver quality software quickly. By making test results visible, actionable, and integrated, teams build trust and continuously improve their processes.\n",
    "\n",
    "---\n",
    "\n",
    "## **\ud83d\udcd6 Next Chapter: Chapter 49 - Accessibility Testing**\n",
    "\n",
    "Now that you understand how to report and measure testing outcomes, Chapter 49 will introduce **Accessibility Testing**:\n",
    "\n",
    "- **What is accessibility?** Importance and legal requirements.\n",
    "- **WCAG guidelines** and how to interpret them.\n",
    "- **Types of disabilities** and assistive technologies.\n",
    "- **Accessibility testing tools** (axe, WAVE, Lighthouse).\n",
    "- **Manual accessibility testing** (keyboard navigation, screen readers).\n",
    "- **Integrating accessibility into CI/CD pipelines.\n",
    "- **Practical examples** and best practices.\n",
    "\n",
    "**Chapter 49 will equip you with the skills to ensure your applications are usable by everyone, including people with disabilities.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='47. devops_and_continuous_testing.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../12. accessibility_and_usability_testing/49. accessibility_testing.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}