{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 36: Performance Test Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "Raw performance metrics\u2014response times, throughput figures, error rates\u2014are merely data points. The true value of performance testing lies in **analysis**: transforming these data points into actionable insights that drive architectural decisions, capacity planning, and optimization strategies.\n",
    "\n",
    "This chapter bridges the gap between data collection and engineering action. You will learn the statistical rigor required to distinguish genuine performance changes from random variance, the mathematical models that predict system behavior under load, and the forensic techniques used to pinpoint the exact commit or configuration that introduced a regression.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.1 Statistical Analysis of Performance Data**\n",
    "\n",
    "Performance data is inherently noisy. Network jitter, garbage collection pauses, background processes, and thermal throttling all introduce variance. Statistical analysis separates signal from noise.\n",
    "\n",
    "### **36.1.1 Understanding Variance and Standard Deviation**\n",
    "\n",
    "**Variance** measures how far data points spread from the mean. **Standard Deviation (\u03c3)** is the square root of variance, expressed in the same units as the metric (milliseconds for latency).\n",
    "\n",
    "```python\n",
    "import statistics\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "class StatisticalAnalyzer:\n",
    "    \"\"\"\n",
    "    Statistical analysis for performance metrics\n",
    "    \"\"\"\n",
    "    \n",
    "    def analyze_latency_distribution(self, samples):\n",
    "        \"\"\"\n",
    "        Comprehensive statistical analysis of latency samples\n",
    "        \"\"\"\n",
    "        n = len(samples)\n",
    "        mean = statistics.mean(samples)\n",
    "        median = statistics.median(samples)\n",
    "        std_dev = statistics.stdev(samples)\n",
    "        variance = statistics.variance(samples)\n",
    "        \n",
    "        # Coefficient of Variation (CV) - normalized standard deviation\n",
    "        cv = (std_dev / mean) * 100\n",
    "        \n",
    "        # Confidence Intervals (95%)\n",
    "        confidence = 0.95\n",
    "        alpha = 1 - confidence\n",
    "        z_score = stats.norm.ppf(1 - alpha/2)\n",
    "        margin_error = z_score * (std_dev / np.sqrt(n))\n",
    "        \n",
    "        ci_lower = mean - margin_error\n",
    "        ci_upper = mean + margin_error\n",
    "        \n",
    "        analysis = {\n",
    "            'descriptive_statistics': {\n",
    "                'count': n,\n",
    "                'mean': mean,\n",
    "                'median': median,\n",
    "                'std_dev': std_dev,\n",
    "                'variance': variance,\n",
    "                'min': min(samples),\n",
    "                'max': max(samples),\n",
    "                'range': max(samples) - min(samples)\n",
    "            },\n",
    "            'variability_metrics': {\n",
    "                'cv_percent': cv,\n",
    "                'std_dev_relative': std_dev / mean,\n",
    "                'interpretation': self._interpret_cv(cv)\n",
    "            },\n",
    "            'confidence_interval_95': {\n",
    "                'lower': ci_lower,\n",
    "                'upper': ci_upper,\n",
    "                'margin_of_error': margin_error,\n",
    "                'interpretation': f\"95% confident true mean is between {ci_lower:.2f} and {ci_upper:.2f}\"\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return analysis\n",
    "    \n",
    "    def _interpret_cv(self, cv):\n",
    "        \"\"\"\n",
    "        Coefficient of Variation interpretation\n",
    "        \"\"\"\n",
    "        if cv < 5:\n",
    "            return \"EXCELLENT: Very consistent performance\"\n",
    "        elif cv < 15:\n",
    "            return \"GOOD: Acceptable variance for production\"\n",
    "        elif cv < 30:\n",
    "            return \"MODERATE: Inconsistent, investigate causes\"\n",
    "        else:\n",
    "            return \"HIGH: Unacceptable variance, system unstable\"\n",
    "    \n",
    "    def detect_outliers(self, samples, method='iqr'):\n",
    "        \"\"\"\n",
    "        Detect statistical outliers that may skew results\n",
    "        \"\"\"\n",
    "        if method == 'iqr':\n",
    "            q1 = np.percentile(samples, 25)\n",
    "            q3 = np.percentile(samples, 75)\n",
    "            iqr = q3 - q1\n",
    "            lower_bound = q1 - (1.5 * iqr)\n",
    "            upper_bound = q3 + (1.5 * iqr)\n",
    "        elif method == 'zscore':\n",
    "            z_scores = np.abs(stats.zscore(samples))\n",
    "            lower_bound = -3\n",
    "            upper_bound = 3\n",
    "        \n",
    "        outliers = [x for x in samples if x < lower_bound or x > upper_bound]\n",
    "        outlier_percentage = (len(outliers) / len(samples)) * 100\n",
    "        \n",
    "        return {\n",
    "            'outliers': outliers,\n",
    "            'count': len(outliers),\n",
    "            'percentage': outlier_percentage,\n",
    "            'bounds': {'lower': lower_bound, 'upper': upper_bound},\n",
    "            'recommendation': 'Remove for baseline analysis' if outlier_percentage < 5 else 'Investigate systemic issue'\n",
    "        }\n",
    "```\n",
    "\n",
    "**Industry Interpretation:**\n",
    "- **CV < 10%**: Production-ready consistency\n",
    "- **CV 10-25%**: Acceptable for most web applications\n",
    "- **CV > 30%**: Indicates resource contention or architectural issues requiring immediate attention\n",
    "\n",
    "### **36.1.2 Statistical Significance Testing (T-Tests)**\n",
    "\n",
    "When comparing two performance results (before/after optimization, baseline vs. current), determine if the difference is statistically significant or random noise.\n",
    "\n",
    "```python\n",
    "class PerformanceComparator:\n",
    "    \"\"\"\n",
    "    Compare two performance test results for statistical significance\n",
    "    \"\"\"\n",
    "    \n",
    "    def compare_samples(self, baseline_samples, current_samples, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Two-sample t-test for comparing means\n",
    "        \n",
    "        Null Hypothesis: No difference between baseline and current\n",
    "        Alternative: Current is significantly different\n",
    "        \"\"\"\n",
    "        # Welch's t-test (doesn't assume equal variance)\n",
    "        t_stat, p_value = stats.ttest_ind(baseline_samples, current_samples, equal_var=False)\n",
    "        \n",
    "        # Effect size (Cohen's d)\n",
    "        pooled_std = np.sqrt((np.var(baseline_samples) + np.var(current_samples)) / 2)\n",
    "        cohens_d = (np.mean(current_samples) - np.mean(baseline_samples)) / pooled_std\n",
    "        \n",
    "        # Interpret effect size\n",
    "        if abs(cohens_d) < 0.2:\n",
    "            effect_size = \"NEGLIGIBLE\"\n",
    "        elif abs(cohens_d) < 0.5:\n",
    "            effect_size = \"SMALL\"\n",
    "        elif abs(cohens_d) < 0.8:\n",
    "            effect_size = \"MEDIUM\"\n",
    "        else:\n",
    "            effect_size = \"LARGE\"\n",
    "        \n",
    "        result = {\n",
    "            'baseline_mean': np.mean(baseline_samples),\n",
    "            'current_mean': np.mean(current_samples),\n",
    "            'difference': np.mean(current_samples) - np.mean(baseline_samples),\n",
    "            'difference_percent': ((np.mean(current_samples) - np.mean(baseline_samples)) / np.mean(baseline_samples)) * 100,\n",
    "            't_statistic': t_stat,\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < alpha,\n",
    "            'cohens_d': cohens_d,\n",
    "            'effect_size': effect_size,\n",
    "            'conclusion': self._generate_conclusion(p_value < alpha, cohens_d)\n",
    "        }\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def _generate_conclusion(self, is_significant, cohens_d):\n",
    "        if not is_significant:\n",
    "            return \"NO SIGNIFICANT CHANGE: Observed difference likely due to random variance\"\n",
    "        \n",
    "        direction = \"IMPROVEMENT\" if cohens_d < 0 else \"REGRESSION\"\n",
    "        return f\"STATISTICALLY SIGNIFICANT {direction}: Change is real, not noise. Effect size: {cohens_d:.2f}\"\n",
    "    \n",
    "    def required_sample_size(self, baseline_mean, expected_effect_percent, std_dev, power=0.8, alpha=0.05):\n",
    "        \"\"\"\n",
    "        Calculate required sample size for detecting a given effect\n",
    "        \n",
    "        Avoid under-powered tests that can't detect real changes\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.power import ttest_power\n",
    "        \n",
    "        effect_size = (baseline_mean * (expected_effect_percent / 100)) / std_dev\n",
    "        \n",
    "        # Solve for n\n",
    "        n = None\n",
    "        for sample_size in range(10, 10000):\n",
    "            power_calc = ttest_power(effect_size, nobs=sample_size, alpha=alpha, alternative='two-sided')\n",
    "            if power_calc >= power:\n",
    "                n = sample_size\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'required_sample_size': n,\n",
    "            'effect_size': effect_size,\n",
    "            'power': power,\n",
    "            'alpha': alpha,\n",
    "            'recommendation': f\"Run test with at least {n} samples to detect {expected_effect_percent}% change\"\n",
    "        }\n",
    "\n",
    "# Usage example\n",
    "comparator = PerformanceComparator()\n",
    "result = comparator.compare_samples(\n",
    "    baseline_samples=[120, 125, 118, 122, 121, 119, 123, 120, 122, 121],\n",
    "    current_samples=[105, 108, 110, 107, 109, 106, 108, 111, 107, 108]\n",
    ")\n",
    "\n",
    "print(f\"Mean improvement: {result['difference_percent']:.1f}%\")\n",
    "print(f\"P-value: {result['p_value']:.4f} ({'Significant' if result['significant'] else 'Not significant'})\")\n",
    "print(f\"Effect size: {result['effect_size']}\")\n",
    "```\n",
    "\n",
    "**Best Practice**: Never compare averages from single test runs. Use at least 30 samples (preferably 100+) and verify statistical significance with p < 0.05.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.2 Latency Modeling and Queueing Theory**\n",
    "\n",
    "Understanding **Little's Law** and basic queueing theory enables you to predict system behavior at loads you've never tested.\n",
    "\n",
    "### **36.2.1 Little's Law**\n",
    "\n",
    "**Formula**: **L = \u03bb \u00d7 W**\n",
    "\n",
    "Where:\n",
    "- **L** = Average number of items in the system (concurrent requests)\n",
    "- **\u03bb** (lambda) = Arrival rate (requests per second)\n",
    "- **W** = Average time spent in the system (response time)\n",
    "\n",
    "**Implications**:\n",
    "- If you know two variables, you can calculate the third\n",
    "- If response time doubles, you need twice the concurrency to maintain same throughput\n",
    "- Systems have saturation points where increasing \u03bb causes W to increase exponentially\n",
    "\n",
    "```python\n",
    "class LittlesLawModel:\n",
    "    \"\"\"\n",
    "    Apply Little's Law to capacity planning\n",
    "    \"\"\"\n",
    "    \n",
    "    def calculate_concurrent_users(self, arrival_rate, avg_response_time):\n",
    "        \"\"\"\n",
    "        L = \u03bb \u00d7 W\n",
    "        \n",
    "        arrival_rate: requests per second\n",
    "        avg_response_time: seconds\n",
    "        \"\"\"\n",
    "        return arrival_rate * avg_response_time\n",
    "    \n",
    "    def predict_response_time(self, arrival_rate, concurrent_users):\n",
    "        \"\"\"\n",
    "        W = L / \u03bb\n",
    "        \n",
    "        Given current concurrency, predict response time\n",
    "        \"\"\"\n",
    "        return concurrent_users / arrival_rate\n",
    "    \n",
    "    def find_saturation_point(self, throughput_data, latency_data):\n",
    "        \"\"\"\n",
    "        Identify where linear relationship breaks (knee of the curve)\n",
    "        \"\"\"\n",
    "        # Calculate slope between consecutive points\n",
    "        slopes = []\n",
    "        for i in range(1, len(throughput_data)):\n",
    "            delta_throughput = throughput_data[i] - throughput_data[i-1]\n",
    "            delta_latency = latency_data[i] - latency_data[i-1]\n",
    "            \n",
    "            if delta_throughput > 0:\n",
    "                slope = delta_latency / delta_throughput\n",
    "                slopes.append(slope)\n",
    "        \n",
    "        # Saturation occurs when slope increases dramatically\n",
    "        saturation_idx = None\n",
    "        for i in range(1, len(slopes)):\n",
    "            if slopes[i] > slopes[i-1] * 3:  # 3x increase in slope\n",
    "                saturation_idx = i\n",
    "                break\n",
    "        \n",
    "        return {\n",
    "            'saturation_throughput': throughput_data[saturation_idx] if saturation_idx else None,\n",
    "            'saturation_latency': latency_data[saturation_idx] if saturation_idx else None,\n",
    "            'max_efficient_throughput': throughput_data[saturation_idx - 1] if saturation_idx else max(throughput_data)\n",
    "        }\n",
    "```\n",
    "\n",
    "### **36.2.2 The Universal Scalability Law (USL)**\n",
    "\n",
    "The USL models system throughput as a function of concurrency, accounting for:\n",
    "- **Contention** (serialization delays)\n",
    "- **Coherence** (cross-talk between parallel processes)\n",
    "\n",
    "**Formula**: **C(N) = N / (1 + \u03b1(N-1) + \u03b2N(N-1))**\n",
    "\n",
    "Where:\n",
    "- **C(N)** = Capacity at concurrency N\n",
    "- **\u03b1** = Contention coefficient\n",
    "- **\u03b2** = Coherence coefficient\n",
    "\n",
    "```python\n",
    "from scipy.optimize import curve_fit\n",
    "\n",
    "class UniversalScalabilityModel:\n",
    "    \"\"\"\n",
    "    USL for predicting system behavior beyond tested loads\n",
    "    \"\"\"\n",
    "    \n",
    "    def usl_function(self, n, sigma, kappa, lambda_):\n",
    "        \"\"\"\n",
    "        C(N) = (lambda * N) / (1 + sigma*(N-1) + kappa*N*(N-1))\n",
    "        \n",
    "        sigma: contention penalty\n",
    "        kappa: coherence penalty\n",
    "        lambda_: linear speedup (usually 1)\n",
    "        \"\"\"\n",
    "        return (lambda_ * n) / (1 + sigma * (n - 1) + kappa * n * (n - 1))\n",
    "    \n",
    "    def fit_model(self, concurrency_levels, throughput_measurements):\n",
    "        \"\"\"\n",
    "        Fit USL to empirical data to predict behavior\n",
    "        \"\"\"\n",
    "        # Initial guess for parameters\n",
    "        p0 = [0.01, 0.0001, 1.0]\n",
    "        \n",
    "        # Curve fitting\n",
    "        popt, _ = curve_fit(self.usl_function, concurrency_levels, throughput_measurements, p0=p0, maxfev=10000)\n",
    "        sigma, kappa, lambda_ = popt\n",
    "        \n",
    "        # Predict maximum throughput (where derivative = 0)\n",
    "        # N_max = sqrt((1 - sigma) / kappa)\n",
    "        if kappa > 0:\n",
    "            max_concurrency = np.sqrt((1 - sigma) / kappa)\n",
    "            max_throughput = self.usl_function(max_concurrency, sigma, kappa, lambda_)\n",
    "        else:\n",
    "            max_concurrency = float('inf')\n",
    "            max_throughput = float('inf')\n",
    "        \n",
    "        return {\n",
    "            'parameters': {\n",
    "                'sigma_contention': sigma,\n",
    "                'kappa_coherence': kappa,\n",
    "                'lambda_speedup': lambda_\n",
    "            },\n",
    "            'predictions': {\n",
    "                'max_concurrency': max_concurrency,\n",
    "                'max_throughput': max_throughput,\n",
    "                'efficiency_at_max': max_throughput / max_concurrency if max_concurrency > 0 else 0\n",
    "            },\n",
    "            'model': lambda n: self.usl_function(n, sigma, kappa, lambda_)\n",
    "        }\n",
    "    \n",
    "    def predict_beyond_tested(self, model_fit, target_concurrency):\n",
    "        \"\"\"\n",
    "        Predict performance at untested concurrency levels\n",
    "        \"\"\"\n",
    "        predicted = model_fit['model'](target_concurrency)\n",
    "        \n",
    "        # Confidence decreases as we extrapolate further\n",
    "        tested_max = max([10, 20, 50, 100])  # Example tested values\n",
    "        extrapolation_ratio = target_concurrency / tested_max\n",
    "        \n",
    "        confidence = max(0, 1 - (extrapolation_ratio - 1) * 0.2)  # 20% confidence loss per 2x extrapolation\n",
    "        \n",
    "        return {\n",
    "            'predicted_throughput': predicted,\n",
    "            'confidence': confidence,\n",
    "            'reliability': 'HIGH' if extrapolation_ratio < 2 else 'MEDIUM' if extrapolation_ratio < 5 else 'LOW'\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "usl = UniversalScalabilityModel()\n",
    "result = usl.fit_model(\n",
    "    concurrency_levels=[1, 5, 10, 20, 50, 100],\n",
    "    throughput_measurements=[100, 480, 950, 1800, 3500, 5500]\n",
    ")\n",
    "\n",
    "print(f\"Predicted max throughput: {result['predictions']['max_throughput']:.0f} req/s\")\n",
    "print(f\"Optimal concurrency: {result['predictions']['max_concurrency']:.0f} users\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **36.3 Comparative Analysis**\n",
    "\n",
    "### **36.3.1 A/B Testing for Performance**\n",
    "\n",
    "When deploying optimizations, use statistical A/B testing to validate improvement:\n",
    "\n",
    "```python\n",
    "class PerformanceABTest:\n",
    "    \"\"\"\n",
    "    Statistical A/B testing for performance changes\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, control_name, treatment_name):\n",
    "        self.control = control_name\n",
    "        self.treatment = treatment_name\n",
    "        \n",
    "    def run_test(self, control_samples, treatment_samples, min_samples=1000):\n",
    "        \"\"\"\n",
    "        Run statistical test with stopping criteria\n",
    "        \"\"\"\n",
    "        if len(control_samples) < min_samples or len(treatment_samples) < min_samples:\n",
    "            return {'status': 'INSUFFICIENT_DATA', 'required': min_samples}\n",
    "        \n",
    "        # Mann-Whitney U test (non-parametric, better for latency distributions)\n",
    "        statistic, p_value = stats.mannwhitneyu(control_samples, treatment_samples, alternative='two-sided')\n",
    "        \n",
    "        # Calculate percentiles for comparison\n",
    "        metrics = ['mean', 'p50', 'p95', 'p99']\n",
    "        comparison = {}\n",
    "        \n",
    "        for metric in metrics:\n",
    "            if metric == 'mean':\n",
    "                c_val = np.mean(control_samples)\n",
    "                t_val = np.mean(treatment_samples)\n",
    "            else:\n",
    "                p = int(metric[1:])\n",
    "                c_val = np.percentile(control_samples, p)\n",
    "                t_val = np.percentile(treatment_samples, p)\n",
    "            \n",
    "            change = ((t_val - c_val) / c_val) * 100\n",
    "            comparison[metric] = {\n",
    "                'control': c_val,\n",
    "                'treatment': t_val,\n",
    "                'change_percent': change,\n",
    "                'improved': change < 0  # Lower is better for latency\n",
    "            }\n",
    "        \n",
    "        # Determine winner\n",
    "        significant_improvement = p_value < 0.05 and comparison['p95']['improved']\n",
    "        \n",
    "        return {\n",
    "            'p_value': p_value,\n",
    "            'significant': p_value < 0.05,\n",
    "            'metrics': comparison,\n",
    "            'recommendation': 'DEPLOY_TREATMENT' if significant_improvement else 'KEEP_CONTROL' if not comparison['p95']['improved'] else 'INCONCLUSIVE'\n",
    "        }\n",
    "    \n",
    "    def sequential_testing(self, control_data_stream, treatment_data_stream, max_samples=10000):\n",
    "        \"\"\"\n",
    "        Sequential testing with early stopping\n",
    "        \"\"\"\n",
    "        control_buffer = []\n",
    "        treatment_buffer = []\n",
    "        \n",
    "        for c_sample, t_sample in zip(control_data_stream, treatment_data_stream):\n",
    "            control_buffer.append(c_sample)\n",
    "            treatment_buffer.append(t_sample)\n",
    "            \n",
    "            if len(control_buffer) % 100 == 0:  # Check every 100 samples\n",
    "                result = self.run_test(control_buffer, treatment_buffer)\n",
    "                \n",
    "                if result['significant']:\n",
    "                    return {\n",
    "                        'result': result,\n",
    "                        'samples_required': len(control_buffer),\n",
    "                        'early_stop': True\n",
    "                    }\n",
    "                \n",
    "                if len(control_buffer) >= max_samples:\n",
    "                    return {\n",
    "                        'result': result,\n",
    "                        'samples_required': max_samples,\n",
    "                        'early_stop': False\n",
    "                    }\n",
    "        \n",
    "        return {'status': 'INCOMPLETE'}\n",
    "```\n",
    "\n",
    "### **36.3.2 Canary Analysis**\n",
    "\n",
    "Analyze canary deployments by comparing production traffic segments:\n",
    "\n",
    "```python\n",
    "class CanaryAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze canary deployment performance\n",
    "    \"\"\"\n",
    "    \n",
    "    def analyze_canary(self, baseline_metrics, canary_metrics, error_threshold=0.001):\n",
    "        \"\"\"\n",
    "        Compare canary (new version) against baseline (stable)\n",
    "        \n",
    "        Returns: 'PROMOTE', 'ROLLBACK', or 'CONTINUE'\n",
    "        \"\"\"\n",
    "        # Check for errors\n",
    "        baseline_error = baseline_metrics['error_rate']\n",
    "        canary_error = canary_metrics['error_rate']\n",
    "        \n",
    "        if canary_error > baseline_error + error_threshold:\n",
    "            return {\n",
    "                'decision': 'ROLLBACK',\n",
    "                'reason': f\"Error rate {canary_error:.4f} exceeds baseline {baseline_error:.4f}\",\n",
    "                'severity': 'CRITICAL'\n",
    "            }\n",
    "        \n",
    "        # Check latency regression\n",
    "        baseline_p99 = baseline_metrics['p99']\n",
    "        canary_p99 = canary_metrics['p99']\n",
    "        \n",
    "        regression_percent = ((canary_p99 - baseline_p99) / baseline_p99) * 100\n",
    "        \n",
    "        if regression_percent > 20:\n",
    "            return {\n",
    "                'decision': 'ROLLBACK',\n",
    "                'reason': f\"P99 regression {regression_percent:.1f}% exceeds 20% threshold\",\n",
    "                'severity': 'HIGH'\n",
    "            }\n",
    "        elif regression_percent > 10:\n",
    "            return {\n",
    "                'decision': 'CONTINUE',\n",
    "                'reason': f\"Minor regression {regression_percent:.1f}%, monitor closely\",\n",
    "                'severity': 'MEDIUM'\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'decision': 'PROMOTE',\n",
    "                'reason': f\"Performance acceptable, error rate stable\",\n",
    "                'improvement_percent': abs(regression_percent) if regression_percent < 0 else 0\n",
    "            }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **36.4 Capacity Planning**\n",
    "\n",
    "### **36.4.1 Growth Projection Models**\n",
    "\n",
    "Predict when you'll need additional capacity based on growth trends:\n",
    "\n",
    "```python\n",
    "class CapacityPlanner:\n",
    "    \"\"\"\n",
    "    Forecast capacity needs based on growth trends\n",
    "    \"\"\"\n",
    "    \n",
    "    def linear_projection(self, historical_data, days_forward):\n",
    "        \"\"\"\n",
    "        Simple linear extrapolation\n",
    "        historical_data: list of (timestamp, throughput) tuples\n",
    "        \"\"\"\n",
    "        timestamps = [x[0] for x in historical_data]\n",
    "        throughputs = [x[1] for x in historical_data]\n",
    "        \n",
    "        # Linear regression\n",
    "        slope, intercept = np.polyfit(timestamps, throughputs, 1)\n",
    "        \n",
    "        future_timestamp = timestamps[-1] + (days_forward * 86400)  # seconds\n",
    "        projected_throughput = slope * future_timestamp + intercept\n",
    "        \n",
    "        return {\n",
    "            'current_throughput': throughputs[-1],\n",
    "            'projected_throughput': projected_throughput,\n",
    "            'growth_rate_per_day': slope * 86400,\n",
    "            'days_until_saturation': None  # Calculate based on max capacity\n",
    "        }\n",
    "    \n",
    "    def seasonal_adjustment(self, data, seasonality='weekly'):\n",
    "        \"\"\"\n",
    "        Adjust for weekly patterns (weekday vs weekend traffic)\n",
    "        \"\"\"\n",
    "        if seasonality == 'weekly':\n",
    "            # Decompose into trend + seasonal + residual\n",
    "            from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "            \n",
    "            # Assuming daily data points\n",
    "            result = seasonal_decompose([x[1] for x in data], model='additive', period=7)\n",
    "            \n",
    "            return {\n",
    "                'trend': result.trend,\n",
    "                'seasonal': result.seasonal,\n",
    "                'residual': result.resid,\n",
    "                'forecast': result.trend[-1] + result.seasonal[-7]  # Same day of week\n",
    "            }\n",
    "    \n",
    "    def headroom_calculation(self, current_load, max_capacity, target_headroom=0.3):\n",
    "        \"\"\"\n",
    "        Calculate remaining capacity with safety margin\n",
    "        \n",
    "        target_headroom: 0.3 = 30% headroom recommended\n",
    "        \"\"\"\n",
    "        utilized = current_load / max_capacity\n",
    "        remaining = 1 - utilized\n",
    "        headroom_ok = remaining > target_headroom\n",
    "        \n",
    "        months_until_full = None\n",
    "        if 'growth_rate_per_day' in dir(self):\n",
    "            daily_growth = self.growth_rate_per_day\n",
    "            if daily_growth > 0:\n",
    "                days = (max_capacity - current_load) / daily_growth\n",
    "                months_until_full = days / 30\n",
    "        \n",
    "        return {\n",
    "            'current_utilization': utilized * 100,\n",
    "            'remaining_capacity_percent': remaining * 100,\n",
    "            'headroom_adequate': headroom_ok,\n",
    "            'recommended_action': 'SCALE_SOON' if remaining < target_headroom else 'MONITOR',\n",
    "            'estimated_months_until_full': months_until_full\n",
    "        }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **36.5 Cost-Performance Optimization**\n",
    "\n",
    "### **36.5.1 Cost-Performance Ratio Analysis**\n",
    "\n",
    "Optimize the balance between cloud spend and application performance:\n",
    "\n",
    "```python\n",
    "class CostPerformanceOptimizer:\n",
    "    \"\"\"\n",
    "    Analyze cost vs performance trade-offs\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, instance_pricing):\n",
    "        \"\"\"\n",
    "        instance_pricing: dict of instance_type -> hourly_cost\n",
    "        \"\"\"\n",
    "        self.pricing = instance_pricing\n",
    "    \n",
    "    def calculate_cost_per_request(self, instance_type, throughput, hourly_cost):\n",
    "        \"\"\"\n",
    "        Cost efficiency metric: $ per 1M requests\n",
    "        \"\"\"\n",
    "        cost_per_hour = hourly_cost\n",
    "        requests_per_hour = throughput * 3600\n",
    "        \n",
    "        if requests_per_hour == 0:\n",
    "            return float('inf')\n",
    "        \n",
    "        cost_per_million = (cost_per_hour / requests_per_hour) * 1_000_000\n",
    "        \n",
    "        return {\n",
    "            'instance': instance_type,\n",
    "            'cost_per_hour': cost_per_hour,\n",
    "            'throughput_per_hour': requests_per_hour,\n",
    "            'cost_per_million_requests': cost_per_million,\n",
    "            'efficiency_score': 1 / cost_per_million  # Higher is better\n",
    "        }\n",
    "    \n",
    "    def find_sweet_spot(self, test_results):\n",
    "        \"\"\"\n",
    "        Find instance type with best cost/performance ratio\n",
    "        \n",
    "        test_results: list of dicts with 'instance_type', 'throughput', 'p95_latency'\n",
    "        \"\"\"\n",
    "        analyses = []\n",
    "        \n",
    "        for result in test_results:\n",
    "            if result['instance_type'] not in self.pricing:\n",
    "                continue\n",
    "                \n",
    "            cost_analysis = self.calculate_cost_per_request(\n",
    "                result['instance_type'],\n",
    "                result['throughput'],\n",
    "                self.pricing[result['instance_type']]\n",
    "            )\n",
    "            \n",
    "            # Score combining performance (lower latency = better) and cost\n",
    "            # Normalize both to 0-1 scale\n",
    "            latency_score = 1 / (result['p95_latency'] / 1000)  # Convert ms to seconds, invert\n",
    "            cost_score = cost_analysis['efficiency_score']\n",
    "            \n",
    "            composite_score = (latency_score * 0.6) + (cost_score * 0.4)  # Weight performance higher\n",
    "            \n",
    "            analyses.append({\n",
    "                **cost_analysis,\n",
    "                'p95_latency': result['p95_latency'],\n",
    "                'composite_score': composite_score\n",
    "            })\n",
    "        \n",
    "        # Sort by composite score descending\n",
    "        analyses.sort(key=lambda x: x['composite_score'], reverse=True)\n",
    "        \n",
    "        return {\n",
    "            'recommendation': analyses[0] if analyses else None,\n",
    "            'rankings': analyses,\n",
    "            'savings_potential': self._calculate_savings(analyses)\n",
    "        }\n",
    "    \n",
    "    def _calculate_savings(self, analyses):\n",
    "        if len(analyses) < 2:\n",
    "            return 0\n",
    "        \n",
    "        best = analyses[0]\n",
    "        current = analyses[-1]  # Assuming last is current production\n",
    "        \n",
    "        savings_percent = ((current['cost_per_million_requests'] - best['cost_per_million_requests']) / \n",
    "                          current['cost_per_million_requests']) * 100\n",
    "        \n",
    "        return {\n",
    "            'percent': savings_percent,\n",
    "            'annual_savings_estimate': (current['cost_per_hour'] - best['cost_per_hour']) * 24 * 365\n",
    "        }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **36.6 Advanced Visualization**\n",
    "\n",
    "### **36.6.1 Latency Heatmaps**\n",
    "\n",
    "Heatmaps show latency distribution over time, revealing patterns:\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def create_latency_heatmap(timestamps, latencies, bucket_size=100):\n",
    "    \"\"\"\n",
    "    Create time-series heatmap of latency distribution\n",
    "    \n",
    "    X-axis: Time\n",
    "    Y-axis: Latency buckets\n",
    "    Color: Frequency (log scale)\n",
    "    \"\"\"\n",
    "    # Create 2D histogram\n",
    "    latency_buckets = np.linspace(0, np.percentile(latencies, 99), 50)\n",
    "    time_buckets = np.linspace(min(timestamps), max(timestamps), 50)\n",
    "    \n",
    "    H, xedges, yedges = np.histogram2d(timestamps, latencies, bins=[time_buckets, latency_buckets])\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Log scale for better visibility\n",
    "    im = ax.imshow(H.T, origin='lower', aspect='auto', \n",
    "                   extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]],\n",
    "                   cmap='YlOrRd', norm=plt.matplotlib.colors.LogNorm())\n",
    "    \n",
    "    ax.set_xlabel('Time')\n",
    "    ax.set_ylabel('Latency (ms)')\n",
    "    ax.set_title('Latency Distribution Heatmap Over Time')\n",
    "    \n",
    "    plt.colorbar(im, label='Request Count (log scale)')\n",
    "    \n",
    "    # Add percentile lines\n",
    "    ax.axhline(y=np.percentile(latencies, 95), color='red', linestyle='--', label='P95')\n",
    "    ax.axhline(y=np.percentile(latencies, 99), color='darkred', linestyle='--', label='P99')\n",
    "    ax.legend()\n",
    "    \n",
    "    plt.savefig('latency_heatmap.png')\n",
    "    return fig\n",
    "```\n",
    "\n",
    "### **36.6.2 Flame Graphs for Profiling**\n",
    "\n",
    "Visualize where time is spent in the application:\n",
    "\n",
    "```python\n",
    "# Conceptual flame graph data generation\n",
    "def generate_flamegraph_data(cpu_samples):\n",
    "    \"\"\"\n",
    "    Convert CPU profiling samples to flame graph format\n",
    "    \n",
    "    Format: func1;func2;func3 count\n",
    "    \"\"\"\n",
    "    stacks = {}\n",
    "    \n",
    "    for sample in cpu_samples:\n",
    "        stack = ';'.join(reversed(sample['call_stack']))  # Root to leaf\n",
    "        stacks[stack] = stacks.get(stack, 0) + sample['count']\n",
    "    \n",
    "    # Output in flame graph format\n",
    "    with open('flamegraph.txt', 'w') as f:\n",
    "        for stack, count in sorted(stacks.items(), key=lambda x: -x[1]):\n",
    "            f.write(f\"{stack} {count}\\n\")\n",
    "    \n",
    "    # Use Brendan Gregg's flamegraph.pl or online tools to generate SVG\n",
    "    print(\"Run: flamegraph.pl flamegraph.txt > flamegraph.svg\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **36.7 Root Cause Analysis**\n",
    "\n",
    "### **36.7.1 Performance Regression Bisection**\n",
    "\n",
    "When performance degrades between versions, use binary search to find the culprit commit:\n",
    "\n",
    "```python\n",
    "class PerformanceBisector:\n",
    "    \"\"\"\n",
    "    Git bisect for performance regressions\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, good_commit, bad_commit, test_runner):\n",
    "        self.good = good_commit\n",
    "        self.bad = bad_commit\n",
    "        self.test = test_runner\n",
    "        self.commits = self._get_commits_between(good_commit, bad_commit)\n",
    "        \n",
    "    def _get_commits_between(self, good, bad):\n",
    "        # Git command: git log --oneline good..bad\n",
    "        import subprocess\n",
    "        result = subprocess.run(['git', 'log', '--format=%H', f'{good}..{bad}'], \n",
    "                                capture_output=True, text=True)\n",
    "        return result.stdout.strip().split('\\n')\n",
    "    \n",
    "    def bisect(self):\n",
    "        \"\"\"\n",
    "        Binary search through commits to find regression\n",
    "        \"\"\"\n",
    "        left = 0\n",
    "        right = len(self.commits) - 1\n",
    "        \n",
    "        while left < right:\n",
    "            mid = (left + right) // 2\n",
    "            commit = self.commits[mid]\n",
    "            \n",
    "            print(f\"Testing commit {commit[:8]}...\")\n",
    "            \n",
    "            # Checkout and test\n",
    "            self._checkout(commit)\n",
    "            result = self.test.run()\n",
    "            \n",
    "            if result.is_good():\n",
    "                # Regression is in right half\n",
    "                left = mid + 1\n",
    "                print(\"  -> Good\")\n",
    "            else:\n",
    "                # Regression is in left half (including this)\n",
    "                right = mid\n",
    "                print(\"  -> Bad\")\n",
    "        \n",
    "        culprit = self.commits[left]\n",
    "        print(f\"First bad commit: {culprit[:8]}\")\n",
    "        print(f\"Author: {self._get_author(culprit)}\")\n",
    "        print(f\"Message: {self._get_message(culprit)}\")\n",
    "        \n",
    "        return culprit\n",
    "    \n",
    "    def _checkout(self, commit):\n",
    "        subprocess.run(['git', 'checkout', commit])\n",
    "        \n",
    "    def _get_author(self, commit):\n",
    "        result = subprocess.run(['git', 'log', '-1', '--format=%an', commit], \n",
    "                                capture_output=True, text=True)\n",
    "        return result.stdout.strip()\n",
    "```\n",
    "\n",
    "### **36.7.2 Correlation Analysis**\n",
    "\n",
    "Identify which metrics correlate with performance degradation:\n",
    "\n",
    "```python\n",
    "def correlation_analysis(metrics_dict, target_metric='latency'):\n",
    "    \"\"\"\n",
    "    Find which system metrics correlate with latency spikes\n",
    "    \n",
    "    metrics_dict: {\n",
    "        'timestamp': [...],\n",
    "        'latency': [...],\n",
    "        'cpu_usage': [...],\n",
    "        'memory_usage': [...],\n",
    "        'disk_io': [...],\n",
    "        'gc_time': [...]\n",
    "    }\n",
    "    \"\"\"\n",
    "    import pandas as pd\n",
    "    \n",
    "    df = pd.DataFrame(metrics_dict)\n",
    "    \n",
    "    # Calculate correlation with target\n",
    "    correlations = df.corr()[target_metric].sort_values(ascending=False)\n",
    "    \n",
    "    # Remove self-correlation\n",
    "    correlations = correlations[correlations.index != target_metric]\n",
    "    \n",
    "    print(\"Correlation with latency (higher = stronger relationship):\")\n",
    "    print(correlations)\n",
    "    \n",
    "    # Identify primary suspects\n",
    "    strong_correlations = correlations[abs(correlations) > 0.7]\n",
    "    \n",
    "    return {\n",
    "        'primary_factors': strong_correlations.to_dict(),\n",
    "        'recommendation': 'Investigate ' + ', '.join(strong_correlations.index.tolist())\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "**Statistical Analysis (36.1):**\n",
    "- **Coefficient of Variation (CV)**: Measure consistency; CV > 30% indicates system instability\n",
    "- **Confidence Intervals**: Always report 95% CI with mean values; avoids false precision\n",
    "- **Statistical Significance**: Use Welch's t-test (p < 0.05) to confirm improvements are real, not noise\n",
    "- **Sample Size**: Minimum 30 samples for basic tests, 100+ for high-confidence comparisons\n",
    "\n",
    "**Latency Modeling (36.2):**\n",
    "- **Little's Law**: **L = \u03bb \u00d7 W** \u2014 Fundamental relationship between concurrency, throughput, and response time\n",
    "- **Saturation Point**: The \"knee\" where linear scaling ends; operate at 80% of this value\n",
    "- **USL (Universal Scalability Law)**: Models contention and coherence costs; predicts maximum theoretical throughput\n",
    "\n",
    "**Comparative Analysis (36.3):**\n",
    "- **A/B Testing**: Use Mann-Whitney U test for latency distributions (non-parametric)\n",
    "- **Canary Analysis**: Automated rollback triggers: P99 regression > 20% or error rate increase > 0.1%\n",
    "- **Sequential Testing**: Early stopping when statistical significance reached\u2014saves test time\n",
    "\n",
    "**Capacity Planning (36.4):**\n",
    "- **Headroom**: Maintain 30% capacity headroom for traffic spikes and failure domains\n",
    "- **Seasonality**: Adjust growth projections for weekly/monthly patterns (weekday vs weekend)\n",
    "- **Extrapolation Limits**: USL predictions become unreliable beyond 2x tested load\n",
    "\n",
    "**Cost-Performance (36.5):**\n",
    "- **Cost per Million Requests**: Normalize efficiency across instance types\n",
    "- **Sweet Spot**: Often not the fastest or cheapest option, but the best ratio (e.g., c5.2xlarge vs c5.4xlarge)\n",
    "- **Autoscaling**: Cost optimization through dynamic scaling vs. always-on capacity\n",
    "\n",
    "**Visualization (36.6):**\n",
    "- **Heatmaps**: Reveal time-based patterns (hourly spikes, GC pauses) invisible in averages\n",
    "- **Flame Graphs**: Identify hot code paths consuming CPU cycles\n",
    "- **Percentile Charts**: Show distribution shape and \"hockey stick\" tail latency\n",
    "\n",
    "**Root Cause Analysis (36.7):**\n",
    "- **Git Bisect**: O(log n) algorithm to find regressing commits; automate in CI/CD\n",
    "- **Correlation Analysis**: Statistical correlation identifies likely culprits (CPU vs. Disk vs. GC)\n",
    "- **Causal Analysis**: Correlation \u2260 causation; verify with controlled experiments\n",
    "\n",
    "**Critical Metrics for Executive Reporting:**\n",
    "- **P95 Latency Trend**: Direction over time (improving/degrading)\n",
    "- **Cost Efficiency**: $ per million requests (normalized for business growth)\n",
    "- **Headroom Months**: How long until capacity exhaustion at current growth rates\n",
    "- **Regression Detection Time**: Mean time to detect performance issues in production\n",
    "\n",
    "---\n",
    "\n",
    "## **\ud83d\udcd6 Next Chapter: Chapter 37 - Security Testing Fundamentals**\n",
    "\n",
    "Now that you have mastered the analytical techniques for performance optimization, **Chapter 37** will transition you to **Security Testing**\u2014ensuring your applications are resilient against malicious attacks while maintaining the performance characteristics you've optimized.\n",
    "\n",
    "In **Chapter 37**, you will master:\n",
    "\n",
    "- **Security Testing Principles**: The CIA triad (Confidentiality, Integrity, Availability), defense in depth, and least privilege\n",
    "- **Common Vulnerabilities**: OWASP Top 10 in detail\u2014Injection, Broken Authentication, Sensitive Data Exposure, XML External Entities (XXE), Broken Access Control, Security Misconfiguration, Cross-Site Scripting (XSS), Insecure Deserialization, Using Components with Known Vulnerabilities, and Insufficient Logging\n",
    "- **Threat Modeling**: STRIDE methodology, attack trees, and risk assessment frameworks\n",
    "- **Security Testing Types**: SAST (Static Application Security Testing), DAST (Dynamic Application Security Testing), IAST (Interactive), and SCA (Software Composition Analysis)\n",
    "- **Authentication & Authorization Testing**: Session management, token security, JWT testing, and privilege escalation\n",
    "- **Input Validation**: Fuzzing, boundary testing, and injection attack prevention\n",
    "- **Security Requirements**: Translating compliance requirements (PCI-DSS, GDPR, HIPAA) into testable security controls\n",
    "\n",
    "This chapter will provide the **foundation for ethical hacking** and security validation, teaching you to think like an attacker while maintaining the performance and functionality your users expect.\n",
    "\n",
    "**Continue to Chapter 37 to learn how to protect your applications from security threats!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='35. performance_test_execution.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../9. security_testing/37. security_testing_fundamentals.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}