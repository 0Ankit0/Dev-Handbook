{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 33: Performance Testing Fundamentals**\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "Performance testing is the practice of determining how a system responds under a particular workload. Unlike functional testing\u2014which verifies *what* the system does\u2014performance testing evaluates *how well* the system does it under stress, load, and varying conditions.\n",
    "\n",
    "In modern software engineering, performance is not a luxury but a requirement. A study by Google found that 53% of mobile site visitors leave a page that takes longer than three seconds to load. Similarly, Amazon discovered that every 100ms of latency cost them 1% in sales. These statistics underscore why performance testing is critical before releasing applications to production.\n",
    "\n",
    "This chapter establishes the foundational knowledge required to design, execute, and analyze performance tests, ensuring your applications meet user expectations and business requirements for speed, stability, and scalability.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.1 What is Performance Testing?**\n",
    "\n",
    "### **Formal Definition**\n",
    "> **Performance Testing** is a type of non-functional testing that evaluates the speed, responsiveness, stability, and resource utilization of a software application under a specific workload.\n",
    "\n",
    "### **Why Performance Testing Matters**\n",
    "\n",
    "**Business Impact:**\n",
    "- **User Experience**: Slow applications drive users to competitors\n",
    "- **Revenue**: Performance directly correlates with conversion rates\n",
    "- **Operational Costs**: Inefficient code requires more infrastructure\n",
    "- **Reputation**: Performance failures often make headlines (e.g., \"Website crashes during Black Friday sale\")\n",
    "\n",
    "**Technical Necessity:**\n",
    "- **Scalability Validation**: Will the system handle 10x traffic during a viral event?\n",
    "- **Bottleneck Identification**: Which component fails first under load?\n",
    "- **Capacity Planning**: How many servers do we actually need?\n",
    "- **Regression Prevention**: Did the new feature slow down the checkout process?\n",
    "\n",
    "### **Performance Testing vs. Functional Testing**\n",
    "\n",
    "| Aspect | Functional Testing | Performance Testing |\n",
    "|--------|-------------------|---------------------|\n",
    "| **Focus** | Correctness of features | Speed, scalability, stability |\n",
    "| **Pass Criteria** | Feature works (Boolean) | Response time < 2 seconds (Metric) |\n",
    "| **Environment** | Single user, isolated | Multi-user, production-like |\n",
    "| **Data** | Controlled, minimal | Realistic, high volume |\n",
    "| **Tools** | Selenium, JUnit | JMeter, Gatling, LoadRunner |\n",
    "| **Timing** | Shift-Left (early) | Usually after functional stability |\n",
    "\n",
    "---\n",
    "\n",
    "## **33.2 Types of Performance Testing**\n",
    "\n",
    "Different performance test types answer different questions about your system's behavior. Understanding when to apply each is crucial for effective quality assurance.\n",
    "\n",
    "### **33.2.1 Load Testing**\n",
    "\n",
    "**Purpose**: Verify that the system can handle expected user loads while maintaining acceptable response times.\n",
    "\n",
    "**Key Questions Answered**:\n",
    "- Can our e-commerce site handle 10,000 concurrent users during a sale?\n",
    "- Does response time remain under 2 seconds at peak load?\n",
    "- At what point does the database connection pool exhaust?\n",
    "\n",
    "**Characteristics**:\n",
    "- Tests with **expected** or **peak** load levels\n",
    "- Gradual ramp-up to target concurrency\n",
    "- Sustained duration (typically 30 minutes to several hours)\n",
    "- Validates SLAs (Service Level Agreements)\n",
    "\n",
    "```python\n",
    "# Conceptual Load Test Pattern\n",
    "class LoadTestScenario:\n",
    "    \"\"\"\n",
    "    Simulates expected user load\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.target_concurrent_users = 1000\n",
    "        self.ramp_up_time = 600  # 10 minutes to reach target\n",
    "        self.sustained_duration = 3600  # 1 hour steady state\n",
    "        \n",
    "    def execute(self):\n",
    "        # Gradual ramp-up prevents thundering herd\n",
    "        for minute in range(0, self.ramp_up_time, 60):\n",
    "            users_to_add = self.target_concurrent_users / (self.ramp_up_time / 60)\n",
    "            self.add_users(users_to_add)\n",
    "            time.sleep(60)\n",
    "            \n",
    "        # Sustained load period\n",
    "        time.sleep(self.sustained_duration)\n",
    "        \n",
    "        # Gradual ramp-down\n",
    "        self.remove_users_gradually()\n",
    "```\n",
    "\n",
    "**Industry Example**: Testing a banking portal with 500 concurrent users performing typical transactions (balance checks, transfers, bill payments) for 4 hours to ensure no memory leaks occur during sustained operation.\n",
    "\n",
    "### **33.2.2 Stress Testing**\n",
    "\n",
    "**Purpose**: Determine the breaking point of the system and observe how it fails.\n",
    "\n",
    "**Key Questions Answered**:\n",
    "- What is the maximum capacity before the system crashes?\n",
    "- Does the system degrade gracefully or fail catastrophically?\n",
    "- Can we recover automatically when load returns to normal?\n",
    "\n",
    "**Characteristics**:\n",
    "- Loads **beyond** expected maximum capacity (120%, 150%, 200%)\n",
    "- Rapid ramp-up to find breaking points\n",
    "- Continues until errors occur or system crashes\n",
    "- Tests error handling and recovery mechanisms\n",
    "\n",
    "```python\n",
    "# Stress Test Pattern\n",
    "class StressTestScenario:\n",
    "    \"\"\"\n",
    "    Pushes system beyond limits to find breaking point\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.normal_capacity = 1000\n",
    "        self.max_test_capacity = 3000  # 3x normal\n",
    "        self.increment = 200\n",
    "        self.step_duration = 300  # 5 minutes per step\n",
    "        \n",
    "    def execute(self):\n",
    "        current_load = self.normal_capacity\n",
    "        \n",
    "        while current_load <= self.max_test_capacity:\n",
    "            print(f\"Testing with {current_load} users...\")\n",
    "            \n",
    "            # Apply load\n",
    "            self.set_load(current_load)\n",
    "            time.sleep(self.step_duration)\n",
    "            \n",
    "            # Measure metrics\n",
    "            metrics = self.collect_metrics()\n",
    "            \n",
    "            # Check for failure indicators\n",
    "            if metrics.error_rate > 0.05 or metrics.response_time > 10:\n",
    "                print(f\"System stressed at {current_load} users\")\n",
    "                print(f\"Error rate: {metrics.error_rate}\")\n",
    "                print(f\"Response time: {metrics.response_time}s\")\n",
    "                break\n",
    "                \n",
    "            current_load += self.increment\n",
    "        \n",
    "        # Recovery test\n",
    "        self.test_recovery()\n",
    "```\n",
    "\n",
    "**Industry Example**: A streaming service increasing load during the finale of a popular show until servers begin dropping connections, verifying that users receive proper error messages rather than infinite loading spinners.\n",
    "\n",
    "### **33.2.3 Spike Testing**\n",
    "\n",
    "**Purpose**: Evaluate system behavior under sudden, extreme increases in load.\n",
    "\n",
    "**Key Questions Answered**:\n",
    "- Can our ticketing website handle the moment tickets go on sale?\n",
    "- Will the API gateway crash when a mobile push notification is sent to 1 million devices simultaneously?\n",
    "- How quickly can the auto-scaling group provision new instances?\n",
    "\n",
    "**Characteristics**:\n",
    "- **Immediate** jump from baseline to peak (seconds, not minutes)\n",
    "- Short duration spike (5-15 minutes typically)\n",
    "- Tests auto-scaling, circuit breakers, and rate limiting\n",
    "- Validates handling of \"thundering herd\" problems\n",
    "\n",
    "```python\n",
    "# Spike Test Pattern\n",
    "class SpikeTestScenario:\n",
    "    \"\"\"\n",
    "    Simulates sudden traffic surge\n",
    "    \"\"\"\n",
    "    def execute(self):\n",
    "        # Baseline load\n",
    "        self.set_load(100)\n",
    "        time.sleep(600)  # 10 minutes baseline\n",
    "        \n",
    "        # Sudden spike\n",
    "        print(\"SPIKE: Increasing to 10,000 users instantly\")\n",
    "        self.set_load(10000)  # Instant jump\n",
    "        \n",
    "        # Monitor for 15 minutes\n",
    "        start = time.time()\n",
    "        while time.time() - start < 900:\n",
    "            metrics = self.collect_metrics()\n",
    "            self.log_metrics(metrics)\n",
    "            \n",
    "            # Check if system stabilizes\n",
    "            if metrics.response_time < 2.0 and metrics.error_rate < 0.01:\n",
    "                print(\"System stabilized under spike\")\n",
    "                \n",
    "        # Return to baseline\n",
    "        self.set_load(100)\n",
    "```\n",
    "\n",
    "**Industry Example**: An e-commerce site experiencing a 50x traffic spike when a celebrity mentions their product on social media. The test verifies that the caching layer activates and the database doesn't crash under the sudden query barrage.\n",
    "\n",
    "### **33.2.4 Endurance Testing (Soak Testing)**\n",
    "\n",
    "**Purpose**: Identify memory leaks, connection pool exhaustion, and degradation over extended periods.\n",
    "\n",
    "**Key Questions Answered**:\n",
    "- Does the application leak memory over 72 hours of operation?\n",
    "- Do database connections get properly released, or do we eventually run out?\n",
    "- Does log file growth eventually fill the disk?\n",
    "\n",
    "**Characteristics**:\n",
    "- Extended duration (8 hours to several days)\n",
    "- **Sustained** average load (not peak, not minimal)\n",
    "- Monitors resource utilization trends over time\n",
    "- Often run over weekends in pre-production\n",
    "\n",
    "```python\n",
    "# Endurance Test Monitoring\n",
    "class EnduranceTest:\n",
    "    \"\"\"\n",
    "    Long-running stability test\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.duration_hours = 72\n",
    "        self.check_interval = 300  # Check every 5 minutes\n",
    "        \n",
    "    def monitor_trends(self):\n",
    "        \"\"\"\n",
    "        Track metrics over time to detect degradation\n",
    "        \"\"\"\n",
    "        memory_readings = []\n",
    "        response_time_readings = []\n",
    "        \n",
    "        start_time = time.time()\n",
    "        end_time = start_time + (self.duration_hours * 3600)\n",
    "        \n",
    "        while time.time() < end_time:\n",
    "            # Collect current metrics\n",
    "            current_memory = self.get_memory_usage()\n",
    "            current_response = self.get_avg_response_time()\n",
    "            \n",
    "            memory_readings.append((time.time(), current_memory))\n",
    "            response_time_readings.append((time.time(), current_response))\n",
    "            \n",
    "            # Check for memory leak trend\n",
    "            if len(memory_readings) > 12:  # After 1 hour\n",
    "                self.check_memory_leak_trend(memory_readings)\n",
    "                \n",
    "            # Check for response time degradation\n",
    "            if len(response_time_readings) > 12:\n",
    "                self.check_response_degradation(response_time_readings)\n",
    "                \n",
    "            time.sleep(self.check_interval)\n",
    "    \n",
    "    def check_memory_leak_trend(self, readings):\n",
    "        \"\"\"\n",
    "        Linear regression to detect upward memory trend\n",
    "        \"\"\"\n",
    "        # Simplified: Check if last hour average > first hour average by >10%\n",
    "        first_hour_avg = sum(r[1] for r in readings[:12]) / 12\n",
    "        last_hour_avg = sum(r[1] for r in readings[-12:]) / 12\n",
    "        \n",
    "        increase_percent = ((last_hour_avg - first_hour_avg) / first_hour_avg) * 100\n",
    "        \n",
    "        if increase_percent > 10:\n",
    "            print(f\"WARNING: Potential memory leak detected\")\n",
    "            print(f\"Memory increased {increase_percent:.2f}% over test duration\")\n",
    "```\n",
    "\n",
    "**Industry Example**: A banking application running for 48 hours with 200 concurrent users performing transactions continuously. The test discovers that the application log grows unbounded and eventually crashes the server when the disk fills\u2014a critical finding before production deployment.\n",
    "\n",
    "### **33.2.5 Scalability Testing**\n",
    "\n",
    "**Purpose**: Determine the system's ability to scale up (vertical) or scale out (horizontal) to handle increased load.\n",
    "\n",
    "**Key Questions Answered**:\n",
    "- If we double the CPU cores, does throughput double (linear scaling)?\n",
    "- Does adding database read replicas reduce query latency?\n",
    "- What is the cost per transaction as we scale?\n",
    "\n",
    "**Characteristics**:\n",
    "- Incremental load increases with corresponding resource monitoring\n",
    "- Correlates user load with resource utilization\n",
    "- Identifies scalability bottlenecks (e.g., database locks preventing linear scaling)\n",
    "- Often tests cloud auto-scaling policies\n",
    "\n",
    "```python\n",
    "# Scalability Analysis\n",
    "class ScalabilityTest:\n",
    "    \"\"\"\n",
    "    Measures how system performance changes with resources\n",
    "    \"\"\"\n",
    "    def test_horizontal_scaling(self):\n",
    "        \"\"\"\n",
    "        Test adding more application servers\n",
    "        \"\"\"\n",
    "        results = []\n",
    "        \n",
    "        for server_count in [1, 2, 4, 8]:\n",
    "            # Configure infrastructure\n",
    "            self.scale_to_n_servers(server_count)\n",
    "            time.sleep(300)  # Wait for stabilization\n",
    "            \n",
    "            # Find max throughput for this configuration\n",
    "            max_users = self.find_saturation_point()\n",
    "            throughput = self.measure_throughput(max_users)\n",
    "            \n",
    "            results.append({\n",
    "                'servers': server_count,\n",
    "                'max_users': max_users,\n",
    "                'throughput': throughput,\n",
    "                'efficiency': throughput / server_count  # Ideal: constant\n",
    "            })\n",
    "            \n",
    "        # Analyze scaling efficiency\n",
    "        self.analyze_scaling_results(results)\n",
    "    \n",
    "    def analyze_scaling_results(self, results):\n",
    "        \"\"\"\n",
    "        Check if scaling is linear, sub-linear, or super-linear\n",
    "        \"\"\"\n",
    "        baseline = results[0]\n",
    "        \n",
    "        for result in results[1:]:\n",
    "            expected_throughput = baseline['throughput'] * (result['servers'] / baseline['servers'])\n",
    "            actual_throughput = result['throughput']\n",
    "            \n",
    "            efficiency = (actual_throughput / expected_throughput) * 100\n",
    "            \n",
    "            print(f\"Servers: {result['servers']}\")\n",
    "            print(f\"Expected throughput: {expected_throughput}\")\n",
    "            print(f\"Actual throughput: {actual_throughput}\")\n",
    "            print(f\"Scaling efficiency: {efficiency:.1f}%\")\n",
    "            \n",
    "            if efficiency < 80:\n",
    "                print(\"WARNING: Sub-linear scaling detected\")\n",
    "                print(\"Likely bottleneck: Database contention or network saturation\")\n",
    "```\n",
    "\n",
    "**Industry Example**: A microservices architecture testing reveals that while adding container instances increases throughput linearly up to 10 instances, beyond that point throughput plateaus due to database connection limits\u2014indicating the need for database connection pooling or sharding before further scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.3 Performance Metrics and KPIs**\n",
    "\n",
    "Quantifying performance requires precise measurement. These metrics form the language of performance engineering.\n",
    "\n",
    "### **33.3.1 Response Time Metrics**\n",
    "\n",
    "**Response Time (Latency)**: The total time between sending a request and receiving a response.\n",
    "\n",
    "**Key Measurements**:\n",
    "- **Average Response Time**: Mean of all response times (can be misleading with outliers)\n",
    "- **Median (P50)**: 50th percentile\u2014half of requests are faster, half slower\n",
    "- **P95 (95th Percentile)**: 95% of requests are faster than this value\n",
    "- **P99 (99th Percentile)**: 99% of requests are faster; critical for SLAs\n",
    "- **Max Response Time**: Worst-case scenario (often includes timeouts)\n",
    "\n",
    "```python\n",
    "# Response time analysis\n",
    "import statistics\n",
    "\n",
    "class ResponseTimeAnalyzer:\n",
    "    def analyze(self, response_times):\n",
    "        \"\"\"\n",
    "        Calculate comprehensive response time statistics\n",
    "        response_times: list of response times in milliseconds\n",
    "        \"\"\"\n",
    "        sorted_times = sorted(response_times)\n",
    "        n = len(sorted_times)\n",
    "        \n",
    "        metrics = {\n",
    "            'count': n,\n",
    "            'mean': statistics.mean(sorted_times),\n",
    "            'median': statistics.median(sorted_times),\n",
    "            'min': min(sorted_times),\n",
    "            'max': max(sorted_times),\n",
    "            'std_dev': statistics.stdev(sorted_times) if n > 1 else 0,\n",
    "            'p95': self._percentile(sorted_times, 95),\n",
    "            'p99': self._percentile(sorted_times, 99),\n",
    "            'p999': self._percentile(sorted_times, 99.9)\n",
    "        }\n",
    "        \n",
    "        # Industry interpretation\n",
    "        self._interpret_metrics(metrics)\n",
    "        return metrics\n",
    "    \n",
    "    def _percentile(self, sorted_data, percentile):\n",
    "        \"\"\"Calculate percentile from sorted data\"\"\"\n",
    "        index = int(len(sorted_data) * (percentile / 100))\n",
    "        return sorted_data[min(index, len(sorted_data) - 1)]\n",
    "    \n",
    "    def _interpret_metrics(self, metrics):\n",
    "        \"\"\"\n",
    "        Provide industry-standard interpretation\n",
    "        \"\"\"\n",
    "        print(\"Performance Analysis:\")\n",
    "        print(f\"Mean: {metrics['mean']:.2f}ms\")\n",
    "        print(f\"Median (P50): {metrics['median']:.2f}ms\")\n",
    "        print(f\"P95: {metrics['p95']:.2f}ms (95% of users experience this or faster)\")\n",
    "        print(f\"P99: {metrics['p99']:.2f}ms (Worst 1% of requests)\")\n",
    "        \n",
    "        # Industry benchmarks\n",
    "        if metrics['p95'] < 200:\n",
    "            print(\"\u2713 EXCELLENT: P95 under 200ms\")\n",
    "        elif metrics['p95'] < 500:\n",
    "            print(\"\u2713 GOOD: P95 under 500ms\")\n",
    "        elif metrics['p95'] < 1000:\n",
    "            print(\"\u26a0 ACCEPTABLE: P95 under 1s\")\n",
    "        else:\n",
    "            print(\"\u2717 POOR: P95 exceeds 1s\")\n",
    "```\n",
    "\n",
    "**Industry Standard**: Google recommends aiming for **P95 < 200ms** for web applications, while **P99 < 1s** is often the minimum acceptable for e-commerce.\n",
    "\n",
    "### **33.3.2 Throughput Metrics**\n",
    "\n",
    "**Throughput**: The number of transactions or requests processed per unit of time.\n",
    "\n",
    "**Measurements**:\n",
    "- **Requests Per Second (RPS/RPM)**: API calls, HTTP requests\n",
    "- **Transactions Per Second (TPS)**: Database transactions, business operations\n",
    "- **Bits Per Second (BPS)**: Network throughput, data transfer rates\n",
    "- **Concurrent Users**: Simultaneous active users (not necessarily all making requests simultaneously)\n",
    "\n",
    "```python\n",
    "# Throughput calculation\n",
    "class ThroughputMetrics:\n",
    "    def calculate_tps(self, transaction_count, duration_seconds):\n",
    "        \"\"\"\n",
    "        Transactions Per Second\n",
    "        \"\"\"\n",
    "        return transaction_count / duration_seconds\n",
    "    \n",
    "    def calculate_hits_per_second(self, log_entries, time_window):\n",
    "        \"\"\"\n",
    "        From web server logs\n",
    "        \"\"\"\n",
    "        return len(log_entries) / time_window\n",
    "    \n",
    "    def saturation_point(self, load_levels):\n",
    "        \"\"\"\n",
    "        Find the point where throughput stops increasing with load\n",
    "        (indicates system bottleneck)\n",
    "        \"\"\"\n",
    "        max_throughput = 0\n",
    "        saturation_load = 0\n",
    "        \n",
    "        for load, throughput in sorted(load_levels.items()):\n",
    "            if throughput > max_throughput:\n",
    "                max_throughput = throughput\n",
    "                saturation_load = load\n",
    "            else:\n",
    "                # Throughput plateaued or decreased\n",
    "                print(f\"System saturates at {saturation_load} users\")\n",
    "                print(f\"Max throughput: {max_throughput} TPS\")\n",
    "                return saturation_load, max_throughput\n",
    "        \n",
    "        return saturation_load, max_throughput\n",
    "```\n",
    "\n",
    "### **33.3.3 Resource Utilization Metrics**\n",
    "\n",
    "**Server Metrics**:\n",
    "- **CPU Utilization**: Percentage of CPU capacity used (< 70% healthy, > 85% concerning)\n",
    "- **Memory Usage**: RAM consumption (watch for leaks trending upward)\n",
    "- **Disk I/O**: Reads/writes per second (IOPS), throughput (MB/s)\n",
    "- **Network I/O**: Bandwidth utilization, packet loss, latency\n",
    "- **Connection Pools**: Active/idle database connections\n",
    "\n",
    "**Database Metrics**:\n",
    "- **Query Execution Time**: Slow query logs\n",
    "- **Lock Waits**: Time spent waiting for row/table locks\n",
    "- **Cache Hit Ratio**: Percentage of queries served from cache vs. disk\n",
    "- **Index Usage**: Efficiency of database indexes\n",
    "\n",
    "```python\n",
    "# Resource monitoring\n",
    "import psutil\n",
    "import time\n",
    "\n",
    "class ResourceMonitor:\n",
    "    def __init__(self):\n",
    "        self.metrics_history = {\n",
    "            'cpu': [],\n",
    "            'memory': [],\n",
    "            'disk_io': [],\n",
    "            'network_io': []\n",
    "        }\n",
    "    \n",
    "    def sample(self):\n",
    "        \"\"\"\n",
    "        Collect current resource metrics\n",
    "        \"\"\"\n",
    "        cpu_percent = psutil.cpu_percent(interval=1)\n",
    "        memory = psutil.virtual_memory()\n",
    "        disk = psutil.disk_io_counters()\n",
    "        network = psutil.net_io_counters()\n",
    "        \n",
    "        timestamp = time.time()\n",
    "        \n",
    "        self.metrics_history['cpu'].append((timestamp, cpu_percent))\n",
    "        self.metrics_history['memory'].append((timestamp, memory.percent))\n",
    "        \n",
    "        return {\n",
    "            'timestamp': timestamp,\n",
    "            'cpu_percent': cpu_percent,\n",
    "            'memory_percent': memory.percent,\n",
    "            'memory_available_mb': memory.available / (1024 * 1024),\n",
    "            'disk_read_mb': disk.read_bytes / (1024 * 1024),\n",
    "            'disk_write_mb': disk.write_bytes / (1024 * 1024)\n",
    "        }\n",
    "    \n",
    "    def check_resource_health(self):\n",
    "        \"\"\"\n",
    "        Alert on concerning resource levels\n",
    "        \"\"\"\n",
    "        recent_cpu = [m[1] for m in self.metrics_history['cpu'][-10:]]\n",
    "        avg_cpu = sum(recent_cpu) / len(recent_cpu)\n",
    "        \n",
    "        if avg_cpu > 85:\n",
    "            print(f\"ALERT: High CPU utilization ({avg_cpu}%)\")\n",
    "            print(\"Recommendation: Scale horizontally or optimize code\")\n",
    "            \n",
    "        recent_memory = [m[1] for m in self.metrics_history['memory'][-10:]]\n",
    "        if recent_memory[-1] > 90:\n",
    "            print(f\"ALERT: Critical memory usage ({recent_memory[-1]}%)\")\n",
    "```\n",
    "\n",
    "### **33.3.4 Error Rate Metrics**\n",
    "\n",
    "**Key Measurements**:\n",
    "- **Error Rate**: Percentage of requests resulting in errors (HTTP 5xx, timeouts, exceptions)\n",
    "- **Success Rate**: Complementary to error rate (should be > 99.9% for critical systems)\n",
    "- **Error Types**: 4xx (client errors) vs 5xx (server errors) vs timeouts\n",
    "\n",
    "**Industry Standard**: \n",
    "- **99.9%** (three nines) uptime = 8.76 hours downtime/year\n",
    "- **99.99%** (four nines) = 52.56 minutes downtime/year\n",
    "- Error rate should typically be **< 0.1%** under normal load\n",
    "\n",
    "---\n",
    "\n",
    "## **33.4 Performance Testing Strategy**\n",
    "\n",
    "A successful performance testing program requires planning, not just execution.\n",
    "\n",
    "### **33.4.1 Defining SLAs (Service Level Agreements)**\n",
    "\n",
    "SLAs define acceptable performance boundaries. They must be **Specific, Measurable, Achievable, Relevant, and Time-bound (SMART)**.\n",
    "\n",
    "**Example SLA Document**:\n",
    "```yaml\n",
    "Performance_SLAs:\n",
    "  Web_Application:\n",
    "    Home_Page:\n",
    "      P95_Response_Time: \"< 200ms\"\n",
    "      P99_Response_Time: \"< 500ms\"\n",
    "      Availability: \"99.95%\"\n",
    "      Max_Concurrent_Users: 10000\n",
    "      \n",
    "  API_Gateway:\n",
    "    Authentication_Endpoint:\n",
    "      P95_Response_Time: \"< 100ms\"\n",
    "      Error_Rate: \"< 0.01%\"\n",
    "      Throughput: \"1000 TPS\"\n",
    "      \n",
    "  Database:\n",
    "    Query_Response_Time:\n",
    "      Simple_Queries: \"< 10ms\"\n",
    "      Complex_Joins: \"< 500ms\"\n",
    "      Connection_Pool_Utilization: \"< 80%\"\n",
    "```\n",
    "\n",
    "### **33.4.2 The Performance Testing Process**\n",
    "\n",
    "**Phase 1: Planning**\n",
    "1. **Identify Critical Scenarios**: Which user journeys are most important? (Login, Checkout, Search)\n",
    "2. **Determine Load Profiles**: Peak users, average users, growth projections\n",
    "3. **Establish Baselines**: Measure current performance before changes\n",
    "4. **Define Success Criteria**: Specific metric thresholds (P95 < 2s, etc.)\n",
    "\n",
    "**Phase 2: Script Development**\n",
    "1. **Record User Scenarios**: Use proxy tools to capture HTTP traffic\n",
    "2. **Parameterize Data**: Replace hardcoded values with variables (usernames, product IDs)\n",
    "3. **Add Correlation**: Extract dynamic values (session IDs, tokens) from responses\n",
    "4. **Implement Think Time**: Add realistic delays between user actions\n",
    "\n",
    "**Phase 3: Environment Setup**\n",
    "1. **Production-like Environment**: Hardware, network latency, database size should mirror production\n",
    "2. **Monitoring Setup**: APM tools (New Relic, Datadog), server metrics, logs\n",
    "3. **Test Data**: Realistic data volumes (database with 1M rows vs 1K rows performs differently)\n",
    "\n",
    "**Phase 4: Execution**\n",
    "1. **Baseline Test**: Single user to establish best-case performance\n",
    "2. **Incremental Load**: Step up load gradually to identify saturation points\n",
    "3. **Target Load**: Run at expected peak for sustained period\n",
    "4. **Stress Test**: Push beyond limits\n",
    "\n",
    "**Phase 5: Analysis**\n",
    "1. **Identify Bottlenecks**: Database slow? CPU bound? Memory leak?\n",
    "2. **Compare to Baselines**: Did the new code improve or degrade performance?\n",
    "3. **Generate Reports**: Executive summary (pass/fail) + technical details for developers\n",
    "\n",
    "### **33.4.3 Key Concepts in Performance Testing**\n",
    "\n",
    "**Think Time**: Realistic pause between user actions (e.g., 3-5 seconds between page clicks).\n",
    "\n",
    "**Pacing**: Controlled rate of transaction execution regardless of response time. If a transaction takes 2 seconds but pacing is set to 5 seconds, the tool waits 3 seconds before the next iteration.\n",
    "\n",
    "**Ramp-Up/Ramp-Down**: Gradual increase (warm-up) and decrease (cool-down) of virtual users to prevent sudden traffic shocks.\n",
    "\n",
    "**Correlation**: Extracting dynamic values from server responses to use in subsequent requests (e.g., extracting a session ID from a login response to use in the next request).\n",
    "\n",
    "**Parameterization**: Using data files to feed different values for each virtual user (e.g., 1000 different login credentials for 1000 virtual users).\n",
    "\n",
    "```python\n",
    "# Conceptual performance test script structure\n",
    "class PerformanceTestScript:\n",
    "    def __init__(self):\n",
    "        self.think_time_min = 2  # seconds\n",
    "        self.think_time_max = 5\n",
    "        self.pacing = 10  # Target 10 seconds per iteration\n",
    "        \n",
    "    def execute_iteration(self, user_credentials):\n",
    "        start_time = time.time()\n",
    "        \n",
    "        # Step 1: Login with correlation\n",
    "        response = self.http_post(\"/login\", {\n",
    "            \"username\": user_credentials['username'],\n",
    "            \"password\": user_credentials['password']\n",
    "        })\n",
    "        session_token = self.extract_token(response)  # Correlation\n",
    "        \n",
    "        self.think_time()\n",
    "        \n",
    "        # Step 2: Browse products\n",
    "        self.http_get(\"/products\", headers={\"Authorization\": session_token})\n",
    "        \n",
    "        self.think_time()\n",
    "        \n",
    "        # Step 3: Add to cart\n",
    "        product_id = random.choice(self.product_catalog)\n",
    "        self.http_post(\"/cart\", {\"product_id\": product_id}, \n",
    "                      headers={\"Authorization\": session_token})\n",
    "        \n",
    "        # Pacing control\n",
    "        elapsed = time.time() - start_time\n",
    "        if elapsed < self.pacing:\n",
    "            time.sleep(self.pacing - elapsed)\n",
    "    \n",
    "    def think_time(self):\n",
    "        \"\"\"Simulate user reading/thinking\"\"\"\n",
    "        time.sleep(random.uniform(self.think_time_min, self.think_time_max))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **33.5 Industry Standards and Best Practices**\n",
    "\n",
    "### **33.5.1 Apdex (Application Performance Index)**\n",
    "\n",
    "Apdex is an open standard for measuring user satisfaction with response times.\n",
    "\n",
    "**Formula**:\n",
    "- **Satisfied**: Response time \u2264 T (threshold, typically 1.5s)\n",
    "- **Tolerating**: T < Response time \u2264 4T\n",
    "- **Frustrated**: Response time > 4T\n",
    "\n",
    "**Apdex Score** = (Satisfied Count + 0.5 \u00d7 Tolerating Count) / Total Samples\n",
    "\n",
    "**Interpretation**:\n",
    "- 1.00 = Excellent (all users satisfied)\n",
    "- 0.94 = Good\n",
    "- 0.85 = Fair\n",
    "- < 0.70 = Poor\n",
    "\n",
    "### **33.5.2 The Four Golden Signals (Google SRE)**\n",
    "\n",
    "For monitoring and performance testing, Google identifies four critical metrics:\n",
    "\n",
    "1. **Latency**: Time to serve requests (distinguish between successful and failed request latency)\n",
    "2. **Traffic**: Demand on the system (requests per second)\n",
    "3. **Errors**: Rate of failed requests (explicit failures vs. degraded responses)\n",
    "4. **Saturation**: Resource utilization approaching capacity (often predicts impending failures)\n",
    "\n",
    "### **33.5.3 Percentile-Based SLAs**\n",
    "\n",
    "Modern performance engineering avoids \"average\" response time in favor of percentiles:\n",
    "\n",
    "- **P50 (Median)**: Typical user experience\n",
    "- **P95**: Threshold for \"good enough\" for most users\n",
    "- **P99**: Worst-case for typical operations (excluding outliers)\n",
    "- **P99.9**: Important for high-scale systems where 0.1% of 1M requests = 1000 affected users\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "**Types of Performance Testing (33.2):**\n",
    "- **Load Testing**: Validates system behavior under expected load (SLA verification)\n",
    "- **Stress Testing**: Finds breaking points and observes failure modes\n",
    "- **Spike Testing**: Sudden traffic surges (viral events, ticket sales)\n",
    "- **Endurance Testing**: Long-running stability (memory leaks, resource exhaustion)\n",
    "- **Scalability Testing**: Linear vs. sub-linear resource scaling\n",
    "\n",
    "**Critical Metrics (33.3):**\n",
    "- **Response Time**: P95 and P99 are more meaningful than averages; target P95 < 200ms for web apps\n",
    "- **Throughput**: Measured in TPS (transactions per second) or RPS (requests per second)\n",
    "- **Resource Utilization**: CPU < 70%, Memory monitored for leaks, Connection pools < 80%\n",
    "- **Error Rates**: Target < 0.1% for production systems; 99.9% availability is standard\n",
    "\n",
    "**Strategy Essentials (33.4):**\n",
    "- **SLAs must be SMART**: Specific thresholds (P95 < 500ms) rather than \"fast\"\n",
    "- **Baseline First**: Measure current state before optimizing; you can't improve what you don't measure\n",
    "- **Realistic Data**: Test with production-like data volumes; empty databases perform artificially well\n",
    "- **Monitoring**: Implement the Four Golden Signals (Latency, Traffic, Errors, Saturation)\n",
    "\n",
    "**Industry Standards (33.5):**\n",
    "- **Apdex**: Quantifies user satisfaction (1.0 = perfect, < 0.7 = poor)\n",
    "- **Percentiles**: P95 represents the experience of 95% of users; critical for SLAs\n",
    "- **Shift-Left**: Performance testing should begin in development, not just pre-production\n",
    "\n",
    "**Common Pitfalls to Avoid:**\n",
    "- Testing in production-scale environments only at the end of the release cycle\n",
    "- Using unrealistic \"ideal\" data instead of production-like datasets\n",
    "- Focusing only on average response times instead of tail latencies (P95/P99)\n",
    "- Ignoring the \"think time\" that real users introduce between actions\n",
    "- Not testing the database under realistic data volumes (indexes behave differently with 1M vs 1K rows)\n",
    "\n",
    "---\n",
    "\n",
    "## **\ud83d\udcd6 Next Chapter: Chapter 34 - Performance Testing Tools**\n",
    "\n",
    "Now that you understand the fundamental concepts and metrics of performance testing, **Chapter 34** will provide hands-on guidance with the **industry-standard tools** used to implement these strategies.\n",
    "\n",
    "In **Chapter 34**, you will master:\n",
    "\n",
    "- **Apache JMeter**: The open-source industry standard for load testing\u2014creating test plans, thread groups, samplers, and result analysis\n",
    "- **Gatling**: Scala-based DSL for high-performance load testing with beautiful HTML reports\n",
    "- **K6**: Modern JavaScript-based load testing tool built for developers and CI/CD integration\n",
    "- **Locust**: Python-based distributed load testing with user behavior modeling\n",
    "- **LoadRunner/NeoLoad**: Enterprise-grade solutions for complex enterprise environments\n",
    "- **APM Integration**: Connecting performance tests with Application Performance Monitoring tools (New Relic, Datadog, Dynatrace)\n",
    "- **CI/CD Integration**: Running performance tests in Jenkins, GitHub Actions, and GitLab CI\n",
    "- **Script Development**: Correlation, parameterization, and assertion techniques for each tool\n",
    "\n",
    "This chapter will transition you from **theory to practice**, providing code examples and configuration files you can immediately apply to test your own applications' performance characteristics.\n",
    "\n",
    "**Continue to Chapter 34 to learn the practical implementation of performance testing with industry-leading tools!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../7. database_testing/32. database_testing_tools.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='34. performance_testing_tools.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}