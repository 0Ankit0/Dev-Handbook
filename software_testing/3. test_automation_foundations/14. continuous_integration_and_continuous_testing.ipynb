{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 14: Continuous Integration and Continuous Testing**\n",
    "\n",
    "---\n",
    "\n",
    "## **14.1 CI/CD Overview**\n",
    "\n",
    "### **What is Continuous Integration (CI)?**\n",
    "\n",
    "**Continuous Integration (CI)** is a software development practice where developers regularly merge their code changes into a central repository, after which automated builds and tests are run. The goal is to catch integration errors as quickly as possible.\n",
    "\n",
    "**Analogy:** Imagine a group of chefs cooking a large meal. Instead of each chef cooking their dish completely separately and only combining everything at the end (which might result in incompatible flavors), they continuously taste and combine their ingredients throughout the cooking process. If the sauce is too salty, they know immediately\u2014not when the guests are already seated.\n",
    "\n",
    "**Formal Definition:**\n",
    "> \"Continuous Integration is a software development practice where members of a team integrate their work frequently, usually each person integrates at least daily\u2014leading to multiple integrations per day. Each integration is verified by an automated build (including test) to detect integration errors as quickly as possible.\" \u2014 Martin Fowler\n",
    "\n",
    "### **What is Continuous Testing?**\n",
    "\n",
    "**Continuous Testing** is the process of executing automated tests as part of the software delivery pipeline to obtain immediate feedback on the business risks associated with a software release candidate. It extends CI by adding comprehensive test automation at every stage.\n",
    "\n",
    "```\n",
    "Traditional Testing vs. Continuous Testing:\n",
    "\n",
    "Traditional:                    Continuous:\n",
    "Dev \u2192 Build \u2192 Manual Test \u2192    Dev \u2192 Build \u2192 Auto Test \u2192\n",
    "Deploy (Days/Weeks)            Deploy (Hours/Minutes)\n",
    "     \u2191                               \u2191\n",
    "   Waterfall                      Feedback Loop\n",
    "```\n",
    "\n",
    "### **The CI/CD Pipeline Stages**\n",
    "\n",
    "```\n",
    "Complete CI/CD Pipeline:\n",
    "\n",
    "Code Commit \u2192 Build \u2192 Unit Test \u2192 Integration Test \u2192 \n",
    "    Deploy to Staging \u2192 E2E Tests \u2192 Performance Tests \u2192 \n",
    "        Security Scan \u2192 Deploy to Production \u2192 Smoke Tests\n",
    "              \u2191                    \u2193\n",
    "         Feedback Loop       Monitoring\n",
    "\n",
    "Testing happens at EVERY stage (Shift-Left and Shift-Right)\n",
    "```\n",
    "\n",
    "### **Why Continuous Testing Matters**\n",
    "\n",
    "1. **Immediate Feedback:** Know within minutes if your code broke something\n",
    "2. **Risk Reduction:** Catch bugs before they reach production\n",
    "3. **Speed:** Automated tests run 24/7 without human intervention\n",
    "4. **Confidence:** Deploy to production knowing everything was tested\n",
    "5. **Cost Savings:** Fixing bugs in development is 100x cheaper than in production\n",
    "\n",
    "---\n",
    "\n",
    "## **14.2 Popular CI/CD Tools**\n",
    "\n",
    "### **14.2.1 Jenkins**\n",
    "\n",
    "**Jenkins** is the most widely used open-source automation server. It provides hundreds of plugins to support building, deploying, and automating any project.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Jobs:** Individual tasks (e.g., \"Run Smoke Tests\")\n",
    "- **Pipelines:** Multi-stage workflows defined in code (Jenkinsfile)\n",
    "- **Nodes/Agents:** Machines that execute the jobs\n",
    "- **Master:** The main Jenkins server that orchestrates\n",
    "\n",
    "**Installation:**\n",
    "\n",
    "```bash\n",
    "# Docker (Recommended for learning)\n",
    "docker run -d -p 8080:8080 -p 50000:50000 \\\n",
    "  -v jenkins_home:/var/jenkins_home \\\n",
    "  --name jenkins jenkins/jenkins:lts\n",
    "\n",
    "# Get initial admin password\n",
    "docker exec jenkins cat /var/jenkins_home/secrets/initialAdminPassword\n",
    "```\n",
    "\n",
    "**Basic Pipeline (Jenkinsfile):**\n",
    "\n",
    "```groovy\n",
    "// Jenkinsfile (Declarative Pipeline)\n",
    "pipeline {\n",
    "    agent any  // Run on any available agent\n",
    "    \n",
    "    environment {\n",
    "        PYTHON_VERSION = '3.11'\n",
    "        TEST_ENV = 'staging'\n",
    "    }\n",
    "    \n",
    "    stages {\n",
    "        stage('Checkout') {\n",
    "            steps {\n",
    "                // Get code from Git\n",
    "                git branch: 'main', \n",
    "                    url: 'https://github.com/company/test-automation.git'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Setup Environment') {\n",
    "            steps {\n",
    "                sh '''\n",
    "                    python -m venv venv\n",
    "                    source venv/bin/activate\n",
    "                    pip install -r requirements.txt\n",
    "                '''\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Run Unit Tests') {\n",
    "            steps {\n",
    "                sh '''\n",
    "                    source venv/bin/activate\n",
    "                    pytest tests/unit/ -v --junitxml=unit-results.xml\n",
    "                '''\n",
    "            }\n",
    "            post {\n",
    "                always {\n",
    "                    // Publish test results\n",
    "                    junit 'unit-results.xml'\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Run Integration Tests') {\n",
    "            steps {\n",
    "                sh '''\n",
    "                    source venv/bin/activate\n",
    "                    pytest tests/integration/ -v --junitxml=int-results.xml\n",
    "                '''\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        stage('Run E2E Tests') {\n",
    "            steps {\n",
    "                sh '''\n",
    "                    source venv/bin/activate\n",
    "                    pytest tests/e2e/ -v --html=e2e-report.html\n",
    "                '''\n",
    "            }\n",
    "            post {\n",
    "                always {\n",
    "                    // Archive artifacts\n",
    "                    publishHTML([\n",
    "                        allowMissing: false,\n",
    "                        alwaysLinkToLastBuild: true,\n",
    "                        keepAll: true,\n",
    "                        reportDir: '.',\n",
    "                        reportFiles: 'e2e-report.html',\n",
    "                        reportName: 'E2E Test Report'\n",
    "                    ])\n",
    "                }\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    post {\n",
    "        always {\n",
    "            // Cleanup\n",
    "            cleanWs()\n",
    "        }\n",
    "        failure {\n",
    "            // Send email on failure\n",
    "            mail to: 'team@company.com',\n",
    "                 subject: \"Failed Pipeline: ${currentBuild.fullDisplayName}\",\n",
    "                 body: \"Something is wrong with ${env.BUILD_URL}\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### **14.2.2 GitHub Actions**\n",
    "\n",
    "**GitHub Actions** is CI/CD built directly into GitHub. It uses YAML workflow files stored in your repository (`.github/workflows/`).\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Workflows:** Automated processes defined in YAML\n",
    "- **Events:** Triggers (push, pull_request, schedule, manual)\n",
    "- **Jobs:** Groups of steps that execute on the same runner\n",
    "- **Runners:** Virtual machines (Ubuntu, Windows, macOS) that execute jobs\n",
    "- **Actions:** Reusable units of code (like functions)\n",
    "\n",
    "**Basic Workflow:**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/test-automation.yml\n",
    "name: Test Automation Pipeline\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main, develop ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "  schedule:\n",
    "    # Run nightly at 2 AM UTC\n",
    "    - cron: '0 2 * * *'\n",
    "  workflow_dispatch:  # Manual trigger\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    strategy:\n",
    "      matrix:\n",
    "        python-version: ['3.9', '3.10', '3.11']\n",
    "        browser: [chrome, firefox]\n",
    "    \n",
    "    steps:\n",
    "    - name: Checkout Code\n",
    "      uses: actions/checkout@v4\n",
    "    \n",
    "    - name: Set up Python ${{ matrix.python-version }}\n",
    "      uses: actions/setup-python@v5\n",
    "      with:\n",
    "        python-version: ${{ matrix.python-version }}\n",
    "    \n",
    "    - name: Cache pip dependencies\n",
    "      uses: actions/cache@v3\n",
    "      with:\n",
    "        path: ~/.cache/pip\n",
    "        key: ${{ runner.os }}-pip-${{ hashFiles('**/requirements.txt') }}\n",
    "    \n",
    "    - name: Install Dependencies\n",
    "      run: |\n",
    "        python -m pip install --upgrade pip\n",
    "        pip install -r requirements.txt\n",
    "        pip install pytest pytest-html pytest-xdist\n",
    "    \n",
    "    - name: Setup Chrome/Firefox\n",
    "      uses: browser-actions/setup-chrome@v1\n",
    "      if: matrix.browser == 'chrome'\n",
    "    \n",
    "    - name: Run Tests\n",
    "      env:\n",
    "        BROWSER: ${{ matrix.browser }}\n",
    "        HEADLESS: 'true'\n",
    "        TEST_ENV: 'ci'\n",
    "      run: |\n",
    "        pytest tests/ \\\n",
    "          -v \\\n",
    "          --html=reports/report.html \\\n",
    "          --self-contained-html \\\n",
    "          --junitxml=reports/junit.xml \\\n",
    "          -n auto  # Parallel execution\n",
    "    \n",
    "    - name: Upload Test Results\n",
    "      uses: actions/upload-artifact@v4\n",
    "      if: always()\n",
    "      with:\n",
    "        name: test-results-${{ matrix.python-version }}-${{ matrix.browser }}\n",
    "        path: |\n",
    "          reports/\n",
    "          screenshots/\n",
    "    \n",
    "    - name: Publish Test Report\n",
    "      uses: dorny/test-reporter@v1\n",
    "      if: always()\n",
    "      with:\n",
    "        name: Test Results\n",
    "        path: reports/junit.xml\n",
    "        reporter: java-junit\n",
    "    \n",
    "    - name: Notify Slack on Failure\n",
    "      uses: 8398a7/action-slack@v3\n",
    "      if: failure()\n",
    "      with:\n",
    "        status: ${{ job.status }}\n",
    "        channel: '#qa-alerts'\n",
    "        webhook_url: ${{ secrets.SLACK_WEBHOOK }}\n",
    "```\n",
    "\n",
    "### **14.2.3 GitLab CI**\n",
    "\n",
    "**GitLab CI/CD** is integrated into GitLab. Configuration is stored in `.gitlab-ci.yml`.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Pipelines:** Top-level component comprising stages and jobs\n",
    "- **Stages:** Groups of jobs (build, test, deploy)\n",
    "- **Jobs:** Individual tasks with scripts\n",
    "- **Runners:** Agents that execute jobs\n",
    "- **Artifacts:** Files passed between stages\n",
    "\n",
    "**Basic Configuration:**\n",
    "\n",
    "```yaml\n",
    "# .gitlab-ci.yml\n",
    "stages:\n",
    "  - build\n",
    "  - test\n",
    "  - report\n",
    "  - deploy\n",
    "\n",
    "variables:\n",
    "  PIP_CACHE_DIR: \"$CI_PROJECT_DIR/.cache/pip\"\n",
    "  PYTEST_ARGS: \"-v --html=report.html --self-contained-html\"\n",
    "\n",
    "cache:\n",
    "  paths:\n",
    "    - .cache/pip\n",
    "    - venv/\n",
    "\n",
    "before_script:\n",
    "  - python -V\n",
    "  - pip install virtualenv\n",
    "  - virtualenv venv\n",
    "  - source venv/bin/activate\n",
    "  - pip install -r requirements.txt\n",
    "\n",
    "build:\n",
    "  stage: build\n",
    "  script:\n",
    "    - echo \"Building test package...\"\n",
    "    - python setup.py build\n",
    "  artifacts:\n",
    "    paths:\n",
    "      - build/\n",
    "\n",
    "unit_tests:\n",
    "  stage: test\n",
    "  script:\n",
    "    - pytest tests/unit/ $PYTEST_ARGS --junitxml=unit.xml\n",
    "  artifacts:\n",
    "    when: always\n",
    "    reports:\n",
    "      junit: unit.xml\n",
    "    paths:\n",
    "      - report.html\n",
    "  coverage: '/TOTAL.*\\s+(\\d+%)$/'\n",
    "\n",
    "integration_tests:\n",
    "  stage: test\n",
    "  script:\n",
    "    - pytest tests/integration/ $PYTEST_ARGS\n",
    "  rules:\n",
    "    - if: $CI_PIPELINE_SOURCE == \"merge_request_event\"\n",
    "    - if: $CI_COMMIT_BRANCH == \"main\"\n",
    "\n",
    "e2e_tests:\n",
    "  stage: test\n",
    "  image: selenium/standalone-chrome:latest\n",
    "  services:\n",
    "    - selenium/standalone-chrome\n",
    "  script:\n",
    "    - pytest tests/e2e/ $PYTEST_ARGS --browser=remote\n",
    "  parallel:\n",
    "    matrix:\n",
    "      - BROWSER: [chrome, firefox]\n",
    "        TEST_SUITE: [smoke, regression]\n",
    "  artifacts:\n",
    "    when: always\n",
    "    paths:\n",
    "      - report.html\n",
    "      - screenshots/\n",
    "    expire_in: 1 week\n",
    "\n",
    "pages:\n",
    "  stage: report\n",
    "  script:\n",
    "    - mkdir public\n",
    "    - cp report.html public/index.html\n",
    "    - cp -r screenshots public/\n",
    "  artifacts:\n",
    "    paths:\n",
    "      - public\n",
    "  only:\n",
    "    - main\n",
    "\n",
    "deploy_staging:\n",
    "  stage: deploy\n",
    "  script:\n",
    "    - echo \"Deploying to staging...\"\n",
    "  environment:\n",
    "    name: staging\n",
    "    url: https://staging.example.com\n",
    "  only:\n",
    "    - main\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.3 Integrating Tests in CI Pipeline**\n",
    "\n",
    "### **The Testing Pyramid in CI**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/comprehensive-testing.yml\n",
    "name: Continuous Testing Pipeline\n",
    "\n",
    "on: [push, pull_request]\n",
    "\n",
    "jobs:\n",
    "  # Level 1: Unit Tests (Fast, isolated)\n",
    "  unit-tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/setup-python@v5\n",
    "        with:\n",
    "          python-version: '3.11'\n",
    "      - run: pip install pytest pytest-cov\n",
    "      - run: pytest tests/unit/ --cov=src --cov-report=xml\n",
    "      - uses: codecov/codecov-action@v3\n",
    "        with:\n",
    "          files: ./coverage.xml\n",
    "\n",
    "  # Level 2: Integration Tests (API, Database)\n",
    "  integration-tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    services:\n",
    "      postgres:\n",
    "        image: postgres:15\n",
    "        env:\n",
    "          POSTGRES_PASSWORD: postgres\n",
    "        options: >-\n",
    "          --health-cmd pg_isready\n",
    "          --health-interval 10s\n",
    "          --health-timeout 5s\n",
    "          --health-retries 5\n",
    "        ports:\n",
    "          - 5432:5432\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - run: |\n",
    "          pip install -r requirements.txt\n",
    "          pytest tests/integration/ -v --db-url=postgresql://postgres:postgres@localhost:5432/test\n",
    "\n",
    "  # Level 3: E2E Tests (UI, slow)\n",
    "  e2e-tests:\n",
    "    runs-on: ubuntu-latest\n",
    "    needs: [unit-tests]  # Only run if unit tests pass\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: browser-actions/setup-chrome@v1\n",
    "      - run: |\n",
    "          pip install -r requirements.txt\n",
    "          pytest tests/e2e/ -v --browser=chrome --headless\n",
    "```\n",
    "\n",
    "### **Test Data Management in CI**\n",
    "\n",
    "```python\n",
    "# conftest.py - Pytest configuration for CI\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "def pytest_configure(config):\n",
    "    \"\"\"Configure test environment based on CI variables\"\"\"\n",
    "    if os.getenv('CI') == 'true':\n",
    "        # Running in CI environment\n",
    "        config.option.headless = True\n",
    "        config.option.browser = 'chrome'\n",
    "        \n",
    "        # Use test database\n",
    "        os.environ['DATABASE_URL'] = os.getenv(\n",
    "            'TEST_DATABASE_URL', \n",
    "            'sqlite:///test.db'\n",
    "        )\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def browser_type():\n",
    "    \"\"\"Determine browser based on environment\"\"\"\n",
    "    return os.getenv('BROWSER', 'chrome')\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def is_headless():\n",
    "    \"\"\"Headless mode for CI\"\"\"\n",
    "    return os.getenv('HEADLESS', 'false').lower() == 'true'\n",
    "\n",
    "@pytest.fixture\n",
    "def test_data():\n",
    "    \"\"\"Load appropriate test data for environment\"\"\"\n",
    "    env = os.getenv('TEST_ENV', 'development')\n",
    "    if env == 'ci':\n",
    "        return load_test_data('ci_test_data.json')\n",
    "    return load_test_data('local_test_data.json')\n",
    "```\n",
    "\n",
    "### **Handling Test Flakiness in CI**\n",
    "\n",
    "```python\n",
    "# Retry mechanism for flaky tests in CI\n",
    "import pytest\n",
    "import os\n",
    "\n",
    "# In CI, retry flaky tests automatically\n",
    "def pytest_runtest_call(item):\n",
    "    if os.getenv('CI') == 'true':\n",
    "        max_retries = 2\n",
    "        for i in range(max_retries):\n",
    "            try:\n",
    "                item.runtest()\n",
    "                break\n",
    "            except Exception as e:\n",
    "                if i == max_retries - 1:\n",
    "                    raise e\n",
    "                else:\n",
    "                    print(f\"Test {item.name} failed on attempt {i+1}. Retrying...\")\n",
    "```\n",
    "---\n",
    "\n",
    "## **14.4 Parallel Test Execution**\n",
    "\n",
    "### **Why Parallel Execution?**\n",
    "\n",
    "As your test suite grows from 10 to 100 to 1000 tests, sequential execution becomes a bottleneck. Running 1000 tests at 30 seconds each takes **8+ hours sequentially** but only **15-30 minutes in parallel** across 40 machines.\n",
    "\n",
    "**Benefits:**\n",
    "- **Speed:** Reduce feedback time from hours to minutes\n",
    "- **Efficiency:** Utilize cloud infrastructure\n",
    "- **Scalability:** Add more agents as tests grow\n",
    "- **Cost:** Pay for compute only when testing (cloud model)\n",
    "\n",
    "### **Parallel Execution Strategies**\n",
    "\n",
    "#### **Strategy 1: Test-Level Parallelism (pytest-xdist)**\n",
    "\n",
    "```python\n",
    "# Run tests across multiple CPUs on single machine\n",
    "# requirements.txt: pytest-xdist\n",
    "\n",
    "# Command line:\n",
    "pytest -n auto  # Auto-detect CPU cores\n",
    "pytest -n 4     # Use 4 processes\n",
    "pytest -n auto --dist=loadfile  # Group tests by file to avoid conflicts\n",
    "\n",
    "# Configuration in pytest.ini\n",
    "\"\"\"\n",
    "[pytest]\n",
    "addopts = -n auto --dist=loadscope\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "**Handling Shared Resources:**\n",
    "\n",
    "```python\n",
    "# conftest.py - Ensure test isolation in parallel execution\n",
    "import pytest\n",
    "import tempfile\n",
    "import shutil\n",
    "\n",
    "@pytest.fixture(scope='function')\n",
    "def isolated_download_dir():\n",
    "    \"\"\"Each test gets unique download directory\"\"\"\n",
    "    temp_dir = tempfile.mkdtemp()\n",
    "    yield temp_dir\n",
    "    shutil.rmtree(temp_dir)\n",
    "\n",
    "@pytest.fixture(scope='function') \n",
    "def unique_user():\n",
    "    \"\"\"Generate unique user to avoid conflicts\"\"\"\n",
    "    import uuid\n",
    "    return f\"test_user_{uuid.uuid4().hex[:8]}\"\n",
    "\n",
    "# Use locks for truly shared resources (databases, files)\n",
    "import filelock\n",
    "\n",
    "@pytest.fixture(scope='session')\n",
    "def database_lock():\n",
    "    return filelock.FileLock(\"test_database.lock\")\n",
    "\n",
    "def test_database_operation(database_lock):\n",
    "    with database_lock:\n",
    "        # Only one test accesses DB at a time\n",
    "        pass\n",
    "```\n",
    "\n",
    "#### **Strategy 2: Browser-Level Parallelism (Selenium Grid)**\n",
    "\n",
    "```yaml\n",
    "# docker-compose.yml for Selenium Grid\n",
    "version: '3'\n",
    "services:\n",
    "  selenium-hub:\n",
    "    image: selenium/hub:latest\n",
    "    ports:\n",
    "      - \"4444:4444\"\n",
    "\n",
    "  chrome-node-1:\n",
    "    image: selenium/node-chrome:latest\n",
    "    depends_on:\n",
    "      - selenium-hub\n",
    "    environment:\n",
    "      - HUB_HOST=selenium-hub\n",
    "      - HUB_PORT=4444\n",
    "      - NODE_MAX_INSTANCES=5\n",
    "      - NODE_MAX_SESSION=5\n",
    "\n",
    "  chrome-node-2:\n",
    "    image: selenium/node-chrome:latest\n",
    "    depends_on:\n",
    "      - selenium-hub\n",
    "    environment:\n",
    "      - HUB_HOST=selenium-hub\n",
    "\n",
    "  firefox-node:\n",
    "    image: selenium/node-firefox:latest\n",
    "    depends_on:\n",
    "      - selenium-hub\n",
    "    environment:\n",
    "      - HUB_HOST=selenium-hub\n",
    "\n",
    "# Test configuration\n",
    "\"\"\"\n",
    "# config/grid_config.py\n",
    "GRID_URL = \"http://localhost:4444/wd/hub\"\n",
    "\n",
    "def create_remote_driver(browser):\n",
    "    options = {\n",
    "        'chrome': webdriver.ChromeOptions(),\n",
    "        'firefox': webdriver.FirefoxOptions()\n",
    "    }[browser]\n",
    "    \n",
    "    return webdriver.Remote(\n",
    "        command_executor=GRID_URL,\n",
    "        options=options\n",
    "    )\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "#### **Strategy 3: CI-Level Parallelism (Matrix Builds)**\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/parallel-tests.yml\n",
    "name: Parallel Test Execution\n",
    "\n",
    "on: [push]\n",
    "\n",
    "jobs:\n",
    "  # Split tests by feature/module\n",
    "  test-authentication:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - run: pytest tests/e2e/test_login.py tests/e2e/test_signup.py -v\n",
    "\n",
    "  test-checkout:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - run: pytest tests/e2e/test_checkout*.py -v\n",
    "\n",
    "  test-search:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - run: pytest tests/e2e/test_search*.py -v\n",
    "\n",
    "  # Combine results\n",
    "  report:\n",
    "    needs: [test-authentication, test-checkout, test-search]\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/download-artifact@v4\n",
    "        with:\n",
    "          path: reports/\n",
    "      - run: |\n",
    "          # Merge all test reports\n",
    "          python merge_reports.py reports/\n",
    "```\n",
    "\n",
    "#### **Strategy 4: Cloud Grid Services**\n",
    "\n",
    "```python\n",
    "# Using Sauce Labs, BrowserStack, or AWS Device Farm\n",
    "import os\n",
    "from selenium import webdriver\n",
    "\n",
    "def get_cloud_driver():\n",
    "    \"\"\"Cross-browser parallel execution in cloud\"\"\"\n",
    "    capabilities = {\n",
    "        'browserName': os.getenv('BROWSER', 'chrome'),\n",
    "        'browserVersion': 'latest',\n",
    "        'platformName': os.getenv('PLATFORM', 'Windows 10'),\n",
    "        'sauce:options': {\n",
    "            'username': os.getenv('SAUCE_USERNAME'),\n",
    "            'accessKey': os.getenv('SAUCE_ACCESS_KEY'),\n",
    "            'build': os.getenv('BUILD_ID', 'local'),\n",
    "            'name': 'Test Automation Suite'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return webdriver.Remote(\n",
    "        command_executor='https://ondemand.saucelabs.com/wd/hub',\n",
    "        desired_capabilities=capabilities\n",
    "    )\n",
    "\n",
    "# Run same test on multiple configurations in parallel\n",
    "# Using pytest parameterized with cloud configs\n",
    "@pytest.mark.parametrize(\"config\", [\n",
    "    {'browser': 'chrome', 'platform': 'Windows 10'},\n",
    "    {'browser': 'safari', 'platform': 'macOS 13'},\n",
    "    {'browser': 'chrome', 'platform': 'Linux'}\n",
    "])\n",
    "def test_cross_browser(config):\n",
    "    driver = get_cloud_driver(**config)\n",
    "    # ... test logic\n",
    "```\n",
    "\n",
    "### **Best Practices for Parallel Execution**\n",
    "\n",
    "1. **Test Independence:** Each test must setup and teardown its own data\n",
    "2. **No Shared State:** Avoid global variables, static classes\n",
    "3. **Unique Identifiers:** Use UUIDs for test data (usernames, emails)\n",
    "4. **Resource Pooling:** Use database connection pools, thread-safe queues\n",
    "5. **Deterministic Ordering:** Parallel should produce same results as serial\n",
    "\n",
    "```python\n",
    "# Anti-pattern: Shared state\n",
    "class TestData:\n",
    "    counter = 0  # Dangerous in parallel!\n",
    "\n",
    "# Best practice: Isolated fixtures\n",
    "@pytest.fixture\n",
    "def test_counter():\n",
    "    return {'value': 0}  # Fresh for each test\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.5 Test Environment Management**\n",
    "\n",
    "### **The Challenge**\n",
    "\n",
    "Test environments are often the biggest bottleneck in CI/CD:\n",
    "- **Inconsistency:** \"Works on my machine\" syndrome\n",
    "- **Configuration Drift:** Dev, Staging, Production differ\n",
    "- **Contention:** Multiple tests competing for same environment\n",
    "- **Data State:** Leftover data from previous runs\n",
    "\n",
    "### **Solution: Environment as Code**\n",
    "\n",
    "#### **Docker for Consistent Environments**\n",
    "\n",
    "```dockerfile\n",
    "# Dockerfile.test\n",
    "FROM python:3.11-slim\n",
    "\n",
    "# Install dependencies\n",
    "RUN apt-get update && apt-get install -y \\\n",
    "    wget \\\n",
    "    gnupg \\\n",
    "    chromium \\\n",
    "    chromium-driver\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements\n",
    "COPY requirements.txt .\n",
    "RUN pip install -r requirements.txt\n",
    "\n",
    "# Copy test code\n",
    "COPY . .\n",
    "\n",
    "# Default command\n",
    "CMD [\"pytest\", \"--html=/reports/report.html\"]\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# docker-compose.test.yml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  app:\n",
    "    build:\n",
    "      context: .\n",
    "      dockerfile: Dockerfile.test\n",
    "    volumes:\n",
    "      - ./reports:/reports\n",
    "      - ./screenshots:/screenshots\n",
    "    environment:\n",
    "      - DATABASE_URL=postgresql://postgres:password@db:5432/testdb\n",
    "      - API_URL=http://api:8080\n",
    "      - HEADLESS=true\n",
    "    depends_on:\n",
    "      - db\n",
    "      - api\n",
    "\n",
    "  db:\n",
    "    image: postgres:15-alpine\n",
    "    environment:\n",
    "      POSTGRES_DB: testdb\n",
    "      POSTGRES_PASSWORD: password\n",
    "    volumes:\n",
    "      - ./test_data/init.sql:/docker-entrypoint-initdb.d/init.sql\n",
    "\n",
    "  api:\n",
    "    image: myapp:latest\n",
    "    environment:\n",
    "      - DB_HOST=db\n",
    "      - DB_NAME=testdb\n",
    "\n",
    "  selenium-hub:\n",
    "    image: selenium/hub:latest\n",
    "\n",
    "  chrome:\n",
    "    image: selenium/node-chrome:latest\n",
    "    environment:\n",
    "      - HUB_HOST=selenium-hub\n",
    "```\n",
    "\n",
    "#### **Dynamic Environment Provisioning**\n",
    "\n",
    "```python\n",
    "# environment_manager.py\n",
    "import docker\n",
    "import time\n",
    "import os\n",
    "\n",
    "class TestEnvironment:\n",
    "    \"\"\"Spin up isolated environment per test run\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.client = docker.from_env()\n",
    "        self.containers = []\n",
    "        self.network = None\n",
    "        \n",
    "    def setup(self):\n",
    "        \"\"\"Create isolated network and services\"\"\"\n",
    "        # Create network\n",
    "        self.network = self.client.networks.create(\n",
    "            f\"test-net-{os.urandom(4).hex()}\",\n",
    "            driver=\"bridge\"\n",
    "        )\n",
    "        \n",
    "        # Start database\n",
    "        db = self.client.containers.run(\n",
    "            \"postgres:15\",\n",
    "            environment={\"POSTGRES_PASSWORD\": \"test\"},\n",
    "            network=self.network.name,\n",
    "            name=f\"db-{os.urandom(4).hex()}\",\n",
    "            detach=True\n",
    "        )\n",
    "        self.containers.append(db)\n",
    "        \n",
    "        # Wait for DB ready\n",
    "        time.sleep(3)\n",
    "        \n",
    "        # Start application under test\n",
    "        app = self.client.containers.run(\n",
    "            \"myapp:test\",\n",
    "            network=self.network.name,\n",
    "            ports={'8080/tcp': None},  # Random port\n",
    "            detach=True\n",
    "        )\n",
    "        self.containers.append(app)\n",
    "        \n",
    "        # Get assigned port\n",
    "        app.reload()\n",
    "        port = app.ports['8080/tcp'][0]['HostPort']\n",
    "        \n",
    "        return f\"http://localhost:{port}\"\n",
    "    \n",
    "    def teardown(self):\n",
    "        \"\"\"Cleanup all resources\"\"\"\n",
    "        for container in self.containers:\n",
    "            container.stop()\n",
    "            container.remove()\n",
    "        if self.network:\n",
    "            self.network.remove()\n",
    "\n",
    "# Usage in tests\n",
    "@pytest.fixture(scope='session')\n",
    "def test_env():\n",
    "    env = TestEnvironment()\n",
    "    url = env.setup()\n",
    "    yield url\n",
    "    env.teardown()\n",
    "```\n",
    "\n",
    "### **Test Data Management Strategies**\n",
    "\n",
    "#### **Strategy 1: Database Migrations**\n",
    "\n",
    "```python\n",
    "# alembic/versions/001_test_data.py\n",
    "\"\"\"Add test data\"\"\"\n",
    "\n",
    "def upgrade():\n",
    "    op.execute(\"\"\"\n",
    "        INSERT INTO users (id, username, email, role)\n",
    "        VALUES \n",
    "            (1, 'admin_test', 'admin@test.com', 'admin'),\n",
    "            (2, 'user_test', 'user@test.com', 'user')\n",
    "    \"\"\")\n",
    "\n",
    "def downgrade():\n",
    "    op.execute(\"DELETE FROM users WHERE email LIKE '%@test.com'\")\n",
    "```\n",
    "\n",
    "#### **Strategy 2: API Seeding**\n",
    "\n",
    "```python\n",
    "@pytest.fixture(scope='module')\n",
    "def test_data(api_client):\n",
    "    \"\"\"Create test data via API\"\"\"\n",
    "    # Create users\n",
    "    admin = api_client.post('/users', json={\n",
    "        'username': f'admin_{uuid.uuid4().hex[:8]}',\n",
    "        'role': 'admin'\n",
    "    }).json()\n",
    "    \n",
    "    yield {'admin': admin}\n",
    "    \n",
    "    # Cleanup\n",
    "    api_client.delete(f\"/users/{admin['id']}\")\n",
    "```\n",
    "\n",
    "#### **Strategy 3: Database Snapshots**\n",
    "\n",
    "```bash\n",
    "# Restore known good state before tests\n",
    "pg_restore --clean --if-exists --dbname=test_db test_data_snapshot.dump\n",
    "\n",
    "# Or use test transactions (rollback after each test)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.6 Infrastructure as Code Testing**\n",
    "\n",
    "### **Testing Infrastructure Changes**\n",
    "\n",
    "As environments become code (Terraform, Ansible, Kubernetes), they need testing too:\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/infra-test.yml\n",
    "name: Infrastructure Testing\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    paths:\n",
    "      - 'terraform/**'\n",
    "      - 'k8s/**'\n",
    "\n",
    "jobs:\n",
    "  terraform-validate:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: hashicorp/terraform@v1\n",
    "      - run: |\n",
    "          cd terraform/\n",
    "          terraform init -backend=false\n",
    "          terraform validate\n",
    "          terraform plan -out=plan.tfplan\n",
    "      - name: Security Scan\n",
    "        uses: aquasecurity/trivy-action@master\n",
    "        with:\n",
    "          scan-type: 'config'\n",
    "          scan-ref: './terraform'\n",
    "\n",
    "  k8s-validate:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - run: |\n",
    "          kubectl apply --dry-run=client -f k8s/\n",
    "          kubeval k8s/*.yaml\n",
    "```\n",
    "\n",
    "### **Testing in Kubernetes**\n",
    "\n",
    "```yaml\n",
    "# kubernetes/test-job.yaml\n",
    "apiVersion: batch/v1\n",
    "kind: Job\n",
    "metadata:\n",
    "  name: test-runner\n",
    "spec:\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: tester\n",
    "        image: myapp-tests:latest\n",
    "        env:\n",
    "        - name: TEST_ENV\n",
    "          value: \"k8s\"\n",
    "        - name: APP_URL\n",
    "          value: \"http://app-service:8080\"\n",
    "      restartPolicy: Never\n",
    "  backoffLimit: 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.7 Monitoring and Observability in Testing**\n",
    "\n",
    "### **Test Observability**\n",
    "\n",
    "Know what's happening during test execution:\n",
    "\n",
    "```python\n",
    "# observability_setup.py\n",
    "import logging\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "\n",
    "# Structured logging for tests\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('test_execution.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "class TestInstrumentor:\n",
    "    \"\"\"Add observability to tests\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.metrics = {\n",
    "            'tests_run': 0,\n",
    "            'tests_passed': 0,\n",
    "            'tests_failed': 0,\n",
    "            'duration': 0\n",
    "        }\n",
    "    \n",
    "    @contextmanager\n",
    "    def measure_test(self, test_name):\n",
    "        \"\"\"Context manager for test timing\"\"\"\n",
    "        start = time.time()\n",
    "        self.metrics['tests_run'] += 1\n",
    "        logging.info(f\"START: {test_name}\")\n",
    "        \n",
    "        try:\n",
    "            yield\n",
    "            self.metrics['tests_passed'] += 1\n",
    "            logging.info(f\"PASS: {test_name}\")\n",
    "        except Exception as e:\n",
    "            self.metrics['tests_failed'] += 1\n",
    "            logging.error(f\"FAIL: {test_name} - {str(e)}\")\n",
    "            raise\n",
    "        finally:\n",
    "            duration = time.time() - start\n",
    "            self.metrics['duration'] += duration\n",
    "            logging.info(f\"DURATION: {test_name} - {duration:.2f}s\")\n",
    "\n",
    "# Usage\n",
    "instrumentor = TestInstrumentor()\n",
    "\n",
    "def test_login():\n",
    "    with instrumentor.measure_test(\"test_login\"):\n",
    "        # test logic\n",
    "        pass\n",
    "\n",
    "# Export metrics to Prometheus/Grafana\n",
    "from prometheus_client import Counter, Histogram\n",
    "\n",
    "test_counter = Counter('tests_total', 'Total tests', ['status'])\n",
    "test_duration = Histogram('test_duration_seconds', 'Test duration')\n",
    "\n",
    "@test_duration.time()\n",
    "def monitored_test():\n",
    "    try:\n",
    "        # test logic\n",
    "        test_counter.labels(status='pass').inc()\n",
    "    except:\n",
    "        test_counter.labels(status='fail').inc()\n",
    "        raise\n",
    "```\n",
    "\n",
    "### **Pipeline Monitoring**\n",
    "\n",
    "```yaml\n",
    "# Send metrics to monitoring systems\n",
    "- name: Publish Test Metrics\n",
    "  if: always()\n",
    "  run: |\n",
    "    # Parse test results\n",
    "    PASSED=$(grep -c 'passed' report.txt || echo 0)\n",
    "    FAILED=$(grep -c 'failed' report.txt || echo 0)\n",
    "    DURATION=$(cat duration.txt)\n",
    "    \n",
    "    # Send to Datadog/CloudWatch\n",
    "    curl -X POST \"https://api.datadoghq.com/api/v1/series\" \\\n",
    "      -H \"Content-Type: application/json\" \\\n",
    "      -H \"DD-API-KEY: ${{ secrets.DD_API_KEY }}\" \\\n",
    "      -d \"{\n",
    "        \\\"series\\\": [{\n",
    "          \\\"metric\\\": \\\"ci.tests.passed\\\",\n",
    "          \\\"points\\\": [[$(date +%s), $PASSED]],\n",
    "          \\\"tags\\\": [\\\"branch:${{ github.ref_name }}\\\", \\\"repo:${{ github.repository }}\\\"]\n",
    "        }]\n",
    "      }\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Continuous Testing:** Testing must happen at every stage of the pipeline, not just at the end. Shift-left (early) and shift-right (production).\n",
    "\n",
    "2. **CI Tools:** \n",
    "   - **Jenkins:** Enterprise standard, highly customizable\n",
    "   - **GitHub Actions:** Integrated with GitHub, YAML-based\n",
    "   - **GitLab CI:** Integrated DevOps platform\n",
    "\n",
    "3. **Pipeline Structure:** Checkout \u2192 Build \u2192 Unit Test \u2192 Integration Test \u2192 E2E Test \u2192 Deploy \u2192 Monitor. Fail fast at each stage.\n",
    "\n",
    "4. **Parallel Execution:** Essential for speed. Use pytest-xdist for local, Selenium Grid for distributed, matrix builds for CI parallelism. Ensure test isolation.\n",
    "\n",
    "5. **Environment Management:** Use Docker for consistency, Infrastructure as Code (Terraform/K8s), and dynamic provisioning. Never share state between tests.\n",
    "\n",
    "6. **Test Data:** Use fixtures, database migrations, or API seeding. Clean up after tests. Use transactions for rollback.\n",
    "\n",
    "7. **Observability:** Log everything, measure test duration, track flakiness. Send metrics to monitoring systems (Datadog, Grafana).\n",
    "\n",
    "8. **Security in CI:** Never commit secrets. Use environment variables, secret managers (Vault, AWS Secrets Manager), and rotate credentials.\n",
    "\n",
    "### **The Continuous Testing Maturity Model:**\n",
    "\n",
    "| **Level** | **Characteristics** |\n",
    "|-----------|---------------------|\n",
    "| **1. Initial** | Manual testing only, ad-hoc automation |\n",
    "| **2. Managed** | Automated unit tests in CI, some integration tests |\n",
    "| **3. Defined** | Full pyramid (unit/integration/E2E), parallel execution, environment automation |\n",
    "| **4. Quantitative** | Metrics-driven, test coverage gates, performance budgets |\n",
    "| **5. Optimizing** | AI-based test selection, chaos engineering, production testing |\n",
    "\n",
    "---\n",
    "\n",
    "## **\ud83d\udcd6 Next Chapter: Chapter 15 - Web Application Fundamentals**\n",
    "\n",
    "Now that you've mastered Continuous Integration and Continuous Testing, **Chapter 15** begins **Part III: Web Application Testing**\u2014the largest and most critical section of the handbook.\n",
    "\n",
    "In **Chapter 15**, you'll learn:\n",
    "\n",
    "- **How Web Applications Work:** HTTP/HTTPS protocols, request/response cycle, state management\n",
    "- **Browser Developer Tools:** Mastering the essential debugging companion for web testers\n",
    "- **DOM Structure:** Understanding the Document Object Model that Selenium interacts with\n",
    "- **Cookies, Sessions, and Storage:** Managing state in web applications\n",
    "- **Cross-Origin Resource Sharing (CORS):** Security implications for testing\n",
    "- **Web Technologies:** HTML, CSS, JavaScript basics for testers\n",
    "- **Architecture Patterns:** Single Page Applications (SPA), Server-Side Rendering, Progressive Web Apps\n",
    "\n",
    "**Why Chapter 15 is Critical:** You cannot effectively test what you do not understand. Before diving into Selenium, Cypress, or Playwright in subsequent chapters, you must understand the underlying web technologies and protocols. This chapter bridges the gap between \"I can write a test\" and \"I understand why the test works.\"\n",
    "\n",
    "**Continue to Chapter 15 to build the foundational web knowledge that will make you an expert web application tester!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='13. version_control_for_test_automation.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../4. web_application_testing/15. web_application_fundamentals.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}