{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 31: Database Testing Techniques**\n",
    "\n",
    "---\n",
    "\n",
    "## **31.1 Introduction to Database Testing Methodologies**\n",
    "\n",
    "Database testing ensures that the data persistence layer of an application functions correctly, securely, and efficiently. While SQL skills (covered in Chapter 30) provide the tools to query data, testing methodologies provide the systematic approach to validate database behavior under various conditions.\n",
    "\n",
    "**The Scope of Database Testing:**\n",
    "\n",
    "Unlike unit testing which isolates code components, database testing validates:\n",
    "- **Structural Integrity:** Tables, columns, constraints, and relationships match design specifications\n",
    "- **Data Integrity:** Business rules are enforced at the database level, preventing invalid data states\n",
    "- **Functional Correctness:** Stored procedures, triggers, and functions execute business logic accurately\n",
    "- **Performance:** Queries execute within acceptable time limits under expected load\n",
    "- **Security:** Access controls prevent unauthorized data access or modification\n",
    "- **Recoverability:** Data can be restored accurately after failures or disasters\n",
    "\n",
    "**Testing Pyramid for Databases:**\n",
    "```\n",
    "         \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "         \u2502   UI/E2E    \u2502  (Verify data flows end-to-end)\n",
    "         \u2502    Tests    \u2502\n",
    "        \u250c\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2510\n",
    "        \u2502  Integration   \u2502  (APIs + Database interactions)\n",
    "        \u2502    Testing     \u2502\n",
    "       \u250c\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2510\n",
    "       \u2502   Database Unit  \u2502  (Stored procedures, functions)\n",
    "       \u2502     Testing      \u2502\n",
    "      \u250c\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2510\n",
    "      \u2502  Schema/Data Unit  \u2502  (Constraints, triggers, CRUD)\n",
    "      \u2502     Testing        \u2502\n",
    "      \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **31.2 Schema Testing**\n",
    "\n",
    "Schema testing validates that the database structure matches the approved design and supports application requirements.\n",
    "\n",
    "### **31.2.1 Structural Validation**\n",
    "\n",
    "Schema testing verifies that tables, columns, data types, and constraints exist as specified.\n",
    "\n",
    "```python\n",
    "class SchemaTesting:\n",
    "    \"\"\"\n",
    "    Automated schema validation against specifications\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_table_existence(self, db_connection, expected_schema):\n",
    "        \"\"\"\n",
    "        Verify all required tables exist\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        # Get actual tables\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT table_name \n",
    "            FROM information_schema.tables \n",
    "            WHERE table_schema = DATABASE()\n",
    "        \"\"\")\n",
    "        actual_tables = {row[0] for row in cursor.fetchall()}\n",
    "        \n",
    "        # Compare with expected\n",
    "        expected_tables = set(expected_schema.keys())\n",
    "        \n",
    "        missing_tables = expected_tables - actual_tables\n",
    "        extra_tables = actual_tables - expected_tables\n",
    "        \n",
    "        assert len(missing_tables) == 0, f\"Missing tables: {missing_tables}\"\n",
    "        \n",
    "        # Extra tables might be acceptable (audit logs, etc.), but flag them\n",
    "        if extra_tables:\n",
    "            print(f\"Warning: Unexpected tables found: {extra_tables}\")\n",
    "    \n",
    "    def test_column_definitions(self, db_connection, table_name, expected_columns):\n",
    "        \"\"\"\n",
    "        Verify column names, types, and constraints\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                column_name,\n",
    "                data_type,\n",
    "                is_nullable,\n",
    "                column_default,\n",
    "                character_maximum_length\n",
    "            FROM information_schema.columns\n",
    "            WHERE table_name = %s\n",
    "            ORDER BY ordinal_position\n",
    "        \"\"\", (table_name,))\n",
    "        \n",
    "        actual_columns = {}\n",
    "        for row in cursor.fetchall():\n",
    "            actual_columns[row[0]] = {\n",
    "                'type': row[1],\n",
    "                'nullable': row[2] == 'YES',\n",
    "                'default': row[3],\n",
    "                'max_length': row[4]\n",
    "            }\n",
    "        \n",
    "        # Validate each expected column\n",
    "        for col_name, specs in expected_columns.items():\n",
    "            assert col_name in actual_columns, f\"Column {col_name} missing from {table_name}\"\n",
    "            \n",
    "            actual = actual_columns[col_name]\n",
    "            \n",
    "            # Data type validation (with aliases normalized)\n",
    "            expected_type = self.normalize_type(specs['type'])\n",
    "            actual_type = self.normalize_type(actual['type'])\n",
    "            \n",
    "            assert actual_type == expected_type, \\\n",
    "                f\"{table_name}.{col_name}: Expected {expected_type}, got {actual_type}\"\n",
    "            \n",
    "            # Nullable constraint\n",
    "            if specs.get('nullable') is not None:\n",
    "                assert actual['nullable'] == specs['nullable'], \\\n",
    "                    f\"{table_name}.{col_name}: Nullable mismatch\"\n",
    "            \n",
    "            # Default value\n",
    "            if 'default' in specs:\n",
    "                assert actual['default'] == specs['default'], \\\n",
    "                    f\"{table_name}.{col_name}: Default value mismatch\"\n",
    "    \n",
    "    def normalize_type(self, data_type):\n",
    "        \"\"\"Normalize type names across databases\"\"\"\n",
    "        type_mapping = {\n",
    "            'int': 'integer',\n",
    "            'bigint': 'bigint',\n",
    "            'varchar': 'character varying',\n",
    "            'text': 'text',\n",
    "            'timestamp': 'timestamp without time zone',\n",
    "            'datetime': 'timestamp',\n",
    "            'bool': 'boolean',\n",
    "            'decimal': 'numeric',\n",
    "            'float': 'double precision'\n",
    "        }\n",
    "        return type_mapping.get(data_type.lower(), data_type.lower())\n",
    "    \n",
    "    def test_index_existence(self, db_connection, expected_indexes):\n",
    "        \"\"\"\n",
    "        Verify required indexes exist for performance\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT \n",
    "                table_name,\n",
    "                index_name,\n",
    "                column_name\n",
    "            FROM information_schema.statistics\n",
    "            WHERE table_schema = DATABASE()\n",
    "            ORDER BY table_name, index_name, seq_in_index\n",
    "        \"\"\")\n",
    "        \n",
    "        actual_indexes = {}\n",
    "        for row in cursor.fetchall():\n",
    "            table, idx_name, col = row\n",
    "            key = f\"{table}.{idx_name}\"\n",
    "            if key not in actual_indexes:\n",
    "                actual_indexes[key] = []\n",
    "            actual_indexes[key].append(col)\n",
    "        \n",
    "        # Check expected indexes exist\n",
    "        for idx_spec in expected_indexes:\n",
    "            table = idx_spec['table']\n",
    "            columns = tuple(idx_spec['columns'])\n",
    "            idx_name = idx_spec.get('name', f\"idx_{table}_{'_'.join(columns)}\")\n",
    "            \n",
    "            # Check if any index covers these columns (in order)\n",
    "            found = False\n",
    "            for key, idx_columns in actual_indexes.items():\n",
    "                if key.startswith(f\"{table}.\") and tuple(idx_columns[:len(columns)]) == columns:\n",
    "                    found = True\n",
    "                    break\n",
    "            \n",
    "            assert found, f\"Index missing on {table}({', '.join(columns)})\"\n",
    "```\n",
    "\n",
    "### **31.2.2 Constraint Validation**\n",
    "\n",
    "Constraints enforce business rules at the database level.\n",
    "\n",
    "```python\n",
    "def test_primary_key_constraints(self, db_connection, table_name, pk_columns):\n",
    "    \"\"\"\n",
    "    Verify primary key constraints (uniqueness and not-null)\n",
    "    \"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    \n",
    "    # Test 1: Uniqueness\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT {', '.join(pk_columns)}, COUNT(*) as cnt\n",
    "        FROM {table_name}\n",
    "        GROUP BY {', '.join(pk_columns)}\n",
    "        HAVING COUNT(*) > 1\n",
    "    \"\"\")\n",
    "    \n",
    "    duplicates = cursor.fetchall()\n",
    "    assert len(duplicates) == 0, f\"Duplicate PKs found in {table_name}: {duplicates}\"\n",
    "    \n",
    "    # Test 2: NOT NULL\n",
    "    for col in pk_columns:\n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT COUNT(*) FROM {table_name} WHERE {col} IS NULL\n",
    "        \"\"\")\n",
    "        null_count = cursor.fetchone()[0]\n",
    "        assert null_count == 0, f\"NULL values found in PK column {table_name}.{col}\"\n",
    "\n",
    "def test_foreign_key_integrity(self, db_connection, child_table, fk_column, parent_table, pk_column):\n",
    "    \"\"\"\n",
    "    Verify referential integrity between tables\n",
    "    \"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    \n",
    "    # Find orphaned records\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT c.{fk_column}\n",
    "        FROM {child_table} c\n",
    "        LEFT JOIN {parent_table} p ON c.{fk_column} = p.{pk_column}\n",
    "        WHERE p.{pk_column} IS NULL\n",
    "          AND c.{fk_column} IS NOT NULL\n",
    "    \"\"\")\n",
    "    \n",
    "    orphans = cursor.fetchall()\n",
    "    assert len(orphans) == 0, f\"Orphaned records in {child_table}: {orphans[:5]}...\"\n",
    "\n",
    "def test_check_constraints(self, db_connection, table_name):\n",
    "    \"\"\"\n",
    "    Verify CHECK constraints are enforced\n",
    "    \"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    \n",
    "    # Example: Price must be > 0\n",
    "    try:\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO {table_name} (name, price) VALUES ('Test', -10)\n",
    "        \"\"\")\n",
    "        db_connection.commit()\n",
    "        assert False, \"CHECK constraint not enforced: Negative price allowed\"\n",
    "    except IntegrityError:\n",
    "        db_connection.rollback()  # Expected behavior\n",
    "    \n",
    "    # Example: Status must be in allowed values\n",
    "    try:\n",
    "        cursor.execute(f\"\"\"\n",
    "            INSERT INTO {table_name} (name, status) VALUES ('Test', 'invalid_status')\n",
    "        \"\"\")\n",
    "        db_connection.commit()\n",
    "        assert False, \"CHECK constraint not enforced: Invalid status allowed\"\n",
    "    except IntegrityError:\n",
    "        db_connection.rollback()\n",
    "\n",
    "def test_unique_constraints(self, db_connection, table_name, unique_columns):\n",
    "    \"\"\"\n",
    "    Verify unique constraints (emails, usernames, etc.)\n",
    "    \"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    \n",
    "    # Find duplicates\n",
    "    cursor.execute(f\"\"\"\n",
    "        SELECT {', '.join(unique_columns)}, COUNT(*) as cnt\n",
    "        FROM {table_name}\n",
    "        GROUP BY {', '.join(unique_columns)}\n",
    "        HAVING COUNT(*) > 1\n",
    "    \"\"\")\n",
    "    \n",
    "    duplicates = cursor.fetchall()\n",
    "    assert len(duplicates) == 0, f\"Unique constraint violations in {table_name}: {duplicates}\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **31.3 Data Integrity Testing**\n",
    "\n",
    "Data integrity testing ensures that business rules are maintained regardless of how data enters the system (UI, API, bulk import, or direct SQL).\n",
    "\n",
    "### **31.3.1 Entity Integrity**\n",
    "\n",
    "```python\n",
    "class DataIntegrityTesting:\n",
    "    \"\"\"\n",
    "    Comprehensive data integrity validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_no_duplicate_records(self, db_connection, table_name, unique_key_fields):\n",
    "        \"\"\"\n",
    "        Detect duplicate business keys (not just surrogate keys)\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        # Composite unique key check\n",
    "        fields_str = ', '.join(unique_key_fields)\n",
    "        \n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT {fields_str}, COUNT(*) as duplicate_count\n",
    "            FROM {table_name}\n",
    "            GROUP BY {fields_str}\n",
    "            HAVING COUNT(*) > 1\n",
    "        \"\"\")\n",
    "        \n",
    "        duplicates = cursor.fetchall()\n",
    "        \n",
    "        if duplicates:\n",
    "            # Log first few duplicates for analysis\n",
    "            for dup in duplicates[:5]:\n",
    "                print(f\"Duplicate found: {dict(zip(unique_key_fields, dup))}\")\n",
    "            \n",
    "            assert False, f\"Found {len(duplicates)} duplicate records in {table_name}\"\n",
    "    \n",
    "    def test_mandatory_fields(self, db_connection, table_name, required_fields):\n",
    "        \"\"\"\n",
    "        Verify critical fields never contain NULL or empty values\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        for field in required_fields:\n",
    "            # Check NULL\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT COUNT(*) FROM {table_name} WHERE {field} IS NULL\n",
    "            \"\"\")\n",
    "            null_count = cursor.fetchone()[0]\n",
    "            \n",
    "            # Check empty strings (for text fields)\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT COUNT(*) FROM {table_name} WHERE {field} = ''\n",
    "            \"\"\")\n",
    "            empty_count = cursor.fetchone()[0]\n",
    "            \n",
    "            total_invalid = null_count + empty_count\n",
    "            \n",
    "            assert total_invalid == 0, \\\n",
    "                f\"{table_name}.{field} has {null_count} NULLs and {empty_count} empty strings\"\n",
    "    \n",
    "    def test_data_format_consistency(self, db_connection, table_name, field, pattern):\n",
    "        \"\"\"\n",
    "        Verify data follows expected formats (regex validation)\n",
    "        \"\"\"\n",
    "        import re\n",
    "        \n",
    "        cursor = db_connection.cursor()\n",
    "        cursor.execute(f\"SELECT {field} FROM {table_name} WHERE {field} IS NOT NULL\")\n",
    "        \n",
    "        invalid_values = []\n",
    "        for (value,) in cursor.fetchall():\n",
    "            if not re.match(pattern, str(value)):\n",
    "                invalid_values.append(value)\n",
    "        \n",
    "        assert len(invalid_values) == 0, \\\n",
    "            f\"Invalid format in {table_name}.{field}: {invalid_values[:10]}\"\n",
    "```\n",
    "\n",
    "### **31.3.2 Referential Integrity Across Systems**\n",
    "\n",
    "When data spans multiple databases or services:\n",
    "\n",
    "```python\n",
    "def test_cross_system_referential_integrity(self, source_db, target_db, mapping_config):\n",
    "    \"\"\"\n",
    "    Verify data consistency between OLTP and Data Warehouse, or microservices\n",
    "    \"\"\"\n",
    "    # Example: Verify all users in Auth Service exist in User Profile Service\n",
    "    source_cursor = source_db.cursor()\n",
    "    target_cursor = target_db.cursor()\n",
    "    \n",
    "    # Get all IDs from source\n",
    "    source_cursor.execute(f\"\"\"\n",
    "        SELECT {mapping_config['source_id_field']}, {mapping_config['checksum_fields']}\n",
    "        FROM {mapping_config['source_table']}\n",
    "        WHERE {mapping_config['filter_condition']}\n",
    "    \"\"\")\n",
    "    \n",
    "    source_data = {row[0]: row[1:] for row in source_cursor.fetchall()}\n",
    "    \n",
    "    # Get corresponding IDs from target\n",
    "    target_cursor.execute(f\"\"\"\n",
    "        SELECT {mapping_config['target_id_field']}, {mapping_config['checksum_fields']}\n",
    "        FROM {mapping_config['target_table']}\n",
    "    \"\"\")\n",
    "    \n",
    "    target_data = {row[0]: row[1:] for row in target_cursor.fetchall()}\n",
    "    \n",
    "    # Find missing in target\n",
    "    missing_in_target = set(source_data.keys()) - set(target_data.keys())\n",
    "    assert len(missing_in_target) == 0, \\\n",
    "        f\"Records missing in target: {list(missing_in_target)[:10]}\"\n",
    "    \n",
    "    # Find data mismatches (if checksums provided)\n",
    "    if mapping_config.get('verify_checksums'):\n",
    "        mismatches = []\n",
    "        for id_key in source_data:\n",
    "            if source_data[id_key] != target_data.get(id_key):\n",
    "                mismatches.append(id_key)\n",
    "        \n",
    "        assert len(mismatches) == 0, \\\n",
    "            f\"Data mismatches for IDs: {mismatches[:10]}\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **31.4 Database Security Testing**\n",
    "\n",
    "Security testing ensures that data is protected from unauthorized access and injection attacks.\n",
    "\n",
    "### **31.4.1 Privilege and Access Control Testing**\n",
    "\n",
    "```python\n",
    "class DatabaseSecurityTesting:\n",
    "    \"\"\"\n",
    "    Security testing for database layer\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_principle_of_least_privilege(self, db_connection, user_config):\n",
    "        \"\"\"\n",
    "        Verify users only have necessary permissions\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        # Get actual permissions\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT table_name, privilege_type\n",
    "            FROM information_schema.table_privileges\n",
    "            WHERE grantee = %s\n",
    "        \"\"\", (user_config['username'],))\n",
    "        \n",
    "        actual_permissions = {(row[0], row[1]) for row in cursor.fetchall()}\n",
    "        expected_permissions = set(user_config['expected_permissions'])\n",
    "        \n",
    "        # Check for excessive permissions\n",
    "        excess = actual_permissions - expected_permissions\n",
    "        assert len(excess) == 0, f\"User has excessive permissions: {excess}\"\n",
    "        \n",
    "        # Check for missing permissions\n",
    "        missing = expected_permissions - actual_permissions\n",
    "        assert len(missing) == 0, f\"User missing required permissions: {missing}\"\n",
    "    \n",
    "    def test_sql_injection_prevention(self, db_connection, vulnerable_endpoint):\n",
    "        \"\"\"\n",
    "        Test parameterized queries prevent injection\n",
    "        \"\"\"\n",
    "        injection_attempts = [\n",
    "            \"' OR '1'='1\",\n",
    "            \"'; DROP TABLE users; --\",\n",
    "            \"1 UNION SELECT * FROM passwords\",\n",
    "            \"' OR 1=1--\",\n",
    "            \"1; DELETE FROM orders WHERE '1'='1\"\n",
    "        ]\n",
    "        \n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        for payload in injection_attempts:\n",
    "            try:\n",
    "                # Attempt injection through application's query method\n",
    "                # This simulates what happens if app doesn't use parameterized queries\n",
    "                result = vulnerable_endpoint.search(payload)\n",
    "                \n",
    "                # If we get results or no error, injection might have worked\n",
    "                if result and len(result) > 0:\n",
    "                    # Check if result contains unexpected data (union attack success)\n",
    "                    if 'password' in str(result).lower() or 'admin' in str(result).lower():\n",
    "                        assert False, f\"Potential SQL Injection vulnerability with payload: {payload}\"\n",
    "                        \n",
    "            except Exception as e:\n",
    "                # Expected: Should raise error or return empty results safely\n",
    "                pass  # Good - query failed safely or returned no data\n",
    "    \n",
    "    def test_sensitive_data_encryption(self, db_connection, table_name, sensitive_fields):\n",
    "        \"\"\"\n",
    "        Verify sensitive data is encrypted at rest\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        for field in sensitive_fields:\n",
    "            cursor.execute(f\"\"\"\n",
    "                SELECT {field} \n",
    "                FROM {table_name} \n",
    "                WHERE {field} IS NOT NULL \n",
    "                LIMIT 10\n",
    "            \"\"\")\n",
    "            \n",
    "            for (value,) in cursor.fetchall():\n",
    "                # Check if value appears encrypted (not plaintext)\n",
    "                # This is a heuristic - adjust based on encryption method\n",
    "                is_encrypted = (\n",
    "                    isinstance(value, bytes) or  # Binary encryption\n",
    "                    value.startswith('enc:') or   # Prefix marker\n",
    "                    len(value) > 100 and not value.isalnum()  # Long random string\n",
    "                )\n",
    "                \n",
    "                # If it looks like plaintext PII, flag it\n",
    "                if '@' in str(value) or str(value).isdigit():  # Email or SSN pattern\n",
    "                    assert False, \\\n",
    "                        f\"Sensitive field {field} appears unencrypted: {value[:10]}...\"\n",
    "    \n",
    "    def test_audit_logging(self, db_connection, audit_table):\n",
    "        \"\"\"\n",
    "        Verify sensitive operations are logged\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        # Perform sensitive operation\n",
    "        test_user = f\"audit_test_{time.time()}\"\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO users (username, email) VALUES (%s, %s)\n",
    "        \"\"\", (test_user, \"audit@test.com\"))\n",
    "        db_connection.commit()\n",
    "        \n",
    "        # Check audit log\n",
    "        time.sleep(0.5)  # Allow async logging\n",
    "        \n",
    "        cursor.execute(f\"\"\"\n",
    "            SELECT * FROM {audit_table}\n",
    "            WHERE table_name = 'users'\n",
    "            AND action = 'INSERT'\n",
    "            AND new_values LIKE %s\n",
    "            ORDER BY timestamp DESC\n",
    "            LIMIT 1\n",
    "        \"\"\", (f\"%{test_user}%\",))\n",
    "        \n",
    "        audit_record = cursor.fetchone()\n",
    "        assert audit_record is not None, \"Audit log entry not created for sensitive operation\"\n",
    "        \n",
    "        # Verify audit contains required fields\n",
    "        assert audit_record['user'] is not None, \"Audit missing user who performed action\"\n",
    "        assert audit_record['timestamp'] is not None, \"Audit missing timestamp\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **31.5 Backup and Recovery Testing**\n",
    "\n",
    "Data durability requires that backups are valid and recovery procedures work within RTO/RPO constraints.\n",
    "\n",
    "### **31.5.1 Backup Validation**\n",
    "\n",
    "```python\n",
    "class BackupTesting:\n",
    "    \"\"\"\n",
    "    Testing database backup and recovery procedures\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_backup_integrity(self, backup_file_path):\n",
    "        \"\"\"\n",
    "        Verify backup file is valid and restorable\n",
    "        \"\"\"\n",
    "        import subprocess\n",
    "        \n",
    "        # Check backup file exists and has content\n",
    "        assert os.path.exists(backup_file_path), \"Backup file not found\"\n",
    "        assert os.path.getsize(backup_file_path) > 0, \"Backup file is empty\"\n",
    "        \n",
    "        # Verify backup format (e.g., SQL dump, binary)\n",
    "        if backup_file_path.endswith('.sql'):\n",
    "            # Check SQL syntax validity (basic)\n",
    "            with open(backup_file_path, 'r') as f:\n",
    "                header = f.read(1000)\n",
    "                assert 'CREATE TABLE' in header or 'INSERT INTO' in header or 'COPY' in header\n",
    "            \n",
    "            # Try restore to temporary database\n",
    "            result = subprocess.run([\n",
    "                'mysql', '--execute', f'SOURCE {backup_file_path}',\n",
    "                '--database', 'test_restore_db'\n",
    "            ], capture_output=True, text=True)\n",
    "            \n",
    "            assert result.returncode == 0, f\"Backup restore failed: {result.stderr}\"\n",
    "    \n",
    "    def test_point_in_time_recovery(self, db_connection, backup_time, test_scenario):\n",
    "        \"\"\"\n",
    "        Verify database can be restored to specific point in time\n",
    "        \"\"\"\n",
    "        # This requires binary logging (MySQL) or WAL archiving (PostgreSQL)\n",
    "        \n",
    "        # 1. Note current state\n",
    "        cursor = db_connection.cursor()\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM transactions\")\n",
    "        count_before = cursor.fetchone()[0]\n",
    "        \n",
    "        # 2. Simulate data loss scenario\n",
    "        cursor.execute(\"DELETE FROM transactions WHERE created_at > %s\", (backup_time,))\n",
    "        db_connection.commit()\n",
    "        \n",
    "        # 3. Restore from backup to temporary instance\n",
    "        # (Implementation depends on infrastructure)\n",
    "        \n",
    "        # 4. Apply binary logs up to specific time\n",
    "        \n",
    "        # 5. Verify data consistency\n",
    "        cursor.execute(\"SELECT COUNT(*) FROM transactions\")\n",
    "        count_after = cursor.fetchone()[0]\n",
    "        \n",
    "        assert count_after == count_before, \"Point-in-time recovery did not restore expected data\"\n",
    "    \n",
    "    def test_replication_lag(self, primary_db, replica_db, max_lag_seconds=5):\n",
    "        \"\"\"\n",
    "        Verify read replicas are within acceptable lag\n",
    "        \"\"\"\n",
    "        # Write to primary\n",
    "        primary_cursor = primary_db.cursor()\n",
    "        test_marker = f\"lag_test_{time.time()}\"\n",
    "        \n",
    "        primary_cursor.execute(\"\"\"\n",
    "            INSERT INTO replication_test (marker, created_at) \n",
    "            VALUES (%s, NOW())\n",
    "        \"\"\", (test_marker,))\n",
    "        primary_db.commit()\n",
    "        write_time = time.time()\n",
    "        \n",
    "        # Poll replica until found or timeout\n",
    "        replica_cursor = replica_db.cursor()\n",
    "        found = False\n",
    "        timeout = time.time() + max_lag_seconds\n",
    "        \n",
    "        while time.time() < timeout:\n",
    "            replica_cursor.execute(\"\"\"\n",
    "                SELECT 1 FROM replication_test WHERE marker = %s\n",
    "            \"\"\", (test_marker,))\n",
    "            if replica_cursor.fetchone():\n",
    "                found = True\n",
    "                lag = time.time() - write_time\n",
    "                break\n",
    "            time.sleep(0.1)\n",
    "        \n",
    "        assert found, f\"Replication lag exceeded {max_lag_seconds} seconds\"\n",
    "        print(f\"Replication lag: {lag:.2f} seconds\")\n",
    "\n",
    "    def test_failover_procedure(self, primary_host, replica_host, app_connection):\n",
    "        \"\"\"\n",
    "        Test automatic failover to replica when primary fails\n",
    "        \"\"\"\n",
    "        # Simulate primary failure (stop service or block network)\n",
    "        # This is typically done in staging environment\n",
    "        \n",
    "        # Verify app switches to replica\n",
    "        # Check for error rates during transition\n",
    "        \n",
    "        # Verify no split-brain (old primary must not accept writes if it comes back)\n",
    "        pass  # Implementation depends on specific HA setup (PgPool, Patroni, etc.)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **31.6 Concurrency and Lock Testing**\n",
    "\n",
    "Testing how the database handles simultaneous operations is critical for multi-user applications.\n",
    "\n",
    "### **31.6.1 Deadlock Detection**\n",
    "\n",
    "```python\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "class ConcurrencyTesting:\n",
    "    def test_deadlock_scenario(self, db_pool):\n",
    "        \"\"\"\n",
    "        Test that concurrent updates don't cause deadlocks\n",
    "        or that deadlocks are handled gracefully\n",
    "        \"\"\"\n",
    "        errors = queue.Queue()\n",
    "        results = queue.Queue()\n",
    "        \n",
    "        def transaction_a():\n",
    "            \"\"\"T1: Update account 1, then account 2\"\"\"\n",
    "            try:\n",
    "                conn = db_pool.get_connection()\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                cursor.execute(\"START TRANSACTION\")\n",
    "                cursor.execute(\"UPDATE accounts SET balance = balance - 100 WHERE id = 1\")\n",
    "                time.sleep(0.1)  # Simulate processing\n",
    "                cursor.execute(\"UPDATE accounts SET balance = balance + 100 WHERE id = 2\")\n",
    "                conn.commit()\n",
    "                results.put(('A', 'success'))\n",
    "            except Exception as e:\n",
    "                if 'deadlock' in str(e).lower():\n",
    "                    results.put(('A', 'deadlock'))\n",
    "                else:\n",
    "                    errors.put(('A', str(e)))\n",
    "            finally:\n",
    "                conn.close()\n",
    "        \n",
    "        def transaction_b():\n",
    "            \"\"\"T2: Update account 2, then account 1 (opposite order - classic deadlock)\"\"\"\n",
    "            try:\n",
    "                conn = db_pool.get_connection()\n",
    "                cursor = conn.cursor()\n",
    "                \n",
    "                cursor.execute(\"START TRANSACTION\")\n",
    "                cursor.execute(\"UPDATE accounts SET balance = balance - 50 WHERE id = 2\")\n",
    "                time.sleep(0.1)\n",
    "                cursor.execute(\"UPDATE accounts SET balance = balance + 50 WHERE id = 1\")\n",
    "                conn.commit()\n",
    "                results.put(('B', 'success'))\n",
    "            except Exception as e:\n",
    "                if 'deadlock' in str(e).lower():\n",
    "                    results.put(('B', 'deadlock'))\n",
    "                else:\n",
    "                    errors.put(('B', str(e)))\n",
    "            finally:\n",
    "                conn.close()\n",
    "        \n",
    "        # Run both transactions simultaneously\n",
    "        t1 = threading.Thread(target=transaction_a)\n",
    "        t2 = threading.Thread(target=transaction_b)\n",
    "        \n",
    "        t1.start()\n",
    "        t2.start()\n",
    "        t1.join()\n",
    "        t2.join()\n",
    "        \n",
    "        # Collect results\n",
    "        outcomes = []\n",
    "        while not results.empty():\n",
    "            outcomes.append(results.get())\n",
    "        \n",
    "        # At least one should succeed, deadlocks should be handled (retried or errored gracefully)\n",
    "        success_count = sum(1 for _, status in outcomes if status == 'success')\n",
    "        deadlock_count = sum(1 for _, status in outcomes if status == 'deadlock')\n",
    "        \n",
    "        assert success_count >= 1, \"Both transactions failed\"\n",
    "        \n",
    "        # If deadlock occurred, verify data consistency (no partial updates)\n",
    "        if deadlock_count > 0:\n",
    "            conn = db_pool.get_connection()\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT balance FROM accounts WHERE id IN (1, 2)\")\n",
    "            balances = cursor.fetchall()\n",
    "            # Verify balances are consistent (no lost updates)\n",
    "```\n",
    "\n",
    "### **31.6.2 Isolation Level Testing**\n",
    "\n",
    "```python\n",
    "def test_read_committed_isolation(self, db_connection):\n",
    "    \"\"\"\n",
    "    Verify READ COMMITTED prevents dirty reads but allows non-repeatable reads\n",
    "    \"\"\"\n",
    "    cursor = db_connection.cursor()\n",
    "    \n",
    "    # Setup\n",
    "    cursor.execute(\"UPDATE accounts SET balance = 1000 WHERE id = 1\")\n",
    "    db_connection.commit()\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    def reader():\n",
    "        \"\"\"Read balance twice with delay\"\"\"\n",
    "        conn = get_new_connection()\n",
    "        cur = conn.cursor()\n",
    "        \n",
    "        # First read\n",
    "        cur.execute(\"SELECT balance FROM accounts WHERE id = 1\")\n",
    "        first_read = cur.fetchone()[0]\n",
    "        \n",
    "        time.sleep(0.5)  # Allow writer to commit\n",
    "        \n",
    "        # Second read (may see different value in READ COMMITTED)\n",
    "        cur.execute(\"SELECT balance FROM accounts WHERE id = 1\")\n",
    "        second_read = cur.fetchone()[0]\n",
    "        \n",
    "        results['reader'] = (first_read, second_read)\n",
    "    \n",
    "    def writer():\n",
    "        \"\"\"Update and commit while reader is paused\"\"\"\n",
    "        time.sleep(0.2)  # Let reader start first\n",
    "        \n",
    "        conn = get_new_connection()\n",
    "        cur = conn.cursor()\n",
    "        cur.execute(\"UPDATE accounts SET balance = 2000 WHERE id = 1\")\n",
    "        conn.commit()\n",
    "        results['writer'] = 'committed'\n",
    "    \n",
    "    t1 = threading.Thread(target=reader)\n",
    "    t2 = threading.Thread(target=writer)\n",
    "    t1.start()\n",
    "    t2.start()\n",
    "    t1.join()\n",
    "    t2.join()\n",
    "    \n",
    "    first, second = results['reader']\n",
    "    # In READ COMMITTED: first might be 1000, second should be 2000 (no dirty read, but non-repeatable)\n",
    "    assert first == 1000, \"Dirty read occurred\"\n",
    "    assert second == 2000, \"Should see committed data on second read\"\n",
    "\n",
    "def test_serializable_isolation(self, db_connection):\n",
    "    \"\"\"\n",
    "    Verify SERIALIZABLE prevents phantom reads\n",
    "    \"\"\"\n",
    "    # This is the strictest isolation level\n",
    "    # Test that range queries are protected from inserts by other transactions\n",
    "    pass  # Implementation similar to above with range queries\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **31.7 Database Migration Testing**\n",
    "\n",
    "Schema changes are high-risk operations requiring thorough validation.\n",
    "\n",
    "### **31.7.1 Schema Change Validation**\n",
    "\n",
    "```python\n",
    "class MigrationTesting:\n",
    "    def test_migration_idempotency(self, migration_script, db_connection):\n",
    "        \"\"\"\n",
    "        Verify migration can run multiple times without error (idempotent)\n",
    "        \"\"\"\n",
    "        # First run\n",
    "        result1 = self.run_migration(migration_script, db_connection)\n",
    "        assert result1['success'], f\"First migration failed: {result1['error']}\"\n",
    "        \n",
    "        # Second run (should succeed or gracefully skip)\n",
    "        result2 = self.run_migration(migration_script, db_connection)\n",
    "        \n",
    "        # Should either succeed with no changes or report already applied\n",
    "        assert result2['success'] or 'already exists' in str(result2.get('error', '')).lower()\n",
    "    \n",
    "    def test_rollback_procedure(self, migration, db_connection):\n",
    "        \"\"\"\n",
    "        Verify downgrade/rollback script works correctly\n",
    "        \"\"\"\n",
    "        # Apply migration\n",
    "        migration.upgrade(db_connection)\n",
    "        \n",
    "        # Add test data in new schema\n",
    "        cursor = db_connection.cursor()\n",
    "        cursor.execute(\"INSERT INTO new_table (col) VALUES ('test')\")\n",
    "        db_connection.commit()\n",
    "        \n",
    "        # Rollback\n",
    "        migration.downgrade(db_connection)\n",
    "        \n",
    "        # Verify old schema restored\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT COUNT(*) FROM information_schema.tables \n",
    "            WHERE table_name = 'new_table'\n",
    "        \"\"\")\n",
    "        count = cursor.fetchone()[0]\n",
    "        assert count == 0, \"Rollback did not remove new table\"\n",
    "        \n",
    "        # Verify application still works with old schema\n",
    "        # (Integration test with app)\n",
    "    \n",
    "    def test_data_migration_accuracy(self, db_connection, migration_config):\n",
    "        \"\"\"\n",
    "        Verify data transformations during migration are accurate\n",
    "        \"\"\"\n",
    "        cursor = db_connection.cursor()\n",
    "        \n",
    "        # Setup pre-migration state\n",
    "        cursor.execute(\"\"\"\n",
    "            INSERT INTO old_orders (total_cents, currency) \n",
    "            VALUES (10000, 'USD'), (5000, 'EUR')\n",
    "        \"\"\")\n",
    "        db_connection.commit()\n",
    "        \n",
    "        # Run migration that splits amount into dollars/cents\n",
    "        self.run_migration(migration_config['script'], db_connection)\n",
    "        \n",
    "        # Verify data transformed correctly\n",
    "        cursor.execute(\"SELECT total_dollars, total_cents, currency FROM new_orders\")\n",
    "        rows = cursor.fetchall()\n",
    "        \n",
    "        assert len(rows) == 2\n",
    "        for dollars, cents, currency in rows:\n",
    "            assert dollars >= 0, \"Negative dollars after migration\"\n",
    "            assert 0 <= cents < 100, \"Cents out of range\"\n",
    "    \n",
    "    def test_zero_downtime_migration(self, db_connection, blue_green_config):\n",
    "        \"\"\"\n",
    "        Test expand-contract pattern for zero-downtime deployments\n",
    "        \"\"\"\n",
    "        # Phase 1: Expand (add new column alongside old)\n",
    "        # Verify reads work from both old and new\n",
    "        # Verify writes update both\n",
    "        \n",
    "        # Phase 2: Migrate data in background\n",
    "        # Verify data consistency between old and new columns\n",
    "        \n",
    "        # Phase 3: Contract (remove old column)\n",
    "        # Verify application uses new column exclusively\n",
    "        pass\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **31.8 NoSQL Database Testing**\n",
    "\n",
    "NoSQL databases require different testing approaches due to schema flexibility and consistency models.\n",
    "\n",
    "### **31.8.1 Document Database Testing (MongoDB)**\n",
    "\n",
    "```python\n",
    "class MongoDBTesting:\n",
    "    \"\"\"\n",
    "    Testing for document databases\n",
    "    \"\"\"\n",
    "    \n",
    "    def test_document_schema_validation(self, db, collection_name, schema):\n",
    "        \"\"\"\n",
    "        Verify documents match expected schema (even if DB is schemaless)\n",
    "        \"\"\"\n",
    "        from jsonschema import validate, ValidationError\n",
    "        \n",
    "        collection = db[collection_name]\n",
    "        errors = []\n",
    "        \n",
    "        for doc in collection.find().limit(1000):  # Sample first 1000\n",
    "            try:\n",
    "                validate(instance=doc, schema=schema)\n",
    "            except ValidationError as e:\n",
    "                errors.append({\n",
    "                    'id': doc.get('_id'),\n",
    "                    'error': str(e),\n",
    "                    'field': e.path\n",
    "                })\n",
    "        \n",
    "        assert len(errors) == 0, f\"Schema violations found: {errors[:5]}\"\n",
    "    \n",
    "    def test_replica_set_consistency(self, primary_db, secondary_db):\n",
    "        \"\"\"\n",
    "        Verify eventual consistency doesn't exceed acceptable delay\n",
    "        \"\"\"\n",
    "        # Write to primary\n",
    "        test_doc = {\"test_id\": f\"consistency_{time.time()}\", \"value\": \"test\"}\n",
    "        primary_db.test_collection.insert_one(test_doc)\n",
    "        write_time = time.time()\n",
    "        \n",
    "        # Read from secondary until found or timeout\n",
    "        timeout = time.time() + 5  # 5 second max lag\n",
    "        \n",
    "        while time.time() < timeout:\n",
    "            doc = secondary_db.test_collection.find_one({\"test_id\": test_doc['test_id']})\n",
    "            if doc:\n",
    "                lag = time.time() - write_time\n",
    "                assert doc['value'] == test_doc['value']\n",
    "                print(f\"Eventual consistency lag: {lag:.3f}s\")\n",
    "                return\n",
    "        \n",
    "        assert False, \"Secondary did not replicate within 5 seconds\"\n",
    "    \n",
    "    def test_sharding_distribution(self, db, collection_name):\n",
    "        \"\"\"\n",
    "        Verify data is evenly distributed across shards\n",
    "        \"\"\"\n",
    "        # MongoDB specific: Check chunk distribution\n",
    "        stats = db.command(\"collStats\", collection_name)\n",
    "        \n",
    "        if 'shards' in stats:\n",
    "            shard_sizes = {shard: info['size'] for shard, info in stats['shards'].items()}\n",
    "            \n",
    "            # Calculate coefficient of variation\n",
    "            import statistics\n",
    "            sizes = list(shard_sizes.values())\n",
    "            mean_size = statistics.mean(sizes)\n",
    "            stdev = statistics.stdev(sizes)\n",
    "            cv = stdev / mean_size if mean_size > 0 else 0\n",
    "            \n",
    "            # CV should be low for even distribution\n",
    "            assert cv < 0.3, f\"Uneven shard distribution: {shard_sizes}\"\n",
    "    \n",
    "    def test_atomic_operations(self, db):\n",
    "        \"\"\"\n",
    "        Test findAndModify atomicity\n",
    "        \"\"\"\n",
    "        collection = db.counter_test\n",
    "        \n",
    "        # Initialize counter\n",
    "        collection.insert_one({\"_id\": \"counter\", \"value\": 0})\n",
    "        \n",
    "        # Concurrent increments\n",
    "        import concurrent.futures\n",
    "        \n",
    "        def increment():\n",
    "            for _ in range(100):\n",
    "                collection.find_one_and_update(\n",
    "                    {\"_id\": \"counter\"},\n",
    "                    {\"$inc\": {\"value\": 1}}\n",
    "                )\n",
    "        \n",
    "        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "            futures = [executor.submit(increment) for _ in range(10)]\n",
    "            concurrent.futures.wait(futures)\n",
    "        \n",
    "        # Verify final count (should be exactly 1000)\n",
    "        final = collection.find_one({\"_id\": \"counter\"})\n",
    "        assert final['value'] == 1000, f\"Atomicity failure: expected 1000, got {final['value']}\"\n",
    "```\n",
    "\n",
    "### **31.8.2 Key-Value Store Testing (Redis)**\n",
    "\n",
    "```python\n",
    "class RedisTesting:\n",
    "    def test_data_expiration(self, redis_client):\n",
    "        \"\"\"\n",
    "        Verify TTL (Time To Live) is respected\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        # Set key with 2 second expiration\n",
    "        redis_client.setex(\"test_key\", 2, \"test_value\")\n",
    "        \n",
    "        # Should exist immediately\n",
    "        assert redis_client.get(\"test_key\") == b\"test_value\"\n",
    "        \n",
    "        # Should exist after 1 second\n",
    "        time.sleep(1)\n",
    "        assert redis_client.get(\"test_key\") == b\"test_value\"\n",
    "        \n",
    "        # Should expire after 3 seconds\n",
    "        time.sleep(2)\n",
    "        assert redis_client.get(\"test_key\") is None\n",
    "    \n",
    "    def test_persistence(self, redis_client):\n",
    "        \"\"\"\n",
    "        Test RDB/AOF persistence\n",
    "        \"\"\"\n",
    "        # Write data\n",
    "        redis_client.set(\"persistent_key\", \"persistent_value\")\n",
    "        \n",
    "        # Trigger BGSAVE (or wait for auto)\n",
    "        redis_client.bgsave()\n",
    "        time.sleep(1)\n",
    "        \n",
    "        # Simulate restart by reconnecting\n",
    "        new_client = get_redis_connection()\n",
    "        \n",
    "        # Verify data survived\n",
    "        value = new_client.get(\"persistent_key\")\n",
    "        assert value == b\"persistent_value\", \"Persistence failed\"\n",
    "    \n",
    "    def test_concurrent_access(self, redis_client):\n",
    "        \"\"\"\n",
    "        Test race conditions with INCR (atomic operation)\n",
    "        \"\"\"\n",
    "        import threading\n",
    "        \n",
    "        redis_client.set(\"counter\", 0)\n",
    "        \n",
    "        def increment():\n",
    "            for _ in range(100):\n",
    "                redis_client.incr(\"counter\")\n",
    "        \n",
    "        threads = [threading.Thread(target=increment) for _ in range(10)]\n",
    "        for t in threads:\n",
    "            t.start()\n",
    "        for t in threads:\n",
    "            t.join()\n",
    "        \n",
    "        final = int(redis_client.get(\"counter\"))\n",
    "        assert final == 1000, f\"Race condition: expected 1000, got {final}\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "### **Key Takeaways from Chapter 31:**\n",
    "\n",
    "**Schema Testing:**\n",
    "- **Automated validation:** Compare actual schema against specifications using information_schema\n",
    "- **Constraint verification:** Ensure PK uniqueness, FK referential integrity, CHECK constraints, and UNIQUE constraints are enforced\n",
    "- **Index validation:** Verify performance-critical indexes exist on foreign keys and frequently queried columns\n",
    "\n",
    "**Data Integrity Testing:**\n",
    "- **Entity integrity:** No duplicate business keys, mandatory fields populated, format consistency (regex validation)\n",
    "- **Referential integrity:** No orphaned records, cascading deletes work correctly, cross-system consistency maintained\n",
    "- **Domain integrity:** Values within valid ranges, valid enumeration values, proper data types\n",
    "\n",
    "**Security Testing:**\n",
    "- **Privilege validation:** Principle of least privilege enforced, no excessive grants\n",
    "- **SQL injection prevention:** Parameterized queries required, input sanitization verified\n",
    "- **Encryption at rest:** Sensitive fields (PII, credentials) stored encrypted, not plaintext\n",
    "- **Audit logging:** Sensitive operations (DDL, DCL, DML on critical tables) logged with user, timestamp, and before/after values\n",
    "\n",
    "**Backup and Recovery:**\n",
    "- **Backup integrity:** Files valid, restorable, corruption-free (checksum validation)\n",
    "- **RPO/RTO validation:** Point-in-time recovery tested, replication lag within SLA (< 5 seconds typical)\n",
    "- **Failover testing:** Automatic failover to replicas works, no split-brain scenarios\n",
    "\n",
    "**Concurrency Testing:**\n",
    "- **Deadlock detection:** Concurrent transactions don't cause indefinite blocking; deadlocks resolved via timeout or retry\n",
    "- **Isolation levels:** READ COMMITTED prevents dirty reads; SERIALIZABLE prevents phantom reads (verify based on requirements)\n",
    "- **Lock contention:** Long-running transactions don't block critical operations\n",
    "\n",
    "**Migration Testing:**\n",
    "- **Idempotency:** Migrations can be re-run safely without errors\n",
    "- **Rollback capability:** Downgrade scripts tested and restore previous schema without data loss\n",
    "- **Zero-downtime:** Expand-contract pattern verified for high-availability systems\n",
    "- **Data transformation:** Migration scripts correctly transform data types and relationships\n",
    "\n",
    "**NoSQL Testing:**\n",
    "- **Schema validation:** Even schemaless DBs need document structure validation (JSON Schema)\n",
    "- **Consistency models:** Eventual consistency lag measured and within acceptable bounds; strong consistency verified where required\n",
    "- **Sharding:** Data evenly distributed; no hot spots\n",
    "- **Atomic operations:** findAndModify, transactions (multi-document) tested for race conditions\n",
    "- **TTL/Persistence:** Expiration works correctly; persistence survives restarts\n",
    "\n",
    "---\n",
    "\n",
    "## **\ud83d\udcd6 Next Chapter: Chapter 32 - Test Data Management**\n",
    "\n",
    "With database testing techniques mastered, **Chapter 32** will focus on **Test Data Management strategies** to ensure you have the right data for comprehensive testing without compromising security or privacy.\n",
    "\n",
    "In **Chapter 32**, you'll learn:\n",
    "\n",
    "- **Test Data Strategy:** Deterministic vs. random data, production cloning vs. synthetic generation, subsetting strategies\n",
    "- **Data Masking and Anonymization:** Techniques for GDPR/CCPA compliance (k-anonymity, l-diversity), tokenization, and format-preserving encryption\n",
    "- **Synthetic Data Generation:** Using tools like Faker, Tonic, or Delphix to create realistic but fake datasets\n",
    "- **Test Data Subsetting:** Extracting representative samples from production while maintaining referential integrity\n",
    "- **Data Refresh Strategies:** Automated refresh pipelines, golden datasets, and self-service test data portals\n",
    "- **PII Handling:** Detecting and protecting personally identifiable information across test environments\n",
    "- **Test Data Pools:** Managing shared test data across teams, reservation systems, and data versioning\n",
    "\n",
    "**Chapter 32** completes your database testing expertise by ensuring you can efficiently create, manage, and maintain test data that enables thorough validation while protecting sensitive information.\n",
    "\n",
    "**Continue to Chapter 32 to master test data management and ensure your testing environments are both effective and compliant!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='30. sql_for_testers.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='32. database_testing_tools.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}