{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 32: Database Testing Tools**\n",
    "\n",
    "---\n",
    "\n",
    "## **Introduction**\n",
    "\n",
    "Database testing requires specialized tools that bridge the gap between application logic and data persistence layers. While manual verification through database clients is essential for exploratory testing, modern software development demands automated, repeatable, and scalable database testing solutions.\n",
    "\n",
    "This chapter explores the comprehensive toolkit available for database testing—from traditional database management clients to modern containerized test databases and automated data generation frameworks. Understanding these tools is crucial for implementing **Shift-Left testing** practices, where data validation occurs early and continuously in the development pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.1 Database Clients**\n",
    "\n",
    "Database clients are software applications that allow testers to interact directly with database systems. They serve as the primary interface for manual verification, ad-hoc querying, and initial test data setup.\n",
    "\n",
    "### **32.1.1 GUI-Based Database Clients**\n",
    "\n",
    "**Universal Database Tools (Cross-Platform):**\n",
    "\n",
    "**DBeaver (Community & Enterprise Editions)**\n",
    "- **Industry Standard**: Open-source universal database tool supporting 80+ database systems\n",
    "- **Testing Features**:\n",
    "  - SQL editor with syntax highlighting and autocomplete\n",
    "  - Query execution plan visualization (crucial for performance testing)\n",
    "  - Data export/import for test data migration\n",
    "  - ER diagram generation for schema validation\n",
    "  - Mock data generation (limited but useful for quick tests)\n",
    "\n",
    "```sql\n",
    "-- Example: Using DBeaver for test data verification\n",
    "-- Query to verify referential integrity after application operation\n",
    "SELECT \n",
    "    o.order_id,\n",
    "    o.customer_id,\n",
    "    c.customer_name,\n",
    "    COUNT(oi.item_id) as item_count,\n",
    "    SUM(oi.quantity * oi.unit_price) as calculated_total,\n",
    "    o.order_total as stored_total\n",
    "FROM orders o\n",
    "JOIN customers c ON o.customer_id = c.customer_id\n",
    "LEFT JOIN order_items oi ON o.order_id = oi.order_id\n",
    "WHERE o.created_date >= '2026-01-01'\n",
    "GROUP BY o.order_id, o.customer_id, c.customer_name, o.order_total\n",
    "HAVING ABS(SUM(oi.quantity * oi.unit_price) - o.order_total) > 0.01;\n",
    "-- This query validates that stored totals match calculated totals (data integrity test)\n",
    "```\n",
    "\n",
    "**DataGrip (JetBrains)**\n",
    "- **Best for**: Developers and testers working in IntelliJ ecosystem\n",
    "- **Features**: \n",
    "  - Smart code completion based on actual schema\n",
    "  - Version control integration for SQL scripts\n",
    "  - Compare database schemas (essential for migration testing)\n",
    "  - Parameterized queries for test data variation\n",
    "\n",
    "**Database-Specific Tools:**\n",
    "\n",
    "| Database | Primary Tool | Key Testing Features |\n",
    "|----------|--------------|---------------------|\n",
    "| **MySQL/MariaDB** | MySQL Workbench | Visual explain plans, data modeling, migration tools |\n",
    "| **PostgreSQL** | pgAdmin 4 | Query analyzer, server status monitoring, backup tools |\n",
    "| **Microsoft SQL Server** | SSMS (SQL Server Management Studio) | Query execution plans, profiler for performance testing, data comparison |\n",
    "| **Oracle** | SQL Developer | Unit testing framework for PL/SQL, data modeler |\n",
    "| **MongoDB** | MongoDB Compass | Aggregation pipeline builder, schema analysis |\n",
    "\n",
    "### **32.1.2 CLI (Command Line) Tools**\n",
    "\n",
    "For automation and CI/CD integration, CLI tools are essential:\n",
    "\n",
    "```bash\n",
    "# PostgreSQL - psql for automated testing scripts\n",
    "psql -h localhost -U testuser -d testdb -c \"\n",
    "  COPY (SELECT * FROM users WHERE created_at > NOW() - INTERVAL '1 day') \n",
    "  TO '/tmp/new_users.csv' WITH CSV HEADER;\n",
    "\"\n",
    "\n",
    "# MySQL - mysql command line for test verification\n",
    "mysql -u root -p -e \"\n",
    "  SELECT table_name, table_rows \n",
    "  FROM information_schema.tables \n",
    "  WHERE table_schema = 'production_db' \n",
    "  ORDER BY table_rows DESC;\n",
    "\"\n",
    "\n",
    "# MongoDB - mongosh for document database testing\n",
    "mongosh \"mongodb://localhost:27017/testdb\" --eval \"\n",
    "  db.orders.find({ status: 'pending', createdAt: { $lt: new Date(Date.now() - 7*24*60*60*1000) }}).count()\n",
    "\"\n",
    "```\n",
    "\n",
    "**Industry Best Practice**: Maintain a library of SQL scripts in version control (Git) for repeatable test data setup and verification queries. Use naming conventions:\n",
    "- `setup_*.sql` - Test data insertion\n",
    "- `verify_*.sql` - Validation queries\n",
    "- `cleanup_*.sql` - Teardown scripts\n",
    "\n",
    "### **32.1.3 Database Connection Management**\n",
    "\n",
    "Secure connection handling is critical for testing across environments:\n",
    "\n",
    "```python\n",
    "# Python: Using environment variables for secure DB testing\n",
    "import os\n",
    "from sqlalchemy import create_engine\n",
    "from sqlalchemy.exc import SQLAlchemyError\n",
    "\n",
    "class DatabaseTestConfig:\n",
    "    \"\"\"\n",
    "    Industry-standard configuration management for test databases\n",
    "    Never hardcode credentials in test scripts\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.host = os.getenv('TEST_DB_HOST', 'localhost')\n",
    "        self.port = os.getenv('TEST_DB_PORT', '5432')\n",
    "        self.database = os.getenv('TEST_DB_NAME', 'test_db')\n",
    "        self.user = os.getenv('TEST_DB_USER', 'test_user')\n",
    "        self.password = os.getenv('TEST_DB_PASSWORD', 'test_pass')\n",
    "        self.ssl_mode = os.getenv('TEST_DB_SSL', 'prefer')\n",
    "        \n",
    "    def get_connection_string(self):\n",
    "        \"\"\"Generate PostgreSQL connection string with SSL\"\"\"\n",
    "        return (f\"postgresql://{self.user}:{self.password}@\"\n",
    "                f\"{self.host}:{self.port}/{self.database}?\"\n",
    "                f\"sslmode={self.ssl_mode}\")\n",
    "    \n",
    "    def get_engine(self, pool_size=5):\n",
    "        \"\"\"\n",
    "        Create SQLAlchemy engine with connection pooling\n",
    "        Essential for performance testing scenarios\n",
    "        \"\"\"\n",
    "        return create_engine(\n",
    "            self.get_connection_string(),\n",
    "            pool_size=pool_size,\n",
    "            max_overflow=10,\n",
    "            pool_timeout=30,\n",
    "            pool_recycle=3600,\n",
    "            echo=False  # Set to True for SQL query logging during debugging\n",
    "        )\n",
    "\n",
    "# Usage in test setup\n",
    "config = DatabaseTestConfig()\n",
    "engine = config.get_engine()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **32.2 Automation with Database Connections**\n",
    "\n",
    "Automated database testing requires programmatic interfaces that allow test scripts to execute SQL commands, verify results, and manage transactions.\n",
    "\n",
    "### **32.2.1 Database Connectivity Standards**\n",
    "\n",
    "**JDBC (Java Database Connectivity)** - Industry standard for Java-based testing:\n",
    "\n",
    "```java\n",
    "// Java: Robust database testing with JDBC and connection pooling\n",
    "import java.sql.*;\n",
    "import com.zaxxer.hikari.HikariConfig;\n",
    "import com.zaxxer.hikari.HikariDataSource;\n",
    "\n",
    "public class DatabaseTestAutomation {\n",
    "    \n",
    "    private HikariDataSource dataSource;\n",
    "    \n",
    "    public void setupConnectionPool() {\n",
    "        // HikariCP - High-performance connection pooling for tests\n",
    "        HikariConfig config = new HikariConfig();\n",
    "        config.setJdbcUrl(System.getenv(\"TEST_DB_URL\"));\n",
    "        config.setUsername(System.getenv(\"TEST_DB_USER\"));\n",
    "        config.setPassword(System.getenv(\"TEST_DB_PASS\"));\n",
    "        config.setMaximumPoolSize(10);\n",
    "        config.setMinimumIdle(2);\n",
    "        config.setConnectionTimeout(30000);\n",
    "        config.setLeakDetectionThreshold(60000); // Detect connection leaks\n",
    "        \n",
    "        // Test query validation\n",
    "        config.setConnectionTestQuery(\"SELECT 1\");\n",
    "        \n",
    "        this.dataSource = new HikariDataSource(config);\n",
    "    }\n",
    "    \n",
    "    public boolean verifyTransactionIntegrity(String accountId, double expectedBalance) \n",
    "            throws SQLException {\n",
    "        String sql = \"SELECT balance, version FROM accounts WHERE account_id = ? \" +\n",
    "                     \"FOR UPDATE\"; // Lock row for concurrent testing\n",
    "        \n",
    "        try (Connection conn = dataSource.getConnection()) {\n",
    "            conn.setAutoCommit(false); // Manual transaction control for testing\n",
    "            \n",
    "            try (PreparedStatement stmt = conn.prepareStatement(sql)) {\n",
    "                stmt.setString(1, accountId);\n",
    "                ResultSet rs = stmt.executeQuery();\n",
    "                \n",
    "                if (rs.next()) {\n",
    "                    double actualBalance = rs.getDouble(\"balance\");\n",
    "                    int version = rs.getInt(\"version\"); // Optimistic locking check\n",
    "                    \n",
    "                    // Verify balance with tolerance for floating point\n",
    "                    boolean match = Math.abs(actualBalance - expectedBalance) < 0.001;\n",
    "                    \n",
    "                    if (!match) {\n",
    "                        conn.rollback();\n",
    "                        return false;\n",
    "                    }\n",
    "                    \n",
    "                    conn.commit();\n",
    "                    return true;\n",
    "                }\n",
    "            } catch (SQLException e) {\n",
    "                conn.rollback();\n",
    "                throw e;\n",
    "            }\n",
    "        }\n",
    "        return false;\n",
    "    }\n",
    "    \n",
    "    // Cleanup method\n",
    "    public void teardown() {\n",
    "        if (dataSource != null && !dataSource.isClosed()) {\n",
    "            dataSource.close();\n",
    "        }\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "**Python DB-API 2.0** - Standard for Python database access:\n",
    "\n",
    "```python\n",
    "# Python: Comprehensive database testing framework\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from contextlib import contextmanager\n",
    "import logging\n",
    "\n",
    "class DatabaseTestFramework:\n",
    "    \"\"\"\n",
    "    Production-ready database testing framework\n",
    "    Implements best practices for connection handling and transaction management\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_params):\n",
    "        self.params = connection_params\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "        \n",
    "    @contextmanager\n",
    "    def get_connection(self, isolation_level=None):\n",
    "        \"\"\"\n",
    "        Context manager for safe connection handling\n",
    "        Ensures connections are always closed, even if exceptions occur\n",
    "        \"\"\"\n",
    "        conn = None\n",
    "        try:\n",
    "            conn = psycopg2.connect(**self.params)\n",
    "            \n",
    "            if isolation_level:\n",
    "                conn.set_session(isolation_level=isolation_level)\n",
    "                \n",
    "            yield conn\n",
    "            \n",
    "            # If we reach here without exception, commit\n",
    "            if not conn.closed:\n",
    "                conn.commit()\n",
    "                \n",
    "        except psycopg2.Error as e:\n",
    "            self.logger.error(f\"Database error: {e}\")\n",
    "            if conn and not conn.closed:\n",
    "                conn.rollback()\n",
    "            raise\n",
    "        finally:\n",
    "            if conn and not conn.closed:\n",
    "                conn.close()\n",
    "    \n",
    "    def execute_test_query(self, query, params=None, fetch=True):\n",
    "        \"\"\"\n",
    "        Execute test query with automatic resource management\n",
    "        Returns results as list of dictionaries for easy assertion\n",
    "        \"\"\"\n",
    "        with self.get_connection() as conn:\n",
    "            with conn.cursor(cursor_factory=RealDictCursor) as cursor:\n",
    "                self.logger.debug(f\"Executing: {cursor.mogrify(query, params)}\")\n",
    "                cursor.execute(query, params)\n",
    "                \n",
    "                if fetch:\n",
    "                    return cursor.fetchall()\n",
    "                else:\n",
    "                    return cursor.rowcount\n",
    "    \n",
    "    def verify_data_integrity(self, table_name, constraints):\n",
    "        \"\"\"\n",
    "        Automated data integrity verification\n",
    "        constraints: dict of column_name -> validation_function\n",
    "        \"\"\"\n",
    "        violations = []\n",
    "        \n",
    "        with self.get_connection() as conn:\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(f\"SELECT * FROM {table_name}\")\n",
    "                rows = cursor.fetchall()\n",
    "                columns = [desc[0] for desc in cursor.description]\n",
    "                \n",
    "                for row in rows:\n",
    "                    row_dict = dict(zip(columns, row))\n",
    "                    for column, validator in constraints.items():\n",
    "                        if column in row_dict:\n",
    "                            if not validator(row_dict[column]):\n",
    "                                violations.append({\n",
    "                                    'row_id': row_dict.get('id'),\n",
    "                                    'column': column,\n",
    "                                    'value': row_dict[column],\n",
    "                                    'rule': validator.__name__\n",
    "                                })\n",
    "        \n",
    "        return violations\n",
    "\n",
    "# Usage example\n",
    "def test_user_email_format():\n",
    "    \"\"\"Test that all emails follow proper format\"\"\"\n",
    "    db = DatabaseTestFramework({\n",
    "        'host': 'localhost',\n",
    "        'database': 'test_db',\n",
    "        'user': 'tester',\n",
    "        'password': 'secure_pass'\n",
    "    })\n",
    "    \n",
    "    # Define validation constraint\n",
    "    def is_valid_email(email):\n",
    "        import re\n",
    "        return bool(re.match(r'^[\\w\\.-]+@[\\w\\.-]+\\.\\w+$', email))\n",
    "    \n",
    "    violations = db.verify_data_integrity(\n",
    "        'users',\n",
    "        {'email': is_valid_email}\n",
    "    )\n",
    "    \n",
    "    assert len(violations) == 0, f\"Email validation failed: {violations}\"\n",
    "```\n",
    "\n",
    "### **32.2.2 ORM (Object-Relational Mapping) Testing**\n",
    "\n",
    "When testing applications using ORMs (Hibernate, SQLAlchemy, Entity Framework), test at both ORM and SQL levels:\n",
    "\n",
    "```python\n",
    "# SQLAlchemy testing example\n",
    "from sqlalchemy import create_engine, Column, Integer, String, ForeignKey\n",
    "from sqlalchemy.orm import sessionmaker, relationship\n",
    "from sqlalchemy.ext.declarative import declarative_base\n",
    "import pytest\n",
    "\n",
    "Base = declarative_base()\n",
    "\n",
    "class User(Base):\n",
    "    __tablename__ = 'users'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    username = Column(String(50), unique=True, nullable=False)\n",
    "    email = Column(String(100))\n",
    "    orders = relationship(\"Order\", back_populates=\"user\")\n",
    "\n",
    "class Order(Base):\n",
    "    __tablename__ = 'orders'\n",
    "    id = Column(Integer, primary_key=True)\n",
    "    user_id = Column(Integer, ForeignKey('users.id'))\n",
    "    total_amount = Column(Integer)  # Stored in cents to avoid float issues\n",
    "    user = relationship(\"User\", back_populates=\"orders\")\n",
    "\n",
    "@pytest.fixture\n",
    "def db_session():\n",
    "    \"\"\"Pytest fixture for database testing with rollback\"\"\"\n",
    "    engine = create_engine('sqlite:///:memory:')  # In-memory for isolation\n",
    "    Base.metadata.create_all(engine)\n",
    "    \n",
    "    Session = sessionmaker(bind=engine)\n",
    "    session = Session()\n",
    "    \n",
    "    yield session\n",
    "    \n",
    "    session.rollback()  # Rollback all changes after test\n",
    "    session.close()\n",
    "\n",
    "def test_cascade_delete(db_session):\n",
    "    \"\"\"Verify that deleting user cascades to orders\"\"\"\n",
    "    user = User(username='testuser', email='test@test.com')\n",
    "    order = Order(total_amount=10000)\n",
    "    user.orders.append(order)\n",
    "    \n",
    "    db_session.add(user)\n",
    "    db_session.commit()\n",
    "    \n",
    "    # Verify initial state\n",
    "    assert db_session.query(User).count() == 1\n",
    "    assert db_session.query(Order).count() == 1\n",
    "    \n",
    "    # Delete user\n",
    "    db_session.delete(user)\n",
    "    db_session.commit()\n",
    "    \n",
    "    # Verify cascade\n",
    "    assert db_session.query(User).count() == 0\n",
    "    assert db_session.query(Order).count() == 0\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **32.3 Test Data Management (TDM)**\n",
    "\n",
    "Test Data Management is a critical discipline ensuring that test environments have appropriate, secure, and representative data while complying with data privacy regulations (GDPR, CCPA, HIPAA).\n",
    "\n",
    "### **32.3.1 Data Generation Strategies**\n",
    "\n",
    "**Synthetic Data Generation:**\n",
    "\n",
    "```python\n",
    "# Using Faker library for realistic test data generation\n",
    "from faker import Faker\n",
    "from faker.providers import internet, credit_card, date_time\n",
    "import random\n",
    "\n",
    "class TestDataGenerator:\n",
    "    \"\"\"\n",
    "    Generates realistic but synthetic test data\n",
    "    Avoids using production data with PII (Personally Identifiable Information)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, locale='en_US', seed=None):\n",
    "        self.fake = Faker(locale)\n",
    "        self.fake.add_provider(internet)\n",
    "        self.fake.add_provider(credit_card)\n",
    "        self.fake.add_provider(date_time)\n",
    "        \n",
    "        if seed:\n",
    "            Faker.seed(seed)  # Reproducible test data\n",
    "        \n",
    "    def generate_user_profile(self, user_type='standard'):\n",
    "        \"\"\"Generate complete user profile for testing\"\"\"\n",
    "        profile = {\n",
    "            'username': self.fake.user_name(),\n",
    "            'email': self.fake.email(),\n",
    "            'first_name': self.fake.first_name(),\n",
    "            'last_name': self.fake.last_name(),\n",
    "            'phone': self.fake.phone_number(),\n",
    "            'address': {\n",
    "                'street': self.fake.street_address(),\n",
    "                'city': self.fake.city(),\n",
    "                'state': self.fake.state(),\n",
    "                'zip': self.fake.zipcode(),\n",
    "                'country': self.fake.country()\n",
    "            },\n",
    "            'date_of_birth': self.fake.date_of_birth(minimum_age=18, maximum_age=90),\n",
    "            'registration_date': self.fake.date_time_this_decade(),\n",
    "            'is_active': random.choice([True, False]),\n",
    "            'user_type': user_type,\n",
    "            'credit_limit': random.randint(1000, 50000) if user_type == 'premium' else 0\n",
    "        }\n",
    "        return profile\n",
    "    \n",
    "    def generate_transaction(self, user_id=None):\n",
    "        \"\"\"Generate financial transaction for testing calculations\"\"\"\n",
    "        return {\n",
    "            'transaction_id': self.fake.uuid4(),\n",
    "            'user_id': user_id or random.randint(1, 10000),\n",
    "            'amount': round(random.uniform(1.00, 10000.00), 2),\n",
    "            'currency': random.choice(['USD', 'EUR', 'GBP', 'JPY']),\n",
    "            'timestamp': self.fake.date_time_this_year(),\n",
    "            'status': random.choice(['completed', 'pending', 'failed', 'refunded']),\n",
    "            'payment_method': random.choice(['credit_card', 'debit_card', 'paypal', 'crypto']),\n",
    "            'merchant_id': self.fake.company()\n",
    "        }\n",
    "    \n",
    "    def bulk_insert_data(self, db_connection, table_name, count=1000):\n",
    "        \"\"\"\n",
    "        Efficient bulk data insertion for performance testing\n",
    "        Uses execute_batch for PostgreSQL or executemany for others\n",
    "        \"\"\"\n",
    "        import psycopg2.extras\n",
    "        \n",
    "        data = [self.generate_user_profile() for _ in range(count)]\n",
    "        \n",
    "        query = \"\"\"\n",
    "            INSERT INTO users (username, email, first_name, last_name, created_at)\n",
    "            VALUES (%(username)s, %(email)s, %(first_name)s, %(last_name)s, %(registration_date)s)\n",
    "        \"\"\"\n",
    "        \n",
    "        with db_connection.cursor() as cursor:\n",
    "            psycopg2.extras.execute_batch(cursor, query, data)\n",
    "            db_connection.commit()\n",
    "```\n",
    "\n",
    "### **32.3.2 Data Masking and Subsetting**\n",
    "\n",
    "When using production-like data, sensitive information must be masked:\n",
    "\n",
    "```python\n",
    "# Data masking for compliance testing\n",
    "import hashlib\n",
    "from cryptography.fernet import Fernet\n",
    "\n",
    "class DataMasking:\n",
    "    \"\"\"\n",
    "    GDPR/HIPAA compliant data masking for test environments\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, encryption_key=None):\n",
    "        self.key = encryption_key or Fernet.generate_key()\n",
    "        self.cipher = Fernet(self.key)\n",
    "    \n",
    "    def mask_email(self, email):\n",
    "        \"\"\"Preserve domain, mask local part\"\"\"\n",
    "        if '@' not in email:\n",
    "            return self.hash_value(email)\n",
    "        local, domain = email.split('@')\n",
    "        masked_local = local[:2] + '***' + local[-1] if len(local) > 3 else '***'\n",
    "        return f\"{masked_local}@{domain}\"\n",
    "    \n",
    "    def mask_credit_card(self, card_number):\n",
    "        \"\"\"PCI DSS compliant masking - show only last 4 digits\"\"\"\n",
    "        clean = str(card_number).replace(' ', '').replace('-', '')\n",
    "        if len(clean) >= 4:\n",
    "            return '*' * (len(clean) - 4) + clean[-4:]\n",
    "        return '****'\n",
    "    \n",
    "    def tokenize(self, value):\n",
    "        \"\"\"Reversible tokenization for values that need to be recovered\"\"\"\n",
    "        return self.cipher.encrypt(value.encode()).decode()\n",
    "    \n",
    "    def detokenize(self, token):\n",
    "        \"\"\"Recover original value\"\"\"\n",
    "        return self.cipher.decrypt(token.encode()).decode()\n",
    "    \n",
    "    def hash_value(self, value, salt=None):\n",
    "        \"\"\"One-way hashing for passwords/SSNs\"\"\"\n",
    "        salted = f\"{value}{salt}\" if salt else value\n",
    "        return hashlib.sha256(salted.encode()).hexdigest()\n",
    "\n",
    "# Database subsetting - extract representative sample\n",
    "def create_test_subset(source_db, target_db, percentage=10):\n",
    "    \"\"\"\n",
    "    Create a subset of production data for testing\n",
    "    Maintains referential integrity while reducing volume\n",
    "    \"\"\"\n",
    "    subset_strategy = {\n",
    "        'users': f\"ORDER BY RANDOM() LIMIT (SELECT COUNT(*) * {percentage/100} FROM users)\",\n",
    "        'orders': \"WHERE user_id IN (SELECT id FROM users)\",  # Referential integrity\n",
    "        'order_items': \"WHERE order_id IN (SELECT id FROM orders)\",\n",
    "        'products': \"WHERE id IN (SELECT product_id FROM order_items)\"  # Only referenced products\n",
    "    }\n",
    "    \n",
    "    for table, condition in subset_strategy.items():\n",
    "        copy_query = f\"\"\"\n",
    "            CREATE TABLE {target_db}.{table} AS \n",
    "            SELECT * FROM {source_db}.{table} \n",
    "            {condition};\n",
    "        \"\"\"\n",
    "        # Execute copy...\n",
    "```\n",
    "\n",
    "### **32.3.3 Enterprise TDM Tools**\n",
    "\n",
    "For large organizations, commercial TDM solutions provide:\n",
    "\n",
    "| Tool | Key Features | Best For |\n",
    "|------|--------------|----------|\n",
    "| **Delphix** | Virtual data copies, masking, self-service | Large enterprises, complex data landscapes |\n",
    "| **Informatica TDM** | Data generation, subsetting, masking | Regulatory compliance, GDPR |\n",
    "| **CA TDM (Broadcom)** | Synthetic data, test matching | Agile teams, CI/CD integration |\n",
    "| **IBM InfoSphere Optim** | Archive, subset, mask | Legacy systems, mainframe |\n",
    "| **K2View** | Micro-databases, privacy | Real-time test data provisioning |\n",
    "\n",
    "**Open Source Alternative**: **PostgreSQL Anonymizer** or **MySQL Data Masking** plugins for basic masking needs.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.4 Database Mocking**\n",
    "\n",
    "Database mocking isolates tests from external dependencies, ensuring tests are fast, reliable, and don't require running database servers.\n",
    "\n",
    "### **32.4.1 In-Memory Databases**\n",
    "\n",
    "```java\n",
    "// Java: Using H2 in-memory database for unit testing\n",
    "import org.junit.jupiter.api.*;\n",
    "import java.sql.*;\n",
    "\n",
    "public class InMemoryDatabaseTest {\n",
    "    \n",
    "    private static Connection conn;\n",
    "    \n",
    "    @BeforeAll\n",
    "    public static void setup() throws SQLException {\n",
    "        // H2 in-memory database - fast, isolated, no external dependencies\n",
    "        String url = \"jdbc:h2:mem:testdb;DB_CLOSE_DELAY=-1;MODE=PostgreSQL\";\n",
    "        conn = DriverManager.getConnection(url, \"sa\", \"\");\n",
    "        \n",
    "        // Create schema\n",
    "        Statement stmt = conn.createStatement();\n",
    "        stmt.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS inventory (\n",
    "                product_id VARCHAR(50) PRIMARY KEY,\n",
    "                quantity INT NOT NULL CHECK (quantity >= 0),\n",
    "                last_updated TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\");\n",
    "    }\n",
    "    \n",
    "    @Test\n",
    "    public void testInventoryUpdate() throws SQLException {\n",
    "        // Insert test data\n",
    "        PreparedStatement insert = conn.prepareStatement(\n",
    "            \"INSERT INTO inventory VALUES (?, ?, ?)\"\n",
    "        );\n",
    "        insert.setString(1, \"PROD-001\");\n",
    "        insert.setInt(2, 100);\n",
    "        insert.setTimestamp(3, new Timestamp(System.currentTimeMillis()));\n",
    "        insert.executeUpdate();\n",
    "        \n",
    "        // Test business logic: decrement inventory\n",
    "        PreparedStatement update = conn.prepareStatement(\n",
    "            \"UPDATE inventory SET quantity = quantity - ? WHERE product_id = ?\"\n",
    "        );\n",
    "        update.setInt(1, 5);\n",
    "        update.setString(2, \"PROD-001\");\n",
    "        int affected = update.executeUpdate();\n",
    "        \n",
    "        Assertions.assertEquals(1, affected);\n",
    "        \n",
    "        // Verify state\n",
    "        Statement stmt = conn.createStatement();\n",
    "        ResultSet rs = stmt.executeQuery(\n",
    "            \"SELECT quantity FROM inventory WHERE product_id = 'PROD-001'\"\n",
    "        );\n",
    "        Assertions.assertTrue(rs.next());\n",
    "        Assertions.assertEquals(95, rs.getInt(\"quantity\"));\n",
    "    }\n",
    "    \n",
    "    @AfterAll\n",
    "    public static void teardown() throws SQLException {\n",
    "        if (conn != null) conn.close();\n",
    "        // H2 in-memory DB destroyed automatically\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### **32.4.2 Testcontainers for Integration Testing**\n",
    "\n",
    "**Testcontainers** is the industry standard for database integration testing, providing real databases in Docker containers:\n",
    "\n",
    "```python\n",
    "# Python: Testcontainers for real database testing\n",
    "import pytest\n",
    "from testcontainers.postgres import PostgresContainer\n",
    "from testcontainers.mysql import MySqlContainer\n",
    "import psycopg2\n",
    "\n",
    "@pytest.fixture(scope=\"module\")\n",
    "def postgres_db():\n",
    "    \"\"\"\n",
    "    Spins up real PostgreSQL in Docker container\n",
    "    Provides full PostgreSQL features unlike SQLite\n",
    "    \"\"\"\n",
    "    with PostgresContainer(\"postgres:15-alpine\") as postgres:\n",
    "        # Wait for database to be ready\n",
    "        conn = psycopg2.connect(\n",
    "            host=postgres.get_container_host_ip(),\n",
    "            port=postgres.get_exposed_port(5432),\n",
    "            user=postgres.username,\n",
    "            password=postgres.password,\n",
    "            dbname=postgres.dbname\n",
    "        )\n",
    "        \n",
    "        # Setup schema\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            CREATE TABLE events (\n",
    "                id SERIAL PRIMARY KEY,\n",
    "                event_type VARCHAR(50) NOT NULL,\n",
    "                payload JSONB,\n",
    "                created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "            )\n",
    "        \"\"\")\n",
    "        conn.commit()\n",
    "        \n",
    "        yield conn\n",
    "        \n",
    "        # Cleanup automatic via context manager\n",
    "\n",
    "def test_json_operations(postgres_db):\n",
    "    \"\"\"Test PostgreSQL-specific JSON operations\"\"\"\n",
    "    cursor = postgres_db.cursor()\n",
    "    \n",
    "    # Insert JSON data\n",
    "    cursor.execute(\"\"\"\n",
    "        INSERT INTO events (event_type, payload) \n",
    "        VALUES ('user_action', '{\"user_id\": 123, \"action\": \"login\"}')\n",
    "    \"\"\")\n",
    "    \n",
    "    # Query JSON\n",
    "    cursor.execute(\"\"\"\n",
    "        SELECT payload->>'user_id' as user_id \n",
    "        FROM events \n",
    "        WHERE event_type = 'user_action'\n",
    "    \"\"\")\n",
    "    \n",
    "    result = cursor.fetchone()\n",
    "    assert result[0] == '123'\n",
    "```\n",
    "\n",
    "### **32.4.3 Mocking Database Connections**\n",
    "\n",
    "For unit testing without any database:\n",
    "\n",
    "```python\n",
    "# Python: Mocking database with unittest.mock\n",
    "from unittest.mock import Mock, MagicMock, patch\n",
    "import pytest\n",
    "\n",
    "def test_with_mocked_db():\n",
    "    \"\"\"\n",
    "    Unit test with completely mocked database\n",
    "    Fastest execution, no I/O\n",
    "    \"\"\"\n",
    "    # Mock connection and cursor\n",
    "    mock_conn = Mock()\n",
    "    mock_cursor = Mock()\n",
    "    \n",
    "    # Configure mock behavior\n",
    "    mock_cursor.fetchall.return_value = [\n",
    "        {'id': 1, 'username': 'alice', 'status': 'active'},\n",
    "        {'id': 2, 'username': 'bob', 'status': 'inactive'}\n",
    "    ]\n",
    "    mock_cursor.rowcount = 2\n",
    "    \n",
    "    mock_conn.cursor.return_value.__enter__ = Mock(return_value=mock_cursor)\n",
    "    mock_conn.cursor.return_value.__exit__ = Mock(return_value=False)\n",
    "    \n",
    "    # Inject mock into application code\n",
    "    from myapp import UserService\n",
    "    service = UserService(mock_conn)\n",
    "    \n",
    "    users = service.get_active_users()\n",
    "    \n",
    "    # Verify interactions\n",
    "    mock_cursor.execute.assert_called_once()\n",
    "    assert len(users) == 2\n",
    "    assert all(u['status'] == 'active' for u in users)\n",
    "\n",
    "# Java: Mockito for database testing\n",
    "/*\n",
    "@ExtendWith(MockitoExtension.class)\n",
    "class UserRepositoryTest {\n",
    "    \n",
    "    @Mock\n",
    "    private Connection mockConnection;\n",
    "    \n",
    "    @Mock\n",
    "    private PreparedStatement mockStatement;\n",
    "    \n",
    "    @Mock\n",
    "    private ResultSet mockResultSet;\n",
    "    \n",
    "    @Test\n",
    "    void testFindUserById() throws SQLException {\n",
    "        // Arrange\n",
    "        when(mockConnection.prepareStatement(anyString()))\n",
    "            .thenReturn(mockStatement);\n",
    "        when(mockStatement.executeQuery()).thenReturn(mockResultSet);\n",
    "        when(mockResultSet.next()).thenReturn(true, false);\n",
    "        when(mockResultSet.getString(\"username\")).thenReturn(\"testuser\");\n",
    "        \n",
    "        UserRepository repo = new UserRepository(mockConnection);\n",
    "        \n",
    "        // Act\n",
    "        User user = repo.findById(1L);\n",
    "        \n",
    "        // Assert\n",
    "        assertEquals(\"testuser\", user.getUsername());\n",
    "        verify(mockStatement).setLong(1, 1L);\n",
    "    }\n",
    "}\n",
    "*/\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **32.5 Code Examples for Database Testing**\n",
    "\n",
    "### **32.5.1 Complete Test Suite Example**\n",
    "\n",
    "```python\n",
    "# Comprehensive database testing suite\n",
    "import pytest\n",
    "import psycopg2\n",
    "from psycopg2.extras import RealDictCursor\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class DatabaseTestSuite:\n",
    "    \"\"\"\n",
    "    Production-ready database testing patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_config):\n",
    "        self.config = db_config\n",
    "        self.setup_scripts = []\n",
    "        self.teardown_scripts = []\n",
    "    \n",
    "    @contextmanager\n",
    "    def transaction_scope(self):\n",
    "        \"\"\"Context manager for test transactions with automatic rollback\"\"\"\n",
    "        conn = psycopg2.connect(**self.config)\n",
    "        conn.set_session(autocommit=False)\n",
    "        try:\n",
    "            yield conn\n",
    "        finally:\n",
    "            conn.rollback()  # Always rollback after test\n",
    "            conn.close()\n",
    "    \n",
    "    def setup_test_data(self, conn, fixture_name):\n",
    "        \"\"\"Load SQL fixtures\"\"\"\n",
    "        with open(f'fixtures/{fixture_name}.sql', 'r') as f:\n",
    "            sql = f.read()\n",
    "            with conn.cursor() as cursor:\n",
    "                cursor.execute(sql)\n",
    "                conn.commit()\n",
    "    \n",
    "    def assert_row_count(self, conn, table_name, expected_count, where_clause=None):\n",
    "        \"\"\"Assertion helper for row counts\"\"\"\n",
    "        query = f\"SELECT COUNT(*) FROM {table_name}\"\n",
    "        if where_clause:\n",
    "            query += f\" WHERE {where_clause}\"\n",
    "        \n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            actual = cursor.fetchone()[0]\n",
    "            assert actual == expected_count, \\\n",
    "                f\"Expected {expected_count} rows in {table_name}, found {actual}\"\n",
    "    \n",
    "    def assert_foreign_key_integrity(self, conn, parent_table, child_table, fk_column):\n",
    "        \"\"\"Verify referential integrity\"\"\"\n",
    "        query = f\"\"\"\n",
    "            SELECT COUNT(*) FROM {child_table} c\n",
    "            LEFT JOIN {parent_table} p ON c.{fk_column} = p.id\n",
    "            WHERE p.id IS NULL AND c.{fk_column} IS NOT NULL\n",
    "        \"\"\"\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(query)\n",
    "            orphan_count = cursor.fetchone()[0]\n",
    "            assert orphan_count == 0, \\\n",
    "                f\"Found {orphan_count} orphaned records in {child_table}\"\n",
    "\n",
    "# Pytest fixtures for reuse\n",
    "@pytest.fixture\n",
    "def db_suite():\n",
    "    config = {\n",
    "        'host': 'localhost',\n",
    "        'dbname': 'test_db',\n",
    "        'user': 'test_user',\n",
    "        'password': 'test_pass'\n",
    "    }\n",
    "    return DatabaseTestSuite(config)\n",
    "\n",
    "def test_order_workflow(db_suite):\n",
    "    \"\"\"End-to-end test of order creation workflow\"\"\"\n",
    "    with db_suite.transaction_scope() as conn:\n",
    "        # Setup\n",
    "        db_suite.setup_test_data(conn, 'initial_users')\n",
    "        \n",
    "        # Execute business operation\n",
    "        with conn.cursor() as cursor:\n",
    "            # Create order\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO orders (user_id, total_amount, status)\n",
    "                VALUES ((SELECT id FROM users LIMIT 1), 100.00, 'pending')\n",
    "                RETURNING id\n",
    "            \"\"\")\n",
    "            order_id = cursor.fetchone()[0]\n",
    "            \n",
    "            # Add items\n",
    "            cursor.execute(\"\"\"\n",
    "                INSERT INTO order_items (order_id, product_id, quantity, price)\n",
    "                VALUES (%s, 1, 2, 50.00)\n",
    "            \"\"\", (order_id,))\n",
    "            \n",
    "            # Update inventory\n",
    "            cursor.execute(\"\"\"\n",
    "                UPDATE inventory \n",
    "                SET quantity = quantity - 2 \n",
    "                WHERE product_id = 1\n",
    "            \"\"\")\n",
    "        \n",
    "        # Verify\n",
    "        db_suite.assert_row_count(conn, 'orders', 1, f\"id = {order_id}\")\n",
    "        db_suite.assert_row_count(conn, 'order_items', 1, f\"order_id = {order_id}\")\n",
    "        \n",
    "        # Verify inventory decrement\n",
    "        with conn.cursor() as cursor:\n",
    "            cursor.execute(\"SELECT quantity FROM inventory WHERE product_id = 1\")\n",
    "            qty = cursor.fetchone()[0]\n",
    "            assert qty == 98  # Assuming started with 100\n",
    "\n",
    "def test_concurrent_updates(db_suite):\n",
    "    \"\"\"Test handling of concurrent database updates\"\"\"\n",
    "    import threading\n",
    "    import queue\n",
    "    \n",
    "    results = queue.Queue()\n",
    "    \n",
    "    def update_balance(account_id, amount):\n",
    "        try:\n",
    "            with db_suite.transaction_scope() as conn:\n",
    "                with conn.cursor() as cursor:\n",
    "                    # Optimistic locking pattern\n",
    "                    cursor.execute(\"\"\"\n",
    "                        UPDATE accounts \n",
    "                        SET balance = balance + %s, \n",
    "                            version = version + 1\n",
    "                        WHERE id = %s \n",
    "                        AND version = (SELECT version FROM accounts WHERE id = %s)\n",
    "                        RETURNING version\n",
    "                    \"\"\", (amount, account_id, account_id))\n",
    "                    \n",
    "                    if cursor.rowcount == 0:\n",
    "                        results.put(('conflict', account_id))\n",
    "                    else:\n",
    "                        results.put(('success', account_id))\n",
    "        except Exception as e:\n",
    "            results.put(('error', str(e)))\n",
    "    \n",
    "    # Simulate concurrent updates\n",
    "    threads = []\n",
    "    for i in range(5):\n",
    "        t = threading.Thread(target=update_balance, args=(1, 100))\n",
    "        threads.append(t)\n",
    "        t.start()\n",
    "    \n",
    "    for t in threads:\n",
    "        t.join()\n",
    "    \n",
    "    # Analyze results - should have some conflicts\n",
    "    outcomes = list(results.queue)\n",
    "    successes = [o for o in outcomes if o[0] == 'success']\n",
    "    conflicts = [o for o in outcomes if o[0] == 'conflict']\n",
    "    \n",
    "    assert len(successes) > 0, \"At least one update should succeed\"\n",
    "    print(f\"Successful updates: {len(successes)}, Conflicts: {len(conflicts)}\")\n",
    "```\n",
    "\n",
    "### **32.5.2 Stored Procedure Testing**\n",
    "\n",
    "```sql\n",
    "-- Testing stored procedures (PostgreSQL example)\n",
    "-- File: test_stored_procedures.sql\n",
    "\n",
    "-- Test Framework Setup\n",
    "CREATE OR REPLACE FUNCTION run_test(test_name TEXT, expected_result BOOLEAN, actual_result BOOLEAN)\n",
    "RETURNS VOID AS $$\n",
    "BEGIN\n",
    "    IF expected_result = actual_result THEN\n",
    "        RAISE NOTICE 'PASS: %', test_name;\n",
    "    ELSE\n",
    "        RAISE EXCEPTION 'FAIL: % - Expected %, Got %', test_name, expected_result, actual_result;\n",
    "    END END IF;\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "-- Test: Calculate Order Total\n",
    "CREATE OR REPLACE FUNCTION test_calculate_order_total()\n",
    "RETURNS VOID AS $$\n",
    "DECLARE\n",
    "    v_order_id INT;\n",
    "    v_result DECIMAL;\n",
    "BEGIN\n",
    "    -- Setup\n",
    "    INSERT INTO orders (customer_id, order_date) VALUES (1, NOW()) RETURNING id INTO v_order_id;\n",
    "    INSERT INTO order_items (order_id, product_id, quantity, unit_price) \n",
    "    VALUES (v_order_id, 1, 2, 25.00), (v_order_id, 2, 1, 50.00);\n",
    "    \n",
    "    -- Execute\n",
    "    SELECT calculate_order_total(v_order_id) INTO v_result;\n",
    "    \n",
    "    -- Assert\n",
    "    PERFORM run_test('Order Total Calculation', v_result = 100.00, ABS(v_result - 100.00) < 0.01);\n",
    "    \n",
    "    -- Teardown\n",
    "    ROLLBACK;  -- Automatic cleanup\n",
    "END;\n",
    "$$ LANGUAGE plpgsql;\n",
    "\n",
    "-- Execute tests\n",
    "SELECT test_calculate_order_total();\n",
    "```\n",
    "\n",
    "### **32.5.3 Performance Baseline Testing**\n",
    "\n",
    "```python\n",
    "# Database performance testing\n",
    "import time\n",
    "import statistics\n",
    "from contextlib import contextmanager\n",
    "\n",
    "class DatabasePerformanceTest:\n",
    "    \"\"\"\n",
    "    Performance testing utilities for databases\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_connection):\n",
    "        self.conn = db_connection\n",
    "        self.measurements = []\n",
    "    \n",
    "    @contextmanager\n",
    "    def measure_query_time(self, query_name):\n",
    "        \"\"\"Context manager to measure query execution time\"\"\"\n",
    "        start = time.perf_counter()\n",
    "        yield\n",
    "        end = time.perf_counter()\n",
    "        duration = (end - start) * 1000  # Convert to milliseconds\n",
    "        self.measurements.append({\n",
    "            'query': query_name,\n",
    "            'duration_ms': duration,\n",
    "            'timestamp': time.time()\n",
    "        })\n",
    "    \n",
    "    def run_load_test(self, query_func, iterations=100, concurrency=1):\n",
    "        \"\"\"\n",
    "        Execute query multiple times to establish baseline\n",
    "        \"\"\"\n",
    "        from concurrent.futures import ThreadPoolExecutor\n",
    "        \n",
    "        def single_execution(i):\n",
    "            with self.measure_query_time(f\"iteration_{i}\"):\n",
    "                return query_func()\n",
    "        \n",
    "        with ThreadPoolExecutor(max_workers=concurrency) as executor:\n",
    "            results = list(executor.map(single_execution, range(iterations)))\n",
    "        \n",
    "        return self._analyze_results()\n",
    "    \n",
    "    def _analyze_results(self):\n",
    "        \"\"\"Calculate performance metrics\"\"\"\n",
    "        times = [m['duration_ms'] for m in self.measurements]\n",
    "        \n",
    "        return {\n",
    "            'count': len(times),\n",
    "            'mean_ms': statistics.mean(times),\n",
    "            'median_ms': statistics.median(times),\n",
    "            'stdev_ms': statistics.stdev(times) if len(times) > 1 else 0,\n",
    "            'min_ms': min(times),\n",
    "            'max_ms': max(times),\n",
    "            'p95_ms': sorted(times)[int(len(times)*0.95)] if times else 0,\n",
    "            'p99_ms': sorted(times)[int(len(times)*0.99)] if times else 0\n",
    "        }\n",
    "    \n",
    "    def assert_performance(self, metric_name, max_acceptable_ms):\n",
    "        \"\"\"Assert that performance meets SLA\"\"\"\n",
    "        analysis = self._analyze_results()\n",
    "        actual = analysis.get(metric_name, 0)\n",
    "        \n",
    "        assert actual <= max_acceptable_ms, \\\n",
    "            f\"Performance regression: {metric_name}={actual}ms exceeds threshold {max_acceptable_ms}ms\"\n",
    "\n",
    "# Usage\n",
    "def test_user_search_performance(db_connection):\n",
    "    perf_test = DatabasePerformanceTest(db_connection)\n",
    "    \n",
    "    def search_query():\n",
    "        cursor = db_connection.cursor()\n",
    "        cursor.execute(\"\"\"\n",
    "            SELECT u.*, COUNT(o.id) as order_count \n",
    "            FROM users u \n",
    "            LEFT JOIN orders o ON u.id = o.user_id \n",
    "            WHERE u.email LIKE '%@example.com%' \n",
    "            GROUP BY u.id \n",
    "            LIMIT 100\n",
    "        \"\"\")\n",
    "        return cursor.fetchall()\n",
    "    \n",
    "    # Run 100 iterations, 5 concurrent threads\n",
    "    results = perf_test.run_load_test(search_query, iterations=100, concurrency=5)\n",
    "    \n",
    "    # Assert against SLA (e.g., 95th percentile must be under 100ms)\n",
    "    perf_test.assert_performance('p95_ms', 100)\n",
    "    \n",
    "    print(f\"Query performance: Mean={results['mean_ms']:.2f}ms, P95={results['p95_ms']:.2f}ms\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "**Database Clients (32.1):**\n",
    "- **DBeaver** and **DataGrip** are industry-standard GUI tools for manual database verification and exploratory testing\n",
    "- CLI tools enable automation and CI/CD integration\n",
    "- Always use environment variables for credentials; never hardcode in test scripts\n",
    "\n",
    "**Automation Connectivity (32.2):**\n",
    "- **Connection Pooling** (HikariCP for Java, SQLAlchemy for Python) is essential for performance testing\n",
    "- Use **context managers** to ensure connections are properly closed\n",
    "- Implement **transaction rollback** after tests to maintain clean state\n",
    "\n",
    "**Test Data Management (32.3):**\n",
    "- **Synthetic data generation** (Faker) is preferred over production data for privacy compliance\n",
    "- **Data masking** is mandatory when using production-derived data (GDPR/HIPAA)\n",
    "- **Subsetting** reduces test environment size while maintaining referential integrity\n",
    "- Enterprise tools (Delphix, Informatica) provide advanced TDM capabilities for large organizations\n",
    "\n",
    "**Database Mocking (32.4):**\n",
    "- **H2/SQLite** in-memory databases provide fast unit testing for simple scenarios\n",
    "- **Testcontainers** offer the best balance—isolated real databases without infrastructure overhead\n",
    "- **Mocking** (Mockito/Mock) is fastest but only suitable for unit testing logic, not SQL validation\n",
    "\n",
    "**Best Practices:**\n",
    "1. **Isolation**: Each test should use independent transactions or database instances\n",
    "2. **Reproducibility**: Use seed values for random data generation to enable debugging\n",
    "3. **Cleanup**: Implement teardown methods; use `TRUNCATE` over `DELETE` for speed\n",
    "4. **Version Control**: Store schema migrations and test data SQL in Git\n",
    "5. **Performance**: Establish baselines and fail tests on regression (SLA violations)\n",
    "\n",
    "### **Tools Reference:**\n",
    "- **GUI**: DBeaver (universal), pgAdmin (PostgreSQL), MySQL Workbench, SSMS (SQL Server)\n",
    "- **Python**: `psycopg2`, `pymysql`, `sqlalchemy`, `faker`, `testcontainers`\n",
    "- **Java**: JDBC, H2, Testcontainers, HikariCP\n",
    "- **TDM**: Delphix, Informatica TDM, PostgreSQL Anonymizer\n",
    "\n",
    "---\n",
    "\n",
    "## **📖 Next Chapter: Chapter 33 - Performance Testing Fundamentals**\n",
    "\n",
    "Now that you understand how to test database functionality and manage test data, **Chapter 33** will introduce you to **Performance Testing**—ensuring your applications meet speed, scalability, and stability requirements under load.\n",
    "\n",
    "In **Chapter 33**, you will master:\n",
    "\n",
    "- **Performance Testing Types**: Load Testing, Stress Testing, Spike Testing, Endurance Testing, and Scalability Testing—understanding when to apply each\n",
    "- **Performance Metrics**: Response time, throughput, latency, error rates, and resource utilization (CPU, memory, disk I/O)\n",
    "- **Performance Testing Strategy**: Defining SLAs (Service Level Agreements), establishing baselines, and identifying performance bottlenecks\n",
    "- **The Performance Testing Process**: From planning and script creation to execution and analysis\n",
    "- **Key Concepts**: Think time, pacing, ramp-up/ramp-down strategies, and correlation\n",
    "- **Industry Standards**: Web performance metrics (Apdex scores), percentiles (P95, P99), and golden signals (latency, traffic, errors, saturation)\n",
    "\n",
    "This chapter will transition you from functional verification to **non-functional testing**, teaching you how to validate that your database queries, APIs, and applications perform efficiently under real-world load conditions.\n",
    "\n",
    "**Continue to Chapter 33 to learn how to ensure your applications scale and perform under pressure!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
