{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 58: Big Data Testing\n",
    "\n",
    "---\n",
    "\n",
    "## 58.1 Introduction to Big Data Testing\n",
    "\n",
    "Big Data refers to datasets that are so large, complex, or fast-moving that traditional data processing tools cannot handle them effectively. Testing big data systems involves verifying that data is correctly ingested, processed, stored, and analyzed across distributed frameworks. It ensures data quality, accuracy, performance, and security in environments where data volume, velocity, and variety are extreme.\n",
    "\n",
    "### 58.1.1 Why Big Data Testing Matters\n",
    "\n",
    "| Reason | Description |\n",
    "|--------|-------------|\n",
    "| **Business Decisions** | Many organizations base critical decisions on data insights; incorrect data leads to wrong decisions. |\n",
    "| **Data Quality** | Inaccurate or incomplete data can propagate through pipelines, corrupting downstream systems. |\n",
    "| **Regulatory Compliance** | Industries like finance and healthcare require data integrity and auditability. |\n",
    "| **Performance** | Big Data systems must handle massive loads within acceptable time windows. |\n",
    "| **Cost** | Inefficient processing wastes computational resources and cloud spending. |\n",
    "\n",
    "---\n",
    "\n",
    "## 58.2 Big Data Fundamentals: The 5 V's\n",
    "\n",
    "Big Data is often characterized by the 5 V's:\n",
    "\n",
    "| V | Description | Testing Implication |\n",
    "|---|-------------|----------------------|\n",
    "| **Volume** | Huge amounts of data (terabytes to petabytes). | Need scalable testing strategies; sampling may be required. |\n",
    "| **Velocity** | High speed of data ingestion and processing (real-time streams). | Test for latency, throughput, and real-time processing accuracy. |\n",
    "| **Variety** | Diverse data types (structured, semi-structured, unstructured). | Validate schema evolution, data type conversions. |\n",
    "| **Veracity** | Data quality and trustworthiness. | Ensure data cleansing, deduplication, anomaly detection. |\n",
    "| **Value** | The business value derived from data. | Validate that analytics produce correct insights. |\n",
    "\n",
    "---\n",
    "\n",
    "## 58.3 Big Data Architecture Components\n",
    "\n",
    "A typical big data pipeline includes:\n",
    "\n",
    "```\n",
    "Data Sources \u2192 Ingestion \u2192 Storage \u2192 Processing \u2192 Analytics/Output\n",
    "```\n",
    "\n",
    "- **Ingestion:** Kafka, Flume, NiFi\n",
    "- **Storage:** HDFS, HBase, Cassandra, S3\n",
    "- **Processing:** Spark, Hadoop MapReduce, Flink\n",
    "- **Analytics:** Hive, Presto, custom ML models\n",
    "\n",
    "Testing must cover each component and their integrations.\n",
    "\n",
    "---\n",
    "\n",
    "## 58.4 Testing Challenges in Big Data\n",
    "\n",
    "| Challenge | Description |\n",
    "|-----------|-------------|\n",
    "| **Data Volume** | Cannot test on full dataset every time; need representative test data. |\n",
    "| **Distributed Nature** | Failures can be partial; testing must account for network partitions, node failures. |\n",
    "| **Non-Determinism** | Parallel processing can produce non-deterministic results if not carefully designed. |\n",
    "| **Data Variety** | Multiple formats (JSON, Avro, Parquet) require different validation approaches. |\n",
    "| **Schema Evolution** | Data schemas change over time; backward/forward compatibility must be tested. |\n",
    "| **Performance** | Must test under realistic loads, not just functional correctness. |\n",
    "| **Data Privacy** | Test data may contain sensitive information; need anonymization. |\n",
    "\n",
    "---\n",
    "\n",
    "## 58.5 Types of Big Data Testing\n",
    "\n",
    "### 58.5.1 Data Ingestion Testing\n",
    "\n",
    "Verify that data is correctly pulled from sources and written into the big data platform.\n",
    "\n",
    "**Test scenarios:**\n",
    "- All expected records are ingested.\n",
    "- Duplicate records are handled (deduplication).\n",
    "- Data format conversions (e.g., CSV to Parquet) are correct.\n",
    "- Ingestion handles failures (source down, network issues) and resumes.\n",
    "- Schema validation rejects malformed records.\n",
    "\n",
    "**Tools:** Custom scripts, Kafka consumer testing, Apache NiFi test runners.\n",
    "\n",
    "### 58.5.2 Data Processing Testing\n",
    "\n",
    "Validate the logic of data transformation jobs (MapReduce, Spark, Flink).\n",
    "\n",
    "**Test levels:**\n",
    "- **Unit testing:** Test individual functions or transformations (e.g., a Spark UDF).\n",
    "- **Integration testing:** Test job with small datasets, comparing output to expected.\n",
    "- **End-to-end testing:** Run job on a representative dataset and verify results.\n",
    "\n",
    "**Challenges:** Data determinism; need to control input data and compare output.\n",
    "\n",
    "**Example: Spark job testing**\n",
    "\n",
    "```python\n",
    "# test_spark_job.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.testing import assertDataFrameEqual\n",
    "from my_etl import transform_data\n",
    "\n",
    "def test_transform_data():\n",
    "    spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "    \n",
    "    input_data = [(\"1\", \"Alice\", 25), (\"2\", \"Bob\", 30)]\n",
    "    input_df = spark.createDataFrame(input_data, [\"id\", \"name\", \"age\"])\n",
    "    \n",
    "    expected_data = [(\"1\", \"Alice\", 26), (\"2\", \"Bob\", 31)]\n",
    "    expected_df = spark.createDataFrame(expected_data, [\"id\", \"name\", \"age_plus_one\"])\n",
    "    \n",
    "    result_df = transform_data(input_df)  # adds 1 to age\n",
    "    \n",
    "    assertDataFrameEqual(result_df, expected_df)\n",
    "    \n",
    "    spark.stop()\n",
    "```\n",
    "\n",
    "### 58.5.3 Data Storage Testing\n",
    "\n",
    "Ensure data is stored correctly and durably.\n",
    "\n",
    "**Test scenarios:**\n",
    "- Data written to HDFS/cloud storage matches expected partitions.\n",
    "- Compression works as intended.\n",
    "- Data retrieval (read) returns correct data.\n",
    "- ACID properties for transactional stores (if applicable).\n",
    "\n",
    "**Tools:** HDFS CLI commands, cloud storage SDKs, custom checks.\n",
    "\n",
    "### 58.5.4 Data Validation Testing\n",
    "\n",
    "Verify the quality of data at rest or after processing.\n",
    "\n",
    "**Key dimensions:**\n",
    "- **Completeness:** All expected records present? No missing fields?\n",
    "- **Accuracy:** Data values match source truth?\n",
    "- **Consistency:** Data across different stores/tables consistent?\n",
    "- **Timeliness:** Data available within SLA?\n",
    "\n",
    "**Tools:** Apache Griffin, Deequ (Amazon), Great Expectations, custom SQL.\n",
    "\n",
    "#### Example: Great Expectations\n",
    "\n",
    "Great Expectations is an open-source tool for data validation.\n",
    "\n",
    "```python\n",
    "import great_expectations as ge\n",
    "\n",
    "# Load a dataset (could be Spark DF, Pandas DF, SQL)\n",
    "df = ge.read_csv(\"sales_data.csv\")\n",
    "\n",
    "# Define expectations\n",
    "expectation_suite = df.expectation_suite\n",
    "df.expect_column_values_to_not_be_null(\"order_id\")\n",
    "df.expect_column_values_to_be_between(\"amount\", min_value=0, max_value=10000)\n",
    "df.expect_column_pair_values_to_be_equal(\"discount\", \"calculated_discount\")\n",
    "\n",
    "# Validate\n",
    "results = df.validate()\n",
    "assert results[\"success\"] == True\n",
    "```\n",
    "\n",
    "#### Example: Deequ (Scala/Java)\n",
    "\n",
    "Deequ is an AWS library for unit testing data.\n",
    "\n",
    "```scala\n",
    "import com.amazon.deequ.VerificationSuite\n",
    "import com.amazon.deequ.checks.{Check, CheckLevel}\n",
    "\n",
    "val verificationResult = VerificationSuite()\n",
    "  .onData(data)\n",
    "  .addCheck(\n",
    "    Check(CheckLevel.Error, \"Sales data checks\")\n",
    "      .isComplete(\"order_id\")\n",
    "      .hasMin(\"amount\", _ >= 0)\n",
    "      .hasMax(\"amount\", _ <= 10000)\n",
    "  ).run()\n",
    "\n",
    "if (verificationResult.status != Status.Success) {\n",
    "  throw new Exception(\"Data quality checks failed!\")\n",
    "}\n",
    "```\n",
    "\n",
    "### 58.5.5 Performance Testing\n",
    "\n",
    "Evaluate how the big data system behaves under load.\n",
    "\n",
    "**Metrics:**\n",
    "- **Throughput:** Records processed per second.\n",
    "- **Latency:** Time from ingestion to availability.\n",
    "- **Resource utilization:** CPU, memory, disk, network.\n",
    "\n",
    "**Tools:** Apache JMeter (with custom plugins), Gatling, custom Spark/Flink monitoring.\n",
    "\n",
    "**Test scenarios:**\n",
    "- Gradually increase data volume to find saturation point.\n",
    "- Burst traffic (spike in ingestion rate).\n",
    "- Long-running stability test (soak test).\n",
    "\n",
    "### 58.5.6 Security Testing\n",
    "\n",
    "Ensure data is protected throughout the pipeline.\n",
    "\n",
    "- **Authentication/Authorization:** Test access controls (Kerberos, IAM roles).\n",
    "- **Encryption:** Data encrypted at rest and in transit.\n",
    "- **Audit logs:** Verify logging of access to sensitive data.\n",
    "\n",
    "---\n",
    "\n",
    "## 58.6 Tools for Big Data Testing\n",
    "\n",
    "| Tool | Purpose |\n",
    "|------|---------|\n",
    "| **Apache Griffin** | Data quality platform for big data (supports Spark). |\n",
    "| **Deequ** | Unit tests for data, built on Spark. |\n",
    "| **Great Expectations** | Data validation with rich expectations. |\n",
    "| **Apache Spark Testing Base** | Utilities for testing Spark jobs. |\n",
    "| **pytest-spark** | Pytest plugin for Spark. |\n",
    "| **Apache JMeter** | Load testing for ingestion endpoints. |\n",
    "| **Apache Kafka** | Provides tools for testing consumers/producers. |\n",
    "| **HiveTest** | Testing Hive queries. |\n",
    "| **Cloud vendor tools** | AWS Glue DataBrew, GCP Dataprep for data quality. |\n",
    "\n",
    "---\n",
    "\n",
    "## 58.7 Code Examples\n",
    "\n",
    "### 58.7.1 Testing a Spark ETL Job with Pytest\n",
    "\n",
    "```python\n",
    "# conftest.py (pytest fixture for SparkSession)\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"pytest-pyspark\") \\\n",
    "        .master(\"local[2]\") \\\n",
    "        .getOrCreate()\n",
    "    yield spark\n",
    "    spark.stop()\n",
    "```\n",
    "\n",
    "```python\n",
    "# test_etl.py\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "\n",
    "def test_transform(spark):\n",
    "    input_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age\", IntegerType(), True)\n",
    "    ])\n",
    "    input_data = [(\"1\", \"Alice\", 25), (\"2\", \"Bob\", 30)]\n",
    "    input_df = spark.createDataFrame(input_data, input_schema)\n",
    "    \n",
    "    expected_schema = StructType([\n",
    "        StructField(\"id\", StringType(), True),\n",
    "        StructField(\"name\", StringType(), True),\n",
    "        StructField(\"age_group\", StringType(), True)\n",
    "    ])\n",
    "    expected_data = [(\"1\", \"Alice\", \"young\"), (\"2\", \"Bob\", \"adult\")]\n",
    "    expected_df = spark.createDataFrame(expected_data, expected_schema)\n",
    "    \n",
    "    from my_etl import categorize_age\n",
    "    result_df = categorize_age(input_df)\n",
    "    \n",
    "    assert result_df.collect() == expected_df.collect()\n",
    "    # Or use assertDataFrameEqual from pyspark.testing\n",
    "```\n",
    "\n",
    "### 58.7.2 Data Quality Check with Deequ (PyDeequ)\n",
    "\n",
    "```python\n",
    "from pydeequ import SparkSession\n",
    "from pydeequ.checks import Check, CheckLevel\n",
    "from pydeequ.verification import VerificationSuite\n",
    "\n",
    "spark = SparkSession.builder.appName(\"test\").getOrCreate()\n",
    "\n",
    "df = spark.read.csv(\"sales_data.csv\", header=True)\n",
    "\n",
    "check = Check(spark, CheckLevel.Error, \"Sales Checks\") \\\n",
    "    .isComplete(\"order_id\") \\\n",
    "    .isComplete(\"amount\") \\\n",
    "    .hasMin(\"amount\", lambda x: x >= 0) \\\n",
    "    .hasMax(\"amount\", lambda x: x <= 10000)\n",
    "\n",
    "result = VerificationSuite(spark).onData(df).addCheck(check).run()\n",
    "\n",
    "result_df = VerificationResult.checkResultsAsDataFrame(spark, result)\n",
    "result_df.show()\n",
    "assert result_df.filter(\"check_status != 'Success'\").count() == 0\n",
    "```\n",
    "\n",
    "### 58.7.3 Load Testing Kafka with Python\n",
    "\n",
    "```python\n",
    "# Simulate high-volume producer\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "import random\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "start = time.time()\n",
    "messages_sent = 0\n",
    "while time.time() - start < 60:  # run for 60 seconds\n",
    "    data = {\n",
    "        'sensor_id': random.randint(1, 100),\n",
    "        'value': random.random() * 100,\n",
    "        'timestamp': time.time()\n",
    "    }\n",
    "    producer.send('sensor_data', value=data)\n",
    "    messages_sent += 1\n",
    "    if messages_sent % 1000 == 0:\n",
    "        print(f\"Sent {messages_sent} messages\")\n",
    "\n",
    "producer.flush()\n",
    "producer.close()\n",
    "print(f\"Throughput: {messages_sent / 60} msg/sec\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 58.8 Best Practices for Big Data Testing\n",
    "\n",
    "1. **Test with representative data** \u2013 Use production-like data (anonymized) to uncover real issues.\n",
    "2. **Automate data quality checks** \u2013 Integrate tools like Great Expectations or Deequ into your pipeline.\n",
    "3. **Use data contracts** \u2013 Define schemas and expectations as code.\n",
    "4. **Test at multiple scales** \u2013 Unit test with small data, integration with medium, performance with large.\n",
    "5. **Mock external dependencies** \u2013 Simulate sources/sinks for isolated testing.\n",
    "6. **Monitor in production** \u2013 Deploy data quality monitors to catch issues post-release.\n",
    "7. **Version control your data** \u2013 Keep sample datasets under version control for repeatability.\n",
    "8. **Test failure scenarios** \u2013 Simulate node failures, network partitions, corrupted data.\n",
    "\n",
    "---\n",
    "\n",
    "## 58.9 Common Challenges and Solutions\n",
    "\n",
    "| Challenge | Solution |\n",
    "|-----------|----------|\n",
    "| **Test data volume too large** | Use sampling, but ensure sample preserves data characteristics. |\n",
    "| **Non-deterministic processing** | Force determinism by setting random seeds, using sorted operations. |\n",
    "| **Environment parity** | Use Docker/Kubernetes to replicate production-like clusters in CI. |\n",
    "| **Slow tests** | Parallelize test execution; use smaller datasets for unit tests. |\n",
    "| **Schema evolution** | Maintain multiple schema versions in test; test backward compatibility. |\n",
    "| **Data privacy** | Use data masking or synthetic data generation. |\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored **Big Data Testing**:\n",
    "\n",
    "- **Big Data fundamentals** \u2013 the 5 V's and typical architecture.\n",
    "- **Testing challenges** \u2013 volume, distribution, variety, non-determinism.\n",
    "- **Types of testing** \u2013 ingestion, processing, storage, validation, performance, security.\n",
    "- **Tools** \u2013 Great Expectations, Deequ, Spark Testing Base, JMeter.\n",
    "- **Code examples** \u2013 Spark job testing, data quality checks, Kafka load testing.\n",
    "- **Best practices** \u2013 representative data, automation, monitoring.\n",
    "\n",
    "**Key Insight:** Big Data testing is not just about verifying correctness but also ensuring data quality, performance, and reliability at massive scale. By combining automated validation with performance testing, teams can deliver trustworthy data products.\n",
    "\n",
    "---\n",
    "\n",
    "## \ud83d\udcd6 Next Chapter: Chapter 59 - Test Documentation Standards\n",
    "\n",
    "Now that you've covered advanced testing domains, Chapter 59 will revisit the foundational but critical topic of **Test Documentation Standards**, covering IEEE 829, test plan templates, and how to document testing in modern agile environments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='57. iot_testing.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../14. test_documentation_and_reporting/59. test_documentation_standards.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}