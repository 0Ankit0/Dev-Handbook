{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 10: Modern Data Architectures in the Cloud**\n",
    "\n",
    "## Introduction: The Data-First Imperative\n",
    "\n",
    "As organizations migrate workloads to the cloud, the volume, velocity, and variety of data have exploded exponentially. While Chapter 9 explored how serverless architectures handle compute elasticity and event processing, modern applications are fundamentally data-driven. Whether it's real-time personalization, predictive analytics, or AI-powered insights, the ability to ingest, store, process, and analyze data at scale separates innovative organizations from their competitors.\n",
    "\n",
    "Traditional on-premises data architectures struggled with rigid schemas, capacity planning challenges, and the inability to handle unstructured data. Cloud-native data architectures eliminate these constraints through decoupled storage and compute, serverless query engines, and unified analytics platforms that can process petabytes of data without provisioning a single server.\n",
    "\n",
    "This chapter examines the evolution from monolithic databases to modern data platforms, covering data lakes, warehouses, streaming pipelines, and the integration of machine learning workflows. We will explore how these components work synergistically with the serverless patterns from Chapter 9 to create intelligent, data-rich applications.\n",
    "\n",
    "---\n",
    "\n",
    "## 10.1 Data Lakes vs. Data Warehouses: Architecture Patterns\n",
    "\n",
    "The choice between data lakes and data warehouses represents fundamental architectural decisions about schema enforcement, data structure, and analytical workloads. Modern organizations often implement both in a unified \"lakehouse\" architecture.\n",
    "\n",
    "### 10.1.1 Understanding the Data Lake\n",
    "\n",
    "A data lake is a centralized repository that allows you to store all your structured and unstructured data at any scale. Unlike traditional databases, data lakes store data in its raw format without requiring upfront schema definition (schema-on-read).\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Raw Storage:** Stores data in native formats (JSON, CSV, Parquet, images, videos, logs)\n",
    "- **Schema-on-Read:** Structure is applied when data is accessed, not when ingested\n",
    "- **Infinite Scale:** Object storage (S3, Azure Data Lake Storage, GCS) provides virtually unlimited capacity\n",
    "- **Cost-Effective:** Store everything; cheap storage for exploratory analytics and data science\n",
    "\n",
    "**The Modern Data Lake Architecture (Three-Layer Pattern):**\n",
    "\n",
    "Industry best practices organize data lakes into distinct zones to balance agility with governance:\n",
    "\n",
    "1. **Bronze Layer (Raw):** Data lands here exactly as received from source systems. Immutable, append-only storage of historical data. No transformations applied.\n",
    "\n",
    "2. **Silver Layer (Cleansed):** Data quality rules applied, deduplication, type casting, and basic normalization. Ready for analytical querying but not yet business-aggregated.\n",
    "\n",
    "3. **Gold Layer (Curated):** Business-level aggregates, star schemas, and feature engineering for ML models. Optimized for specific consumption patterns.\n",
    "\n",
    "**Terraform Implementation of a Secure Data Lake:**\n",
    "\n",
    "```hcl\n",
    "# Infrastructure for a three-tier data lake on AWS S3\n",
    "resource \"aws_s3_bucket\" \"data_lake\" {\n",
    "  bucket = \"company-data-lake-${var.environment}\"\n",
    "  \n",
    "  tags = {\n",
    "    Environment = var.environment\n",
    "    Layer       = \"multi-zone\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Bronze Zone - Raw data ingestion\n",
    "resource \"aws_s3_bucket\" \"bronze\" {\n",
    "  bucket = \"${aws_s3_bucket.data_lake.bucket}-bronze\"\n",
    "}\n",
    "\n",
    "resource \"aws_s3_bucket_lifecycle_configuration\" \"bronze_lifecycle\" {\n",
    "  bucket = aws_s3_bucket.bronze.id\n",
    "  \n",
    "  rule {\n",
    "    id     = \"transition-to-glacier\"\n",
    "    status = \"Enabled\"\n",
    "    \n",
    "    transition {\n",
    "      days          = 90\n",
    "      storage_class = \"GLACIER_IR\"  # Instant Retrieval for occasional access\n",
    "    }\n",
    "    \n",
    "    noncurrent_version_expiration {\n",
    "      noncurrent_days = 30\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Silver Zone - Cleaned and validated data\n",
    "resource \"aws_s3_bucket\" \"silver\" {\n",
    "  bucket = \"${aws_s3_bucket.data_lake.bucket}-silver\"\n",
    "}\n",
    "\n",
    "resource \"aws_s3_bucket_versioning\" \"silver_versioning\" {\n",
    "  bucket = aws_s3_bucket.silver.id\n",
    "  versioning_configuration {\n",
    "    status = \"Enabled\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Gold Zone - Business-ready aggregates\n",
    "resource \"aws_s3_bucket\" \"gold\" {\n",
    "  bucket = \"${aws_s3_bucket.data_lake.bucket}-gold\"\n",
    "}\n",
    "\n",
    "# Security: Bucket policies to enforce encryption and access logging\n",
    "resource \"aws_s3_bucket_server_side_encryption_configuration\" \"encryption\" {\n",
    "  for_each = toset([aws_s3_bucket.bronze.id, aws_s3_bucket.silver.id, aws_s3_bucket.gold.id])\n",
    "  \n",
    "  bucket = each.value\n",
    "  \n",
    "  rule {\n",
    "    apply_server_side_encryption_by_default {\n",
    "      sse_algorithm     = \"aws:kms\"\n",
    "      kms_master_key_id = aws_kms_key.data_lake_key.arn\n",
    "    }\n",
    "    bucket_key_enabled = true\n",
    "  }\n",
    "}\n",
    "\n",
    "# Cross-region replication for disaster recovery\n",
    "resource \"aws_s3_bucket_replication_configuration\" \"replication\" {\n",
    "  role   = aws_iam_role.replication.arn\n",
    "  bucket = aws_s3_bucket.gold.id\n",
    "  \n",
    "  rule {\n",
    "    id     = \"replicate-gold-tier\"\n",
    "    status = \"Enabled\"\n",
    "    \n",
    "    destination {\n",
    "      bucket        = aws_s3_bucket.gold_dr.arn\n",
    "      storage_class = \"STANDARD\"\n",
    "      \n",
    "      encryption_configuration {\n",
    "        replica_kms_key_id = aws_kms_key.data_lake_key_dr.arn\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    source_selection_criteria {\n",
    "      sse_kms_encrypted_objects {\n",
    "        status = \"Enabled\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Zone Separation:** Physical separation via buckets enforces data quality boundaries and allows different lifecycle policies (e.g., raw data moves to Glacier after 90 days, curated data remains immediately accessible)\n",
    "- **Encryption:** All tiers use AWS KMS for server-side encryption; bucket keys reduce KMS API costs for high-throughput workloads\n",
    "- **Versioning:** Enabled on Silver and Gold tiers to protect against accidental deletion and support time-travel queries\n",
    "- **Replication:** Only the Gold tier replicates cross-region, as it contains business-critical aggregated data, while raw data can be re-ingested if needed\n",
    "\n",
    "### 10.1.2 The Data Warehouse: Structured Analytics\n",
    "\n",
    "While data lakes excel at storing vast amounts of diverse data, data warehouses optimize for high-performance SQL analytics on structured data. They employ columnar storage, massively parallel processing (MPP), and sophisticated query optimizers.\n",
    "\n",
    "**Key Characteristics:**\n",
    "- **Schema-on-Write:** Data must conform to defined schemas before ingestion\n",
    "- **Columnar Storage:** Optimized for aggregations (SUM, AVG, COUNT) across billions of rows\n",
    "- **SQL-Native:** Standard SQL interface for business intelligence tools\n",
    "- **Strong Consistency:** ACID transactions for accurate reporting\n",
    "\n",
    "**Modern Cloud Data Warehouses:**\n",
    "- **Amazon Redshift:** Petabyte-scale warehouse with RA3 nodes (managed storage), Spectrum (query S3 directly), and Serverless option\n",
    "- **Google BigQuery:** Fully serverless, separates compute from storage, charges per query (on-demand) or per slot (reserved)\n",
    "- **Azure Synapse Analytics:** Unified analytics platform combining SQL pools, Spark pools, and data exploration\n",
    "- **Snowflake:** Cross-cloud warehouse with instant elasticity, zero-copy cloning, and time travel\n",
    "\n",
    "**BigQuery Example: Creating a Partitioned and Clustered Table:**\n",
    "\n",
    "Partitioning and clustering are critical optimization techniques that reduce query costs and improve performance by limiting the data scanned.\n",
    "\n",
    "```sql\n",
    "-- Creating an optimized table for time-series analytics\n",
    "CREATE OR REPLACE TABLE `analytics.orders_processed`\n",
    "(\n",
    "    order_id STRING NOT NULL,\n",
    "    customer_id STRING NOT NULL,\n",
    "    order_timestamp TIMESTAMP NOT NULL,\n",
    "    amount NUMERIC,\n",
    "    currency STRING,\n",
    "    status STRING,\n",
    "    product_category STRING,\n",
    "    device_type STRING,\n",
    "    geo_region STRING\n",
    ")\n",
    "PARTITION BY DATE(order_timestamp)  -- Daily partitions\n",
    "CLUSTER BY geo_region, product_category  -- Column order matters for filtering\n",
    "OPTIONS(\n",
    "    description = \"Processed orders with optimization for regional and category analysis\",\n",
    "    labels = [(\"team\", \"analytics\"), (\"sensitivity\", \"confidential\")]\n",
    ");\n",
    "\n",
    "-- Query that leverages partitioning and clustering\n",
    "-- This query only scans the last 7 days of data and specific regions\n",
    "SELECT \n",
    "    product_category,\n",
    "    COUNT(*) as order_count,\n",
    "    SUM(amount) as total_revenue\n",
    "FROM `analytics.orders_processed`\n",
    "WHERE order_timestamp >= TIMESTAMP_SUB(CURRENT_TIMESTAMP(), INTERVAL 7 DAY)\n",
    "  AND geo_region IN ('US-West', 'US-East')\n",
    "  AND product_category = 'Electronics'\n",
    "GROUP BY product_category;\n",
    "\n",
    "-- Best Practice: Use preview to estimate bytes processed\n",
    "-- BigQuery shows \"This query will process 12.3 MB\" before execution\n",
    "```\n",
    "\n",
    "**Why This Matters:**\n",
    "- **Partitioning:** Divides the table into daily segments. A query for one day scans only that partition, not the entire table (potentially saving 99% of costs for multi-year datasets)\n",
    "- **Clustering:** Organizes data within partitions by column values. When filtering by `geo_region` and `product_category`, BigQuery skips blocks that don't contain matching values\n",
    "- **Cost Control:** BigQuery charges by bytes scanned ($5 per TB). Proper optimization can reduce costs from hundreds of dollars to cents per query\n",
    "\n",
    "### 10.1.3 The Lakehouse Architecture: Best of Both Worlds\n",
    "\n",
    "Modern architectures blur the lines between lakes and warehouses through the \"Lakehouse\" pattern, enabled by open table formats like Apache Iceberg, Delta Lake, and Apache Hudi.\n",
    "\n",
    "**Benefits:**\n",
    "- **ACID transactions** on data lake storage (S3/ADLS)\n",
    "- **Time travel:** Query data as it existed at any point in time\n",
    "- **Schema evolution:** Add columns without breaking existing queries\n",
    "- **Unified governance:** Single security model across all data\n",
    "\n",
    "**Delta Lake Example (Azure Databricks/AWS Glue):**\n",
    "\n",
    "```python\n",
    "from delta import DeltaTable, configure_spark_with_delta_pip\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, current_timestamp\n",
    "\n",
    "# Initialize Spark with Delta Lake\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"LakehouseETL\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "# Bronze ingestion: Append raw JSON data\n",
    "raw_df = spark.read.json(\"s3a://data-lake-bronze/events/\")\n",
    "\n",
    "# Write to Delta table (Silver tier) with merge schema evolution\n",
    "(raw_df.write\n",
    "    .format(\"delta\")\n",
    "    .mode(\"append\")\n",
    "    .option(\"mergeSchema\", \"true\")  # Automatically handle new columns\n",
    "    .save(\"s3a://data-lake-silver/events/\"))\n",
    "\n",
    "# Create Silver table with quality checks\n",
    "silver_df = spark.read.format(\"delta\").load(\"s3a://data-lake-silver/events/\")\n",
    "\n",
    "# Data quality validation and cleansing\n",
    "clean_df = (silver_df\n",
    "    .filter(col(\"user_id\").isNotNull())\n",
    "    .filter(col(\"event_timestamp\") > \"2020-01-01\")\n",
    "    .withColumn(\"processed_at\", current_timestamp()))\n",
    "\n",
    "# Upsert (Merge) into curated Gold table\n",
    "# This handles late-arriving data and duplicates idempotently\n",
    "delta_table = DeltaTable.forPath(spark, \"s3a://data-lake-gold/user-events/\")\n",
    "\n",
    "(delta_table.alias(\"target\")\n",
    "    .merge(clean_df.alias(\"source\"), \"target.event_id = source.event_id\")\n",
    "    .whenMatchedUpdateAll()\n",
    "    .whenNotMatchedInsertAll()\n",
    "    .execute())\n",
    "\n",
    "# Time Travel: Query data as it was yesterday\n",
    "yesterday_df = (spark.read\n",
    "    .format(\"delta\")\n",
    "    .option(\"timestampAsOf\", \"2026-02-01T00:00:00Z\")\n",
    "    .load(\"s3a://data-lake-gold/user-events/\"))\n",
    "\n",
    "# Optimize table layout for query performance\n",
    "spark.sql(\"OPTIMIZE delta.`s3a://data-lake-gold/user-events/` ZORDER BY (user_id)\")\n",
    "```\n",
    "\n",
    "**Key Operations Explained:**\n",
    "- **Merge/Upsert:** Atomically updates existing records and inserts new ones, handling late-arriving data without duplicates\n",
    "- **Time Travel:** Audit compliance and debugging by querying historical states without maintaining separate snapshots\n",
    "- **Z-Ordering:** Co-locates related data in the same files, reducing I/O for analytical queries that filter on specific columns\n",
    "\n",
    "---\n",
    "\n",
    "## 10.2 Data Pipelines: ETL vs. ELT Patterns\n",
    "\n",
    "Data pipelines move and transform data between systems. Cloud architectures have shifted from traditional ETL (Extract, Transform, Load) to ELT (Extract, Load, Transform) to leverage the scalability of target warehouses.\n",
    "\n",
    "### 10.2.1 Understanding ETL vs. ELT\n",
    "\n",
    "**ETL (Extract, Transform, Load):**\n",
    "- Transformations occur in a dedicated engine (often Spark or Dataflow) before loading\n",
    "- **Pros:** Data is clean before reaching the warehouse; protects production systems from heavy transformation loads\n",
    "- **Cons:** Requires capacity planning for transformation clusters; schema changes require pipeline modifications\n",
    "\n",
    "**ELT (Extract, Load, Transform):**\n",
    "- Raw data loads directly into the warehouse; transformations occur there using SQL\n",
    "- **Pros:** Leverages warehouse scalability; analysts can transform data using familiar SQL; schema-on-read flexibility\n",
    "- **Cons:** Requires disciplined cost management; raw data quality issues can propagate\n",
    "\n",
    "**Modern Approach:** Hybrid ELT/ETL where lightweight cleansing happens during extraction (ETL), but heavy aggregations occur in the warehouse (ELT).\n",
    "\n",
    "### 10.2.2 Managed Pipeline Services\n",
    "\n",
    "Cloud providers offer fully managed, serverless ETL/ELT services that eliminate infrastructure management:\n",
    "\n",
    "**AWS Glue (Serverless Spark):**\n",
    "- Automatically generates ETL code from data sources\n",
    "- Glue Data Catalog serves as the central metadata repository\n",
    "- Glue Studio provides visual ETL development\n",
    "\n",
    "**Azure Data Factory:**\n",
    "- Cloud-based ETL and data integration service\n",
    "- Orchestrates data movement and transformation across cloud and on-premises sources\n",
    "- Integration with Azure Synapse Analytics for big data processing\n",
    "\n",
    "**Google Cloud Dataflow:**\n",
    "- Apache Beam-based streaming and batch processing\n",
    "- Autoscaling from zero to thousands of workers\n",
    "- Exactly-once processing semantics for streaming data\n",
    "\n",
    "**AWS Glue ETL Job Example (Python/PySpark):**\n",
    "\n",
    "```python\n",
    "import sys\n",
    "from awsglue.transforms import *\n",
    "from awsglue.utils import getResolvedOptions\n",
    "from pyspark.context import SparkContext\n",
    "from awsglue.context import GlueContext\n",
    "from awsglue.job import Job\n",
    "from awsglue.dynamicframe import DynamicFrame\n",
    "from pyspark.sql.functions import udf, col, to_timestamp\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Initialize Glue context\n",
    "args = getResolvedOptions(sys.argv, ['JOB_NAME'])\n",
    "sc = SparkContext()\n",
    "glueContext = GlueContext(sc)\n",
    "spark = glueContext.spark_session\n",
    "job = Job(glueContext)\n",
    "job.init(args['JOB_NAME'], args)\n",
    "\n",
    "# Read from Data Catalog (Bronze layer)\n",
    "datasource = glueContext.create_dynamic_frame.from_catalog(\n",
    "    database = \"ecommerce_raw\",\n",
    "    table_name = \"orders_json\",\n",
    "    transformation_ctx = \"datasource\"\n",
    ")\n",
    "\n",
    "# Convert to DataFrame for complex transformations\n",
    "df = datasource.toDF()\n",
    "\n",
    "# Data quality transformations\n",
    "@udf(StringType())\n",
    "def normalize_phone(phone):\n",
    "    \"\"\"Standardize phone number formats\"\"\"\n",
    "    if not phone:\n",
    "        return None\n",
    "    digits = ''.join(filter(str.isdigit, phone))\n",
    "    return f\"+1-{digits[:3]}-{digits[3:6]}-{digits[6:]}\" if len(digits) == 10 else None\n",
    "\n",
    "# Apply transformations\n",
    "cleaned_df = (df\n",
    "    .filter(col(\"order_id\").isNotNull())  # Remove null keys\n",
    "    .withColumn(\"order_date\", to_timestamp(col(\"timestamp\"), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    .withColumn(\"phone_normalized\", normalize_phone(col(\"customer_phone\")))\n",
    "    .dropDuplicates([\"order_id\"])  # Idempotent processing\n",
    "    .cache()  # Optimize for multiple sinks\n",
    ")\n",
    "\n",
    "# Convert back to DynamicFrame for Glue features\n",
    "cleaned_dynamic_frame = DynamicFrame.fromDF(cleaned_df, glueContext, \"cleaned\")\n",
    "\n",
    "# Write to Silver layer in Parquet format (columnar, compressed)\n",
    "glueContext.write_dynamic_frame.from_options(\n",
    "    frame = cleaned_dynamic_frame,\n",
    "    connection_type = \"s3\",\n",
    "    connection_options = {\n",
    "        \"path\": \"s3://data-lake-silver/orders/\",\n",
    "        \"partitionKeys\": [\"year\", \"month\", \"day\"]  # Hive-style partitioning\n",
    "    },\n",
    "    format = \"parquet\",\n",
    "    transformation_ctx = \"datasink\"\n",
    ")\n",
    "\n",
    "# Also write to Redshift (Gold layer) for immediate BI access\n",
    "glueContext.write_dynamic_frame.from_jdbc_conf(\n",
    "    frame = cleaned_dynamic_frame,\n",
    "    catalog_connection = \"redshift-connection\",\n",
    "    connection_options = {\n",
    "        \"dbtable\": \"public.orders_staging\",\n",
    "        \"database\": \"analytics\"\n",
    "    },\n",
    "    redshift_tmp_dir = \"s3://temp-bucket/redshift/\",\n",
    "    transformation_ctx = \"redshift_sink\"\n",
    ")\n",
    "\n",
    "job.commit()\n",
    "```\n",
    "\n",
    "**Architecture Highlights:**\n",
    "- **DynamicFrames:** Glue-specific abstraction that handles schema variations gracefully (missing fields, type changes)\n",
    "- **Pushdown Predicates:** When reading from JDBC sources, Glue pushes filters to the database to minimize data transfer\n",
    "- **Bookmarking:** Glue automatically tracks processed data to enable incremental loads without manual offset management\n",
    "- **Parquet Format:** Columnar storage with Snappy compression provides 10x better query performance than JSON\n",
    "\n",
    "### 10.2.3 Orchestration with Apache Airflow/MWAA\n",
    "\n",
    "Complex pipelines require orchestration to handle dependencies, retries, and scheduling. Apache Airflow (available as Amazon MWAA, Google Cloud Composer, or Azure Managed Airflow) is the industry standard.\n",
    "\n",
    "**DAG (Directed Acyclic Graph) Definition:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.providers.amazon.aws.operators.glue import GlueJobOperator\n",
    "from airflow.providers.amazon.aws.operators.redshift_data import RedshiftDataOperator\n",
    "from airflow.providers.amazon.aws.sensors.glue import GlueJobSensor\n",
    "from airflow.utils.task_group import TaskGroup\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data-engineering',\n",
    "    'depends_on_past': False,\n",
    "    'email_on_failure': True,\n",
    "    'email': ['data-team@company.com'],\n",
    "    'retries': 2,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "    'sla': timedelta(hours=2)  # Service Level Agreement\n",
    "}\n",
    "\n",
    "with DAG(\n",
    "    'ecommerce_daily_etl',\n",
    "    default_args=default_args,\n",
    "    description='Daily ETL from raw data to Redshift',\n",
    "    schedule_interval='0 3 * * *',  # Daily at 3 AM\n",
    "    start_date=datetime(2026, 1, 1),\n",
    "    catchup=False,\n",
    "    tags=['ecommerce', 'daily', 'critical'],\n",
    "    max_active_runs=1  # Prevent overlapping executions\n",
    ") as dag:\n",
    "\n",
    "    # Task Group for Bronze ingestion (parallelizable)\n",
    "    with TaskGroup(group_id='bronze_ingestion') as bronze_group:\n",
    "        ingest_orders = GlueJobOperator(\n",
    "            task_id='ingest_orders',\n",
    "            job_name='bronze_orders_ingestion',\n",
    "            region_name='us-east-1',\n",
    "            wait_for_completion=False  # Async execution\n",
    "        )\n",
    "        \n",
    "        ingest_customers = GlueJobOperator(\n",
    "            task_id='ingest_customers',\n",
    "            job_name='bronze_customers_ingestion',\n",
    "            region_name='us-east-1',\n",
    "            wait_for_completion=False\n",
    "        )\n",
    "        \n",
    "        ingest_products = GlueJobOperator(\n",
    "            task_id='ingest_products',\n",
    "            job_name='bronze_products_ingestion',\n",
    "            region_name='us-east-1',\n",
    "            wait_for_completion=False\n",
    "        )\n",
    "    \n",
    "    # Wait for all bronze jobs\n",
    "    wait_for_bronze = GlueJobSensor(\n",
    "        task_id='wait_for_bronze',\n",
    "        job_name='dummy',  # Updated dynamically via XCom\n",
    "        region_name='us-east-1',\n",
    "        mode='reschedule',  # Free up worker slot while waiting\n",
    "        poke_interval=60\n",
    "    )\n",
    "    \n",
    "    # Silver transformation (single job for consistency)\n",
    "    silver_transformation = GlueJobOperator(\n",
    "        task_id='silver_transform',\n",
    "        job_name='silver_orders_transformation',\n",
    "        region_name='us-east-1',\n",
    "        script_args={\n",
    "            '--TARGET_DATE': '{{ ds }}',  # Jinja templating for execution date\n",
    "            '--ENV': 'production'\n",
    "        }\n",
    "    )\n",
    "    \n",
    "    # Gold layer: Redshift transformations using SQL\n",
    "    gold_aggregations = RedshiftDataOperator(\n",
    "        task_id='update_daily_metrics',\n",
    "        cluster_identifier='analytics-cluster',\n",
    "        database='analytics',\n",
    "        sql=\"\"\"\n",
    "            BEGIN;\n",
    "            \n",
    "            -- Upsert daily aggregates\n",
    "            DELETE FROM daily_metrics \n",
    "            WHERE metric_date = '{{ ds }}';\n",
    "            \n",
    "            INSERT INTO daily_metrics\n",
    "            SELECT \n",
    "                DATE(order_date) as metric_date,\n",
    "                COUNT(*) as total_orders,\n",
    "                SUM(amount) as total_revenue,\n",
    "                COUNT(DISTINCT customer_id) as unique_customers\n",
    "            FROM silver.orders\n",
    "            WHERE DATE(order_date) = '{{ ds }}'\n",
    "            GROUP BY 1;\n",
    "            \n",
    "            COMMIT;\n",
    "        \"\"\",\n",
    "        aws_conn_id='aws_default'\n",
    "    )\n",
    "    \n",
    "    # Data quality check\n",
    "    quality_check = RedshiftDataOperator(\n",
    "        task_id='check_data_quality',\n",
    "        cluster_identifier='analytics-cluster',\n",
    "        database='analytics',\n",
    "        sql=\"\"\"\n",
    "            SELECT CASE \n",
    "                WHEN COUNT(*) > 1000 AND SUM(amount) > 0 THEN 'PASS'\n",
    "                ELSE 'FAIL'\n",
    "            END as quality_check\n",
    "            FROM daily_metrics\n",
    "            WHERE metric_date = '{{ ds }}'\n",
    "        \"\"\"\n",
    "    )\n",
    "    \n",
    "    # Define dependencies\n",
    "    bronze_group >> wait_for_bronze >> silver_transformation >> gold_aggregations >> quality_check\n",
    "```\n",
    "\n",
    "**Orchestration Best Practices:**\n",
    "- **Task Groups:** Group related tasks for visual organization and parallel execution\n",
    "- **Sensors vs. Operators:** Use Sensors (like `GlueJobSensor`) to wait for external resources without consuming worker slots (mode='reschedule')\n",
    "- **SLAs:** Define service level agreements to detect late pipelines\n",
    "- **Backfilling:** Airflow's `catchup=False` prevents automatic backfills, but manual backfills can be triggered for historical data processing\n",
    "\n",
    "---\n",
    "\n",
    "## 10.3 Streaming Data and Real-Time Analytics\n",
    "\n",
    "Batch processing (running jobs every hour or day) is insufficient for use cases requiring immediate action: fraud detection, IoT monitoring, real-time personalization, and operational dashboards. Streaming architectures process data continuously as it arrives.\n",
    "\n",
    "### 10.3.1 Streaming Fundamentals\n",
    "\n",
    "**Stream Processing Concepts:**\n",
    "- **Event Time vs. Processing Time:** Event time is when the event occurred; processing time is when the system handles it. Watermarks handle late-arriving data.\n",
    "- **Windowing:** Aggregating events over time windows (tumbling, sliding, session windows)\n",
    "- **Exactly-Once Semantics:** Ensuring each event affects results exactly once, even during failures\n",
    "\n",
    "**Managed Streaming Services:**\n",
    "- **Amazon Kinesis:** Managed Kafka alternative with Data Streams (provisioning) and Data Firehose (auto-scaling delivery to S3/Redshift/OpenSearch)\n",
    "- **Azure Event Hubs:** Kafka-compatible event streaming platform with Capture feature (automatic archiving to Blob Storage)\n",
    "- **Google Pub/Sub:** Global messaging service with at-least-once delivery and Push/Pull subscription models\n",
    "- **Confluent Cloud:** Fully managed Apache Kafka across clouds\n",
    "\n",
    "### 10.3.2 Real-Time Stream Processing\n",
    "\n",
    "**Apache Flink (Amazon Managed Flink, Azure Stream Analytics):**\n",
    "Stateful stream processing with sub-second latency, handling complex event processing (CEP) and windowed aggregations.\n",
    "\n",
    "**Kinesis Data Analytics (Flink) Example:**\n",
    "\n",
    "```java\n",
    "// Flink job for real-time fraud detection\n",
    "public class FraudDetectionJob {\n",
    "    \n",
    "    public static void main(String[] args) throws Exception {\n",
    "        StreamExecutionEnvironment env = StreamExecutionEnvironment.getExecutionEnvironment();\n",
    "        \n",
    "        // Configure checkpointing for exactly-once semantics\n",
    "        env.enableCheckpointing(5000); // 5-second checkpoints\n",
    "        env.getCheckpointConfig().setCheckpointingMode(CheckpointingMode.EXACTLY_ONCE);\n",
    "        \n",
    "        // Read from Kinesis Data Stream\n",
    "        FlinkKinesisConsumer<Transaction> source = new FlinkKinesisConsumer<>(\n",
    "            \"transactions\",\n",
    "            new TransactionSchema(),\n",
    "            kinesisConsumerConfig()\n",
    "        );\n",
    "        \n",
    "        DataStream<Transaction> transactions = env.addSource(source);\n",
    "        \n",
    "        // Key by customer and detect patterns within 10-minute windows\n",
    "        Pattern<Transaction, ?> fraudPattern = Pattern\n",
    "            .<Transaction>begin(\"start\")\n",
    "            .where(new SimpleCondition<Transaction>() {\n",
    "                public boolean filter(Transaction t) {\n",
    "                    return t.amount > 10000; // Large transaction\n",
    "                }\n",
    "            })\n",
    "            .next(\"middle\")\n",
    "            .where(new SimpleCondition<Transaction>() {\n",
    "                public boolean filter(Transaction t) {\n",
    "                    return t.location != t.previousLocation; // Different location\n",
    "                }\n",
    "            })\n",
    "            .within(Time.minutes(10));\n",
    "        \n",
    "        // Apply CEP pattern\n",
    "        PatternStream<Transaction> patternStream = CEP.pattern(\n",
    "            transactions.keyBy(Transaction::getCustomerId),\n",
    "            fraudPattern\n",
    "        );\n",
    "        \n",
    "        // Alert generation\n",
    "        DataStream<Alert> alerts = patternStream\n",
    "            .select(new PatternSelectFunction<Transaction, Alert>() {\n",
    "                public Alert select(Map<String, List<Transaction>> pattern) {\n",
    "                    Transaction start = pattern.get(\"start\").get(0);\n",
    "                    Transaction middle = pattern.get(\"middle\").get(0);\n",
    "                    return new Alert(start.customerId, \"SUSPICIOUS_ACTIVITY\", \n",
    "                        String.format(\"Large tx followed by location change: %s to %s\", \n",
    "                        start.location, middle.location));\n",
    "                }\n",
    "            });\n",
    "        \n",
    "        // Sink to SNS for notifications\n",
    "        alerts.addSink(new SNSAlertSink());\n",
    "        \n",
    "        env.execute(\"Fraud Detection Pipeline\");\n",
    "    }\n",
    "}\n",
    "```\n",
    "\n",
    "### 10.3.3 Lambda Architecture vs. Kappa Architecture\n",
    "\n",
    "**Lambda Architecture (Traditional):**\n",
    "- **Batch Layer:** Process all historical data (accurate but high latency)\n",
    "- **Speed Layer:** Process real-time data (approximate, low latency)\n",
    "- **Serving Layer:** Merge batch and speed views for querying\n",
    "- **Cons:** Complex, requires maintaining two codebases (batch and stream)\n",
    "\n",
    "**Kappa Architecture (Modern):**\n",
    "- **Unified Processing:** Use streaming for both real-time and historical reprocessing\n",
    "- **Reprocessing:** Replay streams from beginning to rebuild state\n",
    "- **Pros:** Single codebase, simplified architecture\n",
    "- **Tools:** Kafka with Kafka Streams, Kinesis with Lambda/Flink\n",
    "\n",
    "**Implementing Kappa Architecture with DynamoDB Streams and Lambda:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Lambda function triggered by DynamoDB Stream\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Real-time materialized view maintenance\n",
    "    Updates aggregate counts as orders arrive\n",
    "    \"\"\"\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    aggregates_table = dynamodb.Table('real-time-aggregates')\n",
    "    \n",
    "    for record in event['Records']:\n",
    "        if record['eventName'] in ['INSERT', 'MODIFY']:\n",
    "            new_image = record['dynamodb']['NewImage']\n",
    "            \n",
    "            # Extract fields from DynamoDB stream format\n",
    "            category = new_image['product_category']['S']\n",
    "            amount = float(new_image['amount']['N'])\n",
    "            timestamp = new_image['order_timestamp']['S']\n",
    "            hour_key = timestamp[:13]  # YYYY-MM-DD-HH\n",
    "            \n",
    "            # Update real-time aggregates atomically\n",
    "            try:\n",
    "                aggregates_table.update_item(\n",
    "                    Key={\n",
    "                        'aggregate_id': f\"category:{category}:hour:{hour_key}\"\n",
    "                    },\n",
    "                    UpdateExpression=\"\"\"\n",
    "                        SET \n",
    "                            #cnt = if_not_exists(#cnt, :zero) + :inc,\n",
    "                            total_amount = if_not_exists(total_amount, :zero) + :amt,\n",
    "                            last_updated = :now\n",
    "                    \"\"\",\n",
    "                    ExpressionAttributeNames={\n",
    "                        '#cnt': 'count'\n",
    "                    },\n",
    "                    ExpressionAttributeValues={\n",
    "                        ':inc': 1,\n",
    "                        ':amt': amount,\n",
    "                        ':zero': 0,\n",
    "                        ':now': datetime.utcnow().isoformat()\n",
    "                    }\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error updating aggregate: {e}\")\n",
    "                raise\n",
    "    \n",
    "    return {'processed': len(event['Records'])}\n",
    "```\n",
    "\n",
    "**Architecture Explanation:**\n",
    "- **Change Data Capture (CDC):** DynamoDB Streams captures item-level modifications, enabling reactive architectures\n",
    "- **Idempotent Updates:** The `if_not_exists` function ensures the update is idempotent (safe to retry)\n",
    "- **Hot Partitions:** The `hour_key` creates time-bound partitions to prevent throttling on a single \"category\" partition key\n",
    "\n",
    "---\n",
    "\n",
    "## 10.4 AI/ML Integration in Data Architectures\n",
    "\n",
    "Modern data platforms must support machine learning workflows, from feature engineering to model training and inference. Cloud providers offer managed ML platforms that integrate with data lakes and warehouses.\n",
    "\n",
    "### 10.4.1 Feature Stores\n",
    "\n",
    "A Feature Store is a centralized repository for storing, sharing, and serving ML features. It ensures consistency between training and inference (training-serving skew) and enables feature reuse across teams.\n",
    "\n",
    "**Components:**\n",
    "- **Offline Store:** Historical features for training (often in S3/Parquet)\n",
    "- **Online Store:** Low-latency feature serving for real-time inference (Redis/DynamoDB)\n",
    "- **Feature Registry:** Catalog of available features with metadata and versioning\n",
    "\n",
    "**Amazon SageMaker Feature Store Example:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker.feature_store.feature_group import FeatureGroup\n",
    "from sagemaker.feature_store.inputs import TableFormatEnum\n",
    "\n",
    "# Initialize Feature Store session\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = sagemaker.get_execution_role()\n",
    "featurestore_runtime = boto3.client('sagemaker-featurestore-runtime')\n",
    "\n",
    "# Define feature group for customer features\n",
    "customer_fg = FeatureGroup(\n",
    "    name=\"customer-demographics-features\",\n",
    "    sagemaker_session=sagemaker_session\n",
    ")\n",
    "\n",
    "# Define schema\n",
    "customer_fg.load_feature_definitions(data_frame=customer_df)\n",
    "\n",
    "# Create feature group with offline (S3) and online (DynamoDB) stores\n",
    "customer_fg.create(\n",
    "    s3_uri=f\"s3://{bucket}/feature-store/\",\n",
    "    record_identifier_name=\"customer_id\",\n",
    "    event_time_feature_name=\"event_time\",\n",
    "    role_arn=role,\n",
    "    enable_online_store=True,\n",
    "    table_format=TableFormatEnum.ICEBERG  # Open table format for lakehouse integration\n",
    ")\n",
    "\n",
    "# Ingest features\n",
    "customer_fg.ingest(data_frame=customer_df, max_workers=3, wait=True)\n",
    "\n",
    "# Real-time feature retrieval for inference\n",
    "response = featurestore_runtime.get_record(\n",
    "    FeatureGroupName=\"customer-demographics-features\",\n",
    "    RecordIdentifierValueAsString=\"cust-12345\"\n",
    ")\n",
    "\n",
    "# Features returned as key-value pairs for model input\n",
    "features = {f['FeatureName']: f['ValueAsString'] for f in response['Record']}\n",
    "# Result: {'age': '34', 'income_bracket': 'high', 'lifetime_value': '4500.00'}\n",
    "```\n",
    "\n",
    "### 10.4.2 MLOps Pipelines\n",
    "\n",
    "MLOps applies DevOps principles to machine learning: version control for models, automated training pipelines, A/B testing, and model monitoring.\n",
    "\n",
    "**SageMaker Pipelines Example:**\n",
    "\n",
    "```python\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "from sagemaker.workflow.steps import ProcessingStep, TrainingStep\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep\n",
    "from sagemaker.processing import ScriptProcessor\n",
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# Data preprocessing step\n",
    "processor = ScriptProcessor(\n",
    "    image_uri=preprocess_image_uri,\n",
    "    command=['python3'],\n",
    "    role=role,\n",
    "    instance_count=2,\n",
    "    instance_type='ml.m5.xlarge'\n",
    ")\n",
    "\n",
    "step_process = ProcessingStep(\n",
    "    name='PreprocessData',\n",
    "    processor=processor,\n",
    "    inputs=[...],\n",
    "    outputs=[...],\n",
    "    job_arguments=['--train-test-split-ratio', '0.2']\n",
    ")\n",
    "\n",
    "# Model training\n",
    "estimator = Estimator(\n",
    "    image_uri=training_image_uri,\n",
    "    role=role,\n",
    "    instance_count=1,\n",
    "    instance_type='ml.p3.2xlarge',\n",
    "    hyperparameters={'epochs': 10}\n",
    ")\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name='TrainModel',\n",
    "    estimator=estimator,\n",
    "    inputs={\n",
    "        'training': step_process.properties.ProcessingOutputConfig.Outputs['train'].S3Output.S3Uri\n",
    "    }\n",
    ")\n",
    "\n",
    "# Model evaluation\n",
    "step_eval = ProcessingStep(\n",
    "    name='EvaluateModel',\n",
    "    processor=eval_processor,\n",
    "    inputs=[...],\n",
    "    property_files=[...]\n",
    ")\n",
    "\n",
    "# Conditional registration: Only register if accuracy > 0.8\n",
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step_name='EvaluateModel',\n",
    "        property_file='evaluation',\n",
    "        json_path='classification_metrics.accuracy.value'\n",
    "    ),\n",
    "    right=0.8\n",
    ")\n",
    "\n",
    "step_cond = ConditionStep(\n",
    "    name='AccuracyCheck',\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[RegisterModelStep(...)],  # Register in Model Registry\n",
    "    else_steps=[FailStep(...)]  # Fail pipeline if accuracy too low\n",
    ")\n",
    "\n",
    "# Create and start pipeline\n",
    "pipeline = Pipeline(\n",
    "    name='CustomerChurnPipeline',\n",
    "    steps=[step_process, step_train, step_eval, step_cond]\n",
    ")\n",
    "\n",
    "pipeline.upsert(role_arn=role)\n",
    "execution = pipeline.start()\n",
    "```\n",
    "\n",
    "### 10.4.3 Vector Databases and Retrieval-Augmented Generation (RAG)\n",
    "\n",
    "With the rise of Large Language Models (LLMs), vector databases have become critical components of data architectures. They store embeddings (numerical representations of text, images, or audio) and enable semantic search.\n",
    "\n",
    "**Use Case:** RAG architecture where LLMs retrieve relevant documents from a vector store before generating responses, grounding the AI in private data.\n",
    "\n",
    "**Pinecone/OpenSearch Vector Search Example:**\n",
    "\n",
    "```python\n",
    "from langchain.embeddings import BedrockEmbeddings\n",
    "from langchain.vectorstores import Pinecone\n",
    "import pinecone\n",
    "\n",
    "# Initialize embedding model\n",
    "embeddings = BedrockEmbeddings(\n",
    "    credentials_profile_name='default',\n",
    "    region_name='us-east-1',\n",
    "    model_id='amazon.titan-embed-text-v1'\n",
    ")\n",
    "\n",
    "# Initialize vector database\n",
    "pinecone.init(api_key='key', environment='us-east-1-aws')\n",
    "index = pinecone.Index('knowledge-base')\n",
    "\n",
    "# Document ingestion pipeline\n",
    "def ingest_documents(documents):\n",
    "    \"\"\"\n",
    "    Convert documents to embeddings and store in vector DB\n",
    "    Documents come from S3/Data Lake\n",
    "    \"\"\"\n",
    "    vectorstore = Pinecone(\n",
    "        index=index,\n",
    "        embedding_function=embeddings.embed_query,\n",
    "        text_key='text'\n",
    "    )\n",
    "    \n",
    "    # Chunk documents and create embeddings\n",
    "    vectorstore.add_texts(\n",
    "        texts=[doc.page_content for doc in documents],\n",
    "        metadatas=[{\n",
    "            'source': doc.metadata['s3_path'],\n",
    "            'category': doc.metadata['doc_type'],\n",
    "            'created_at': doc.metadata['timestamp']\n",
    "        } for doc in documents]\n",
    "    )\n",
    "\n",
    "# Retrieval for RAG\n",
    "def retrieve_context(query, top_k=5):\n",
    "    \"\"\"\n",
    "    Retrieve relevant documents for LLM context\n",
    "    \"\"\"\n",
    "    vectorstore = Pinecone(\n",
    "        index=index,\n",
    "        embedding_function=embeddings.embed_query\n",
    "    )\n",
    "    \n",
    "    # Semantic search finds conceptually related content, not just keyword matches\n",
    "    docs = vectorstore.similarity_search(query, k=top_k)\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# Usage in Lambda function (from Chapter 9)\n",
    "def lambda_handler(event, context):\n",
    "    query = event['query']\n",
    "    context = retrieve_context(query)\n",
    "    \n",
    "    # Call LLM with retrieved context\n",
    "    response = bedrock.invoke_model(\n",
    "        modelId='anthropic.claude-3-sonnet',\n",
    "        body=json.dumps({\n",
    "            \"prompt\": f\"Context: {context}\\n\\nQuestion: {query}\\nAnswer:\",\n",
    "            \"max_tokens\": 500\n",
    "        })\n",
    "    )\n",
    "    return {'answer': response}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 10.5 Chapter Summary and Transition\n",
    "\n",
    "This chapter has established the foundation of modern cloud data architecture, transitioning from the compute paradigms of serverless functions to the sophisticated data ecosystems that power intelligent applications. We examined the strategic differences between data lakes (flexible, schema-on-read storage for exploration) and data warehouses (structured, high-performance analytics), and explored how Lakehouse architectures unify these approaches through open table formats like Delta Lake and Apache Iceberg.\n",
    "\n",
    "We implemented production-grade data pipelines using both ETL and ELT patterns, leveraging managed services like AWS Glue and Apache Airflow to orchestrate complex workflows without managing infrastructure. The critical shift from batch to streaming architectures was demonstrated through Apache Flink and Kinesis, enabling real-time fraud detection and operational analytics with exactly-once processing guarantees. Finally, we integrated machine learning workflows through Feature Stores, MLOps pipelines, and vector databases for AI applications.\n",
    "\n",
    "The key insight across these patterns is the decoupling of storage from computeâ€”whether in Redshift's separate scaling of storage and compute, BigQuery's serverless querying, or streaming architectures' independent scaling of producers and consumers. This separation enables cost optimization, elastic scaling, and technological flexibility.\n",
    "\n",
    "However, modern organizations rarely commit to a single cloud provider for all data workloads. Regulatory requirements, best-of-breed service selection, and risk mitigation strategies increasingly demand distributing data and applications across multiple clouds and on-premises environments. The challenge shifts from optimizing within one platform to governing, securing, and moving data across heterogeneous environments.\n",
    "\n",
    "In **Chapter 11: Hybrid and Multi-Cloud Architectures**, we will address these challenges head-on. You will learn strategies for data portability across AWS, Azure, and GCP, techniques for maintaining consistency in hybrid deployments, and governance models that provide centralized control while enabling distributed execution. We will explore Kubernetes as the common abstraction layer, data replication strategies, and the architectural patterns that allow you to leverage the best capabilities of each cloud while avoiding vendor lock-in."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
