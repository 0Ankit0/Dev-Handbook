{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 20: AI-Native Cloud Computing**\n",
    "\n",
    "## Introduction: The Intelligent Infrastructure Paradigm\n",
    "\n",
    "The previous chapters established cloud computing as a platform for scalable, resilient, and efficient infrastructure. We explored how to architect systems that handle massive throughput, maintain high availability, and optimize costs. Yet, a fundamental shift is underway that transcends traditional compute paradigms: the integration of Artificial Intelligence not merely as an application workload, but as a foundational layer of the cloud itself. We are transitioning from an era where AI was a specialized service accessed via API (\"AI-added\") to an era where AI capabilities are woven into the fabric of infrastructure, databases, and development tools (\"AI-native\").\n",
    "\n",
    "In the AI-native cloud, infrastructure becomes intelligent. Storage systems automatically tier data based on access patterns predicted by machine learning models. Security systems detect anomalies in real-time behavioral baselines rather than static rule sets. Databases optimize query execution plans using learned cost models. Development environments autocomplete code and generate infrastructure-as-code from natural language descriptions. This shift demands that cloud architects possess fluency not only in compute and networking but in model lifecycle management, vector embeddings, and prompt engineering.\n",
    "\n",
    "This chapter explores the architecture of AI-native systems. We will examine the specialized hardware infrastructure\u2014GPUs, TPUs, and custom accelerators\u2014that makes large-scale AI economically viable. We will detail the MLOps pipelines necessary to move models from experimentation to production with the same rigor applied to traditional software. We will dive into the Generative AI revolution, implementing Retrieval-Augmented Generation (RAG) architectures that combine the reasoning capabilities of Large Language Models (LLMs) with proprietary enterprise data. Finally, we will look inward at AIOps, where AI optimizes the very cloud infrastructure that hosts it, creating self-tuning, self-healing systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 20.1 The AI Infrastructure Stack: From Silicon to Services\n",
    "\n",
    "Training and deploying modern AI models\u2014particularly deep learning and Large Language Models\u2014requires computational density far exceeding general-purpose CPUs. Understanding the hardware landscape is essential for cost-effective AI architecture.\n",
    "\n",
    "### 20.1.1 Accelerated Computing Hardware\n",
    "\n",
    "**Concept Explanation:**\n",
    "CPUs are designed for serial processing with complex instruction sets optimized for diverse tasks. AI workloads, specifically matrix multiplications central to neural networks, benefit from massive parallelism. Accelerators specialize in these operations.\n",
    "\n",
    "**Hardware Taxonomy:**\n",
    "\n",
    "**1. GPUs (Graphics Processing Units):**\n",
    "Originally designed for rendering graphics, GPUs possess thousands of cores capable of parallel floating-point operations.\n",
    "- **NVIDIA A100/H100:** Industry standard for training and inference. A100 offers 312 TFLOPS (Tensor Core FP16); H100 offers ~4x performance.\n",
    "- **NVIDIA T4/L4:** Cost-effective inference GPUs for deployed models.\n",
    "- **Architecture:** CUDA cores (NVIDIA) or ROCm (AMD). The CUDA ecosystem (cuDNN, TensorRT) remains the dominant software stack.\n",
    "\n",
    "**2. TPUs (Tensor Processing Units):**\n",
    "Google's custom application-specific integrated circuits (ASICs) designed specifically for TensorFlow workloads.\n",
    "- **Architecture:** Systolic array design optimized for matrix multiplication (MXU - Matrix Multiply Unit).\n",
    "- **Use Case:** Highly efficient for large-scale training jobs (e.g., training LLMs) and high-throughput inference.\n",
    "- **Availability:** Exclusively on Google Cloud (Vertex AI).\n",
    "\n",
    "**3. Cloud-Specific ASICs:**\n",
    "- **AWS Inferentia/Trainium:** Custom silicon designed for high efficiency at lower cost than general-purpose GPUs.\n",
    "    - *Inferentia2:* High throughput, low latency inference (up to 40% better price-performance than GPU).\n",
    "    - *Trainium:* High-performance training (part of AWS EC2 Trn1 instances).\n",
    "- **Azure Maia:** Custom AI accelerator for Microsoft's AI workloads (Copilot, OpenAI models).\n",
    "\n",
    "**Selection Criteria:**\n",
    "\n",
    "| Workload Type | Recommended Hardware | Rationale |\n",
    "|---------------|----------------------|-----------|\n",
    "| LLM Training (175B+ params) | NVIDIA H100, Google TPU v5 Pod | Memory bandwidth, interconnect speed (NVLink/ICI) |\n",
    "| LLM Inference (Production) | AWS Inferentia2, NVIDIA L4, TPU v5e | Cost per token, latency |\n",
    "| Computer Vision (Training) | NVIDIA A100 80GB | Large batch sizes, high memory |\n",
    "| Computer Vision (Inference) | NVIDIA T4, CPU + OpenVINO | Moderate performance needs, edge deployment |\n",
    "\n",
    "### 20.1.2 Managed AI Infrastructure\n",
    "\n",
    "Deploying and managing GPU clusters involves complex orchestration (drivers, CUDA versions, container runtimes, distributed training frameworks). Managed services abstract this complexity.\n",
    "\n",
    "**AWS SageMaker:**\n",
    "Fully managed service covering labeling, training, tuning, and deployment.\n",
    "- *SageMaker Training:* Managed compute clusters that spin up for training jobs and tear down after, eliminating idle GPU costs.\n",
    "- *SageMaker Endpoints:* Auto-scaling model deployment with multi-variant A/B testing.\n",
    "\n",
    "**Google Vertex AI:**\n",
    "Unified platform integrating AutoML, custom training, and Model Garden (pre-trained models).\n",
    "- *Vertex AI Training:* Custom containers or pre-built containers for TensorFlow/PyTorch.\n",
    "- *Vertex AI Prediction:* Scalable serving with GPU/TPU support.\n",
    "\n",
    "**Azure Machine Learning:**\n",
    "Enterprise-focused MLOps platform with strong integration into the Microsoft ecosystem.\n",
    "- *Azure ML Compute:* Managed clusters for training and inference.\n",
    "- *Prompt Flow:* Tooling for LLM prompt engineering and evaluation.\n",
    "\n",
    "**Terraform: Provisioning AWS SageMaker for Training:**\n",
    "\n",
    "```hcl\n",
    "# IAM Role for SageMaker\n",
    "resource \"aws_iam_role\" \"sagemaker_execution\" {\n",
    "  name = \"sagemaker-execution-role\"\n",
    "\n",
    "  assume_role_policy = jsonencode({\n",
    "    Version = \"2012-10-17\"\n",
    "    Statement = [{\n",
    "      Action = \"sts:AssumeRole\"\n",
    "      Effect = \"Allow\"\n",
    "      Principal = {\n",
    "        Service = \"sagemaker.amazonaws.com\"\n",
    "      }\n",
    "    }]\n",
    "  })\n",
    "}\n",
    "\n",
    "# Attach S3 access policy\n",
    "resource \"aws_iam_role_policy_attachment\" \"sagemaker_s3\" {\n",
    "  role       = aws_iam_role.sagemaker_execution.name\n",
    "  policy_arn = \"arn:aws:iam::aws:policy/AmazonS3FullAccess\"\n",
    "}\n",
    "\n",
    "# SageMaker Training Job Definition (Reusable)\n",
    "resource \"aws_sagemaker_training_job\" \"bert_fine_tune\" {\n",
    "  name                = \"bert-fine-tune-${formatdate(\"YYYYMMDD-hhmm\", timestamp())}\"\n",
    "  role_arn            = aws_iam_role.sagemaker_execution.arn\n",
    "\n",
    "  algorithm_specification {\n",
    "    training_image = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-training:2.0.0-transformers4.28.1-gpu-py310-cu118-ubuntu20.04\"\n",
    "    training_input_mode = \"File\"\n",
    "  }\n",
    "\n",
    "  input_data_config {\n",
    "    channel_name = \"train\"\n",
    "    data_source {\n",
    "      s3_data_source {\n",
    "        s3_data_type = \"S3Prefix\"\n",
    "        s3_uri       = \"s3://${var.training_data_bucket}/train/\"\n",
    "        s3_data_distribution_type = \"FullyReplicated\"\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "\n",
    "  output_data_config {\n",
    "    s3_output_path = \"s3://${var.model_output_bucket}/models/\"\n",
    "  }\n",
    "\n",
    "  resource_config {\n",
    "    instance_type     = \"ml.p3.2xlarge\"  # Single V100 GPU\n",
    "    instance_count    = 1\n",
    "    volume_size_in_gb = 30\n",
    "  }\n",
    "\n",
    "  stopping_condition {\n",
    "    max_runtime_in_seconds = 86400  # 24 hours\n",
    "  }\n",
    "\n",
    "  hyper_parameters = {\n",
    "    \"epochs\"        = \"3\"\n",
    "    \"batch_size\"    = \"32\"\n",
    "    \"learning_rate\" = \"5e-5\"\n",
    "    \"model_name\"    = \"bert-base-uncased\"\n",
    "  }\n",
    "\n",
    "  environment = {\n",
    "    \"HF_TOKEN\" = var.huggingface_token\n",
    "  }\n",
    "\n",
    "  tags = {\n",
    "    Project = \"SentimentAnalysis\"\n",
    "    CostCenter = \"AI-R&D\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Model Deployment (Real-time Endpoint)\n",
    "resource \"aws_sagemaker_model\" \"inference_model\" {\n",
    "  name               = \"bert-sentiment-v1\"\n",
    "  execution_role_arn = aws_iam_role.sagemaker_execution.arn\n",
    "\n",
    "  primary_container {\n",
    "    image          = \"763104351884.dkr.ecr.us-east-1.amazonaws.com/huggingface-pytorch-inference:2.0.0-transformers4.28.1-cpu-py310-ubuntu20.04\"\n",
    "    model_data_url = \"s3://${var.model_output_bucket}/models/${aws_sagemaker_training_job.bert_fine_tune.name}/output/model.tar.gz\"\n",
    "    environment = {\n",
    "      \"MMS_MAX_RESPONSE_SIZE\" = \"10000000\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "resource \"aws_sagemaker_endpoint_configuration\" \"prod\" {\n",
    "  name = \"bert-sentiment-config-v1\"\n",
    "\n",
    "  production_variants {\n",
    "    variant_name           = \"variant-1\"\n",
    "    model_name             = aws_sagemaker_model.inference_model.name\n",
    "    initial_instance_count = 2\n",
    "    instance_type          = \"ml.m5.xlarge\"  # CPU for low-traffic inference\n",
    "    # Use ml.inf1.xlarge for Inferentia cost savings\n",
    "  }\n",
    "}\n",
    "\n",
    "resource \"aws_sagemaker_endpoint\" \"prod\" {\n",
    "  name                 = \"bert-sentiment-endpoint\"\n",
    "  endpoint_config_name = aws_sagemaker_endpoint_configuration.prod.name\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 20.2 MLOps: The Machine Learning Lifecycle Pipeline\n",
    "\n",
    "Software engineering has CI/CD; machine learning requires CI/CD/CT (Continuous Training). Models drift, data changes, and models must be retrained and redeployed automatically.\n",
    "\n",
    "### 20.2.1 The MLOps Maturity Model\n",
    "\n",
    "**Level 0: Manual Process:**\n",
    "- Script-based experimentation in notebooks.\n",
    "- Manual deployment of models.\n",
    "- No monitoring; drift detected only when performance degrades visibly.\n",
    "\n",
    "**Level 1: ML Pipeline Automation:**\n",
    "- Automated training pipelines triggered by data arrival or schedule.\n",
    "- Centralized feature store for consistency.\n",
    "- Model registry for versioning.\n",
    "- Manual deployment approval.\n",
    "\n",
    "**Level 2: CI/CD/CT Automation:**\n",
    "- Full CI/CD for model code and infrastructure.\n",
    "- Continuous training triggered by performance metrics (accuracy drift) or data drift detection.\n",
    "- Automated A/B testing and canary deployments.\n",
    "- Comprehensive model monitoring (feature attribution drift, inference latency).\n",
    "\n",
    "### 20.2.2 Architecture: End-to-End MLOps Pipeline\n",
    "\n",
    "**Components:**\n",
    "1. **Data Ingestion:** Raw data ingestion from sources (S3, databases, streaming).\n",
    "2. **Feature Engineering:** Transformation pipelines (Spark, Pandas, dbt).\n",
    "3. **Feature Store:** Centralized repository for features (Feast, SageMaker Feature Store, Vertex Feature Store).\n",
    "4. **Training Pipeline:** Distributed training jobs.\n",
    "5. **Model Registry:** Versioned artifact storage with metadata (MLflow, SageMaker Model Registry).\n",
    "6. **Serving Infrastructure:** Real-time endpoints or batch inference.\n",
    "7. **Monitoring:** Performance tracking, drift detection.\n",
    "\n",
    "**Implementation: Kubeflow Pipelines on Kubernetes:**\n",
    "\n",
    "```python\n",
    "# Kubeflow Pipeline definition for Continuous Training\n",
    "from kfp import dsl\n",
    "from kfp.v2 import compiler\n",
    "from kfp.v2.dsl import component, Output, Model, Metrics\n",
    "\n",
    "@component(base_image=\"python:3.9\", packages_to_install=[\"pandas\", \"scikit-learn\", \"joblib\"])\n",
    "def preprocess_data(\n",
    "    raw_data_path: str,\n",
    "    processed_data_path: Output[dsl.Dataset]\n",
    "):\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    \n",
    "    df = pd.read_csv(raw_data_path)\n",
    "    # Preprocessing logic...\n",
    "    df = df.dropna()\n",
    "    \n",
    "    train, test = train_test_split(df, test_size=0.2)\n",
    "    \n",
    "    train.to_csv(f\"{processed_data_path.path}/train.csv\", index=False)\n",
    "    test.to_csv(f\"{processed_data_path.path}/test.csv\", index=False)\n",
    "\n",
    "@component(base_image=\"python:3.9\", packages_to_install=[\"scikit-learn\", \"joblib\"])\n",
    "def train_model(\n",
    "    dataset: dsl.Dataset,\n",
    "    model: Output[Model],\n",
    "    metrics: Output[Metrics]\n",
    "):\n",
    "    import pandas as pd\n",
    "    import joblib\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "    from sklearn.metrics import accuracy_score\n",
    "    \n",
    "    train_df = pd.read_csv(f\"{dataset.path}/train.csv\")\n",
    "    test_df = pd.read_csv(f\"{dataset.path}/test.csv\")\n",
    "    \n",
    "    X_train, y_train = train_df.drop(\"target\", axis=1), train_df[\"target\"]\n",
    "    X_test, y_test = test_df.drop(\"target\", axis=1), test_df[\"target\"]\n",
    "    \n",
    "    clf = RandomForestClassifier(n_estimators=100)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    predictions = clf.predict(X_test)\n",
    "    acc = accuracy_score(y_test, predictions)\n",
    "    \n",
    "    joblib.dump(clf, model.path)\n",
    "    \n",
    "    metrics.log_metric(\"accuracy\", acc)\n",
    "\n",
    "@component(base_image=\"python:3.9\")\n",
    "def deploy_model(\n",
    "    model: dsl.Model,\n",
    "    project_id: str,\n",
    "    region: str\n",
    "):\n",
    "    # Logic to push model to Vertex AI Endpoint or SageMaker\n",
    "    import google.cloud.aiplatform as aip\n",
    "    \n",
    "    aip.init(project=project_id, location=region)\n",
    "    \n",
    "    uploaded_model = aip.Model.upload(\n",
    "        display_name=\"churn-predictor\",\n",
    "        artifact_uri=model.uri,\n",
    "        serving_container_image_uri=\"us-docker.pkg.dev/vertex-ai/prediction/sklearn-cpu.1-0:latest\"\n",
    "    )\n",
    "    \n",
    "    endpoint = aip.Endpoint.create(display_name=\"churn-endpoint\")\n",
    "    deployed_model = uploaded_model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        machine_type=\"n1-standard-4\"\n",
    "    )\n",
    "\n",
    "@dsl.pipeline(\n",
    "    name=\"continuous-training-pipeline\",\n",
    "    description=\"Retrains model on new data\"\n",
    ")\n",
    "def ml_pipeline(raw_data_path: str, project_id: str, region: str):\n",
    "    preprocess_task = preprocess_data(raw_data_path=raw_data_path)\n",
    "    \n",
    "    train_task = train_model(dataset=preprocess_task.outputs[\"processed_data_path\"])\n",
    "    \n",
    "    # Conditional deployment if accuracy > threshold\n",
    "    with dsl.If(train_task.outputs[\"metrics\"].accuracy > 0.85):\n",
    "        deploy_task = deploy_model(\n",
    "            model=train_task.outputs[\"model\"],\n",
    "            project_id=project_id,\n",
    "            region=region\n",
    "        )\n",
    "\n",
    "# Compile the pipeline\n",
    "compiler.Compiler().compile(\n",
    "    pipeline_func=ml_pipeline,\n",
    "    package_path=\"ml_pipeline.json\"\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 20.3 Generative AI in the Cloud\n",
    "\n",
    "The rise of Large Language Models (LLMs) and diffusion models represents a paradigm shift in AI capability. Cloud providers are racing to offer \"Model-as-a-Service\" to lower the barrier to entry.\n",
    "\n",
    "### 20.3.1 Managed Foundation Models\n",
    "\n",
    "**Concept Explanation:**\n",
    "Foundation models (FMs) are large-scale models trained on vast datasets that can be adapted to many tasks. Rather than training from scratch, organizations consume these models via API or fine-tune them on proprietary data.\n",
    "\n",
    "**Offerings:**\n",
    "- **AWS Bedrock:** Access to models from AI21, Anthropic, Cohere, Meta (Llama), and Amazon Titan via single API.\n",
    "- **Google Vertex AI Model Garden:** Gemini, Llama, Mistral, and open-source models.\n",
    "- **Azure OpenAI Service:** GPT-4, GPT-3.5-turbo, DALL-E, Whisper hosted on Azure infrastructure.\n",
    "\n",
    "**Implementation: AWS Bedrock for Text Generation:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "\n",
    "# Bedrock client\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "def generate_product_description(product_name, features):\n",
    "    \"\"\"\n",
    "    Generate marketing copy using Claude 3 on Bedrock\n",
    "    \"\"\"\n",
    "    prompt = f\"\"\"\n",
    "    Human: You are a marketing copywriter. Write a compelling product description for the following product.\n",
    "    \n",
    "    Product Name: {product_name}\n",
    "    Key Features: {', '.join(features)}\n",
    "    \n",
    "    Write a 100-word description that highlights the benefits and appeals to tech-savvy consumers.\n",
    "    \n",
    "    Assistant:\n",
    "    \"\"\"\n",
    "    \n",
    "    # Claude 3 Sonnet request body\n",
    "    request_body = {\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 300,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": [{\"type\": \"text\", \"text\": prompt}]\n",
    "            }\n",
    "        ],\n",
    "        \"temperature\": 0.7,\n",
    "        \"top_p\": 0.9\n",
    "    }\n",
    "    \n",
    "    response = bedrock_runtime.invoke_model(\n",
    "        modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "        contentType='application/json',\n",
    "        accept='application/json',\n",
    "        body=json.dumps(request_body)\n",
    "    )\n",
    "    \n",
    "    response_body = json.loads(response['body'].read())\n",
    "    \n",
    "    return response_body['content'][0]['text']\n",
    "\n",
    "# Usage\n",
    "description = generate_product_description(\n",
    "    \"CloudScale Pro\",\n",
    "    [\"Auto-scaling storage\", \"AI-powered optimization\", \"99.999% durability\"]\n",
    ")\n",
    "print(description)\n",
    "```\n",
    "\n",
    "### 20.3.2 RAG (Retrieval-Augmented Generation)\n",
    "\n",
    "**Concept Explanation:**\n",
    "LLMs have knowledge cutoffs (training date) and lack access to private enterprise data. RAG combines information retrieval with text generation:\n",
    "1. User query is converted to a vector embedding.\n",
    "2. Vector database retrieves relevant documents from the knowledge base.\n",
    "3. Retrieved documents are injected into the LLM prompt as context.\n",
    "4. LLM generates an answer grounded in the retrieved data.\n",
    "\n",
    "**Architecture Components:**\n",
    "- **Embedding Model:** Converts text to vectors (e.g., `text-embedding-3-small`, Cohere Embed).\n",
    "- **Vector Database:** Stores and indexes embeddings (Pinecone, Weaviate, pgvector, OpenSearch).\n",
    "- **LLM:** Generates the final response.\n",
    "\n",
    "**Implementation: RAG with OpenSearch and LangChain:**\n",
    "\n",
    "```python\n",
    "from langchain_community.vectorstores import OpenSearchVectorSearch\n",
    "from langchain_openai import OpenAIEmbeddings, ChatOpenAI\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.chains import RetrievalQA\n",
    "from langchain_community.document_loaders import S3FileLoader\n",
    "import boto3\n",
    "import os\n",
    "\n",
    "# Configuration\n",
    "OPENSEARCH_URL = \"https://your-domain.us-east-1.aoss.amazonaws.com\"\n",
    "INDEX_NAME = \"company-knowledge-base\"\n",
    "OPENAI_API_KEY = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "class RAGSystem:\n",
    "    def __init__(self):\n",
    "        # Embedding model (converts text to vectors)\n",
    "        self.embeddings = OpenAIEmbeddings(\n",
    "            model=\"text-embedding-3-small\",\n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "        \n",
    "        # Vector store (OpenSearch Serverless)\n",
    "        self.vectorstore = OpenSearchVectorSearch(\n",
    "            opensearch_url=OPENSEARCH_URL,\n",
    "            index_name=INDEX_NAME,\n",
    "            embedding_function=self.embeddings,\n",
    "            use_ssl=True,\n",
    "            verify_certs=True\n",
    "        )\n",
    "        \n",
    "        # LLM for generation\n",
    "        self.llm = ChatOpenAI(\n",
    "            model_name=\"gpt-4-turbo\",\n",
    "            temperature=0,\n",
    "            openai_api_key=OPENAI_API_KEY\n",
    "        )\n",
    "        \n",
    "        # Retrieval chain\n",
    "        self.qa_chain = RetrievalQA.from_chain_type(\n",
    "            llm=self.llm,\n",
    "            chain_type=\"stuff\",  # \"Stuff\" all retrieved docs into prompt\n",
    "            retriever=self.vectorstore.as_retriever(\n",
    "                search_type=\"similarity\",\n",
    "                search_kwargs={\"k\": 5}  # Retrieve top 5 documents\n",
    "            ),\n",
    "            return_source_documents=True\n",
    "        )\n",
    "    \n",
    "    def ingest_documents(self, bucket, key):\n",
    "        \"\"\"\n",
    "        Load document from S3, split into chunks, and embed into vector store\n",
    "        \"\"\"\n",
    "        # 1. Load document\n",
    "        loader = S3FileLoader(bucket, key)\n",
    "        documents = loader.load()\n",
    "        \n",
    "        # 2. Split into chunks (necessary for embedding limits and retrieval precision)\n",
    "        text_splitter = RecursiveCharacterTextSplitter(\n",
    "            chunk_size=1000,\n",
    "            chunk_overlap=200,\n",
    "            length_function=len\n",
    "        )\n",
    "        chunks = text_splitter.split_documents(documents)\n",
    "        \n",
    "        # 3. Embed and store (batch processing)\n",
    "        self.vectorstore.add_documents(chunks)\n",
    "        \n",
    "        return len(chunks)\n",
    "    \n",
    "    def query(self, question):\n",
    "        \"\"\"\n",
    "        Retrieve relevant context and generate answer\n",
    "        \"\"\"\n",
    "        response = self.qa_chain.invoke({\"query\": question})\n",
    "        \n",
    "        return {\n",
    "            \"answer\": response[\"result\"],\n",
    "            \"sources\": [\n",
    "                {\n",
    "                    \"content\": doc.page_content[:200] + \"...\",\n",
    "                    \"metadata\": doc.metadata\n",
    "                }\n",
    "                for doc in response[\"source_documents\"]\n",
    "            ]\n",
    "        }\n",
    "\n",
    "# Usage\n",
    "rag = RAGSystem()\n",
    "\n",
    "# Ingest internal documentation (run once or as batch job)\n",
    "# rag.ingest_documents(\"company-docs-bucket\", \"engineering/playbook.pdf\")\n",
    "\n",
    "# Query the system\n",
    "result = rag.query(\"What is our incident response process for database outages?\")\n",
    "print(f\"Answer: {result['answer']}\")\n",
    "print(f\"\\nSources: {result['sources']}\")\n",
    "```\n",
    "\n",
    "**Terraform: OpenSearch Serverless for Vector Search:**\n",
    "\n",
    "```hcl\n",
    "# OpenSearch Serverless Collection for Vector Database\n",
    "resource \"aws_opensearchserverless_collection\" \"knowledge_base\" {\n",
    "  name = \"company-knowledge-base\"\n",
    "  type = \"VECTORSEARCH\"  # Optimized for vector search workloads\n",
    "  \n",
    "  description = \"Vector database for RAG applications\"\n",
    "}\n",
    "\n",
    "# Encryption security policy\n",
    "resource \"aws_opensearchserverless_security_policy\" \"encryption\" {\n",
    "  name = \"knowledge-base-encryption\"\n",
    "  type = \"encryption\"\n",
    "  \n",
    "  policy = jsonencode({\n",
    "    Rules = [\n",
    "      {\n",
    "        Resource = [\"collection/company-knowledge-base\"],\n",
    "        ResourceType = \"collection\"\n",
    "      }\n",
    "    ],\n",
    "    AWSOwnedKey = true\n",
    "  })\n",
    "}\n",
    "\n",
    "# Network security policy (VPC access)\n",
    "resource \"aws_opensearchserverless_security_policy\" \"network\" {\n",
    "  name = \"knowledge-base-network\"\n",
    "  type = \"network\"\n",
    "  \n",
    "  policy = jsonencode([\n",
    "    {\n",
    "      Rules = [\n",
    "        {\n",
    "          Resource = [\"collection/company-knowledge-base\"],\n",
    "          ResourceType = \"collection\"\n",
    "        }\n",
    "      ],\n",
    "      AllowFromPublic = true  # Set to false for VPC-only access\n",
    "    }\n",
    "  ])\n",
    "}\n",
    "\n",
    "# Data access policy (IAM permissions)\n",
    "resource \"aws_opensearchserverless_access_policy\" \"data_access\" {\n",
    "  name = \"knowledge-base-access\"\n",
    "  type = \"data\"\n",
    "  \n",
    "  policy = jsonencode([\n",
    "    {\n",
    "      Rules = [\n",
    "        {\n",
    "          Resource = [\"collection/company-knowledge-base\"],\n",
    "          Permission = [\n",
    "            \"aoss:CreateCollectionItems\",\n",
    "            \"aoss:DeleteCollectionItems\",\n",
    "            \"aoss:UpdateCollectionItems\",\n",
    "            \"aoss:DescribeCollectionItems\"\n",
    "          ],\n",
    "          ResourceType = \"collection\"\n",
    "        },\n",
    "        {\n",
    "          Resource = [\"index/company-knowledge-base/*\"],\n",
    "          Permission = [\n",
    "            \"aoss:CreateIndex\",\n",
    "            \"aoss:DeleteIndex\",\n",
    "            \"aoss:UpdateIndex\",\n",
    "            \"aoss:DescribeIndex\",\n",
    "            \"aoss:ReadDocument\",\n",
    "            \"aoss:WriteDocument\"\n",
    "          ],\n",
    "          ResourceType = \"index\"\n",
    "        }\n",
    "      ],\n",
    "      Principal = [aws_iam_role.lambda_execution_role.arn]\n",
    "    }\n",
    "  ])\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 20.4 AI-Enhanced Operations (AIOps)\n",
    "\n",
    "AI-native cloud computing includes using AI to optimize the infrastructure itself. AIOps applies machine learning to IT operations data (logs, metrics, traces) to detect anomalies, predict failures, and automate remediation.\n",
    "\n",
    "### 20.4.1 Intelligent Observability\n",
    "\n",
    "**Anomaly Detection:**\n",
    "CloudWatch and other platforms use ML models to learn normal system behavior and flag deviations without static thresholds.\n",
    "\n",
    "**Log Analysis:**\n",
    "NLP models cluster similar log messages, identifying patterns and reducing alert fatigue.\n",
    "\n",
    "**Code Implementation: CloudWatch Anomaly Detector:**\n",
    "\n",
    "```hcl\n",
    "# CloudWatch Anomaly Detector for EC2 CPU\n",
    "resource \"aws_cloudwatch_metric_stream\" \"anomaly_stream\" {\n",
    "  name          = \"anomaly-detection-stream\"\n",
    "  role_arn      = aws_iam_role.cloudwatch_stream.arn\n",
    "  firehose_arn  = aws_kinesis_firehose_delivery_stream.metrics.arn\n",
    "  output_format = \"json\"\n",
    "}\n",
    "\n",
    "resource \"aws_cloudwatch_anomaly_detector\" \"high_cpu\" {\n",
    "  metric_name           = \"CPUUtilization\"\n",
    "  namespace            = \"AWS/EC2\"\n",
    "  \n",
    "  configuration {\n",
    "    excluded_time_periods {\n",
    "      start = \"2026-01-01T00:00:00Z\"\n",
    "      end   = \"2026-01-02T00:00:00Z\"\n",
    "    }\n",
    "    metric_timezone = \"UTC\"\n",
    "  }\n",
    "  \n",
    "  # Optional: Statistic for anomaly detection\n",
    "  stat = \"Average\"\n",
    "}\n",
    "\n",
    "resource \"aws_cloudwatch_metric_alarm\" \"cpu_anomaly_alarm\" {\n",
    "  alarm_name          = \"high-cpu-anomaly\"\n",
    "  comparison_operator = \"GreaterThanUpperThreshold\"\n",
    "  evaluation_periods  = 2\n",
    "  threshold_metric_id = \"e1\"\n",
    "  \n",
    "  metric_query {\n",
    "    id          = \"e1\"\n",
    "    expression  = \"ANOMALY_DETECTION_BAND(m1)\"\n",
    "    label       = \"CPUUtilization (Expected)\"\n",
    "    return_data = true\n",
    "  }\n",
    "  \n",
    "  metric_query {\n",
    "    id          = \"m1\"\n",
    "    return_data = true\n",
    "    metric {\n",
    "      metric_name = \"CPUUtilization\"\n",
    "      namespace   = \"AWS/EC2\"\n",
    "      period      = 300\n",
    "      stat        = \"Average\"\n",
    "      dimensions = {\n",
    "        InstanceId = aws_instance.web.id\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  alarm_description = \"This alarm triggers when CPU exceeds expected anomaly band\"\n",
    "  alarm_actions     = [aws_sns_topic.alerts.arn]\n",
    "}\n",
    "```\n",
    "\n",
    "### 20.4.2 Predictive Scaling\n",
    "\n",
    "Traditional auto-scaling reacts to current load. Predictive scaling uses ML on historical data to forecast future load, scaling out *before* the traffic spike arrives.\n",
    "\n",
    "**Implementation: AWS Predictive Scaling:**\n",
    "\n",
    "```hcl\n",
    "resource \"aws_autoscaling_policy\" \"predictive_scale\" {\n",
    "  name                   = \"predictive-scaling-policy\"\n",
    "  autoscaling_group_name = aws_autoscaling_group.web.name\n",
    "  policy_type            = \"PredictiveScaling\"\n",
    "  \n",
    "  predictive_scaling_configuration {\n",
    "    metric_specification {\n",
    "      target_value = 50.0  # Target CPU utilization\n",
    "      \n",
    "      predefined_metric_pair_specification {\n",
    "        predefined_metric_type = \"ASGCPUUtilization\"\n",
    "      }\n",
    "    }\n",
    "    \n",
    "    mode = \"ForecastAndScale\"  # \"ForecastOnly\" for monitoring\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary and Transition to Chapter 21\n",
    "\n",
    "This chapter explored the transformation of cloud computing through the lens of artificial intelligence, moving beyond AI as a discrete workload to AI as a foundational infrastructure capability. We examined the hardware underpinnings of modern AI\u2014from the parallel processing power of GPUs (NVIDIA A100/H100) to the matrix-optimized efficiency of TPUs and the custom economics of AWS Inferentia\u2014establishing the criteria for selecting appropriate accelerators for training versus inference workloads.\n",
    "\n",
    "The transition from ad-hoc model training to industrialized ML engineering demands MLOps pipelines that implement CI/CD/CT (Continuous Integration, Delivery, and Training). We architected end-to-end pipelines using Kubeflow, demonstrating how to automate data preprocessing, model training, evaluation, and deployment with the same rigor applied to traditional software. The distinction between manual experimentation (Level 0) and fully automated CT (Level 2) provides a maturity roadmap for organizations scaling AI initiatives.\n",
    "\n",
    "Generative AI represents the current frontier, and we implemented Retrieval-Augmented Generation (RAG) architectures that ground Large Language Models in enterprise truth. By combining vector databases (OpenSearch, pgvector) with embedding models and managed foundation models (AWS Bedrock, Azure OpenAI), we enabled question-answering systems that reference internal documentation rather than hallucinating answers.\n",
    "\n",
    "Finally, we turned the lens inward to AIOps, where machine learning optimizes the cloud itself\u2014detecting anomalies in CloudWatch metrics without static thresholds and predicting scaling requirements before traffic spikes materialize. This self-optimizing infrastructure hints at the future of autonomous computing.\n",
    "\n",
    "As AI workloads grow larger and more complex, they encounter the physical limits of classical computing. Training the largest models today consumes megawatts of power and weeks of compute time. A new computational paradigm looms on the horizon, promising to solve problems intractable for classical systems. In **Chapter 21: Quantum Computing in the Cloud**, we will step into the probabilistic realm of qubits and superposition. You will learn the fundamental concepts of quantum mechanics as applied to computation\u2014superposition, entanglement, and interference\u2014without requiring a physics degree. We will explore how cloud providers (AWS Braket, Azure Quantum, Google Cirq) democratize access to quantum processors, the types of problems suitable for quantum speedup (optimization, simulation, cryptography), and the hybrid architectures that will define the near-term NISQ (Noisy Intermediate-Scale Quantum) era, bridging classical cloud infrastructure with the quantum future."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='19. edge_computing.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='21. quantum_computing_in_the_cloud.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}