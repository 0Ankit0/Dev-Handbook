{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 16: Cloud Financial Management (FinOps) and Governance**\n",
    "\n",
    "## Introduction: Economic Optimization of Cloud Investments\n",
    "\n",
    "The cloud's promise of infinite scalability carries a corresponding risk: infinite costs. While preceding chapters focused on technical implementation of secure, scalable architectures, organizations rapidly discover that architectural elegance means little if economic sustainability is compromised. The shift from capital expenditure (CapEx) models\u2014where organizations purchased physical infrastructure for multi-year depreciation cycles\u2014to operational expenditure (OpEx) models\u2014where every API call, gigabyte transferred, and compute minute incurs marginal cost\u2014requires new disciplines.\n",
    "\n",
    "FinOps (Cloud Financial Management) represents the intersection of financial accountability, technical optimization, and business value. It is not merely \"cost cutting\" but rather \"cost optimization\"\u2014maximizing the business value per dollar spent on cloud resources. This chapter addresses the economic dimension of the architectures we've built: how to attribute costs to business units accurately, optimize compute spend through purchasing strategies and right-sizing, implement intelligent storage tiering that balances accessibility with economics, govern multi-cloud spending to prevent waste, and establish automated guardrails that prevent budget overruns without impeding innovation.\n",
    "\n",
    "As cloud environments scale to millions of dollars in annual spending, FinOps transitions from spreadsheet tracking to programmatic governance. We will implement tagging strategies that enable precise cost allocation, automated policies that stop non-compliant resources, purchasing strategies that reduce compute costs by up to 72%, and data lifecycle management that moves cold data to archive storage costing fractions of a cent per gigabyte. These practices ensure that the sophisticated architectures built in previous chapters deliver business value economically.\n",
    "\n",
    "---\n",
    "\n",
    "## 16.1 The FinOps Framework: Inform, Optimize, Operate\n",
    "\n",
    "The FinOps Foundation (a Linux Foundation project) defines a framework with three iterative phases that guide cloud financial management maturity.\n",
    "\n",
    "### 16.1.1 The Three Phases of FinOps\n",
    "\n",
    "**Inform:** Visibility and allocation\u2014understanding what is being spent, by whom, and for what purpose. Without accurate visibility, optimization is impossible. This phase implements tagging strategies, cost allocation tags, and dashboarding.\n",
    "\n",
    "**Optimize:** Rate optimization (paying less per unit) and usage optimization (using fewer units). This includes Reserved Instances, Savings Plans, Spot instances, rightsizing, and eliminating waste.\n",
    "\n",
    "**Operate:** Continuous governance and automation\u2014ensuring that optimized states persist through policy enforcement, budget alerts, and automated remediation of cost anomalies.\n",
    "\n",
    "### 16.1.2 Organizational Alignment\n",
    "\n",
    "FinOps requires collaboration between Engineering (who provision resources), Finance (who manage budgets), and Business Units (who consume services). This \"FinOps Team\" breaks down silos where Engineering optimizes for speed, Finance for cost, and Business for features.\n",
    "\n",
    "**Tagging Strategy as the Foundation:**\n",
    "\n",
    "Effective cost allocation requires a comprehensive tagging strategy implemented through infrastructure as code and enforced through policy.\n",
    "\n",
    "```hcl\n",
    "# Terraform module for mandatory cost allocation tags\n",
    "variable \"mandatory_tags\" {\n",
    "  type = map(string)\n",
    "  default = {\n",
    "    # Business Context\n",
    "    BusinessUnit    = \"Engineering\"      # Who pays the bill\n",
    "    CostCenter      = \"CC-12345\"         # Accounting code\n",
    "    Project         = \"PlatformMigration\"  # Initiative attribution\n",
    "    \n",
    "    # Technical Context\n",
    "    Environment     = \"Production\"       # Prod, Staging, Dev\n",
    "    Application     = \"PaymentService\"     # Service name\n",
    "    Component       = \"Database\"           # App, DB, Cache, etc.\n",
    "    \n",
    "    # Operational Context\n",
    "    DataClassification = \"Confidential\"    # Security-driven retention\n",
    "    Criticality      = \"Tier1\"           # Business criticality\n",
    "    Automation       = \"Terraform\"         # Infrastructure source\n",
    "  }\n",
    "}\n",
    "\n",
    "# AWS Provider default tags - automatically applied to all resources\n",
    "provider \"aws\" {\n",
    "  region = \"us-east-1\"\n",
    "  \n",
    "  default_tags {\n",
    "    tags = var.mandatory_tags\n",
    "  }\n",
    "}\n",
    "\n",
    "# Azure Policy for mandatory tagging\n",
    "resource \"azurerm_policy_definition\" \"mandatory_tags\" {\n",
    "  name         = \"mandatory-cost-tags\"\n",
    "  policy_type  = \"Custom\"\n",
    "  mode         = \"Indexed\"\n",
    "  display_name = \"Require Cost Allocation Tags\"\n",
    "  \n",
    "  policy_rule = jsonencode({\n",
    "    if = {\n",
    "      not = {\n",
    "        field = \"tags['BusinessUnit']\"\n",
    "        exists = \"true\"\n",
    "      }\n",
    "    }\n",
    "    then = {\n",
    "      effect = \"deny\"\n",
    "    }\n",
    "  })\n",
    "}\n",
    "\n",
    "# GCP Organization Policy Constraint\n",
    "resource \"google_org_policy_policy\" \"mandatory_labels\" {\n",
    "  name   = \"projects/${var.project_id}/policies/gcp.resourceLabels\"\n",
    "  parent = \"projects/${var.project_id}\"\n",
    "  \n",
    "  spec {\n",
    "    rules {\n",
    "      enforce = true\n",
    "      values {\n",
    "        allowed_values = [\"BusinessUnit\", \"CostCenter\", \"Environment\"]\n",
    "      }\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Tagging Governance Automation:**\n",
    "\n",
    "```python\n",
    "# Lambda function to enforce tagging compliance\n",
    "import boto3\n",
    "import json\n",
    "import os\n",
    "\n",
    "REQUIRED_TAGS = ['BusinessUnit', 'CostCenter', 'Environment', 'Application']\n",
    "AUTOMATION_TAG = 'Automation'\n",
    "\n",
    "def lambda_handler(event, context):\n",
    "    \"\"\"\n",
    "    Triggered by CloudTrail API calls for resource creation\n",
    "    Enforces mandatory tags or applies default tags\n",
    "    \"\"\"\n",
    "    resource_arn = event['detail']['responseElements'].get('resourceArn') or \\\n",
    "                   event['detail']['responseElements'].get('functionArn')\n",
    "    \n",
    "    if not resource_arn:\n",
    "        return {'status': 'no_resource'}\n",
    "    \n",
    "    ec2 = boto3.client('ec2')\n",
    "    resource_tagging = boto3.client('resourcegroupstaggingapi')\n",
    "    \n",
    "    # Check existing tags\n",
    "    try:\n",
    "        response = resource_tagging.get_resources(\n",
    "            ResourceARNList=[resource_arn]\n",
    "        )\n",
    "        existing_tags = {tag['Key']: tag['Value'] \n",
    "                        for tag in response['ResourceTagMappingList'][0].get('Tags', [])}\n",
    "    except Exception:\n",
    "        existing_tags = {}\n",
    "    \n",
    "    # Determine missing tags\n",
    "    missing_tags = [tag for tag in REQUIRED_TAGS if tag not in existing_tags]\n",
    "    \n",
    "    if missing_tags:\n",
    "        # Option 1: Apply default tags from CloudTrail user identity\n",
    "        default_tags = {\n",
    "            'BusinessUnit': infer_business_unit(event['detail']['userIdentity']),\n",
    "            'Environment': 'Unspecified',\n",
    "            'Automation': 'AutoRemediated'\n",
    "        }\n",
    "        \n",
    "        # Merge with existing\n",
    "        new_tags = {**default_tags, **existing_tags}\n",
    "        \n",
    "        resource_tagging.tag_resources(\n",
    "            ResourceARNList=[resource_arn],\n",
    "            Tags=new_tags\n",
    "        )\n",
    "        \n",
    "        # Notify for manual correction\n",
    "        sns = boto3.client('sns')\n",
    "        sns.publish(\n",
    "            TopicArn=os.environ['TAGGING_ALERT_TOPIC'],\n",
    "            Subject='Auto-tagged Non-Compliant Resource',\n",
    "            Message=json.dumps({\n",
    "                'resource': resource_arn,\n",
    "                'missing_tags': missing_tags,\n",
    "                'applied_defaults': default_tags,\n",
    "                'user': event['detail']['userIdentity']['arn']\n",
    "            })\n",
    "        )\n",
    "        \n",
    "        return {'status': 'remediated', 'applied_tags': new_tags}\n",
    "    \n",
    "    return {'status': 'compliant'}\n",
    "\n",
    "def infer_business_unit(identity):\n",
    "    \"\"\"Infer business unit from IAM user/role path\"\"\"\n",
    "    arn = identity.get('arn', '')\n",
    "    if '/engineering/' in arn:\n",
    "        return 'Engineering'\n",
    "    elif '/marketing/' in arn:\n",
    "        return 'Marketing'\n",
    "    return 'Unallocated'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 16.2 Cost Allocation and Showback/Chargeback\n",
    "\n",
    "Accurate cost allocation requires not just resource tagging, but also handling shared costs (common databases, load balancers, Kubernetes clusters) and amortizing upfront commitments (Reserved Instances, Savings Plans) across consuming teams.\n",
    "\n",
    "### 16.2.1 Shared Cost Allocation\n",
    "\n",
    "**Kubernetes Cost Allocation:**\n",
    "In containerized environments, multiple teams share underlying nodes. Cost allocation requires pod-level resource tracking.\n",
    "\n",
    "```yaml\n",
    "# Kubecost / OpenCost implementation for Kubernetes cost allocation\n",
    "apiVersion: v1\n",
    "kind: ConfigMap\n",
    "metadata:\n",
    "  name: cost-allocation-config\n",
    "  namespace: kubecost\n",
    "data:\n",
    "  config.yaml: |\n",
    "    # Allocation strategy for shared cluster costs\n",
    "    allocation:\n",
    "      # CPU/RAM allocated by pod requests vs. actual usage\n",
    "      cpuAllocationMethod: \"request\"  # or \"usage\"\n",
    "      memoryAllocationMethod: \"request\"\n",
    "      \n",
    "      # Shared overhead allocation (kube-system, monitoring)\n",
    "      sharedOverhead:\n",
    "        enabled: true\n",
    "        namespaces:\n",
    "          - kube-system\n",
    "          - monitoring\n",
    "          - ingress-nginx\n",
    "        allocationStrategy: \"weighted\"  # Split by namespace resource consumption\n",
    "      \n",
    "      # Idle cost allocation (unused node capacity)\n",
    "      idle:\n",
    "        enabled: true\n",
    "        strategy: \"share\"  # Distribute idle costs to consuming namespaces\n",
    "      \n",
    "      # Network costs (service mesh, ingress)\n",
    "      network:\n",
    "        enabled: true\n",
    "        ingressClass: \"nginx\"\n",
    "        serviceMesh: \"istio\"\n",
    "    \n",
    "    # Export to S3 for BI integration\n",
    "    export:\n",
    "      enabled: true\n",
    "      schedule: \"0 * * * *\"  # Hourly\n",
    "      s3:\n",
    "        bucket: \"k8s-cost-data\"\n",
    "        prefix: \"allocations/\"\n",
    "        region: \"us-east-1\"\n",
    "```\n",
    "\n",
    "**Custom Cost Allocation Script:**\n",
    "\n",
    "```python\n",
    "# Allocate RDS shared database costs by connection/user\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def allocate_shared_database_costs():\n",
    "    \"\"\"\n",
    "    Distribute RDS costs to business units based on database connections\n",
    "    \"\"\"\n",
    "    ce = boto3.client('ce')  # Cost Explorer\n",
    "    rds = boto3.client('rds')\n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "    \n",
    "    # Get last month's RDS costs\n",
    "    start_date = (datetime.now() - timedelta(days=30)).strftime('%Y-%m-01')\n",
    "    end_date = datetime.now().strftime('%Y-%m-01')\n",
    "    \n",
    "    response = ce.get_cost_and_usage(\n",
    "        TimePeriod={\n",
    "            'Start': start_date,\n",
    "            'End': end_date\n",
    "        },\n",
    "        Granularity='MONTHLY',\n",
    "        Filter={\n",
    "            'Dimensions': {\n",
    "                'Key': 'SERVICE',\n",
    "                'Values': ['Amazon Relational Database Service']\n",
    "            }\n",
    "        },\n",
    "        Metrics=['UnblendedCost'],\n",
    "        GroupBy=[{'Type': 'DIMENSION', 'Key': 'RESOURCE_ID'}]\n",
    "    )\n",
    "    \n",
    "    allocations = []\n",
    "    \n",
    "    for group in response['ResultsByTime'][0]['Groups']:\n",
    "        resource_id = group['Keys'][0]\n",
    "        total_cost = float(group['Metrics']['UnblendedCost']['Amount'])\n",
    "        \n",
    "        # Get connection metrics by database user\n",
    "        metrics = cloudwatch.get_metric_statistics(\n",
    "            Namespace='AWS/RDS',\n",
    "            MetricName='DatabaseConnections',\n",
    "            Dimensions=[\n",
    "                {'Name': 'DBInstanceIdentifier', 'Value': resource_id.split('/')[-1]}\n",
    "            ],\n",
    "            StartTime=datetime.now() - timedelta(days=30),\n",
    "            EndTime=datetime.now(),\n",
    "            Period=86400,  # Daily granularity\n",
    "            Statistics=['Average']\n",
    "        )\n",
    "        \n",
    "        # Query CloudTrail for connection patterns (simplified)\n",
    "        # In production, query database logs for actual user connection time\n",
    "        business_units = {\n",
    "            'Engineering': 0.6,      # 60% of connections\n",
    "            'Marketing': 0.25,       # 25% of connections\n",
    "            'DataScience': 0.15      # 15% of connections\n",
    "        }\n",
    "        \n",
    "        # Allocate costs\n",
    "        for bu, percentage in business_units.items():\n",
    "            allocations.append({\n",
    "                'resource': resource_id,\n",
    "                'total_cost': total_cost,\n",
    "                'business_unit': bu,\n",
    "                'allocated_cost': round(total_cost * percentage, 2),\n",
    "                'allocation_basis': f'Connection share: {percentage*100}%',\n",
    "                'period': start_date\n",
    "            })\n",
    "    \n",
    "    # Write to cost allocation database\n",
    "    dynamodb = boto3.resource('dynamodb')\n",
    "    table = dynamodb.Table('cost-allocations')\n",
    "    \n",
    "    with table.batch_writer() as batch:\n",
    "        for allocation in allocations:\n",
    "            batch.put_item(Item=allocation)\n",
    "    \n",
    "    return allocations\n",
    "```\n",
    "\n",
    "### 16.2.2 Amortization of Commitment-Based Discounts\n",
    "\n",
    "Reserved Instances (RIs) and Savings Plans provide discounts (up to 72%) in exchange for usage commitments. However, cost allocation must amortize these upfront or monthly fees across the resources actually consuming them.\n",
    "\n",
    "**Savings Plans Amortization Logic:**\n",
    "\n",
    "```python\n",
    "def calculate_blended_cost(on_demand_cost, savings_plan_coverage):\n",
    "    \"\"\"\n",
    "    Calculate effective cost after applying Savings Plans discounts\n",
    "    \"\"\"\n",
    "    # Example: $100 on-demand cost with 70% SP coverage at 40% discount\n",
    "    \n",
    "    covered_amount = on_demand_cost * (savings_plan_coverage / 100)\n",
    "    uncovered_amount = on_demand_cost - covered_amount\n",
    "    \n",
    "    # Savings Plans typically offer 30-40% discount for compute\n",
    "    sp_discount_rate = 0.35\n",
    "    sp_effective_rate = 1 - sp_discount_rate\n",
    "    \n",
    "    covered_cost = covered_amount * sp_effective_rate\n",
    "    uncovered_cost = uncovered_amount  # Pay on-demand rate\n",
    "    \n",
    "    total_effective_cost = covered_cost + uncovered_cost\n",
    "    effective_discount = (on_demand_cost - total_effective_cost) / on_demand_cost\n",
    "    \n",
    "    return {\n",
    "        'on_demand_equivalent': on_demand_cost,\n",
    "        'effective_cost': total_effective_cost,\n",
    "        'savings': on_demand_cost - total_effective_cost,\n",
    "        'effective_discount_percent': effective_discount * 100,\n",
    "        'covered_by_sp': covered_amount,\n",
    "        'overage': uncovered_amount\n",
    "    }\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 16.3 Compute Optimization: Purchasing Strategies\n",
    "\n",
    "Compute typically represents 60-70% of cloud spending. Optimization requires intelligent purchasing strategies beyond simple on-demand provisioning.\n",
    "\n",
    "### 16.3.1 Reserved Instances and Savings Plans\n",
    "\n",
    "**AWS Savings Plans (SP):** Flexible commitment-based discount applied automatically to any compute usage matching the attributes (region, instance family, tenancy).\n",
    "\n",
    "Types:\n",
    "- **Compute Savings Plans:** Most flexible (any region, any instance family, any tenancy). ~30% discount.\n",
    "- **EC2 Instance Savings Plans:** Specific to a region and instance family. ~40% discount.\n",
    "- **SageMaker Savings Plans:** For ML workloads.\n",
    "\n",
    "**Terraform for Savings Plans Management:**\n",
    "\n",
    "```hcl\n",
    "# Note: Savings Plans are purchased via AWS CLI or Console, not Terraform\n",
    "# This module manages SP monitoring and recommendations\n",
    "\n",
    "# Lambda to analyze coverage and recommend purchases\n",
    "resource \"aws_lambda_function\" \"savings_plan_analyzer\" {\n",
    "  filename      = \"sp_analyzer.zip\"\n",
    "  function_name = \"savings-plan-optimizer\"\n",
    "  role          = aws_iam_role.lambda_role.arn\n",
    "  handler       = \"index.handler\"\n",
    "  runtime       = \"python3.11\"\n",
    "  timeout       = 300\n",
    "  \n",
    "  environment {\n",
    "    variables = {\n",
    "      MIN_UTILIZATION_THRESHOLD = \"0.85\"  # Purchase only if >85% utilization guaranteed\n",
    "      LOOKBACK_DAYS            = \"30\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  schedule {\n",
    "    rate = \"rate(7 days)\"  # Weekly analysis\n",
    "  }\n",
    "}\n",
    "\n",
    "# Cost Anomaly Detection for commitment utilization\n",
    "resource \"aws_ce_anomaly_monitor\" \"savings_plan_utilization\" {\n",
    "  name              = \"SP-Utilization-Monitor\"\n",
    "  monitor_type      = \"DIMENSIONAL\"\n",
    "  monitor_dimension = \"SERVICE\"\n",
    "  \n",
    "  # Alert if SP utilization drops below threshold (wasted commitment)\n",
    "  tags = {\n",
    "    Purpose = \"FinOps\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Automated Rightsizing Recommendations:**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_rightsizing_recommendations():\n",
    "    \"\"\"\n",
    "    Analyze CloudWatch metrics to identify over-provisioned instances\n",
    "    \"\"\"\n",
    "    ec2 = boto3.client('ec2')\n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "    ce = boto3.client('ce')\n",
    "    \n",
    "    # Get all running instances\n",
    "    instances = ec2.describe_instances(\n",
    "        Filters=[\n",
    "            {'Name': 'instance-state-name', 'Values': ['running']},\n",
    "            {'Name': 'tag:Environment', 'Values': ['Production']}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    recommendations = []\n",
    "    \n",
    "    for reservation in instances['Reservations']:\n",
    "        for instance in reservation['Instances']:\n",
    "            instance_id = instance['InstanceId']\n",
    "            instance_type = instance['InstanceType']\n",
    "            \n",
    "            # Get CPU utilization over last 14 days\n",
    "            metrics = cloudwatch.get_metric_statistics(\n",
    "                Namespace='AWS/EC2',\n",
    "                MetricName='CPUUtilization',\n",
    "                Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],\n",
    "                StartTime=datetime.now() - timedelta(days=14),\n",
    "                EndTime=datetime.now(),\n",
    "                Period=3600,  # Hourly\n",
    "                Statistics=['Average', 'Maximum']\n",
    "            )\n",
    "            \n",
    "            if not metrics['Datapoints']:\n",
    "                continue\n",
    "                \n",
    "            avg_cpu = sum(dp['Average'] for dp in metrics['Datapoints']) / len(metrics['Datapoints'])\n",
    "            max_cpu = max(dp['Maximum'] for dp in metrics['Datapoints'])\n",
    "            \n",
    "            # Rightsizing logic\n",
    "            if avg_cpu < 20 and max_cpu < 50:\n",
    "                # Severely underutilized - recommend smaller instance\n",
    "                current_family = instance_type.split('.')[0]\n",
    "                current_size = instance_type.split('.')[1]\n",
    "                \n",
    "                # Simple mapping (in production, use comprehensive matrix)\n",
    "                downsizing_map = {\n",
    "                    'xlarge': 'large',\n",
    "                    'large': 'medium',\n",
    "                    '2xlarge': 'xlarge'\n",
    "                }\n",
    "                \n",
    "                if current_size in downsizing_map:\n",
    "                    recommended = f\"{current_family}.{downsizing_map[current_size]}\"\n",
    "                    \n",
    "                    # Calculate savings\n",
    "                    current_cost = get_monthly_cost(instance_type)\n",
    "                    recommended_cost = get_monthly_cost(recommended)\n",
    "                    savings = current_cost - recommended_cost\n",
    "                    \n",
    "                    recommendations.append({\n",
    "                        'instance_id': instance_id,\n",
    "                        'current_type': instance_type,\n",
    "                        'recommended_type': recommended,\n",
    "                        'avg_cpu': round(avg_cpu, 2),\n",
    "                        'max_cpu': round(max_cpu, 2),\n",
    "                        'estimated_monthly_savings': round(savings, 2),\n",
    "                        'confidence': 'High' if avg_cpu < 10 else 'Medium'\n",
    "                    })\n",
    "            \n",
    "            # Check for GPU instances with low utilization\n",
    "            if 'g4' in instance_type or 'p3' in instance_type:\n",
    "                gpu_util = cloudwatch.get_metric_statistics(\n",
    "                    Namespace='AWS/EC2',\n",
    "                    MetricName='GPUUtilization',\n",
    "                    Dimensions=[{'Name': 'InstanceId', 'Value': instance_id}],\n",
    "                    StartTime=datetime.now() - timedelta(days=7),\n",
    "                    EndTime=datetime.now(),\n",
    "                    Period=3600,\n",
    "                    Statistics=['Average']\n",
    "                )\n",
    "                \n",
    "                if gpu_util['Datapoints']:\n",
    "                    avg_gpu = sum(dp['Average'] for dp in gpu_util['Datapoints']) / len(gpu_util['Datapoints'])\n",
    "                    if avg_gpu < 10:\n",
    "                        recommendations.append({\n",
    "                            'instance_id': instance_id,\n",
    "                            'issue': 'Underutilized GPU',\n",
    "                            'avg_gpu_util': round(avg_gpu, 2),\n",
    "                            'recommendation': 'Consider stopping or downsizing GPU instance',\n",
    "                            'estimated_monthly_savings': get_monthly_cost(instance_type) * 0.9\n",
    "                        })\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "def get_monthly_cost(instance_type):\n",
    "    \"\"\"Lookup monthly on-demand cost (simplified)\"\"\"\n",
    "    pricing = {\n",
    "        'm5.large': 70.08,\n",
    "        'm5.xlarge': 140.16,\n",
    "        'm5.2xlarge': 280.32,\n",
    "        'g4dn.xlarge': 378.00,\n",
    "        'p3.2xlarge': 3060.00\n",
    "    }\n",
    "    return pricing.get(instance_type, 100.0)  # Default estimate\n",
    "```\n",
    "\n",
    "### 16.3.2 Spot Instances and Spot Fleet\n",
    "\n",
    "Spot instances utilize spare EC2 capacity at discounts up to 90%, but can be interrupted with 2-minute warning. Ideal for fault-tolerant, stateless workloads.\n",
    "\n",
    "**Spot Fleet with Diversified Instance Types:**\n",
    "\n",
    "```hcl\n",
    "resource \"aws_spot_fleet_request\" \"worker_nodes\" {\n",
    "  iam_fleet_role                      = aws_iam_role.spot_fleet.arn\n",
    "  target_capacity                     = 20\n",
    "  allocation_strategy                 = \"capacity-optimized\"  # Best availability\n",
    "  instance_pools_to_use_count         = 3\n",
    "  on_demand_allocation_strategy       = \"lowestPrice\"\n",
    "  on_demand_target_capacity           = 5      # 25% on-demand for baseline\n",
    "  spot_target_capacity                = 15     # 75% spot for cost optimization\n",
    "  excess_capacity_termination_policy  = \"default\"\n",
    "  \n",
    "  launch_specification {\n",
    "    instance_type          = \"m5.large\"\n",
    "    ami                    = data.aws_ami.eks_worker.id\n",
    "    spot_price             = \"0.05\"  # Max willing to pay (on-demand is ~$0.096)\n",
    "    key_name               = aws_key_pair.deployer.key_name\n",
    "    subnet_id              = aws_subnet.private_a.id\n",
    "    vpc_security_group_ids = [aws_security_group.worker_nodes.id]\n",
    "    user_data              = base64encode(local.eks_worker_userdata)\n",
    "    \n",
    "    root_block_device {\n",
    "      volume_size = 50\n",
    "      volume_type = \"gp3\"\n",
    "    }\n",
    "    \n",
    "    tags = {\n",
    "      BusinessUnit = \"Engineering\"\n",
    "      CostOptimization = \"Spot\"\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Diversify across instance types for availability\n",
    "  launch_specification {\n",
    "    instance_type = \"m5a.large\"  # AMD variant\n",
    "    ami           = data.aws_ami.eks_worker.id\n",
    "    spot_price    = \"0.05\"\n",
    "    subnet_id     = aws_subnet.private_b.id\n",
    "    # ... same configuration\n",
    "  }\n",
    "  \n",
    "  launch_specification {\n",
    "    instance_type = \"m6i.large\"  # Intel variant\n",
    "    ami           = data.aws_ami.eks_worker.id\n",
    "    spot_price    = \"0.05\"\n",
    "    subnet_id     = aws_subnet.private_c.id\n",
    "    # ... same configuration\n",
    "  }\n",
    "  \n",
    "  # Capacity rebalance notification\n",
    "  spot_maintenance_strategies {\n",
    "    capacity_rebalance {\n",
    "      replacement_strategy = \"launch\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Handling Spot Interruptions:**\n",
    "\n",
    "```python\n",
    "# Lambda triggered by EC2 Spot Interruption Warning (2 minutes notice)\n",
    "def handle_spot_interruption(event, context):\n",
    "    \"\"\"\n",
    "    Gracefully handle spot instance interruption\n",
    "    \"\"\"\n",
    "    instance_id = event['detail']['instance-id']\n",
    "    \n",
    "    # Query if this is a Kubernetes node\n",
    "    eks = boto3.client('eks')\n",
    "    ec2 = boto3.client('ec2')\n",
    "    \n",
    "    # Cordon and drain node if part of EKS cluster\n",
    "    import subprocess\n",
    "    \n",
    "    # Get instance tags to find cluster info\n",
    "    tags = ec2.describe_instances(InstanceIds=[instance_id])['Reservations'][0]['Instances'][0]['Tags']\n",
    "    tag_dict = {t['Key']: t['Value'] for t in tags}\n",
    "    \n",
    "    if 'eks:cluster-name' in tag_dict:\n",
    "        cluster_name = tag_dict['eks:cluster-name']\n",
    "        \n",
    "        # Trigger pod migration via Kubernetes API\n",
    "        # In practice, use EKS node termination handler DaemonSet\n",
    "        subprocess.run([\n",
    "            'kubectl', 'drain', instance_id,\n",
    "            '--ignore-daemonsets',\n",
    "            '--delete-local-data',\n",
    "            '--force',\n",
    "            '--grace-period=60'\n",
    "        ])\n",
    "        \n",
    "        # Notify Auto Scaling Group to replace capacity\n",
    "        asg = boto3.client('autoscaling')\n",
    "        asg.complete_lifecycle_action(\n",
    "            LifecycleHookName='spot-termination-hook',\n",
    "            AutoScalingGroupName=tag_dict['aws:autoscaling:groupName'],\n",
    "            LifecycleActionToken=event['detail']['LifecycleActionToken'],\n",
    "            LifecycleActionResult='CONTINUE'\n",
    "        )\n",
    "    \n",
    "    return {'status': 'drained', 'instance': instance_id}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 16.4 Storage and Data Lifecycle Cost Management\n",
    "\n",
    "Storage costs grow linearly with data volume, but access patterns follow power laws\u201480% of data is rarely accessed after 30 days. Intelligent tiering and lifecycle policies reduce costs by 90% for archive data.\n",
    "\n",
    "### 16.4.1 S3 Intelligent Tiering and Lifecycle Policies\n",
    "\n",
    "```hcl\n",
    "resource \"aws_s3_bucket\" \"data_lake\" {\n",
    "  bucket = \"company-data-lake\"\n",
    "  \n",
    "  tags = {\n",
    "    BusinessUnit = \"DataEngineering\"\n",
    "    CostCenter = \"CC-Data-001\"\n",
    "  }\n",
    "}\n",
    "\n",
    "# Intelligent Tiering configuration\n",
    "resource \"aws_s3_bucket_intelligent_tiering_configuration\" \"analytics\" {\n",
    "  bucket = aws_s3_bucket.data_lake.bucket\n",
    "  name   = \"AnalyticsDataOptimization\"\n",
    "\n",
    "  tiering {\n",
    "    access_tier = \"ARCHIVE_ACCESS\"\n",
    "    days        = 90\n",
    "  }\n",
    "  \n",
    "  tiering {\n",
    "    access_tier = \"DEEP_ARCHIVE_ACCESS\"\n",
    "    days        = 180\n",
    "  }\n",
    "  \n",
    "  # Filter for specific prefixes or tags\n",
    "  filter {\n",
    "    prefix = \"raw-data/\"\n",
    "    \n",
    "    tags = {\n",
    "      ArchiveEligible = \"true\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# Lifecycle rules for non-intelligent tiering buckets\n",
    "resource \"aws_s3_bucket_lifecycle_configuration\" \"logs\" {\n",
    "  bucket = aws_s3_bucket.logs.id\n",
    "\n",
    "  rule {\n",
    "    id     = \"transition-to-ia\"\n",
    "    status = \"Enabled\"\n",
    "    \n",
    "    filter {\n",
    "      prefix = \"app-logs/\"\n",
    "    }\n",
    "    \n",
    "    transition {\n",
    "      days          = 30\n",
    "      storage_class = \"STANDARD_IA\"\n",
    "    }\n",
    "    \n",
    "    transition {\n",
    "      days          = 90\n",
    "      storage_class = \"GLACIER_IR\"  # Instant Retrieval - cheaper than IA, slower\n",
    "    }\n",
    "    \n",
    "    transition {\n",
    "      days          = 365\n",
    "      storage_class = \"DEEP_ARCHIVE\"  # Cheapest storage - $0.00099/GB\n",
    "    }\n",
    "    \n",
    "    expiration {\n",
    "      days = 2555  # 7 years compliance retention\n",
    "    }\n",
    "    \n",
    "    noncurrent_version_expiration {\n",
    "      noncurrent_days = 30  # Clean up old versions\n",
    "    }\n",
    "  }\n",
    "  \n",
    "  # Abort incomplete multipart uploads (cost savings)\n",
    "  rule {\n",
    "    id     = \"abort-incomplete-uploads\"\n",
    "    status = \"Enabled\"\n",
    "    \n",
    "    abort_incomplete_multipart_upload {\n",
    "      days_after_initiation = 7\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "**Cost Analysis Script:**\n",
    "\n",
    "```python\n",
    "def analyze_storage_costs():\n",
    "    \"\"\"\n",
    "    Identify cost optimization opportunities in S3\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "    \n",
    "    buckets = s3.list_buckets()['Buckets']\n",
    "    recommendations = []\n",
    "    \n",
    "    for bucket in buckets:\n",
    "        bucket_name = bucket['Name']\n",
    "        \n",
    "        # Get storage class breakdown\n",
    "        try:\n",
    "            metrics = cloudwatch.get_metric_statistics(\n",
    "                Namespace='AWS/S3',\n",
    "                MetricName='BucketSizeBytes',\n",
    "                Dimensions=[\n",
    "                    {'Name': 'BucketName', 'Value': bucket_name},\n",
    "                    {'Name': 'StorageType', 'Value': 'StandardStorage'}\n",
    "                ],\n",
    "                StartTime=datetime.now() - timedelta(days=1),\n",
    "                EndTime=datetime.now(),\n",
    "                Period=86400,\n",
    "                Statistics=['Average']\n",
    "            )\n",
    "            \n",
    "            standard_bytes = metrics['Datapoints'][0]['Average'] if metrics['Datapoints'] else 0\n",
    "            standard_gb = standard_bytes / (1024**3)\n",
    "            \n",
    "            # Check if Intelligent Tiering would save money\n",
    "            # (requires analyzing access patterns - simplified here)\n",
    "            if standard_gb > 1000:  # 1TB+\n",
    "                recommendations.append({\n",
    "                    'bucket': bucket_name,\n",
    "                    'current_standard_gb': round(standard_gb, 2),\n",
    "                    'recommendation': 'Enable Intelligent Tiering',\n",
    "                    'estimated_savings': round(standard_gb * 0.023 * 0.4, 2),  # ~40% of Standard cost\n",
    "                    'implementation': 'aws_s3_bucket_intelligent_tiering_configuration'\n",
    "                })\n",
    "            \n",
    "            # Check for incomplete multipart uploads\n",
    "            mpu = s3.list_multipart_uploads(Bucket=bucket_name)\n",
    "            if mpu.get('Uploads'):\n",
    "                total_mpu_size = sum(\n",
    "                    part['Size'] for upload in mpu['Uploads'] \n",
    "                    for part in s3.list_parts(Bucket=bucket_name, Key=upload['Key'], UploadId=upload['UploadId']).get('Parts', [])\n",
    "                )\n",
    "                if total_mpu_size > 100 * 1024**3:  # 100GB\n",
    "                    recommendations.append({\n",
    "                        'bucket': bucket_name,\n",
    "                        'issue': 'Incomplete Multipart Uploads',\n",
    "                        'wasted_space_gb': round(total_mpu_size / (1024**3), 2),\n",
    "                        'action': 'Add lifecycle rule to abort incomplete uploads after 7 days'\n",
    "                    })\n",
    "                    \n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {bucket_name}: {e}\")\n",
    "    \n",
    "    return recommendations\n",
    "```\n",
    "\n",
    "### 16.4.2 EBS Volume Optimization\n",
    "\n",
    "Unused EBS volumes and over-provisioned IOPS represent significant waste.\n",
    "\n",
    "```hcl\n",
    "# Lambda to identify and snapshot unused volumes\n",
    "resource \"aws_lambda_function\" \"ebs_optimizer\" {\n",
    "  filename      = \"ebs_optimizer.zip\"\n",
    "  function_name = \"ebs-cost-optimizer\"\n",
    "  role          = aws_iam_role.lambda_role.arn\n",
    "  handler       = \"index.handler\"\n",
    "  runtime       = \"python3.11\"\n",
    "  \n",
    "  environment {\n",
    "    variables = {\n",
    "      IDLE_DAYS_THRESHOLD = \"30\"\n",
    "      SNAPSHOT_RETENTION_DAYS = \"30\"\n",
    "    }\n",
    "  }\n",
    "}\n",
    "\n",
    "# CloudWatch Event to trigger weekly\n",
    "resource \"aws_cloudwatch_event_rule\" \"weekly_optimization\" {\n",
    "  name                = \"ebs-optimization-schedule\"\n",
    "  description         = \"Run EBS optimization scan weekly\"\n",
    "  schedule_expression = \"cron(0 9 ? * MON *)\"  # Mondays at 9am\n",
    "}\n",
    "\n",
    "resource \"aws_cloudwatch_event_target\" \"lambda_trigger\" {\n",
    "  rule      = aws_cloudwatch_event_rule.weekly_optimization.name\n",
    "  target_id = \"EBSOptimizer\"\n",
    "  arn       = aws_lambda_function.ebs_optimizer.arn\n",
    "}\n",
    "```\n",
    "\n",
    "**EBS Optimization Logic:**\n",
    "\n",
    "```python\n",
    "def optimize_ebs_volumes():\n",
    "    \"\"\"\n",
    "    Identify unattached and underutilized EBS volumes\n",
    "    \"\"\"\n",
    "    ec2 = boto3.client('ec2')\n",
    "    cloudwatch = boto3.client('cloudwatch')\n",
    "    \n",
    "    # Find unattached volumes\n",
    "    unattached = ec2.describe_volumes(\n",
    "        Filters=[{'Name': 'status', 'Values': ['available']}]\n",
    "    )['Volumes']\n",
    "    \n",
    "    actions = []\n",
    "    \n",
    "    for volume in unattached:\n",
    "        volume_id = volume['VolumeId']\n",
    "        size = volume['Size']\n",
    "        volume_type = volume['VolumeType']\n",
    "        monthly_cost = size * 0.10  # gp3 is $0.10/GB-month\n",
    "        \n",
    "        # Create snapshot before deletion\n",
    "        snapshot = ec2.create_snapshot(\n",
    "            VolumeId=volume_id,\n",
    "            Description=f'Pre-deletion backup for {volume_id}',\n",
    "            TagSpecifications=[{\n",
    "                'ResourceType': 'snapshot',\n",
    "                'Tags': [\n",
    "                    {'Key': 'SourceVolume', 'Value': volume_id},\n",
    "                    {'Key': 'DeletionDate', 'Value': datetime.now().isoformat()}\n",
    "                ]\n",
    "            }]\n",
    "        )\n",
    "        \n",
    "        # Tag volume for deletion after snapshot completes\n",
    "        ec2.create_tags(\n",
    "            Resources=[volume_id],\n",
    "            Tags=[{'Key': 'Status', 'Value': 'PendingDeletion'}]\n",
    "        )\n",
    "        \n",
    "        actions.append({\n",
    "            'volume_id': volume_id,\n",
    "            'action': 'SnapshotAndDelete',\n",
    "            'monthly_savings': monthly_cost,\n",
    "            'snapshot_id': snapshot['SnapshotId']\n",
    "        })\n",
    "    \n",
    "    # Find underutilized volumes (low IOPS)\n",
    "    attached_volumes = ec2.describe_volumes(\n",
    "        Filters=[{'Name': 'status', 'Values': ['in-use']}]\n",
    "    )['Volumes']\n",
    "    \n",
    "    for volume in attached_volumes:\n",
    "        if volume['VolumeType'] in ['io1', 'io2']:\n",
    "            # Check provisioned IOPS utilization\n",
    "            metrics = cloudwatch.get_metric_statistics(\n",
    "                Namespace='AWS/EBS',\n",
    "                MetricName='VolumeReadOps',\n",
    "                Dimensions=[{'Name': 'VolumeId', 'Value': volume['VolumeId']}],\n",
    "                StartTime=datetime.now() - timedelta(days=7),\n",
    "                EndTime=datetime.now(),\n",
    "                Period=86400,\n",
    "                Statistics=['Sum']\n",
    "            )\n",
    "            \n",
    "            if metrics['Datapoints']:\n",
    "                avg_iops = sum(dp['Sum'] for dp in metrics['Datapoints']) / len(metrics['Datapoints']) / 86400\n",
    "                provisioned_iops = volume.get('Iops', 0)\n",
    "                \n",
    "                if avg_iops < provisioned_iops * 0.3:  # Using less than 30% of provisioned\n",
    "                    actions.append({\n",
    "                        'volume_id': volume['VolumeId'],\n",
    "                        'action': 'DownsizeIOPS',\n",
    "                        'current_iops': provisioned_iops,\n",
    "                        'recommended_iops': int(provisioned_iops * 0.5),\n",
    "                        'estimated_savings': (provisioned_iops - int(provisioned_iops * 0.5)) * 0.065  # IOPS cost\n",
    "                    })\n",
    "    \n",
    "    return actions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 16.5 Multi-Cloud Cost Governance\n",
    "\n",
    "Organizations utilizing multiple clouds face the challenge of normalized visibility and consistent governance across disparate billing models and APIs.\n",
    "\n",
    "### 16.5.1 Unified Cost Dashboard\n",
    "\n",
    "**Terraform for CloudHealth/Cloudability Integration:**\n",
    "\n",
    "```hcl\n",
    "# AWS IAM Role for third-party cost management platform\n",
    "resource \"aws_iam_role\" \"cost_management\" {\n",
    "  name = \"CostManagementPlatformRole\"\n",
    "  \n",
    "  assume_role_policy = jsonencode({\n",
    "    Version = \"2012-10-17\"\n",
    "    Statement = [\n",
    "      {\n",
    "        Action = \"sts:AssumeRole\"\n",
    "        Effect = \"Allow\"\n",
    "        Principal = {\n",
    "          AWS = \"arn:aws:iam::COST_PLATFORM_ACCOUNT:root\"\n",
    "        }\n",
    "        Condition = {\n",
    "          StringEquals = {\n",
    "            \"sts:ExternalId\" = var.cost_platform_external_id\n",
    "          }\n",
    "        }\n",
    "      }\n",
    "    ]\n",
    "  })\n",
    "}\n",
    "\n",
    "resource \"aws_iam_role_policy\" \"cost_management_policy\" {\n",
    "  name = \"CostAndUsagePolicy\"\n",
    "  role = aws_iam_role.cost_management.id\n",
    "  \n",
    "  policy = jsonencode({\n",
    "    Version = \"2012-10-17\"\n",
    "    Statement = [\n",
    "      {\n",
    "        Effect = \"Allow\"\n",
    "        Action = [\n",
    "          \"ce:GetCostAndUsage\",\n",
    "          \"ce:GetCostForecast\",\n",
    "          \"ce:GetUsageForecast\",\n",
    "          \"ce:GetReservationUtilization\",\n",
    "          \"ce:GetSavingsPlansUtilization\",\n",
    "          \"cur:DescribeReportDefinitions\",\n",
    "          \"s3:GetObject\",\n",
    "          \"s3:ListBucket\"\n",
    "        ]\n",
    "        Resource = \"*\"\n",
    "      }\n",
    "    ]\n",
    "  })\n",
    "}\n",
    "\n",
    "# Azure Cost Management export\n",
    "resource \"azurerm_subscription_cost_management_export\" \"daily\" {\n",
    "  name                         = \"daily-cost-export\"\n",
    "  subscription_id              = data.azurerm_subscription.current.subscription_id\n",
    "  recurrence_type              = \"Daily\"\n",
    "  recurrence_period_start_date = \"2026-01-01T00:00:00Z\"\n",
    "  recurrence_period_end_date   = \"2026-12-31T00:00:00Z\"\n",
    "  \n",
    "  export_data_storage_location {\n",
    "    container_id     = azurerm_storage_container.cost_exports.resource_manager_id\n",
    "    root_folder_path = \"/cost-data\"\n",
    "  }\n",
    "  \n",
    "  export_data_options {\n",
    "    type = \"Usage\"\n",
    "    time_frame = \"MonthToDate\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 16.5.2 Budget Enforcement Automation\n",
    "\n",
    "```hcl\n",
    "# AWS Budgets with automated enforcement\n",
    "resource \"aws_budgets_budget\" \"engineering\" {\n",
    "  name              = \"Engineering-Monthly-Budget\"\n",
    "  budget_type       = \"COST\"\n",
    "  limit_amount      = \"50000\"\n",
    "  limit_unit        = \"USD\"\n",
    "  time_period_start = \"2026-01-01_00:00\"\n",
    "  time_unit         = \"MONTHLY\"\n",
    "  \n",
    "  cost_filter {\n",
    "    name = \"TagKeyValue\"\n",
    "    values = [\n",
    "      \"user:BusinessUnit$Engineering\"\n",
    "    ]\n",
    "  }\n",
    "  \n",
    "  notification {\n",
    "    comparison_operator        = \"GREATER_THAN\"\n",
    "    threshold                  = 80\n",
    "    threshold_type             = \"PERCENTAGE\"\n",
    "    notification_type          = \"ACTUAL\"\n",
    "    subscriber_email_addresses = [\"finops@company.com\", \"engineering-leads@company.com\"]\n",
    "  }\n",
    "  \n",
    "  # Critical threshold - trigger Lambda for enforcement\n",
    "  notification {\n",
    "    comparison_operator        = \"GREATER_THAN\"\n",
    "    threshold                  = 100\n",
    "    threshold_type             = \"PERCENTAGE\"\n",
    "    notification_type          = \"ACTUAL\"\n",
    "    subscriber_sns_topic_arns    = [aws_sns_topic.budget_alerts.arn]\n",
    "  }\n",
    "}\n",
    "\n",
    "# Lambda to enforce budget compliance\n",
    "resource \"aws_lambda_function\" \"budget_enforcer\" {\n",
    "  filename      = \"budget_enforcer.zip\"\n",
    "  function_name = \"budget-compliance-enforcer\"\n",
    "  role          = aws_iam_role.lambda_role.arn\n",
    "  handler       = \"index.handler\"\n",
    "  runtime       = \"python3.11\"\n",
    "}\n",
    "\n",
    "resource \"aws_sns_topic_subscription\" \"budget_trigger\" {\n",
    "  topic_arn = aws_sns_topic.budget_alerts.arn\n",
    "  protocol  = \"lambda\"\n",
    "  endpoint  = aws_lambda_function.budget_enforcer.arn\n",
    "}\n",
    "```\n",
    "\n",
    "**Budget Enforcement Logic:**\n",
    "\n",
    "```python\n",
    "def enforce_budget_compliance(notification):\n",
    "    \"\"\"\n",
    "    Automated actions when budget threshold exceeded\n",
    "    \"\"\"\n",
    "    message = json.loads(notification['Message'])\n",
    "    budget_name = message['BudgetName']\n",
    "    threshold = message['Threshold']\n",
    "    \n",
    "    # Parse budget name to determine scope\n",
    "    if 'Engineering' in budget_name:\n",
    "        # Stop non-prod resources\n",
    "        stop_non_production_resources()\n",
    "        \n",
    "        # Notify via Slack\n",
    "        send_slack_alert({\n",
    "            'channel': '#engineering-cost',\n",
    "            'text': f'Budget threshold {threshold}% exceeded. Non-prod resources stopped.'\n",
    "        })\n",
    "    \n",
    "    elif 'DataScience' in budget_name:\n",
    "        # Spot instance only policy\n",
    "        convert_on_demand_to_spot()\n",
    "    \n",
    "    return {'status': 'enforcement_actions_triggered'}\n",
    "\n",
    "def stop_non_production_resources():\n",
    "    ec2 = boto3.client('ec2')\n",
    "    rds = boto3.client('rds')\n",
    "    \n",
    "    # Stop dev/test EC2 instances\n",
    "    dev_instances = ec2.describe_instances(\n",
    "        Filters=[\n",
    "            {'Name': 'tag:Environment', 'Values': ['Development', 'Testing']},\n",
    "            {'Name': 'instance-state-name', 'Values': ['running']}\n",
    "        ]\n",
    "    )\n",
    "    \n",
    "    instance_ids = []\n",
    "    for res in dev_instances['Reservations']:\n",
    "        for inst in res['Instances']:\n",
    "            instance_ids.append(inst['InstanceId'])\n",
    "    \n",
    "    if instance_ids:\n",
    "        ec2.stop_instances(InstanceIds=instance_ids)\n",
    "        \n",
    "        # Tag with stop reason\n",
    "        ec2.create_tags(\n",
    "            Resources=instance_ids,\n",
    "            Tags=[{'Key': 'StoppedBy', 'Value': 'BudgetEnforcement'}]\n",
    "        )\n",
    "    \n",
    "    # Stop dev RDS instances\n",
    "    dev_dbs = rds.describe_db_instances()\n",
    "    for db in dev_dbs['DBInstances']:\n",
    "        if any(tag['Key'] == 'Environment' and tag['Value'] in ['Development', 'Testing'] \n",
    "               for tag in db.get('TagList', [])):\n",
    "            if db['DBInstanceStatus'] == 'available':\n",
    "                rds.stop_db_instance(\n",
    "                    DBInstanceIdentifier=db['DBInstanceIdentifier']\n",
    "                )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 16.6 Chapter Summary and Transition\n",
    "\n",
    "This chapter has operationalized cloud economics through the FinOps framework, establishing practices that ensure architectural sophistication translates to economic sustainability. We implemented comprehensive tagging strategies that enable precise cost attribution to business units, projects, and environments, coupled with automated governance that prevents untagged resources from proliferating unmonitored.\n",
    "\n",
    "Compute optimization strategies demonstrated the economic impact of purchasing commitments\u2014Reserved Instances and Savings Plans reducing compute costs by up to 72%\u2014and architectural patterns like Spot Instances that utilize spare capacity at fractional costs. Rightsizing automation identified over-provisioned resources, while storage lifecycle policies automatically tiered data from expensive hot storage to glacier archives as access patterns cooled, reducing storage costs by orders of magnitude for historical data.\n",
    "\n",
    "Multi-cloud governance addressed the complexity of normalized visibility across disparate cloud billing models, implementing unified dashboards and automated budget enforcement that stops non-production resources when thresholds are exceeded. These practices transform cloud spending from an opaque operational burden to a governed, optimized investment with measurable business value.\n",
    "\n",
    "However, cost optimization must never compromise operational reliability. Aggressive cost cutting\u2014overly aggressive Spot Instance usage without proper interruption handling, excessive data archival that impedes legitimate access, or under-provisioned compute that creates performance bottlenecks\u2014creates technical debt and operational risk. The most cost-efficient architecture is one that scales elastically, performs reliably, and provides observable telemetry to prevent costly outages.\n",
    "\n",
    "In **Chapter 17: Cloud Observability and Site Reliability Engineering (SRE)**, we will balance economic efficiency with operational excellence. You will learn to implement comprehensive observability stacks that provide visibility into system health and cost-correlated performance, establish Service Level Objectives (SLOs) that define acceptable error budgets, practice Chaos Engineering to validate that cost-optimized architectures remain resilient, and implement automated scaling policies that optimize costs during low-traffic periods while maintaining performance during peaks. We will explore the Golden Signals (latency, traffic, errors, saturation) and the three pillars of observability\u2014metrics, logs, and traces\u2014in the context of FinOps-informed infrastructure, ensuring that efficiency never compromises reliability."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../5. security_governance_and_compliance/15. cloud_security_operations.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='17. implementing_finops.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}