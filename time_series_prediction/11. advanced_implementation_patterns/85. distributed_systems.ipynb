{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 85: Distributed Systems\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the need for distributed computing in large\u2011scale time\u2011series prediction systems.\n",
    "- Distinguish between different parallelism strategies: data parallelism, model parallelism, and task parallelism.\n",
    "- Implement distributed data processing using frameworks like Dask, Ray, or Apache Spark.\n",
    "- Design distributed training for models that cannot fit on a single machine or require faster training.\n",
    "- Build distributed inference services that can handle high\u2011throughput prediction requests (e.g., for thousands of stocks).\n",
    "- Handle data partitioning (sharding) by time, symbol, or other keys to enable scalability.\n",
    "- Understand consistency and fault tolerance challenges in distributed ML systems.\n",
    "- Monitor and debug distributed pipelines.\n",
    "- Evaluate trade\u2011offs between different distributed architectures for time\u2011series workloads.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.1 Introduction to Distributed Systems for Time\u2011Series Prediction**\n",
    "\n",
    "As our time\u2011series prediction system grows, we may encounter limitations of a single machine:\n",
    "\n",
    "- **Data volume**: The NEPSE dataset may be small, but a system covering thousands of stocks with years of tick\u2011by\u2011tick data can easily reach terabytes.\n",
    "- **Model complexity**: Deep learning models with millions of parameters may require multiple GPUs for training.\n",
    "- **Inference throughput**: Serving predictions for many symbols in real time may exceed the capacity of a single server.\n",
    "- **Feature engineering**: Computing rolling statistics across many time series can be computationally intensive and benefit from parallelisation.\n",
    "\n",
    "Distributed systems allow us to scale horizontally by adding more machines. In this chapter, we will explore how to distribute the components of a time\u2011series prediction system: data storage, feature engineering, model training, and inference. We'll use frameworks like Dask (for parallel computing in Python) and Ray (for distributed execution) to illustrate the concepts.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.2 Fundamentals of Distributed Computing**\n",
    "\n",
    "### **85.2.1 Cluster, Nodes, and Tasks**\n",
    "A **cluster** is a collection of machines (nodes) working together. A **node** can be a physical server or a container. Work is divided into **tasks** that are scheduled across nodes.\n",
    "\n",
    "### **85.2.2 Shared\u2011Nothing Architecture**\n",
    "Most distributed systems use a shared\u2011nothing architecture: each node has its own CPU, memory, and disk. Nodes communicate via network messages (e.g., using TCP). This design scales well because there is no contention for shared resources.\n",
    "\n",
    "### **85.2.3 Data Parallelism vs. Model Parallelism**\n",
    "- **Data parallelism**: The same model is replicated on multiple nodes, and each node processes a different subset of the data. Gradients are aggregated (e.g., all\u2011reduce) to update the model. This is the most common approach for training large models on large datasets.\n",
    "- **Model parallelism**: Different parts of the model are placed on different nodes. This is used when the model itself is too large to fit on one node (e.g., some deep learning models with billions of parameters).\n",
    "\n",
    "For time\u2011series, data parallelism is more common: we can train the same model on different chunks of time series (e.g., different stocks, different time windows) and synchronise updates.\n",
    "\n",
    "### **85.2.4 Task Parallelism**\n",
    "Many tasks (e.g., feature computation for each stock) are independent and can be executed in parallel. This is often easier to implement than model parallelism.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.3 Distributed Data Processing with Dask**\n",
    "\n",
    "Dask is a flexible parallel computing library for Python that integrates with pandas, NumPy, and scikit\u2011learn. It can scale from a single machine to a cluster.\n",
    "\n",
    "### **85.3.1 Dask DataFrames**\n",
    "For large time\u2011series datasets, we can use Dask DataFrames, which partition a pandas\u2011like DataFrame across multiple workers.\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Read a large CSV with Dask (partitioned automatically)\n",
    "df = dd.read_csv('nepse_all_stocks.csv', blocksize='100MB')\n",
    "\n",
    "# Perform operations lazily\n",
    "df['daily_return'] = df.groupby('symbol')['close'].pct_change()\n",
    "df = df.dropna()\n",
    "\n",
    "# Compute result (triggers execution)\n",
    "result = df.compute()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Dask reads the CSV in chunks (partitions). Operations like `groupby` are performed in parallel on each partition, then combined.\n",
    "- `compute()` brings the result into a single pandas DataFrame (if it fits in memory). For larger results, you can write to disk or further process in Dask.\n",
    "\n",
    "### **85.3.2 Parallel Feature Engineering with Dask**\n",
    "\n",
    "Feature engineering for many stocks can be parallelised naturally: each stock's time series can be processed independently.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import dask\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Start a local Dask client\n",
    "client = Client()  # uses all cores on local machine\n",
    "\n",
    "# Function to engineer features for a single stock\n",
    "def engineer_features(symbol, df_symbol):\n",
    "    # df_symbol is a pandas DataFrame for one stock\n",
    "    df = df_symbol.copy()\n",
    "    df = df.sort_values('date')\n",
    "    # Compute features (lags, rolling, etc.)\n",
    "    df['lag_1'] = df['close'].shift(1)\n",
    "    df['sma_10'] = df['close'].rolling(10).mean()\n",
    "    df['volatility'] = df['close'].rolling(20).std()\n",
    "    # Drop NaNs\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "\n",
    "# Read full data (assuming it's a pandas DataFrame)\n",
    "# For demonstration, we'll simulate data for multiple stocks\n",
    "symbols = ['AAPL', 'GOOG', 'MSFT', 'NEPSE']\n",
    "dfs = []\n",
    "for sym in symbols:\n",
    "    dates = pd.date_range('2020-01-01', periods=1000, freq='D')\n",
    "    prices = 100 + np.cumsum(np.random.randn(1000))\n",
    "    dfs.append(pd.DataFrame({'date': dates, 'symbol': sym, 'close': prices}))\n",
    "df_all = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Group by symbol and apply function in parallel using Dask delayed\n",
    "lazy_results = []\n",
    "for symbol, group in df_all.groupby('symbol'):\n",
    "    lazy_result = dask.delayed(engineer_features)(symbol, group)\n",
    "    lazy_results.append(lazy_result)\n",
    "\n",
    "# Compute all in parallel\n",
    "results = dask.compute(*lazy_results)\n",
    "feature_df = pd.concat(results, ignore_index=True)\n",
    "print(feature_df.head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We use `dask.delayed` to wrap the function call for each symbol. These calls are scheduled on the Dask cluster and executed in parallel.\n",
    "- `dask.compute` triggers the computation and returns the results as a list of pandas DataFrames, which we concatenate.\n",
    "- This pattern is ideal for \"embarrassingly parallel\" workloads like per\u2011symbol feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.4 Distributed Training**\n",
    "\n",
    "When training a model on a large dataset (e.g., many stocks with many years of data), we can use distributed training to reduce wall\u2011clock time. Dask\u2011ML and Ray provide integrations with popular ML libraries.\n",
    "\n",
    "### **85.4.1 Distributed Training with Dask\u2011XGBoost**\n",
    "\n",
    "XGBoost supports distributed training via its native interface, and Dask can act as a backend.\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import dask.dataframe as dd\n",
    "from dask_cuda import LocalCUDACluster  # for GPU\n",
    "from dask.distributed import Client\n",
    "\n",
    "# Start a Dask cluster with GPU workers\n",
    "cluster = LocalCUDACluster()\n",
    "client = Client(cluster)\n",
    "\n",
    "# Load data as Dask DataFrame\n",
    "df = dd.read_parquet('features.parquet')\n",
    "\n",
    "# Split into features and target\n",
    "X = df[feature_cols]\n",
    "y = df['target']\n",
    "\n",
    "# Train distributed XGBoost\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X, y)\n",
    "params = {'objective': 'reg:squarederror', 'max_depth': 5, 'eta': 0.1}\n",
    "output = xgb.dask.train(client, params, dtrain, num_boost_round=100)\n",
    "\n",
    "# Get the booster\n",
    "booster = output['booster']\n",
    "\n",
    "# Predict in parallel\n",
    "predictions = xgb.dask.predict(client, booster, X).compute()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Dask\u2011XGBoost uses the Dask cluster to distribute data and computation. The `DaskDMatrix` holds the distributed data.\n",
    "- Training is performed in parallel across workers, with gradient aggregation handled by XGBoost's built\u2011in all\u2011reduce.\n",
    "- This scales to datasets that do not fit in a single machine's memory.\n",
    "\n",
    "### **85.4.2 Distributed Deep Learning with Ray**\n",
    "\n",
    "Ray is a general\u2011purpose distributed execution framework with libraries for machine learning (Ray Train, Ray Tune). It integrates with PyTorch and TensorFlow.\n",
    "\n",
    "Example: Distributed PyTorch training with Ray Train.\n",
    "\n",
    "```python\n",
    "# pip install ray[train] torch\n",
    "import ray\n",
    "from ray import train\n",
    "from ray.train import Trainer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Define a simple model\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.fc(out[:, -1, :])\n",
    "        return out\n",
    "\n",
    "# Training function\n",
    "def train_func(config):\n",
    "    # This function runs on each worker\n",
    "    model = LSTMModel(input_size=config['input_size'], hidden_size=64, num_layers=2)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    # Load data (each worker gets a different shard)\n",
    "    # Assume we have a function to get sharded data\n",
    "    train_loader = ...  # DataLoader for this worker's shard\n",
    "    for epoch in range(config['epochs']):\n",
    "        for batch in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            loss = compute_loss(model, batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        # Report metrics to Ray\n",
    "        train.report(epoch=epoch, loss=loss.item())\n",
    "\n",
    "# Initialize Ray\n",
    "ray.init(address='auto')  # or start a cluster\n",
    "\n",
    "# Set up distributed training\n",
    "trainer = Trainer(backend='torch', num_workers=4)\n",
    "trainer.start()\n",
    "results = trainer.run(\n",
    "    train_func,\n",
    "    config={'input_size': 10, 'epochs': 5}\n",
    ")\n",
    "trainer.shutdown()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Ray Train handles distributed data loading (sharding), gradient synchronisation, and checkpointing.\n",
    "- Each worker runs the `train_func` on its shard of data. PyTorch's DistributedDataParallel is used under the hood.\n",
    "- This enables scaling deep learning to multiple GPUs across nodes.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.5 Distributed Inference**\n",
    "\n",
    "For serving predictions at scale, we need to distribute the inference load across multiple instances. This can be done by:\n",
    "\n",
    "- **Load balancing**: Requests are routed to any available instance. Suitable when the model is small and stateless.\n",
    "- **Sharding**: Each instance is responsible for a subset of symbols. This can improve cache locality and reduce model loading overhead.\n",
    "\n",
    "### **85.5.1 Sharding by Symbol**\n",
    "\n",
    "We can partition the symbol space (e.g., by hash) and assign each partition to a dedicated inference service. A gateway receives requests and forwards them to the appropriate shard.\n",
    "\n",
    "```python\n",
    "# Simplified gateway\n",
    "class PredictionGateway:\n",
    "    def __init__(self, shard_map):\n",
    "        self.shard_map = shard_map  # e.g., {0: 'http://shard0:8000', 1: 'http://shard1:8000'}\n",
    "    \n",
    "    def predict(self, symbol, date):\n",
    "        shard_id = hash(symbol) % len(self.shard_map)\n",
    "        url = self.shard_map[shard_id]\n",
    "        # Forward request to shard\n",
    "        response = requests.post(f\"{url}/predict\", json={\"symbol\": symbol, \"date\": date})\n",
    "        return response.json()\n",
    "```\n",
    "\n",
    "Each shard loads only the models for its assigned symbols, reducing memory usage and allowing independent scaling.\n",
    "\n",
    "### **85.5.2 Load Balancing with Kubernetes**\n",
    "\n",
    "If the model is small and stateless, we can deploy multiple replicas behind a load balancer. Kubernetes services provide this natively. Each replica runs the same prediction service and can handle any request.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.6 Data Partitioning (Sharding) Strategies**\n",
    "\n",
    "In distributed systems, data is often partitioned to enable parallel processing. For time\u2011series, common partitioning keys are:\n",
    "\n",
    "- **By symbol**: Each partition contains all data for a subset of symbols. This works well for per\u2011symbol operations (feature engineering, training, inference).\n",
    "- **By time**: Partition by date ranges (e.g., monthly). This is useful for time\u2011series joins or when processing windows that span all symbols.\n",
    "- **Hybrid**: Partition by symbol first, then by time within each symbol.\n",
    "\n",
    "In Dask, you can set the partition index when reading data.\n",
    "\n",
    "```python\n",
    "# Partition by symbol (assuming symbol column)\n",
    "df = dd.read_parquet('data.parquet').set_index('symbol')\n",
    "# Now operations grouped by symbol will be efficient.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **85.7 Consistency and Fault Tolerance**\n",
    "\n",
    "### **85.7.1 Consistency Models**\n",
    "In distributed training, we often use **asynchronous** or **synchronous** gradient updates:\n",
    "- **Synchronous**: All workers compute gradients on their batch, then average them. This ensures consistent model updates but can be slow if workers are stragglers.\n",
    "- **Asynchronous**: Workers update the model independently. This is faster but can lead to stale gradients and less stable convergence.\n",
    "\n",
    "Most frameworks support both; the choice depends on the application.\n",
    "\n",
    "### **85.7.2 Fault Tolerance**\n",
    "Distributed systems must handle node failures. Common techniques:\n",
    "\n",
    "- **Checkpointing**: Periodically save model state to durable storage. If a node fails, restart from the latest checkpoint.\n",
    "- **Replication**: Run multiple copies of critical components.\n",
    "- **Task retries**: If a task fails, reschedule it on another node (Dask and Spark do this automatically).\n",
    "\n",
    "Dask can recover from worker failures by re\u2011running lost tasks. For long\u2011running training jobs, use checkpointing with libraries like PyTorch Lightning.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.8 Case Study: Scaling NEPSE Prediction with Dask**\n",
    "\n",
    "Let's design a distributed pipeline for the NEPSE system that processes data for 1000 stocks.\n",
    "\n",
    "**Components**:\n",
    "1. **Raw data stored in Parquet**, partitioned by symbol.\n",
    "2. **Feature engineering** using Dask delayed per symbol.\n",
    "3. **Model training** using Dask\u2011XGBoost on the full feature set.\n",
    "4. **Distributed inference** with sharding by symbol.\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "from dask.distributed import Client\n",
    "import dask\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "\n",
    "# Start Dask client\n",
    "client = Client(n_workers=4, threads_per_worker=2, memory_limit='4GB')\n",
    "\n",
    "# Step 1: Read partitioned data\n",
    "# Assume data is stored as parquet files partitioned by symbol\n",
    "# e.g., 'data/symbol=AAPL/*.parquet', 'data/symbol=GOOG/*.parquet'\n",
    "df = dd.read_parquet('data/partitions/symbol=*/*.parquet')\n",
    "\n",
    "# Step 2: Feature engineering per symbol (using map_partitions)\n",
    "def engineer_features(partition):\n",
    "    # partition is a pandas DataFrame for a single symbol (due to partitioning)\n",
    "    partition = partition.sort_values('date')\n",
    "    partition['lag_1'] = partition['close'].shift(1)\n",
    "    partition['lag_5'] = partition['close'].shift(5)\n",
    "    partition['sma_10'] = partition['close'].rolling(10).mean()\n",
    "    partition['volatility'] = partition['close'].rolling(20).std()\n",
    "    partition['target'] = partition['close'].shift(-1)  # next day close\n",
    "    return partition.dropna()\n",
    "\n",
    "df_feat = df.map_partitions(engineer_features)\n",
    "\n",
    "# Step 3: Prepare data for training\n",
    "# Drop rows with NaN and select features/target\n",
    "feature_cols = ['lag_1', 'lag_5', 'sma_10', 'volatility']\n",
    "X = df_feat[feature_cols]\n",
    "y = df_feat['target']\n",
    "\n",
    "# Step 4: Distributed training with XGBoost\n",
    "dtrain = xgb.dask.DaskDMatrix(client, X, y)\n",
    "params = {'objective': 'reg:squarederror', 'max_depth': 5, 'eta': 0.1}\n",
    "output = xgb.dask.train(client, params, dtrain, num_boost_round=100)\n",
    "model = output['booster']\n",
    "\n",
    "# Step 5: Save model\n",
    "model.save_model('nepse_model.json')\n",
    "\n",
    "# Step 6: Distributed inference (on new data)\n",
    "# Assume new_data is a Dask DataFrame for today's features\n",
    "new_data = ...  # load\n",
    "X_new = new_data[feature_cols]\n",
    "predictions = xgb.dask.predict(client, model, X_new).compute()\n",
    "\n",
    "print(predictions.head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- By storing data partitioned by symbol, Dask can process each symbol independently and in parallel (`map_partitions` ensures that each partition corresponds to a single symbol).\n",
    "- XGBoost training is distributed across the Dask workers, leveraging all available cores.\n",
    "- Inference is also distributed, with predictions computed in parallel.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.9 Monitoring and Debugging Distributed Systems**\n",
    "\n",
    "Distributed systems are harder to debug. Essential tools:\n",
    "\n",
    "- **Dask dashboard**: Provides real\u2011time metrics on task execution, memory, and worker status.\n",
    "- **Ray dashboard**: Similar for Ray.\n",
    "- **Logging**: Centralised logging (e.g., ELK stack) to aggregate logs from all nodes.\n",
    "- **Distributed tracing**: Tools like Jaeger can trace requests across services (useful for inference pipelines).\n",
    "\n",
    "Dask's dashboard is particularly useful: you can see which tasks are running, memory usage, and identify bottlenecks.\n",
    "\n",
    "---\n",
    "\n",
    "## **85.10 Best Practices and Trade\u2011offs**\n",
    "\n",
    "- **Start with a single machine** and scale only when necessary. Distributed systems add complexity.\n",
    "- **Choose the right level of parallelism**: For many time\u2011series tasks, per\u2011symbol parallelism is natural and easy.\n",
    "- **Monitor resource usage**: Ensure that workers are not overloaded or underutilised.\n",
    "- **Handle data skew**: Some symbols may have much more data than others, causing stragglers. Use partitioning strategies to balance load.\n",
    "- **Use columnar storage (Parquet)** for efficient I/O.\n",
    "- **For training**, consider whether you need distributed training or if a single GPU suffices. Many time\u2011series datasets are small enough to fit on one machine.\n",
    "- **For inference**, consider caching models and precomputing features to reduce latency.\n",
    "- **Plan for failure**: Use checkpointing and retries.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored distributed systems for time\u2011series prediction. We covered fundamental concepts, parallelism strategies, and practical implementations using Dask and Ray. We demonstrated distributed feature engineering, training with XGBoost, and inference sharding. We also discussed data partitioning, consistency, fault tolerance, and monitoring. Distributed systems enable us to scale our NEPSE prediction system to thousands of stocks and high\u2011throughput requests, but they come with increased complexity. The key is to apply them judiciously where the benefits outweigh the costs.\n",
    "\n",
    "In the next chapter, we will continue with **Development Best Practices**, focusing on code quality, testing, and documentation.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 85**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='84. real_time_learning_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../12. industry_best_practices_and_standards/86. development_best_practices.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}