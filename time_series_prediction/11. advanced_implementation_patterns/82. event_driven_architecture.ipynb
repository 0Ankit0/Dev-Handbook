{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 82: Event-Driven Architecture\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the principles of event‑driven architecture (EDA) and how it complements microservices.\n",
    "- Distinguish between event notifications, event‑carried state transfer, and event sourcing.\n",
    "- Design events and topics for a time‑series prediction system (e.g., NEPSE data updates, feature changes, model retraining triggers).\n",
    "- Implement event producers and consumers using Apache Kafka.\n",
    "- Apply stream processing to transform event streams in real time.\n",
    "- Understand the concepts of event sourcing and Command Query Responsibility Segregation (CQRS).\n",
    "- Handle event ordering, idempotency, and exactly‑once processing.\n",
    "- Integrate event‑driven patterns with the monitoring and alerting system from Chapter 73.\n",
    "- Evaluate the benefits and challenges of event‑driven architectures for time‑series systems.\n",
    "\n",
    "---\n",
    "\n",
    "## **82.1 Introduction to Event‑Driven Architecture**\n",
    "\n",
    "Event‑driven architecture (EDA) is a software architecture pattern in which components communicate by producing and consuming events. An event is a significant change in state – for example, “new NEPSE data arrived”, “feature vector computed”, “model retrained”. Events are captured as messages and published to an event broker (like Apache Kafka, RabbitMQ, or AWS Kinesis). Other services subscribe to relevant events and react accordingly.\n",
    "\n",
    "EDA is a natural fit for time‑series prediction systems because:\n",
    "\n",
    "- Data arrives continuously (e.g., daily CSV, real‑time sensor readings).\n",
    "- Multiple downstream processes depend on the same data (feature engineering, monitoring, alerting).\n",
    "- Different components have different latency requirements (e.g., real‑time predictions vs. batch model retraining).\n",
    "- It decouples producers and consumers, making the system more scalable and resilient.\n",
    "\n",
    "In the context of our NEPSE system, we can use events to:\n",
    "\n",
    "- Notify the feature service when new raw data is available.\n",
    "- Trigger model retraining when enough new data has accumulated.\n",
    "- Send predictions to a monitoring service for drift detection.\n",
    "- Alert operators when anomalies are detected (as in Chapter 73).\n",
    "\n",
    "In this chapter, we will design an event‑driven version of the NEPSE system, building on the microservices introduced in Chapter 81.\n",
    "\n",
    "---\n",
    "\n",
    "## **82.2 Core Concepts of Event‑Driven Architecture**\n",
    "\n",
    "### **82.2.1 Events**\n",
    "An event is a record of something that happened. It typically contains:\n",
    "\n",
    "- **Event type** (e.g., `DataIngested`, `FeaturesComputed`, `PredictionMade`).\n",
    "- **Timestamp** (when the event occurred).\n",
    "- **Payload** (data relevant to the event, e.g., symbol, date, values).\n",
    "- **Metadata** (e.g., version, producer ID).\n",
    "\n",
    "Events are immutable facts. Once recorded, they should not be changed.\n",
    "\n",
    "### **82.2.2 Event Broker**\n",
    "The event broker is the backbone of EDA. It receives events from producers, stores them durably, and makes them available to consumers. Apache Kafka is a popular choice because it provides:\n",
    "\n",
    "- High throughput, low latency.\n",
    "- Persistent storage (events can be replayed).\n",
    "- Partitioning for scalability.\n",
    "- Exactly‑once semantics (with appropriate configuration).\n",
    "\n",
    "### **82.2.3 Producers and Consumers**\n",
    "- **Producer**: A service that creates events and publishes them to the broker.\n",
    "- **Consumer**: A service that subscribes to certain event types and processes them.\n",
    "\n",
    "A service can be both a producer and a consumer (e.g., the feature service consumes `DataIngested` events and produces `FeaturesComputed` events).\n",
    "\n",
    "### **82.2.4 Topics and Partitions**\n",
    "Events are organised into **topics** (e.g., `raw-data`, `features`, `predictions`). Topics are divided into **partitions** to allow parallel processing. Events with the same key (e.g., symbol) are sent to the same partition, preserving order.\n",
    "\n",
    "### **82.2.5 Event Patterns**\n",
    "There are several common patterns:\n",
    "\n",
    "- **Event Notification**: A simple signal that something happened; the consumer may need to fetch more data. (e.g., “new data available” – consumer then calls an API to get it).\n",
    "- **Event‑Carried State Transfer**: The event contains all the data needed; the consumer can update its own state without further requests. (e.g., “feature vector computed” with the vector included).\n",
    "- **Event Sourcing**: All changes to application state are stored as a sequence of events. The current state can be reconstructed by replaying events.\n",
    "\n",
    "For the NEPSE system, we will use a mix: event‑carried state transfer for features (so prediction service doesn’t need to recompute), and event notification for retraining triggers.\n",
    "\n",
    "---\n",
    "\n",
    "## **82.3 Designing Events for the NEPSE System**\n",
    "\n",
    "Let's define the key events in our system.\n",
    "\n",
    "### **82.3.1 `DataIngested`**\n",
    "Produced by the Data Ingestion Service when new raw data is available.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"event_type\": \"DataIngested\",\n",
    "  \"timestamp\": \"2023-06-01T12:00:00Z\",\n",
    "  \"producer\": \"ingestion-service\",\n",
    "  \"payload\": {\n",
    "    \"symbol\": \"NEPSE\",\n",
    "    \"date\": \"2023-06-01\",\n",
    "    \"data_location\": \"s3://nepse-raw/2023-06-01.parquet\",\n",
    "    \"row_count\": 350\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **82.3.2 `FeaturesComputed`**\n",
    "Produced by the Feature Service after computing features for a given symbol and date. Contains the full feature vector (event‑carried state transfer).\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"event_type\": \"FeaturesComputed\",\n",
    "  \"timestamp\": \"2023-06-01T12:05:00Z\",\n",
    "  \"producer\": \"feature-service\",\n",
    "  \"payload\": {\n",
    "    \"symbol\": \"NEPSE\",\n",
    "    \"date\": \"2023-06-01\",\n",
    "    \"features\": {\n",
    "      \"Close_Lag_1\": 1250.5,\n",
    "      \"SMA_20\": 1245.2,\n",
    "      \"RSI\": 62.3,\n",
    "      \"Volume_Z_Score\": 1.2,\n",
    "      ...\n",
    "    }\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **82.3.3 `PredictionMade`**\n",
    "Produced by the Prediction Service each time a prediction is made. Used for monitoring and drift detection.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"event_type\": \"PredictionMade\",\n",
    "  \"timestamp\": \"2023-06-01T12:10:00Z\",\n",
    "  \"producer\": \"prediction-service\",\n",
    "  \"payload\": {\n",
    "    \"symbol\": \"NEPSE\",\n",
    "    \"date\": \"2023-06-01\",\n",
    "    \"predicted_close\": 1275.3,\n",
    "    \"model_version\": \"v2.3.1\",\n",
    "    \"features_used\": [\"Close_Lag_1\", \"SMA_20\", \"RSI\", ...]\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **82.3.4 `ActualAvailable`**\n",
    "Produced later when the actual closing price becomes known (e.g., next day). Used to compute error.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"event_type\": \"ActualAvailable\",\n",
    "  \"timestamp\": \"2023-06-02T12:00:00Z\",\n",
    "  \"producer\": \"ingestion-service\",\n",
    "  \"payload\": {\n",
    "    \"symbol\": \"NEPSE\",\n",
    "    \"date\": \"2023-06-01\",\n",
    "    \"actual_close\": 1260.2\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **82.3.5 `ModelRetrainingTriggered`**\n",
    "Produced by a scheduler or a monitoring service when conditions for retraining are met (e.g., after 100 new data points).\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"event_type\": \"ModelRetrainingTriggered\",\n",
    "  \"timestamp\": \"2023-06-01T12:00:00Z\",\n",
    "  \"producer\": \"retraining-scheduler\",\n",
    "  \"payload\": {\n",
    "    \"reason\": \"weekly_schedule\",\n",
    "    \"data_end_date\": \"2023-06-01\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### **82.3.6 `ModelTrained`**\n",
    "Produced by the Training Service after a new model is trained and registered.\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"event_type\": \"ModelTrained\",\n",
    "  \"timestamp\": \"2023-06-01T14:00:00Z\",\n",
    "  \"producer\": \"training-service\",\n",
    "  \"payload\": {\n",
    "    \"model_version\": \"v2.4.0\",\n",
    "    \"performance\": {\"mae\": 12.3, \"rmse\": 18.7},\n",
    "    \"features_used\": [\"Close_Lag_1\", \"SMA_20\", \"RSI\", \"Volume_Z_Score\"],\n",
    "    \"artifact_location\": \"s3://nepse-models/v2.4.0/model.pkl\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **82.4 Implementing an Event Broker with Apache Kafka**\n",
    "\n",
    "We'll use Apache Kafka as the event broker. For local development, we can run Kafka using Docker Compose. Here's a minimal `docker-compose.yml`:\n",
    "\n",
    "```yaml\n",
    "version: '3'\n",
    "services:\n",
    "  zookeeper:\n",
    "    image: confluentinc/cp-zookeeper:latest\n",
    "    environment:\n",
    "      ZOOKEEPER_CLIENT_PORT: 2181\n",
    "      ZOOKEEPER_TICK_TIME: 2000\n",
    "    ports:\n",
    "      - 2181:2181\n",
    "\n",
    "  kafka:\n",
    "    image: confluentinc/cp-kafka:latest\n",
    "    depends_on:\n",
    "      - zookeeper\n",
    "    ports:\n",
    "      - 9092:9092\n",
    "    environment:\n",
    "      KAFKA_BROKER_ID: 1\n",
    "      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181\n",
    "      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092\n",
    "      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1\n",
    "```\n",
    "\n",
    "Start with `docker-compose up -d`.\n",
    "\n",
    "### **82.4.1 Producing Events in Python**\n",
    "\n",
    "We'll use the `kafka-python` library. Install with `pip install kafka-python`.\n",
    "\n",
    "```python\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "class EventProducer:\n",
    "    def __init__(self, bootstrap_servers='localhost:9092'):\n",
    "        self.producer = KafkaProducer(\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            value_serializer=lambda v: json.dumps(v).encode('utf-8'),\n",
    "            key_serializer=lambda k: k.encode('utf-8') if k else None\n",
    "        )\n",
    "    \n",
    "    def publish(self, topic, event, key=None):\n",
    "        \"\"\"\n",
    "        Publish an event to a topic.\n",
    "        key is optional; if provided, ensures events with same key go to same partition.\n",
    "        \"\"\"\n",
    "        future = self.producer.send(topic, value=event, key=key)\n",
    "        result = future.get(timeout=10)  # wait for acknowledgement\n",
    "        return result\n",
    "\n",
    "# Example usage\n",
    "producer = EventProducer()\n",
    "event = {\n",
    "    \"event_type\": \"DataIngested\",\n",
    "    \"timestamp\": \"2023-06-01T12:00:00Z\",\n",
    "    \"producer\": \"ingestion-service\",\n",
    "    \"payload\": {\n",
    "        \"symbol\": \"NEPSE\",\n",
    "        \"date\": \"2023-06-01\",\n",
    "        \"data_location\": \"s3://nepse-raw/2023-06-01.parquet\"\n",
    "    }\n",
    "}\n",
    "producer.publish(\"raw-data\", event, key=\"NEPSE\")\n",
    "```\n",
    "\n",
    "### **82.4.2 Consuming Events**\n",
    "\n",
    "```python\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "class EventConsumer:\n",
    "    def __init__(self, topics, bootstrap_servers='localhost:9092', group_id='my-group'):\n",
    "        self.consumer = KafkaConsumer(\n",
    "            *topics,\n",
    "            bootstrap_servers=bootstrap_servers,\n",
    "            group_id=group_id,\n",
    "            value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "            key_deserializer=lambda m: m.decode('utf-8') if m else None,\n",
    "            auto_offset_reset='earliest',  # start from beginning if no offset\n",
    "            enable_auto_commit=True\n",
    "        )\n",
    "    \n",
    "    def consume(self, handler):\n",
    "        for message in self.consumer:\n",
    "            handler(message.topic, message.key, message.value)\n",
    "\n",
    "# Example handler\n",
    "def handle_event(topic, key, event):\n",
    "    print(f\"Received event on {topic}: key={key}, type={event['event_type']}\")\n",
    "    # Process event...\n",
    "\n",
    "consumer = EventConsumer(topics=['raw-data', 'features'])\n",
    "consumer.consume(handle_event)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **82.5 Stream Processing**\n",
    "\n",
    "Sometimes we need to transform or enrich event streams in real time. This is **stream processing**. For example, we might want to compute a running average of prediction errors from `PredictionMade` and `ActualAvailable` events.\n",
    "\n",
    "Apache Kafka provides **Kafka Streams** (Java), but for Python we can use **Faust** or a simpler approach with consumers and producers.\n",
    "\n",
    "### **82.5.1 Example: Computing Prediction Error Stream**\n",
    "\n",
    "We'll create a service that consumes `PredictionMade` and `ActualAvailable` events, joins them by symbol and date, and produces an `ErrorComputed` event.\n",
    "\n",
    "```python\n",
    "# error_computation_service.py\n",
    "from kafka import KafkaConsumer, KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'predictions', 'actuals',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    group_id='error-computation',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8')),\n",
    "    enable_auto_commit=False\n",
    ")\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    ")\n",
    "\n",
    "# Store predictions and actuals in a simple dictionary (in production, use a state store)\n",
    "predictions = {}\n",
    "actuals = {}\n",
    "\n",
    "for msg in consumer:\n",
    "    topic = msg.topic\n",
    "    event = msg.value\n",
    "    key = msg.key.decode() if msg.key else None\n",
    "    payload = event['payload']\n",
    "    symbol = payload.get('symbol')\n",
    "    date = payload.get('date')\n",
    "    if topic == 'predictions':\n",
    "        predictions[(symbol, date)] = payload['predicted_close']\n",
    "    elif topic == 'actuals':\n",
    "        actuals[(symbol, date)] = payload['actual_close']\n",
    "    \n",
    "    # If we have both for a given key, compute error\n",
    "    if (symbol, date) in predictions and (symbol, date) in actuals:\n",
    "        pred = predictions[(symbol, date)]\n",
    "        actual = actuals[(symbol, date)]\n",
    "        error = pred - actual\n",
    "        abs_error = abs(error)\n",
    "        pct_error = (error / actual) * 100 if actual != 0 else None\n",
    "        \n",
    "        error_event = {\n",
    "            'event_type': 'ErrorComputed',\n",
    "            'timestamp': time.time(),\n",
    "            'producer': 'error-computation',\n",
    "            'payload': {\n",
    "                'symbol': symbol,\n",
    "                'date': date,\n",
    "                'predicted': pred,\n",
    "                'actual': actual,\n",
    "                'error': error,\n",
    "                'abs_error': abs_error,\n",
    "                'pct_error': pct_error\n",
    "            }\n",
    "        }\n",
    "        producer.send('errors', value=error_event, key=key)\n",
    "        # Remove processed items (optional)\n",
    "        del predictions[(symbol, date)]\n",
    "        del actuals[(symbol, date)]\n",
    "    \n",
    "    consumer.commit()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This service consumes from two topics, `predictions` and `actuals`.\n",
    "- It stores each event in a local dictionary (in a real system, you'd use a persistent state store like RocksDB to handle restarts).\n",
    "- When both a prediction and actual are available for the same (symbol, date), it computes the error and publishes an `ErrorComputed` event.\n",
    "- This event can then be consumed by the monitoring service to track model performance and trigger alerts.\n",
    "\n",
    "---\n",
    "\n",
    "## **82.6 Event Sourcing and CQRS**\n",
    "\n",
    "**Event Sourcing** is a pattern where state changes are stored as a sequence of events. Instead of storing the current state, you store all events, and the current state is derived by replaying them. This provides a complete audit log and allows rebuilding state at any point in time.\n",
    "\n",
    "**CQRS (Command Query Responsibility Segregation)** separates the write side (commands) from the read side (queries). Often used with event sourcing: commands produce events, and the read side builds projections from those events.\n",
    "\n",
    "For the NEPSE system, we might use event sourcing for the feature store. Instead of storing the latest feature vector, we store every `FeaturesComputed` event. When we need the current features, we replay the events for that symbol to get the latest. This adds complexity but gives a complete history.\n",
    "\n",
    "A simpler approach is to use **event‑carried state transfer** (as we did with `FeaturesComputed`) and store the latest state in a database. This is more practical for most use cases.\n",
    "\n",
    "---\n",
    "\n",
    "## **82.7 Handling Event Ordering, Idempotency, and Exactly‑Once Processing**\n",
    "\n",
    "### **82.7.1 Ordering**\n",
    "Kafka preserves order **within a partition** if you use the same key. For example, if we set the key to `symbol`, all events for NEPSE will go to the same partition, and consumers will see them in order. This is important for stateful operations (e.g., computing features from raw data, where order matters).\n",
    "\n",
    "### **82.7.2 Idempotency**\n",
    "Consumers may receive the same event multiple times (e.g., after a rebalance). Processing must be idempotent: applying the same event twice should have the same effect as applying it once. For example, when storing a feature vector, use a upsert operation keyed by (symbol, date). If the event is replayed, it simply overwrites the existing value.\n",
    "\n",
    "### **82.7.3 Exactly‑Once Processing**\n",
    "Exactly‑once semantics guarantee that each event is processed exactly once, even in the face of failures. Kafka supports exactly‑once semantics for streams, but it requires careful configuration (enable.idempotence, transactional.id). For many applications, at‑least‑once with idempotent processing is sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "## **82.8 Integrating with the Alerting System (Chapter 73)**\n",
    "\n",
    "Our event‑driven system can feed directly into the alerting framework. For example, the monitoring service can consume `ErrorComputed` events and, if the error exceeds a threshold, trigger an alert via the `AlertManager`.\n",
    "\n",
    "```python\n",
    "# monitoring_service.py (partial)\n",
    "from alerting import AlertManager, AlertRule\n",
    "\n",
    "alert_manager = AlertManager()\n",
    "# ... register channels and rules ...\n",
    "\n",
    "def handle_error_event(event):\n",
    "    payload = event['payload']\n",
    "    abs_error = payload['abs_error']\n",
    "    symbol = payload['symbol']\n",
    "    if abs_error > 50:  # threshold\n",
    "        alert_manager.process_row({\n",
    "            'symbol': symbol,\n",
    "            'abs_error': abs_error,\n",
    "            'timestamp': event['timestamp']\n",
    "        })\n",
    "\n",
    "# In the consumer loop, after receiving an ErrorComputed event\n",
    "handle_error_event(event)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **82.9 Case Study: Event‑Driven NEPSE System**\n",
    "\n",
    "Let's sketch the complete event flow for the NEPSE system:\n",
    "\n",
    "1. **Data Ingestion Service** reads new CSV and publishes `DataIngested` events to `raw-data` topic.\n",
    "2. **Feature Service** consumes `raw-data`, computes features, and publishes `FeaturesComputed` to `features` topic.\n",
    "3. **Prediction Service** consumes `features` (event‑carried state transfer), retrieves the current model from registry, predicts, and publishes `PredictionMade` to `predictions` topic.\n",
    "4. **Monitoring Service** consumes `predictions` and, later, `ActualAvailable` events (from ingestion), computes errors, and publishes `ErrorComputed` to `errors` topic.\n",
    "5. **Alerting** consumes `errors` and triggers notifications when thresholds are breached.\n",
    "6. **Retraining Scheduler** (a cron job) publishes `ModelRetrainingTriggered` to `training` topic periodically.\n",
    "7. **Training Service** consumes `training` and `features` (for training data), trains a new model, and publishes `ModelTrained` to `models` topic.\n",
    "8. **Prediction Service** subscribes to `models` to be notified when a new model is available; it can then load the new model for future predictions.\n",
    "\n",
    "All services are loosely coupled, communicating only through events. This makes the system scalable and resilient.\n",
    "\n",
    "---\n",
    "\n",
    "## **82.10 Benefits and Challenges of Event‑Driven Architecture**\n",
    "\n",
    "### **Benefits**\n",
    "- **Decoupling**: Services evolve independently.\n",
    "- **Scalability**: Each component can scale based on its load; event partitioning allows parallel processing.\n",
    "- **Resilience**: If a consumer fails, events are still in the broker and can be replayed.\n",
    "- **Auditability**: The event log provides a complete history of what happened.\n",
    "- **Real‑time reactions**: Events can trigger immediate downstream processing.\n",
    "\n",
    "### **Challenges**\n",
    "- **Complexity**: More moving parts; requires good monitoring and debugging tools.\n",
    "- **Eventual consistency**: Systems are eventually consistent, which may be hard to reason about.\n",
    "- **Message ordering**: Ensuring correct order across partitions requires careful design.\n",
    "- **Idempotency**: Consumers must handle duplicate events gracefully.\n",
    "- **Schema evolution**: Events change over time; need to manage versions (e.g., with Avro or Protobuf).\n",
    "\n",
    "---\n",
    "\n",
    "## **82.11 Best Practices**\n",
    "\n",
    "- **Define clear event schemas** and version them. Use schema registries (e.g., Confluent Schema Registry) to enforce compatibility.\n",
    "- **Use keys for ordering** when needed (e.g., symbol).\n",
    "- **Make events self‑contained** (event‑carried state transfer) when possible to reduce coupling.\n",
    "- **Design for idempotency** – every consumer should be able to process the same event twice without ill effect.\n",
    "- **Monitor consumer lag** – if consumers fall behind, alerts should fire.\n",
    "- **Test with failure scenarios** – kill a consumer, restart a broker, and ensure the system recovers.\n",
    "- **Use dead letter queues** for events that cannot be processed.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored event‑driven architecture and its application to time‑series prediction systems. We defined key events for the NEPSE system, implemented producers and consumers with Apache Kafka, and demonstrated stream processing to compute prediction errors. We discussed event sourcing, CQRS, and the importance of idempotency and ordering. By integrating events with our alerting framework, we built a fully decoupled, scalable system. Event‑driven architecture is a powerful pattern for systems that need to react to continuous data streams, and it complements the microservices approach from Chapter 81.\n",
    "\n",
    "In the next chapter, we will delve into **Multi‑Model Systems**, where multiple prediction models are combined or routed dynamically.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 82**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
