{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 84: Real-Time Learning Systems\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the difference between batch learning and real\u2011time (online) learning.\n",
    "- Identify scenarios where real\u2011time learning is beneficial, especially in time\u2011series prediction.\n",
    "- Implement incremental learning algorithms using libraries like `scikit\u2011learn` (with `partial_fit`) and `river`.\n",
    "- Handle streaming feature engineering and maintain state across time.\n",
    "- Detect and adapt to concept drift using statistical tests and adaptive models.\n",
    "- Build a real\u2011time learning system for the NEPSE stock prediction problem, where the model updates as new daily data arrives.\n",
    "- Evaluate model performance in a streaming context using prequential (test\u2011then\u2011train) evaluation.\n",
    "- Deploy real\u2011time learning systems with considerations for latency, state management, and monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "## **84.1 Introduction to Real\u2011Time Learning**\n",
    "\n",
    "In traditional machine learning, models are trained on a fixed historical dataset and then deployed. This is **batch learning**. However, many time\u2011series applications, including stock prediction, demand forecasting, and IoT analytics, generate data continuously. Batch learning has several limitations:\n",
    "\n",
    "- Models become stale as new data arrives, leading to performance degradation (concept drift).\n",
    "- Retraining from scratch periodically is computationally expensive and may not keep up with the data rate.\n",
    "- The model cannot adapt to recent patterns until the next retraining cycle.\n",
    "\n",
    "**Real\u2011time learning** (also called online learning or incremental learning) addresses these issues by updating the model incrementally as each new data point arrives. The model learns continuously, adapting to changes in the underlying data distribution. This is particularly valuable in financial markets like NEPSE, where market conditions can shift rapidly due to news, regulations, or macroeconomic events.\n",
    "\n",
    "In this chapter, we will build a real\u2011time learning system for the NEPSE prediction task. The model will be updated daily after each new trading day's data becomes available, allowing it to adapt to recent market behaviour.\n",
    "\n",
    "---\n",
    "\n",
    "## **84.2 Online Learning vs. Batch Learning**\n",
    "\n",
    "### **84.2.1 Key Differences**\n",
    "\n",
    "| Aspect | Batch Learning | Online Learning |\n",
    "|--------|----------------|-----------------|\n",
    "| Training data | All historical data at once | One instance at a time (or mini\u2011batches) |\n",
    "| Model update | Infrequent, expensive | Incremental, cheap |\n",
    "| Adaptation to change | Only after retraining | Continuous |\n",
    "| Memory usage | High (stores all data) | Low (only current model state) |\n",
    "| Latency | High for retraining | Low for each update |\n",
    "| Evaluation | Train/validation/test split | Prequential (test\u2011then\u2011train) |\n",
    "\n",
    "### **84.2.2 When to Use Online Learning**\n",
    "\n",
    "Online learning is ideal when:\n",
    "\n",
    "- Data arrives in a stream (e.g., sensor readings, stock ticks).\n",
    "- The underlying concept may change over time (concept drift).\n",
    "- Low\u2011latency predictions are required.\n",
    "- Storage is limited (cannot keep all historical data).\n",
    "- You want the model to personalise quickly to user behaviour.\n",
    "\n",
    "For the NEPSE system, we receive one new data point per day per stock. This is a low\u2011velocity stream, but concept drift (e.g., due to a market crash) can happen suddenly. Online learning allows the model to adapt quickly without waiting for a weekly retraining job.\n",
    "\n",
    "---\n",
    "\n",
    "## **84.3 Incremental Learning Algorithms**\n",
    "\n",
    "Many machine learning algorithms have online versions that support incremental updates. We'll focus on those available in Python.\n",
    "\n",
    "### **84.3.1 `partial_fit` in scikit\u2011learn**\n",
    "\n",
    "Several scikit\u2011learn estimators implement the `partial_fit` method, which allows incremental training. These include:\n",
    "\n",
    "- `SGDRegressor` / `SGDClassifier` (Stochastic Gradient Descent)\n",
    "- `PassiveAggressiveRegressor` / `PassiveAggressiveClassifier`\n",
    "- `Perceptron`\n",
    "- `MiniBatchKMeans`\n",
    "- `IncrementalPCA`\n",
    "\n",
    "These are linear models or simple neural networks. They are suitable for high\u2011dimensional data and can be updated efficiently.\n",
    "\n",
    "### **84.3.2 River Library**\n",
    "\n",
    "[River](https://github.com/online-ml/river) is a Python library specifically designed for online machine learning. It provides a wide range of algorithms, including linear models, decision trees (Hoeffding trees), neural networks, and preprocessing tools. River is well\u2011suited for streaming data because it maintains state and supports incremental learning natively.\n",
    "\n",
    "### **84.3.3 Example: Online Linear Regression with SGD**\n",
    "\n",
    "Let's start with a simple online linear regression using `SGDRegressor` on the NEPSE data.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import SGDRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "# Generate synthetic NEPSE data (as before)\n",
    "def generate_nepse_data(days=1500):\n",
    "    dates = pd.date_range(start='2020-01-01', periods=days, freq='B')\n",
    "    prices = 1000 + np.cumsum(np.random.randn(days) * 5)\n",
    "    df = pd.DataFrame({\n",
    "        'date': dates,\n",
    "        'close': prices,\n",
    "        'volume': np.random.lognormal(12, 1, days)\n",
    "    })\n",
    "    # Add some features\n",
    "    df['lag_1'] = df['close'].shift(1)\n",
    "    df['lag_5'] = df['close'].shift(5)\n",
    "    df['sma_10'] = df['close'].rolling(10).mean()\n",
    "    df['volatility'] = df['close'].rolling(20).std()\n",
    "    df = df.dropna().reset_index(drop=True)\n",
    "    return df\n",
    "\n",
    "df = generate_nepse_data(days=1500)\n",
    "\n",
    "# Features and target (predict next day close)\n",
    "feature_cols = ['lag_1', 'lag_5', 'sma_10', 'volatility', 'volume']\n",
    "X = df[feature_cols]\n",
    "y = df['close'].shift(-1).dropna()\n",
    "X = X.iloc[:-1]  # align\n",
    "\n",
    "# Online learning simulation: we'll iterate through the data in order\n",
    "# We'll use a pipeline with scaling, because SGD is sensitive to feature scale.\n",
    "model = make_pipeline(\n",
    "    StandardScaler(),\n",
    "    SGDRegressor(learning_rate='adaptive', eta0=0.01, random_state=42)\n",
    ")\n",
    "\n",
    "# Prequential evaluation: test on current sample, then train on it\n",
    "predictions = []\n",
    "true_values = []\n",
    "for i in range(len(X)):\n",
    "    X_i = X.iloc[i:i+1]  # keep as DataFrame for scaling\n",
    "    y_i = y.iloc[i]\n",
    "    \n",
    "    # Predict before training (test)\n",
    "    pred = model.predict(X_i)[0]\n",
    "    predictions.append(pred)\n",
    "    true_values.append(y_i)\n",
    "    \n",
    "    # Train on this sample (partial fit)\n",
    "    # Note: for the first sample, the model is not fitted yet; we need to call partial_fit with classes for classification, but for regression we can just call partial_fit.\n",
    "    # However, the pipeline with StandardScaler does not support partial_fit directly. We need to handle scaling manually or use River.\n",
    "    # For simplicity, we'll use River for this example, but here's a workaround:\n",
    "    if i == 0:\n",
    "        # Initialize the model with a dummy partial_fit\n",
    "        model.named_steps['sgdregressor'].partial_fit(X_i, [y_i])\n",
    "    else:\n",
    "        model.named_steps['sgdregressor'].partial_fit(X_i, [y_i])\n",
    "    # Note: StandardScaler is not updated incrementally. This is a limitation.\n",
    "    # In a real online system, you'd use an incremental scaler (e.g., from River).\n",
    "\n",
    "# Compute metrics\n",
    "mae = mean_absolute_error(true_values, predictions)\n",
    "print(f\"Online SGD MAE: {mae:.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We simulate a stream by iterating through the DataFrame in chronological order.\n",
    "- For each new sample, we first predict (test) and then update the model (train). This is **prequential evaluation** (test\u2011then\u2011train).\n",
    "- The `SGDRegressor` is updated with `partial_fit`. However, the `StandardScaler` in the pipeline is not updated incrementally \u2013 it would need to be fitted on the entire dataset beforehand, which violates the online principle. In a true online setting, we need an incremental scaler (like `River`'s `StandardScaler`).\n",
    "\n",
    "This example illustrates the concept but has limitations. Let's now use **River**, which is designed for exactly this purpose.\n",
    "\n",
    "### **84.3.4 Online Learning with River**\n",
    "\n",
    "River provides estimators that are fully incremental, including preprocessing.\n",
    "\n",
    "```python\n",
    "# pip install river\n",
    "from river import linear_model, preprocessing, metrics\n",
    "import numpy as np\n",
    "\n",
    "# Prepare data as stream of dictionaries\n",
    "X_stream = X.to_dict(orient='records')\n",
    "y_stream = y.values\n",
    "\n",
    "# Build a pipeline with an incremental scaler and a linear regression\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "\n",
    "# Initialize metric\n",
    "metric = metrics.MAE()\n",
    "\n",
    "# Prequential evaluation\n",
    "predictions = []\n",
    "for xi, yi in zip(X_stream, y_stream):\n",
    "    # Predict\n",
    "    y_pred = model.predict_one(xi)\n",
    "    if y_pred is not None:\n",
    "        predictions.append(y_pred)\n",
    "        metric.update(yi, y_pred)\n",
    "    # Train\n",
    "    model.learn_one(xi, yi)\n",
    "\n",
    "print(f\"River Linear Regression MAE: {metric.get():.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- River's `StandardScaler` maintains running mean and standard deviation, updating incrementally.\n",
    "- `LinearRegression` in River uses stochastic gradient descent with a configurable optimizer.\n",
    "- `predict_one` and `learn_one` operate on single dictionaries.\n",
    "- The metric is updated after each prediction, giving us a running estimate of MAE.\n",
    "\n",
    "River also provides more sophisticated models like `HoeffdingTreeRegressor`, `AdaptiveRandomForest`, and neural networks. It also includes drift detection methods.\n",
    "\n",
    "---\n",
    "\n",
    "## **84.4 Streaming Feature Engineering**\n",
    "\n",
    "In a real\u2011time learning system, features must be computed incrementally from the stream. For example, lag features require storing recent values; rolling statistics require maintaining a window. River provides stateful transformers that can be used in a pipeline.\n",
    "\n",
    "### **84.4.1 Lag and Rolling Features with River**\n",
    "\n",
    "River has a `Rolling` class that can compute statistics over a sliding window, and `Lag` for lagged values.\n",
    "\n",
    "```python\n",
    "from river import feature_extraction as fx\n",
    "from river import compose\n",
    "\n",
    "# Define a feature extractor that adds lagged values and rolling mean\n",
    "def add_features():\n",
    "    return (\n",
    "        compose.Select('close', 'volume')\n",
    "        | (fx.Lag('close', lags=[1, 5]) + fx.Lag('volume', lags=[1]))\n",
    "        | (fx.Rolling('close', window=10, func='mean') + fx.Rolling('close', window=20, func='std'))\n",
    "    )\n",
    "\n",
    "# We need to simulate a stream of raw data without precomputed features.\n",
    "# For demonstration, we'll create a stream of raw dicts with only 'close' and 'volume'.\n",
    "raw_stream = df[['close', 'volume']].to_dict(orient='records')\n",
    "\n",
    "# We'll combine the feature extractor with a model in a pipeline\n",
    "model = (\n",
    "    add_features() |\n",
    "    preprocessing.StandardScaler() |\n",
    "    linear_model.LinearRegression()\n",
    ")\n",
    "\n",
    "# Now we can process the stream\n",
    "metric = metrics.MAE()\n",
    "for x, y in zip(raw_stream, y_stream):\n",
    "    y_pred = model.predict_one(x)\n",
    "    if y_pred is not None:\n",
    "        metric.update(y, y_pred)\n",
    "    model.learn_one(x, y)\n",
    "\n",
    "print(f\"MAE with streaming features: {metric.get():.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The feature extractor is built using River's `compose` and `feature_extraction` modules. It adds lag features and rolling statistics on the fly, maintaining internal state (e.g., a deque for rolling windows).\n",
    "- This allows us to feed raw data directly and get engineered features incrementally.\n",
    "- The model learns from each sample after prediction.\n",
    "\n",
    "---\n",
    "\n",
    "## **84.5 Handling Concept Drift**\n",
    "\n",
    "Concept drift occurs when the statistical properties of the target variable change over time. In financial markets, drift is common due to changing volatility, new regulations, or market sentiment shifts. Online learning systems must detect and adapt to drift.\n",
    "\n",
    "### **84.5.1 Types of Drift**\n",
    "\n",
    "- **Sudden drift**: abrupt change (e.g., after a news event).\n",
    "- **Gradual drift**: slow change over time (e.g., evolving market trends).\n",
    "- **Recurring concepts**: patterns that reappear (e.g., seasonal effects).\n",
    "\n",
    "### **84.5.2 Drift Detection Methods**\n",
    "\n",
    "River provides several drift detectors:\n",
    "\n",
    "- **ADWIN** (Adaptive Windowing): keeps a sliding window and detects changes when the mean of two sub\u2011windows differs significantly.\n",
    "- **Page-Hinkley**: sequential test for change in the mean of a signal.\n",
    "- **DDM** (Drift Detection Method): monitors error rate of a model.\n",
    "\n",
    "We can use these detectors to trigger model adaptation, such as resetting the model or switching to a new model.\n",
    "\n",
    "### **84.5.3 Adaptive Models**\n",
    "\n",
    "River also includes models that are inherently adaptive, such as:\n",
    "\n",
    "- `HoeffdingTreeRegressor` (incrementally builds a decision tree)\n",
    "- `AdaptiveRandomForestRegressor` (ensemble that adapts to drift)\n",
    "- `EWARegressor` (Exponentially Weighted Average, which gives more weight to recent data)\n",
    "\n",
    "### **84.5.4 Example: Using ADWIN to Detect Drift in Prediction Errors**\n",
    "\n",
    "We can monitor the prediction error (e.g., absolute error) with ADWIN. If drift is detected, we might reset the model or switch to a more recent model.\n",
    "\n",
    "```python\n",
    "from river import drift\n",
    "\n",
    "# Initialize ADWIN detector\n",
    "adwin = drift.ADWIN()\n",
    "\n",
    "# Run online learning and check for drift\n",
    "model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "metric = metrics.MAE()\n",
    "errors = []\n",
    "\n",
    "for xi, yi in zip(X_stream, y_stream):\n",
    "    y_pred = model.predict_one(xi)\n",
    "    if y_pred is not None:\n",
    "        error = abs(yi - y_pred)\n",
    "        errors.append(error)\n",
    "        adwin.update(error)\n",
    "        if adwin.change_detected:\n",
    "            print(f\"Drift detected at step {len(errors)}! Mean error changed.\")\n",
    "            # Optionally reset model or adapt\n",
    "            # model = preprocessing.StandardScaler() | linear_model.LinearRegression()\n",
    "            adwin.reset()  # reset detector after handling\n",
    "    model.learn_one(xi, yi)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- ADWIN monitors the stream of absolute errors. If it detects a significant change in the mean error, `change_detected` becomes True.\n",
    "- We can then take action, such as resetting the model (starting fresh) or switching to a different model.\n",
    "\n",
    "---\n",
    "\n",
    "## **84.6 Implementing a Real\u2011Time Learning System for NEPSE**\n",
    "\n",
    "Let's put it all together into a production\u2011ready real\u2011time learning system for NEPSE. The system will:\n",
    "\n",
    "1. Receive daily raw data (open, high, low, close, volume).\n",
    "2. Compute features incrementally (lags, rolling stats, technical indicators).\n",
    "3. Update the online model (e.g., an adaptive random forest).\n",
    "4. Make predictions for the next day.\n",
    "5. Monitor for drift and adjust.\n",
    "\n",
    "We'll use River's `AdaptiveRandomForestRegressor`, which handles drift internally by weighting recent trees.\n",
    "\n",
    "```python\n",
    "from river import ensemble, metrics, preprocessing, compose, feature_extraction as fx\n",
    "\n",
    "# Define a streaming feature pipeline\n",
    "def feature_pipeline():\n",
    "    return (\n",
    "        compose.Select('close', 'volume', 'open', 'high', 'low')\n",
    "        | (\n",
    "            fx.Lag('close', lags=[1, 2, 5]) +\n",
    "            fx.Lag('volume', lags=[1]) +\n",
    "            fx.Rolling('close', window=5, func='mean') +\n",
    "            fx.Rolling('close', window=10, func='std') +\n",
    "            fx.Rolling('volume', window=5, func='mean')\n",
    "        )\n",
    "        # Add a custom feature: daily return\n",
    "        | compose.FuncTransformer(lambda x: {'return': (x['close'] - x['close_lag_1']) / x['close_lag_1'] if x.get('close_lag_1') else 0})\n",
    "    )\n",
    "\n",
    "# Create the online model\n",
    "model = (\n",
    "    feature_pipeline() |\n",
    "    preprocessing.StandardScaler() |\n",
    "    ensemble.AdaptiveRandomForestRegressor(\n",
    "        n_models=10,\n",
    "        max_depth=10,\n",
    "        seed=42\n",
    "    )\n",
    ")\n",
    "\n",
    "# Prepare raw stream (only raw columns)\n",
    "raw_columns = ['close', 'volume', 'open', 'high', 'low']\n",
    "# We need to simulate a stream where each sample is a dict with these keys.\n",
    "# In practice, you'd get this from your ingestion service.\n",
    "raw_stream = df[raw_columns].to_dict(orient='records')\n",
    "y_stream = df['close'].shift(-1).dropna().values\n",
    "# Align: the last row of raw_stream corresponds to the last target? Need to drop last raw.\n",
    "raw_stream = raw_stream[:-1]\n",
    "\n",
    "# Run online learning\n",
    "metric = metrics.MAE()\n",
    "for xi, yi in zip(raw_stream, y_stream):\n",
    "    # Predict\n",
    "    y_pred = model.predict_one(xi)\n",
    "    if y_pred is not None:\n",
    "        metric.update(yi, y_pred)\n",
    "    # Learn\n",
    "    model.learn_one(xi, yi)\n",
    "    \n",
    "    # Optionally log progress every N steps\n",
    "    # if metric.n_samples % 100 == 0:\n",
    "    #     print(f\"Step {metric.n_samples}: MAE = {metric.get():.2f}\")\n",
    "\n",
    "print(f\"Final MAE: {metric.get():.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The feature pipeline computes lagged values, rolling means and standard deviations, and a custom return feature. All state (windows) is maintained internally by River.\n",
    "- `AdaptiveRandomForestRegressor` is an ensemble of Hoeffding trees that adapts to concept drift by weighting recent trees more heavily and replacing poor performers.\n",
    "- We use a standard scaler that normalizes features incrementally.\n",
    "- The loop processes one day at a time: predict, update metric, then train.\n",
    "\n",
    "This system continuously adapts to the latest NEPSE data and can be deployed in production.\n",
    "\n",
    "---\n",
    "\n",
    "## **84.7 Evaluation in Streaming Context**\n",
    "\n",
    "In batch learning, we split data into train/test. In online learning, we use **prequential evaluation** (test\u2011then\u2011train) as shown above. This gives a realistic estimate of how the model would perform if deployed.\n",
    "\n",
    "We can also compute metrics over a sliding window to track performance over time, which helps detect degradation.\n",
    "\n",
    "```python\n",
    "from river import metrics\n",
    "\n",
    "# Use a windowed MAE to see recent performance\n",
    "window_mae = metrics.Rolling(metrics.MAE(), window_size=30)\n",
    "\n",
    "for xi, yi in zip(raw_stream, y_stream):\n",
    "    y_pred = model.predict_one(xi)\n",
    "    if y_pred is not None:\n",
    "        window_mae.update(yi, y_pred)\n",
    "        # Print every 30 days\n",
    "        if window_mae.n_samples % 30 == 0:\n",
    "            print(f\"Day {window_mae.n_samples}: Rolling MAE = {window_mae.get():.2f}\")\n",
    "    model.learn_one(xi, yi)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `Rolling` computes the metric over the last `window_size` samples, giving a local view of performance.\n",
    "- This is useful for detecting when the model starts to fail (e.g., due to drift).\n",
    "\n",
    "---\n",
    "\n",
    "## **84.8 Deployment Considerations**\n",
    "\n",
    "Deploying a real\u2011time learning system requires careful design.\n",
    "\n",
    "### **84.8.1 State Persistence**\n",
    "The model's internal state (e.g., trees, scaler means, feature windows) must be persisted so that after a restart, the system can resume learning. River models can be serialized with `pickle` or `joblib`. However, the state of feature extractors (like rolling windows) also needs to be saved.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save the entire pipeline\n",
    "joblib.dump(model, 'online_model.pkl')\n",
    "\n",
    "# Later, load\n",
    "model = joblib.load('online_model.pkl')\n",
    "```\n",
    "\n",
    "### **84.8.2 Microservice Integration**\n",
    "In a microservices architecture (Chapter 81), the real\u2011time learner could be a service that:\n",
    "\n",
    "- Consumes raw data events (e.g., from Kafka).\n",
    "- Maintains model state in memory or a fast database.\n",
    "- Exposes a prediction endpoint that returns the latest prediction and optionally updates the model.\n",
    "- Periodically checkpoints the model to object storage.\n",
    "\n",
    "### **84.8.3 Latency and Throughput**\n",
    "For daily data, latency is not an issue. For high\u2011frequency streams, ensure that the model's update time is less than the inter\u2011arrival time. River's models are optimised for speed.\n",
    "\n",
    "### **84.8.4 Monitoring**\n",
    "Monitor:\n",
    "\n",
    "- Prediction error (rolling metric)\n",
    "- Drift detection alerts\n",
    "- Model update latency\n",
    "- Feature distribution shifts\n",
    "\n",
    "Integrate with the alerting system from Chapter 73.\n",
    "\n",
    "### **84.8.5 Model Versioning**\n",
    "You may want to keep snapshots of the model at different times (e.g., daily) for debugging or rollback. This can be done by saving the model after each update (or periodically) to a versioned store.\n",
    "\n",
    "---\n",
    "\n",
    "## **84.9 Best Practices**\n",
    "\n",
    "1. **Start with a simple model**: Online linear regression is a good baseline. Add complexity only if needed.\n",
    "2. **Monitor for drift**: Use drift detectors to alert you when performance changes.\n",
    "3. **Use incremental preprocessing**: Never use batch scaling; always use incremental statistics.\n",
    "4. **Test in production carefully**: Shadow deploy the online model alongside a batch model before fully switching.\n",
    "5. **Handle missing values**: In streaming, you may need to impute or skip samples.\n",
    "6. **Consider mini\u2011batches**: If data arrives in bursts, you can update in mini\u2011batches for efficiency.\n",
    "7. **Regularly evaluate on hold\u2011out data**: Even though you're learning online, maintain a separate test set from an earlier time period to check for catastrophic forgetting.\n",
    "8. **Document the learning process**: Record model updates, detected drifts, and any interventions.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored real\u2011time (online) learning systems and applied them to the NEPSE stock prediction problem. We contrasted online learning with batch learning, introduced incremental algorithms using scikit\u2011learn and River, and implemented streaming feature engineering. We discussed concept drift and demonstrated how to detect and adapt using River's drift detectors and adaptive models. We built a complete online learning pipeline for NEPSE using `AdaptiveRandomForestRegressor`. Finally, we covered deployment considerations, including state persistence, monitoring, and best practices.\n",
    "\n",
    "Real\u2011time learning enables models to stay current with the latest data, adapt to changing conditions, and provide up\u2011to\u2011date predictions. It is a powerful addition to the time\u2011series practitioner's toolkit.\n",
    "\n",
    "In the next chapter, we will delve into **Distributed Systems**, exploring how to scale time\u2011series prediction across multiple machines for large\u2011scale applications.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 84**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='83. multi_model_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='85. distributed_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}