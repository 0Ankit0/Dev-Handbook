{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 81: Microservices Architecture\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the principles of microservices architecture and how they apply to time‑series prediction systems.\n",
    "- Decompose a monolithic prediction system into loosely coupled, independently deployable services.\n",
    "- Design service boundaries based on domain‑driven design and business capabilities.\n",
    "- Implement inter‑service communication using synchronous (REST, gRPC) and asynchronous (message queues, events) patterns.\n",
    "- Manage data consistency and transactions across microservices.\n",
    "- Handle service discovery, load balancing, and resilience (circuit breakers, retries, timeouts).\n",
    "- Apply containerisation and orchestration (Docker, Kubernetes) to microservices.\n",
    "- Monitor and trace requests across distributed services.\n",
    "- Evaluate the trade‑offs between microservices and monoliths for time‑series systems.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.1 Introduction to Microservices Architecture**\n",
    "\n",
    "Microservices architecture is an approach to software development where a single application is composed of many loosely coupled, independently deployable services. Each service runs its own process and communicates with others through well‑defined APIs. This contrasts with a monolithic architecture, where all components are bundled together.\n",
    "\n",
    "For a time‑series prediction system like the NEPSE stock predictor, a monolithic design might include:\n",
    "\n",
    "- Data ingestion\n",
    "- Feature engineering\n",
    "- Model training\n",
    "- Prediction API\n",
    "- Monitoring and alerting\n",
    "\n",
    "all in one codebase. As the system grows, this becomes difficult to maintain, scale, and deploy. Microservices allow each component to be developed, scaled, and deployed independently, which is especially valuable when different parts have different resource requirements (e.g., model training needs GPUs, while the prediction API needs low latency).\n",
    "\n",
    "However, microservices introduce complexity: network latency, distributed data management, service discovery, and fault tolerance must be handled explicitly.\n",
    "\n",
    "In this chapter, we will design a microservices architecture for the NEPSE prediction system and implement key patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.2 Core Principles of Microservices**\n",
    "\n",
    "Before diving into implementation, let's review the guiding principles:\n",
    "\n",
    "- **Single Responsibility**: Each service should do one thing well (e.g., a Feature Service that only serves feature vectors).\n",
    "- **Loose Coupling**: Services should have minimal knowledge of each other; changes to one should not require changes to others.\n",
    "- **High Cohesion**: Related functionality should be grouped together.\n",
    "- **Independent Deployability**: Each service can be deployed, scaled, and updated independently.\n",
    "- **Decentralised Data Management**: Each service owns its database; no shared database.\n",
    "- **Infrastructure Automation**: Continuous integration and deployment pipelines are essential.\n",
    "- **Design for Failure**: Services must handle failures gracefully (retries, circuit breakers, fallbacks).\n",
    "- **Observability**: Logs, metrics, and traces must be aggregated to understand system behaviour.\n",
    "\n",
    "For the NEPSE system, we can identify candidate services:\n",
    "\n",
    "- **Data Ingestion Service**: Responsible for fetching raw NEPSE data from CSV/APIs and storing it.\n",
    "- **Feature Service**: Computes and serves feature vectors for a given timestamp and symbol.\n",
    "- **Model Training Service**: Trains models on historical data, registers them in a model registry.\n",
    "- **Prediction Service**: Accepts requests for predictions, retrieves features, loads the model, and returns predictions.\n",
    "- **Model Registry Service**: Manages model versions and metadata.\n",
    "- **Monitoring Service**: Collects metrics and triggers alerts.\n",
    "- **User Interface Service**: Serves dashboards and visualisations.\n",
    "\n",
    "These services communicate via APIs or asynchronous messages.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.3 Designing Service Boundaries**\n",
    "\n",
    "The most critical step is defining the right boundaries. A common approach is **Domain‑Driven Design (DDD)** , where we identify bounded contexts. In the NEPSE system, we might have:\n",
    "\n",
    "- **Data Acquisition Context**: Raw data collection, validation, and storage.\n",
    "- **Feature Engineering Context**: Transforming raw data into features.\n",
    "- **Model Management Context**: Training, validation, and versioning of models.\n",
    "- **Prediction Context**: Serving predictions in real time.\n",
    "- **Monitoring Context**: Observability and alerting.\n",
    "\n",
    "Each context becomes a service or a set of services.\n",
    "\n",
    "Let's outline the responsibilities and APIs for each service.\n",
    "\n",
    "### **81.3.1 Data Ingestion Service**\n",
    "\n",
    "- **Responsibilities**:\n",
    "  - Periodically fetch new NEPSE data (e.g., daily CSV).\n",
    "  - Validate schema and data quality.\n",
    "  - Store raw data in a data lake (Parquet files) or a time‑series database.\n",
    "  - Publish events when new data arrives (e.g., to Kafka).\n",
    "\n",
    "- **API**:\n",
    "  - `POST /ingest` – manually trigger ingestion (for testing).\n",
    "  - `GET /health` – health check.\n",
    "\n",
    "- **Data Storage**: Raw data in Parquet (e.g., on S3) or a database like InfluxDB.\n",
    "\n",
    "### **81.3.2 Feature Service**\n",
    "\n",
    "- **Responsibilities**:\n",
    "  - Read raw data and compute features on demand or in batch.\n",
    "  - Store feature vectors for quick retrieval (feature store).\n",
    "  - Provide an API to get features for a given symbol and timestamp.\n",
    "\n",
    "- **API**:\n",
    "  - `GET /features?symbol=NEPSE&timestamp=2023-01-01` – return feature vector.\n",
    "  - `POST /features/batch` – accept list of requests and return batch features.\n",
    "\n",
    "- **Data Storage**: Feature store (e.g., Redis for online, Parquet for offline).\n",
    "\n",
    "### **81.3.3 Model Training Service**\n",
    "\n",
    "- **Responsibilities**:\n",
    "  - Periodically trigger training (e.g., weekly) using historical features.\n",
    "  - Load data from feature store, train models, evaluate.\n",
    "  - Register models in the model registry.\n",
    "  - Optionally, perform hyperparameter tuning.\n",
    "\n",
    "- **API**: Minimal (maybe just a trigger endpoint). Typically runs as a scheduled job.\n",
    "\n",
    "### **81.3.4 Model Registry Service**\n",
    "\n",
    "- **Responsibilities**:\n",
    "  - Store model metadata: version, creation date, performance metrics, feature list, artifact location.\n",
    "  - Allow querying for the latest production model or a specific version.\n",
    "  - Manage model stages (staging, production, archived).\n",
    "\n",
    "- **API**:\n",
    "  - `POST /models` – register a new model.\n",
    "  - `GET /models/latest?stage=production` – get latest production model metadata.\n",
    "  - `GET /models/{version}` – get metadata for a specific version.\n",
    "\n",
    "- **Data Storage**: Database (PostgreSQL) for metadata; model artifacts stored in blob storage.\n",
    "\n",
    "### **81.3.5 Prediction Service**\n",
    "\n",
    "- **Responsibilities**:\n",
    "  - Accept prediction requests.\n",
    "  - Query feature service for the required features.\n",
    "  - Retrieve the current production model from the registry.\n",
    "  - Run inference and return result.\n",
    "  - Log predictions for monitoring.\n",
    "\n",
    "- **API**:\n",
    "  - `POST /predict` – with symbol and date, return predicted close price.\n",
    "  - `POST /predict/batch` – batch predictions.\n",
    "\n",
    "- **Data Storage**: None (stateless), but may cache models locally.\n",
    "\n",
    "### **81.3.6 Monitoring Service**\n",
    "\n",
    "- **Responsibilities**:\n",
    "  - Collect logs and metrics from all services.\n",
    "  - Compute performance metrics (e.g., prediction error) by comparing predictions with actuals.\n",
    "  - Trigger alerts (using Chapter 73) when thresholds are breached.\n",
    "\n",
    "- **API**: Typically internal; may expose metrics for Prometheus.\n",
    "\n",
    "### **81.3.7 User Interface Service**\n",
    "\n",
    "- **Responsibilities**:\n",
    "  - Serve web dashboard for visualisation and manual intervention.\n",
    "  - Communicate with prediction service and feature service.\n",
    "\n",
    "- **API**: Serves HTML/JavaScript, calls backend services.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.4 Inter‑Service Communication**\n",
    "\n",
    "Services need to communicate. Two main styles:\n",
    "\n",
    "- **Synchronous**: HTTP/REST, gRPC. Simple but can introduce coupling and cascading failures.\n",
    "- **Asynchronous**: Message queues (RabbitMQ, Kafka), events. Decouples services but adds complexity.\n",
    "\n",
    "For the NEPSE system, we can mix both:\n",
    "\n",
    "- **Prediction Service → Feature Service**: Synchronous (REST) because predictions need features immediately.\n",
    "- **Data Ingestion → Feature Service**: Asynchronous (event) – when new data arrives, the feature service can be notified to pre‑compute features.\n",
    "- **Prediction Service → Model Registry**: Synchronous (REST) on startup to fetch model, then cached.\n",
    "- **Prediction Service → Monitoring**: Asynchronous (log or message) – send prediction events for later analysis.\n",
    "\n",
    "### **81.4.1 Synchronous Communication with REST**\n",
    "\n",
    "REST is simple and widely understood. We'll use FastAPI for our services. Example: Feature Service endpoint.\n",
    "\n",
    "```python\n",
    "# feature_service.py (simplified)\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import date\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "# Load feature engineering pipeline (pre‑fitted)\n",
    "feature_pipeline = joblib.load(\"feature_pipeline.pkl\")\n",
    "\n",
    "class FeatureRequest(BaseModel):\n",
    "    symbol: str\n",
    "    date: date\n",
    "\n",
    "@app.post(\"/features\")\n",
    "def get_features(request: FeatureRequest):\n",
    "    # In practice, retrieve raw data from database and compute features\n",
    "    # Here we simulate by loading a pre‑computed feature store\n",
    "    df = pd.read_parquet(\"feature_store.parquet\")\n",
    "    row = df[(df['symbol']==request.symbol) & (df['date']==request.date)]\n",
    "    if row.empty:\n",
    "        raise HTTPException(status_code=404, detail=\"Features not found\")\n",
    "    return row.to_dict(orient='records')[0]\n",
    "```\n",
    "\n",
    "### **81.4.2 Asynchronous Communication with Kafka**\n",
    "\n",
    "We'll use Kafka to publish events when new data is ingested. Services can subscribe to topics.\n",
    "\n",
    "```python\n",
    "# data_ingestion_service.py (publisher)\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "\n",
    "producer = KafkaProducer(bootstrap_servers='localhost:9092',\n",
    "                         value_serializer=lambda v: json.dumps(v).encode('utf-8'))\n",
    "\n",
    "def ingest_and_publish(file_path):\n",
    "    df = pd.read_csv(file_path)\n",
    "    # ... save raw data ...\n",
    "    # Publish event for each symbol-date\n",
    "    for _, row in df.iterrows():\n",
    "        event = {\n",
    "            'symbol': row['Symbol'],\n",
    "            'date': row['Date'].isoformat() if 'Date' in row else None,\n",
    "            'event_type': 'NEW_DATA'\n",
    "        }\n",
    "        producer.send('raw-data-events', value=event)\n",
    "```\n",
    "\n",
    "The feature service can consume these events and trigger feature computation.\n",
    "\n",
    "```python\n",
    "# feature_service_consumer.py\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "\n",
    "consumer = KafkaConsumer('raw-data-events',\n",
    "                         bootstrap_servers='localhost:9092',\n",
    "                         value_deserializer=lambda m: json.loads(m.decode('utf-8')))\n",
    "\n",
    "for message in consumer:\n",
    "    event = message.value\n",
    "    if event['event_type'] == 'NEW_DATA':\n",
    "        symbol = event['symbol']\n",
    "        date = event['date']\n",
    "        # Trigger feature computation for this symbol and date\n",
    "        compute_features(symbol, date)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **81.5 Data Management in Microservices**\n",
    "\n",
    "Each service should own its database. Sharing a database between services creates coupling. For the NEPSE system:\n",
    "\n",
    "- **Data Ingestion Service**: Writes raw data to a data lake (Parquet) and possibly a time‑series DB.\n",
    "- **Feature Service**: Owns the feature store (could be Redis for online, and Parquet for offline).\n",
    "- **Model Registry Service**: Owns metadata database (PostgreSQL).\n",
    "- **Prediction Service**: Stateless; may cache models locally.\n",
    "\n",
    "This decentralisation means that queries spanning multiple services must be handled at the application level (e.g., API composition). For example, a dashboard that needs to show both a prediction and the features that drove it would call the prediction service, which in turn calls the feature service.\n",
    "\n",
    "### **81.5.1 Eventual Consistency**\n",
    "\n",
    "Because services have separate databases, we must accept eventual consistency. For example, when new data is ingested, the feature service may take some time to compute features. During that window, prediction requests for that date might fail or use stale features. This is acceptable as long as the system is designed for it (e.g., by returning a 404 or a warning).\n",
    "\n",
    "### **81.5.2 Transactions Across Services**\n",
    "\n",
    "Distributed transactions (e.g., two‑phase commit) are generally avoided in microservices. Instead, use the **Saga pattern**: a sequence of local transactions with compensating actions. For instance, if model training requires updating both the model registry and the prediction service's cache, we could:\n",
    "\n",
    "1. Register model in registry (local transaction).\n",
    "2. Send a message to prediction service to update its cache.\n",
    "3. If cache update fails, send a compensation to roll back the registry (or mark model as invalid).\n",
    "\n",
    "This is complex; often we accept that services are eventually consistent.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.6 Service Discovery and Load Balancing**\n",
    "\n",
    "In a dynamic environment where services are scaled up/down, we need a way for services to find each other. Tools like **Kubernetes** provide built‑in service discovery via DNS. Alternatively, we can use a service registry like **Consul** or **Eureka**.\n",
    "\n",
    "For the NEPSE system deployed on Kubernetes, each service gets a DNS name (e.g., `feature-service.default.svc.cluster.local`). The prediction service can use that to call the feature service.\n",
    "\n",
    "Load balancing is handled by Kubernetes or by client‑side load balancing (e.g., using a library like Ribbon). In our FastAPI services, we can use simple HTTP clients with retries.\n",
    "\n",
    "```python\n",
    "import httpx\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "class FeatureServiceClient:\n",
    "    def __init__(self, base_url=\"http://feature-service:8000\"):\n",
    "        self.base_url = base_url\n",
    "    \n",
    "    @retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=2, max=10))\n",
    "    async def get_features(self, symbol, date):\n",
    "        async with httpx.AsyncClient() as client:\n",
    "            resp = await client.post(f\"{self.base_url}/features\", json={\"symbol\": symbol, \"date\": date.isoformat()})\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **81.7 Resilience Patterns**\n",
    "\n",
    "Microservices must be resilient to failures. Key patterns:\n",
    "\n",
    "- **Retries** with exponential backoff (as above).\n",
    "- **Circuit Breaker**: Prevents calling a failing service repeatedly. Implemented with libraries like `pybreaker`.\n",
    "- **Timeouts**: Always set timeouts on external calls.\n",
    "- **Bulkhead**: Isolate resources so that failure in one part doesn't cascade.\n",
    "- **Fallback**: Provide default responses when a service is unavailable.\n",
    "\n",
    "Example circuit breaker for the feature service client:\n",
    "\n",
    "```python\n",
    "import pybreaker\n",
    "\n",
    "breaker = pybreaker.CircuitBreaker(fail_max=5, reset_timeout=60)\n",
    "\n",
    "class FeatureServiceClientWithBreaker:\n",
    "    def __init__(self, base_url):\n",
    "        self.base_url = base_url\n",
    "        self.client = httpx.AsyncClient()\n",
    "    \n",
    "    @breaker\n",
    "    async def get_features(self, symbol, date):\n",
    "        try:\n",
    "            resp = await self.client.post(f\"{self.base_url}/features\", json={\"symbol\": symbol, \"date\": date.isoformat()}, timeout=5.0)\n",
    "            resp.raise_for_status()\n",
    "            return resp.json()\n",
    "        except (httpx.TimeoutException, httpx.HTTPStatusError) as e:\n",
    "            # Circuit breaker will count this as failure\n",
    "            raise\n",
    "        except Exception as e:\n",
    "            # Unexpected errors also count\n",
    "            raise\n",
    "\n",
    "    async def get_features_with_fallback(self, symbol, date):\n",
    "        try:\n",
    "            return await self.get_features(symbol, date)\n",
    "        except pybreaker.CircuitBreakerError:\n",
    "            # Fallback: return cached features or a default\n",
    "            return {\"error\": \"Feature service unavailable\", \"fallback\": True}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **81.8 Containerisation and Orchestration**\n",
    "\n",
    "Microservices are typically packaged as Docker containers and orchestrated with Kubernetes.\n",
    "\n",
    "### **81.8.1 Dockerising a Service**\n",
    "\n",
    "Example Dockerfile for the prediction service:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "COPY . .\n",
    "\n",
    "CMD [\"uvicorn\", \"prediction_service:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "### **81.8.2 Kubernetes Deployment**\n",
    "\n",
    "A simple deployment and service for the prediction service:\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: prediction-service\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: prediction\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: prediction\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: prediction\n",
    "        image: nepse/prediction-service:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: FEATURE_SERVICE_URL\n",
    "          value: \"http://feature-service:8000\"\n",
    "        - name: MODEL_REGISTRY_URL\n",
    "          value: \"http://model-registry:8000\"\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: prediction-service\n",
    "spec:\n",
    "  selector:\n",
    "    app: prediction\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "```\n",
    "\n",
    "Kubernetes provides service discovery: the prediction service can reach `feature-service` via its DNS name.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.9 Observability in Microservices**\n",
    "\n",
    "With many services, monitoring becomes challenging. We need:\n",
    "\n",
    "- **Centralised logging**: All services log to a central system (e.g., ELK stack, Loki).\n",
    "- **Metrics aggregation**: Prometheus scrapes metrics from each service; Grafana for dashboards.\n",
    "- **Distributed tracing**: Follow a request across services using tools like Jaeger or Zipkin.\n",
    "\n",
    "### **81.9.1 Logging**\n",
    "\n",
    "Each service should log in a structured format (JSON) to facilitate aggregation. In FastAPI, we can use `python-json-logger`.\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logHandler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter()\n",
    "logHandler.setFormatter(formatter)\n",
    "logger.addHandler(logHandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "```\n",
    "\n",
    "### **81.9.2 Metrics**\n",
    "\n",
    "Prometheus client libraries can expose metrics. Example for prediction service:\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, generate_latest\n",
    "from fastapi import Response\n",
    "\n",
    "predictions_total = Counter('predictions_total', 'Total number of predictions', ['symbol'])\n",
    "prediction_duration = Histogram('prediction_duration_seconds', 'Prediction duration')\n",
    "\n",
    "@app.get(\"/metrics\")\n",
    "def metrics():\n",
    "    return Response(content=generate_latest(), media_type=\"text/plain\")\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "@prediction_duration.time()\n",
    "def predict(request: PredictionRequest):\n",
    "    predictions_total.labels(symbol=request.symbol).inc()\n",
    "    # ... prediction logic ...\n",
    "```\n",
    "\n",
    "### **81.9.3 Distributed Tracing**\n",
    "\n",
    "We can instrument our services with OpenTelemetry. For FastAPI, use the `opentelemetry-instrumentation` package. This automatically propagates trace context across HTTP calls.\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n",
    "from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "\n",
    "trace.set_tracer_provider(TracerProvider())\n",
    "jaeger_exporter = JaegerExporter(\n",
    "    agent_host_name=\"jaeger\",\n",
    "    agent_port=6831,\n",
    ")\n",
    "trace.get_tracer_provider().add_span_processor(\n",
    "    BatchSpanProcessor(jaeger_exporter)\n",
    ")\n",
    "\n",
    "app = FastAPI()\n",
    "FastAPIInstrumentor.instrument_app(app)\n",
    "```\n",
    "\n",
    "Now, when a request flows from prediction service to feature service, we can see the entire path in Jaeger.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.10 Case Study: Refactoring the NEPSE Monolith into Microservices**\n",
    "\n",
    "Let's walk through a concrete refactoring of the NEPSE prediction system we built in Chapter 74.\n",
    "\n",
    "**Original Monolith**: A single Python application that:\n",
    "- Loads CSV daily (ingestion).\n",
    "- Computes features.\n",
    "- Trains a model weekly.\n",
    "- Serves predictions via FastAPI.\n",
    "- Logs and alerts.\n",
    "\n",
    "**Step 1: Identify services** – As earlier: ingestion, feature, training, registry, prediction, monitoring.\n",
    "\n",
    "**Step 2: Extract ingestion service** – Move data fetching and storage to a separate service. It writes raw Parquet to shared storage (e.g., S3). It also publishes events to Kafka.\n",
    "\n",
    "**Step 3: Extract feature service** – This service listens to Kafka events, computes features, and stores them in Redis (for online) and Parquet (for offline). It exposes a REST API for feature retrieval.\n",
    "\n",
    "**Step 4: Extract model registry** – A simple service with a database to store model metadata. It exposes APIs to register and retrieve models.\n",
    "\n",
    "**Step 5: Extract training service** – A batch job (could be a Kubernetes CronJob) that pulls features from the feature store, trains a model, and registers it.\n",
    "\n",
    "**Step 6: Extract prediction service** – Stateless service that calls feature service and model registry (caching the model locally). It returns predictions.\n",
    "\n",
    "**Step 7: Extract monitoring service** – Consumes prediction logs, computes errors, and triggers alerts.\n",
    "\n",
    "**Step 8: Deploy with Kubernetes** – Each service gets its own deployment and service. Use ConfigMaps for configuration (e.g., Kafka brokers, database URLs).\n",
    "\n",
    "**Step 9: Set up observability** – Deploy Prometheus, Grafana, Loki, and Jaeger in the cluster. Instrument services accordingly.\n",
    "\n",
    "**Step 10: Implement CI/CD** – Each service has its own build pipeline; changes to one do not require rebuilding others.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.11 Trade‑offs and When to Use Microservices**\n",
    "\n",
    "Microservices are not a silver bullet. They add complexity in development, testing, and operations. Consider them when:\n",
    "\n",
    "- The team is large enough to own multiple services.\n",
    "- Different parts have different scalability requirements (e.g., prediction service needs many instances, training needs GPUs).\n",
    "- Independent deployment cycles are needed.\n",
    "- Technology heterogeneity is desired (e.g., use a different language for some services).\n",
    "\n",
    "For the NEPSE system, if it's a small project, a well‑structured monolith may be sufficient. However, as the system grows to include multiple models, real‑time and batch predictions, and a larger team, microservices become beneficial.\n",
    "\n",
    "---\n",
    "\n",
    "## **81.12 Best Practices**\n",
    "\n",
    "- **Start with a monolith** and extract services as needed (Strangler pattern).\n",
    "- **Define clear API contracts** and version them.\n",
    "- **Use API gateways** to handle cross‑cutting concerns (authentication, rate limiting).\n",
    "- **Automate everything** – build, test, deploy.\n",
    "- **Design for failure** – assume networks will fail.\n",
    "- **Keep services small but not too small** – a service should be manageable by a small team.\n",
    "- **Document service boundaries and dependencies**.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored microservices architecture in the context of time‑series prediction systems, using the NEPSE system as a running example. We identified candidate services, discussed inter‑service communication patterns, data management, resilience, containerisation, and observability. We walked through a refactoring of the NEPSE monolith into microservices and highlighted the trade‑offs. Microservices can bring scalability and independence but require significant investment in infrastructure and operations. For many systems, a well‑modularised monolith remains a pragmatic choice.\n",
    "\n",
    "In the next chapter, we will dive deeper into **Event‑Driven Architecture**, a complementary pattern that fits well with microservices.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 81**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
