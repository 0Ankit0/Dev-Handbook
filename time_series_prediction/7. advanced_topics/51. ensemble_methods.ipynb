{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 51: Ensemble Methods\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the principles behind ensemble learning and why combining multiple models often yields better predictions\n",
    "- Implement bagging methods, such as Random Forest, for time\u2011series classification or regression using the NEPSE dataset\n",
    "- Apply boosting techniques (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost) and understand their strengths and weaknesses\n",
    "- Construct stacking ensembles that combine diverse base models with a meta\u2011learner\n",
    "- Use blending (holdout stacking) to create robust ensembles without overfitting\n",
    "- Implement voting classifiers and regressors for simple averaging of predictions\n",
    "- Appreciate the importance of diversity in ensemble members and how to achieve it\n",
    "- Evaluate ensemble models and avoid common pitfalls like overfitting and excessive computational cost\n",
    "- Apply ensemble methods to the NEPSE stock prediction problem and compare their performance\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "No single machine learning model is universally the best\u2014this is the essence of the **No Free Lunch Theorem**. However, by combining multiple models, we can often achieve better and more robust predictions than any single model alone. This is the core idea behind **ensemble methods**. Ensembles work because different models make different errors; by averaging or voting, those errors can cancel out, leading to improved accuracy and stability.\n",
    "\n",
    "For the NEPSE stock prediction system, ensemble methods are particularly valuable. Financial time series are noisy, and different models may capture different aspects of the data: linear models might capture trends, tree\u2011based models might capture non\u2011linear interactions, and neural networks might capture complex patterns. An ensemble can blend these strengths.\n",
    "\n",
    "In this chapter, we will explore the main families of ensemble methods: **bagging**, **boosting**, **stacking**, and **voting**. We will implement them using Python libraries (scikit\u2011learn, XGBoost, etc.) and apply them to our NEPSE prediction task. By the end, you will be equipped to build powerful ensembles for your own time\u2011series problems.\n",
    "\n",
    "---\n",
    "\n",
    "## 51.1 Ensemble Learning Principles\n",
    "\n",
    "Ensemble learning combines several base models (also called weak learners or base learners) to produce one final prediction. The key requirement for an ensemble to be effective is that the base models are **accurate** and **diverse**. Accuracy means each model performs better than random guessing. Diversity means the models make different errors on different instances. If all models make the same errors, the ensemble will not improve.\n",
    "\n",
    "There are two main ways to create diversity:\n",
    "\n",
    "1. **Using different training data** (e.g., bootstrap samples in bagging).\n",
    "2. **Using different model architectures or hyperparameters** (e.g., different algorithms in stacking).\n",
    "\n",
    "Ensembles can be used for both classification and regression. For classification, common combination rules are **majority voting** (hard voting) or **averaging predicted probabilities** (soft voting). For regression, we typically average the predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 51.2 Bagging (Bootstrap Aggregating)\n",
    "\n",
    "Bagging creates multiple models by training each on a different bootstrap sample (random sample with replacement) of the original training data. The final prediction is the average (for regression) or majority vote (for classification) of all models. Bagging reduces variance without increasing bias, making it especially effective for high\u2011variance models like decision trees.\n",
    "\n",
    "### 51.2.1 Random Forest\n",
    "\n",
    "Random Forest is an extension of bagging applied to decision trees. In addition to using bootstrap samples, it also randomly selects a subset of features at each split, further increasing diversity. This makes Random Forest one of the most popular and effective off\u2011the\u2011shelf machine learning methods.\n",
    "\n",
    "**Applying Random Forest to NEPSE Data**\n",
    "\n",
    "Assume we have already engineered features for each trading day and created a binary target: 1 if next day's closing price is higher than today's, else 0.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# Load the feature\u2011engineered dataset (created in previous chapters)\n",
    "df = pd.read_csv('nepse_features.csv', parse_dates=['Date'])\n",
    "df = df.sort_values('Date')\n",
    "\n",
    "# Define features and target\n",
    "feature_cols = [col for col in df.columns if col not in ['Date', 'Symbol', 'Close', 'Target']]\n",
    "X = df[feature_cols]\n",
    "y = df['Target']  # 1 if next day up, 0 otherwise\n",
    "\n",
    "# Time\u2011based split: train on data before 2024, test on 2024\n",
    "train_mask = df['Date'] < '2024-01-01'\n",
    "test_mask = df['Date'] >= '2024-01-01'\n",
    "X_train, X_test = X[train_mask], X[test_mask]\n",
    "y_train, y_test = y[train_mask], y[test_mask]\n",
    "\n",
    "# Train Random Forest\n",
    "rf = RandomForestClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=10,\n",
    "    min_samples_split=5,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Predict and evaluate\n",
    "y_pred = rf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Random Forest Accuracy: {accuracy:.4f}\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "# Feature importance\n",
    "importances = pd.Series(rf.feature_importances_, index=feature_cols).sort_values(ascending=False)\n",
    "print(\"\\nTop 10 Important Features:\")\n",
    "print(importances.head(10))\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We create a Random Forest classifier with 100 trees. The `n_jobs=-1` uses all available CPU cores for parallel training. After fitting, we evaluate on the test set. The `feature_importances_` attribute tells us which features the forest found most predictive (e.g., lagged returns, RSI, volume). This interpretability is a key advantage of tree\u2011based ensembles.\n",
    "\n",
    "**Tuning Random Forest**\n",
    "\n",
    "Hyperparameters to tune include `n_estimators`, `max_depth`, `min_samples_split`, `max_features`. Use `RandomizedSearchCV` or `GridSearchCV` with time\u2011series cross\u2011validation.\n",
    "\n",
    "---\n",
    "\n",
    "## 51.3 Boosting\n",
    "\n",
    "Boosting builds models sequentially, where each new model tries to correct the errors of the previous ones. Unlike bagging, boosting reduces both bias and variance. Popular boosting algorithms include AdaBoost, Gradient Boosting, and their modern variants XGBoost, LightGBM, and CatBoost.\n",
    "\n",
    "### 51.3.1 AdaBoost (Adaptive Boosting)\n",
    "\n",
    "AdaBoost assigns weights to training instances. Initially, all weights are equal. After each weak learner (often a shallow decision tree), it increases the weights of misclassified instances, so the next learner focuses on them. The final prediction is a weighted vote of all learners.\n",
    "\n",
    "**AdaBoost with scikit\u2011learn**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "base_estimator = DecisionTreeClassifier(max_depth=1)  # stump\n",
    "ada = AdaBoostClassifier(\n",
    "    base_estimator=base_estimator,\n",
    "    n_estimators=100,\n",
    "    learning_rate=1.0,\n",
    "    random_state=42\n",
    ")\n",
    "ada.fit(X_train, y_train)\n",
    "y_pred_ada = ada.predict(X_test)\n",
    "print(f\"AdaBoost Accuracy: {accuracy_score(y_test, y_pred_ada):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "AdaBoost with stumps (depth\u20111 trees) often works well. The `learning_rate` controls the contribution of each weak learner. Lower values require more estimators but can generalise better.\n",
    "\n",
    "### 51.3.2 Gradient Boosting Machines (GBM)\n",
    "\n",
    "Gradient boosting generalises AdaBoost by allowing any differentiable loss function. It fits new models to the **residuals** (errors) of the previous ensemble. scikit\u2011learn provides `GradientBoostingClassifier` and `GradientBoostingRegressor`.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbm = GradientBoostingClassifier(\n",
    "    n_estimators=100,\n",
    "    max_depth=3,\n",
    "    learning_rate=0.1,\n",
    "    random_state=42\n",
    ")\n",
    "gbm.fit(X_train, y_train)\n",
    "y_pred_gbm = gbm.predict(X_test)\n",
    "print(f\"Gradient Boosting Accuracy: {accuracy_score(y_test, y_pred_gbm):.4f}\")\n",
    "```\n",
    "\n",
    "### 51.3.3 XGBoost\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an optimised implementation of gradient boosting with additional features like regularisation, handling of missing values, and built\u2011in cross\u2011validation. It is widely used in Kaggle competitions and industry.\n",
    "\n",
    "**XGBoost for NEPSE**\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "# Create DMatrix (optimised data structure)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 5,\n",
    "    'eta': 0.1,          # learning rate\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'eval_metric': 'logloss',\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "watchlist = [(dtrain, 'train'), (dtest, 'eval')]\n",
    "num_rounds = 1000\n",
    "model = xgb.train(\n",
    "    params,\n",
    "    dtrain,\n",
    "    num_rounds,\n",
    "    evals=watchlist,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=100\n",
    ")\n",
    "\n",
    "# Predict probabilities\n",
    "y_pred_proba = model.predict(dtest)\n",
    "y_pred = (y_pred_proba > 0.5).astype(int)\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"XGBoost Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "XGBoost's `train` method accepts an evaluation set to monitor performance and stop early if no improvement, preventing overfitting. The parameters `subsample` and `colsample_bytree` introduce randomness, similar to random forest, to improve generalisation.\n",
    "\n",
    "**XGBoost with scikit\u2011learn API**\n",
    "\n",
    "```python\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb_clf = XGBClassifier(\n",
    "    n_estimators=1000,\n",
    "    max_depth=5,\n",
    "    learning_rate=0.1,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    early_stopping_rounds=50,\n",
    "    random_state=42\n",
    ")\n",
    "xgb_clf.fit(X_train, y_train, eval_set=[(X_test, y_test)], verbose=False)\n",
    "y_pred_xgb = xgb_clf.predict(X_test)\n",
    "```\n",
    "\n",
    "### 51.3.4 LightGBM\n",
    "\n",
    "LightGBM is another gradient boosting framework that uses histogram\u2011based algorithms for faster training and lower memory usage. It often outperforms XGBoost on large datasets.\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "test_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n",
    "\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1\n",
    "}\n",
    "\n",
    "model_lgb = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[test_data],\n",
    "    num_boost_round=1000,\n",
    "    early_stopping_rounds=50,\n",
    "    verbose_eval=False\n",
    ")\n",
    "\n",
    "y_pred_lgb = (model_lgb.predict(X_test) > 0.5).astype(int)\n",
    "print(f\"LightGBM Accuracy: {accuracy_score(y_test, y_pred_lgb):.4f}\")\n",
    "```\n",
    "\n",
    "### 51.3.5 CatBoost\n",
    "\n",
    "CatBoost is designed to handle categorical features automatically, but it also works well with numerical data. It uses ordered boosting to reduce overfitting.\n",
    "\n",
    "```python\n",
    "from catboost import CatBoostClassifier\n",
    "\n",
    "catboost = CatBoostClassifier(\n",
    "    iterations=1000,\n",
    "    learning_rate=0.1,\n",
    "    depth=5,\n",
    "    eval_metric='Accuracy',\n",
    "    verbose=False,\n",
    "    random_seed=42\n",
    ")\n",
    "catboost.fit(X_train, y_train, eval_set=(X_test, y_test), early_stopping_rounds=50)\n",
    "y_pred_cat = catboost.predict(X_test)\n",
    "print(f\"CatBoost Accuracy: {accuracy_score(y_test, y_pred_cat):.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 51.4 Stacking (Stacked Generalization)\n",
    "\n",
    "Stacking combines multiple base models (level\u20110 models) and trains a meta\u2011model (level\u20111 model) to make the final prediction using the base models' outputs as features. The base models are typically diverse (e.g., Random Forest, XGBoost, SVM). To avoid overfitting, the base models are trained on out\u2011of\u2011fold predictions (using cross\u2011validation).\n",
    "\n",
    "**Stacking with scikit\u2011learn**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import StackingClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "# Define base models\n",
    "base_models = [\n",
    "    ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "    ('xgb', XGBClassifier(n_estimators=100, learning_rate=0.1, random_state=42)),\n",
    "    ('svm', SVC(probability=True, random_state=42))  # need probabilities for meta-model\n",
    "]\n",
    "\n",
    "# Meta-model (logistic regression)\n",
    "meta_model = LogisticRegression()\n",
    "\n",
    "# Stacking ensemble\n",
    "stack = StackingClassifier(\n",
    "    estimators=base_models,\n",
    "    final_estimator=meta_model,\n",
    "    cv=5  # use 5-fold cross-validation to generate out-of-fold predictions\n",
    ")\n",
    "stack.fit(X_train, y_train)\n",
    "y_pred_stack = stack.predict(X_test)\n",
    "print(f\"Stacking Accuracy: {accuracy_score(y_test, y_pred_stack):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`StackingClassifier` automatically performs cross\u2011validation for each base model: it splits the training data into folds, trains on k\u20111 folds, predicts on the held\u2011out fold, and uses those predictions as features for the meta\u2011model. This prevents the meta\u2011model from seeing the same data the base models were trained on, reducing overfitting.\n",
    "\n",
    "**Custom Stacking Implementation (for deeper understanding)**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import numpy as np\n",
    "\n",
    "def stacking_cv(base_models, meta_model, X, y, n_folds=5):\n",
    "    \"\"\"\n",
    "    Generate out-of-fold predictions for base models and train meta-model.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=42)\n",
    "    # Array to hold out-of-fold predictions (samples x n_models)\n",
    "    oof_preds = np.zeros((X.shape[0], len(base_models)))\n",
    "\n",
    "    for i, (name, model) in enumerate(base_models):\n",
    "        for train_idx, val_idx in skf.split(X, y):\n",
    "            X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "\n",
    "            model_clone = clone(model)\n",
    "            model_clone.fit(X_train_fold, y_train_fold)\n",
    "            oof_preds[val_idx, i] = model_clone.predict_proba(X_val_fold)[:, 1]\n",
    "\n",
    "    # Train meta-model on out-of-fold predictions\n",
    "    meta_model.fit(oof_preds, y)\n",
    "\n",
    "    # Now retrain base models on full training data\n",
    "    for name, model in base_models:\n",
    "        model.fit(X, y)\n",
    "\n",
    "    return meta_model, base_models\n",
    "\n",
    "# Usage\n",
    "from sklearn.base import clone\n",
    "meta, base = stacking_cv(base_models, LogisticRegression(), X_train, y_train)\n",
    "\n",
    "# For test set, get base model predictions and feed to meta-model\n",
    "test_preds = np.column_stack([model.predict_proba(X_test)[:, 1] for _, model in base])\n",
    "y_pred_stack_custom = meta.predict(test_preds)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This manual implementation shows the inner workings of stacking. The out\u2011of\u2011fold predictions become the training set for the meta\u2011model. After the meta\u2011model is trained, we retrain each base model on the full training set so they can be used for test predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## 51.5 Blending\n",
    "\n",
    "Blending is a simpler variant of stacking. Instead of using cross\u2011validation, you hold out a portion of the training data (e.g., 10%) as a validation set. Base models are trained on the remaining data, then predict on the validation set. Those predictions become the training data for the meta\u2011model. The meta\u2011model is then trained, and base models are retrained on the full training set (or left as is).\n",
    "\n",
    "**Blending example**\n",
    "\n",
    "```python\n",
    "# Split training data into train and blend sets\n",
    "X_train_base, X_blend, y_train_base, y_blend = train_test_split(\n",
    "    X_train, y_train, test_size=0.1, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "# Train base models on train_base\n",
    "base_models = [\n",
    "    RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "    XGBClassifier(n_estimators=100, random_state=42),\n",
    "    SVC(probability=True, random_state=42)\n",
    "]\n",
    "for model in base_models:\n",
    "    model.fit(X_train_base, y_train_base)\n",
    "\n",
    "# Generate blend predictions\n",
    "blend_preds = np.column_stack([model.predict_proba(X_blend)[:, 1] for model in base_models])\n",
    "\n",
    "# Train meta-model on blend predictions\n",
    "meta_model = LogisticRegression()\n",
    "meta_model.fit(blend_preds, y_blend)\n",
    "\n",
    "# Retrain base models on full training data (optional)\n",
    "for model in base_models:\n",
    "    model.fit(X_train, y_train)\n",
    "\n",
    "# Test predictions\n",
    "test_preds_base = np.column_stack([model.predict_proba(X_test)[:, 1] for model in base_models])\n",
    "y_pred_blend = meta_model.predict(test_preds_base)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Blending is faster than stacking because it requires only one training of base models (plus the retraining step). However, it uses less data to train the meta\u2011model, so it may be less robust than stacking with cross\u2011validation.\n",
    "\n",
    "---\n",
    "\n",
    "## 51.6 Voting Classifiers and Regressors\n",
    "\n",
    "The simplest ensemble is voting: combine predictions from multiple models by averaging (for regression) or majority vote (for classification). Scikit\u2011learn provides `VotingClassifier` and `VotingRegressor`.\n",
    "\n",
    "**Hard Voting (majority rule)**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import VotingClassifier\n",
    "\n",
    "voting_clf = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBClassifier(n_estimators=100, random_state=42)),\n",
    "        ('svm', SVC(probability=False, random_state=42))  # hard voting needs no probabilities\n",
    "    ],\n",
    "    voting='hard'\n",
    ")\n",
    "voting_clf.fit(X_train, y_train)\n",
    "y_pred_vote = voting_clf.predict(X_test)\n",
    "print(f\"Hard Voting Accuracy: {accuracy_score(y_test, y_pred_vote):.4f}\")\n",
    "```\n",
    "\n",
    "**Soft Voting (average of probabilities)**\n",
    "\n",
    "```python\n",
    "voting_clf_soft = VotingClassifier(\n",
    "    estimators=[\n",
    "        ('rf', RandomForestClassifier(n_estimators=100, random_state=42)),\n",
    "        ('xgb', XGBClassifier(n_estimators=100, random_state=42)),\n",
    "        ('svm', SVC(probability=True, random_state=42))\n",
    "    ],\n",
    "    voting='soft'\n",
    ")\n",
    "voting_clf_soft.fit(X_train, y_train)\n",
    "y_pred_soft = voting_clf_soft.predict(X_test)\n",
    "print(f\"Soft Voting Accuracy: {accuracy_score(y_test, y_pred_soft):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Soft voting often performs better because it takes into account the confidence of each model. Note that `SVC` must have `probability=True` to output calibrated probabilities.\n",
    "\n",
    "---\n",
    "\n",
    "## 51.7 Ensemble Diversity\n",
    "\n",
    "Diversity is crucial. If all models are similar, the ensemble will not improve much. Ways to increase diversity:\n",
    "\n",
    "- **Different algorithms** (trees, linear models, neural networks).\n",
    "- **Different feature subsets** (feature bagging).\n",
    "- **Different training data** (bagging, boosting).\n",
    "- **Different hyperparameters** (e.g., shallow vs. deep trees).\n",
    "\n",
    "You can measure diversity using metrics like **Yule's Q statistic** or **correlation of errors**. In practice, simply combining a few well\u2011performing but different models often works well.\n",
    "\n",
    "---\n",
    "\n",
    "## 51.8 Dynamic Selection and Ensemble Pruning\n",
    "\n",
    "Instead of using all base models all the time, **dynamic selection** chooses, for each test instance, the most competent model(s) based on the local region of the feature space. Methods like **KNORA** (K\u2011Nearest Oracles) evaluate base models on the k\u2011nearest neighbours of the test point and select those that performed well.\n",
    "\n",
    "**Ensemble pruning** removes redundant or underperforming models from the ensemble to reduce complexity and sometimes improve accuracy. Greedy forward selection or optimisation\u2011based methods can be used.\n",
    "\n",
    "These advanced techniques are beyond the scope of this chapter, but they are worth exploring if you need to further optimise your ensemble.\n",
    "\n",
    "---\n",
    "\n",
    "## 51.9 Implementation Strategies\n",
    "\n",
    "When implementing ensembles for production (like the NEPSE system), consider:\n",
    "\n",
    "- **Computational cost**: Training many models can be expensive. Use parallelisation (`n_jobs=-1`) and cloud resources.\n",
    "- **Model persistence**: Save all trained models (e.g., with `joblib`) so they can be loaded for inference.\n",
    "- **Inference latency**: For real\u2011time predictions, an ensemble of many models may be too slow. Consider using a single strong model (like XGBoost) or pruning the ensemble.\n",
    "- **Updating models**: In a streaming context, you may need to update ensemble members incrementally. Some libraries (e.g., `river`) support online ensembles.\n",
    "\n",
    "**Saving and loading an ensemble**\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "\n",
    "# Save\n",
    "joblib.dump(stack, 'nepse_stacking_ensemble.pkl')\n",
    "\n",
    "# Load\n",
    "stack_loaded = joblib.load('nepse_stacking_ensemble.pkl')\n",
    "predictions = stack_loaded.predict(X_new)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 51.10 Best Practices and Pitfalls\n",
    "\n",
    "**Best practices:**\n",
    "\n",
    "1. **Start simple**: Try a single good model (e.g., XGBoost) before building a complex ensemble.\n",
    "2. **Ensure diversity**: Use models that are truly different.\n",
    "3. **Use cross\u2011validation** to evaluate ensembles (time\u2011series cross\u2011validation for temporal data).\n",
    "4. **Monitor for overfitting**: If the ensemble performs much better on training than test, it may be overfitting.\n",
    "5. **Consider computational constraints** \u2013 ensembles are slower to train and serve.\n",
    "\n",
    "**Pitfalls:**\n",
    "\n",
    "- **Overfitting**: Stacking with many base models and a complex meta\u2011model can overfit the small out\u2011of\u2011fold dataset. Use simple meta\u2011models (e.g., logistic regression).\n",
    "- **Look\u2011ahead bias**: In time series, ensure that when generating out\u2011of\u2011fold predictions for stacking, you do not use future data. Use time\u2011series cross\u2011validation (e.g., `TimeSeriesSplit`) instead of random folds.\n",
    "- **Ignoring calibration**: For probability ensembles, ensure base models output well\u2011calibrated probabilities (use `calibrate` in scikit\u2011learn if needed).\n",
    "\n",
    "**Time\u2011series cross\u2011validation for stacking**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "# Then use tscv.split(X) instead of KFold in the custom stacking implementation.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored ensemble methods and their application to the NEPSE stock prediction system. We covered:\n",
    "\n",
    "- The fundamental principle of combining diverse and accurate models.\n",
    "- Bagging and Random Forest, demonstrating feature importance.\n",
    "- Boosting algorithms (AdaBoost, Gradient Boosting, XGBoost, LightGBM, CatBoost) and their sequential error\u2011correction mechanism.\n",
    "- Stacking, which uses a meta\u2011model to learn how to best combine base models, with both library and custom implementations.\n",
    "- Blending as a simpler alternative to stacking.\n",
    "- Voting classifiers for straightforward averaging or majority voting.\n",
    "- The importance of diversity and how to achieve it.\n",
    "- Practical considerations for deploying ensembles in production.\n",
    "\n",
    "Ensemble methods are a powerful tool in any data scientist's arsenal. By combining models, you can often achieve state\u2011of\u2011the\u2011art performance on challenging problems like financial time\u2011series prediction. For the NEPSE system, you might find that a well\u2011tuned XGBoost or a stacking ensemble of a few diverse models yields the best accuracy.\n",
    "\n",
    "In the next chapter, we will discuss **Transfer Learning and Pre\u2011training**, exploring how to leverage models trained on large datasets to improve predictions when data is limited.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 51**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../6. production_systems/50. cloud_deployment.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='52. transfer_learning_and_pre_training.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}