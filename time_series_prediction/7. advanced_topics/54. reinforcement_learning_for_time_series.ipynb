{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 54: Reinforcement Learning for Time-Series\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamentals of reinforcement learning (RL) and its applicability to time‑series prediction and trading\n",
    "- Formulate a time‑series problem as a Markov Decision Process (MDP) with states, actions, rewards, and transitions\n",
    "- Implement tabular Q‑learning for discrete state‑action spaces and understand its limitations\n",
    "- Apply Deep Q‑Networks (DQN) to handle continuous state spaces using neural networks\n",
    "- Understand policy gradient methods and their advantages for continuous action spaces\n",
    "- Implement actor‑critic methods (A2C, PPO) for stable and efficient learning\n",
    "- Design custom environments for financial time‑series using libraries like Gymnasium\n",
    "- Address exploration‑exploitation trade‑offs with epsilon‑greedy, Boltzmann exploration, and noise injection\n",
    "- Construct reward functions that align with trading objectives (e.g., profit, risk‑adjusted returns)\n",
    "- Recognize practical challenges when applying RL to financial markets: non‑stationarity, overfitting, transaction costs, and market impact\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In previous chapters, we treated stock prediction as a supervised learning problem: given historical features, we predict the next day's direction or price. But what if we want an agent that not only predicts but also **decides** when to buy, sell, or hold? This is where **reinforcement learning** (RL) shines. RL is a paradigm where an agent learns to make sequential decisions by interacting with an environment, receiving rewards or penalties, and aiming to maximise cumulative reward.\n",
    "\n",
    "For the NEPSE system, an RL agent could be trained to trade a portfolio of stocks. It would observe the market state (prices, volumes, technical indicators), take actions (buy, sell, hold), and receive rewards based on profit or loss. Over time, it learns a policy that maximises long‑term returns, potentially adapting to changing market conditions.\n",
    "\n",
    "This chapter introduces reinforcement learning concepts and algorithms, with a focus on time‑series applications. We will build a simple trading environment for NEPSE data and implement several RL algorithms using Python and libraries such as Gymnasium, Stable‑Baselines3, and PyTorch.\n",
    "\n",
    "---\n",
    "\n",
    "## 54.1 Reinforcement Learning Fundamentals\n",
    "\n",
    "Reinforcement learning is characterised by an **agent** interacting with an **environment**. At each time step `t`, the agent observes a **state** `s_t`, selects an **action** `a_t`, and receives a **reward** `r_t`. The environment then transitions to a new state `s_{t+1}`. The goal is to learn a **policy** `π(a|s)` that maximises the expected cumulative discounted reward, often called the **return**:\n",
    "\n",
    "`G_t = r_t + γ r_{t+1} + γ² r_{t+2} + …`\n",
    "\n",
    "where `γ ∈ [0,1]` is the discount factor that balances immediate and future rewards.\n",
    "\n",
    "Key components:\n",
    "\n",
    "- **Policy**: The agent's behaviour, mapping states to actions. It can be deterministic (`a = μ(s)`) or stochastic (`π(a|s)`).\n",
    "- **Value function**: The expected return starting from state `s` and following policy `π`: `V^π(s) = E[G_t | s_t = s]`.\n",
    "- **Action‑value function** (Q‑function): The expected return starting from state `s`, taking action `a`, then following policy `π`: `Q^π(s,a) = E[G_t | s_t = s, a_t = a]`.\n",
    "- **Model**: The agent's representation of the environment dynamics (transition probabilities and reward function). RL methods can be **model‑based** (learn the model) or **model‑free** (learn directly from experience).\n",
    "\n",
    "For financial trading, the environment is the market, the state includes market data and possibly agent's holdings, actions are trading decisions, and rewards are typically profit (or risk‑adjusted profit).\n",
    "\n",
    "---\n",
    "\n",
    "## 54.2 Formulating the Trading Problem as an MDP\n",
    "\n",
    "To apply RL, we must define the state space, action space, reward function, and transition dynamics (though transitions are often learned from data).\n",
    "\n",
    "### State Space\n",
    "\n",
    "The state should capture all relevant information for decision making. For NEPSE trading, the state at time `t` could include:\n",
    "\n",
    "- Current price and volume (possibly normalized)\n",
    "- Technical indicators (SMA, RSI, MACD) over recent windows\n",
    "- Current portfolio holdings (cash, shares of each stock)\n",
    "- Time features (day of week, month)\n",
    "\n",
    "We can represent the state as a vector of numerical values. To respect the temporal nature, the state must be constructed from information available up to time `t` only (no look‑ahead).\n",
    "\n",
    "### Action Space\n",
    "\n",
    "Actions can be discrete or continuous:\n",
    "\n",
    "- **Discrete**: For each stock, actions could be {Buy, Sell, Hold}. If multiple stocks are traded simultaneously, the action space becomes combinatorial.\n",
    "- **Continuous**: The action could be the fraction of portfolio to allocate to each stock (e.g., portfolio weights). This is more realistic but also more challenging.\n",
    "\n",
    "For simplicity, we start with a single stock and three discrete actions.\n",
    "\n",
    "### Reward Function\n",
    "\n",
    "The reward should reflect the trading objective. Common choices:\n",
    "\n",
    "- **Profit**: `r_t = (price_{t+1} - price_t) * shares_held` (unrealised profit) or realised profit after a sell.\n",
    "- **Risk‑adjusted reward**: Sharpe ratio or profit minus penalty for volatility.\n",
    "- **Including transaction costs**: `r_t = profit - cost * |trade_size|`.\n",
    "\n",
    "We must be careful: rewards based on unrealised profits can encourage excessive risk‑taking. A common approach is to reward only realised profits when a trade is closed.\n",
    "\n",
    "### Transitions\n",
    "\n",
    "The environment's next state depends on the market's evolution, which is not known to the agent. In a model‑free RL setting, the agent learns directly from historical data by replaying sequences of states, actions, and rewards.\n",
    "\n",
    "---\n",
    "\n",
    "## 54.3 Tabular Q‑Learning\n",
    "\n",
    "Q‑learning is a model‑free algorithm that learns the optimal action‑value function `Q*(s,a)`. The update rule:\n",
    "\n",
    "`Q(s_t, a_t) ← Q(s_t, a_t) + α [ r_t + γ max_a Q(s_{t+1}, a) - Q(s_t, a_t) ]`\n",
    "\n",
    "For tabular Q‑learning, we maintain a table of Q‑values for every state‑action pair. This is only feasible when the state space is small and discrete. Our trading state, however, is continuous. We could discretise the state variables (e.g., bin prices into low/medium/high), but this loses information and suffers from the curse of dimensionality.\n",
    "\n",
    "Nonetheless, for illustration, we can build a simple discretised trading environment.\n",
    "\n",
    "**Example: Discretised Trading with Q‑Learning**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class DiscretizedTradingEnv:\n",
    "    def __init__(self, prices, n_bins=10):\n",
    "        self.prices = prices\n",
    "        self.n_bins = n_bins\n",
    "        self.current_step = 0\n",
    "        self.position = 0  # -1: short, 0: neutral, 1: long\n",
    "        self.cash = 0\n",
    "        self.max_steps = len(prices) - 1\n",
    "\n",
    "        # Discretize price into bins\n",
    "        price_min, price_max = prices.min(), prices.max()\n",
    "        self.bins = np.linspace(price_min, price_max, n_bins+1)\n",
    "        self.bin_indices = np.digitize(prices, self.bins) - 1\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.position = 0\n",
    "        self.cash = 0\n",
    "        return self._get_state()\n",
    "\n",
    "    def _get_state(self):\n",
    "        # State: (price bin, current position)\n",
    "        price_bin = self.bin_indices[self.current_step]\n",
    "        return (price_bin, self.position + 1)  # position shifted to 0,1,2\n",
    "\n",
    "    def step(self, action):\n",
    "        # Actions: 0=buy, 1=sell, 2=hold\n",
    "        price = self.prices[self.current_step]\n",
    "        reward = 0\n",
    "        if action == 0:  # buy\n",
    "            if self.position == 0:\n",
    "                self.position = 1\n",
    "                self.cash -= price\n",
    "            # else already long, do nothing (or could be invalid)\n",
    "        elif action == 1:  # sell\n",
    "            if self.position == 1:\n",
    "                self.position = 0\n",
    "                reward = self.cash + price - 0  # profit (simplified)\n",
    "                self.cash = 0\n",
    "            elif self.position == 0:\n",
    "                self.position = -1\n",
    "                self.cash += price\n",
    "        # else hold: do nothing\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= self.max_steps\n",
    "        next_state = self._get_state() if not done else None\n",
    "        return next_state, reward, done, {}\n",
    "```\n",
    "\n",
    "Now we can run Q‑learning on this environment.\n",
    "\n",
    "```python\n",
    "# Initialize Q-table: states = (price_bin, position) -> 3 actions\n",
    "n_price_bins = 10\n",
    "n_positions = 3\n",
    "Q = np.zeros((n_price_bins, n_positions, 3))\n",
    "\n",
    "env = DiscretizedTradingEnv(prices)\n",
    "alpha = 0.1\n",
    "gamma = 0.95\n",
    "epsilon = 0.1\n",
    "episodes = 1000\n",
    "\n",
    "for ep in range(episodes):\n",
    "    state = env.reset()\n",
    "    done = False\n",
    "    while not done:\n",
    "        # epsilon-greedy\n",
    "        if np.random.rand() < epsilon:\n",
    "            action = np.random.randint(3)\n",
    "        else:\n",
    "            action = np.argmax(Q[state[0], state[1], :])\n",
    "\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        if next_state is not None:\n",
    "            # Q-learning update\n",
    "            best_next = np.max(Q[next_state[0], next_state[1], :])\n",
    "            td_target = reward + gamma * best_next\n",
    "        else:\n",
    "            td_target = reward\n",
    "\n",
    "        Q[state[0], state[1], action] += alpha * (td_target - Q[state[0], state[1], action])\n",
    "        state = next_state\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We discretise the price into bins and use position as part of the state. The Q‑table is updated using the Bellman equation. This simple agent can learn a policy, but its performance is limited by the coarse discretisation.\n",
    "\n",
    "---\n",
    "\n",
    "## 54.4 Deep Q‑Networks (DQN)\n",
    "\n",
    "To handle continuous state spaces, we can approximate the Q‑function with a neural network: `Q(s,a; θ)`. The **Deep Q‑Network (DQN)** algorithm uses a replay buffer to store experiences and a target network to stabilise training.\n",
    "\n",
    "**Algorithm outline:**\n",
    "- Maintain a replay buffer of tuples `(s, a, r, s', done)`.\n",
    "- Sample a mini‑batch uniformly from the buffer.\n",
    "- Compute target: `y = r + γ max_a' Q_target(s', a')` (if not done).\n",
    "- Update the main network by minimising `(y - Q_main(s,a))²`.\n",
    "- Periodically copy main network weights to target network.\n",
    "\n",
    "**Implementing DQN for NEPSE Trading with Stable‑Baselines3**\n",
    "\n",
    "Stable‑Baselines3 provides robust implementations of DQN and other RL algorithms. First, we need to create a custom Gym environment.\n",
    "\n",
    "```python\n",
    "import gymnasium as gym\n",
    "from gymnasium import spaces\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class NEPSEtradingEnv(gym.Env):\n",
    "    \"\"\"\n",
    "    Custom Environment for NEPSE trading.\n",
    "    State: vector of features (price, volume, technical indicators, position)\n",
    "    Actions: 0=hold, 1=buy, 2=sell\n",
    "    Reward: change in portfolio value after transaction costs.\n",
    "    \"\"\"\n",
    "    def __init__(self, df, window_size=10, transaction_cost=0.001):\n",
    "        super().__init__()\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.window_size = window_size\n",
    "        self.transaction_cost = transaction_cost\n",
    "        self.n_features = len(df.columns)  # assume df already has features\n",
    "\n",
    "        # Action space: discrete 0,1,2\n",
    "        self.action_space = spaces.Discrete(3)\n",
    "\n",
    "        # Observation space: window of past features + position indicator\n",
    "        self.observation_space = spaces.Box(\n",
    "            low=-np.inf, high=np.inf,\n",
    "            shape=(window_size, self.n_features + 1), dtype=np.float32\n",
    "        )\n",
    "\n",
    "    def _get_obs(self):\n",
    "        # Get last `window_size` rows of features\n",
    "        end = self.current_step\n",
    "        start = end - self.window_size\n",
    "        obs = self.df.iloc[start:end].values\n",
    "        # Add position as an extra feature at each time step (repeat)\n",
    "        position_feature = np.full((self.window_size, 1), self.position)\n",
    "        obs = np.concatenate([obs, position_feature], axis=1)\n",
    "        return obs.astype(np.float32)\n",
    "\n",
    "    def reset(self, seed=None):\n",
    "        super().reset(seed=seed)\n",
    "        self.current_step = self.window_size\n",
    "        self.position = 0  # 0: no position, 1: long, -1: short\n",
    "        self.cash = 1.0  # initial capital normalized\n",
    "        self.shares = 0\n",
    "        return self._get_obs(), {}\n",
    "\n",
    "    def step(self, action):\n",
    "        price = self.df.iloc[self.current_step]['Close']  # assume 'Close' column\n",
    "        done = False\n",
    "        reward = 0\n",
    "\n",
    "        # Execute action\n",
    "        if action == 1:  # buy\n",
    "            if self.position == 0:\n",
    "                # Buy with all cash\n",
    "                self.shares = self.cash / price * (1 - self.transaction_cost)\n",
    "                self.cash = 0\n",
    "                self.position = 1\n",
    "            # else already long, do nothing (or could allow increasing position)\n",
    "        elif action == 2:  # sell\n",
    "            if self.position == 1:\n",
    "                # Sell all shares\n",
    "                self.cash = self.shares * price * (1 - self.transaction_cost)\n",
    "                self.shares = 0\n",
    "                self.position = 0\n",
    "            elif self.position == 0:\n",
    "                # Short (simplified)\n",
    "                self.shares = -self.cash / price * (1 - self.transaction_cost)\n",
    "                self.cash = 0\n",
    "                self.position = -1\n",
    "        # else hold: do nothing\n",
    "\n",
    "        # Move to next step\n",
    "        self.current_step += 1\n",
    "        if self.current_step >= len(self.df) - 1:\n",
    "            done = True\n",
    "\n",
    "        # Compute reward as change in total portfolio value\n",
    "        new_price = self.df.iloc[self.current_step]['Close'] if not done else price\n",
    "        if self.position == 1:\n",
    "            new_value = self.shares * new_price\n",
    "        elif self.position == -1:\n",
    "            new_value = -self.shares * new_price  # short position value negative? simplify\n",
    "        else:\n",
    "            new_value = self.cash\n",
    "        reward = new_value - (self.cash + self.shares * price)  # delta\n",
    "\n",
    "        next_obs = self._get_obs() if not done else None\n",
    "        return next_obs, reward, done, False, {}\n",
    "```\n",
    "\n",
    "Now we can use Stable‑Baselines3 to train a DQN agent.\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.env_checker import check_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback\n",
    "\n",
    "# Create environment\n",
    "df = pd.read_csv('nepse_features.csv')  # assume preprocessed\n",
    "env = NEPSEtradingEnv(df)\n",
    "check_env(env)  # verify environment\n",
    "\n",
    "# Instantiate DQN agent\n",
    "model = DQN('MlpPolicy', env, verbose=1,\n",
    "            learning_rate=1e-3,\n",
    "            buffer_size=50000,\n",
    "            learning_starts=1000,\n",
    "            batch_size=32,\n",
    "            tau=0.1,\n",
    "            gamma=0.99,\n",
    "            exploration_fraction=0.1,\n",
    "            exploration_final_eps=0.02)\n",
    "\n",
    "# Optional: evaluation callback\n",
    "eval_env = NEPSEtradingEnv(df)  # separate env for evaluation\n",
    "eval_callback = EvalCallback(eval_env, best_model_save_path='./logs/',\n",
    "                             log_path='./logs/', eval_freq=1000,\n",
    "                             deterministic=True, render=False)\n",
    "\n",
    "# Train\n",
    "model.learn(total_timesteps=50000, callback=eval_callback)\n",
    "\n",
    "# Save\n",
    "model.save(\"dqn_nepse_trader\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The environment provides a state consisting of a window of features plus the current position. DQN uses a neural network to approximate Q‑values for each action. The agent learns by interacting with historical data, but careful: training on a single historical path may cause overfitting. Often we use multiple episodes with random start points or a sliding window.\n",
    "\n",
    "---\n",
    "\n",
    "## 54.5 Policy Gradient Methods\n",
    "\n",
    "Instead of learning a value function and deriving a policy, policy gradient methods directly optimise the policy `π(a|s; θ)` using gradient ascent on expected return. The REINFORCE algorithm is a simple policy gradient method:\n",
    "\n",
    "`∇θ J(θ) ≈ E[ ∑_t ∇θ log π(a_t|s_t; θ) G_t ]`\n",
    "\n",
    "where `G_t` is the return from time `t`. This is an unbiased estimate but has high variance.\n",
    "\n",
    "**Advantages:**\n",
    "- Naturally handles continuous action spaces.\n",
    "- Can learn stochastic policies (useful for exploration).\n",
    "- Often more stable for certain problems.\n",
    "\n",
    "**Example: REINFORCE for NEPSE trading**\n",
    "\n",
    "We'll use PyTorch to implement a simple policy network.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.distributions import Categorical\n",
    "\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim, n_actions):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.fc3 = nn.Linear(hidden_dim, n_actions)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        return torch.softmax(self.fc3(x), dim=-1)\n",
    "\n",
    "def reinforce(env, policy, optimizer, episodes=1000, gamma=0.99):\n",
    "    for episode in range(episodes):\n",
    "        log_probs = []\n",
    "        rewards = []\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            state_t = torch.FloatTensor(state).unsqueeze(0)\n",
    "            probs = policy(state_t)\n",
    "            m = Categorical(probs)\n",
    "            action = m.sample()\n",
    "            log_prob = m.log_prob(action)\n",
    "            next_state, reward, done, _ = env.step(action.item())\n",
    "            log_probs.append(log_prob)\n",
    "            rewards.append(reward)\n",
    "            state = next_state\n",
    "\n",
    "        # Compute discounted returns\n",
    "        returns = []\n",
    "        R = 0\n",
    "        for r in reversed(rewards):\n",
    "            R = r + gamma * R\n",
    "            returns.insert(0, R)\n",
    "        returns = torch.tensor(returns)\n",
    "        returns = (returns - returns.mean()) / (returns.std() + 1e-8)  # normalize\n",
    "\n",
    "        # Compute loss\n",
    "        loss = []\n",
    "        for log_prob, R in zip(log_probs, returns):\n",
    "            loss.append(-log_prob * R)\n",
    "        loss = torch.cat(loss).sum()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "# Usage\n",
    "env = NEPSEtradingEnv(df)\n",
    "policy = PolicyNetwork(input_dim=env.observation_space.shape[0]*env.observation_space.shape[1], \n",
    "                       hidden_dim=64, n_actions=3)\n",
    "optimizer = optim.Adam(policy.parameters(), lr=1e-3)\n",
    "reinforce(env, policy, optimizer)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The policy network outputs probabilities over actions. The loss is the negative log‑probability of the taken action weighted by the return. Over episodes, the policy shifts towards actions that lead to higher returns.\n",
    "\n",
    "---\n",
    "\n",
    "## 54.6 Actor‑Critic Methods\n",
    "\n",
    "Actor‑critic methods combine the benefits of value‑based and policy‑based methods. They have two components:\n",
    "\n",
    "- **Actor**: the policy network that selects actions.\n",
    "- **Critic**: a value network (usually state‑value `V(s)` or advantage) that evaluates the actor's choices, reducing variance.\n",
    "\n",
    "**Advantage Actor‑Critic (A2C)** uses the advantage function `A(s,a) = Q(s,a) - V(s)` to weight the policy gradient. The critic learns to estimate `V(s)`, and the actor is updated with:\n",
    "\n",
    "`∇θ J(θ) ≈ E[ ∇θ log π(a|s) A(s,a) ]`\n",
    "\n",
    "**Proximal Policy Optimization (PPO)** is a more advanced actor‑critic method that clips the policy update to prevent too large changes, ensuring stable learning. Stable‑Baselines3 provides PPO and A2C implementations.\n",
    "\n",
    "**Example: Training a PPO agent for NEPSE trading**\n",
    "\n",
    "```python\n",
    "from stable_baselines3 import PPO\n",
    "from stable_baselines3.common.vec_env import DummyVecEnv\n",
    "\n",
    "# Vectorise environment (for parallel training, optional)\n",
    "env = DummyVecEnv([lambda: NEPSEtradingEnv(df)])\n",
    "\n",
    "# PPO agent\n",
    "model = PPO('MlpPolicy', env, verbose=1,\n",
    "            learning_rate=3e-4,\n",
    "            n_steps=2048,\n",
    "            batch_size=64,\n",
    "            n_epochs=10,\n",
    "            gamma=0.99,\n",
    "            gae_lambda=0.95,\n",
    "            clip_range=0.2)\n",
    "\n",
    "model.learn(total_timesteps=100000)\n",
    "model.save(\"ppo_nepse_trader\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "PPO is often more robust than DQN and can handle continuous action spaces (if we extend to portfolio allocation). It uses a clipped surrogate objective to avoid destructive large policy updates.\n",
    "\n",
    "---\n",
    "\n",
    "## 54.7 Time‑Series RL Applications\n",
    "\n",
    "Beyond trading, RL can be applied to other time‑series tasks:\n",
    "\n",
    "- **Dynamic asset allocation**: Allocate portfolio weights across multiple assets.\n",
    "- **Order execution**: Learn to split a large order to minimise market impact.\n",
    "- **Market making**: Provide liquidity by placing limit orders.\n",
    "- **Parameter adaptation**: Adjust model hyperparameters (e.g., of a prediction model) in response to market changes.\n",
    "\n",
    "For the NEPSE system, a multi‑asset trading agent could manage a portfolio of top stocks, rebalancing periodically. The state would include features for each stock and current holdings.\n",
    "\n",
    "---\n",
    "\n",
    "## 54.8 Exploration Strategies\n",
    "\n",
    "In RL, the agent must explore to discover rewarding actions. Common strategies:\n",
    "\n",
    "- **ε‑greedy**: With probability ε, take a random action; otherwise, take the greedy action. Used in DQN.\n",
    "- **Boltzmann (softmax) exploration**: Sample actions according to their estimated value, with a temperature parameter controlling randomness.\n",
    "- **Noise injection**: Add noise to the policy (e.g., Ornstein‑Uhlenbeck process for continuous actions).\n",
    "- **Upper Confidence Bound (UCB)**: Select actions based on both estimated value and uncertainty.\n",
    "\n",
    "For financial applications, exploration must be cautious to avoid large losses. Often, we start with high exploration and gradually decay (e.g., ε decay in DQN). In policy gradient methods, the stochastic policy itself provides exploration.\n",
    "\n",
    "---\n",
    "\n",
    "## 54.9 Reward Design\n",
    "\n",
    "Reward design is critical in RL. A poorly designed reward can lead to unintended behaviour. For trading:\n",
    "\n",
    "- **Simple profit**: `reward = profit` after each step (or at the end of an episode). This may encourage the agent to hold positions through adverse moves.\n",
    "- **Risk‑adjusted profit**: Penalise volatility or drawdowns. For example, `reward = profit - λ * (return_std)`.\n",
    "- **Sharpe ratio**: Reward per unit of risk. Can be computed over a window.\n",
    "- **Realised P&L only**: Reward only when a trade is closed (sell). This avoids rewarding unrealised gains.\n",
    "\n",
    "Also, include **transaction costs** to discourage excessive trading. A typical reward step:\n",
    "\n",
    "```python\n",
    "# After executing action, compute portfolio value change\n",
    "new_value = cash + shares * current_price\n",
    "reward = new_value - previous_value - transaction_cost * abs(trade_volume)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 54.10 Practical Considerations\n",
    "\n",
    "Applying RL to financial time‑series is challenging due to:\n",
    "\n",
    "- **Non‑stationarity**: Market dynamics change over time. An agent trained on past data may fail in the future. Solutions: retrain periodically, use adaptive algorithms, or include recent data in training.\n",
    "- **Overfitting**: With a single historical path, the agent can memorise patterns. Use multiple episodes with different starting points, and validate on out‑of‑sample periods.\n",
    "- **Transaction costs and liquidity**: Realistic simulations must include costs, slippage, and limited liquidity (orders affect prices).\n",
    "- **Evaluation**: Use walk‑forward testing: train on period 1, test on period 2; retrain on period 1+2, test on period 3, etc. Compute average performance.\n",
    "- **Risk management**: An RL agent might take excessive risks. Incorporate risk constraints (e.g., maximum position size, stop‑loss) either in the environment or in the reward.\n",
    "- **Computational resources**: RL training can be slow. Use vectorised environments and GPUs for neural networks.\n",
    "\n",
    "**Example: Walk‑forward validation**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "# Assume df has a 'Date' column\n",
    "dates = pd.to_datetime(df['Date'])\n",
    "train_start, train_end = '2020-01-01', '2021-12-31'\n",
    "test_start, test_end = '2022-01-01', '2022-12-31'\n",
    "\n",
    "# Split data\n",
    "train_df = df[(dates >= train_start) & (dates <= train_end)]\n",
    "test_df = df[(dates >= test_start) & (dates <= test_end)]\n",
    "\n",
    "# Create envs\n",
    "train_env = DummyVecEnv([lambda: NEPSEtradingEnv(train_df)])\n",
    "test_env = NEPSEtradingEnv(test_df)\n",
    "\n",
    "# Train\n",
    "model = PPO('MlpPolicy', train_env, verbose=0)\n",
    "model.learn(total_timesteps=50000)\n",
    "\n",
    "# Evaluate on test\n",
    "obs = test_env.reset()\n",
    "total_reward = 0\n",
    "done = False\n",
    "while not done:\n",
    "    action, _ = model.predict(obs, deterministic=True)\n",
    "    obs, reward, done, _ = test_env.step(action)\n",
    "    total_reward += reward\n",
    "print(f\"Test total reward: {total_reward}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we introduced reinforcement learning and its application to time‑series prediction and trading, using the NEPSE stock market as a running example. We covered:\n",
    "\n",
    "- The fundamental concepts of RL: agent, environment, state, action, reward, policy, value functions.\n",
    "- How to formulate a trading problem as a Markov Decision Process (MDP).\n",
    "- Tabular Q‑learning and its limitations for continuous states.\n",
    "- Deep Q‑Networks (DQN) with experience replay and target networks, implemented using Stable‑Baselines3.\n",
    "- Policy gradient methods (REINFORCE) and their advantages.\n",
    "- Actor‑critic methods (A2C, PPO) for stable and efficient learning.\n",
    "- Designing custom Gym environments for trading.\n",
    "- Exploration strategies and reward design tailored to financial objectives.\n",
    "- Practical challenges like non‑stationarity, overfitting, transaction costs, and validation.\n",
    "\n",
    "Reinforcement learning offers a powerful framework for developing adaptive trading strategies that go beyond simple prediction. However, it requires careful design and rigorous validation to succeed in the real world. For the NEPSE system, RL could be used to create a trading agent that learns to exploit patterns in Nepalese stocks, but must be constantly monitored and retrained.\n",
    "\n",
    "In the next chapter, we will discuss **Probabilistic Forecasting**, which provides uncertainty estimates alongside predictions—a crucial aspect for risk management.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 54**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
