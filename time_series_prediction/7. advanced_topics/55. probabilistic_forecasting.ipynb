{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 55: Probabilistic Forecasting\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Distinguish between deterministic (point) forecasts and probabilistic forecasts that quantify uncertainty\n",
    "- Understand why uncertainty quantification is critical for risk management in financial time\u2011series prediction\n",
    "- Generate prediction intervals using quantile regression and conformal prediction\n",
    "- Implement probabilistic models such as Bayesian linear regression and Gaussian processes\n",
    "- Apply ensemble methods to estimate prediction uncertainty\n",
    "- Evaluate probabilistic forecasts using proper scoring rules (CRPS, log\u2011likelihood, interval coverage)\n",
    "- Communicate probabilistic predictions effectively to stakeholders\n",
    "- Integrate uncertainty estimates into trading decisions and risk management for the NEPSE system\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Throughout this handbook, we have focused on **point forecasts**\u2014predictions that give a single value (e.g., tomorrow's closing price or the probability that the price will go up). However, in financial markets, uncertainty is pervasive. A point forecast tells you nothing about how confident the model is. A prediction of 1050 with a narrow confidence interval is very different from the same prediction with a wide interval. For risk management, position sizing, and decision making, understanding the uncertainty around a prediction is as important as the prediction itself.\n",
    "\n",
    "**Probabilistic forecasting** addresses this by producing a full probability distribution over future outcomes. Instead of saying \u201ctomorrow\u2019s close will be 1050\u201d, we say \u201cthere is a 90% chance that tomorrow\u2019s close will be between 1020 and 1080\u201d. This extra information allows traders to quantify risk, set stop\u2011losses, and allocate capital more intelligently.\n",
    "\n",
    "In this chapter, we will explore methods for generating probabilistic forecasts, from simple techniques like quantile regression to advanced approaches like Gaussian processes and Bayesian neural networks. We will apply these methods to the NEPSE stock prediction problem and discuss how to evaluate and use probabilistic predictions in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## 55.1 Deterministic vs. Probabilistic Forecasting\n",
    "\n",
    "A **deterministic forecast** (or point forecast) is a single value: `\u0177`. A **probabilistic forecast** is a probability distribution over possible values of the target variable. For a continuous target, this could be a density function; for practical use, we often summarise it by **prediction intervals** at various confidence levels (e.g., 50%, 80%, 95%).\n",
    "\n",
    "Why go probabilistic?\n",
    "\n",
    "- **Risk assessment**: Know the likelihood of extreme outcomes (e.g., a price drop of more than 5%).\n",
    "- **Decision making**: Optimal decisions under uncertainty require knowing the full distribution, not just the mean. For example, in portfolio optimisation, we need expected return and covariance.\n",
    "- **Model diagnostics**: A well\u2011calibrated probabilistic model produces intervals that contain the true value the specified percentage of the time (e.g., 90% intervals contain the truth 90% of the time). This is a stronger check than just point forecast accuracy.\n",
    "- **Communication**: Stakeholders (traders, risk managers) can understand the range of possible outcomes.\n",
    "\n",
    "For the NEPSE system, a probabilistic forecast might answer: \u201cGiven the current market conditions, what is the distribution of tomorrow\u2019s return for NABIL stock? What is the 5th percentile (worst case) and 95th percentile (best case)?\u201d\n",
    "\n",
    "---\n",
    "\n",
    "## 55.2 Prediction Intervals from Residuals\n",
    "\n",
    "A simple way to obtain prediction intervals is to assume that the errors of a point forecast model are normally distributed with constant variance. Then an approximate 95% interval is `\u0177 \u00b1 1.96 * \u03c3`, where `\u03c3` is the standard deviation of the residuals on a validation set.\n",
    "\n",
    "This method is easy but has strong assumptions: homoscedasticity (constant variance) and normality. Financial returns often exhibit heteroscedasticity (volatility clustering) and heavy tails, so these intervals may be poorly calibrated.\n",
    "\n",
    "**Example: Linear regression with residual\u2011based intervals**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Assume X_train, y_train, X_test, y_test are prepared\n",
    "model = LinearRegression()\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Predict on validation set (time\u2011based)\n",
    "y_val_pred = model.predict(X_val)\n",
    "residuals = y_val - y_val_pred\n",
    "residual_std = np.std(residuals)\n",
    "\n",
    "# Predict on test\n",
    "y_test_pred = model.predict(X_test)\n",
    "lower = y_test_pred - 1.96 * residual_std\n",
    "upper = y_test_pred + 1.96 * residual_std\n",
    "\n",
    "# Check coverage\n",
    "coverage = np.mean((y_test >= lower) & (y_test <= upper))\n",
    "print(f\"95% prediction interval coverage: {coverage:.3f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We compute the standard deviation of residuals on validation data (not training, to avoid optimism). Then we construct intervals assuming normality. The actual coverage may differ; if it's significantly below 95%, the normality/constant variance assumption is violated.\n",
    "\n",
    "---\n",
    "\n",
    "## 55.3 Quantile Regression\n",
    "\n",
    "Quantile regression directly models conditional quantiles of the target distribution, without assuming a parametric form. For a desired quantile `\u03c4` (e.g., 0.1 for the 10th percentile), we minimise the pinball loss:\n",
    "\n",
    "`L(y, \u0177) = (\u03c4 - 1) * (y - \u0177)  if y < \u0177 else \u03c4 * (y - \u0177)`\n",
    "\n",
    "This yields a model that predicts the `\u03c4`\u2011quantile. By training separate models for different quantiles (e.g., 0.1, 0.5, 0.9), we can construct prediction intervals.\n",
    "\n",
    "**Example: Quantile Regression with scikit\u2011learn**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "# Model for median (\u03c4=0.5)\n",
    "model_median = GradientBoostingRegressor(loss='quantile', alpha=0.5, random_state=42)\n",
    "model_median.fit(X_train, y_train)\n",
    "\n",
    "# Model for lower quantile (\u03c4=0.1)\n",
    "model_lower = GradientBoostingRegressor(loss='quantile', alpha=0.1, random_state=42)\n",
    "model_lower.fit(X_train, y_train)\n",
    "\n",
    "# Model for upper quantile (\u03c4=0.9)\n",
    "model_upper = GradientBoostingRegressor(loss='quantile', alpha=0.9, random_state=42)\n",
    "model_upper.fit(X_train, y_train)\n",
    "\n",
    "# Predict\n",
    "y_lower = model_lower.predict(X_test)\n",
    "y_median = model_median.predict(X_test)\n",
    "y_upper = model_upper.predict(X_test)\n",
    "\n",
    "# 80% prediction interval: [y_lower, y_upper]\n",
    "coverage = np.mean((y_test >= y_lower) & (y_test <= y_upper))\n",
    "print(f\"80% interval coverage: {coverage:.3f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`GradientBoostingRegressor` with `loss='quantile'` implements quantile regression. We train three models for different quantiles. The intervals are formed by the lower and upper quantile predictions. This method is flexible and can capture heteroscedasticity because the quantile models can adapt to changing variance.\n",
    "\n",
    "**Caveat:** The intervals from independent quantile models may cross (e.g., the predicted 90th percentile might be below the predicted 10th percentile for some inputs). This is undesirable but can be mitigated by post\u2011processing or using models that ensure monotonicity (e.g., quantile regression forests).\n",
    "\n",
    "---\n",
    "\n",
    "## 55.4 Conformal Prediction\n",
    "\n",
    "Conformal prediction is a framework that produces prediction sets with guaranteed coverage, regardless of the underlying model, under the assumption of exchangeability. It is model\u2011agnostic and works with any point predictor.\n",
    "\n",
    "The basic idea for regression:\n",
    "\n",
    "1. Split the training data into a proper training set and a calibration set.\n",
    "2. Train a model on the proper training set.\n",
    "3. On the calibration set, compute **non\u2011conformity scores** (e.g., absolute error `|y_i - \u0177_i|`).\n",
    "4. For a new test point, compute its prediction `\u0177_test` and then form the prediction interval as `\u0177_test \u00b1 q`, where `q` is the `(1-\u03b1)`\u2011th quantile of the calibration scores.\n",
    "\n",
    "This yields a prediction interval that, under exchangeability, covers the true value with probability at least `1-\u03b1` (on average).\n",
    "\n",
    "**Example: Conformal prediction for NEPSE returns**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Split data into training, calibration, test\n",
    "# Assume X, y are sorted by time\n",
    "n = len(X)\n",
    "n_train = int(0.6 * n)\n",
    "n_cal = int(0.2 * n)\n",
    "X_train = X[:n_train]\n",
    "y_train = y[:n_train]\n",
    "X_cal = X[n_train:n_train+n_cal]\n",
    "y_cal = y[n_train:n_train+n_cal]\n",
    "X_test = X[n_train+n_cal:]\n",
    "y_test = y[n_train+n_cal:]\n",
    "\n",
    "# Train model on proper training set\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Compute non\u2011conformity scores on calibration set\n",
    "y_cal_pred = model.predict(X_cal)\n",
    "scores = np.abs(y_cal - y_cal_pred)\n",
    "\n",
    "# For a desired coverage 90%, find the 90th percentile of scores\n",
    "alpha = 0.1\n",
    "q = np.percentile(scores, 100 * (1 - alpha))\n",
    "\n",
    "# Predict on test\n",
    "y_test_pred = model.predict(X_test)\n",
    "lower = y_test_pred - q\n",
    "upper = y_test_pred + q\n",
    "\n",
    "# Check coverage\n",
    "coverage = np.mean((y_test >= lower) & (y_test <= upper))\n",
    "print(f\"Conformal 90% interval coverage: {coverage:.3f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We reserve a calibration set that is not used for training. The scores (absolute errors) on this set represent the typical prediction error. The interval `\u0177 \u00b1 q` will cover the true value for approximately 90% of test points, assuming exchangeability (i.e., that the calibration and test sets are drawn from the same distribution). This method is very simple and works with any model, but the intervals have constant width (`2q`) for all test points. Variants like **locally weighted conformal prediction** can produce adaptive widths.\n",
    "\n",
    "---\n",
    "\n",
    "## 55.5 Bayesian Methods\n",
    "\n",
    "Bayesian approaches naturally produce probabilistic forecasts by treating model parameters as random variables and computing the posterior predictive distribution. Given training data `D`, the predictive distribution for a new input `x*` is:\n",
    "\n",
    "`p(y* | x*, D) = \u222b p(y* | x*, \u03b8) p(\u03b8 | D) d\u03b8`\n",
    "\n",
    "This integral accounts for both aleatoric uncertainty (noise in the data) and epistemic uncertainty (uncertainty about model parameters).\n",
    "\n",
    "### 55.5.1 Bayesian Linear Regression\n",
    "\n",
    "In Bayesian linear regression, we place a prior on the coefficients, typically a Gaussian, and the likelihood is Gaussian with known variance (or we also put a prior on the variance). The posterior and predictive are analytically tractable.\n",
    "\n",
    "**Example with PyMC3**\n",
    "\n",
    "```python\n",
    "import pymc3 as pm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Prepare data (add intercept)\n",
    "X_train_with_intercept = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "X_test_with_intercept = np.c_[np.ones(X_test.shape[0]), X_test]\n",
    "\n",
    "with pm.Model() as linear_model:\n",
    "    # Priors for coefficients\n",
    "    sigma = pm.HalfCauchy('sigma', beta=10)\n",
    "    beta = pm.Normal('beta', mu=0, sigma=10, shape=X_train_with_intercept.shape[1])\n",
    "    \n",
    "    # Likelihood\n",
    "    mu = pm.math.dot(X_train_with_intercept, beta)\n",
    "    y_obs = pm.Normal('y_obs', mu=mu, sigma=sigma, observed=y_train)\n",
    "    \n",
    "    # Inference\n",
    "    trace = pm.sample(2000, tune=1000, return_inferencedata=False)\n",
    "\n",
    "# Posterior predictive\n",
    "with linear_model:\n",
    "    y_pred = pm.sample_posterior_predictive(trace, samples=500,\n",
    "                                             var_names=['y_obs'],\n",
    "                                             given={'X': X_test_with_intercept})\n",
    "\n",
    "# y_pred['y_obs'] has shape (500, n_test)\n",
    "y_pred_samples = y_pred['y_obs']\n",
    "# Compute mean and quantiles\n",
    "y_mean = y_pred_samples.mean(axis=0)\n",
    "y_lower = np.percentile(y_pred_samples, 5, axis=0)\n",
    "y_upper = np.percentile(y_pred_samples, 95, axis=0)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We define a probabilistic model with priors and likelihood. MCMC sampling gives us samples from the posterior distribution of parameters. Then we generate predictive samples, which give a full distribution for each test point. The 90% credible interval is the 5th to 95th percentiles of these samples.\n",
    "\n",
    "### 55.5.2 Gaussian Processes\n",
    "\n",
    "Gaussian processes (GPs) are a powerful non\u2011parametric Bayesian method for regression. They place a prior over functions directly, and the posterior predictive is Gaussian with mean and variance given by kernel computations. GPs naturally provide uncertainty estimates that grow in regions with few data points.\n",
    "\n",
    "**Example with scikit\u2011learn**\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel\n",
    "\n",
    "# Define kernel: RBF + noise\n",
    "kernel = 1.0 * RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)\n",
    "\n",
    "gp = GaussianProcessRegressor(kernel=kernel, alpha=0.0, normalize_y=True, random_state=42)\n",
    "gp.fit(X_train, y_train)\n",
    "\n",
    "# Predict with uncertainty\n",
    "y_mean, y_std = gp.predict(X_test, return_std=True)\n",
    "\n",
    "# 90% interval\n",
    "lower = y_mean - 1.645 * y_std\n",
    "upper = y_mean + 1.645 * y_std\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`GaussianProcessRegressor` fits a GP model. The kernel captures similarity between points; the RBF kernel assumes that nearby points have correlated outputs. The `WhiteKernel` accounts for noise. The prediction returns mean and standard deviation, which can be used to form Gaussian intervals. Note that these intervals assume Gaussian predictive distribution, which may be reasonable but is an approximation.\n",
    "\n",
    "---\n",
    "\n",
    "## 55.6 Ensemble Methods for Uncertainty\n",
    "\n",
    "Ensemble methods, such as Random Forest or neural network ensembles, can provide uncertainty estimates by looking at the variance of predictions across ensemble members. This captures epistemic uncertainty (disagreement among models).\n",
    "\n",
    "**Example: Random Forest prediction intervals**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf.fit(X_train, y_train)\n",
    "\n",
    "# Get predictions from all trees\n",
    "all_preds = np.array([tree.predict(X_test) for tree in rf.estimators_])\n",
    "# all_preds shape: (n_trees, n_test)\n",
    "\n",
    "y_mean = all_preds.mean(axis=0)\n",
    "y_std = all_preds.std(axis=0)\n",
    "\n",
    "# 90% interval (assuming Gaussian)\n",
    "lower = y_mean - 1.645 * y_std\n",
    "upper = y_mean + 1.645 * y_std\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Each tree in a Random Forest gives a prediction. The variance across trees reflects uncertainty due to different bootstrap samples and feature subsets. This works reasonably well but may underestimate uncertainty if all trees are similar. Also, the assumption of Gaussianity for the interval may be poor.\n",
    "\n",
    "### 55.6.1 Quantile Regression Forests\n",
    "\n",
    "Quantile Regression Forests (QRF) extend Random Forest to estimate conditional quantiles. Instead of averaging predictions, they keep all leaf values and compute the empirical distribution of training targets in each leaf.\n",
    "\n",
    "**Example with `quantile-forest` library**\n",
    "\n",
    "```python\n",
    "from quantile_forest import RandomForestQuantileRegressor\n",
    "\n",
    "qrf = RandomForestQuantileRegressor(n_estimators=100, random_state=42)\n",
    "qrf.fit(X_train, y_train)\n",
    "\n",
    "# Predict quantiles\n",
    "y_lower = qrf.predict(X_test, quantiles=0.1)\n",
    "y_median = qrf.predict(X_test, quantiles=0.5)\n",
    "y_upper = qrf.predict(X_test, quantiles=0.9)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`RandomForestQuantileRegressor` stores the training targets in each leaf. For a test point, it finds the leaf for each tree and collects all training targets in that leaf. The quantiles are then computed from the pooled set across trees. This gives non\u2011parametric intervals that can capture asymmetry and heteroscedasticity.\n",
    "\n",
    "---\n",
    "\n",
    "## 55.7 Probabilistic Forecasting with Neural Networks\n",
    "\n",
    "Neural networks can also produce probabilistic forecasts by:\n",
    "\n",
    "- **Mean + Variance networks**: Output both mean and variance, and train with negative log\u2011likelihood of a Gaussian (or other) distribution.\n",
    "- **Quantile regression**: Output multiple quantiles simultaneously (using a multi\u2011head network).\n",
    "- **Bayesian neural networks**: Place distributions over weights and use variational inference or Monte Carlo dropout.\n",
    "\n",
    "### 55.7.1 Mean\u2011Variance Network\n",
    "\n",
    "We can modify a neural network to output two values: mean `\u03bc` and log\u2011variance `log(\u03c3\u00b2)`. The loss is the negative log\u2011likelihood of a Gaussian:\n",
    "\n",
    "`L = \u00bd log(\u03c3\u00b2) + \u00bd (y - \u03bc)\u00b2 / \u03c3\u00b2 + constant`\n",
    "\n",
    "This allows the network to learn heteroscedastic uncertainty.\n",
    "\n",
    "**Example with PyTorch**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class MeanVarianceNet(nn.Module):\n",
    "    def __init__(self, input_dim, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
    "        self.fc2 = nn.Linear(hidden_dim, hidden_dim)\n",
    "        self.mean_out = nn.Linear(hidden_dim, 1)\n",
    "        self.logvar_out = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        h = torch.relu(self.fc2(h))\n",
    "        mean = self.mean_out(h).squeeze()\n",
    "        logvar = self.logvar_out(h).squeeze()\n",
    "        return mean, logvar\n",
    "\n",
    "def nll_loss(mean, logvar, target):\n",
    "    # Negative log\u2011likelihood of Gaussian\n",
    "    return (0.5 * logvar + 0.5 * (target - mean)**2 / torch.exp(logvar)).mean()\n",
    "\n",
    "# Training loop\n",
    "model = MeanVarianceNet(input_dim=X_train.shape[1], hidden_dim=64)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(100):\n",
    "    optimizer.zero_grad()\n",
    "    mean, logvar = model(X_train_tensor)\n",
    "    loss = nll_loss(mean, logvar, y_train_tensor)\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, loss {loss.item():.4f}\")\n",
    "\n",
    "# Predict\n",
    "mean, logvar = model(X_test_tensor)\n",
    "std = torch.exp(0.5 * logvar).detach().numpy()\n",
    "mean = mean.detach().numpy()\n",
    "lower = mean - 1.645 * std\n",
    "upper = mean + 1.645 * std\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The network outputs both mean and log\u2011variance. The loss is the negative log\u2011likelihood, which encourages the network to output high variance where errors are large and low variance where predictions are accurate. This yields input\u2011dependent uncertainty.\n",
    "\n",
    "### 55.7.2 Monte Carlo Dropout\n",
    "\n",
    "Monte Carlo dropout approximates Bayesian inference in neural networks. By applying dropout at test time and running multiple forward passes, we obtain a distribution of predictions whose variance captures model uncertainty.\n",
    "\n",
    "```python\n",
    "def mc_dropout_predict(model, X, n_passes=100):\n",
    "    model.train()  # enable dropout\n",
    "    preds = []\n",
    "    for _ in range(n_passes):\n",
    "        preds.append(model(X).detach().numpy())\n",
    "    return np.array(preds)\n",
    "\n",
    "# Assume model was trained with dropout\n",
    "pred_samples = mc_dropout_predict(model, X_test_tensor)\n",
    "mean = pred_samples.mean(axis=0)\n",
    "std = pred_samples.std(axis=0)\n",
    "lower = mean - 1.645 * std\n",
    "upper = mean + 1.645 * std\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Dropout is typically used only during training. Here we keep it on during inference, generating different predictions each pass. The variance across passes reflects the model's uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## 55.8 Evaluating Probabilistic Forecasts\n",
    "\n",
    "Evaluating probabilistic forecasts requires metrics that assess both calibration (the intervals contain the true value the right proportion of the time) and sharpness (the intervals are as narrow as possible while being well\u2011calibrated).\n",
    "\n",
    "### 55.8.1 Coverage and Interval Width\n",
    "\n",
    "The simplest check: for a nominal coverage `1-\u03b1`, compute the empirical coverage on a test set:\n",
    "\n",
    "```python\n",
    "coverage = np.mean((y_test >= lower) & (y_test <= upper))\n",
    "```\n",
    "\n",
    "If coverage is much lower than `1-\u03b1`, the model is overconfident; if much higher, it's underconfident.\n",
    "\n",
    "Also compute the average interval width:\n",
    "\n",
    "```python\n",
    "width = np.mean(upper - lower)\n",
    "```\n",
    "\n",
    "A well\u2011calibrated model should have narrow intervals.\n",
    "\n",
    "### 55.8.2 Continuous Ranked Probability Score (CRPS)\n",
    "\n",
    "CRPS measures the difference between the predicted cumulative distribution function (CDF) and the empirical CDF of the observation. It is a proper scoring rule that rewards both calibration and sharpness. For a forecast that gives a distribution, CRPS is:\n",
    "\n",
    "`CRPS(F, y) = \u222b (F(z) - \ud835\udfd9(z \u2265 y))\u00b2 dz`\n",
    "\n",
    "Lower CRPS is better.\n",
    "\n",
    "If the forecast is a sample of predictions (e.g., from MCMC or MC dropout), we can compute CRPS using the `properscoring` library.\n",
    "\n",
    "```python\n",
    "import properscoring as ps\n",
    "\n",
    "# pred_samples shape: (n_samples, n_test)\n",
    "crps = ps.crps_ensemble(y_test, pred_samples)\n",
    "mean_crps = crps.mean()\n",
    "print(f\"Mean CRPS: {mean_crps:.4f}\")\n",
    "```\n",
    "\n",
    "### 55.8.3 Pinball Loss (Quantile Loss)\n",
    "\n",
    "For quantile forecasts, the pinball loss at quantile `\u03c4` is:\n",
    "\n",
    "`L_\u03c4(y, \u0177_\u03c4) = (\u03c4 - 1)(y - \u0177_\u03c4) if y < \u0177_\u03c4 else \u03c4 (y - \u0177_\u03c4)`\n",
    "\n",
    "Average pinball loss over all quantiles gives a measure of quantile forecast performance.\n",
    "\n",
    "```python\n",
    "def pinball_loss(y_true, y_pred, tau):\n",
    "    error = y_true - y_pred\n",
    "    return np.mean(np.where(error >= 0, tau * error, (tau - 1) * error))\n",
    "\n",
    "# For multiple quantiles\n",
    "taus = [0.1, 0.5, 0.9]\n",
    "losses = [pinball_loss(y_test, y_pred_tau, tau) for tau in taus]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 55.9 Communicating Probabilistic Predictions\n",
    "\n",
    "Presenting probabilistic forecasts to stakeholders (traders, managers) requires care. Instead of showing full distributions, we often summarise:\n",
    "\n",
    "- **Fan charts**: Show prediction intervals with varying transparency over time.\n",
    "- **Tables**: Provide median and 90% interval for key decisions.\n",
    "- **Scenario analysis**: \"There is a 10% chance that the price will drop below 1000.\"\n",
    "\n",
    "For the NEPSE system, you might produce a daily report with:\n",
    "\n",
    "```\n",
    "Stock NABIL:\n",
    "- Tomorrow's expected return: +0.5%\n",
    "- 80% confidence interval: [-1.2%, +2.3%]\n",
    "- Probability of positive return: 62%\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 55.10 Integrating Uncertainty into Trading Decisions\n",
    "\n",
    "Probabilistic forecasts can directly inform trading:\n",
    "\n",
    "- **Position sizing**: Allocate more capital to trades with higher confidence (narrow intervals).\n",
    "- **Stop\u2011loss setting**: Set stop\u2011loss at the lower bound of a high\u2011confidence interval.\n",
    "- **Risk limits**: Do not take positions if the potential loss (e.g., 5th percentile) exceeds a threshold.\n",
    "- **Portfolio optimisation**: Use the full predicted distribution to compute expected utility, not just mean.\n",
    "\n",
    "For example, using the predicted mean `\u03bc` and variance `\u03c3\u00b2` from a probabilistic model, you could apply a mean\u2011variance criterion:\n",
    "\n",
    "`allocation \u221d (\u03bc - r_f) / \u03c3\u00b2`\n",
    "\n",
    "This allocates more to assets with higher expected return and lower uncertainty.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored probabilistic forecasting and its importance for the NEPSE stock prediction system. We covered:\n",
    "\n",
    "- The distinction between point forecasts and probabilistic forecasts.\n",
    "- Simple residual\u2011based prediction intervals and their limitations.\n",
    "- Quantile regression using gradient boosting to model conditional quantiles.\n",
    "- Conformal prediction, a model\u2011agnostic method with guaranteed coverage.\n",
    "- Bayesian methods, including Bayesian linear regression and Gaussian processes.\n",
    "- Ensemble\u2011based uncertainty from random forests and quantile regression forests.\n",
    "- Neural network approaches: mean\u2011variance networks and Monte Carlo dropout.\n",
    "- Evaluation metrics: coverage, interval width, CRPS, pinball loss.\n",
    "- Communicating probabilistic forecasts effectively.\n",
    "- Using uncertainty estimates to inform trading decisions and risk management.\n",
    "\n",
    "Probabilistic forecasting transforms a prediction from a single guess into a rich description of possible futures. For the NEPSE system, this means traders can better understand the risks they are taking and make more informed decisions. In the next chapter, we will discuss **Multi\u2011Variate Time\u2011Series**, where we model multiple related time series simultaneously.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 55**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='54. reinforcement_learning_for_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='56. multi_variate_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}