{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 56: Multi‑Variate Time‑Series\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the need for multi‑variate time‑series models when multiple interrelated variables are observed over time\n",
    "- Apply classical multi‑variate models like Vector Autoregression (VAR) to capture linear dependencies among series\n",
    "- Implement deep learning approaches such as multivariate LSTMs and sequence‑to‑sequence models for forecasting multiple related time series\n",
    "- Use graph neural networks to model explicit relationships between variables (e.g., stocks in the same sector)\n",
    "- Incorporate attention mechanisms to focus on relevant series and time steps\n",
    "- Understand dynamic factor models for dimensionality reduction in high‑dimensional settings\n",
    "- Perform causal analysis using Granger causality tests to discover lead‑lag relationships\n",
    "- Evaluate multi‑variate forecasts using metrics that account for cross‑series correlations\n",
    "- Apply these methods to the NEPSE system, where multiple stocks interact through market dynamics\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "So far, we have often treated each stock in the NEPSE as an independent time series, building separate models for each symbol or using the same model with symbol as a feature. However, in financial markets, stocks do not move in isolation. They are influenced by common factors (e.g., market index, interest rates, sector news) and by each other (e.g., correlated movements, lead‑lag relationships). Ignoring these inter‑dependencies can lead to suboptimal forecasts and missed opportunities.\n",
    "\n",
    "**Multi‑variate time‑series** modeling explicitly accounts for the relationships among multiple time series. By modeling them jointly, we can:\n",
    "\n",
    "- Improve forecast accuracy by leveraging information from related series.\n",
    "- Understand the dynamic interactions (e.g., how a shock to one stock propagates to others).\n",
    "- Generate consistent forecasts across a portfolio (e.g., ensuring predicted prices for different stocks are plausible together).\n",
    "\n",
    "In this chapter, we will explore a range of multi‑variate techniques, from classical econometric models to modern deep learning architectures. Using the NEPSE dataset, we will model a basket of stocks simultaneously, capturing the rich interdependencies of the Nepalese market.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.1 Why Multi‑Variate Time‑Series?\n",
    "\n",
    "Consider predicting the closing price of NABIL (Nepal Bank Ltd.). Its price is influenced not only by its own history but also by:\n",
    "\n",
    "- The performance of other banks (sector effect).\n",
    "- The overall market index (NEPSE index).\n",
    "- Macroeconomic indicators (interest rates, remittance inflows).\n",
    "- News or events affecting related industries.\n",
    "\n",
    "A univariate model for NABIL can only use its own lags, missing these external drivers. A multi‑variate model can include other stocks, the index, and external variables as inputs, potentially improving predictions.\n",
    "\n",
    "Moreover, for portfolio management, we need joint forecasts of multiple assets to estimate correlations and diversification benefits. Multi‑variate models provide not only point forecasts but also predictive distributions that capture cross‑series covariances.\n",
    "\n",
    "In the NEPSE context, we might model the top 10 most liquid stocks together, or model each stock alongside the sector index and a market index.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.2 Vector Autoregression (VAR)\n",
    "\n",
    "The Vector Autoregression (VAR) model is the workhorse of classical multi‑variate time‑series analysis. It models each variable as a linear function of its own lagged values and the lagged values of all other variables in the system.\n",
    "\n",
    "A VAR of order `p` (VAR(p)) is defined as:\n",
    "\n",
    "`y_t = c + A_1 y_{t-1} + A_2 y_{t-2} + ... + A_p y_{t-p} + ε_t`\n",
    "\n",
    "where `y_t` is a vector of `k` time series at time `t`, `c` is a vector of intercepts, `A_i` are `k × k` coefficient matrices, and `ε_t` is a vector of white noise innovations (typically with covariance matrix `Σ`).\n",
    "\n",
    "VAR models are estimated equation‑by‑equation using ordinary least squares (OLS), as each equation has the same regressors (all lagged variables). The main challenge is choosing the lag order `p`, often via information criteria (AIC, BIC) or cross‑validation.\n",
    "\n",
    "### 56.2.1 Implementing VAR with statsmodels\n",
    "\n",
    "We'll use the `statsmodels` library to fit a VAR model to NEPSE returns for a few stocks.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from statsmodels.tsa.api import VAR\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load daily returns for a selection of stocks\n",
    "# Assume we have a DataFrame with columns: Date, NABIL, NTC, SBI, HRL, NICA (returns)\n",
    "df = pd.read_csv('nepse_returns.csv', index_col='Date', parse_dates=True)\n",
    "\n",
    "# Check stationarity (returns should be stationary)\n",
    "for col in df.columns:\n",
    "    result = adfuller(df[col].dropna())\n",
    "    print(f\"{col}: p-value = {result[1]:.4f}\")\n",
    "\n",
    "# Fit VAR model\n",
    "model = VAR(df)\n",
    "# Select lag order using AIC\n",
    "results_aic = model.select_order(maxlags=15)\n",
    "print(results_aic.summary())\n",
    "\n",
    "# Fit with chosen lag (e.g., p=5)\n",
    "p = results_aic.selected_orders['aic']\n",
    "var_model = model.fit(p)\n",
    "print(var_model.summary())\n",
    "\n",
    "# Forecast next 5 days\n",
    "lag_order = var_model.k_ar\n",
    "forecast = var_model.forecast(df.values[-lag_order:], steps=5)\n",
    "forecast_df = pd.DataFrame(forecast, index=pd.date_range(start=df.index[-1], periods=5+1, freq='D')[1:],\n",
    "                           columns=df.columns)\n",
    "print(forecast_df)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We first ensure the series are stationary (returns usually are). The `select_order` method computes information criteria for different lags. We pick the lag that minimizes AIC. The `VAR` object is then fit with that lag. The `forecast` method generates multi‑step forecasts using the estimated coefficients. These forecasts are conditional on the last observed values.\n",
    "\n",
    "**Interpreting VAR coefficients:**  \n",
    "Each equation gives the effect of lagged variables on the target. For example, the coefficient of `NABIL(t-1)` in the equation for `NTC(t)` measures how much a 1% return in NABIL yesterday affects NTC's return today, holding other variables constant. Granger causality tests (see later) can formally test these relationships.\n",
    "\n",
    "### 56.2.2 Impulse Response Functions\n",
    "\n",
    "A key output of VAR models is the **impulse response function (IRF)**, which traces the effect of a one‑time shock to one variable on the entire system over time.\n",
    "\n",
    "```python\n",
    "# Compute impulse responses\n",
    "irf = var_model.irf(periods=10)\n",
    "irf.plot(orth=False)  # orthogonalized vs non‑orthogonalized\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The IRF shows, for example, how a shock to NABIL's return propagates to other stocks in subsequent days. Orthogonalized IRFs use a Cholesky decomposition of the residual covariance matrix to isolate shocks, which depends on the ordering of variables.\n",
    "\n",
    "### 56.2.3 Limitations of VAR\n",
    "\n",
    "- Linear only; cannot capture non‑linear dependencies.\n",
    "- Parameter explosion: with `k` variables and lag `p`, we estimate `k + p*k²` parameters. For large `k`, this becomes infeasible (curse of dimensionality).\n",
    "- Assumes stationarity and constant relationships over time.\n",
    "\n",
    "For NEPSE with many stocks, VAR may overfit. We can reduce dimensionality by including only a few key stocks or using factor models.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.3 Multivariate LSTM\n",
    "\n",
    "Long Short‑Term Memory (LSTM) networks can naturally handle multi‑variate inputs by having the input layer accept a vector of features at each time step. For multi‑variate forecasting, we typically use a **sequence‑to‑sequence** architecture: input a window of past observations for all variables, output predictions for one or more future steps for one or all variables.\n",
    "\n",
    "### 56.3.1 Multi‑variate Input, Multi‑variate Output\n",
    "\n",
    "We want to predict future values of all `k` variables given their joint history. This can be done with a single LSTM layer followed by a dense layer with `k` units.\n",
    "\n",
    "**Data preparation:** We create sequences of length `window` as inputs, and the next value (or next `horizon` values) as targets.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "def create_sequences(data, window, horizon=1):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (n_samples, n_features)\n",
    "    returns X (n_sequences, window, n_features), y (n_sequences, n_features)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window - horizon + 1):\n",
    "        X.append(data[i:i+window])\n",
    "        y.append(data[i+window:i+window+horizon].mean(axis=0))  # average over horizon\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Assume df is a DataFrame of returns for multiple stocks\n",
    "data = df.values.astype(np.float32)\n",
    "window = 10\n",
    "X, y = create_sequences(data, window)\n",
    "\n",
    "# Train/val/test split (chronological)\n",
    "train_size = int(0.7 * len(X))\n",
    "val_size = int(0.15 * len(X))\n",
    "X_train, y_train = X[:train_size], y[:train_size]\n",
    "X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]\n",
    "X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]\n",
    "\n",
    "# Convert to tensors\n",
    "X_train_t = torch.tensor(X_train, dtype=torch.float32)\n",
    "y_train_t = torch.tensor(y_train, dtype=torch.float32)\n",
    "X_val_t = torch.tensor(X_val, dtype=torch.float32)\n",
    "y_val_t = torch.tensor(y_val, dtype=torch.float32)\n",
    "X_test_t = torch.tensor(X_test, dtype=torch.float32)\n",
    "y_test_t = torch.tensor(y_test, dtype=torch.float32)\n",
    "\n",
    "train_dataset = TensorDataset(X_train_t, y_train_t)\n",
    "val_dataset = TensorDataset(X_val_t, y_val_t)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=False)  # important: no shuffle for time series\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "```\n",
    "\n",
    "Now define an LSTM model:\n",
    "\n",
    "```python\n",
    "class MultiVariateLSTM(nn.Module):\n",
    "    def __init__(self, n_features, hidden_size, num_layers, horizon=1):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size=n_features, hidden_size=hidden_size,\n",
    "                            num_layers=num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, n_features * horizon)  # predict all features for horizon steps\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, n_features)\n",
    "        out, _ = self.lstm(x)  # out: (batch, seq_len, hidden_size)\n",
    "        out = out[:, -1, :]     # last time step\n",
    "        out = self.fc(out)      # (batch, n_features * horizon)\n",
    "        return out\n",
    "\n",
    "model = MultiVariateLSTM(n_features=data.shape[1], hidden_size=64, num_layers=2, horizon=1)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training loop\n",
    "epochs = 50\n",
    "for epoch in range(epochs):\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_X, batch_y in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        output = model(batch_X)\n",
    "        loss = criterion(output, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "    # Validation\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for batch_X, batch_y in val_loader:\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            val_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Train Loss: {train_loss/len(train_loader):.6f}, Val Loss: {val_loss/len(val_loader):.6f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We create sequences of past observations. The LSTM processes the sequence and the last hidden state is passed to a fully connected layer that outputs predictions for all variables (for the next time step). This model captures temporal dependencies within each series and cross‑series dependencies because the LSTM sees all series together.\n",
    "\n",
    "### 56.3.2 Sequence‑to‑Sequence for Multi‑step Forecasting\n",
    "\n",
    "For multi‑step ahead forecasts (e.g., predict next 5 days), we can modify the output layer to produce `horizon × n_features` values. Alternatively, we can use an encoder‑decoder architecture where the decoder produces one step at a time, feeding its output back as input. This is more complex but can be more flexible.\n",
    "\n",
    "**Simpler approach:** Use a direct multi‑output model as above, training to predict the average over the horizon, or to predict each step separately with a larger output layer.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.4 Graph Neural Networks for Time‑Series\n",
    "\n",
    "Graph Neural Networks (GNNs) have emerged as a powerful tool for modeling data with explicit relational structure. In finance, we often have a natural graph: stocks are nodes, and edges can represent industry sector, correlation, or supply chain relationships. GNNs allow information to propagate along these edges, capturing how one stock's movement affects another.\n",
    "\n",
    "### 56.4.1 Defining the Graph\n",
    "\n",
    "For NEPSE, we might define edges based on:\n",
    "\n",
    "- **Sector membership**: Stocks in the same sector (banks, hydropower, insurance) are connected.\n",
    "- **Correlation threshold**: Connect stocks whose historical returns are highly correlated.\n",
    "- **Lead‑lag relationships**: Directed edges based on Granger causality.\n",
    "\n",
    "We need an adjacency matrix `A` where `A_ij` indicates the strength of connection from `j` to `i` (often symmetric and normalized).\n",
    "\n",
    "**Example: Building a correlation‑based graph**\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import pandas as pd\n",
    "\n",
    "# Compute correlation matrix of returns\n",
    "corr = df.corr().values\n",
    "# Threshold to create adjacency matrix (e.g., |corr| > 0.5)\n",
    "adj = np.where(np.abs(corr) > 0.5, 1, 0)\n",
    "np.fill_diagonal(adj, 0)  # no self‑loops\n",
    "\n",
    "# Convert to a graph\n",
    "G = nx.from_numpy_array(adj)\n",
    "```\n",
    "\n",
    "### 56.4.2 Graph Convolutional LSTM\n",
    "\n",
    "We can combine a GNN with an LSTM by replacing the linear transformations in the LSTM with graph convolutions. The **Graph Convolutional LSTM (GC‑LSTM)** cell uses graph convolution operations on the input and hidden state.\n",
    "\n",
    "For simplicity, we'll use the `torch_geometric` library to implement a model that processes node features over time.\n",
    "\n",
    "**Example: Using a simple GNN + LSTM**\n",
    "\n",
    "We'll treat each time step as a graph with node features (price, volume, technical indicators). We first apply a GNN at each time step to update node features based on neighbors, then feed the sequence of updated features to an LSTM.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.data import Data, DataLoader as GeoDataLoader\n",
    "\n",
    "class GNN_LSTM(nn.Module):\n",
    "    def __init__(self, node_features, hidden_size, num_nodes, num_layers):\n",
    "        super().__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.gcn = GCNConv(node_features, hidden_size)\n",
    "        self.lstm = nn.LSTM(input_size=hidden_size * num_nodes,\n",
    "                            hidden_size=hidden_size,\n",
    "                            num_layers=num_layers,\n",
    "                            batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, num_nodes)  # predict next value for each node\n",
    "\n",
    "    def forward(self, x_seq, edge_index):\n",
    "        # x_seq: (batch, seq_len, num_nodes, node_features)\n",
    "        batch_size, seq_len, _, _ = x_seq.shape\n",
    "        # Process each time step with GCN\n",
    "        out_seq = []\n",
    "        for t in range(seq_len):\n",
    "            # x_t: (batch, num_nodes, node_features)\n",
    "            x_t = x_seq[:, t, :, :].reshape(batch_size * self.num_nodes, -1)\n",
    "            # Apply GCN\n",
    "            x_t = self.gcn(x_t, edge_index)\n",
    "            x_t = F.relu(x_t)\n",
    "            x_t = x_t.view(batch_size, -1)  # flatten nodes\n",
    "            out_seq.append(x_t)\n",
    "        # Stack along sequence\n",
    "        lstm_in = torch.stack(out_seq, dim=1)  # (batch, seq_len, hidden_size * num_nodes)\n",
    "        lstm_out, _ = self.lstm(lstm_in)       # (batch, seq_len, hidden_size)\n",
    "        last_out = lstm_out[:, -1, :]          # last hidden\n",
    "        pred = self.fc(last_out)               # (batch, num_nodes)\n",
    "        return pred\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "At each time step, we run a graph convolution on node features, using a fixed adjacency `edge_index`. The GCN aggregates neighbor information. The resulting node embeddings are flattened and fed to an LSTM that captures temporal dynamics. Finally, a linear layer produces predictions for each node (stock).\n",
    "\n",
    "**Training:** We need to create a batched version where each sample is a sequence of graphs. This is complex but manageable with careful reshaping.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.5 Attention Mechanisms for Multivariate Forecasting\n",
    "\n",
    "Attention mechanisms, especially the Transformer architecture, have become popular for multivariate time‑series. They can capture long‑range dependencies and allow the model to focus on relevant parts of the input sequence and relevant variables.\n",
    "\n",
    "### 56.5.1 Transformer for Multivariate Time‑Series\n",
    "\n",
    "A Transformer encoder can process the sequence of multivariate observations. We add positional encoding to retain order, and the self‑attention layers allow each time step to attend to all others, capturing cross‑time and cross‑variable interactions implicitly.\n",
    "\n",
    "**Example with PyTorch's TransformerEncoder**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000):\n",
    "        super().__init__()\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        self.pe = pe.unsqueeze(0)  # (1, max_len, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, d_model)\n",
    "        return x + self.pe[:, :x.size(1)]\n",
    "\n",
    "class TransformerTimeSeries(nn.Module):\n",
    "    def __init__(self, n_features, d_model=64, nhead=4, num_layers=3, horizon=1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(n_features, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=d_model, nhead=nhead, batch_first=True)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.output_proj = nn.Linear(d_model, n_features * horizon)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, n_features)\n",
    "        x = self.input_proj(x)\n",
    "        x = self.pos_encoder(x)\n",
    "        x = self.transformer_encoder(x)\n",
    "        x = x[:, -1, :]  # take last time step's representation\n",
    "        out = self.output_proj(x)\n",
    "        return out\n",
    "\n",
    "model = TransformerTimeSeries(n_features=data.shape[1], d_model=64, nhead=4, num_layers=3, horizon=1)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The input sequence is projected to `d_model` dimensions, positional encoding added, then passed through Transformer encoder layers. The output at the last position is used for prediction. The self‑attention mechanism can learn which past time steps and which variables are most relevant.\n",
    "\n",
    "### 56.5.2 Interpretability with Attention Weights\n",
    "\n",
    "One advantage of attention is that we can inspect the attention weights to understand what the model focuses on. For multivariate forecasting, we can see which stocks and time lags are most influential for a given prediction.\n",
    "\n",
    "```python\n",
    "# To get attention weights, we need to modify the forward to return them\n",
    "# This is model‑specific; here we show a simple way to extract from a single layer\n",
    "def forward_with_attention(self, x):\n",
    "    x = self.input_proj(x)\n",
    "    x = self.pos_encoder(x)\n",
    "    # Get attention from the first layer\n",
    "    attn_weights = []\n",
    "    for layer in self.transformer_encoder.layers:\n",
    "        x, attn = layer(x, return_attention=True)  # requires custom layer modification\n",
    "        attn_weights.append(attn)\n",
    "    x = x[:, -1, :]\n",
    "    out = self.output_proj(x)\n",
    "    return out, attn_weights\n",
    "```\n",
    "\n",
    "**Note:** Standard PyTorch Transformer layers do not return attention by default; you may need to use a custom implementation or a library like `HuggingFace`'s `Bert` adapted for time‑series.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.6 Dynamic Factor Models\n",
    "\n",
    "When the number of series `k` is large (e.g., all NEPSE stocks), a VAR would have too many parameters. **Dynamic Factor Models (DFMs)** reduce dimensionality by assuming that a small number of latent factors drive the co‑movements of the series. Each observed series is a linear combination of the common factors plus an idiosyncratic component.\n",
    "\n",
    "`y_t = Λ f_t + ε_t`\n",
    "\n",
    "where `f_t` is a vector of `r` latent factors (r << k), `Λ` is a factor loading matrix, and `ε_t` is idiosyncratic noise. The factors themselves often follow a VAR process: `f_t = Φ f_{t-1} + η_t`.\n",
    "\n",
    "DFMs can be estimated via principal components (for the static part) and state‑space methods (Kalman filter) for the dynamics.\n",
    "\n",
    "### 56.6.1 Estimating Factors with PCA\n",
    "\n",
    "A simple two‑step approach: estimate factors by PCA on the covariance matrix of `y_t`, then model the factors with a VAR.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# Assume Y is (n_samples, n_series)\n",
    "pca = PCA(n_components=5)  # choose number of factors\n",
    "factors = pca.fit_transform(Y)  # (n_samples, 5)\n",
    "\n",
    "# Now treat factors as a multivariate time series and fit a VAR\n",
    "model_factors = VAR(factors)\n",
    "results = model_factors.fit(ic='aic')\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "PCA extracts the top principal components, which are linear combinations of the original series that explain most variance. These are treated as factors. Then we model the dynamics of the factors with a VAR. To forecast a specific series, we predict the factors and multiply by the corresponding loadings.\n",
    "\n",
    "### 56.6.2 State‑Space Formulation and Kalman Filter\n",
    "\n",
    "For a more rigorous treatment, we can set up a state‑space model and estimate using the Kalman filter. The `statsmodels` library has a `DynamicFactorMQ` class for medium‑to‑large dimensions.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.statespace.dynamic_factor_mq import DynamicFactorMQ\n",
    "\n",
    "# Fit a dynamic factor model with 2 factors\n",
    "dfm = DynamicFactorMQ(Y, factors=2, factor_orders=1)  # factor VAR order 1\n",
    "dfm_result = dfm.fit()\n",
    "\n",
    "# Forecast\n",
    "forecast = dfm_result.forecast(steps=5)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`DynamicFactorMQ` handles missing data and can incorporate multiple blocks of series (e.g., sectors). It estimates factors and loadings via maximum likelihood and can produce forecasts.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.7 Causal Inference and Granger Causality\n",
    "\n",
    "Understanding which variables *cause* others is crucial for building interpretable models and for policy decisions (e.g., if we can influence one variable, does it affect others?). **Granger causality** is a statistical concept: a variable `X` Granger‑causes `Y` if past values of `X` help predict `Y` beyond past values of `Y` alone.\n",
    "\n",
    "In a VAR framework, we can test Granger causality using F‑tests or chi‑squared tests.\n",
    "\n",
    "### 56.7.1 Granger Causality Test with statsmodels\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "\n",
    "# Test whether NABIL returns Granger‑cause NTC returns\n",
    "data_pair = df[['NABIL', 'NTC']].dropna()\n",
    "gc_result = grangercausalitytests(data_pair, maxlag=5, verbose=True)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "For each lag, the test reports F‑statistic and p‑value. A low p‑value (e.g., <0.05) indicates that NABIL helps predict NTC. Note that Granger causality is about predictive ability, not true causality (which would require controlled experiments).\n",
    "\n",
    "For multiple variables, we can test all pairs and build a directed graph. This graph can then be used as the adjacency matrix for a GNN.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.8 Feature Interactions and Cross‑Correlation\n",
    "\n",
    "Even without a full model, we can explore cross‑correlations at various lags to understand lead‑lag relationships. The **cross‑correlation function (CCF)** plots correlation between two series at different lags.\n",
    "\n",
    "```python\n",
    "import statsmodels.tsa.api as smt\n",
    "\n",
    "# Compute CCF between NABIL and NTC\n",
    "ccf = smt.ccf(df['NABIL'], df['NTC'], adjusted=False)\n",
    "lags = range(-10, 11)\n",
    "plt.stem(lags, ccf[:21])  # first 21 lags (including negative)\n",
    "plt.xlabel('Lag')\n",
    "plt.ylabel('Cross‑correlation')\n",
    "plt.title('Cross‑correlation: NABIL vs NTC')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Positive lags mean NABIL leads NTC (NABIL at t‑k correlates with NTC at t). Negative lags mean NTC leads NABIL. This can inform which lags to include in a multivariate model.\n",
    "\n",
    "---\n",
    "\n",
    "## 56.9 Implementation Considerations for NEPSE\n",
    "\n",
    "When applying multivariate methods to NEPSE data, consider:\n",
    "\n",
    "- **Choice of stocks**: Too many stocks will overburden the model. Start with a subset (e.g., top 10 by liquidity) or use factor models.\n",
    "- **Data frequency**: Daily data is common. For higher frequency, computational demands increase.\n",
    "- **Non‑stationarity**: Use returns or log‑returns, not prices.\n",
    "- **Exogenous variables**: Include market index, sector indices, or macro variables if available.\n",
    "- **Validation**: Use walk‑forward validation (time‑based splits) to evaluate forecasting performance.\n",
    "\n",
    "**Example pipeline for NEPSE multivariate forecasting:**\n",
    "\n",
    "```python\n",
    "# 1. Select a set of stocks (e.g., 5 from different sectors)\n",
    "symbols = ['NABIL', 'NTC', 'HRL', 'SBI', 'NICA']\n",
    "df_returns = get_returns(symbols)  # DataFrame with columns as symbols, index date\n",
    "\n",
    "# 2. Fit a VAR model (or LSTM, etc.)\n",
    "# 3. Forecast next day returns for all symbols\n",
    "# 4. Evaluate using portfolio metrics (e.g., mean squared error per symbol, or portfolio return if acted upon)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 56.10 Evaluation of Multivariate Models\n",
    "\n",
    "Evaluating multivariate forecasts is more complex than univariate because we care about both per‑series accuracy and the joint distribution (e.g., correlations). Common metrics:\n",
    "\n",
    "- **Per‑series RMSE/MAE**: Still useful to see if each series is well predicted.\n",
    "- **Average RMSE across series**: `mean(rmse_i)`.\n",
    "- **Trace of covariance matrix of errors**: Measures total squared error across series.\n",
    "- **Log‑likelihood**: For probabilistic forecasts, the joint log‑likelihood under a multivariate normal assumption.\n",
    "- **Portfolio performance**: If forecasts are used to construct a portfolio, evaluate the portfolio's return and risk.\n",
    "\n",
    "**Example: Computing average RMSE**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# y_test_true: (n_test, n_series)\n",
    "# y_test_pred: (n_test, n_series)\n",
    "rmse_per_series = np.sqrt(mean_squared_error(y_test_true, y_test_pred, multioutput='raw_values'))\n",
    "avg_rmse = np.mean(rmse_per_series)\n",
    "print(f\"RMSE per series: {rmse_per_series}\")\n",
    "print(f\"Average RMSE: {avg_rmse:.4f}\")\n",
    "```\n",
    "\n",
    "**Example: Evaluating correlation of errors**\n",
    "\n",
    "```python\n",
    "errors = y_test_true - y_test_pred\n",
    "corr_errors = np.corrcoef(errors.T)  # (n_series, n_series)\n",
    "# We want the errors to be uncorrelated ideally, so off‑diagonal close to zero\n",
    "mean_abs_corr = np.mean(np.abs(corr_errors[np.triu_indices_from(corr_errors, k=1)]))\n",
    "print(f\"Mean absolute error correlation: {mean_abs_corr:.4f}\")\n",
    "```\n",
    "\n",
    "A good model should have low error correlation, meaning it captures the dependencies well.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored the rich field of multi‑variate time‑series modeling and its application to the NEPSE stock market. We covered:\n",
    "\n",
    "- The motivation for modeling multiple interrelated series jointly.\n",
    "- Classical Vector Autoregression (VAR), including lag selection, impulse responses, and Granger causality.\n",
    "- Deep learning approaches: multivariate LSTM and sequence‑to‑sequence models.\n",
    "- Graph Neural Networks to leverage explicit relationships among stocks.\n",
    "- Attention mechanisms and Transformers for capturing complex dependencies.\n",
    "- Dynamic Factor Models for dimensionality reduction when many series exist.\n",
    "- Causal analysis with Granger causality tests.\n",
    "- Cross‑correlation analysis to discover lead‑lag relationships.\n",
    "- Practical considerations and evaluation metrics for multivariate forecasting.\n",
    "\n",
    "By modeling multiple stocks together, we can achieve better forecasts and gain insights into the market structure. For the NEPSE system, multivariate models can improve portfolio‑level predictions and risk assessments. In the next chapter, we will discuss **Hierarchical and Grouped Time‑Series**, which deals with time series that have aggregation constraints (e.g., regional sales, or sector indices and their constituents).\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 56**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
