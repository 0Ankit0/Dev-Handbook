{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 52: Transfer Learning and Pre\u2011training\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the core concepts of transfer learning and why it is beneficial when data is limited\n",
    "- Distinguish between different pre\u2011training strategies (supervised, self\u2011supervised, multi\u2011task)\n",
    "- Apply fine\u2011tuning techniques to adapt a pre\u2011trained model to a new domain or task\n",
    "- Implement domain adaptation methods to handle shifts in data distribution between source and target\n",
    "- Explore self\u2011supervised learning approaches to learn useful representations from unlabeled time\u2011series data\n",
    "- Use contrastive learning to build robust features without labels\n",
    "- Understand the emerging role of foundation models for time\u2011series forecasting\n",
    "- Apply few\u2011shot learning techniques to make predictions with very few labeled examples\n",
    "- Recognise the limitations and potential pitfalls of transfer learning in financial time\u2011series\n",
    "- Implement practical transfer learning workflows using PyTorch or TensorFlow for the NEPSE prediction system\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous chapters, we trained our NEPSE stock prediction models from scratch using historical data from the Nepal Stock Exchange. However, what if we have only a few years of data? Or what if a new stock is listed and we have very little trading history? Training a deep neural network from scratch on such small datasets often leads to overfitting. **Transfer learning** offers a solution: we can leverage knowledge learned from a related task or a larger dataset and adapt it to our target problem.\n",
    "\n",
    "Transfer learning has revolutionised computer vision and natural language processing. For time\u2011series forecasting, especially in finance, it is gaining traction. We can pre\u2011train a model on a large corpus of stocks from other exchanges (e.g., US, Indian) and then fine\u2011tune it on the smaller NEPSE dataset. The pre\u2011trained model may have learned general patterns of price movements, volatility, and technical indicators that are useful across markets.\n",
    "\n",
    "In this chapter, we will explore various transfer learning techniques, from simple fine\u2011tuning to more advanced methods like domain adaptation, self\u2011supervised learning, and few\u2011shot learning. We will use the NEPSE system as a running example, showing how to implement these ideas in practice with deep learning frameworks.\n",
    "\n",
    "---\n",
    "\n",
    "## 52.1 Transfer Learning Concepts\n",
    "\n",
    "**Transfer learning** is a machine learning technique where a model developed for a task is reused as the starting point for a model on a second task. It is especially popular in deep learning because training deep networks from scratch requires massive amounts of data and computational resources.\n",
    "\n",
    "The key idea is that the first layers of a neural network learn general features (e.g., edges in images, or trend and seasonality in time\u2011series), while later layers learn task\u2011specific features. By transferring the general features, we can train a model for the new task with less data and fewer iterations.\n",
    "\n",
    "### 52.1.1 When to Use Transfer Learning\n",
    "\n",
    "- **Limited target data**: NEPSE has a relatively short history and few stocks compared to developed markets.\n",
    "- **Similar source domain**: There exists a related domain with abundant data (e.g., other stock exchanges, or even synthetic data).\n",
    "- **Computational efficiency**: Pre\u2011training on a large dataset can be done once, then reused.\n",
    "- **Improved generalisation**: Transfer learning can reduce overfitting on the small target dataset.\n",
    "\n",
    "### 52.1.2 Transfer Learning Scenarios\n",
    "\n",
    "1. **Inductive transfer**: Source and target tasks are different but related. Example: pre\u2011train on predicting next\u2011day price movement for US stocks, fine\u2011tune on NEPSE.\n",
    "2. **Transductive transfer**: Tasks are the same, but domains differ. Example: train on data from 2010\u20132015, adapt to 2020\u20132025 (concept drift).\n",
    "3. **Unsupervised transfer**: Source has no labels (self\u2011supervised learning). Example: pre\u2011train a model to predict masked values in a time\u2011series, then fine\u2011tune on classification.\n",
    "\n",
    "---\n",
    "\n",
    "## 52.2 Pre\u2011training Strategies\n",
    "\n",
    "Pre\u2011training is the process of training a model on a source task before adapting it to the target task. The choice of pre\u2011training strategy depends on the availability of labels and the similarity of tasks.\n",
    "\n",
    "### 52.2.1 Supervised Pre\u2011training\n",
    "\n",
    "If we have a large labeled dataset from a related domain (e.g., all stocks on the New York Stock Exchange with daily returns), we can pre\u2011train a model on that dataset. The model learns to map features to returns, which may be transferable.\n",
    "\n",
    "**Example: Pre\u2011training on US stock data**\n",
    "\n",
    "Suppose we have a dataset of daily features for 3000 US stocks over 20 years. We train an LSTM to predict next\u2011day return direction.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Assume we have preprocessed US stock data: X_us (samples, sequence_length, features), y_us (binary)\n",
    "X_us = torch.tensor(us_features, dtype=torch.float32)\n",
    "y_us = torch.tensor(us_labels, dtype=torch.float32)\n",
    "\n",
    "dataset = TensorDataset(X_us, y_us)\n",
    "dataloader = DataLoader(dataset, batch_size=256, shuffle=True)\n",
    "\n",
    "class LSTMPredictor(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(hidden_size, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]  # last time step\n",
    "        out = self.fc(out)\n",
    "        return self.sigmoid(out).squeeze()\n",
    "\n",
    "model = LSTMPredictor(input_size=10, hidden_size=64, num_layers=2)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "for epoch in range(10):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "\n",
    "# Save pre-trained model\n",
    "torch.save(model.state_dict(), 'pretrained_us_lstm.pth')\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We train an LSTM classifier on the large US dataset. The model learns to extract temporal patterns that are predictive of returns. These patterns (e.g., momentum, mean reversion) may generalise to other markets.\n",
    "\n",
    "### 52.2.2 Self\u2011supervised Pre\u2011training\n",
    "\n",
    "Self\u2011supervised learning (SSL) creates labels from the data itself, without human annotation. For time\u2011series, common SSL tasks include:\n",
    "\n",
    "- **Masked reconstruction**: Mask a portion of the input and train the model to predict the missing values.\n",
    "- **Contrastive learning**: Pull representations of similar samples (e.g., different views of the same time\u2011series) closer, and push apart representations of different samples.\n",
    "- **Temporal ordering**: Predict whether two segments are in correct chronological order.\n",
    "- **Forecasting**: Predict future values (this is inherently supervised but can be done on unlabeled data).\n",
    "\n",
    "SSL can learn powerful representations that capture the underlying structure of the data, which can then be fine\u2011tuned for downstream tasks with few labels.\n",
    "\n",
    "**Example: Masked autoencoder for time\u2011series**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class TimeSeriesMAE(nn.Module):\n",
    "    \"\"\"Masked Autoencoder for time-series.\"\"\"\n",
    "    def __init__(self, input_dim, hidden_dim, mask_ratio=0.3):\n",
    "        super().__init__()\n",
    "        self.mask_ratio = mask_ratio\n",
    "        self.encoder = nn.LSTM(input_dim, hidden_dim, batch_first=True)\n",
    "        self.decoder = nn.Linear(hidden_dim, input_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x shape: (batch, seq_len, input_dim)\n",
    "        # Randomly mask some time steps\n",
    "        mask = torch.rand(x.shape[0], x.shape[1], 1) > self.mask_ratio\n",
    "        masked_x = x * mask.float()\n",
    "        # Encode\n",
    "        enc_out, _ = self.encoder(masked_x)  # (batch, seq_len, hidden_dim)\n",
    "        # Decode (predict original values)\n",
    "        recon = self.decoder(enc_out)  # (batch, seq_len, input_dim)\n",
    "        # Compute loss only on masked positions\n",
    "        loss_mask = (~mask).float()\n",
    "        loss = ((recon - x) ** 2 * loss_mask).sum() / (loss_mask.sum() + 1e-8)\n",
    "        return recon, loss\n",
    "\n",
    "# Training loop\n",
    "model = TimeSeriesMAE(input_dim=10, hidden_dim=64)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "for epoch in range(50):\n",
    "    total_loss = 0\n",
    "    for batch_x in dataloader_unlabeled:  # unlabeled data\n",
    "        _, loss = model(batch_x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {total_loss/len(dataloader):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The model learns to reconstruct masked portions of the input time\u2011series. The encoder must capture the temporal dependencies to fill in the missing values. The resulting encoder can then be used as a feature extractor for downstream tasks.\n",
    "\n",
    "### 52.2.3 Multi\u2011task Pre\u2011training\n",
    "\n",
    "Multi\u2011task learning trains a model on multiple related tasks simultaneously. For time\u2011series, these tasks could be predicting next\u2011day return, volatility, and volume. The shared representation may be more robust.\n",
    "\n",
    "```python\n",
    "class MultiTaskLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.return_head = nn.Linear(hidden_size, 1)\n",
    "        self.volatility_head = nn.Linear(hidden_size, 1)\n",
    "        self.volume_head = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        ret = torch.sigmoid(self.return_head(out))\n",
    "        vol = self.volatility_head(out)  # regression\n",
    "        vol_pred = self.volume_head(out)\n",
    "        return ret, vol, vol_pred\n",
    "```\n",
    "\n",
    "Training with combined loss (e.g., BCE for return, MSE for volatility and volume) encourages the LSTM to learn features useful for all tasks.\n",
    "\n",
    "---\n",
    "\n",
    "## 52.3 Fine\u2011tuning Techniques\n",
    "\n",
    "After pre\u2011training, we adapt the model to the target task (NEPSE prediction) via **fine\u2011tuning**. The standard approach:\n",
    "\n",
    "1. Load the pre\u2011trained model.\n",
    "2. Replace the final classification/regression layer(s) with new randomly initialised layers suited to the target task.\n",
    "3. Optionally freeze some of the early layers to preserve general features and prevent overfitting on the small target dataset.\n",
    "4. Train on the target data with a lower learning rate.\n",
    "\n",
    "**Example: Fine\u2011tuning the US pre\u2011trained LSTM on NEPSE data**\n",
    "\n",
    "```python\n",
    "# Load pre-trained model\n",
    "model = LSTMPredictor(input_size=10, hidden_size=64, num_layers=2)\n",
    "model.load_state_dict(torch.load('pretrained_us_lstm.pth'))\n",
    "\n",
    "# Replace the final layer (since target may have different number of classes)\n",
    "# In our case, still binary classification, but we may want to reset it.\n",
    "model.fc = nn.Linear(64, 1)  # reinitialise\n",
    "model.sigmoid = nn.Sigmoid()\n",
    "\n",
    "# Optionally freeze early layers\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lstm' in name:\n",
    "        param.requires_grad = False  # freeze LSTM layers\n",
    "\n",
    "# Only train the new head (and maybe unfreeze later)\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=0.001)\n",
    "\n",
    "# Prepare NEPSE data\n",
    "X_nepse = torch.tensor(nepse_features, dtype=torch.float32)\n",
    "y_nepse = torch.tensor(nepse_labels, dtype=torch.float32)\n",
    "dataset = TensorDataset(X_nepse, y_nepse)\n",
    "dataloader = DataLoader(dataset, batch_size=32, shuffle=True)\n",
    "\n",
    "# Fine-tune\n",
    "for epoch in range(10):\n",
    "    for batch_x, batch_y in dataloader:\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(batch_x)\n",
    "        loss = criterion(outputs, batch_y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    print(f\"Epoch {epoch+1}, Loss: {loss.item():.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We first freeze the LSTM layers to avoid destroying the useful features learned from US data. Only the new final layer is trained for a few epochs. Then we may unfreeze the later LSTM layers and continue training with a very low learning rate to adapt the features to NEPSE specifics.\n",
    "\n",
    "### 52.3.2 Progressive Unfreezing\n",
    "\n",
    "A more sophisticated fine\u2011tuning strategy is **progressive unfreezing**: start with only the new head trainable, then gradually unfreeze layers from the top down, each time lowering the learning rate.\n",
    "\n",
    "```python\n",
    "# Phase 1: train only head\n",
    "for name, param in model.named_parameters():\n",
    "    if 'fc' not in name:\n",
    "        param.requires_grad = False\n",
    "# train for a few epochs...\n",
    "\n",
    "# Phase 2: unfreeze last LSTM layer\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lstm.2' in name:  # last layer\n",
    "        param.requires_grad = True\n",
    "# continue training with lower LR...\n",
    "\n",
    "# Phase 3: unfreeze all\n",
    "for param in model.parameters():\n",
    "    param.requires_grad = True\n",
    "# train with very low LR\n",
    "```\n",
    "\n",
    "### 52.3.3 Discriminative Learning Rates\n",
    "\n",
    "Instead of a single learning rate, use different rates for different layers: lower for early layers (which capture general features) and higher for later layers (task\u2011specific). This is common in fine\u2011tuning NLP models.\n",
    "\n",
    "```python\n",
    "# Group parameters\n",
    "params = []\n",
    "for name, param in model.named_parameters():\n",
    "    if 'lstm.0' in name:\n",
    "        params.append({'params': param, 'lr': 1e-5})\n",
    "    elif 'lstm.1' in name:\n",
    "        params.append({'params': param, 'lr': 1e-4})\n",
    "    elif 'fc' in name:\n",
    "        params.append({'params': param, 'lr': 1e-3})\n",
    "optimizer = optim.Adam(params)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 52.4 Domain Adaptation\n",
    "\n",
    "Domain adaptation is a subfield of transfer learning that deals with situations where the source and target domains have different distributions (e.g., US stocks vs. NEPSE stocks). The goal is to adapt the model to the target domain, often without any labels in the target (unsupervised domain adaptation) or with few labels (semi\u2011supervised).\n",
    "\n",
    "### 52.4.1 Maximum Mean Discrepancy (MMD)\n",
    "\n",
    "MMD is a distance between distributions in a reproducing kernel Hilbert space. By minimising MMD between source and target feature representations, we can learn domain\u2011invariant features.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "def mmd_loss(x_source, x_target, kernel='rbf'):\n",
    "    \"\"\"Compute MMD between source and target features.\"\"\"\n",
    "    if kernel == 'rbf':\n",
    "        sigma = 1.0\n",
    "        source_source = torch.exp(-torch.cdist(x_source, x_source)**2 / (2 * sigma**2))\n",
    "        target_target = torch.exp(-torch.cdist(x_target, x_target)**2 / (2 * sigma**2))\n",
    "        source_target = torch.exp(-torch.cdist(x_source, x_target)**2 / (2 * sigma**2))\n",
    "        mmd = source_source.mean() + target_target.mean() - 2 * source_target.mean()\n",
    "    return mmd\n",
    "\n",
    "# In training loop, after forward pass, get features from some layer (e.g., before classifier)\n",
    "features_source = model.get_features(source_batch)\n",
    "features_target = model.get_features(target_batch)\n",
    "loss_mmd = mmd_loss(features_source, features_target)\n",
    "total_loss = loss_task + lambda_mmd * loss_mmd\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "By adding MMD loss to the task loss, we encourage the model to learn representations that are similar across domains, thus improving generalisation to the target.\n",
    "\n",
    "### 52.4.2 Adversarial Domain Adaptation\n",
    "\n",
    "Inspired by GANs, adversarial domain adaptation uses a domain discriminator to distinguish source from target features, and the feature extractor is trained to fool it. This results in domain\u2011invariant features.\n",
    "\n",
    "```python\n",
    "class FeatureExtractor(nn.Module):\n",
    "    # LSTM part\n",
    "    def forward(self, x):\n",
    "        # return features\n",
    "\n",
    "class TaskClassifier(nn.Module):\n",
    "    # final layers for prediction\n",
    "\n",
    "class DomainDiscriminator(nn.Module):\n",
    "    def forward(self, features):\n",
    "        # binary classifier: source vs target\n",
    "\n",
    "# Training loop\n",
    "for source_x, source_y, target_x in dataloader:\n",
    "    # Extract features\n",
    "    source_features = feature_extractor(source_x)\n",
    "    target_features = feature_extractor(target_x)\n",
    "\n",
    "    # Task loss on source\n",
    "    task_output = task_classifier(source_features)\n",
    "    task_loss = criterion(task_output, source_y)\n",
    "\n",
    "    # Domain loss\n",
    "    domain_source = domain_discriminator(source_features.detach())\n",
    "    domain_target = domain_discriminator(target_features.detach())\n",
    "    domain_loss_source = domain_criterion(domain_source, torch.ones_like(domain_source))\n",
    "    domain_loss_target = domain_criterion(domain_target, torch.zeros_like(domain_target))\n",
    "    domain_loss = (domain_loss_source + domain_loss_target) / 2\n",
    "\n",
    "    # Total loss for discriminator\n",
    "    disc_loss = domain_loss\n",
    "    disc_optimizer.zero_grad()\n",
    "    disc_loss.backward()\n",
    "    disc_optimizer.step()\n",
    "\n",
    "    # Adversarial loss for feature extractor (reverse gradient)\n",
    "    domain_source_adv = domain_discriminator(source_features)\n",
    "    domain_target_adv = domain_discriminator(target_features)\n",
    "    adv_loss = -domain_criterion(domain_source_adv, torch.ones_like(domain_source_adv)) \\\n",
    "               -domain_criterion(domain_target_adv, torch.zeros_like(domain_target_adv))\n",
    "    total_loss = task_loss + lambda_adv * adv_loss\n",
    "    feat_optimizer.zero_grad()\n",
    "    total_loss.backward()\n",
    "    feat_optimizer.step()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The feature extractor tries to make features indistinguishable to the domain discriminator, while the discriminator tries to tell them apart. The equilibrium yields domain\u2011invariant features.\n",
    "\n",
    "---\n",
    "\n",
    "## 52.5 Self\u2011Supervised Learning for Time\u2011Series\n",
    "\n",
    "Self\u2011supervised learning (SSL) has become a dominant paradigm for learning representations without labels. Several SSL methods have been adapted to time\u2011series.\n",
    "\n",
    "### 52.5.1 Contrastive Learning (SimCLR for Time\u2011Series)\n",
    "\n",
    "Contrastive learning aims to pull together representations of different views of the same sample (positive pairs) and push apart views of different samples (negative pairs). For time\u2011series, we can create views by:\n",
    "\n",
    "- Adding noise\n",
    "- Cropping and resizing\n",
    "- Time warping\n",
    "- Masking\n",
    "\n",
    "**Example: SimCLR\u2011style contrastive learning for time\u2011series**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class ContrastiveTransform:\n",
    "    \"\"\"Apply two random augmentations to a time-series.\"\"\"\n",
    "    def __call__(self, x):\n",
    "        # x: (seq_len, features)\n",
    "        aug1 = add_noise(x, scale=0.01)\n",
    "        aug2 = time_warp(x, warp_factor=0.1)\n",
    "        return aug1, aug2\n",
    "\n",
    "def contrastive_loss(z1, z2, temperature=0.5):\n",
    "    \"\"\"NT-Xent loss.\"\"\"\n",
    "    batch_size = z1.shape[0]\n",
    "    z = torch.cat([z1, z2], dim=0)  # 2N\n",
    "    similarity = F.cosine_similarity(z.unsqueeze(1), z.unsqueeze(0), dim=2) / temperature\n",
    "    # Mask out self-comparisons\n",
    "    mask = torch.eye(2*batch_size, device=z.device).bool()\n",
    "    similarity.masked_fill_(mask, -float('inf'))\n",
    "    # Positive pairs: (i, i+batch_size) for i=0..N-1\n",
    "    labels = torch.cat([torch.arange(batch_size, 2*batch_size), torch.arange(0, batch_size)], dim=0)\n",
    "    loss = F.cross_entropy(similarity, labels)\n",
    "    return loss\n",
    "\n",
    "# Training loop\n",
    "model = TimeSeriesEncoder()  # e.g., LSTM or Transformer\n",
    "projector = nn.Sequential(nn.Linear(hidden_dim, 128), nn.ReLU(), nn.Linear(128, 64))\n",
    "transform = ContrastiveTransform()\n",
    "\n",
    "for batch_x in unlabeled_dataloader:\n",
    "    x1, x2 = transform(batch_x), transform(batch_x)  # two views\n",
    "    h1 = model(x1)\n",
    "    h2 = model(x2)\n",
    "    z1 = projector(h1)\n",
    "    z2 = projector(h2)\n",
    "    loss = contrastive_loss(z1, z2)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The model learns to produce similar representations for two augmented views of the same time\u2011series, while distinguishing them from views of other series. After training, the encoder can be used for downstream tasks.\n",
    "\n",
    "### 52.5.2 Temporal Contrastive Learning\n",
    "\n",
    "For time\u2011series, we can also use temporal proximity: nearby time steps should have similar representations, while distant ones should differ. This is similar to CPC (Contrastive Predictive Coding).\n",
    "\n",
    "```python\n",
    "# Example: predict future representations from past\n",
    "past = encoder(series[:, :t])\n",
    "future = encoder(series[:, t:])\n",
    "# Use contrastive loss to match correct future with its past against negatives\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 52.6 Foundation Models for Time\u2011Series\n",
    "\n",
    "Recently, large pre\u2011trained models for time\u2011series have emerged, analogous to GPT and BERT in NLP. These **foundation models** are trained on massive collections of time\u2011series data and can be fine\u2011tuned for various tasks. Examples include:\n",
    "\n",
    "- **Chronos** (Amazon): Pre\u2011trained on a large corpus of time\u2011series.\n",
    "- **Lag\u2011Llama**: A foundation model for forecasting.\n",
    "- **Moirai**: Another time\u2011series foundation model.\n",
    "\n",
    "These models can be used in a zero\u2011shot or few\u2011shot manner, potentially outperforming models trained from scratch on small datasets like NEPSE.\n",
    "\n",
    "### 52.6.1 Using a Pre\u2011trained Foundation Model\n",
    "\n",
    "Assuming we have access to a pre\u2011trained Chronos model (via HuggingFace or similar), we can use it for NEPSE prediction.\n",
    "\n",
    "```python\n",
    "from transformers import ChronosModel, ChronosConfig\n",
    "\n",
    "# Load pre-trained model\n",
    "model = ChronosModel.from_pretrained('amazon/chronos-t5-small')\n",
    "\n",
    "# Prepare NEPSE data in the required format\n",
    "# (may need tokenization or patching)\n",
    "\n",
    "# Fine-tune on NEPSE (if supported)\n",
    "# Or use zero-shot: simply feed NEPSE data and get predictions\n",
    "```\n",
    "\n",
    "**Note:** Foundation models are still an emerging area; availability and ease of use vary.\n",
    "\n",
    "---\n",
    "\n",
    "## 52.7 Multi\u2011Task Learning\n",
    "\n",
    "Multi\u2011task learning (MTL) can be seen as a form of transfer learning where knowledge is shared across tasks. In the NEPSE context, we might jointly predict:\n",
    "\n",
    "- Next\u2011day direction (classification)\n",
    "- Next\u2011week volatility (regression)\n",
    "- Trading volume (regression)\n",
    "\n",
    "The shared layers learn representations useful for all tasks, which can improve performance on each, especially if data for some tasks is limited.\n",
    "\n",
    "**Implementation with PyTorch (shared LSTM + task\u2011specific heads)** \u2013 as shown earlier.\n",
    "\n",
    "---\n",
    "\n",
    "## 52.8 Few\u2011Shot Learning\n",
    "\n",
    "Few\u2011shot learning aims to learn from a very small number of labeled examples. This is relevant for NEPSE when a new stock is listed with only a few months of data. Techniques include:\n",
    "\n",
    "- **Metric\u2011based methods**: Learn an embedding space where examples of the same class are close. For a new class, compare its few examples to the embeddings of known classes.\n",
    "- **Prototypical networks**: Compute a prototype (mean embedding) for each class from few shots; classify new points by nearest prototype.\n",
    "- **Meta\u2011learning (learning to learn)**: Train on many episodes of few\u2011shot tasks to learn how to adapt quickly.\n",
    "\n",
    "**Example: Prototypical Network for few\u2011shot classification of NEPSE patterns**\n",
    "\n",
    "```python\n",
    "class PrototypicalNetwork(nn.Module):\n",
    "    def __init__(self, encoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "\n",
    "    def forward(self, support, query, n_way, k_shot):\n",
    "        # support: (n_way * k_shot, seq_len, features)\n",
    "        # query: (n_way * query_per_class, ...)\n",
    "        support_emb = self.encoder(support)\n",
    "        query_emb = self.encoder(query)\n",
    "\n",
    "        # Compute prototypes (mean per class)\n",
    "        prototypes = support_emb.view(n_way, k_shot, -1).mean(dim=1)\n",
    "\n",
    "        # Compute distances from query to prototypes\n",
    "        dist = torch.cdist(query_emb, prototypes, p=2)  # (n_query, n_way)\n",
    "        logits = -dist\n",
    "        return logits\n",
    "\n",
    "# Training: sample episodes from base classes\n",
    "# Fine-tuning: on new stock with few examples\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The model learns an embedding function that clusters same\u2011class examples together. When faced with a new class (e.g., a newly listed stock), we compute its prototype from the few labeled examples and classify by proximity.\n",
    "\n",
    "---\n",
    "\n",
    "## 52.9 Implementation Considerations\n",
    "\n",
    "When applying transfer learning to the NEPSE system, consider:\n",
    "\n",
    "- **Data preprocessing**: Ensure source and target data are preprocessed similarly (same features, scaling, sequence length). If not, you may need to adapt.\n",
    "- **Model architecture**: The pre\u2011trained model's input size must match the target. If features differ, you may need to add a projection layer.\n",
    "- **Overfitting**: Fine\u2011tuning on a very small dataset can still overfit. Use strong regularisation (dropout, weight decay) and early stopping.\n",
    "- **Evaluation**: Use time\u2011series cross\u2011validation to estimate performance. Compare with training from scratch.\n",
    "- **Computational cost**: Pre\u2011training on large datasets may be expensive. Use cloud GPUs or existing pre\u2011trained models.\n",
    "\n",
    "**Example: Projection layer for mismatched features**\n",
    "\n",
    "```python\n",
    "# If pre-trained model expects 10 features but we have 8\n",
    "class AdaptedModel(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super().__init__()\n",
    "        self.projection = nn.Linear(8, 10)  # map our 8 to 10\n",
    "        self.pretrained = pretrained_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.projection(x)\n",
    "        return self.pretrained(x)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 52.10 Limitations and Pitfalls\n",
    "\n",
    "- **Negative transfer**: If source and target are too dissimilar, pre\u2011training may hurt performance.\n",
    "- **Catastrophic forgetting**: During fine\u2011tuning, the model may forget useful source knowledge. Use lower learning rates and possibly replay.\n",
    "- **Domain shift**: Financial markets differ significantly across countries (regulations, investor behaviour). Simple fine\u2011tuning may not suffice; domain adaptation may be necessary.\n",
    "- **Temporal shift**: Even within the same market, the distribution changes over time. Pre\u2011training on old data and fine\u2011tuning on recent may still face concept drift.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored transfer learning and pre\u2011training techniques and their application to the NEPSE stock prediction system. We covered:\n",
    "\n",
    "- The core concepts of transfer learning and when it is beneficial.\n",
    "- Supervised pre\u2011training on a related large dataset (e.g., US stocks).\n",
    "- Self\u2011supervised pre\u2011training methods like masked autoencoders and contrastive learning to learn from unlabeled data.\n",
    "- Multi\u2011task pre\u2011training to learn shared representations.\n",
    "- Fine\u2011tuning strategies, including progressive unfreezing and discriminative learning rates.\n",
    "- Domain adaptation techniques (MMD, adversarial) to handle distribution shifts.\n",
    "- Few\u2011shot learning for scenarios with very limited labeled data.\n",
    "- Emerging foundation models for time\u2011series.\n",
    "- Practical implementation tips and potential pitfalls.\n",
    "\n",
    "By leveraging transfer learning, we can build more accurate NEPSE prediction models even with limited local data, and adapt more quickly to new stocks or changing market conditions. In the next chapter, we will discuss **Automated Machine Learning** and how to automate the process of model selection, feature engineering, and hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 52**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='51. ensemble_methods.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='53. automated_machine_learning.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}