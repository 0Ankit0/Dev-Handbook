{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 60: Advanced Optimization Techniques\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the limitations of basic gradient descent and when advanced optimizers are needed\n",
    "- Implement and apply gradient descent variants: SGD with momentum, Nesterov accelerated gradient, AdaGrad, RMSProp, and Adam\n",
    "- Leverage second‑order optimization methods (Newton's method, quasi‑Newton, L‑BFGS) for faster convergence\n",
    "- Apply adaptive optimization techniques that adjust learning rates per parameter\n",
    "- Implement distributed optimization for training large models across multiple machines\n",
    "- Formulate and solve multi‑objective optimization problems (e.g., balancing profit and risk in trading)\n",
    "- Handle constrained optimization using Lagrange multipliers and projection methods\n",
    "- Automate hyperparameter optimization using grid search, random search, Bayesian methods, and population‑based training\n",
    "- Understand neural architecture search (NAS) for finding optimal network structures\n",
    "- Apply meta‑learning to learn optimizers or initialization strategies\n",
    "- Recognise practical tips and pitfalls when applying advanced optimizers to the NEPSE prediction system\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Throughout this handbook, we have trained numerous models for the NEPSE prediction system: linear models, tree‑based ensembles, neural networks, and more. Each of these models was optimised using some form of gradient‑based algorithm, typically stochastic gradient descent (SGD) or one of its variants. But as models become deeper, datasets larger, and objectives more complex, the choice of optimisation algorithm becomes critical. The right optimiser can mean the difference between a model that converges in hours versus days, or one that finds a good local minimum versus getting stuck.\n",
    "\n",
    "**Optimisation** in machine learning is the process of minimising (or maximising) an objective function, typically the loss function, with respect to the model's parameters. While basic SGD is simple and effective for many problems, advanced techniques can accelerate convergence, handle non‑stationary objectives, escape poor local minima, and scale to massive datasets.\n",
    "\n",
    "In this chapter, we will explore a spectrum of optimisation techniques, from improved gradient‑based methods to second‑order approaches, distributed optimisation, and even optimisation of the optimisation process itself (meta‑learning). Using the NEPSE prediction system as a motivating example, we will implement these techniques and discuss when and why to use them.\n",
    "\n",
    "---\n",
    "\n",
    "## 60.1 Gradient Descent Variants\n",
    "\n",
    "Gradient descent updates parameters in the direction of the negative gradient of the loss function. The basic update is:\n",
    "\n",
    "`θ ← θ - η ∇_θ L(θ)`\n",
    "\n",
    "where `η` is the learning rate. Variants modify this update to improve convergence speed and stability.\n",
    "\n",
    "### 60.1.1 Stochastic Gradient Descent (SGD) with Momentum\n",
    "\n",
    "Momentum helps accelerate gradients in the right direction and dampens oscillations. It accumulates a velocity vector:\n",
    "\n",
    "`v_t = β v_{t-1} + (1 - β) ∇_θ L(θ)`\n",
    "`θ ← θ - η v_t`\n",
    "\n",
    "Typical values: `β = 0.9` or `0.99`.\n",
    "\n",
    "**PyTorch implementation:**\n",
    "\n",
    "```python\n",
    "import torch.optim as optim\n",
    "\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Momentum helps the optimizer build up speed in directions of consistent gradient, smoothing out updates and speeding convergence. For NEPSE training, momentum can help escape shallow local minima and navigate noisy loss landscapes.\n",
    "\n",
    "### 60.1.2 Nesterov Accelerated Gradient (NAG)\n",
    "\n",
    "Nesterov momentum looks ahead by computing the gradient at the approximate future position:\n",
    "\n",
    "`v_t = β v_{t-1} + (1 - β) ∇_θ L(θ - β v_{t-1})`\n",
    "`θ ← θ - η v_t`\n",
    "\n",
    "This \"peek ahead\" often leads to faster convergence and better performance.\n",
    "\n",
    "```python\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.9, nesterov=True)\n",
    "```\n",
    "\n",
    "### 60.1.3 AdaGrad\n",
    "\n",
    "AdaGrad adapts the learning rate per parameter based on the historical sum of squared gradients. Parameters with large gradients get smaller learning rates, and vice versa.\n",
    "\n",
    "`G_t = G_{t-1} + (∇_θ L(θ))²`\n",
    "`θ ← θ - η / (√(G_t + ε)) * ∇_θ L(θ)`\n",
    "\n",
    "AdaGrad works well for sparse features (like one‑hot encoded categories) but the learning rate can become too small over time.\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=0.01)\n",
    "```\n",
    "\n",
    "### 60.1.4 RMSProp\n",
    "\n",
    "RMSProp modifies AdaGrad to use a moving average of squared gradients, preventing the learning rate from decaying too aggressively.\n",
    "\n",
    "`E[g²]_t = β E[g²]_{t-1} + (1 - β) (∇_θ L(θ))²`\n",
    "`θ ← θ - η / √(E[g²]_t + ε) * ∇_θ L(θ)`\n",
    "\n",
    "Typical `β = 0.9`.\n",
    "\n",
    "```python\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=0.001, alpha=0.9)\n",
    "```\n",
    "\n",
    "### 60.1.5 Adam (Adaptive Moment Estimation)\n",
    "\n",
    "Adam combines momentum and RMSProp, maintaining both a moving average of gradients (first moment) and squared gradients (second moment).\n",
    "\n",
    "`m_t = β₁ m_{t-1} + (1 - β₁) ∇_θ L(θ)`\n",
    "`v_t = β₂ v_{t-1} + (1 - β₂) (∇_θ L(θ))²`\n",
    "`m̂_t = m_t / (1 - β₁ᵗ)`  (bias correction)\n",
    "`v̂_t = v_t / (1 - β₂ᵗ)`\n",
    "`θ ← θ - η m̂_t / (√(v̂_t) + ε)`\n",
    "\n",
    "Default values: `β₁ = 0.9`, `β₂ = 0.999`, `ε = 1e-8`. Adam is often the default choice for training neural networks.\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "```\n",
    "\n",
    "**Which to use for NEPSE?**  \n",
    "For the LSTM models we built earlier, Adam is a safe starting point. For linear models or simple networks, SGD with momentum may suffice. The choice can be tuned as a hyperparameter.\n",
    "\n",
    "---\n",
    "\n",
    "## 60.2 Second‑Order Methods\n",
    "\n",
    "First‑order methods use only gradient information. Second‑order methods also use curvature information (the Hessian matrix) to make more informed steps, potentially converging in fewer iterations.\n",
    "\n",
    "### 60.2.1 Newton's Method\n",
    "\n",
    "Newton's method updates parameters using:\n",
    "\n",
    "`θ ← θ - H⁻¹ ∇_θ L(θ)`\n",
    "\n",
    "where `H` is the Hessian matrix (second derivatives). This can give quadratic convergence near the optimum, but computing and inverting the Hessian is expensive (O(n³)) and infeasible for large models.\n",
    "\n",
    "### 60.2.2 Quasi‑Newton Methods (BFGS, L‑BFGS)\n",
    "\n",
    "Quasi‑Newton methods approximate the inverse Hessian using only gradient information, avoiding the O(n³) cost. BFGS (Broyden–Fletcher–Goldfarb–Shanno) builds an approximation iteratively. L‑BFGS (Limited‑memory BFGS) is a memory‑efficient version that stores only a few vectors, making it suitable for large problems.\n",
    "\n",
    "L‑BFGS is often used for optimisation problems with a small number of iterations, such as training logistic regression or support vector machines.\n",
    "\n",
    "**Example with `scipy`**\n",
    "\n",
    "```python\n",
    "from scipy.optimize import minimize\n",
    "import numpy as np\n",
    "\n",
    "# Define loss function and gradient for a simple model\n",
    "def loss(params, X, y):\n",
    "    # params: model weights\n",
    "    y_pred = X @ params\n",
    "    return np.mean((y - y_pred)**2)\n",
    "\n",
    "def grad(params, X, y):\n",
    "    y_pred = X @ params\n",
    "    return -2 * X.T @ (y - y_pred) / len(y)\n",
    "\n",
    "# Optimize\n",
    "X_train, y_train = ...  # numpy arrays\n",
    "initial_params = np.zeros(X_train.shape[1])\n",
    "result = minimize(loss, initial_params, args=(X_train, y_train), method='L-BFGS-B', jac=grad)\n",
    "print(result.x)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "L‑BFGS is a batch method (uses full dataset) and is well‑suited for convex problems with a moderate number of parameters. For the NEPSE system, it could be used to train linear models or the final layer of a neural network.\n",
    "\n",
    "---\n",
    "\n",
    "## 60.3 Adaptive Optimization\n",
    "\n",
    "Adaptive optimisers (AdaGrad, RMSProp, Adam) are a class of methods that adjust learning rates per parameter based on historical gradients. They are particularly useful when the loss landscape has different curvatures in different directions, which is common in deep learning.\n",
    "\n",
    "### 60.3.1 Learning Rate Scheduling\n",
    "\n",
    "Even with adaptive methods, manually annealing the learning rate can improve final performance. Common schedules:\n",
    "\n",
    "- **Step decay**: Reduce learning rate by a factor every few epochs.\n",
    "- **Exponential decay**: `η = η₀ * exp(-k t)`\n",
    "- **Cosine annealing**: `η = η_min + 0.5(η_max - η_min)(1 + cos(t/T π))`\n",
    "\n",
    "**PyTorch example with step decay:**\n",
    "\n",
    "```python\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=10, gamma=0.1)\n",
    "\n",
    "for epoch in range(50):\n",
    "    train_one_epoch()\n",
    "    scheduler.step()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "After every 10 epochs, the learning rate is multiplied by 0.1. This allows large steps early and fine‑tuning later.\n",
    "\n",
    "### 60.3.2 Warmup\n",
    "\n",
    "For very deep networks or transformers, a **learning rate warmup** (gradually increasing the learning rate from a small value to the target) prevents early instability.\n",
    "\n",
    "```python\n",
    "def warmup_lambda(epoch):\n",
    "    if epoch < 5:\n",
    "        return epoch / 5  # linear increase\n",
    "    else:\n",
    "        return 1.0\n",
    "\n",
    "scheduler = optim.lr_scheduler.LambdaLR(optimizer, lr_lambda=warmup_lambda)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 60.4 Distributed Optimization\n",
    "\n",
    "When the dataset or model is too large to fit on one machine, we must distribute the optimisation across multiple workers. Two main paradigms exist:\n",
    "\n",
    "- **Data parallelism**: Each worker has a copy of the model and processes a different subset of data; gradients are aggregated.\n",
    "- **Model parallelism**: Different parts of the model are placed on different devices (e.g., layers on different GPUs).\n",
    "\n",
    "### 60.4.1 Synchronous SGD\n",
    "\n",
    "In synchronous SGD, all workers compute gradients on their batch, then gradients are averaged (e.g., via all‑reduce) before updating the model. This is equivalent to using a larger batch size.\n",
    "\n",
    "**PyTorch distributed example (simplified):**\n",
    "\n",
    "```python\n",
    "import torch.distributed as dist\n",
    "import torch.multiprocessing as mp\n",
    "\n",
    "def train(rank, world_size):\n",
    "    dist.init_process_group('nccl', rank=rank, world_size=world_size)\n",
    "    model = MyModel().to(rank)\n",
    "    model = nn.parallel.DistributedDataParallel(model, device_ids=[rank])\n",
    "    optimizer = optim.Adam(model.parameters())\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for data, target in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(data.to(rank))\n",
    "            loss = criterion(output, target.to(rank))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    world_size = 4\n",
    "    mp.spawn(train, args=(world_size,), nprocs=world_size)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`DistributedDataParallel` synchronises gradients across processes using all‑reduce. This scales linearly with the number of GPUs.\n",
    "\n",
    "### 60.4.2 Asynchronous SGD\n",
    "\n",
    "In asynchronous SGD, workers update a central parameter server independently, without waiting for others. This can be faster but may lead to stale gradients and convergence issues. It is less common today.\n",
    "\n",
    "### 60.4.3 Distributed Optimisers for Large‑Scale NEPSE\n",
    "\n",
    "If we were to train a deep neural network on decades of tick data for all 200+ NEPSE stocks, distributed optimisation would be essential. Using PyTorch's `DistributedDataParallel` on a multi‑GPU server or across a cluster would be the practical approach.\n",
    "\n",
    "---\n",
    "\n",
    "## 60.5 Multi‑Objective Optimization\n",
    "\n",
    "In the NEPSE prediction system, we often have multiple, possibly conflicting objectives. For example, we might want to:\n",
    "\n",
    "- Maximise prediction accuracy\n",
    "- Minimise model complexity (to avoid overfitting)\n",
    "- Maximise trading profit\n",
    "- Minimise risk (drawdown)\n",
    "\n",
    "**Multi‑objective optimisation** seeks to find a set of solutions that trade off between objectives, known as the **Pareto front**.\n",
    "\n",
    "### 60.5.1 Scalarisation\n",
    "\n",
    "The simplest approach is to combine objectives into a single scalar loss:\n",
    "\n",
    "`L_total = λ₁ L₁ + λ₂ L₂ + ...`\n",
    "\n",
    "The weights `λ` reflect preferences. For example, in training a trading agent, we could combine profit and risk:\n",
    "\n",
    "`L = -profit + λ * volatility`\n",
    "\n",
    "**Example: Training a neural network with L1 regularisation (sparsity)**\n",
    "\n",
    "```python\n",
    "def loss_fn(output, target, model, lambda_reg):\n",
    "    mse = nn.MSELoss()(output, target)\n",
    "    l1 = sum(p.abs().sum() for p in model.parameters())\n",
    "    return mse + lambda_reg * l1\n",
    "```\n",
    "\n",
    "### 60.5.2 Pareto Optimisation\n",
    "\n",
    "When we want to explore the entire trade‑off surface, we can use evolutionary algorithms like **NSGA‑II** (Non‑dominated Sorting Genetic Algorithm). These maintain a population of solutions and evolve them towards the Pareto front.\n",
    "\n",
    "**Example with `pymoo` library:**\n",
    "\n",
    "```python\n",
    "from pymoo.algorithms.nsga2 import NSGA2\n",
    "from pymoo.factory import get_problem\n",
    "from pymoo.optimize import minimize\n",
    "from pymoo.core.problem import Problem\n",
    "\n",
    "class NEPSEProblem(Problem):\n",
    "    def __init__(self):\n",
    "        super().__init__(n_var=10, n_obj=2, xl=-5, xu=5)\n",
    "\n",
    "    def _evaluate(self, x, out, *args, **kwargs):\n",
    "        # x: decision variables (e.g., model hyperparameters)\n",
    "        # Compute two objectives: negative profit and risk\n",
    "        profit = compute_profit(x)\n",
    "        risk = compute_risk(x)\n",
    "        out[\"F\"] = np.column_stack([-profit, risk])\n",
    "\n",
    "problem = NEPSEProblem()\n",
    "algorithm = NSGA2(pop_size=100)\n",
    "res = minimize(problem, algorithm, ('n_gen', 50), seed=42, verbose=True)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "NSGA‑2 evolves a population of solutions, each represented by a vector of decision variables (e.g., learning rate, network depth, regularisation strength). The result is a set of Pareto‑optimal solutions, each offering a different trade‑off between profit and risk.\n",
    "\n",
    "---\n",
    "\n",
    "## 60.6 Constrained Optimization\n",
    "\n",
    "Sometimes we have constraints: the portfolio weights must sum to 1, or the predicted price must be positive, or the model's complexity must stay below a threshold.\n",
    "\n",
    "### 60.6.1 Projected Gradient Descent\n",
    "\n",
    "For simple constraints like box constraints or norm constraints, we can project the parameters back into the feasible set after each gradient step.\n",
    "\n",
    "```python\n",
    "def project_weights(weights):\n",
    "    # Ensure weights sum to 1 and are non‑negative\n",
    "    weights = np.maximum(weights, 0)\n",
    "    return weights / weights.sum()\n",
    "\n",
    "# In training loop\n",
    "optimizer.step()\n",
    "with torch.no_grad():\n",
    "    model.fc.weight.data = project_weights(model.fc.weight.data)\n",
    "```\n",
    "\n",
    "### 60.6.2 Lagrange Multipliers\n",
    "\n",
    "For more complex constraints, we can use the method of Lagrange multipliers, converting a constrained problem into an unconstrained one by adding penalty terms.\n",
    "\n",
    "**Example: Training with a constraint on model size (number of non‑zero weights)**\n",
    "\n",
    "We can add a Lagrangian term: `L_total = L + λ (||θ||₀ - target)` but the L0 norm is non‑differentiable. Instead, we often use L1 regularisation as a proxy.\n",
    "\n",
    "### 60.6.3 Augmented Lagrangian and ADMM\n",
    "\n",
    "The **Augmented Lagrangian Method** and **Alternating Direction Method of Multipliers (ADMM)** are powerful techniques for constrained optimisation, often used in distributed settings.\n",
    "\n",
    "---\n",
    "\n",
    "## 60.7 Hyperparameter Optimization\n",
    "\n",
    "Hyperparameters (learning rate, batch size, network architecture) are not learned during training but must be tuned. This is itself an optimisation problem.\n",
    "\n",
    "### 60.7.1 Grid Search and Random Search\n",
    "\n",
    "Grid search exhaustively tries all combinations in a predefined grid. Random search samples combinations randomly and often finds good configurations faster.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import uniform, randint\n",
    "\n",
    "param_dist = {\n",
    "    'learning_rate': uniform(1e-4, 1e-2),\n",
    "    'batch_size': randint(16, 128),\n",
    "    'num_layers': randint(1, 5)\n",
    "}\n",
    "\n",
    "random_search = RandomizedSearchCV(estimator, param_dist, n_iter=20, cv=3)\n",
    "random_search.fit(X_train, y_train)\n",
    "print(random_search.best_params_)\n",
    "```\n",
    "\n",
    "### 60.7.2 Bayesian Optimization\n",
    "\n",
    "Bayesian optimisation builds a probabilistic model (e.g., Gaussian process) of the objective function and uses it to select the most promising hyperparameters to evaluate next. It is more efficient than random search.\n",
    "\n",
    "**Example with `scikit-optimize`**\n",
    "\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Real, Integer\n",
    "\n",
    "search_spaces = {\n",
    "    'learning_rate': Real(1e-4, 1e-2, prior='log'),\n",
    "    'batch_size': Integer(16, 128),\n",
    "    'num_layers': Integer(1, 5)\n",
    "}\n",
    "\n",
    "bayes_search = BayesSearchCV(estimator, search_spaces, n_iter=30, cv=3)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "print(bayes_search.best_params_)\n",
    "```\n",
    "\n",
    "### 60.7.3 Population‑Based Training (PBT)\n",
    "\n",
    "PBT, used in DeepMind's AlphaStar, trains a population of models in parallel, periodically copying the weights of better‑performing models and mutating their hyperparameters. It dynamically adapts hyperparameters during training.\n",
    "\n",
    "**Simplified PBT loop:**\n",
    "\n",
    "```python\n",
    "# Population of models with different hyperparameters\n",
    "population = [Model(hp) for hp in initial_hps]\n",
    "\n",
    "for step in range(total_steps):\n",
    "    for model in population:\n",
    "        model.train_one_step()\n",
    "    if step % exploit_interval == 0:\n",
    "        # Sort by performance\n",
    "        population.sort(key=lambda m: m.performance)\n",
    "        # Replace worst 20% with mutated copies of best 20%\n",
    "        for i in range(len(population) // 5):\n",
    "            best = population[-i-1]\n",
    "            worst = population[i]\n",
    "            worst.load_state_dict(best.state_dict())\n",
    "            worst.hp = mutate(best.hp)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 60.8 Neural Architecture Search (NAS)\n",
    "\n",
    "NAS automates the design of neural network architectures. It is computationally expensive but can find architectures that outperform manual designs.\n",
    "\n",
    "### 60.8.1 Reinforcement Learning‑based NAS\n",
    "\n",
    "A controller (e.g., an RNN) proposes architectures, which are trained and evaluated. The validation accuracy is used as a reward to update the controller.\n",
    "\n",
    "### 60.8.2 One‑shot / Weight‑sharing NAS\n",
    "\n",
    "To reduce cost, one‑shot methods train a super‑network that contains all possible architectures, then search for the best sub‑network without retraining.\n",
    "\n",
    "**Example with `autokeras`**\n",
    "\n",
    "```python\n",
    "import autokeras as ak\n",
    "\n",
    "clf = ak.StructuredDataClassifier(max_trials=10)  # tries 10 architectures\n",
    "clf.fit(X_train, y_train)\n",
    "model = clf.export_model()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "AutoKeras searches over different network architectures (number of layers, units, activation functions) using Bayesian optimisation and weight sharing.\n",
    "\n",
    "---\n",
    "\n",
    "## 60.9 Meta‑Learning\n",
    "\n",
    "Meta‑learning, or \"learning to learn\", aims to design optimisers or initialisations that generalise across tasks. Two popular approaches:\n",
    "\n",
    "- **Optimiser learning**: Learn an update rule (e.g., a neural network) that outperforms hand‑designed optimisers like Adam.\n",
    "- **Model‑agnostic meta‑learning (MAML)**: Learn an initialisation that can be fine‑tuned quickly to new tasks with few gradient steps.\n",
    "\n",
    "### 60.9.1 Learned Optimisers\n",
    "\n",
    "Instead of using a fixed formula like Adam, we can train an RNN to propose parameter updates. The RNN itself is trained on many optimisation tasks to minimise total loss.\n",
    "\n",
    "**Simplified concept:**\n",
    "\n",
    "```python\n",
    "# Optimizer network (e.g., LSTM) takes gradient and state, outputs update\n",
    "optimizer_net = LSTMOptimizer()\n",
    "state = None\n",
    "\n",
    "for step in range(T):\n",
    "    loss = model(data)\n",
    "    loss.backward()\n",
    "    grads = [p.grad for p in model.parameters()]\n",
    "    update, state = optimizer_net(grads, state)\n",
    "    apply_update(model, update)\n",
    "```\n",
    "\n",
    "### 60.9.2 MAML for Fast Adaptation in NEPSE\n",
    "\n",
    "Imagine we have many stocks, each with limited data. MAML could learn an initial model that adapts quickly to a new stock with just a few gradient steps. This is a form of transfer learning.\n",
    "\n",
    "**MAML implementation sketch:**\n",
    "\n",
    "```python\n",
    "def maml_inner_loop(model, support_set, inner_steps=5, inner_lr=0.01):\n",
    "    adapted_model = clone(model)\n",
    "    for _ in range(inner_steps):\n",
    "        loss = compute_loss(adapted_model, support_set)\n",
    "        grad = torch.autograd.grad(loss, adapted_model.parameters())\n",
    "        adapted_model = update(adapted_model, grad, inner_lr)\n",
    "    return adapted_model\n",
    "\n",
    "def maml_outer_loop(model, tasks, outer_lr=0.001):\n",
    "    meta_grads = []\n",
    "    for task in tasks:\n",
    "        support, query = task\n",
    "        adapted = maml_inner_loop(model, support)\n",
    "        query_loss = compute_loss(adapted, query)\n",
    "        meta_grads.append(torch.autograd.grad(query_loss, model.parameters()))\n",
    "    # Average gradients and update meta-model\n",
    "    avg_grad = average_gradients(meta_grads)\n",
    "    model = update(model, avg_grad, outer_lr)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "MAML trains the initial model such that after one (or a few) gradient steps on a new task, it performs well. For NEPSE, tasks could be individual stocks, and we want a model that can quickly adapt to a new stock's price dynamics.\n",
    "\n",
    "---\n",
    "\n",
    "## 60.10 Practical Tips and Pitfalls\n",
    "\n",
    "1. **Start with Adam**: For most deep learning problems on NEPSE, Adam with default settings is a solid baseline. Tune learning rate first.\n",
    "2. **Learning rate is the most important hyperparameter**: Always include it in hyperparameter searches.\n",
    "3. **Use learning rate schedules**: Step decay or cosine annealing often improve final performance.\n",
    "4. **Monitor loss curves**: If loss oscillates wildly, reduce learning rate or increase batch size.\n",
    "5. **Gradient clipping**: For RNNs, clip gradients to avoid explosion.\n",
    "   ```python\n",
    "   torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "   ```\n",
    "6. **Batch normalisation**: Helps with optimisation by normalising layer inputs.\n",
    "7. **Weight decay (L2 regularisation)**: Often included in optimisers (AdamW is Adam with correct weight decay).\n",
    "8. **Distributed training**: Ensure that the effective batch size (batch per worker × workers) matches the learning rate (linear scaling rule: double batch size, double learning rate).\n",
    "9. **Early stopping**: Stop training when validation loss plateaus to avoid overfitting.\n",
    "10. **Reproducibility**: Set random seeds and log all hyperparameters.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored advanced optimisation techniques that can significantly improve the training of models for the NEPSE prediction system. We covered:\n",
    "\n",
    "- Gradient descent variants: SGD with momentum, NAG, AdaGrad, RMSProp, and Adam.\n",
    "- Second‑order methods: Newton, BFGS, L‑BFGS for faster convergence on smaller problems.\n",
    "- Adaptive optimisation and learning rate scheduling.\n",
    "- Distributed optimisation for scaling to large datasets and models.\n",
    "- Multi‑objective optimisation for trading off conflicting goals like profit and risk.\n",
    "- Constrained optimisation using projections and Lagrange multipliers.\n",
    "- Hyperparameter optimisation via random search, Bayesian optimisation, and PBT.\n",
    "- Neural architecture search to automate network design.\n",
    "- Meta‑learning for learning to learn and fast adaptation.\n",
    "\n",
    "Choosing the right optimisation algorithm and tuning it properly is essential for achieving state‑of‑the‑art performance. For the NEPSE system, a combination of Adam with a cosine annealing schedule and hyperparameter tuning via Bayesian optimisation is a powerful and practical approach.\n",
    "\n",
    "This chapter concludes **Part XIII: Emerging Technologies and Future Trends**. In the next part, we will move to **Appendices**, which provide reference material, checklists, and templates for building and deploying time‑series prediction systems.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 60**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
