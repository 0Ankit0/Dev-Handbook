{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 58: Anomaly Detection\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand different types of anomalies in time‑series data: point, contextual, and collective\n",
    "- Apply statistical methods such as Z‑score, IQR, and modified Z‑score to detect outliers in NEPSE stock data\n",
    "- Implement machine learning models for anomaly detection, including Isolation Forest, One‑Class SVM, and Local Outlier Factor\n",
    "- Use deep learning techniques like autoencoders, LSTM autoencoders, and variational autoencoders for more complex anomaly patterns\n",
    "- Recognise the importance of domain knowledge in distinguishing true anomalies from normal market events\n",
    "- Evaluate anomaly detection performance using precision, recall, and F1‑score when labels are available\n",
    "- Build real‑time anomaly detection pipelines for monitoring NEPSE data streams\n",
    "- Interpret detected anomalies to inform trading decisions or data quality checks\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In financial markets, anomalies can signal both opportunities and risks. A sudden spike in trading volume might indicate an impending price move, an erroneous data feed, or insider trading. A sharp drop in price could be a flash crash or a genuine market correction. Detecting these unusual events in real time allows traders to react quickly and data engineers to clean noisy data.\n",
    "\n",
    "**Anomaly detection** is the task of identifying patterns in data that do not conform to expected behaviour. In time‑series, anomalies can be of three types:\n",
    "\n",
    "- **Point anomalies**: A single data point that deviates significantly from the rest (e.g., a price jump caused by a typo).\n",
    "- **Contextual anomalies**: A data point that is unusual in a specific context (e.g., a high temperature in winter; a stock price spike during a typically quiet period).\n",
    "- **Collective anomalies**: A sequence of points that is anomalous as a whole, even if individual points are normal (e.g., a flatline in price for several hours).\n",
    "\n",
    "For the NEPSE system, anomaly detection can be applied to:\n",
    "\n",
    "- **Data quality**: Detect missing or corrupted ticks in the live feed.\n",
    "- **Market surveillance**: Identify unusual trading activity that may precede announcements.\n",
    "- **Risk management**: Flag extreme price movements that could trigger circuit breakers.\n",
    "- **Feature engineering**: Create features indicating anomalous events (e.g., \"was yesterday an anomaly?\").\n",
    "\n",
    "In this chapter, we will explore a range of anomaly detection techniques, from simple statistical rules to sophisticated deep learning models. We will use the NEPSE dataset to illustrate each method and discuss how to deploy them in production.\n",
    "\n",
    "---\n",
    "\n",
    "## 58.1 Types of Anomalies\n",
    "\n",
    "Before diving into algorithms, it is essential to understand the nature of anomalies in time‑series.\n",
    "\n",
    "### 58.1.1 Point Anomalies\n",
    "\n",
    "A point anomaly is an individual data point that is far from the rest of the data. In stock prices, this could be a data entry error (e.g., a price of 10,000 NPR when the typical range is 500–600) or a genuine but extreme event (e.g., a 10% drop in one day). Point anomalies are often detected using statistical tests on the value itself.\n",
    "\n",
    "### 58.1.2 Contextual Anomalies\n",
    "\n",
    "A contextual anomaly is a point that is normal in the global sense but abnormal in a specific context. For example, a 2% daily gain might be normal, but if it occurs during a typically flat period (e.g., lunch hour), it could be suspicious. Contextual anomalies require modelling the expected behaviour conditioned on time, season, or other variables.\n",
    "\n",
    "### 58.1.3 Collective Anomalies\n",
    "\n",
    "A collective anomaly is a subsequence of the time‑series that is anomalous as a whole, even if each point individually is within normal range. Examples include a prolonged period of zero volatility (flatline) or a repeating pattern that deviates from the usual seasonality. Collective anomalies often require analysing the shape of the curve, not just individual values.\n",
    "\n",
    "In financial markets, all three types occur. A flash crash (a rapid drop and recovery) is a collective anomaly. A single erroneous tick is a point anomaly. A price spike during an earnings announcement is a contextual anomaly (normal in that context, but if it happens on a random Tuesday, it might be anomalous).\n",
    "\n",
    "---\n",
    "\n",
    "## 58.2 Statistical Methods\n",
    "\n",
    "Statistical methods are simple, fast, and often effective for point anomalies. They assume that the data follows some distribution (usually Gaussian) and flag points that deviate significantly.\n",
    "\n",
    "### 58.2.1 Z‑Score\n",
    "\n",
    "The Z‑score measures how many standard deviations a point is from the mean. For a normally distributed variable, values with |Z| > 3 are often considered outliers.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def zscore_outliers(data, threshold=3):\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    z_scores = (data - mean) / std\n",
    "    return np.abs(z_scores) > threshold\n",
    "\n",
    "# Example on NEPSE daily returns\n",
    "returns = df['NABIL'].pct_change().dropna()\n",
    "outliers = zscore_outliers(returns)\n",
    "print(f\"Detected {outliers.sum()} point anomalies via Z-score\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The Z‑score assumes the data is Gaussian and independent. Financial returns often have heavy tails, so a threshold of 3 might still produce many false positives. A higher threshold (e.g., 4) can be used for extreme anomalies.\n",
    "\n",
    "### 58.2.2 Modified Z‑Score\n",
    "\n",
    "The modified Z‑score uses the median and median absolute deviation (MAD), which are robust to outliers. It is preferred for non‑normal or heavy‑tailed distributions.\n",
    "\n",
    "```python\n",
    "def modified_zscore_outliers(data, threshold=3.5):\n",
    "    median = np.median(data)\n",
    "    mad = np.median(np.abs(data - median))\n",
    "    modified_z = 0.6745 * (data - median) / mad  # 0.6745 makes MAD comparable to std for normal\n",
    "    return np.abs(modified_z) > threshold\n",
    "\n",
    "outliers_mod = modified_zscore_outliers(returns)\n",
    "print(f\"Detected {outliers_mod.sum()} anomalies via modified Z-score\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The factor 0.6745 scales the MAD to be a consistent estimator of the standard deviation for normal data. The threshold 3.5 is commonly used.\n",
    "\n",
    "### 58.2.3 IQR (Interquartile Range)\n",
    "\n",
    "The IQR method defines outliers as points below Q1 – 1.5×IQR or above Q3 + 1.5×IQR. It is robust and does not assume normality.\n",
    "\n",
    "```python\n",
    "def iqr_outliers(data, k=1.5):\n",
    "    Q1 = np.percentile(data, 25)\n",
    "    Q3 = np.percentile(data, 75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - k * IQR\n",
    "    upper_bound = Q3 + k * IQR\n",
    "    return (data < lower_bound) | (data > upper_bound)\n",
    "\n",
    "outliers_iqr = iqr_outliers(returns)\n",
    "print(f\"Detected {outliers_iqr.sum()} anomalies via IQR\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The multiplier `k` controls sensitivity. For financial data, `k=3` might be used to detect extreme events, while `k=1.5` captures mild outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## 58.3 Machine Learning Methods\n",
    "\n",
    "Machine learning models can capture more complex patterns and are suitable for high‑dimensional data (e.g., multiple features per time step). Many anomaly detection algorithms are unsupervised, learning the normal behaviour from the data and flagging deviations.\n",
    "\n",
    "### 58.3.1 Isolation Forest\n",
    "\n",
    "Isolation Forest isolates anomalies by randomly partitioning the data. Anomalies are few and different, so they require fewer splits to isolate. The algorithm builds an ensemble of trees, and the anomaly score is based on the average path length to isolation.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import IsolationForest\n",
    "\n",
    "# Prepare data: use multiple features (e.g., returns, volume, volatility)\n",
    "features = df[['return_NABIL', 'volume_NABIL', 'volatility_NABIL']].dropna()\n",
    "X = features.values\n",
    "\n",
    "iso_forest = IsolationForest(contamination=0.01, random_state=42)  # assume 1% anomalies\n",
    "preds = iso_forest.fit_predict(X)  # -1 for anomaly, 1 for normal\n",
    "anomalies = preds == -1\n",
    "print(f\"Detected {anomalies.sum()} anomalies with Isolation Forest\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`contamination` is the expected proportion of anomalies in the data. If unknown, you can set it to 'auto' or tune it. The algorithm works well with high dimensions and does not assume any distribution.\n",
    "\n",
    "### 58.3.2 One‑Class SVM\n",
    "\n",
    "One‑Class SVM learns a boundary around the normal data and classifies points outside as anomalies. It is effective when the normal data is clustered.\n",
    "\n",
    "```python\n",
    "from sklearn.svm import OneClassSVM\n",
    "\n",
    "svm = OneClassSVM(nu=0.01, kernel='rbf', gamma='auto')\n",
    "preds = svm.fit_predict(X)\n",
    "anomalies = preds == -1\n",
    "print(f\"Detected {anomalies.sum()} anomalies with One-Class SVM\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`nu` is an upper bound on the fraction of anomalies (similar to contamination). The RBF kernel can capture non‑linear boundaries. One‑Class SVM can be slow on large datasets.\n",
    "\n",
    "### 58.3.3 Local Outlier Factor (LOF)\n",
    "\n",
    "LOF measures the local density deviation of a point compared to its neighbours. Points with substantially lower density than neighbours are considered outliers.\n",
    "\n",
    "```python\n",
    "from sklearn.neighbors import LocalOutlierFactor\n",
    "\n",
    "lof = LocalOutlierFactor(n_neighbors=20, contamination=0.01)\n",
    "preds = lof.fit_predict(X)\n",
    "anomalies = preds == -1\n",
    "print(f\"Detected {anomalies.sum()} anomalies with LOF\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "LOF is local: it works well when the data has regions of varying density. The parameter `n_neighbors` controls the locality.\n",
    "\n",
    "---\n",
    "\n",
    "## 58.4 Deep Learning Methods\n",
    "\n",
    "Deep learning models can capture complex temporal dependencies and are particularly suited for collective and contextual anomalies.\n",
    "\n",
    "### 58.4.1 Autoencoders\n",
    "\n",
    "An autoencoder is a neural network trained to reconstruct its input. The assumption is that normal data can be reconstructed well, while anomalies result in high reconstruction error. The model is trained on normal data only (or on all data but with the assumption that anomalies are rare).\n",
    "\n",
    "**Example: Simple dense autoencoder for multivariate time steps**\n",
    "\n",
    "```python\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "class Autoencoder(nn.Module):\n",
    "    def __init__(self, input_dim, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(input_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, encoding_dim),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(encoding_dim, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(64, input_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        encoded = self.encoder(x)\n",
    "        decoded = self.decoder(encoded)\n",
    "        return decoded\n",
    "\n",
    "# Prepare data: each sample is a vector of features at a single time step\n",
    "# (if you want to capture temporal context, you need to include lags)\n",
    "X_train = torch.tensor(features.values, dtype=torch.float32)\n",
    "\n",
    "model = Autoencoder(input_dim=X_train.shape[1], encoding_dim=16)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Train on normal data (assuming most is normal)\n",
    "epochs = 100\n",
    "batch_size = 64\n",
    "dataset = torch.utils.data.TensorDataset(X_train)\n",
    "dataloader = torch.utils.data.DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    for batch in dataloader:\n",
    "        x = batch[0]\n",
    "        reconstructed = model(x)\n",
    "        loss = criterion(reconstructed, x)\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch}, loss {loss.item():.4f}\")\n",
    "\n",
    "# Compute reconstruction error on all data\n",
    "model.eval()\n",
    "with torch.no_grad():\n",
    "    reconstructions = model(X_train)\n",
    "    mse = torch.mean((X_train - reconstructions)**2, dim=1).numpy()\n",
    "\n",
    "# Set threshold (e.g., 95th percentile)\n",
    "threshold = np.percentile(mse, 95)\n",
    "anomalies = mse > threshold\n",
    "print(f\"Detected {anomalies.sum()} anomalies with autoencoder\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The autoencoder learns a compressed representation of normal data. Anomalies will have higher reconstruction error. The threshold can be set based on a validation set with known anomalies or a percentile of the training errors.\n",
    "\n",
    "### 58.4.2 LSTM Autoencoder\n",
    "\n",
    "For time‑series, it's natural to use recurrent networks. An LSTM autoencoder reads the input sequence, compresses it into a context vector, and then reconstructs the sequence. It can capture temporal dependencies.\n",
    "\n",
    "```python\n",
    "class LSTMAutoencoder(nn.Module):\n",
    "    def __init__(self, seq_len, n_features, encoding_dim):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len\n",
    "        self.n_features = n_features\n",
    "        self.encoder_lstm = nn.LSTM(input_size=n_features, hidden_size=64, batch_first=True)\n",
    "        self.encoder_fc = nn.Linear(64, encoding_dim)\n",
    "        self.decoder_fc = nn.Linear(encoding_dim, 64)\n",
    "        self.decoder_lstm = nn.LSTM(input_size=64, hidden_size=n_features, batch_first=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, seq_len, n_features)\n",
    "        # Encode\n",
    "        _, (hidden, _) = self.encoder_lstm(x)\n",
    "        # hidden: (1, batch, 64)\n",
    "        hidden = hidden[-1]  # (batch, 64)\n",
    "        encoding = self.encoder_fc(hidden)  # (batch, encoding_dim)\n",
    "        # Decode\n",
    "        decoded_hidden = self.decoder_fc(encoding).unsqueeze(0)  # (1, batch, 64)\n",
    "        decoded_cell = torch.zeros_like(decoded_hidden)\n",
    "        # Repeat the same hidden state for each time step\n",
    "        decoder_input = torch.zeros(x.size(0), 1, 64)  # initial input\n",
    "        outputs = []\n",
    "        for t in range(self.seq_len):\n",
    "            decoder_input, (decoded_hidden, decoded_cell) = self.decoder_lstm(decoder_input, (decoded_hidden, decoded_cell))\n",
    "            outputs.append(decoder_input)\n",
    "        outputs = torch.cat(outputs, dim=1)  # (batch, seq_len, 64)\n",
    "        # Final projection to features\n",
    "        outputs = outputs  # already n_features from LSTM output? In this simple example, we need a linear layer.\n",
    "        # For clarity, we'll add a linear layer after LSTM to project to n_features.\n",
    "        # (Simplification: we can set decoder LSTM to output n_features directly)\n",
    "        return outputs\n",
    "\n",
    "# This is a simplified sketch; a full implementation is more involved.\n",
    "# Libraries like `keras` provide LSTM autoencoder layers more conveniently.\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The encoder LSTM compresses the sequence into a fixed‑size vector. The decoder LSTM then reconstructs the sequence step by step. Anomalies yield higher reconstruction error.\n",
    "\n",
    "### 58.4.3 Variational Autoencoder (VAE)\n",
    "\n",
    "VAEs learn a probabilistic latent space and can provide a measure of anomaly through reconstruction probability rather than error. They are more robust to noise.\n",
    "\n",
    "```python\n",
    "# Using a library like `pyod` which includes VAE for anomaly detection\n",
    "from pyod.models.vae import VAE\n",
    "\n",
    "vae = VAE(encoder_neurons=[64, 32], decoder_neurons=[32, 64], contamination=0.01)\n",
    "vae.fit(X_train)\n",
    "y_pred = vae.predict(X_train)  # 0 normal, 1 anomaly\n",
    "anomalies = y_pred == 1\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "VAE assumes a probabilistic generative process. The reconstruction probability (the likelihood of the data given the latent encoding) is used as anomaly score.\n",
    "\n",
    "---\n",
    "\n",
    "## 58.5 Time‑Series Specific Methods\n",
    "\n",
    "### 58.5.1 Seasonal Decomposition + Residual Analysis\n",
    "\n",
    "A classic approach for univariate time‑series with seasonality: decompose the series into trend, seasonal, and residual components, then detect anomalies in the residual. The STL (Seasonal‑Trend decomposition using Loess) is a robust method.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.seasonal import STL\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume daily data with weekly seasonality (period=5 for NEPSE trading days)\n",
    "series = df['NABIL_close'].dropna()\n",
    "stl = STL(series, period=5, robust=True)\n",
    "result = stl.fit()\n",
    "\n",
    "# Extract residual\n",
    "residual = result.resid\n",
    "\n",
    "# Detect outliers in residual (e.g., using IQR)\n",
    "outliers = iqr_outliers(residual, k=3)\n",
    "print(f\"Detected {outliers.sum()} anomalies in residual\")\n",
    "\n",
    "# Plot\n",
    "fig = result.plot()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The residual contains the irregular component after removing trend and seasonality. Anomalies will appear as spikes in the residual. This method captures contextual anomalies because it accounts for expected seasonal patterns.\n",
    "\n",
    "### 58.5.2 Twitter's AnomalyDetection\n",
    "\n",
    "Twitter's AnomalyDetection R package (also available in Python via `twitter‑ads‑anomaly‑detection`) uses seasonal decomposition and statistical testing to flag anomalies.\n",
    "\n",
    "```python\n",
    "# Install: pip install twitter-adspackage\n",
    "from twitter_adspackage import detect_anomalies\n",
    "\n",
    "# Uses ETS decomposition and generalized extreme studentized deviate test\n",
    "results = detect_anomalies(series, max_anoms=0.02, direction='both')\n",
    "anomalies = results['anomaly']\n",
    "print(anomalies)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 58.6 Evaluation\n",
    "\n",
    "When ground truth labels are available (e.g., manually flagged erroneous ticks), we can evaluate anomaly detectors using standard classification metrics:\n",
    "\n",
    "- **Precision** = TP / (TP + FP)\n",
    "- **Recall** = TP / (TP + FN)\n",
    "- **F1‑score** = 2 * (Precision * Recall) / (Precision + Recall)\n",
    "\n",
    "For unlabeled data, we can use domain knowledge to check if detected anomalies coincide with known events (earnings announcements, circuit breaker triggers).\n",
    "\n",
    "**Example evaluation:**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assuming true_labels is a boolean array (True if anomaly)\n",
    "true_labels = df['is_anomaly'].values  # from external source\n",
    "pred_labels = outliers  # from some detector\n",
    "\n",
    "print(classification_report(true_labels, pred_labels))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 58.7 Real‑Time Anomaly Detection\n",
    "\n",
    "For production, anomaly detection must run in real time on streaming data. The methods can be adapted:\n",
    "\n",
    "- **Statistical methods**: maintain rolling mean/std and flag points as they arrive.\n",
    "- **Isolation Forest**: can be trained offline and used online (scoring is fast).\n",
    "- **Autoencoder**: can score new points instantly.\n",
    "\n",
    "**Example: Rolling Z‑score on a stream**\n",
    "\n",
    "```python\n",
    "class RollingZScore:\n",
    "    def __init__(self, window=100, threshold=3):\n",
    "        self.window = window\n",
    "        self.threshold = threshold\n",
    "        self.values = []\n",
    "\n",
    "    def update(self, value):\n",
    "        self.values.append(value)\n",
    "        if len(self.values) > self.window:\n",
    "            self.values.pop(0)\n",
    "        if len(self.values) < 2:\n",
    "            return False\n",
    "        mean = np.mean(self.values)\n",
    "        std = np.std(self.values)\n",
    "        if std == 0:\n",
    "            return False\n",
    "        z = (value - mean) / std\n",
    "        return abs(z) > self.threshold\n",
    "\n",
    "# Simulate stream\n",
    "detector = RollingZScore()\n",
    "for price in stream:\n",
    "    if detector.update(price):\n",
    "        print(f\"Anomaly detected: {price}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This class maintains a sliding window of recent values and computes Z‑score online. It is suitable for detecting point anomalies in real time.\n",
    "\n",
    "---\n",
    "\n",
    "## 58.8 Interpretation of Anomalies\n",
    "\n",
    "Detecting an anomaly is only half the work; understanding why it happened is crucial. Domain knowledge must be applied:\n",
    "\n",
    "- **Market events**: Check if the anomaly coincides with news, earnings, or regulatory changes.\n",
    "- **Data issues**: Examine raw data for obvious errors (e.g., price outside bid‑ask spread).\n",
    "- **Model limitations**: Some anomalies may be false positives caused by model inadequacy (e.g., failure to capture volatility changes).\n",
    "\n",
    "Visualisation helps: plot the time series with anomalies highlighted, and overlay relevant context (e.g., trading volume, news indicators).\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12,6))\n",
    "plt.plot(series.index, series.values, label='Price')\n",
    "plt.scatter(series.index[anomalies], series.values[anomalies], color='red', label='Anomaly')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 58.9 Case Study: Detecting Anomalies in NEPSE Volume\n",
    "\n",
    "Let's apply some methods to detect unusual volume spikes in NEPSE stocks. Volume anomalies can indicate institutional activity or data errors.\n",
    "\n",
    "```python\n",
    "# Load volume data for a stock\n",
    "volume = df['NABIL_volume'].dropna()\n",
    "\n",
    "# Method 1: IQR on log volume (since volume is log‑normally distributed)\n",
    "log_vol = np.log(volume + 1)\n",
    "outliers_iqr = iqr_outliers(log_vol, k=2.5)\n",
    "\n",
    "# Method 2: Isolation Forest using multiple features (volume, price change, volatility)\n",
    "features = pd.DataFrame({\n",
    "    'volume': volume,\n",
    "    'return': df['NABIL_return'].loc[volume.index],\n",
    "    'volatility': df['NABIL_volatility'].loc[volume.index]\n",
    "}).dropna()\n",
    "X = features.values\n",
    "\n",
    "iso = IsolationForest(contamination=0.02, random_state=42)\n",
    "preds = iso.fit_predict(X)\n",
    "outliers_if = preds == -1\n",
    "\n",
    "# Compare\n",
    "print(f\"IQR outliers: {outliers_iqr.sum()}\")\n",
    "print(f\"Isolation Forest outliers: {outliers_if.sum()}\")\n",
    "\n",
    "# Check if they coincide\n",
    "common = (outliers_iqr[outliers_iqr.index.isin(features.index)] & outliers_if)\n",
    "print(f\"Common: {common.sum()}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Volume often follows a log‑normal distribution, so log transform improves IQR. Using multiple features (return and volatility) helps detect contextual anomalies (e.g., high volume on a low‑volatility day).\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored anomaly detection techniques and their application to the NEPSE stock prediction system. We covered:\n",
    "\n",
    "- The three types of anomalies: point, contextual, and collective.\n",
    "- Statistical methods: Z‑score, modified Z‑score, IQR – simple and fast for point anomalies.\n",
    "- Machine learning methods: Isolation Forest, One‑Class SVM, Local Outlier Factor – suitable for multivariate data.\n",
    "- Deep learning methods: autoencoders, LSTM autoencoders, VAEs – capture complex patterns and temporal dependencies.\n",
    "- Time‑series specific methods: STL decomposition, Twitter's AnomalyDetection – incorporate seasonality.\n",
    "- Evaluation metrics and real‑time implementation considerations.\n",
    "- Interpretation of anomalies using domain knowledge and visualisation.\n",
    "\n",
    "Anomaly detection is a vital component of a robust prediction system, ensuring data quality, enabling rapid response to market events, and providing input features for models. In the next chapter, we will discuss **Causal Inference**, exploring how to go beyond correlation to understand cause‑effect relationships in financial time‑series.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 58**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
