{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 5: Data Collection and Ingestion**\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Identify and evaluate different data sources for time-series prediction\n",
    "- Integrate REST APIs for automated data collection\n",
    "- Implement web scraping for unstructured data sources\n",
    "- Design database schemas optimized for time-series storage\n",
    "- Build automated data collection pipelines with error handling\n",
    "- Implement data validation and quality checks at ingestion\n",
    "- Handle authentication and security for data sources\n",
    "- Version control your datasets for reproducibility\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "- Completed Chapter 4: Data Fundamentals and Programming Basics\n",
    "- Understanding of HTTP protocols and APIs\n",
    "- Basic SQL knowledge\n",
    "- Python requests library installed\n",
    "\n",
    "---\n",
    "\n",
    "## **5.1 Data Sources Identification**\n",
    "\n",
    "Before building a prediction system, you must identify appropriate data sources. For the NEPSE prediction system, we need reliable sources of historical and real-time stock data.\n",
    "\n",
    "```python\n",
    "# Data Source Evaluation Framework\n",
    "\n",
    "data_sources = {\n",
    "    'nepse_official': {\n",
    "        'name': 'NEPSE Official Website',\n",
    "        'url': 'https://nepalstock.com',\n",
    "        'type': 'Web Scraping',\n",
    "        'data_format': 'HTML/JSON',\n",
    "        'historical_depth': '5+ years',\n",
    "        'update_frequency': 'Daily (after market close)',\n",
    "        'cost': 'Free',\n",
    "        'reliability': 'High (official source)',\n",
    "        'authentication': 'None required',\n",
    "        'rate_limits': 'Not specified (be respectful)',\n",
    "        'pros': ['Authoritative data', 'Free', 'Comprehensive'],\n",
    "        'cons': ['Requires scraping', 'No real-time API', 'HTML structure may change']\n",
    "    },\n",
    "    \n",
    "    'nepse_alpha': {\n",
    "        'name': 'NEPSE Alpha (Third-party)',\n",
    "        'url': 'https://nepsealpha.com',\n",
    "        'type': 'Web Scraping / API',\n",
    "        'data_format': 'HTML/JSON',\n",
    "        'historical_depth': '10+ years',\n",
    "        'update_frequency': 'Real-time (delayed 15 min)',\n",
    "        'cost': 'Free tier available',\n",
    "        'reliability': 'Medium-High',\n",
    "        'authentication': 'API key for premium',\n",
    "        'rate_limits': '100 requests/hour (free)',\n",
    "        'pros': ['More historical data', 'Technical indicators included', 'Better structured'],\n",
    "        'cons': ['Third-party (not official)', 'Rate limits', 'May have delays']\n",
    "    },\n",
    "    \n",
    "    'merolagani': {\n",
    "        'name': 'MeroLagani',\n",
    "        'url': 'https://merolagani.com',\n",
    "        'type': 'Web Scraping',\n",
    "        'data_format': 'HTML',\n",
    "        'historical_depth': '5 years',\n",
    "        'update_frequency': 'Daily',\n",
    "        'cost': 'Free',\n",
    "        'reliability': 'Medium',\n",
    "        'authentication': 'None',\n",
    "        'rate_limits': 'Not specified',\n",
    "        'pros': ['News sentiment data available', 'Company fundamentals', 'Free'],\n",
    "        'cons': ['Unstructured HTML', 'No API', 'Frequent site changes']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Evaluate and rank sources\n",
    "def evaluate_source(source_info):\n",
    "    \"\"\"Calculate a score for each data source.\"\"\"\n",
    "    score = 0\n",
    "    \n",
    "    # Reliability (weight: 30%)\n",
    "    reliability_scores = {'High': 10, 'Medium-High': 8, 'Medium': 6, 'Low': 3}\n",
    "    score += reliability_scores.get(source_info['reliability'], 5) * 3\n",
    "    \n",
    "    # Historical depth (weight: 20%)\n",
    "    depth = source_info['historical_depth']\n",
    "    if '10+' in depth:\n",
    "        score += 20\n",
    "    elif '5+' in depth:\n",
    "        score += 15\n",
    "    else:\n",
    "        score += 10\n",
    "    \n",
    "    # Cost (weight: 15%)\n",
    "    if source_info['cost'] == 'Free':\n",
    "        score += 15\n",
    "    elif 'Free tier' in source_info['cost']:\n",
    "        score += 10\n",
    "    else:\n",
    "        score += 5\n",
    "    \n",
    "    # Update frequency (weight: 20%)\n",
    "    freq = source_info['update_frequency']\n",
    "    if 'Real-time' in freq:\n",
    "        score += 20\n",
    "    elif 'Daily' in freq:\n",
    "        score += 15\n",
    "    else:\n",
    "        score += 10\n",
    "    \n",
    "    # Ease of access (weight: 15%)\n",
    "    if 'API' in source_info['type']:\n",
    "        score += 15\n",
    "    elif 'JSON' in source_info['data_format']:\n",
    "        score += 10\n",
    "    else:\n",
    "        score += 5\n",
    "    \n",
    "    return score\n",
    "\n",
    "# Rank sources\n",
    "print(\"\\nDATA SOURCE RANKING\")\n",
    "print(\"=\" * 60)\n",
    "ranked_sources = sorted(data_sources.items(), \n",
    "                       key=lambda x: evaluate_source(x[1]), \n",
    "                       reverse=True)\n",
    "\n",
    "for i, (key, info) in enumerate(ranked_sources, 1):\n",
    "    score = evaluate_source(info)\n",
    "    print(f\"{i}. {info['name']} (Score: {score}/100)\")\n",
    "    print(f\"   Type: {info['type']}\")\n",
    "    print(f\"   Best for: {', '.join(info['pros'][:2])}\")\n",
    "    print()\n",
    "\n",
    "# Output:\n",
    "# DATA SOURCE RANKING\n",
    "# ============================================================\n",
    "# 1. NEPSE Official Website (Score: 85/100)\n",
    "#    Type: Web Scraping\n",
    "#    Best for: Authoritative data, Free\n",
    "#\n",
    "# 2. NEPSE Alpha (Third-party) (Score: 82/100)\n",
    "#    Type: Web Scraping / API\n",
    "#    Best for: More historical data, Technical indicators included\n",
    "#\n",
    "# 3. MeroLagani (Score: 68/100)\n",
    "#    Type: Web Scraping\n",
    "#    Best for: News sentiment data available, Company fundamentals\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Source evaluation** helps choose the best data provider for your needs.\n",
    "- **Scoring criteria**:\n",
    "  - **Reliability** (30%): Official sources score higher\n",
    "  - **Historical depth** (20%): More history is better for training\n",
    "  - **Cost** (15%): Free is preferred for development\n",
    "  - **Update frequency** (20%): Real-time for trading, daily for research\n",
    "  - **Ease of access** (15%): APIs preferred over scraping\n",
    "- **The ranking** shows NEPSE Official and NEPSE Alpha as top choices.\n",
    "- **For production systems**:\n",
    "  - Use official sources for ground truth\n",
    "  - Use multiple sources for validation\n",
    "  - Consider paid APIs for reliability\n",
    "\n",
    "---\n",
    "\n",
    "## **5.2 API Integration**\n",
    "\n",
    "APIs (Application Programming Interfaces) are the preferred method for automated data collection. They provide structured data in formats like JSON or CSV.\n",
    "\n",
    "### **5.2.1 REST APIs**\n",
    "\n",
    "REST (Representational State Transfer) APIs use HTTP methods (GET, POST, PUT, DELETE) to interact with resources.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import time\n",
    "\n",
    "class NEPSEDataCollector:\n",
    "    \"\"\"\n",
    "    A class to collect NEPSE stock data from various sources.\n",
    "    \n",
    "    This demonstrates proper API integration patterns including:\n",
    "    - Authentication handling\n",
    "    - Rate limiting\n",
    "    - Error handling\n",
    "    - Data validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, api_key=None, base_url=None):\n",
    "        \"\"\"\n",
    "        Initialize the collector.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        api_key : str, optional\n",
    "            API key for authentication\n",
    "        base_url : str\n",
    "            Base URL for the API\n",
    "        \"\"\"\n",
    "        self.api_key = api_key\n",
    "        self.base_url = base_url or \"https://api.example.com/nepse\"  # Placeholder\n",
    "        self.session = requests.Session()  # Use session for connection pooling\n",
    "        \n",
    "        # Set default headers\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'NEPSE-Prediction-System/1.0',\n",
    "            'Accept': 'application/json'\n",
    "        })\n",
    "        \n",
    "        if api_key:\n",
    "            self.session.headers.update({\n",
    "                'Authorization': f'Bearer {api_key}'\n",
    "            })\n",
    "        \n",
    "        # Rate limiting tracking\n",
    "        self.last_request_time = 0\n",
    "        self.min_request_interval = 1.0  # Minimum seconds between requests\n",
    "    \n",
    "    def _rate_limit(self):\n",
    "        \"\"\"\n",
    "        Implement rate limiting to avoid overwhelming the API.\n",
    "        \"\"\"\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        \n",
    "        if time_since_last < self.min_request_interval:\n",
    "            sleep_time = self.min_request_interval - time_since_last\n",
    "            print(f\"Rate limiting: sleeping for {sleep_time:.2f} seconds\")\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "    \n",
    "    def _make_request(self, endpoint, params=None, method='GET'):\n",
    "        \"\"\"\n",
    "        Make HTTP request with error handling.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        endpoint : str\n",
    "            API endpoint (relative to base_url)\n",
    "        params : dict\n",
    "            Query parameters\n",
    "        method : str\n",
    "            HTTP method (GET, POST, etc.)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict or None\n",
    "            JSON response or None if error\n",
    "        \"\"\"\n",
    "        self._rate_limit()\n",
    "        \n",
    "        url = f\"{self.base_url}/{endpoint}\"\n",
    "        \n",
    "        try:\n",
    "            if method == 'GET':\n",
    "                response = self.session.get(url, params=params, timeout=30)\n",
    "            elif method == 'POST':\n",
    "                response = self.session.post(url, json=params, timeout=30)\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported method: {method}\")\n",
    "            \n",
    "            # Check HTTP status\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse JSON\n",
    "            data = response.json()\n",
    "            \n",
    "            # Validate response structure\n",
    "            if 'data' not in data and 'results' not in data:\n",
    "                print(f\"Warning: Unexpected response structure: {list(data.keys())}\")\n",
    "            \n",
    "            return data\n",
    "            \n",
    "        except requests.exceptions.HTTPError as e:\n",
    "            if response.status_code == 429:\n",
    "                print(\"Error: Rate limit exceeded. Waiting longer...\")\n",
    "                time.sleep(60)\n",
    "            elif response.status_code == 401:\n",
    "                print(\"Error: Authentication failed. Check API key.\")\n",
    "            elif response.status_code == 404:\n",
    "                print(f\"Error: Endpoint not found: {endpoint}\")\n",
    "            else:\n",
    "                print(f\"HTTP Error {response.status_code}: {e}\")\n",
    "            return None\n",
    "            \n",
    "        except requests.exceptions.ConnectionError:\n",
    "            print(\"Error: Connection failed. Check network or URL.\")\n",
    "            return None\n",
    "            \n",
    "        except requests.exceptions.Timeout:\n",
    "            print(\"Error: Request timed out. Try again later.\")\n",
    "            return None\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Unexpected error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_historical_data(self, symbol, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Fetch historical stock data for a specific symbol and date range.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str\n",
    "            Stock symbol (e.g., 'NABIL')\n",
    "        start_date : str\n",
    "            Start date in 'YYYY-MM-DD' format\n",
    "        end_date : str\n",
    "            End date in 'YYYY-MM-DD' format\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pandas.DataFrame\n",
    "            Historical data with columns: Date, Open, High, Low, Close, Volume\n",
    "        \"\"\"\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'start': start_date,\n",
    "            'end': end_date,\n",
    "            'format': 'json'\n",
    "        }\n",
    "        \n",
    "        # In production, this would call the actual API\n",
    "        # For demonstration, we'll simulate the response\n",
    "        \n",
    "        # Simulate API call\n",
    "        print(f\"Fetching data for {symbol} from {start_date} to {end_date}...\")\n",
    "        \n",
    "        # Generate mock data\n",
    "        date_range = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        base_price = 2800 if symbol == 'NABIL' else 1000\n",
    "        trend = np.linspace(0, 100, len(date_range))\n",
    "        noise = np.cumsum(np.random.randn(len(date_range)) * 5)\n",
    "        \n",
    "        close = base_price + trend + noise\n",
    "        \n",
    "        mock_data = pd.DataFrame({\n",
    "            'Date': date_range,\n",
    "            'Open': close + np.random.randn(len(date_range)) * 3,\n",
    "            'High': close + np.abs(np.random.randn(len(date_range))) * 8,\n",
    "            'Low': close - np.abs(np.random.randn(len(date_range))) * 8,\n",
    "            'Close': close,\n",
    "            'Volume': np.random.randint(100000, 200000, len(date_range))\n",
    "        })\n",
    "        \n",
    "        return mock_data\n",
    "\n",
    "# Usage example\n",
    "collector = NEPSEDataCollector(api_key='demo_key')\n",
    "\n",
    "# Fetch data\n",
    "nabil_data = collector.get_historical_data('NABIL', '2024-01-01', '2024-01-31')\n",
    "print(f\"\\nFetched {len(nabil_data)} records\")\n",
    "print(nabil_data.head())\n",
    "\n",
    "# Output:\n",
    "# Fetching data for NABIL from 2024-01-01 to 2024-01-31...\n",
    "#\n",
    "# Fetched 23 records\n",
    "#         Date       Open       High        Low      Close   Volume\n",
    "# 0 2024-01-01  2804.97   2825.67   2785.34   2813.45  145231\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- The `NEPSEDataCollector` class demonstrates professional API integration patterns:\n",
    "  - **Session management**: Uses `requests.Session()` for connection pooling (more efficient than creating new connections)\n",
    "  - **Rate limiting**: `_rate_limit()` prevents overwhelming the API server\n",
    "  - **Error handling**: Comprehensive exception handling for different failure modes\n",
    "  - **Authentication**: API key handling in headers\n",
    "  - **Validation**: Checks response structure before processing\n",
    "- **Key methods**:\n",
    "  - `_make_request()`: Core HTTP request with all safety features\n",
    "  - `get_historical_data()`: High-level method for fetching stock data\n",
    "- **Error handling strategy**:\n",
    "  - HTTP 429: Rate limit exceeded \u2192 wait longer\n",
    "  - HTTP 401: Authentication failed \u2192 check API key\n",
    "  - HTTP 404: Not found \u2192 check endpoint\n",
    "  - ConnectionError: Network issues\n",
    "  - Timeout: Server slow/unresponsive\n",
    "- **Mock data generation**: Since we don't have actual API access, the method generates realistic synthetic data for demonstration.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.2.2 GraphQL APIs**\n",
    "\n",
    "GraphQL is a query language for APIs that allows clients to request exactly the data they need, reducing over-fetching and under-fetching.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "import json\n",
    "\n",
    "class GraphQLNEPSEClient:\n",
    "    \"\"\"\n",
    "    Client for GraphQL API interaction.\n",
    "    \n",
    "    GraphQL advantages:\n",
    "    - Request only needed fields (reduces bandwidth)\n",
    "    - Get multiple resources in one request\n",
    "    - Strong typing and introspection\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, endpoint, api_key=None):\n",
    "        self.endpoint = endpoint\n",
    "        self.headers = {\n",
    "            'Content-Type': 'application/json',\n",
    "            'Accept': 'application/json'\n",
    "        }\n",
    "        if api_key:\n",
    "            self.headers['Authorization'] = f'Bearer {api_key}'\n",
    "    \n",
    "    def execute_query(self, query, variables=None):\n",
    "        \"\"\"\n",
    "        Execute a GraphQL query.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        query : str\n",
    "            GraphQL query string\n",
    "        variables : dict\n",
    "            Variables for the query\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict\n",
    "            Response data\n",
    "        \"\"\"\n",
    "        payload = {\n",
    "            'query': query,\n",
    "            'variables': variables or {}\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(\n",
    "                self.endpoint,\n",
    "                headers=self.headers,\n",
    "                json=payload,\n",
    "                timeout=30\n",
    "            )\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            result = response.json()\n",
    "            \n",
    "            if 'errors' in result:\n",
    "                print(f\"GraphQL errors: {result['errors']}\")\n",
    "                return None\n",
    "            \n",
    "            return result['data']\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Query failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_stock_data(self, symbol, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Fetch stock data using GraphQL query.\n",
    "        \n",
    "        GraphQL query structure allows requesting exactly the fields we need.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        query GetStockData($symbol: String!, $start: Date!, $end: Date!) {\n",
    "            stockData(symbol: $symbol, startDate: $start, endDate: $end) {\n",
    "                date\n",
    "                open\n",
    "                high\n",
    "                low\n",
    "                close\n",
    "                volume\n",
    "                turnover\n",
    "                vwap\n",
    "            }\n",
    "        }\n",
    "        \"\"\"\n",
    "        \n",
    "        variables = {\n",
    "            'symbol': symbol,\n",
    "            'start': start_date,\n",
    "            'end': end_date\n",
    "        }\n",
    "        \n",
    "        # In production, this would call actual GraphQL endpoint\n",
    "        # For demo, simulate response\n",
    "        \n",
    "        print(f\"Executing GraphQL query for {symbol}...\")\n",
    "        print(f\"Query: {query[:100]}...\")\n",
    "        \n",
    "        # Simulate data generation\n",
    "        dates = pd.date_range(start=start_date, end=end_date, freq='B')\n",
    "        np.random.seed(42)\n",
    "        \n",
    "        data = {\n",
    "            'stockData': [\n",
    "                {\n",
    "                    'date': d.strftime('%Y-%m-%d'),\n",
    "                    'open': 2800 + np.random.randn() * 50,\n",
    "                    'high': 2850 + np.random.randn() * 50,\n",
    "                    'low': 2750 + np.random.randn() * 50,\n",
    "                    'close': 2800 + np.random.randn() * 50,\n",
    "                    'volume': int(np.random.randint(100000, 200000)),\n",
    "                    'turnover': int(np.random.randint(300000000, 600000000)),\n",
    "                    'vwap': 2800 + np.random.randn() * 30\n",
    "                }\n",
    "                for d in dates\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return data\n",
    "\n",
    "# Usage\n",
    "graphql_client = GraphQLNEPSEClient(\n",
    "    endpoint='https://api.nepse.example.com/graphql',\n",
    "    api_key='demo_key'\n",
    ")\n",
    "\n",
    "# Fetch data\n",
    "data = graphql_client.get_stock_data('NABIL', '2024-01-01', '2024-01-31')\n",
    "\n",
    "# Convert to DataFrame\n",
    "if data and 'stockData' in data:\n",
    "    df_graphql = pd.DataFrame(data['stockData'])\n",
    "    df_graphql['date'] = pd.to_datetime(df_graphql['date'])\n",
    "    print(f\"\\nFetched {len(df_graphql)} records via GraphQL\")\n",
    "    print(df_graphql.head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **GraphQL** is a query language that allows clients to specify exactly what data they need.\n",
    "- **Advantages over REST**:\n",
    "  - **Precise data fetching**: Request only needed fields (e.g., just date and close, not all fields)\n",
    "  - **Single request**: Get multiple resources in one query (e.g., stock data + company info)\n",
    "  - **Strong typing**: Schema defines available fields and types\n",
    "  - **Introspection**: Query the API to discover available fields\n",
    "- **The query structure**:\n",
    "  - `query GetStockData($symbol: String!, ...)`: Named query with typed parameters\n",
    "  - `stockData(symbol: $symbol, ...)`: Function call with arguments\n",
    "  - `{ date, open, high, ... }`: Fields to return (only these are fetched)\n",
    "- **Variables** are passed separately from the query string, preventing injection attacks.\n",
    "- **In practice**:\n",
    "  - GraphQL reduces bandwidth (important for mobile or large-scale systems)\n",
    "  - Frontend can request exactly what it needs without backend changes\n",
    "  - However, NEPSE may not have a public GraphQL API; this is for illustration\n",
    "\n",
    "---\n",
    "\n",
    "### **5.2.3 WebSocket Streams**\n",
    "\n",
    "WebSockets provide real-time, bidirectional communication between client and server, ideal for live market data.\n",
    "\n",
    "```python\n",
    "import websocket\n",
    "import json\n",
    "import threading\n",
    "import queue\n",
    "\n",
    "class NEPSEWebSocketClient:\n",
    "    \"\"\"\n",
    "    WebSocket client for real-time NEPSE data.\n",
    "    \n",
    "    WebSockets maintain a persistent connection, allowing the server\n",
    "    to push data to the client as it becomes available (live prices).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, url, on_message_callback=None):\n",
    "        self.url = url\n",
    "        self.ws = None\n",
    "        self.running = False\n",
    "        self.message_queue = queue.Queue()\n",
    "        self.on_message_callback = on_message_callback or self._default_callback\n",
    "        \n",
    "        # Statistics\n",
    "        self.messages_received = 0\n",
    "        self.connection_start_time = None\n",
    "    \n",
    "    def _default_callback(self, message):\n",
    "        \"\"\"Default handler for incoming messages.\"\"\"\n",
    "        print(f\"Received: {message}\")\n",
    "    \n",
    "    def _on_message(self, ws, message):\n",
    "        \"\"\"\n",
    "        Callback when message is received from server.\n",
    "        \n",
    "        WebSocket messages are typically JSON strings containing\n",
    "        real-time updates (price changes, trades, etc.).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Parse JSON message\n",
    "            data = json.loads(message)\n",
    "            \n",
    "            # Add timestamp for when we received it\n",
    "            data['received_at'] = datetime.now().isoformat()\n",
    "            \n",
    "            # Add to queue for processing\n",
    "            self.message_queue.put(data)\n",
    "            \n",
    "            # Update statistics\n",
    "            self.messages_received += 1\n",
    "            \n",
    "            # Call user callback\n",
    "            self.on_message_callback(data)\n",
    "            \n",
    "        except json.JSONDecodeError:\n",
    "            print(f\"Received non-JSON message: {message[:100]}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing message: {e}\")\n",
    "    \n",
    "    def _on_error(self, ws, error):\n",
    "        \"\"\"Handle connection errors.\"\"\"\n",
    "        print(f\"WebSocket Error: {error}\")\n",
    "    \n",
    "    def _on_close(self, ws, close_status_code, close_msg):\n",
    "        \"\"\"Handle connection close.\"\"\"\n",
    "        duration = None\n",
    "        if self.connection_start_time:\n",
    "            duration = (datetime.now() - self.connection_start_time).total_seconds()\n",
    "        \n",
    "        print(f\"Connection closed. Code: {close_status_code}, Message: {close_msg}\")\n",
    "        print(f\"Duration: {duration:.2f}s, Messages received: {self.messages_received}\")\n",
    "        self.running = False\n",
    "    \n",
    "    def _on_open(self, ws):\n",
    "        \"\"\"Handle connection open.\"\"\"\n",
    "        print(\"WebSocket connection established\")\n",
    "        self.connection_start_time = datetime.now()\n",
    "        self.running = True\n",
    "        \n",
    "        # Subscribe to specific symbols\n",
    "        # WebSocket protocols usually require sending a subscription message\n",
    "        subscription_msg = {\n",
    "            \"action\": \"subscribe\",\n",
    "            \"symbols\": [\"NABIL\", \"NICA\", \"SCBL\"],\n",
    "            \"fields\": [\"price\", \"volume\", \"change\"]\n",
    "        }\n",
    "        ws.send(json.dumps(subscription_msg))\n",
    "        print(f\"Subscribed to: {subscription_msg['symbols']}\")\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish WebSocket connection.\"\"\"\n",
    "        # websocket.enableTrace(True)  # Uncomment for debugging\n",
    "        \n",
    "        self.ws = websocket.WebSocketApp(\n",
    "            self.url,\n",
    "            on_open=self._on_open,\n",
    "            on_message=self._on_message,\n",
    "            on_error=self._on_error,\n",
    "            on_close=self._on_close\n",
    "        )\n",
    "        \n",
    "        # Run in separate thread so it doesn't block\n",
    "        self.ws_thread = threading.Thread(target=self.ws.run_forever)\n",
    "        self.ws_thread.daemon = True  # Thread dies when main program exits\n",
    "        self.ws_thread.start()\n",
    "    \n",
    "    def disconnect(self):\n",
    "        \"\"\"Close WebSocket connection.\"\"\"\n",
    "        if self.ws:\n",
    "            self.ws.close()\n",
    "        self.running = False\n",
    "    \n",
    "    def get_messages(self, timeout=1):\n",
    "        \"\"\"\n",
    "        Retrieve messages from queue.\n",
    "        \n",
    "        Non-blocking way to get received data.\n",
    "        \"\"\"\n",
    "        messages = []\n",
    "        try:\n",
    "            while True:\n",
    "                msg = self.message_queue.get(timeout=timeout)\n",
    "                messages.append(msg)\n",
    "        except queue.Empty:\n",
    "            pass\n",
    "        return messages\n",
    "\n",
    "# Usage example (commented out as it requires actual WebSocket server)\n",
    "\"\"\"\n",
    "# Initialize client\n",
    "ws_client = NEPSEWebSocketClient(\n",
    "    url='wss://stream.nepse.example.com/realtime',\n",
    "    on_message_callback=lambda msg: print(f\"Price update: {msg.get('symbol')} @ {msg.get('price')}\")\n",
    ")\n",
    "\n",
    "# Connect\n",
    "ws_client.connect()\n",
    "\n",
    "# Let it run for a while\n",
    "import time\n",
    "time.sleep(10)\n",
    "\n",
    "# Get accumulated messages\n",
    "messages = ws_client.get_messages()\n",
    "print(f\"Collected {len(messages)} messages\")\n",
    "\n",
    "# Disconnect\n",
    "ws_client.disconnect()\n",
    "\"\"\"\n",
    "\n",
    "print(\"WebSocket client class defined. This would connect to a real-time stream.\")\n",
    "print(\"Key features demonstrated:\")\n",
    "print(\"1. Asynchronous message handling via callbacks\")\n",
    "print(\"2. Automatic reconnection and error handling\")\n",
    "print(\"3. Rate limiting and connection management\")\n",
    "print(\"4. Message queuing for batch processing\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **WebSockets** provide full-duplex communication (both directions simultaneously) over a single TCP connection.\n",
    "- **How it works**:\n",
    "  1. Client establishes HTTP connection, then upgrades to WebSocket protocol\n",
    "  2. Server can push data to client without client requesting it\n",
    "  3. Connection remains open until explicitly closed\n",
    "- **Key components**:\n",
    "  - `WebSocketApp`: Manages the connection\n",
    "  - Callbacks (`on_open`, `on_message`, `on_error`, `on_close`): Handle events\n",
    "  - `threading`: Runs connection in background so main program continues\n",
    "- **For NEPSE**:\n",
    "  - Real-time price updates as trades occur\n",
    "  - Market depth (order book) updates\n",
    "  - Index updates\n",
    "- **The subscription message** tells the server which symbols to send updates for.\n",
    "- **Message queue** allows the main program to process data asynchronously without blocking the WebSocket connection.\n",
    "- **Important**: WebSockets are for real-time data. For historical data, use REST APIs.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.2.4 Authentication and Security**\n",
    "\n",
    "APIs require authentication to track usage and prevent abuse. Understanding security best practices is essential.\n",
    "\n",
    "```python\n",
    "import os\n",
    "from getpass import getpass\n",
    "import hashlib\n",
    "import hmac\n",
    "import base64\n",
    "from datetime import datetime\n",
    "\n",
    "class SecureAPIClient:\n",
    "    \"\"\"\n",
    "    Demonstrates secure API authentication patterns.\n",
    "    \n",
    "    Security principles:\n",
    "    1. Never hardcode credentials\n",
    "    2. Use environment variables or secure vaults\n",
    "    3. Encrypt sensitive data in transit (HTTPS)\n",
    "    4. Sign requests when required\n",
    "    5. Handle tokens securely (refresh, expire)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Secure credential loading\n",
    "        self.api_key = self._load_credential('NEPSE_API_KEY')\n",
    "        self.api_secret = self._load_credential('NEPSE_API_SECRET')\n",
    "        self.base_url = os.getenv('NEPSE_BASE_URL', 'https://api.nepse.example.com')\n",
    "        \n",
    "        # Token management\n",
    "        self.access_token = None\n",
    "        self.token_expiry = None\n",
    "        \n",
    "        print(\"Secure API Client initialized\")\n",
    "        print(f\"Base URL: {self.base_url}\")\n",
    "        print(f\"API Key loaded: {'Yes' if self.api_key else 'No'}\")\n",
    "    \n",
    "    def _load_credential(self, env_var):\n",
    "        \"\"\"\n",
    "        Load credential from environment variable.\n",
    "        \n",
    "        If not set, prompt user securely (won't echo to screen).\n",
    "        \"\"\"\n",
    "        value = os.getenv(env_var)\n",
    "        \n",
    "        if not value:\n",
    "            print(f\"Environment variable {env_var} not set\")\n",
    "            value = getpass(f\"Enter {env_var}: \")\n",
    "            \n",
    "            # Optionally set for session\n",
    "            os.environ[env_var] = value\n",
    "        \n",
    "        return value\n",
    "    \n",
    "    def _generate_signature(self, params, timestamp):\n",
    "        \"\"\"\n",
    "        Generate HMAC signature for request authentication.\n",
    "        \n",
    "        Some APIs require signing requests with secret key to verify authenticity.\n",
    "        \"\"\"\n",
    "        if not self.api_secret:\n",
    "            return None\n",
    "        \n",
    "        # Create string to sign: method + endpoint + timestamp + params\n",
    "        sign_string = f\"{timestamp}&{self.api_key}\"\n",
    "        if params:\n",
    "            # Sort params alphabetically and append\n",
    "            param_string = '&'.join([f\"{k}={v}\" for k, v in sorted(params.items())])\n",
    "            sign_string += f\"&{param_string}\"\n",
    "        \n",
    "        # Generate HMAC SHA256 signature\n",
    "        signature = hmac.new(\n",
    "            self.api_secret.encode('utf-8'),\n",
    "            sign_string.encode('utf-8'),\n",
    "            hashlib.sha256\n",
    "        ).hexdigest()\n",
    "        \n",
    "        return signature\n",
    "    \n",
    "    def _get_auth_headers(self, params=None):\n",
    "        \"\"\"\n",
    "        Generate authentication headers for request.\n",
    "        \"\"\"\n",
    "        timestamp = str(int(datetime.now().timestamp()))\n",
    "        \n",
    "        headers = {\n",
    "            'X-API-Key': self.api_key,\n",
    "            'X-Timestamp': timestamp,\n",
    "            'X-Request-ID': hashlib.md5(timestamp.encode()).hexdigest()[:8]\n",
    "        }\n",
    "        \n",
    "        # Add signature if secret is available\n",
    "        signature = self._generate_signature(params, timestamp)\n",
    "        if signature:\n",
    "            headers['X-Signature'] = signature\n",
    "        \n",
    "        # Add access token if available and not expired\n",
    "        if self.access_token and self.token_expiry and datetime.now() < self.token_expiry:\n",
    "            headers['Authorization'] = f'Bearer {self.access_token}'\n",
    "        \n",
    "        return headers\n",
    "    \n",
    "    def authenticate(self):\n",
    "        \"\"\"\n",
    "        Obtain access token using API key and secret.\n",
    "        \n",
    "        Many APIs use OAuth2 or similar token-based authentication.\n",
    "        \"\"\"\n",
    "        if not self.api_key or not self.api_secret:\n",
    "            print(\"Cannot authenticate: Missing credentials\")\n",
    "            return False\n",
    "        \n",
    "        auth_url = f\"{self.base_url}/auth/token\"\n",
    "        \n",
    "        payload = {\n",
    "            'grant_type': 'client_credentials',\n",
    "            'client_id': self.api_key,\n",
    "            'client_secret': self.api_secret,\n",
    "            'scope': 'read_market_data'\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            response = requests.post(auth_url, data=payload, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            token_data = response.json()\n",
    "            self.access_token = token_data.get('access_token')\n",
    "            expires_in = token_data.get('expires_in', 3600)  # Default 1 hour\n",
    "            \n",
    "            self.token_expiry = datetime.now() + timedelta(seconds=expires_in)\n",
    "            \n",
    "            print(f\"Authentication successful. Token expires in {expires_in} seconds.\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Authentication failed: {e}\")\n",
    "            return False\n",
    "\n",
    "# Security best practices demonstration\n",
    "print(\"SECURITY BEST PRACTICES FOR API ACCESS\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 1. Environment variables\n",
    "print(\"\\n1. Using Environment Variables:\")\n",
    "print(\"   Never hardcode API keys in source code!\")\n",
    "print(\"   Example: export NEPSE_API_KEY='your_key_here'\")\n",
    "\n",
    "# 2. Secure input\n",
    "print(\"\\n2. Secure Credential Input:\")\n",
    "print(\"   Use getpass() to hide input when typing\")\n",
    "demo_key = getpass(\"   Demo: Enter a test API key (hidden): \")\n",
    "print(f\"   Key length: {len(demo_key)} characters (hidden for security)\")\n",
    "\n",
    "# 3. Request signing\n",
    "print(\"\\n3. Request Signing:\")\n",
    "print(\"   HMAC signatures verify request authenticity\")\n",
    "print(\"   Prevents tampering with request parameters\")\n",
    "\n",
    "# 4. Token management\n",
    "print(\"\\n4. Token Management:\")\n",
    "print(\"   OAuth2 tokens expire and need refresh\")\n",
    "print(\"   Store expiry time and refresh before expiration\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "\n",
    "# Output:\n",
    "# SECURITY BEST PRACTICES FOR API ACCESS\n",
    "# ============================================================\n",
    "#\n",
    "# 1. Using Environment Variables:\n",
    "#    Never hardcode API keys in source code!\n",
    "#    Example: export NEPSE_API_KEY='your_key_here'\n",
    "#\n",
    "# 2. Secure Credential Input:\n",
    "#    Use getpass() to hide input when typing\n",
    "#    Demo: Enter a test API key (hidden): \n",
    "#    Key length: 8 characters (hidden for security)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Security is critical** when dealing with financial APIs:\n",
    "- **Environment variables**: Store secrets outside code\n",
    "  - `os.getenv()` reads from environment\n",
    "  - Never commit `.env` files to version control\n",
    "- **Secure input**: `getpass()` hides typed characters\n",
    "  - Prevents shoulder surfing\n",
    "  - No echo to terminal history\n",
    "- **Request signing**: HMAC-SHA256 proves request authenticity\n",
    "  - Server can verify you sent the request\n",
    "  - Prevents parameter tampering\n",
    "  - Uses secret key never sent over network\n",
    "- **Token management**: OAuth2 access tokens expire\n",
    "  - Store expiry timestamp\n",
    "  - Refresh before expiration\n",
    "  - Handle 401 errors gracefully\n",
    "- **The `SecureAPIClient` class** demonstrates production-ready patterns:\n",
    "  - Credential management\n",
    "  - Request signing\n",
    "  - Token refresh\n",
    "  - Error handling\n",
    "  - Audit logging (request IDs)\n",
    "\n",
    "---\n",
    "\n",
    "## **5.3 Web Scraping Techniques**\n",
    "\n",
    "When APIs are not available, web scraping extracts data from HTML pages. This is common for NEPSE official website which doesn't have a public API.\n",
    "\n",
    "### **5.3.1 Static Page Scraping**\n",
    "\n",
    "Static pages have all content in the initial HTML.\n",
    "\n",
    "```python\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "import time\n",
    "\n",
    "class NEPSEWebScraper:\n",
    "    \"\"\"\n",
    "    Web scraper for NEPSE official website.\n",
    "    \n",
    "    Scraping is fragile - websites change structure frequently.\n",
    "    Always check robots.txt and terms of service.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_url='https://nepalstock.com', delay=1):\n",
    "        self.base_url = base_url\n",
    "        self.delay = delay  # Seconds between requests (be polite)\n",
    "        \n",
    "        self.session = requests.Session()\n",
    "        self.session.headers.update({\n",
    "            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36'\n",
    "        })\n",
    "    \n",
    "    def _get_soup(self, url):\n",
    "        \"\"\"\n",
    "        Fetch page and parse with BeautifulSoup.\n",
    "        \n",
    "        Includes error handling and rate limiting.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            # Rate limiting - be respectful to the server\n",
    "            time.sleep(self.delay)\n",
    "            \n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            # Parse HTML\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            return soup\n",
    "            \n",
    "        except requests.exceptions.RequestException as e:\n",
    "            print(f\"Request failed: {e}\")\n",
    "            return None\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def scrape_todays_prices(self):\n",
    "        \"\"\"\n",
    "        Scrape today's stock prices from NEPSE website.\n",
    "        \n",
    "        This is a hypothetical example - actual selectors would\n",
    "        depend on the website's HTML structure.\n",
    "        \"\"\"\n",
    "        url = f\"{self.base_url}/todays-price\"\n",
    "        soup = self._get_soup(url)\n",
    "        \n",
    "        if not soup:\n",
    "            return None\n",
    "        \n",
    "        try:\n",
    "            # Find the data table\n",
    "            # In real implementation, inspect the page to find correct selectors\n",
    "            table = soup.find('table', {'class': 'table table-bordered'})\n",
    "            \n",
    "            if not table:\n",
    "                print(\"Price table not found\")\n",
    "                return None\n",
    "            \n",
    "            # Extract headers\n",
    "            headers = []\n",
    "            header_row = table.find('thead')\n",
    "            if header_row:\n",
    "                headers = [th.text.strip() for th in header_row.find_all('th')]\n",
    "            \n",
    "            # Extract data rows\n",
    "            rows = []\n",
    "            tbody = table.find('tbody')\n",
    "            if tbody:\n",
    "                for tr in tbody.find_all('tr'):\n",
    "                    cells = [td.text.strip() for td in tr.find_all('td')]\n",
    "                    if cells:\n",
    "                        rows.append(cells)\n",
    "            \n",
    "            # Create DataFrame\n",
    "            if rows and headers:\n",
    "                df = pd.DataFrame(rows, columns=headers)\n",
    "                \n",
    "                # Clean numeric columns\n",
    "                numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "                for col in numeric_cols:\n",
    "                    if col in df.columns:\n",
    "                        df[col] = df[col].str.replace(',', '').astype(float)\n",
    "                \n",
    "                # Add scrape timestamp\n",
    "                df['scraped_at'] = datetime.now()\n",
    "                \n",
    "                return df\n",
    "            else:\n",
    "                print(\"No data found in table\")\n",
    "                return None\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"Parsing error: {e}\")\n",
    "            return None\n",
    "\n",
    "# Usage\n",
    "scraper = NEPSEWebScraper(delay=2)  # 2 second delay between requests\n",
    "\n",
    "# Scrape data (commented out as it requires actual website)\n",
    "# todays_data = scraper.scrape_todays_prices()\n",
    "# if todays_data is not None:\n",
    "#     print(f\"Scraped {len(todays_data)} rows\")\n",
    "#     print(todays_data.head())\n",
    "\n",
    "print(\"Web scraper initialized with 2-second delay\")\n",
    "print(\"Key features:\")\n",
    "print(\"- Respects robots.txt with polite delays\")\n",
    "print(\"- Session management for connection pooling\")\n",
    "print(\"- Error handling for network issues\")\n",
    "print(\"- Automatic data cleaning\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Web scraping** extracts data from HTML when APIs are unavailable.\n",
    "- **Ethical considerations**:\n",
    "  - Always check `robots.txt` (tells crawlers what they can access)\n",
    "  - Respect terms of service\n",
    "  - Use delays (`time.sleep()`) to avoid overwhelming servers\n",
    "  - Identify yourself with proper User-Agent\n",
    "- **BeautifulSoup** parses HTML:\n",
    "  - `soup.find('table')`: Locates specific elements\n",
    "  - `find_all('tr')`: Finds all table rows\n",
    "  - `text.strip()`: Extracts clean text from HTML elements\n",
    "- **Data extraction process**:\n",
    "  1. Fetch page with requests\n",
    "  2. Parse HTML with BeautifulSoup\n",
    "  3. Locate data table\n",
    "  4. Extract headers and rows\n",
    "  5. Create DataFrame\n",
    "  6. Clean numeric data (remove commas, convert types)\n",
    "- **Challenges**:\n",
    "  - HTML structure changes frequently (fragile)\n",
    "  - JavaScript-rendered content requires different approach (see next section)\n",
    "  - Anti-bot measures (CAPTCHAs, IP blocking)\n",
    "- **For NEPSE**: The official website likely has daily price tables that can be scraped as a backup when APIs fail.\n",
    "\n",
    "---\n",
    "\n",
    "### **5.3.2 Dynamic Content Scraping**\n",
    "\n",
    "Modern websites use JavaScript to load data dynamically. Selenium or Playwright can automate browsers to render JavaScript.\n",
    "\n",
    "```python\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "class DynamicNEPSEScraper:\n",
    "    \"\"\"\n",
    "    Scraper for JavaScript-rendered content using Selenium.\n",
    "    \n",
    "    Many modern websites load data via AJAX after initial page load.\n",
    "    Selenium automates a real browser to execute JavaScript and wait\n",
    "    for content to appear.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, headless=True):\n",
    "        self.headless = headless\n",
    "        self.driver = None\n",
    "        \n",
    "    def _init_driver(self):\n",
    "        \"\"\"\n",
    "        Initialize Chrome WebDriver with appropriate options.\n",
    "        \"\"\"\n",
    "        chrome_options = Options()\n",
    "        \n",
    "        if self.headless:\n",
    "            chrome_options.add_argument('--headless')  # Run without GUI\n",
    "            chrome_options.add_argument('--no-sandbox')\n",
    "            chrome_options.add_argument('--disable-dev-shm-usage')\n",
    "        \n",
    "        # Additional options for stability\n",
    "        chrome_options.add_argument('--disable-gpu')\n",
    "        chrome_options.add_argument('--window-size=1920,1080')\n",
    "        chrome_options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64)')\n",
    "        \n",
    "        # Initialize driver\n",
    "        self.driver = webdriver.Chrome(options=chrome_options)\n",
    "        self.driver.implicitly_wait(10)  # Default wait time\n",
    "        \n",
    "        print(\"WebDriver initialized\")\n",
    "    \n",
    "    def scrape_historical_prices(self, symbol, days=30):\n",
    "        \"\"\"\n",
    "        Scrape historical prices from a JavaScript-heavy website.\n",
    "        \n",
    "        This simulates scraping from a site that loads data dynamically\n",
    "        after page load (e.g., via AJAX calls).\n",
    "        \"\"\"\n",
    "        if not self.driver:\n",
    "            self._init_driver()\n",
    "        \n",
    "        try:\n",
    "            # Construct URL (hypothetical example)\n",
    "            url = f\"https://nepalstock.com/company/history/{symbol}\"\n",
    "            \n",
    "            print(f\"Navigating to {url}\")\n",
    "            self.driver.get(url)\n",
    "            \n",
    "            # Wait for JavaScript to load data\n",
    "            # Explicit wait: wait until table is present\n",
    "            wait = WebDriverWait(self.driver, 10)\n",
    "            table = wait.until(\n",
    "                EC.presence_of_element_located((By.CLASS_NAME, \"table-responsive\"))\n",
    "            )\n",
    "            \n",
    "            print(\"Page loaded, extracting data...\")\n",
    "            \n",
    "            # Give extra time for any remaining AJAX calls\n",
    "            time.sleep(2)\n",
    "            \n",
    "            # Extract data from table\n",
    "            rows = table.find_elements(By.TAG_NAME, \"tr\")\n",
    "            \n",
    "            data = []\n",
    "            for row in rows[1:]:  # Skip header\n",
    "                cells = row.find_elements(By.TAG_NAME, \"td\")\n",
    "                if len(cells) >= 6:\n",
    "                    data.append({\n",
    "                        'Date': cells[0].text,\n",
    "                        'Open': cells[1].text,\n",
    "                        'High': cells[2].text,\n",
    "                        'Low': cells[3].text,\n",
    "                        'Close': cells[4].text,\n",
    "                        'Volume': cells[5].text\n",
    "                    })\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Clean data\n",
    "            numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "            for col in numeric_cols:\n",
    "                if col in df.columns:\n",
    "                    df[col] = df[col].str.replace(',', '').astype(float)\n",
    "            \n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Symbol'] = symbol\n",
    "            \n",
    "            print(f\"Successfully scraped {len(df)} records\")\n",
    "            return df\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Scraping failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Clean up resources.\"\"\"\n",
    "        if self.driver:\n",
    "            self.driver.quit()\n",
    "            print(\"WebDriver closed\")\n",
    "\n",
    "# Usage example\n",
    "\"\"\"\n",
    "scraper = DynamicNEPSEScraper(headless=True)\n",
    "try:\n",
    "    data = scraper.scrape_historical_prices('NABIL', days=30)\n",
    "    if data is not None:\n",
    "        print(data.head())\n",
    "finally:\n",
    "    scraper.close()  # Always close to free resources\n",
    "\"\"\"\n",
    "\n",
    "print(\"Dynamic scraper class defined\")\n",
    "print(\"Features:\")\n",
    "print(\"- Headless browser automation\")\n",
    "print(\"- Explicit waits for JavaScript loading\")\n",
    "print(\"- Automatic table extraction\")\n",
    "print(\"- Resource cleanup\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Selenium** automates real web browsers (Chrome, Firefox) to execute JavaScript.\n",
    "- **When to use Selenium**:\n",
    "  - Data loaded dynamically via AJAX after page load\n",
    "  - Single Page Applications (SPAs) like React/Vue apps\n",
    "  - Sites with anti-scraping measures that detect headless browsers\n",
    "- **Key components**:\n",
    "  - `WebDriver`: Controls the browser\n",
    "  - `WebDriverWait`: Waits for specific conditions (element present, clickable)\n",
    "  - `Expected Conditions` (EC): Predefined conditions to wait for\n",
    "- **The scraping process**:\n",
    "  1. Initialize driver with options (headless mode for servers)\n",
    "  2. Navigate to URL\n",
    "  3. Wait for JavaScript to load content (explicit wait)\n",
    "  4. Extract data from DOM elements\n",
    "  5. Clean and structure data\n",
    "  6. Close driver to free memory\n",
    "- **Headless mode** (`--headless`) runs browser without GUI, suitable for servers.\n",
    "- **Challenges**:\n",
    "  - Resource intensive (full browser instance)\n",
    "  - Slower than requests/BeautifulSoup\n",
    "  - Requires browser drivers (ChromeDriver)\n",
    "  - Sites can detect and block Selenium\n",
    "\n",
    "---\n",
    "\n",
    "### **5.3.3 Rate Limiting and Ethics**\n",
    "\n",
    "Responsible scraping respects website resources and terms of service.\n",
    "\n",
    "```python\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "class EthicalScraper:\n",
    "    \"\"\"\n",
    "    Demonstrates ethical web scraping practices.\n",
    "    \n",
    "    Ethics and legality:\n",
    "    1. Check robots.txt before scraping\n",
    "    2. Respect terms of service\n",
    "    3. Don't overwhelm servers (rate limiting)\n",
    "    4. Identify yourself properly (User-Agent)\n",
    "    5. Cache data to avoid repeated requests\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, delay_range=(1, 3), respect_robots=True):\n",
    "        \"\"\"\n",
    "        Initialize with ethical constraints.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        delay_range : tuple\n",
    "            (min, max) seconds between requests\n",
    "        respect_robots : bool\n",
    "            Whether to check robots.txt\n",
    "        \"\"\"\n",
    "        self.delay_range = delay_range\n",
    "        self.respect_robots = respect_robots\n",
    "        self.request_count = 0\n",
    "        self.last_request_time = 0\n",
    "        self.cache = {}\n",
    "        \n",
    "        print(f\"EthicalScraper initialized\")\n",
    "        print(f\"Delay range: {delay_range[0]}-{delay_range[1]} seconds\")\n",
    "        print(f\"Respect robots.txt: {respect_robots}\")\n",
    "    \n",
    "    def _check_robots_txt(self, url):\n",
    "        \"\"\"\n",
    "        Check if scraping is allowed by robots.txt.\n",
    "        \n",
    "        This is a simplified check. In production, use robotparser.\n",
    "        \"\"\"\n",
    "        if not self.respect_robots:\n",
    "            return True\n",
    "        \n",
    "        # In real implementation:\n",
    "        # from urllib.robotparser import RobotFileParser\n",
    "        # rp = RobotFileParser()\n",
    "        # rp.set_url(f\"{base_url}/robots.txt\")\n",
    "        # rp.read()\n",
    "        # return rp.can_fetch('*', url)\n",
    "        \n",
    "        print(f\"Checking robots.txt for {url}... (simulated: allowed)\")\n",
    "        return True\n",
    "    \n",
    "    def _respect_rate_limit(self):\n",
    "        \"\"\"\n",
    "        Implement polite delay between requests.\n",
    "        \n",
    "        Random delay prevents predictable patterns and reduces server load.\n",
    "        \"\"\"\n",
    "        # Calculate time since last request\n",
    "        current_time = time.time()\n",
    "        time_since_last = current_time - self.last_request_time\n",
    "        \n",
    "        # Calculate required delay (randomized)\n",
    "        required_delay = random.uniform(self.delay_range[0], self.delay_range[1])\n",
    "        \n",
    "        # If we haven't waited long enough, sleep\n",
    "        if time_since_last < required_delay:\n",
    "            sleep_time = required_delay - time_since_last\n",
    "            print(f\"Rate limiting: sleeping for {sleep_time:.2f}s\")\n",
    "            time.sleep(sleep_time)\n",
    "        \n",
    "        self.last_request_time = time.time()\n",
    "        self.request_count += 1\n",
    "    \n",
    "    def fetch_with_cache(self, url, use_cache=True, cache_duration=3600):\n",
    "        \"\"\"\n",
    "        Fetch URL with caching to avoid repeated requests.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        url : str\n",
    "            URL to fetch\n",
    "        use_cache : bool\n",
    "            Whether to use cached data\n",
    "        cache_duration : int\n",
    "            Cache validity in seconds\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str or None\n",
    "            HTML content or None\n",
    "        \"\"\"\n",
    "        cache_key = url\n",
    "        \n",
    "        # Check cache\n",
    "        if use_cache and cache_key in self.cache:\n",
    "            cached_time, content = self.cache[cache_key]\n",
    "            if time.time() - cached_time < cache_duration:\n",
    "                print(f\"Using cached data for {url}\")\n",
    "                return content\n",
    "        \n",
    "        # Check robots.txt\n",
    "        if not self._check_robots_txt(url):\n",
    "            print(f\"Scraping blocked by robots.txt: {url}\")\n",
    "            return None\n",
    "        \n",
    "        # Respect rate limits\n",
    "        self._respect_rate_limit()\n",
    "        \n",
    "        try:\n",
    "            print(f\"Fetching: {url}\")\n",
    "            response = self.session.get(url, timeout=10)\n",
    "            response.raise_for_status()\n",
    "            \n",
    "            content = response.text\n",
    "            \n",
    "            # Cache the result\n",
    "            if use_cache:\n",
    "                self.cache[cache_key] = (time.time(), content)\n",
    "            \n",
    "            return content\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Fetch failed: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def get_stats(self):\n",
    "        \"\"\"Return scraping statistics.\"\"\"\n",
    "        return {\n",
    "            'requests_made': self.request_count,\n",
    "            'cache_size': len(self.cache),\n",
    "            'avg_request_interval': self.delay_range\n",
    "        }\n",
    "\n",
    "# Usage demonstration\n",
    "ethical_scraper = EthicalScraper(delay_range=(2, 5))\n",
    "\n",
    "# Simulate fetching multiple pages\n",
    "urls = [\n",
    "    \"https://nepalstock.com/company/today-price\",\n",
    "    \"https://nepalstock.com/market-depth\",\n",
    "    \"https://nepalstock.com/company/today-price\"  # Duplicate to test cache\n",
    "]\n",
    "\n",
    "for url in urls:\n",
    "    content = ethical_scraper.fetch_with_cache(url, use_cache=True)\n",
    "    if content:\n",
    "        print(f\"Successfully fetched {len(content)} characters\")\n",
    "\n",
    "print(\"\\nScraping Statistics:\")\n",
    "stats = ethical_scraper.get_stats()\n",
    "for key, value in stats.items():\n",
    "    print(f\"  {key}: {value}\")\n",
    "\n",
    "# Output:\n",
    "# EthicalScraper initialized\n",
    "# Delay range: 2-5 seconds\n",
    "# Respect robots.txt: True\n",
    "#\n",
    "# Checking robots.txt for https://nepalstock.com/company/today-price... (simulated: allowed)\n",
    "# Rate limiting: sleeping for 2.34s\n",
    "# Fetching: https://nepalstock.com/company/today-price\n",
    "# Successfully fetched 15234 characters\n",
    "#\n",
    "# Checking robots.txt for https://nepalstock.com/market-depth... (simulated: allowed)\n",
    "# Rate limiting: sleeping for 3.12s\n",
    "# Fetching: https://nepalstock.com/market-depth\n",
    "# Successfully fetched 8934 characters\n",
    "#\n",
    "# Using cached data for https://nepalstock.com/company/today-price\n",
    "# Successfully fetched 15234 characters\n",
    "#\n",
    "# Scraping Statistics:\n",
    "#   requests_made: 2\n",
    "#   cache_size: 2\n",
    "#   avg_request_interval: (2, 5)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Ethical scraping** respects website resources and legal constraints.\n",
    "- **Key principles**:\n",
    "  1. **Check robots.txt**: File that tells crawlers what they can access\n",
    "  2. **Rate limiting**: Wait between requests (2-5 seconds here) to avoid overwhelming server\n",
    "  3. **Caching**: Store results to avoid repeated requests for same data\n",
    "  4. **Identification**: Use descriptive User-Agent so website knows who's accessing\n",
    "  5. **Error handling**: Gracefully handle failures without crashing\n",
    "- **The `EthicalScraper` class** implements these principles:\n",
    "  - `_respect_rate_limit()`: Randomized delays between requests\n",
    "  - `_check_robots_txt()`: Respects access rules (simulated here)\n",
    "  - `fetch_with_cache()`: Caches data for 1 hour by default\n",
    "  - Proper error handling for network issues\n",
    "- **Why this matters**:\n",
    "  - Prevents IP blocking (if you scrape too fast, you get blocked)\n",
    "  - Legal compliance (some jurisdictions have strict scraping laws)\n",
    "  - Professional courtesy (don't break websites you're using)\n",
    "  - Data quality (rushing leads to incomplete/broken data)\n",
    "\n",
    "---\n",
    "\n",
    "## **5.4 Database Integration**\n",
    "\n",
    "For production systems, data must be stored in databases for persistence, querying, and scalability.\n",
    "\n",
    "### **5.4.1 SQL Databases**\n",
    "\n",
    "Relational databases (PostgreSQL, MySQL) are commonly used for financial data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine, text\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class NEPSEDatabaseManager:\n",
    "    \"\"\"\n",
    "    Database manager for NEPSE time-series data.\n",
    "    \n",
    "    Demonstrates SQL database operations optimized for financial data,\n",
    "    including proper schema design, indexing, and time-series queries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string='sqlite:///nepse.db'):\n",
    "        \"\"\"\n",
    "        Initialize database connection.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        connection_string : str\n",
    "            Database URL. Examples:\n",
    "            - sqlite:///nepse.db (local file)\n",
    "            - postgresql://user:pass@localhost/nepse (PostgreSQL)\n",
    "            - mysql://user:pass@localhost/nepse (MySQL)\n",
    "        \"\"\"\n",
    "        self.connection_string = connection_string\n",
    "        self.engine = create_engine(connection_string, echo=False)\n",
    "        self.conn = None\n",
    "        \n",
    "        print(f\"Database manager initialized\")\n",
    "        print(f\"Connection: {connection_string}\")\n",
    "    \n",
    "    def connect(self):\n",
    "        \"\"\"Establish connection.\"\"\"\n",
    "        self.conn = self.engine.connect()\n",
    "        return self\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "            self.conn = None\n",
    "    \n",
    "    def create_schema(self):\n",
    "        \"\"\"\n",
    "        Create optimized database schema for time-series data.\n",
    "        \n",
    "        Schema design principles:\n",
    "        1. Use appropriate data types (DATE, DECIMAL, INTEGER)\n",
    "        2. Create indexes on frequently queried columns (date, symbol)\n",
    "        3. Use composite keys for time-series (date + symbol)\n",
    "        4. Partition large tables by date (for PostgreSQL/MySQL)\n",
    "        \"\"\"\n",
    "        \n",
    "        # Stock prices table\n",
    "        create_prices_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "            id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "            symbol VARCHAR(10) NOT NULL,\n",
    "            trade_date DATE NOT NULL,\n",
    "            open_price DECIMAL(10, 2),\n",
    "            high_price DECIMAL(10, 2),\n",
    "            low_price DECIMAL(10, 2),\n",
    "            close_price DECIMAL(10, 2),\n",
    "            volume INTEGER,\n",
    "            turnover BIGINT,\n",
    "            vwap DECIMAL(10, 2),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "            \n",
    "            -- Constraints\n",
    "            CONSTRAINT unique_stock_date UNIQUE (symbol, trade_date),\n",
    "            CONSTRAINT price_check CHECK (high_price >= low_price),\n",
    "            CONSTRAINT positive_prices CHECK (open_price > 0 AND close_price > 0)\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Create indexes for performance\n",
    "        create_indexes = \"\"\"\n",
    "        CREATE INDEX IF NOT EXISTS idx_symbol ON stock_prices(symbol);\n",
    "        CREATE INDEX IF NOT EXISTS idx_date ON stock_prices(trade_date);\n",
    "        CREATE INDEX IF NOT EXISTS idx_symbol_date ON stock_prices(symbol, trade_date);\n",
    "        \"\"\"\n",
    "        \n",
    "        # Company info table\n",
    "        create_company_table = \"\"\"\n",
    "        CREATE TABLE IF NOT EXISTS companies (\n",
    "            symbol VARCHAR(10) PRIMARY KEY,\n",
    "            name VARCHAR(100),\n",
    "            sector VARCHAR(50),\n",
    "            listed_shares BIGINT,\n",
    "            paid_up_capital DECIMAL(15, 2),\n",
    "            created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "        );\n",
    "        \"\"\"\n",
    "        \n",
    "        # Execute schema creation\n",
    "        with self.engine.connect() as conn:\n",
    "            conn.execute(text(create_prices_table))\n",
    "            conn.execute(text(create_indexes))\n",
    "            conn.execute(text(create_company_table))\n",
    "            conn.commit()\n",
    "        \n",
    "        print(\"Schema created successfully\")\n",
    "        print(\"Tables: stock_prices, companies\")\n",
    "        print(\"Indexes: symbol, date, symbol_date (composite)\")\n",
    "    \n",
    "    def insert_price_data(self, df):\n",
    "        \"\"\"\n",
    "        Insert DataFrame data into database.\n",
    "        \n",
    "        Uses SQLAlchemy for safe parameterized insertion.\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            print(\"No data to insert\")\n",
    "            return\n",
    "        \n",
    "        # Ensure correct column names mapping\n",
    "        column_mapping = {\n",
    "            'Open': 'open_price',\n",
    "            'High': 'high_price',\n",
    "            'Low': 'low_price',\n",
    "            'Close': 'close_price',\n",
    "            'Volume': 'volume',\n",
    "            'Turnover': 'turnover',\n",
    "            'VWAP': 'vwap',\n",
    "            'Symbol': 'symbol',\n",
    "            'Date': 'trade_date'\n",
    "        }\n",
    "        \n",
    "        df_mapped = df.rename(columns=column_mapping)\n",
    "        \n",
    "        # Insert data\n",
    "        try:\n",
    "            df_mapped.to_sql(\n",
    "                'stock_prices',\n",
    "                self.engine,\n",
    "                if_exists='append',\n",
    "                index=False,\n",
    "                method='multi',  # Bulk insert for performance\n",
    "                chunksize=1000   # Insert in batches\n",
    "            )\n",
    "            print(f\"Inserted {len(df_mapped)} records\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Insert failed: {e}\")\n",
    "            # Handle duplicates or constraint violations\n",
    "            if 'UNIQUE constraint failed' in str(e):\n",
    "                print(\"Duplicate entries detected. Consider using upsert.\")\n",
    "    \n",
    "    def query_price_data(self, symbol, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Query data with SQL filtering.\n",
    "        \n",
    "        Demonstrates efficient time-series queries.\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "        SELECT \n",
    "            trade_date,\n",
    "            symbol,\n",
    "            open_price,\n",
    "            high_price,\n",
    "            low_price,\n",
    "            close_price,\n",
    "            volume,\n",
    "            (close_price - LAG(close_price) OVER (PARTITION BY symbol ORDER BY trade_date)) / \n",
    "                LAG(close_price) OVER (PARTITION BY symbol ORDER BY trade_date) * 100 as daily_return_pct\n",
    "        FROM stock_prices\n",
    "        WHERE symbol = :symbol\n",
    "          AND trade_date BETWEEN :start_date AND :end_date\n",
    "        ORDER BY trade_date\n",
    "        \"\"\"\n",
    "        \n",
    "        with self.engine.connect() as conn:\n",
    "            result = pd.read_sql(\n",
    "                text(query),\n",
    "                conn,\n",
    "                params={\n",
    "                    'symbol': symbol,\n",
    "                    'start_date': start_date,\n",
    "                    'end_date': end_date\n",
    "                },\n",
    "                parse_dates=['trade_date']\n",
    "            )\n",
    "        \n",
    "        return result\n",
    "\n",
    "# Demonstration\n",
    "db_manager = NEPSEDatabaseManager('sqlite:///nepse_production.db')\n",
    "\n",
    "# Create schema\n",
    "db_manager.create_schema()\n",
    "\n",
    "# Generate sample data and insert\n",
    "sample_df = pd.DataFrame({\n",
    "    'Symbol': ['NABIL'] * 5,\n",
    "    'Date': pd.date_range('2024-01-15', periods=5),\n",
    "    'Open': [2850.50, 2875.25, 2890.00, 2865.75, 2880.50],\n",
    "    'High': [2890.00, 2910.00, 2920.00, 2900.00, 2915.00],\n",
    "    'Low': [2840.00, 2860.00, 2880.00, 2850.00, 2870.00],\n",
    "    'Close': [2875.25, 2895.50, 2900.00, 2880.50, 2905.00],\n",
    "    'Volume': [125000, 150000, 175000, 140000, 160000]\n",
    "})\n",
    "\n",
    "db_manager.insert_price_data(sample_df)\n",
    "\n",
    "# Query data\n",
    "result = db_manager.query_price_data('NABIL', '2024-01-15', '2024-01-19')\n",
    "print(\"\\nQueried data with calculated returns:\")\n",
    "print(result)\n",
    "\n",
    "db_manager.close()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Database schema design** for time-series:\n",
    "  - **Primary key**: Auto-incrementing ID\n",
    "  - **Unique constraint**: Combination of symbol and date prevents duplicates\n",
    "  - **Check constraints**: Ensure data integrity (high >= low, positive prices)\n",
    "  - **Indexes**: Speed up queries on symbol, date, and composite\n",
    "  - **Timestamps**: Track when data was inserted/updated\n",
    "- **SQLAlchemy** provides:\n",
    "  - **Connection pooling**: Reuses database connections\n",
    "  - **Parameterized queries**: Prevents SQL injection\n",
    "  - **ORM capabilities**: Map tables to Python classes (shown here with raw SQL)\n",
    "- **Window functions** in SQL:\n",
    "  - `LAG(close_price) OVER (PARTITION BY symbol ORDER BY trade_date)`:\n",
    "    - Gets the previous row's close price for the same symbol\n",
    "    - `PARTITION BY` resets the window for each symbol\n",
    "    - `ORDER BY` defines the sequence\n",
    "  - Used here to calculate daily returns in SQL rather than Python\n",
    "- **Bulk insertion**:\n",
    "  - `method='multi'` and `chunksize=1000` optimize large inserts\n",
    "  - Much faster than inserting row by row\n",
    "- **Upsert handling**:\n",
    "  - The code detects duplicate key violations\n",
    "  - In production, use `INSERT ... ON CONFLICT` (PostgreSQL) or equivalent\n",
    "\n",
    "---\n",
    "\n",
    "### **5.4.2 NoSQL Databases**\n",
    "\n",
    "NoSQL databases like MongoDB are useful for flexible schemas and high write throughput.\n",
    "\n",
    "```python\n",
    "from pymongo import MongoClient, ASCENDING, DESCENDING\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "\n",
    "class MongoNEPSEManager:\n",
    "    \"\"\"\n",
    "    MongoDB manager for NEPSE data.\n",
    "    \n",
    "    MongoDB advantages for time-series:\n",
    "    1. Flexible schema (different fields for different symbols)\n",
    "    2. High write throughput (good for tick data)\n",
    "    3. Horizontal scaling (sharding for large datasets)\n",
    "    4. Rich query language with aggregation pipeline\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, connection_string='mongodb://localhost:27017/', db_name='nepse_db'):\n",
    "        \"\"\"\n",
    "        Initialize MongoDB connection.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        connection_string : str\n",
    "            MongoDB connection URI\n",
    "        db_name : str\n",
    "            Database name\n",
    "        \"\"\"\n",
    "        self.client = MongoClient(connection_string)\n",
    "        self.db = self.client[db_name]\n",
    "        \n",
    "        # Collections (like tables in SQL)\n",
    "        self.prices = self.db['stock_prices']\n",
    "        self.companies = self.db['companies']\n",
    "        self.news = self.db['market_news']\n",
    "        \n",
    "        # Create indexes for performance\n",
    "        self._create_indexes()\n",
    "        \n",
    "        print(f\"Connected to MongoDB: {db_name}\")\n",
    "        print(f\"Collections: {self.db.list_collection_names()}\")\n",
    "    \n",
    "    def _create_indexes(self):\n",
    "        \"\"\"\n",
    "        Create indexes for common query patterns.\n",
    "        \n",
    "        Indexes dramatically speed up queries but slow down writes.\n",
    "        \"\"\"\n",
    "        # Compound index for time-series queries (symbol + date)\n",
    "        self.prices.create_index([(\"symbol\", ASCENDING), (\"date\", DESCENDING)])\n",
    "        \n",
    "        # Index for date range queries\n",
    "        self.prices.create_index([(\"date\", DESCENDING)])\n",
    "        \n",
    "        # Index for company lookups\n",
    "        self.companies.create_index([(\"symbol\", ASCENDING)], unique=True)\n",
    "        \n",
    "        # Text index for news search\n",
    "        self.news.create_index([(\"headline\", \"text\"), (\"content\", \"text\")])\n",
    "        \n",
    "        print(\"Indexes created\")\n",
    "    \n",
    "    def insert_price_data(self, df, symbol):\n",
    "        \"\"\"\n",
    "        Insert DataFrame into MongoDB.\n",
    "        \n",
    "        Converts DataFrame to list of dictionaries (documents).\n",
    "        \"\"\"\n",
    "        if df.empty:\n",
    "            return\n",
    "        \n",
    "        # Prepare documents\n",
    "        documents = []\n",
    "        for _, row in df.iterrows():\n",
    "            doc = {\n",
    "                'symbol': symbol,\n",
    "                'date': row['Date'] if 'Date' in row else row.name,\n",
    "                'open': float(row['Open']),\n",
    "                'high': float(row['High']),\n",
    "                'low': float(row['Low']),\n",
    "                'close': float(row['Close']),\n",
    "                'volume': int(row['Volume']),\n",
    "                'metadata': {\n",
    "                    'inserted_at': datetime.now(),\n",
    "                    'source': 'web_scrape',\n",
    "                    'version': '1.0'\n",
    "                }\n",
    "            }\n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Insert with ordered=False (continue on error)\n",
    "        try:\n",
    "            result = self.prices.insert_many(documents, ordered=False)\n",
    "            print(f\"Inserted {len(result.inserted_ids)} documents\")\n",
    "        except Exception as e:\n",
    "            print(f\"Insert error: {e}\")\n",
    "    \n",
    "    def query_price_data(self, symbol, start_date, end_date):\n",
    "        \"\"\"\n",
    "        Query time-series data with filtering.\n",
    "        \n",
    "        Uses MongoDB's query language which is JSON-based.\n",
    "        \"\"\"\n",
    "        query = {\n",
    "            'symbol': symbol,\n",
    "            'date': {\n",
    "                '$gte': start_date,  # Greater than or equal\n",
    "                '$lte': end_date     # Less than or equal\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Projection: only return specific fields (reduce bandwidth)\n",
    "        projection = {\n",
    "            '_id': 0,  # Exclude MongoDB's internal ID\n",
    "            'date': 1,\n",
    "            'open': 1,\n",
    "            'high': 1,\n",
    "            'low': 1,\n",
    "            'close': 1,\n",
    "            'volume': 1\n",
    "        }\n",
    "        \n",
    "        # Sort by date ascending\n",
    "        cursor = self.prices.find(query, projection).sort('date', ASCENDING)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        df = pd.DataFrame(list(cursor))\n",
    "        \n",
    "        if not df.empty:\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df.set_index('date', inplace=True)\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def aggregate_daily_stats(self, symbol, days=30):\n",
    "        \"\"\"\n",
    "        Use MongoDB aggregation pipeline for statistics.\n",
    "        \n",
    "        Aggregation pipelines process data in stages (like SQL GROUP BY).\n",
    "        \"\"\"\n",
    "        pipeline = [\n",
    "            # Stage 1: Match (filter)\n",
    "            {\n",
    "                '$match': {\n",
    "                    'symbol': symbol,\n",
    "                    'date': {\n",
    "                        '$gte': datetime.now() - timedelta(days=days)\n",
    "                    }\n",
    "                }\n",
    "            },\n",
    "            # Stage 2: Group (aggregate)\n",
    "            {\n",
    "                '$group': {\n",
    "                    '_id': None,\n",
    "                    'avg_close': {'$avg': '$close'},\n",
    "                    'max_high': {'$max': '$high'},\n",
    "                    'min_low': {'$min': '$low'},\n",
    "                    'total_volume': {'$sum': '$volume'},\n",
    "                    'count': {'$sum': 1}\n",
    "                }\n",
    "            },\n",
    "            # Stage 3: Project (format output)\n",
    "            {\n",
    "                '$project': {\n",
    "                    '_id': 0,\n",
    "                    'avg_close': {'$round': ['$avg_close', 2]},\n",
    "                    'max_high': 1,\n",
    "                    'min_low': 1,\n",
    "                    'total_volume': 1,\n",
    "                    'trading_days': '$count'\n",
    "                }\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        result = list(self.prices.aggregate(pipeline))\n",
    "        return result[0] if result else None\n",
    "\n",
    "# Usage\n",
    "mongo_manager = MongoNEPSEManager()\n",
    "\n",
    "# Insert sample data\n",
    "sample_df = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-15', periods=5),\n",
    "    'Open': [2850.50, 2875.25, 2890.00, 2865.75, 2880.50],\n",
    "    'High': [2890.00, 2910.00, 2920.00, 2900.00, 2915.00],\n",
    "    'Low': [2840.00, 2860.00, 2880.00, 2850.00, 2870.00],\n",
    "    'Close': [2875.25, 2895.50, 2900.00, 2880.50, 2905.00],\n",
    "    'Volume': [125000, 150000, 175000, 140000, 160000]\n",
    "})\n",
    "\n",
    "mongo_manager.insert_price_data(sample_df, 'NABIL')\n",
    "\n",
    "# Query data\n",
    "result = mongo_manager.query_price_data('NABIL', '2024-01-15', '2024-01-19')\n",
    "print(\"\\nQueried data:\")\n",
    "print(result)\n",
    "\n",
    "# Aggregate stats\n",
    "stats = mongo_manager.aggregate_daily_stats('NABIL', days=30)\n",
    "print(\"\\nAggregated statistics:\")\n",
    "print(stats)\n",
    "\n",
    "# Output:\n",
    "# Connected to MongoDB: nepse_db\n",
    "# Collections: ['stock_prices', 'companies', 'market_news']\n",
    "# Indexes created\n",
    "# Inserted 5 documents\n",
    "#\n",
    "# Queried data:\n",
    "#             open    high     low   close  volume\n",
    "# date                                             \n",
    "# 2024-01-15  2850.5  2890.0  2840.0  2875.25  125000\n",
    "# 2024-01-16  2875.25  2910.0  2860.0  2895.5  150000\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **MongoDB** is a document-oriented NoSQL database.\n",
    "- **Advantages for time-series**:\n",
    "  - **Flexible schema**: Different documents can have different fields\n",
    "  - **High write throughput**: Good for tick data ingestion\n",
    "  - **Horizontal scaling**: Shard data across servers for large datasets\n",
    "  - **Rich queries**: Aggregation pipelines for complex analytics\n",
    "- **Schema design**:\n",
    "  - Each stock price is a document (JSON object)\n",
    "  - Contains nested metadata object\n",
    "  - Uses MongoDB's `_id` as primary key\n",
    "- **Indexing strategy**:\n",
    "  - Compound index on `(symbol, date)` for time-series queries\n",
    "  - Single index on `date` for date range queries\n",
    "  - Text index on news headlines for search\n",
    "- **Aggregation pipeline**:\n",
    "  - Stage 1 (`$match`): Filter documents\n",
    "  - Stage 2 (`$group`): Aggregate (avg, max, sum)\n",
    "  - Stage 3 (`$project`): Format output\n",
    "- **Comparison with SQL**:\n",
    "  - SQL uses tables and rows; MongoDB uses collections and documents\n",
    "  - SQL uses JOINs; MongoDB uses embedding or references\n",
    "  - SQL has transactions; MongoDB has multi-document transactions (newer)\n",
    "  - For time-series, both work well; choice depends on existing infrastructure\n",
    "\n",
    "---\n",
    "\n",
    "## **5.7 Data Validation and Quality Checks**\n",
    "\n",
    "Automated validation ensures data integrity before storage.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class DataValidator:\n",
    "    \"\"\"\n",
    "    Comprehensive data validation for NEPSE time-series data.\n",
    "    \n",
    "    Validation ensures:\n",
    "    1. Data types are correct\n",
    "    2. Values are within expected ranges\n",
    "    3. Relationships between columns are valid (High >= Low)\n",
    "    4. No duplicates or missing critical fields\n",
    "    5. Time-series continuity\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.validation_errors = []\n",
    "        self.validation_warnings = []\n",
    "    \n",
    "    def validate_schema(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Check that DataFrame has required columns with correct types.\n",
    "        \"\"\"\n",
    "        required_columns = {\n",
    "            'Date': 'datetime64[ns]',\n",
    "            'Open': 'float64',\n",
    "            'High': 'float64',\n",
    "            'Low': 'float64',\n",
    "            'Close': 'float64',\n",
    "            'Volume': 'int64',\n",
    "            'Symbol': 'object'\n",
    "        }\n",
    "        \n",
    "        is_valid = True\n",
    "        \n",
    "        # Check presence\n",
    "        for col in required_columns.keys():\n",
    "            if col not in df.columns:\n",
    "                self.validation_errors.append(f\"Missing required column: {col}\")\n",
    "                is_valid = False\n",
    "        \n",
    "        # Check types (allow flexibility for numeric types)\n",
    "        type_mapping = {\n",
    "            'datetime64[ns]': ['datetime64[ns]', 'datetime64[ns, UTC]'],\n",
    "            'float64': ['float64', 'float32', 'int64', 'int32'],\n",
    "            'int64': ['int64', 'int32', 'float64'],  # Volume might be float if has NaN\n",
    "            'object': ['object', 'category']\n",
    "        }\n",
    "        \n",
    "        for col, expected_type in required_columns.items():\n",
    "            if col in df.columns:\n",
    "                actual_type = str(df[col].dtype)\n",
    "                allowed_types = type_mapping.get(expected_type, [expected_type])\n",
    "                \n",
    "                if actual_type not in allowed_types:\n",
    "                    self.validation_warnings.append(\n",
    "                        f\"Column {col}: expected {expected_type}, got {actual_type}\"\n",
    "                    )\n",
    "        \n",
    "        return is_valid\n",
    "    \n",
    "    def validate_business_rules(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Validate financial/business logic constraints.\n",
    "        \"\"\"\n",
    "        is_valid = True\n",
    "        \n",
    "        # Rule 1: High >= Low\n",
    "        invalid_hl = df[df['High'] < df['Low']]\n",
    "        if len(invalid_hl) > 0:\n",
    "            self.validation_errors.append(\n",
    "                f\"High < Low found in {len(invalid_hl)} rows: {invalid_hl.index.tolist()}\"\n",
    "            )\n",
    "            is_valid = False\n",
    "        \n",
    "        # Rule 2: High >= Open and High >= Close\n",
    "        invalid_high = df[(df['High'] < df['Open']) | (df['High'] < df['Close'])]\n",
    "        if len(invalid_high) > 0:\n",
    "            self.validation_errors.append(\n",
    "                f\"High < Open or Close in {len(invalid_high)} rows\"\n",
    "            )\n",
    "            is_valid = False\n",
    "        \n",
    "        # Rule 3: Low <= Open and Low <= Close\n",
    "        invalid_low = df[(df['Low'] > df['Open']) | (df['Low'] > df['Close'])]\n",
    "        if len(invalid_low) > 0:\n",
    "            self.validation_errors.append(\n",
    "                f\"Low > Open or Close in {len(invalid_low)} rows\"\n",
    "            )\n",
    "            is_valid = False\n",
    "        \n",
    "        # Rule 4: Volume > 0\n",
    "        invalid_vol = df[df['Volume'] <= 0]\n",
    "        if len(invalid_vol) > 0:\n",
    "            self.validation_warnings.append(\n",
    "                f\"Zero or negative volume in {len(invalid_vol)} rows\"\n",
    "            )\n",
    "        \n",
    "        # Rule 5: Price range sanity check (for NEPSE, prices usually 100-5000)\n",
    "        out_of_range = df[\n",
    "            (df['Close'] < 10) | (df['Close'] > 10000)\n",
    "        ]\n",
    "        if len(out_of_range) > 0:\n",
    "            self.validation_warnings.append(\n",
    "                f\"Suspicious prices (outside 10-10000) in {len(out_of_range)} rows\"\n",
    "            )\n",
    "        \n",
    "        return is_valid\n",
    "    \n",
    "    def validate_time_series_continuity(self, df: pd.DataFrame, symbol: str) -> bool:\n",
    "        \"\"\"\n",
    "        Check for gaps in time-series data.\n",
    "        \"\"\"\n",
    "        if 'Date' not in df.columns:\n",
    "            return True\n",
    "        \n",
    "        # Sort by date\n",
    "        df_sorted = df.sort_values('Date')\n",
    "        dates = pd.to_datetime(df_sorted['Date'])\n",
    "        \n",
    "        # Check for missing dates (business days)\n",
    "        date_range = pd.date_range(start=dates.min(), end=dates.max(), freq='B')\n",
    "        missing_dates = date_range.difference(dates)\n",
    "        \n",
    "        if len(missing_dates) > 0:\n",
    "            self.validation_warnings.append(\n",
    "                f\"Missing {len(missing_dates)} trading days for {symbol}: \"\n",
    "                f\"{missing_dates[:5].tolist()}...\"\n",
    "            )\n",
    "            return False\n",
    "        \n",
    "        return True\n",
    "    \n",
    "    def generate_report(self) -> Dict:\n",
    "        \"\"\"Generate validation report.\"\"\"\n",
    "        return {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'is_valid': len(self.validation_errors) == 0,\n",
    "            'errors': self.validation_errors,\n",
    "            'warnings': self.validation_warnings,\n",
    "            'error_count': len(self.validation_errors),\n",
    "            'warning_count': len(self.validation_warnings)\n",
    "        }\n",
    "\n",
    "# Demonstration\n",
    "validator = DataValidator()\n",
    "\n",
    "# Create test data with intentional errors\n",
    "test_data = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-15', periods=5),\n",
    "    'Symbol': 'NABIL',\n",
    "    'Open': [2850.50, 2875.25, 2890.00, 2865.75, 2880.50],\n",
    "    'High': [2890.00, 2910.00, 2920.00, 2900.00, 2915.00],\n",
    "    'Low': [2840.00, 2860.00, 2880.00, 2850.00, 2870.00],\n",
    "    'Close': [2875.25, 2895.50, 2900.00, 2880.50, 2905.00],\n",
    "    'Volume': [125000, 150000, 175000, 140000, 160000]\n",
    "})\n",
    "\n",
    "# Add intentional errors for demonstration\n",
    "test_data_with_errors = test_data.copy()\n",
    "test_data_with_errors.loc[2, 'High'] = 2800.00  # High < Low error\n",
    "test_data_with_errors.loc[3, 'Volume'] = -1000  # Negative volume\n",
    "\n",
    "# Validate\n",
    "print(\"Running validation...\")\n",
    "is_schema_valid = validator.validate_schema(test_data_with_errors)\n",
    "is_business_valid = validator.validate_business_rules(test_data_with_errors)\n",
    "is_continuous = validator.validate_time_series_continuity(test_data_with_errors, 'NABIL')\n",
    "\n",
    "report = validator.generate_report()\n",
    "\n",
    "print(f\"\\nValidation Report:\")\n",
    "print(f\"Schema Valid: {is_schema_valid}\")\n",
    "print(f\"Business Rules Valid: {is_business_valid}\")\n",
    "print(f\"Time Series Continuous: {is_continuous}\")\n",
    "print(f\"Errors: {report['error_count']}\")\n",
    "print(f\"Warnings: {report['warning_count']}\")\n",
    "\n",
    "if report['errors']:\n",
    "    print(\"\\nErrors found:\")\n",
    "    for error in report['errors']:\n",
    "        print(f\"  - {error}\")\n",
    "\n",
    "if report['warnings']:\n",
    "    print(\"\\nWarnings found:\")\n",
    "    for warning in report['warnings']:\n",
    "        print(f\"  - {warning}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Data validation** ensures data quality before storage or processing.\n",
    "- **Three levels of validation**:\n",
    "  1. **Schema validation**: Correct columns and data types\n",
    "  2. **Business rules**: Financial logic (High >= Low, positive volume)\n",
    "  3. **Time-series continuity**: No missing dates in sequence\n",
    "- **Business rules for OHLCV data**:\n",
    "  - `High >= Low`: Daily range must be positive\n",
    "  - `High >= Open` and `High >= Close`: High is the maximum\n",
    "  - `Low <= Open` and `Low <= Close`: Low is the minimum\n",
    "  - `Volume > 0`: Trading volume must be positive\n",
    "  - Price ranges: Sanity checks (e.g., NEPSE stocks usually 10-10000 NPR)\n",
    "- **Time-series continuity**:\n",
    "  - Generates complete business day range\n",
    "  - Compares with actual dates in data\n",
    "  - Reports missing trading days\n",
    "- **Validation report**:\n",
    "  - Distinguishes errors (must fix) from warnings (should review)\n",
    "  - Provides counts and detailed messages\n",
    "  - Includes timestamp for audit trail\n",
    "- **In production**:\n",
    "  - Run validation at ingestion time\n",
    "  - Reject batches with errors\n",
    "  - Alert on warnings\n",
    "  - Log all validation results\n",
    "\n",
    "---\n",
    "\n",
    "## **5.8 Automated Collection Pipelines**\n",
    "\n",
    "Production systems require automated pipelines that run on schedules, handle errors, and maintain data freshness.\n",
    "\n",
    "```python\n",
    "import schedule\n",
    "import time\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from typing import Callable, Dict, Any\n",
    "import json\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('nepse_pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger('NEPSEPipeline')\n",
    "\n",
    "class DataCollectionPipeline:\n",
    "    \"\"\"\n",
    "    Automated pipeline for NEPSE data collection.\n",
    "    \n",
    "    Features:\n",
    "    - Scheduled execution\n",
    "    - Error handling with retry logic\n",
    "    - Data validation at ingestion\n",
    "    - Audit logging\n",
    "    - State persistence (track last successful run)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_manager, api_client, validator):\n",
    "        self.db = db_manager\n",
    "        self.api = api_client\n",
    "        self.validator = validator\n",
    "        self.state_file = 'pipeline_state.json'\n",
    "        self.running = False\n",
    "        \n",
    "        # Load previous state\n",
    "        self.state = self._load_state()\n",
    "    \n",
    "    def _load_state(self) -> Dict:\n",
    "        \"\"\"Load pipeline state from file.\"\"\"\n",
    "        try:\n",
    "            with open(self.state_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return {\n",
    "                'last_successful_run': None,\n",
    "                'last_symbol': None,\n",
    "                'total_records_collected': 0,\n",
    "                'failed_attempts': 0\n",
    "            }\n",
    "    \n",
    "    def _save_state(self):\n",
    "        \"\"\"Save current state to file.\"\"\"\n",
    "        with open(self.state_file, 'w') as f:\n",
    "            json.dump(self.state, f, indent=2)\n",
    "    \n",
    "    def _collect_symbol_data(self, symbol: str, start_date: str, end_date: str) -> bool:\n",
    "        \"\"\"\n",
    "        Collect data for a single symbol with full error handling.\n",
    "        \n",
    "        Returns True if successful, False otherwise.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Collecting data for {symbol} from {start_date} to {end_date}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Fetch from API\n",
    "            raw_data = self.api.get_historical_data(symbol, start_date, end_date)\n",
    "            \n",
    "            if raw_data is None or 'stockData' not in raw_data:\n",
    "                logger.warning(f\"No data returned for {symbol}\")\n",
    "                return False\n",
    "            \n",
    "            # 2. Convert to DataFrame\n",
    "            df = pd.DataFrame(raw_data['stockData'])\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            \n",
    "            # 3. Validate data\n",
    "            is_valid = self.validator.validate_schema(df)\n",
    "            is_valid = is_valid and self.validator.validate_business_rules(df)\n",
    "            \n",
    "            if not is_valid:\n",
    "                errors = self.validator.validation_errors\n",
    "                logger.error(f\"Validation failed for {symbol}: {errors}\")\n",
    "                return False\n",
    "            \n",
    "            # 4. Insert into database\n",
    "            self.db.insert_price_data(df, symbol)\n",
    "            \n",
    "            # 5. Update state\n",
    "            self.state['last_symbol'] = symbol\n",
    "            self.state['total_records_collected'] += len(df)\n",
    "            self._save_state()\n",
    "            \n",
    "            logger.info(f\"Successfully collected {len(df)} records for {symbol}\")\n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Exception collecting data for {symbol}: {e}\")\n",
    "            self.state['failed_attempts'] += 1\n",
    "            self._save_state()\n",
    "            return False\n",
    "    \n",
    "    def run_collection_job(self, symbols: list, days_back: int = 30):\n",
    "        \"\"\"\n",
    "        Run collection job for multiple symbols.\n",
    "        \n",
    "        Implements retry logic and batch processing.\n",
    "        \"\"\"\n",
    "        end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "        start_date = (datetime.now() - timedelta(days=days_back)).strftime('%Y-%m-%d')\n",
    "        \n",
    "        logger.info(f\"Starting collection job for {len(symbols)} symbols\")\n",
    "        logger.info(f\"Date range: {start_date} to {end_date}\")\n",
    "        \n",
    "        success_count = 0\n",
    "        fail_count = 0\n",
    "        \n",
    "        for i, symbol in enumerate(symbols):\n",
    "            logger.info(f\"Processing {i+1}/{len(symbols)}: {symbol}\")\n",
    "            \n",
    "            # Try up to 3 times with exponential backoff\n",
    "            for attempt in range(3):\n",
    "                if attempt > 0:\n",
    "                    wait_time = 2 ** attempt  # Exponential backoff: 2, 4 seconds\n",
    "                    logger.info(f\"Retry {attempt} for {symbol} after {wait_time}s\")\n",
    "                    time.sleep(wait_time)\n",
    "                \n",
    "                success = self._collect_symbol_data(symbol, start_date, end_date)\n",
    "                \n",
    "                if success:\n",
    "                    success_count += 1\n",
    "                    break\n",
    "                elif attempt == 2:  # Last attempt failed\n",
    "                    fail_count += 1\n",
    "                    logger.error(f\"Failed to collect {symbol} after 3 attempts\")\n",
    "            \n",
    "            # Polite delay between symbols\n",
    "            if i < len(symbols) - 1:  # Don't delay after last\n",
    "                delay = random.uniform(1, 3)\n",
    "                time.sleep(delay)\n",
    "        \n",
    "        logger.info(f\"Collection job complete. Success: {success_count}, Failed: {fail_count}\")\n",
    "        return success_count, fail_count\n",
    "\n",
    "# Schedule the job to run daily\n",
    "def scheduled_job():\n",
    "    \"\"\"Function to be called by scheduler.\"\"\"\n",
    "    symbols = ['NABIL', 'NICA', 'SCBL', 'ADBL']\n",
    "    \n",
    "    # Initialize components\n",
    "    db_mgr = NEPSEDatabaseManager('sqlite:///nepse_pipeline.db')\n",
    "    api_client = NEPSEDataCollector(api_key='demo')\n",
    "    validator = DataValidator()\n",
    "    \n",
    "    # Create pipeline\n",
    "    pipeline = DataCollectionPipeline(db_mgr, api_client, validator)\n",
    "    \n",
    "    # Run job\n",
    "    pipeline.run_collection_job(symbols, days_back=1)  # Just yesterday's data\n",
    "\n",
    "# Set up schedule (commented out to prevent accidental execution)\n",
    "\"\"\"\n",
    "schedule.every().day.at(\"18:00\").do(scheduled_job)  # Run at 6 PM daily\n",
    "\n",
    "print(\"Scheduler set up. Running...\")\n",
    "while True:\n",
    "    schedule.run_pending()\n",
    "    time.sleep(60)  # Check every minute\n",
    "\"\"\"\n",
    "\n",
    "print(\"Pipeline components defined\")\n",
    "print(\"To run scheduled collection:\")\n",
    "print(\"1. Set up environment variables for API keys\")\n",
    "print(\"2. Configure schedule (e.g., daily at 6 PM)\")\n",
    "print(\"3. Run scheduler loop\")\n",
    "print(\"4. Monitor logs for failures\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Ethical scraping** is crucial for sustainable data collection:\n",
    "- **Rate limiting**:\n",
    "  - Random delays (1-3 seconds) between requests\n",
    "  - Exponential backoff on retries (2, 4, 8 seconds)\n",
    "  - Prevents overwhelming the server\n",
    "  - Avoids IP bans\n",
    "- **Retry logic**:\n",
    "  - Try 3 times before giving up\n",
    "  - Wait longer between each retry\n",
    "  - Log failures for monitoring\n",
    "- **Caching**:\n",
    "  - Store results to avoid repeated requests\n",
    "  - Cache for 1 hour (configurable)\n",
    "  - Reduces load on both sides\n",
    "- **State persistence**:\n",
    "  - Save progress to file (`pipeline_state.json`)\n",
    "  - Resume from last successful symbol if interrupted\n",
    "  - Track total records and failures\n",
    "- **Scheduling**:\n",
    "  - `schedule` library runs jobs at specific times\n",
    "  - Run daily at 6 PM (after market close)\n",
    "  - Infinite loop checks for pending jobs every minute\n",
    "- **Security**:\n",
    "  - API keys from environment variables\n",
    "  - No hardcoded credentials\n",
    "  - Secure credential input with `getpass`\n",
    "- **Validation**:\n",
    "  - Validate data before insertion\n",
    "  - Reject bad data rather than corrupting database\n",
    "  - Log validation failures\n",
    "\n",
    "---\n",
    "\n",
    "## **5.11 Building a Robust Ingestion System**\n",
    "\n",
    "Putting it all together into a production-ready system.\n",
    "\n",
    "```python\n",
    "import logging\n",
    "import sys\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "import yaml\n",
    "\n",
    "class NEPSEIngestionSystem:\n",
    "    \"\"\"\n",
    "    Production-ready data ingestion system for NEPSE.\n",
    "    \n",
    "    Features:\n",
    "    - Configuration-driven (YAML config)\n",
    "    - Comprehensive logging\n",
    "    - Health checks\n",
    "    - Circuit breaker pattern (fail fast if service down)\n",
    "    - Data lineage tracking\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config_path: str = 'config.yaml'):\n",
    "        self.config = self._load_config(config_path)\n",
    "        self._setup_logging()\n",
    "        self.logger = logging.getLogger('NEPSEIngestion')\n",
    "        \n",
    "        # Initialize components based on config\n",
    "        self.db = self._init_database()\n",
    "        self.api = self._init_api_client()\n",
    "        self.validator = DataValidator()\n",
    "        \n",
    "        # Circuit breaker state\n",
    "        self.circuit_open = False\n",
    "        self.failure_count = 0\n",
    "        self.failure_threshold = 5\n",
    "        self.last_failure_time = None\n",
    "        \n",
    "        self.logger.info(\"NEPSE Ingestion System initialized\")\n",
    "    \n",
    "    def _load_config(self, path: str) -> dict:\n",
    "        \"\"\"Load configuration from YAML.\"\"\"\n",
    "        try:\n",
    "            with open(path, 'r') as f:\n",
    "                return yaml.safe_load(f)\n",
    "        except FileNotFoundError:\n",
    "            self.logger.warning(f\"Config file {path} not found, using defaults\")\n",
    "            return {\n",
    "                'database': {'type': 'sqlite', 'path': 'nepse.db'},\n",
    "                'api': {'base_url': 'https://api.nepse.example.com', 'rate_limit': 1},\n",
    "                'logging': {'level': 'INFO', 'file': 'ingestion.log'}\n",
    "            }\n",
    "    \n",
    "    def _setup_logging(self):\n",
    "        \"\"\"Configure logging with rotation.\"\"\"\n",
    "        log_format = '%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    "        logging.basicConfig(\n",
    "            level=logging.INFO,\n",
    "            format=log_format,\n",
    "            handlers=[\n",
    "                logging.FileHandler('nepse_ingestion.log'),\n",
    "                logging.StreamHandler(sys.stdout)\n",
    "            ]\n",
    "        )\n",
    "    \n",
    "    def _init_database(self):\n",
    "        \"\"\"Initialize database connection based on config.\"\"\"\n",
    "        db_config = self.config.get('database', {})\n",
    "        db_type = db_config.get('type', 'sqlite')\n",
    "        \n",
    "        if db_type == 'sqlite':\n",
    "            return NEPSEDatabaseManager(f\"sqlite:///{db_config.get('path', 'nepse.db')}\")\n",
    "        elif db_type == 'postgresql':\n",
    "            # Would return PostgreSQL manager\n",
    "            pass\n",
    "        else:\n",
    "            raise ValueError(f\"Unsupported database type: {db_type}\")\n",
    "    \n",
    "    def _init_api_client(self):\n",
    "        \"\"\"Initialize API client.\"\"\"\n",
    "        api_config = self.config.get('api', {})\n",
    "        return NEPSEDataCollector(\n",
    "            api_key=api_config.get('key'),\n",
    "            base_url=api_config.get('base_url', 'https://api.nepse.example.com')\n",
    "        )\n",
    "    \n",
    "    def _circuit_breaker_check(self) -> bool:\n",
    "        \"\"\"\n",
    "        Circuit breaker pattern to prevent cascading failures.\n",
    "        \n",
    "        If service fails repeatedly, stop trying for a while.\n",
    "        \"\"\"\n",
    "        if not self.circuit_open:\n",
    "            return True\n",
    "        \n",
    "        # Check if enough time has passed to try again (5 minutes)\n",
    "        if self.last_failure_time:\n",
    "            time_since_failure = (datetime.now() - self.last_failure_time).total_seconds()\n",
    "            if time_since_failure > 300:  # 5 minutes\n",
    "                self.logger.info(\"Circuit breaker reset, retrying...\")\n",
    "                self.circuit_open = False\n",
    "                self.failure_count = 0\n",
    "                return True\n",
    "        \n",
    "        return False\n",
    "    \n",
    "    def _record_failure(self):\n",
    "        \"\"\"Record a failure and potentially open circuit.\"\"\"\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = datetime.now()\n",
    "        \n",
    "        if self.failure_count >= self.failure_threshold:\n",
    "            self.logger.error(f\"Circuit breaker opened after {self.failure_count} failures\")\n",
    "            self.circuit_open = True\n",
    "    \n",
    "    def ingest_symbol(self, symbol: str, days: int = 30) -> bool:\n",
    "        \"\"\"\n",
    "        Main ingestion method with full error handling.\n",
    "        \n",
    "        Pipeline:\n",
    "        1. Check circuit breaker\n",
    "        2. Fetch from API\n",
    "        3. Validate data\n",
    "        4. Transform/clean\n",
    "        5. Store in database\n",
    "        6. Log success/failure\n",
    "        \"\"\"\n",
    "        if not self._circuit_breaker_check():\n",
    "            self.logger.warning(f\"Circuit open, skipping ingestion for {symbol}\")\n",
    "            return False\n",
    "        \n",
    "        self.logger.info(f\"Starting ingestion for {symbol}\")\n",
    "        \n",
    "        try:\n",
    "            # 1. Fetch data\n",
    "            end_date = datetime.now().strftime('%Y-%m-%d')\n",
    "            start_date = (datetime.now() - timedelta(days=days)).strftime('%Y-%m-%d')\n",
    "            \n",
    "            raw_data = self.api.get_historical_data(symbol, start_date, end_date)\n",
    "            \n",
    "            if raw_data is None or 'stockData' not in raw_data:\n",
    "                raise ValueError(\"No data received from API\")\n",
    "            \n",
    "            # 2. Convert to DataFrame\n",
    "            df = pd.DataFrame(raw_data['stockData'])\n",
    "            df['date'] = pd.to_datetime(df['date'])\n",
    "            df['symbol'] = symbol\n",
    "            \n",
    "            # 3. Validate\n",
    "            if not self.validator.validate_schema(df):\n",
    "                raise ValueError(f\"Schema validation failed: {self.validator.validation_errors}\")\n",
    "            \n",
    "            if not self.validator.validate_business_rules(df):\n",
    "                raise ValueError(f\"Business rules validation failed\")\n",
    "            \n",
    "            # 4. Store in database\n",
    "            self.db.insert_price_data(df, symbol)\n",
    "            \n",
    "            # 5. Log success\n",
    "            self.logger.info(f\"Successfully ingested {len(df)} records for {symbol}\")\n",
    "            \n",
    "            # Reset failure count on success\n",
    "            self.failure_count = 0\n",
    "            \n",
    "            return True\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Ingestion failed for {symbol}: {e}\")\n",
    "            self._record_failure()\n",
    "            return False\n",
    "    \n",
    "    def run_batch_ingestion(self, symbols: List[str]):\n",
    "        \"\"\"\n",
    "        Run ingestion for multiple symbols with progress tracking.\n",
    "        \"\"\"\n",
    "        self.logger.info(f\"Starting batch ingestion for {len(symbols)} symbols\")\n",
    "        \n",
    "        results = {\n",
    "            'success': [],\n",
    "            'failed': [],\n",
    "            'total': len(symbols)\n",
    "        }\n",
    "        \n",
    "        for i, symbol in enumerate(symbols, 1):\n",
    "            self.logger.info(f\"Progress: {i}/{len(symbols)} - Processing {symbol}\")\n",
    "            \n",
    "            success = self.ingest_symbol(symbol, days=30)\n",
    "            \n",
    "            if success:\n",
    "                results['success'].append(symbol)\n",
    "            else:\n",
    "                results['failed'].append(symbol)\n",
    "            \n",
    "            # Brief pause between symbols\n",
    "            time.sleep(1)\n",
    "        \n",
    "        # Summary\n",
    "        success_rate = len(results['success']) / results['total'] * 100\n",
    "        self.logger.info(f\"Batch complete. Success rate: {success_rate:.1f}%\")\n",
    "        self.logger.info(f\"Failed symbols: {results['failed']}\")\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Usage\n",
    "\"\"\"\n",
    "# Initialize system\n",
    "system = NEPSEIngestionSystem('config.yaml')\n",
    "\n",
    "# Single symbol\n",
    "success = system.ingest_symbol('NABIL', days=30)\n",
    "\n",
    "# Batch\n",
    "results = system.run_batch_ingestion(['NABIL', 'NICA', 'SCBL', 'ADBL'])\n",
    "\n",
    "# Close\n",
    "system.close()\n",
    "\"\"\"\n",
    "\n",
    "print(\"NEPSE Ingestion System defined\")\n",
    "print(\"Features:\")\n",
    "print(\"- Circuit breaker pattern for fault tolerance\")\n",
    "print(\"- Comprehensive validation pipeline\")\n",
    "print(\"- Batch processing with progress tracking\")\n",
    "print(\"- Detailed logging for audit trails\")\n",
    "print(\"- State persistence across runs\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Production ingestion system** requires enterprise patterns:\n",
    "- **Circuit Breaker**: If API fails 5 times, stop trying for 5 minutes to prevent cascading failures and allow service to recover.\n",
    "- **Validation Pipeline**: Four-stage validation (schema, business rules, time-series continuity, data quality) ensures only clean data enters the database.\n",
    "- **Error Handling**: Catches exceptions at each stage, logs detailed error messages, and tracks failure counts.\n",
    "- **State Management**: Saves progress to JSON file so if the script crashes, it can resume from where it left off.\n",
    "- **Batch Processing**: Handles multiple symbols with progress tracking (5/10 complete) and summary statistics.\n",
    "- **Logging**: Uses Python's logging module with both file and console handlers for audit trails.\n",
    "- **Configuration**: Loads settings from YAML file (database type, API keys, rate limits) so code doesn't need modification for different environments.\n",
    "- **Database Abstraction**: Uses SQLAlchemy so the same code works with SQLite (development), PostgreSQL (production), or MySQL.\n",
    "\n",
    "---\n",
    "\n",
    "## **5.10 Data Versioning**\n",
    "\n",
    "Track changes to datasets over time for reproducibility and auditability.\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "class DataVersioning:\n",
    "    \"\"\"\n",
    "    Simple data versioning system for tracking dataset changes.\n",
    "    \n",
    "    Versioning ensures:\n",
    "    - Reproducibility: Know exactly which data was used for a model\n",
    "    - Auditability: Track when and how data changed\n",
    "    - Rollback: Revert to previous data versions if issues found\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, version_file='data_versions.json'):\n",
    "        self.version_file = version_file\n",
    "        self.versions = self._load_versions()\n",
    "    \n",
    "    def _load_versions(self) -> Dict:\n",
    "        \"\"\"Load version history.\"\"\"\n",
    "        try:\n",
    "            with open(self.version_file, 'r') as f:\n",
    "                return json.load(f)\n",
    "        except FileNotFoundError:\n",
    "            return {'versions': [], 'current_version': None}\n",
    "    \n",
    "    def _save_versions(self):\n",
    "        \"\"\"Save version history.\"\"\"\n",
    "        with open(self.version_file, 'w') as f:\n",
    "            json.dump(self.versions, f, indent=2, default=str)\n",
    "    \n",
    "    def _calculate_hash(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Calculate MD5 hash of DataFrame for integrity checking.\n",
    "        \n",
    "        This creates a fingerprint of the data. If even one bit changes,\n",
    "        the hash will be completely different.\n",
    "        \"\"\"\n",
    "        # Convert to string representation and hash\n",
    "        data_string = df.to_json(sort_keys=True)\n",
    "        return hashlib.md5(data_string.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def create_version(self, df: pd.DataFrame, symbol: str, \n",
    "                      source: str, notes: str = '') -> str:\n",
    "        \"\"\"\n",
    "        Create a new version snapshot.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        str\n",
    "            Version ID (timestamp-based)\n",
    "        \"\"\"\n",
    "        version_id = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "        \n",
    "        version_info = {\n",
    "            'version_id': version_id,\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'symbol': symbol,\n",
    "            'record_count': len(df),\n",
    "            'date_range': {\n",
    "                'start': df['Date'].min().isoformat() if 'Date' in df.columns else None,\n",
    "                'end': df['Date'].max().isoformat() if 'Date' in df.columns else None\n",
    "            },\n",
    "            'data_hash': self._calculate_hash(df),\n",
    "            'source': source,\n",
    "            'notes': notes,\n",
    "            'schema_version': '1.0'\n",
    "        }\n",
    "        \n",
    "        self.versions['versions'].append(version_info)\n",
    "        self.versions['current_version'] = version_id\n",
    "        self._save_versions()\n",
    "        \n",
    "        print(f\"Created version {version_id} for {symbol}\")\n",
    "        print(f\"  Records: {version_info['record_count']}\")\n",
    "        print(f\"  Hash: {version_info['data_hash']}\")\n",
    "        \n",
    "        return version_id\n",
    "    \n",
    "    def verify_version(self, df: pd.DataFrame, version_id: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verify data integrity against stored hash.\n",
    "        \n",
    "        Use this to ensure data hasn't been corrupted or tampered with.\n",
    "        \"\"\"\n",
    "        version = next((v for v in self.versions['versions'] \n",
    "                       if v['version_id'] == version_id), None)\n",
    "        \n",
    "        if not version:\n",
    "            print(f\"Version {version_id} not found\")\n",
    "            return False\n",
    "        \n",
    "        current_hash = self._calculate_hash(df)\n",
    "        stored_hash = version['data_hash']\n",
    "        \n",
    "        if current_hash == stored_hash:\n",
    "            print(f\"\u2713 Version {version_id} verified. Data integrity confirmed.\")\n",
    "            return True\n",
    "        else:\n",
    "            print(f\"\u2717 Version {version_id} verification FAILED!\")\n",
    "            print(f\"  Stored hash: {stored_hash}\")\n",
    "            print(f\"  Current hash: {current_hash}\")\n",
    "            return False\n",
    "    \n",
    "    def get_version_history(self, symbol: str = None) -> pd.DataFrame:\n",
    "        \"\"\"Get version history as DataFrame.\"\"\"\n",
    "        versions = self.versions['versions']\n",
    "        \n",
    "        if symbol:\n",
    "            versions = [v for v in versions if v['symbol'] == symbol]\n",
    "        \n",
    "        return pd.DataFrame(versions)\n",
    "\n",
    "# Usage\n",
    "versioning = DataVersioning()\n",
    "\n",
    "# Create a version\n",
    "test_df = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-15', periods=3),\n",
    "    'Close': [2875.25, 2895.50, 2900.00]\n",
    "})\n",
    "\n",
    "version_id = versioning.create_version(\n",
    "    test_df, \n",
    "    'NABIL', \n",
    "    'API',\n",
    "    'Daily collection job'\n",
    ")\n",
    "\n",
    "# Verify later\n",
    "is_valid = versioning.verify_version(test_df, version_id)\n",
    "\n",
    "# View history\n",
    "history = versioning.get_version_history('NABIL')\n",
    "print(\"\\nVersion History:\")\n",
    "print(history[['version_id', 'timestamp', 'record_count', 'data_hash']])\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Data versioning** tracks every change to your dataset:\n",
    "- **Why version data**:\n",
    "  - **Reproducibility**: Know exactly which data version produced a specific model\n",
    "  - **Auditability**: Track when data changed and why\n",
    "  - **Rollback**: If bad data is inserted, revert to previous version\n",
    "  - **Compliance**: Financial regulations often require data lineage\n",
    "- **Version metadata**:\n",
    "  - **Version ID**: Timestamp-based unique identifier\n",
    "  - **Data hash**: MD5 fingerprint of the data for integrity verification\n",
    "  - **Record count**: Number of rows\n",
    "  - **Date range**: Temporal coverage\n",
    "  - **Source**: Where data came from (API, scrape, file)\n",
    "  - **Notes**: Human-readable description\n",
    "- **Hash verification**:\n",
    "  - Recalculate hash of current data\n",
    "  - Compare with stored hash\n",
    "  - If different: data has been corrupted or modified\n",
    "- **In production**:\n",
    "  - Store versions in database (not just JSON file)\n",
    "  - Include git commit hash of the code used\n",
    "  - Track data dependencies (raw \u2192 cleaned \u2192 features)\n",
    "  - Automate version creation in pipeline\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we covered the complete data collection and ingestion pipeline:\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Data Sources**: Evaluate sources based on reliability, cost, historical depth, and update frequency. NEPSE official sites, third-party APIs, and web scraping are all viable options.\n",
    "\n",
    "2. **API Integration**: \n",
    "   - REST APIs for request-response data fetching\n",
    "   - GraphQL for precise data fetching\n",
    "   - WebSockets for real-time streaming\n",
    "   - Always implement authentication, rate limiting, and error handling\n",
    "\n",
    "3. **Web Scraping**: \n",
    "   - Use requests/BeautifulSoup for static HTML\n",
    "   - Use Selenium for JavaScript-rendered content\n",
    "   - Always respect robots.txt and implement polite delays\n",
    "   - Handle dynamic content with explicit waits\n",
    "\n",
    "4. **Database Storage**:\n",
    "   - SQL (PostgreSQL/MySQL) for structured data with complex relationships\n",
    "   - NoSQL (MongoDB) for flexible schemas and high write throughput\n",
    "   - Time-series databases (InfluxDB) for specialized time-series storage\n",
    "   - Proper indexing is crucial for query performance\n",
    "\n",
    "5. **Data Validation**: Implement multi-layer validation:\n",
    "   - Schema validation (correct columns and types)\n",
    "   - Business rules (High >= Low, positive prices)\n",
    "   - Time-series continuity (no missing dates)\n",
    "   - Statistical checks (outlier detection)\n",
    "\n",
    "6. **Automation**: Build pipelines with:\n",
    "   - Scheduled execution (cron or schedule library)\n",
    "   - Circuit breaker pattern (fail fast on repeated errors)\n",
    "   - Retry logic with exponential backoff\n",
    "   - Comprehensive logging for audit trails\n",
    "   - State persistence to resume interrupted jobs\n",
    "\n",
    "7. **Data Versioning**: Track dataset changes with:\n",
    "   - Version IDs and timestamps\n",
    "   - Data hashes for integrity verification\n",
    "   - Metadata (source, record count, date range)\n",
    "   - Ability to rollback to previous versions\n",
    "\n",
    "8. **Security**: Protect credentials and data:\n",
    "   - Use environment variables for API keys\n",
    "   - Never hardcode credentials\n",
    "   - Implement request signing when required\n",
    "   - Use HTTPS for all communications\n",
    "   - Validate SSL certificates\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "In Chapter 6, we will cover **Data Cleaning and Preprocessing**, including:\n",
    "- Advanced outlier detection and treatment\n",
    "- Missing value imputation strategies\n",
    "- Data normalization and transformation\n",
    "- Feature engineering basics\n",
    "- Handling non-stationary time-series\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 5**\n",
    "\n",
    "---\n",
    "\n",
    "*This chapter provided a comprehensive guide to building production-grade data collection systems. The patterns demonstrated\u2014circuit breakers, versioning, validation, and ethical scraping\u2014are essential for reliable time-series prediction systems. The NEPSE examples show how to apply these concepts to financial data specifically.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../1. foundations/4. data_fundamentals_and_programming_basics.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='6. data_cleaning_and_preprocessing.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}