{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 9: Data Pipelines and Automation**\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Design and implement batch processing pipelines for time-series data\n",
    "- Build streaming pipelines for real-time data ingestion\n",
    "- Orchestrate complex workflows using Apache Airflow\n",
    "- Implement data quality gates and validation checks\n",
    "- Monitor pipeline health and performance\n",
    "- Handle errors gracefully with retry logic and recovery mechanisms\n",
    "- Optimize pipelines for cost and scalability\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "- Completed Chapter 8: Data Storage and Management\n",
    "- Understanding of Python programming and decorators\n",
    "- Basic knowledge of Docker (helpful but not required)\n",
    "- Familiarity with cron jobs or task scheduling concepts\n",
    "\n",
    "---\n",
    "\n",
    "## **9.1 Pipeline Architecture Patterns**\n",
    "\n",
    "Data pipelines move data from source to destination while transforming it along the way. Understanding architectural patterns helps you build robust, maintainable systems.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Pipeline Architecture Patterns for Time-Series Data\n",
    "\n",
    "This module demonstrates common architectural patterns used in building\n",
    "data pipelines for the NEPSE stock prediction system.\n",
    "\n",
    "Patterns covered:\n",
    "1. ETL (Extract, Transform, Load)\n",
    "2. ELT (Extract, Load, Transform)\n",
    "3. Lambda Architecture (Batch + Speed layers)\n",
    "4. Kappa Architecture (Streaming only)\n",
    "5. Medallion Architecture (Bronze, Silver, Gold)\n",
    "\"\"\"\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "from typing import Dict, List, Any, Optional, Callable\n",
    "from dataclasses import dataclass\n",
    "from datetime import datetime, timedelta\n",
    "from enum import Enum\n",
    "import pandas as pd\n",
    "import logging\n",
    "\n",
    "# Configure logging for pipelines\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class PipelineStage(Enum):\n",
    "    \"\"\"Enumeration of pipeline stages.\"\"\"\n",
    "    EXTRACT = \"extract\"\n",
    "    TRANSFORM = \"transform\"\n",
    "    LOAD = \"load\"\n",
    "    VALIDATE = \"validate\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineContext:\n",
    "    \"\"\"\n",
    "    Context object passed between pipeline stages.\n",
    "    \n",
    "    This dataclass maintains state throughout the pipeline execution,\n",
    "    allowing stages to communicate and share metadata.\n",
    "    \"\"\"\n",
    "    execution_id: str\n",
    "    start_time: datetime\n",
    "    data: Optional[Any] = None\n",
    "    metadata: Dict[str, Any] = None\n",
    "    metrics: Dict[str, float] = None\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        if self.metadata is None:\n",
    "            self.metadata = {}\n",
    "        if self.metrics is None:\n",
    "            self.metrics = {}\n",
    "\n",
    "\n",
    "class Pipeline(ABC):\n",
    "    \"\"\"\n",
    "    Abstract base class for data pipelines.\n",
    "    \n",
    "    Defines the interface that all pipeline implementations must follow.\n",
    "    Uses the Template Method pattern to define the execution flow while\n",
    "    allowing subclasses to customize specific steps.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, name: str):\n",
    "        self.name = name\n",
    "        self.logger = logging.getLogger(f\"{__name__}.{name}\")\n",
    "    \n",
    "    @abstractmethod\n",
    "    def extract(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"Extract data from source.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def transform(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"Transform data.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    @abstractmethod\n",
    "    def load(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"Load data to destination.\"\"\"\n",
    "        pass\n",
    "    \n",
    "    def validate(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"\n",
    "        Optional validation step.\n",
    "        \n",
    "        Default implementation checks for empty data.\n",
    "        Subclasses can override for specific validation logic.\n",
    "        \"\"\"\n",
    "        if context.data is None or (isinstance(context.data, pd.DataFrame) and context.data.empty):\n",
    "            raise ValueError(\"Pipeline validation failed: No data to process\")\n",
    "        \n",
    "        self.logger.info(f\"Validation passed: {len(context.data)} records\")\n",
    "        return context\n",
    "    \n",
    "    def run(self, **kwargs) -> PipelineContext:\n",
    "        \"\"\"\n",
    "        Execute the pipeline.\n",
    "        \n",
    "        This template method defines the execution order and handles\n",
    "        error propagation and metrics collection.\n",
    "        \"\"\"\n",
    "        execution_id = f\"{self.name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        context = PipelineContext(\n",
    "            execution_id=execution_id,\n",
    "            start_time=datetime.now()\n",
    "        )\n",
    "        \n",
    "        self.logger.info(f\"Starting pipeline {self.name} [ID: {execution_id}]\")\n",
    "        \n",
    "        try:\n",
    "            # Execute stages in order\n",
    "            context = self.extract(context)\n",
    "            context = self.transform(context)\n",
    "            context = self.validate(context)\n",
    "            context = self.load(context)\n",
    "            \n",
    "            # Calculate duration\n",
    "            duration = (datetime.now() - context.start_time).total_seconds()\n",
    "            context.metrics['total_duration_seconds'] = duration\n",
    "            \n",
    "            self.logger.info(f\"Pipeline completed successfully in {duration:.2f}s\")\n",
    "            return context\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "class ETLPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    Traditional ETL (Extract, Transform, Load) Pipeline.\n",
    "    \n",
    "    Best for: Complex transformations that should happen before loading,\n",
    "    data cleansing, aggregations, feature engineering.\n",
    "    \n",
    "    Use case: NEPSE daily data processing - extract from API,\n",
    "    calculate technical indicators (transform), load to database.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 extractor: Callable,\n",
    "                 transformer: Callable,\n",
    "                 loader: Callable):\n",
    "        super().__init__(\"ETL_Pipeline\")\n",
    "        self.extractor = extractor\n",
    "        self.transformer = transformer\n",
    "        self.loader = loader\n",
    "    \n",
    "    def extract(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"Extract raw data.\"\"\"\n",
    "        self.logger.info(\"Extracting data...\")\n",
    "        context.data = self.extractor()\n",
    "        context.metadata['extracted_at'] = datetime.now()\n",
    "        return context\n",
    "    \n",
    "    def transform(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"Transform data before loading.\"\"\"\n",
    "        self.logger.info(\"Transforming data...\")\n",
    "        context.data = self.transformer(context.data)\n",
    "        context.metadata['transformed_at'] = datetime.now()\n",
    "        return context\n",
    "    \n",
    "    def load(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"Load transformed data.\"\"\"\n",
    "        self.logger.info(\"Loading data...\")\n",
    "        self.loader(context.data)\n",
    "        context.metadata['loaded_at'] = datetime.now()\n",
    "        return context\n",
    "\n",
    "\n",
    "class ELTPipeline(Pipeline):\n",
    "    \"\"\"\n",
    "    ELT (Extract, Load, Transform) Pipeline.\n",
    "    \n",
    "    Best for: When destination is a powerful data warehouse (BigQuery, Snowflake),\n",
    "    raw data preservation is important, transformations are SQL-based.\n",
    "    \n",
    "    Use case: Load raw NEPSE data to data warehouse, use SQL views for transformations.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self,\n",
    "                 extractor: Callable,\n",
    "                 loader: Callable,\n",
    "                 transform_query: str):\n",
    "        super().__init__(\"ELT_Pipeline\")\n",
    "        self.extractor = extractor\n",
    "        self.loader = loader\n",
    "        self.transform_query = transform_query\n",
    "    \n",
    "    def extract(self, context: PipelineContext) -> PipelineContext:\n",
    "        self.logger.info(\"Extracting data...\")\n",
    "        context.data = self.extractor()\n",
    "        return context\n",
    "    \n",
    "    def transform(self, context: PipelineContext) -> PipelineContext:\n",
    "        \"\"\"\n",
    "        In ELT, transformation happens after load.\n",
    "        \n",
    "        We might do minimal cleaning here, but heavy lifting is in DB.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Minimal pre-load transformation...\")\n",
    "        # Just basic type conversion or column renaming\n",
    "        if isinstance(context.data, pd.DataFrame):\n",
    "            context.data.columns = [c.lower().replace(' ', '_') for c in context.data.columns]\n",
    "        return context\n",
    "    \n",
    "    def load(self, context: PipelineContext) -> PipelineContext:\n",
    "        self.logger.info(\"Loading raw data...\")\n",
    "        self.loader(context.data, raw=True)\n",
    "        \n",
    "        # Execute transformation in database\n",
    "        self.logger.info(\"Executing DB transformations...\")\n",
    "        # Here you would execute self.transform_query against the DB\n",
    "        return context\n",
    "\n",
    "\n",
    "class LambdaArchitecture:\n",
    "    \"\"\"\n",
    "    Lambda Architecture combines batch and real-time (speed) layers.\n",
    "    \n",
    "    Structure:\n",
    "    - Batch Layer: Process all historical data (high latency, high accuracy)\n",
    "    - Speed Layer: Process recent data in real-time (low latency, approximate)\n",
    "    - Serving Layer: Merges batch and speed views\n",
    "    \n",
    "    Use case: NEPSE prediction system with daily batch training (batch layer)\n",
    "    and real-time price alerts (speed layer).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.batch_pipeline = None\n",
    "        self.speed_pipeline = None\n",
    "        self.logger = logging.getLogger(\"LambdaArchitecture\")\n",
    "    \n",
    "    def set_batch_pipeline(self, pipeline: Pipeline):\n",
    "        \"\"\"Set the batch processing pipeline.\"\"\"\n",
    "        self.batch_pipeline = pipeline\n",
    "    \n",
    "    def set_speed_pipeline(self, pipeline: Pipeline):\n",
    "        \"\"\"Set the speed (real-time) pipeline.\"\"\"\n",
    "        self.speed_pipeline = pipeline\n",
    "    \n",
    "    def run_batch(self, **kwargs) -> PipelineContext:\n",
    "        \"\"\"Execute batch layer (process all historical data).\"\"\"\n",
    "        self.logger.info(\"Running batch layer...\")\n",
    "        if not self.batch_pipeline:\n",
    "            raise ValueError(\"Batch pipeline not configured\")\n",
    "        return self.batch_pipeline.run(**kwargs)\n",
    "    \n",
    "    def run_speed(self, data: Any) -> PipelineContext:\n",
    "        \"\"\"Execute speed layer (process recent data).\"\"\"\n",
    "        self.logger.info(\"Running speed layer...\")\n",
    "        if not self.speed_pipeline:\n",
    "            raise ValueError(\"Speed pipeline not configured\")\n",
    "        \n",
    "        # Speed layer typically processes a small batch of recent data\n",
    "        context = PipelineContext(\n",
    "            execution_id=f\"speed_{datetime.now().strftime('%H%M%S')}\",\n",
    "            start_time=datetime.now(),\n",
    "            data=data\n",
    "        )\n",
    "        \n",
    "        # Skip extract, go straight to transform and load\n",
    "        context = self.speed_pipeline.transform(context)\n",
    "        context = self.speed_pipeline.load(context)\n",
    "        return context\n",
    "\n",
    "\n",
    "class MedallionArchitecture:\n",
    "    \"\"\"\n",
    "    Medallion Architecture organizes data in three layers:\n",
    "    \n",
    "    Bronze: Raw data as-ingested (immutable, append-only)\n",
    "    Silver: Cleaned, conformed data (deduplicated, schema enforced)\n",
    "    Gold: Aggregated, business-level data (features, aggregations)\n",
    "    \n",
    "    Use case: NEPSE data lakehouse:\n",
    "    - Bronze: Raw CSV files from NEPSE API\n",
    "    - Silver: Cleaned data with proper types, no duplicates\n",
    "    - Gold: Technical indicators, features for ML models\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = \"./medallion\"):\n",
    "        self.base_path = base_path\n",
    "        self.layers = ['bronze', 'silver', 'gold']\n",
    "        self.logger = logging.getLogger(\"MedallionArchitecture\")\n",
    "        \n",
    "        # Ensure directories exist\n",
    "        import os\n",
    "        for layer in self.layers:\n",
    "            os.makedirs(f\"{base_path}/{layer}\", exist_ok=True)\n",
    "    \n",
    "    def write_bronze(self, data: pd.DataFrame, filename: str):\n",
    "        \"\"\"\n",
    "        Write raw data to Bronze layer.\n",
    "        \n",
    "        Bronze characteristics:\n",
    "        - Immutable (never update, only append)\n",
    "        - Schema-on-read (flexible)\n",
    "        - Raw format (keep original files)\n",
    "        \"\"\"\n",
    "        path = f\"{self.base_path}/bronze/{filename}\"\n",
    "        data.to_parquet(path, index=False)\n",
    "        self.logger.info(f\"Wrote {len(data)} records to Bronze: {filename}\")\n",
    "    \n",
    "    def write_silver(self, data: pd.DataFrame, filename: str):\n",
    "        \"\"\"\n",
    "        Write cleaned data to Silver layer.\n",
    "        \n",
    "        Silver characteristics:\n",
    "        - Deduplicated\n",
    "        - Schema enforced\n",
    "        - Basic cleaning applied\n",
    "        \"\"\"\n",
    "        path = f\"{self.base_path}/silver/{filename}\"\n",
    "        \n",
    "        # Deduplication\n",
    "        if 'symbol' in data.columns and 'date' in data.columns:\n",
    "            data = data.drop_duplicates(subset=['symbol', 'date'], keep='last')\n",
    "        \n",
    "        # Schema enforcement\n",
    "        if 'volume' in data.columns:\n",
    "            data['volume'] = pd.to_numeric(data['volume'], errors='coerce')\n",
    "        \n",
    "        data.to_parquet(path, index=False)\n",
    "        self.logger.info(f\"Wrote {len(data)} records to Silver: {filename}\")\n",
    "    \n",
    "    def write_gold(self, data: pd.DataFrame, filename: str):\n",
    "        \"\"\"\n",
    "        Write business-level data to Gold layer.\n",
    "        \n",
    "        Gold characteristics:\n",
    "        - Aggregated\n",
    "        - Feature-engineered\n",
    "        - Optimized for querying\n",
    "        \"\"\"\n",
    "        path = f\"{self.base_path}/gold/{filename}\"\n",
    "        data.to_parquet(path, index=False)\n",
    "        self.logger.info(f\"Wrote {len(data)} records to Gold: {filename}\")\n",
    "    \n",
    "    def read_layer(self, layer: str, filename: str) -> pd.DataFrame:\n",
    "        \"\"\"Read data from specified layer.\"\"\"\n",
    "        path = f\"{self.base_path}/{layer}/{filename}\"\n",
    "        return pd.read_parquet(path)\n",
    "\n",
    "\n",
    "def demonstrate_architecture_patterns():\n",
    "    \"\"\"\n",
    "    Demonstrate different pipeline architecture patterns.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Pipeline Architecture Patterns\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Sample data for demonstration\n",
    "    sample_data = pd.DataFrame({\n",
    "        'symbol': ['NABIL', 'NICA', 'SCBL'],\n",
    "        'date': ['2024-01-15', '2024-01-15', '2024-01-15'],\n",
    "        'close': [865.0, 790.0, 530.0],\n",
    "        'volume': [125000, 98000, 76000]\n",
    "    })\n",
    "    \n",
    "    print(\"\\n1. ETL Pattern (Extract -> Transform -> Load)\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    # Define ETL functions\n",
    "    def extract():\n",
    "        print(\"  Extracting from NEPSE API...\")\n",
    "        return sample_data.copy()\n",
    "    \n",
    "    def transform(df):\n",
    "        print(\"  Calculating technical indicators...\")\n",
    "        df['sma_5'] = df['close'].rolling(window=5, min_periods=1).mean()\n",
    "        return df\n",
    "    \n",
    "    def load(df):\n",
    "        print(f\"  Loading {len(df)} records to database...\")\n",
    "    \n",
    "    etl = ETLPipeline(extract, transform, load)\n",
    "    etl.run()\n",
    "    \n",
    "    print(\"\\n2. Medallion Architecture (Bronze -> Silver -> Gold)\")\n",
    "    print(\"-\" * 50)\n",
    "    medallion = MedallionArchitecture(\"./nepse_medallion\")\n",
    "    \n",
    "    # Bronze: Raw data\n",
    "    medallion.write_bronze(sample_data, \"raw_20240115.parquet\")\n",
    "    \n",
    "    # Silver: Cleaned\n",
    "    cleaned_data = sample_data.copy()\n",
    "    cleaned_data['date'] = pd.to_datetime(cleaned_data['date'])\n",
    "    medallion.write_silver(cleaned_data, \"cleaned_20240115.parquet\")\n",
    "    \n",
    "    # Gold: Features\n",
    "    features = cleaned_data.copy()\n",
    "    features['price_momentum'] = features['close'].pct_change()\n",
    "    medallion.write_gold(features, \"features_20240115.parquet\")\n",
    "    \n",
    "    print(\"\\n3. Lambda Architecture (Batch + Speed layers)\")\n",
    "    print(\"-\" * 50)\n",
    "    lambda_arch = LambdaArchitecture()\n",
    "    lambda_arch.set_batch_pipeline(etl)\n",
    "    print(\"  Batch layer configured for daily processing\")\n",
    "    print(\"  Speed layer would handle real-time ticks\")\n",
    "    \n",
    "    return etl, medallion, lambda_arch\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_architecture_patterns()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Pipeline Context**: The `PipelineContext` dataclass acts as a carrier for data and metadata between stages. It maintains execution ID, timing metrics, and allows stages to communicate state. This pattern avoids global variables and makes pipelines testable.\n",
    "\n",
    "2. **ETL vs ELT**: \n",
    "   - **ETL** transforms data before loading, suitable when the destination has limited processing power or when you need to protect the database from dirty data.\n",
    "   - **ELT** loads raw data first then transforms in the database, leveraging the DB's processing power and preserving raw data for debugging.\n",
    "\n",
    "3. **Lambda Architecture**: Combines batch (accuracy) and speed (latency) layers. For NEPSE, the batch layer retrains models daily on full history, while the speed layer provides real-time alerts on price movements without waiting for the daily batch.\n",
    "\n",
    "4. **Medallion Architecture**: Organizes data quality into three zones:\n",
    "   - **Bronze**: Immutable raw data (source of truth)\n",
    "   - **Silver**: Cleaned, deduplicated, schema-validated\n",
    "   - **Gold**: Business-ready features and aggregations\n",
    "\n",
    "---\n",
    "\n",
    "## **9.2 Batch Processing Pipelines**\n",
    "\n",
    "Batch processing handles data in discrete chunks, typically scheduled to run at intervals (hourly, daily). This is the most common pattern for financial data like NEPSE.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Batch Processing Pipelines for NEPSE Data\n",
    "\n",
    "Batch processing is suitable for:\n",
    "- Daily stock data ingestion\n",
    "- End-of-day model training\n",
    "- Historical backtesting\n",
    "- Nightly report generation\n",
    "\n",
    "Components:\n",
    "1. Scheduler (cron, Airflow)\n",
    "2. Data extraction (API, files)\n",
    "3. Transformation (cleaning, feature engineering)\n",
    "4. Loading (database, file storage)\n",
    "5. Notification (success/failure alerts)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, List, Optional, Callable\n",
    "import logging\n",
    "import json\n",
    "from pathlib import Path\n",
    "import sqlite3\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class BatchPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready batch pipeline for NEPSE daily data processing.\n",
    "    \n",
    "    Features:\n",
    "    - Idempotency (running twice doesn't duplicate data)\n",
    "    - Checkpointing (resume from failure)\n",
    "    - Data validation\n",
    "    - Error handling with retries\n",
    "    - Audit logging\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 pipeline_id: str,\n",
    "                 db_connection: str = \"./nepse_pipeline.db\"):\n",
    "        self.pipeline_id = pipeline_id\n",
    "        self.db_connection = db_connection\n",
    "        self._init_checkpoint_db()\n",
    "        \n",
    "        # Processing statistics\n",
    "        self.stats = {\n",
    "            'records_extracted': 0,\n",
    "            'records_transformed': 0,\n",
    "            'records_loaded': 0,\n",
    "            'errors': []\n",
    "        }\n",
    "    \n",
    "    def _init_checkpoint_db(self):\n",
    "        \"\"\"Initialize SQLite database for checkpointing.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_connection)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS pipeline_runs (\n",
    "                run_id TEXT PRIMARY KEY,\n",
    "                pipeline_id TEXT,\n",
    "                status TEXT,\n",
    "                start_time TIMESTAMP,\n",
    "                end_time TIMESTAMP,\n",
    "                records_processed INTEGER,\n",
    "                checkpoint_data TEXT,\n",
    "                error_message TEXT\n",
    "            )\n",
    "        ''')\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def _save_checkpoint(self, run_id: str, status: str, checkpoint_data: Dict = None):\n",
    "        \"\"\"Save pipeline state for fault tolerance.\"\"\"\n",
    "        conn = sqlite3.connect(self.db_connection)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute('''\n",
    "            INSERT OR REPLACE INTO pipeline_runs \n",
    "            (run_id, pipeline_id, status, start_time, end_time, records_processed, checkpoint_data)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "        ''', (\n",
    "            run_id,\n",
    "            self.pipeline_id,\n",
    "            status,\n",
    "            datetime.now(),\n",
    "            datetime.now() if status in ['completed', 'failed'] else None,\n",
    "            self.stats['records_loaded'],\n",
    "            json.dumps(checkpoint_data) if checkpoint_data else None\n",
    "        ))\n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "    \n",
    "    def extract_daily_data(self, trade_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Extract NEPSE data for a specific trading date.\n",
    "        \n",
    "        In production, this would call the NEPSE API or scrape the website.\n",
    "        For demonstration, we generate synthetic data.\n",
    "        \"\"\"\n",
    "        logger.info(f\"Extracting data for {trade_date.date()}\")\n",
    "        \n",
    "        symbols = ['NABIL', 'NICA', 'SCBL', 'ADBL', 'EBL', 'GBIME', 'HBL']\n",
    "        data = []\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            # Simulate API call\n",
    "            base_price = np.random.uniform(200, 1000)\n",
    "            data.append({\n",
    "                'symbol': symbol,\n",
    "                'trade_date': trade_date.strftime('%Y-%m-%d'),\n",
    "                'open': round(base_price * np.random.uniform(0.98, 1.02), 2),\n",
    "                'high': round(base_price * np.random.uniform(1.01, 1.05), 2),\n",
    "                'low': round(base_price * np.random.uniform(0.95, 0.99), 2),\n",
    "                'close': round(base_price * np.random.uniform(0.98, 1.02), 2),\n",
    "                'volume': int(np.random.uniform(10000, 500000)),\n",
    "                'turnover': round(np.random.uniform(1000000, 50000000), 2)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        self.stats['records_extracted'] = len(df)\n",
    "        logger.info(f\"Extracted {len(df)} records\")\n",
    "        return df\n",
    "    \n",
    "    def validate_raw_data(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Validate raw data before processing.\n",
    "        \n",
    "        Checks:\n",
    "        - No missing symbols\n",
    "        - Price ranges are reasonable (0 < price < 100000)\n",
    "        - Volume is positive\n",
    "        - No duplicate symbols for same date\n",
    "        \"\"\"\n",
    "        validation_rules = [\n",
    "            (df['symbol'].notna().all(), \"Missing symbols detected\"),\n",
    "            ((df['close'] > 0).all(), \"Non-positive prices detected\"),\n",
    "            ((df['close'] < 100000).all(), \"Suspiciously high prices detected\"),\n",
    "            ((df['volume'] >= 0).all(), \"Negative volume detected\"),\n",
    "            (df.groupby('trade_date')['symbol'].nunique() == len(df), \"Duplicate symbols in date\")\n",
    "        ]\n",
    "        \n",
    "        for condition, message in validation_rules:\n",
    "            if not condition:\n",
    "                logger.error(f\"Validation failed: {message}\")\n",
    "                return False\n",
    "        \n",
    "        logger.info(\"Data validation passed\")\n",
    "        return True\n",
    "    \n",
    "    def transform_data(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform raw data into analysis-ready format.\n",
    "        \n",
    "        Transformations:\n",
    "        1. Data type conversion\n",
    "        2. Calculate daily returns\n",
    "        3. Calculate technical indicators (SMA, RSI)\n",
    "        4. Add metadata columns\n",
    "        \"\"\"\n",
    "        logger.info(\"Transforming data...\")\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Ensure correct types\n",
    "        df['trade_date'] = pd.to_datetime(df['trade_date'])\n",
    "        df['close'] = pd.to_numeric(df['close'], errors='coerce')\n",
    "        df['volume'] = pd.to_numeric(df['volume'], errors='coerce')\n",
    "        \n",
    "        # Sort by symbol and date for calculations\n",
    "        df = df.sort_values(['symbol', 'trade_date'])\n",
    "        \n",
    "        # Calculate daily returns (would need historical data in practice)\n",
    "        df['daily_return'] = df.groupby('symbol')['close'].pct_change()\n",
    "        \n",
    "        # Simple Moving Average (5-day)\n",
    "        df['sma_5'] = df.groupby('symbol')['close'].transform(\n",
    "            lambda x: x.rolling(window=5, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # Price range percentage\n",
    "        df['range_pct'] = ((df['high'] - df['low']) / df['low']) * 100\n",
    "        \n",
    "        # Add processing metadata\n",
    "        df['processed_at'] = datetime.now()\n",
    "        df['pipeline_version'] = '1.0.0'\n",
    "        \n",
    "        self.stats['records_transformed'] = len(df)\n",
    "        logger.info(f\"Transformation complete: {len(df)} records\")\n",
    "        return df\n",
    "    \n",
    "    def load_to_warehouse(self, df: pd.DataFrame, \n",
    "                         connection_string: str = \"sqlite:///nepse_warehouse.db\"):\n",
    "        \"\"\"\n",
    "        Load transformed data to data warehouse.\n",
    "        \n",
    "        Implements upsert logic (INSERT OR REPLACE) to ensure idempotency.\n",
    "        \"\"\"\n",
    "        logger.info(\"Loading to warehouse...\")\n",
    "        \n",
    "        # In production, use SQLAlchemy or dedicated connector\n",
    "        # Here we use SQLite for demonstration\n",
    "        conn = sqlite3.connect(connection_string.replace('sqlite:///', ''))\n",
    "        \n",
    "        # Create table if not exists\n",
    "        df.head(0).to_sql('stock_prices', conn, if_exists='append', index=False)\n",
    "        \n",
    "        # Upsert logic: Delete existing records for these symbols/dates\n",
    "        # then insert new ones (ensures idempotency)\n",
    "        symbols = df['symbol'].unique().tolist()\n",
    "        dates = df['trade_date'].dt.strftime('%Y-%m-%d').unique().tolist()\n",
    "        \n",
    "        placeholders_sym = ','.join(['?' for _ in symbols])\n",
    "        placeholders_date = ','.join(['?' for _ in dates])\n",
    "        \n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(f'''\n",
    "            DELETE FROM stock_prices \n",
    "            WHERE symbol IN ({placeholders_sym}) \n",
    "            AND trade_date IN ({placeholders_date})\n",
    "        ''', symbols + dates)\n",
    "        \n",
    "        # Insert new data\n",
    "        df.to_sql('stock_prices', conn, if_exists='append', index=False)\n",
    "        \n",
    "        conn.commit()\n",
    "        conn.close()\n",
    "        \n",
    "        self.stats['records_loaded'] = len(df)\n",
    "        logger.info(f\"Loaded {len(df)} records to warehouse\")\n",
    "    \n",
    "    def run(self, trade_date: Optional[datetime] = None) -> Dict:\n",
    "        \"\"\"\n",
    "        Execute the full batch pipeline.\n",
    "        \n",
    "        Args:\n",
    "            trade_date: Date to process (default: yesterday)\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with execution statistics\n",
    "        \"\"\"\n",
    "        if trade_date is None:\n",
    "            trade_date = datetime.now() - timedelta(days=1)\n",
    "        \n",
    "        run_id = f\"{self.pipeline_id}_{trade_date.strftime('%Y%m%d')}\"\n",
    "        \n",
    "        try:\n",
    "            # Check if already processed (idempotency)\n",
    "            conn = sqlite3.connect(self.db_connection)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute('''\n",
    "                SELECT status FROM pipeline_runs \n",
    "                WHERE run_id = ? AND status = 'completed'\n",
    "            ''', (run_id,))\n",
    "            \n",
    "            if cursor.fetchone():\n",
    "                logger.info(f\"Pipeline already completed for {run_id}, skipping\")\n",
    "                return {'status': 'skipped', 'run_id': run_id}\n",
    "            \n",
    "            conn.close()\n",
    "            \n",
    "            # Save checkpoint: started\n",
    "            self._save_checkpoint(run_id, 'running')\n",
    "            \n",
    "            # Execute pipeline stages\n",
    "            raw_data = self.extract_daily_data(trade_date)\n",
    "            \n",
    "            if not self.validate_raw_data(raw_data):\n",
    "                raise ValueError(\"Data validation failed\")\n",
    "            \n",
    "            transformed_data = self.transform_data(raw_data)\n",
    "            self.load_to_warehouse(transformed_data)\n",
    "            \n",
    "            # Save checkpoint: completed\n",
    "            self._save_checkpoint(run_id, 'completed', \n",
    "                                {'records': len(transformed_data)})\n",
    "            \n",
    "            return {\n",
    "                'status': 'success',\n",
    "                'run_id': run_id,\n",
    "                'stats': self.stats,\n",
    "                'trade_date': trade_date.strftime('%Y-%m-%d')\n",
    "            }\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            self._save_checkpoint(run_id, 'failed', {'error': str(e)})\n",
    "            raise\n",
    "\n",
    "\n",
    "class BatchScheduler:\n",
    "    \"\"\"\n",
    "    Simple scheduler for batch pipelines.\n",
    "    \n",
    "    In production, use Apache Airflow, Prefect, or cron.\n",
    "    This demonstrates the concepts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.jobs: Dict[str, Dict] = {}\n",
    "    \n",
    "    def add_job(self, \n",
    "                job_id: str, \n",
    "                pipeline: BatchPipeline,\n",
    "                schedule: str,  # 'daily', 'hourly', or cron-like\n",
    "                start_date: datetime):\n",
    "        \"\"\"\n",
    "        Add a job to the scheduler.\n",
    "        \n",
    "        Args:\n",
    "            job_id: Unique identifier\n",
    "            pipeline: Pipeline instance to run\n",
    "            schedule: Frequency ('daily', 'hourly')\n",
    "            start_date: When to start\n",
    "        \"\"\"\n",
    "        self.jobs[job_id] = {\n",
    "            'pipeline': pipeline,\n",
    "            'schedule': schedule,\n",
    "            'start_date': start_date,\n",
    "            'last_run': None,\n",
    "            'next_run': start_date\n",
    "        }\n",
    "        logger.info(f\"Scheduled job {job_id} with schedule {schedule}\")\n",
    "    \n",
    "    def should_run(self, job_id: str) -> bool:\n",
    "        \"\"\"Check if job should run now.\"\"\"\n",
    "        job = self.jobs.get(job_id)\n",
    "        if not job:\n",
    "            return False\n",
    "        \n",
    "        return datetime.now() >= job['next_run']\n",
    "    \n",
    "    def execute_job(self, job_id: str):\n",
    "        \"\"\"Execute a scheduled job.\"\"\"\n",
    "        job = self.jobs[job_id]\n",
    "        \n",
    "        logger.info(f\"Executing scheduled job: {job_id}\")\n",
    "        result = job['pipeline'].run()\n",
    "        \n",
    "        # Update schedule\n",
    "        job['last_run'] = datetime.now()\n",
    "        \n",
    "        if job['schedule'] == 'daily':\n",
    "            job['next_run'] = job['last_run'] + timedelta(days=1)\n",
    "        elif job['schedule'] == 'hourly':\n",
    "            job['next_run'] = job['last_run'] + timedelta(hours=1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "def demonstrate_batch_pipeline():\n",
    "    \"\"\"\n",
    "    Demonstrate batch processing pipeline.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Batch Processing Pipeline for NEPSE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize pipeline\n",
    "    pipeline = BatchPipeline(\n",
    "        pipeline_id=\"nepse_daily_ingestion\",\n",
    "        db_connection=\"./pipeline_meta.db\"\n",
    "    )\n",
    "    \n",
    "    # Run for specific date\n",
    "    result = pipeline.run(trade_date=datetime(2024, 1, 15))\n",
    "    \n",
    "    print(f\"\\nPipeline Result: {result['status']}\")\n",
    "    print(f\"Records processed: {result['stats']['records_loaded']}\")\n",
    "    \n",
    "    # Demonstrate scheduler\n",
    "    print(\"\\nScheduler Example:\")\n",
    "    scheduler = BatchScheduler()\n",
    "    scheduler.add_job(\n",
    "        job_id=\"daily_nepse\",\n",
    "        pipeline=pipeline,\n",
    "        schedule='daily',\n",
    "        start_date=datetime.now()\n",
    "    )\n",
    "    \n",
    "    print(f\"Job scheduled. Next run: {scheduler.jobs['daily_nepse']['next_run']}\")\n",
    "    \n",
    "    return pipeline, scheduler\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_batch_pipeline()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Idempotency**: The pipeline uses \"upsert\" logic (delete then insert) to ensure running twice doesn't create duplicates. This is critical for reliable scheduling.\n",
    "\n",
    "2. **Checkpointing**: The SQLite checkpoint database tracks pipeline runs. If a job fails, you can check the status and resume from the last successful stage.\n",
    "\n",
    "3. **Validation Gates**: Before transformation, data is validated for:\n",
    "   - Schema compliance (correct types)\n",
    "   - Business rules (positive prices, reasonable ranges)\n",
    "   - Uniqueness constraints (no duplicate symbols per date)\n",
    "\n",
    "4. **Error Handling**: Try-except blocks catch errors, log them, and save failure status to the checkpoint database for debugging.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.3 Stream Processing Pipelines**\n",
    "\n",
    "Stream processing handles data in real-time as it arrives, enabling immediate reactions to market events.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Stream Processing Pipelines for Real-Time NEPSE Data\n",
    "\n",
    "Stream processing is used for:\n",
    "- Real-time price alerts\n",
    "- Live trading signals\n",
    "- Immediate anomaly detection\n",
    "- Real-time dashboards\n",
    "\n",
    "Tools: Apache Kafka, Apache Flink, Redis Streams, AWS Kinesis\n",
    "\"\"\"\n",
    "\n",
    "import asyncio\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Callable, Optional\n",
    "from dataclasses import dataclass, asdict\n",
    "import threading\n",
    "import queue\n",
    "import time\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class StockTick:\n",
    "    \"\"\"\n",
    "    Represents a single stock tick (price update).\n",
    "    \n",
    "    In real NEPSE streaming, this would come from WebSocket\n",
    "    or streaming API.\n",
    "    \"\"\"\n",
    "    symbol: str\n",
    "    timestamp: datetime\n",
    "    price: float\n",
    "    volume: int\n",
    "    tick_type: str = 'trade'  # 'trade', 'bid', 'ask'\n",
    "    \n",
    "    def to_json(self) -> str:\n",
    "        return json.dumps({\n",
    "            'symbol': self.symbol,\n",
    "            'timestamp': self.timestamp.isoformat(),\n",
    "            'price': self.price,\n",
    "            'volume': self.volume,\n",
    "            'tick_type': self.tick_type\n",
    "        })\n",
    "\n",
    "\n",
    "class StreamProcessor:\n",
    "    \"\"\"\n",
    "    Real-time stream processor for NEPSE ticks.\n",
    "    \n",
    "    Architecture:\n",
    "    - Ingest: Receive ticks from source (simulated here)\n",
    "    - Process: Apply transformations/filters in real-time\n",
    "    - Sink: Output to alerts, database, or dashboard\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size_seconds: int = 60):\n",
    "        self.window_size = window_size_seconds\n",
    "        self.buffer: Dict[str, List[StockTick]] = {}\n",
    "        self.callbacks: List[Callable] = []\n",
    "        self.running = False\n",
    "        self.stats = {\n",
    "            'ticks_processed': 0,\n",
    "            'alerts_triggered': 0\n",
    "        }\n",
    "    \n",
    "    def register_callback(self, callback: Callable[[StockTick], None]):\n",
    "        \"\"\"\n",
    "        Register a callback function to process each tick.\n",
    "        \n",
    "        Callbacks can be:\n",
    "        - Alert generators\n",
    "        - Database writers\n",
    "        - Feature calculators\n",
    "        \"\"\"\n",
    "        self.callbacks.append(callback)\n",
    "    \n",
    "    def ingest_tick(self, tick: StockTick):\n",
    "        \"\"\"\n",
    "        Ingest a single tick into the stream.\n",
    "        \n",
    "        In production, this would be called by Kafka consumer\n",
    "        or WebSocket handler.\n",
    "        \"\"\"\n",
    "        # Add to buffer for windowed calculations\n",
    "        if tick.symbol not in self.buffer:\n",
    "            self.buffer[tick.symbol] = []\n",
    "        \n",
    "        self.buffer[tick.symbol].append(tick)\n",
    "        \n",
    "        # Clean old ticks outside window\n",
    "        cutoff = datetime.now() - timedelta(seconds=self.window_size)\n",
    "        self.buffer[tick.symbol] = [\n",
    "            t for t in self.buffer[tick.symbol] \n",
    "            if t.timestamp > cutoff\n",
    "        ]\n",
    "        \n",
    "        # Process tick through all callbacks\n",
    "        for callback in self.callbacks:\n",
    "            try:\n",
    "                callback(tick)\n",
    "            except Exception as e:\n",
    "                print(f\"Callback error: {e}\")\n",
    "        \n",
    "        self.stats['ticks_processed'] += 1\n",
    "    \n",
    "    def get_moving_average(self, symbol: str, seconds: int = 300) -> Optional[float]:\n",
    "        \"\"\"\n",
    "        Calculate moving average for a symbol over last N seconds.\n",
    "        \n",
    "        This is a stateful operation on the stream buffer.\n",
    "        \"\"\"\n",
    "        if symbol not in self.buffer:\n",
    "            return None\n",
    "        \n",
    "        cutoff = datetime.now() - timedelta(seconds=seconds)\n",
    "        recent_ticks = [t for t in self.buffer[symbol] if t.timestamp > cutoff]\n",
    "        \n",
    "        if not recent_ticks:\n",
    "            return None\n",
    "        \n",
    "        prices = [t.price for t in recent_ticks]\n",
    "        return sum(prices) / len(prices)\n",
    "    \n",
    "    def detect_anomaly(self, tick: StockTick) -> bool:\n",
    "        \"\"\"\n",
    "        Detect price anomalies (sudden spikes/drops).\n",
    "        \n",
    "        Returns True if price change > 5% from moving average.\n",
    "        \"\"\"\n",
    "        avg = self.get_moving_average(tick.symbol, seconds=300)\n",
    "        if avg is None:\n",
    "            return False\n",
    "        \n",
    "        change_pct = abs(tick.price - avg) / avg\n",
    "        return change_pct > 0.05  # 5% threshold\n",
    "\n",
    "\n",
    "class AlertManager:\n",
    "    \"\"\"\n",
    "    Manages real-time alerts based on stream processing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.alerts: List[Dict] = []\n",
    "        self.cooldowns: Dict[str, datetime] = {}\n",
    "    \n",
    "    def check_price_alert(self, tick: StockTick):\n",
    "        \"\"\"\n",
    "        Check if tick triggers any alerts.\n",
    "        \n",
    "        Implements cooldown to prevent spam (max 1 alert per 5 min per symbol).\n",
    "        \"\"\"\n",
    "        # Check cooldown\n",
    "        last_alert = self.cooldowns.get(tick.symbol)\n",
    "        if last_alert and (datetime.now() - last_alert).seconds < 300:\n",
    "            return\n",
    "        \n",
    "        # Check conditions\n",
    "        if tick.price > 1000:  # Price threshold\n",
    "            self._trigger_alert(tick, f\"Price above 1000: {tick.price}\")\n",
    "        \n",
    "        if tick.volume > 1000000:  # Volume spike\n",
    "            self._trigger_alert(tick, f\"Volume spike: {tick.volume}\")\n",
    "    \n",
    "    def _trigger_alert(self, tick: StockTick, message: str):\n",
    "        \"\"\"Record and send alert.\"\"\"\n",
    "        alert = {\n",
    "            'timestamp': datetime.now(),\n",
    "            'symbol': tick.symbol,\n",
    "            'message': message,\n",
    "            'severity': 'high'\n",
    "        }\n",
    "        self.alerts.append(alert)\n",
    "        self.cooldowns[tick.symbol] = datetime.now()\n",
    "        \n",
    "        # In production: send email, SMS, Slack notification\n",
    "        print(f\"\ud83d\udea8 ALERT [{tick.symbol}]: {message}\")\n",
    "\n",
    "\n",
    "class KafkaSimulator:\n",
    "    \"\"\"\n",
    "    Simulates Apache Kafka for demonstration purposes.\n",
    "    \n",
    "    In production, use kafka-python library:\n",
    "    from kafka import KafkaConsumer, KafkaProducer\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.topics: Dict[str, queue.Queue] = {}\n",
    "        self.consumers: List[threading.Thread] = []\n",
    "    \n",
    "    def create_topic(self, topic_name: str):\n",
    "        \"\"\"Create a topic (message queue).\"\"\"\n",
    "        self.topics[topic_name] = queue.Queue()\n",
    "    \n",
    "    def produce(self, topic: str, message: str):\n",
    "        \"\"\"Produce message to topic.\"\"\"\n",
    "        if topic in self.topics:\n",
    "            self.topics[topic].put(message)\n",
    "    \n",
    "    def consume(self, topic: str, processor: Callable[[str], None]):\n",
    "        \"\"\"\n",
    "        Consume messages from topic in background thread.\n",
    "        \"\"\"\n",
    "        def consumer_loop():\n",
    "            while True:\n",
    "                try:\n",
    "                    message = self.topics[topic].get(timeout=1)\n",
    "                    processor(message)\n",
    "                except queue.Empty:\n",
    "                    continue\n",
    "                except Exception as e:\n",
    "                    print(f\"Consumer error: {e}\")\n",
    "        \n",
    "        thread = threading.Thread(target=consumer_loop, daemon=True)\n",
    "        thread.start()\n",
    "        self.consumers.append(thread)\n",
    "\n",
    "\n",
    "def demonstrate_stream_processing():\n",
    "    \"\"\"\n",
    "    Demonstrate stream processing concepts.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Stream Processing Pipeline (Real-Time)\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize components\n",
    "    processor = StreamProcessor(window_size_seconds=60)\n",
    "    alert_manager = AlertManager()\n",
    "    \n",
    "    # Register callbacks\n",
    "    processor.register_callback(alert_manager.check_price_alert)\n",
    "    processor.register_callback(\n",
    "        lambda tick: print(f\"Processed: {tick.symbol} @ {tick.price}\")\n",
    "    )\n",
    "    \n",
    "    # Simulate incoming ticks\n",
    "    print(\"\\nSimulating real-time ticks...\")\n",
    "    symbols = ['NABIL', 'NICA']\n",
    "    \n",
    "    for i in range(20):\n",
    "        symbol = symbols[i % 2]\n",
    "        \n",
    "        # Simulate price spike for NABIL on 10th tick\n",
    "        if i == 10 and symbol == 'NABIL':\n",
    "            price = 1050  # Anomaly\n",
    "        else:\n",
    "            price = 850 + (i % 10)\n",
    "        \n",
    "        tick = StockTick(\n",
    "            symbol=symbol,\n",
    "            timestamp=datetime.now(),\n",
    "            price=price,\n",
    "            volume=50000 + (i * 1000)\n",
    "        )\n",
    "        \n",
    "        processor.ingest_tick(tick)\n",
    "        \n",
    "        if processor.detect_anomaly(tick):\n",
    "            print(f\"\u26a0\ufe0f  Anomaly detected in {symbol}!\")\n",
    "        \n",
    "        time.sleep(0.1)  # Simulate real-time delay\n",
    "    \n",
    "    print(f\"\\nStream Stats:\")\n",
    "    print(f\"  Ticks processed: {processor.stats['ticks_processed']}\")\n",
    "    print(f\"  Alerts triggered: {len(alert_manager.alerts)}\")\n",
    "    \n",
    "    return processor, alert_manager\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_stream_processing()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Stream Buffer**: Maintains a sliding window of recent ticks per symbol in memory. This enables stateful operations like moving averages without querying a database.\n",
    "\n",
    "2. **Callback Pattern**: Each tick is passed through a chain of callback functions (alert checker, feature calculator, database writer). This is the Chain of Responsibility pattern.\n",
    "\n",
    "3. **Anomaly Detection**: Real-time detection compares current price against 5-minute moving average. If deviation > 5%, trigger alert.\n",
    "\n",
    "4. **Cooldown Mechanism**: Prevents alert spam by limiting alerts to one per 5 minutes per symbol, even if conditions remain triggered.\n",
    "\n",
    "---\n",
    "\n",
    "## **9.4 Pipeline Orchestration**\n",
    "\n",
    "Orchestration tools manage complex workflows with dependencies, scheduling, and monitoring.\n",
    "\n",
    "### **9.4.1 Apache Airflow**\n",
    "\n",
    "Apache Airflow is the industry standard for workflow orchestration, using Python to define DAGs (Directed Acyclic Graphs).\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Apache Airflow Integration for NEPSE Pipelines\n",
    "\n",
    "Airflow concepts:\n",
    "- DAG: Directed Acyclic Graph (the workflow definition)\n",
    "- Task: A unit of work (Python function, SQL query, etc.)\n",
    "- Operator: Defines what a task does (PythonOperator, BashOperator, etc.)\n",
    "- Sensor: Waits for external event (file, API, etc.)\n",
    "- XCom: Cross-communication (share data between tasks)\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any\n",
    "\n",
    "# Note: In production, these imports come from airflow\n",
    "# from airflow import DAG\n",
    "# from airflow.operators.python import PythonOperator\n",
    "# from airflow.sensors.filesystem import FileSensor\n",
    "\n",
    "\n",
    "class MockDAG:\n",
    "    \"\"\"\n",
    "    Mock DAG class to demonstrate Airflow concepts without installation.\n",
    "    \n",
    "    In production:\n",
    "    from airflow import DAG\n",
    "    \n",
    "    dag = DAG(\n",
    "        'nepse_daily_pipeline',\n",
    "        default_args=default_args,\n",
    "        description='NEPSE daily data pipeline',\n",
    "        schedule_interval=timedelta(days=1),\n",
    "        start_date=datetime(2024, 1, 1),\n",
    "        catchup=False\n",
    "    )\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, dag_id: str, schedule_interval: timedelta, start_date: datetime):\n",
    "        self.dag_id = dag_id\n",
    "        self.schedule_interval = schedule_interval\n",
    "        self.start_date = start_date\n",
    "        self.tasks: Dict[str, Any] = {}\n",
    "        self.dependencies: Dict[str, List[str]] = {}\n",
    "    \n",
    "    def add_task(self, task_id: str, python_callable: callable, **kwargs):\n",
    "        \"\"\"Add a task to the DAG.\"\"\"\n",
    "        self.tasks[task_id] = {\n",
    "            'callable': python_callable,\n",
    "            'kwargs': kwargs\n",
    "        }\n",
    "        return self\n",
    "    \n",
    "    def set_dependency(self, upstream: str, downstream: str):\n",
    "        \"\"\"Set task dependency: upstream >> downstream\"\"\"\n",
    "        if downstream not in self.dependencies:\n",
    "            self.dependencies[downstream] = []\n",
    "        self.dependencies[downstream].append(upstream)\n",
    "    \n",
    "    def run(self):\n",
    "        \"\"\"Execute the DAG (simplified simulation).\"\"\"\n",
    "        print(f\"Running DAG: {self.dag_id}\")\n",
    "        \n",
    "        # Topological sort to determine execution order\n",
    "        executed = set()\n",
    "        pending = set(self.tasks.keys())\n",
    "        \n",
    "        while pending:\n",
    "            ready = {\n",
    "                task for task in pending \n",
    "                if all(dep in executed for dep in self.dependencies.get(task, []))\n",
    "            }\n",
    "            \n",
    "            if not ready:\n",
    "                raise ValueError(\"Circular dependency detected\")\n",
    "            \n",
    "            for task_id in ready:\n",
    "                print(f\"Executing task: {task_id}\")\n",
    "                task = self.tasks[task_id]\n",
    "                task['callable'](**task['kwargs'])\n",
    "                executed.add(task_id)\n",
    "                pending.remove(task_id)\n",
    "        \n",
    "        print(\"DAG completed successfully\")\n",
    "\n",
    "\n",
    "def create_nepse_dag():\n",
    "    \"\"\"\n",
    "    Create a production-grade NEPSE pipeline DAG.\n",
    "    \n",
    "    This demonstrates the structure of a real Airflow DAG.\n",
    "    \"\"\"\n",
    "    # Default arguments for all tasks\n",
    "    default_args = {\n",
    "        'owner': 'nepse-data-team',\n",
    "        'depends_on_past': False,  # Don't wait for previous run\n",
    "        'email': ['alerts@nepse-system.com'],\n",
    "        'email_on_failure': True,\n",
    "        'email_on_retry': False,\n",
    "        'retries': 3,\n",
    "        'retry_delay': timedelta(minutes=5),\n",
    "    }\n",
    "    \n",
    "    # Create DAG\n",
    "    dag = MockDAG(\n",
    "        dag_id='nepse_daily_etl',\n",
    "        schedule_interval=timedelta(days=1),  # Daily at midnight\n",
    "        start_date=datetime(2024, 1, 1)\n",
    "    )\n",
    "    \n",
    "    # Task 1: Check if source data is available (Sensor)\n",
    "    def check_source_data():\n",
    "        \"\"\"Wait for NEPSE API to publish daily data.\"\"\"\n",
    "        print(\"Checking NEPSE API for daily data...\")\n",
    "        # In production: Check if file exists or API endpoint returns 200\n",
    "        return True\n",
    "    \n",
    "    # Task 2: Extract raw data\n",
    "    def extract_task():\n",
    "        \"\"\"Extract data from NEPSE API.\"\"\"\n",
    "        print(\"Extracting from NEPSE API...\")\n",
    "        # Return data via XCom in real Airflow\n",
    "        return {'date': '2024-01-15', 'records': 200}\n",
    "    \n",
    "    # Task 3: Validate data quality\n",
    "    def validate_task():\n",
    "        \"\"\"Run data quality checks.\"\"\"\n",
    "        print(\"Validating data quality...\")\n",
    "        checks = {\n",
    "            'row_count': 200,\n",
    "            'null_percentage': 0.0,\n",
    "            'price_range_valid': True\n",
    "        }\n",
    "        \n",
    "        if checks['null_percentage'] > 0.05:\n",
    "            raise ValueError(\"Too many null values!\")\n",
    "        \n",
    "        return checks\n",
    "    \n",
    "    # Task 4: Transform data\n",
    "    def transform_task():\n",
    "        \"\"\"Calculate features and indicators.\"\"\"\n",
    "        print(\"Transforming data...\")\n",
    "        # Read from XCom, process, write back\n",
    "        return {'features_calculated': 15}\n",
    "    \n",
    "    # Task 5: Load to warehouse\n",
    "    def load_task():\n",
    "        \"\"\"Load to data warehouse.\"\"\"\n",
    "        print(\"Loading to warehouse...\")\n",
    "        return {'records_loaded': 200}\n",
    "    \n",
    "    # Task 6: Generate report\n",
    "    def report_task():\n",
    "        \"\"\"Send success notification.\"\"\"\n",
    "        print(\"Sending success email...\")\n",
    "    \n",
    "    # Add tasks to DAG\n",
    "    dag.add_task('check_source', check_source_data)\n",
    "    dag.add_task('extract', extract_task)\n",
    "    dag.add_task('validate', validate_task)\n",
    "    dag.add_task('transform', transform_task)\n",
    "    dag.add_task('load', load_task)\n",
    "    dag.add_task('report', report_task)\n",
    "    \n",
    "    # Define dependencies (execution order)\n",
    "    # check_source >> extract >> validate >> transform >> load >> report\n",
    "    dag.set_dependency('check_source', 'extract')\n",
    "    dag.set_dependency('extract', 'validate')\n",
    "    dag.set_dependency('validate', 'transform')\n",
    "    dag.set_dependency('transform', 'load')\n",
    "    dag.set_dependency('load', 'report')\n",
    "    \n",
    "    return dag\n",
    "\n",
    "\n",
    "# Production Airflow DAG file (would be saved as nepse_dag.py):\n",
    "\"\"\"\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from airflow.providers.postgres.operators.postgres import PostgresOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'airflow',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'email_on_failure': False,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'nepse_daily_pipeline',\n",
    "    default_args=default_args,\n",
    "    description='NEPSE daily ETL pipeline',\n",
    "    schedule_interval='0 18 * * 1-5',  # 6 PM, Mon-Fri (after market close)\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "# Extract task\n",
    "def extract_nepse_data(**context):\n",
    "    from nepse_api import get_daily_data\n",
    "    data = get_daily_data()\n",
    "    # Push to XCom for downstream tasks\n",
    "    context['task_instance'].xcom_push(key='raw_data', value=data)\n",
    "\n",
    "extract = PythonOperator(\n",
    "    task_id='extract',\n",
    "    python_callable=extract_nepse_data,\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Transform task\n",
    "transform = PythonOperator(\n",
    "    task_id='transform',\n",
    "    python_callable=lambda **c: transform_data(c['task_instance'].xcom_pull(task_ids='extract')),\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Load task using PostgresOperator\n",
    "load = PostgresOperator(\n",
    "    task_id='load',\n",
    "    postgres_conn_id='nepse_warehouse',\n",
    "    sql=\"\"\"\n",
    "        INSERT INTO stock_prices (date, symbol, close, volume)\n",
    "        VALUES (%(date)s, %(symbol)s, %(close)s, %(volume)s)\n",
    "        ON CONFLICT (date, symbol) DO UPDATE SET\n",
    "            close = EXCLUDED.close,\n",
    "            volume = EXCLUDED.volume;\n",
    "    \"\"\",\n",
    "    dag=dag,\n",
    ")\n",
    "\n",
    "# Set dependencies\n",
    "extract >> transform >> load\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "def demonstrate_airflow():\n",
    "    \"\"\"\n",
    "    Demonstrate Airflow DAG concepts.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Apache Airflow Orchestration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    dag = create_nepse_dag()\n",
    "    dag.run()\n",
    "    \n",
    "    print(\"\\nKey Airflow Features for NEPSE:\")\n",
    "    print(\"  - Schedule: Daily at 6 PM (after market close)\")\n",
    "    print(\"  - Retries: 3 attempts with 5-min delays\")\n",
    "    print(\"  - Dependencies: Linear pipeline with validation gates\")\n",
    "    print(\"  - XCom: Pass data between tasks\")\n",
    "    print(\"  - Sensors: Wait for API availability\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_airflow()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **DAG Structure**: The workflow is a Directed Acyclic Graph - tasks flow in one direction with no loops. This ensures predictable execution.\n",
    "\n",
    "2. **Operators**: \n",
    "   - **PythonOperator**: Run Python functions\n",
    "   - **PostgresOperator**: Execute SQL\n",
    "   - **BashOperator**: Run shell commands\n",
    "   - **Sensor**: Wait for external condition (file, API)\n",
    "\n",
    "3. **XCom (Cross-Communication)**: Tasks pass data via XCom. Task A pushes data, Task B pulls it. For large data, use intermediate storage (S3, database) instead.\n",
    "\n",
    "4. **Scheduling**: Cron expressions (`0 18 * * 1-5`) define when to run: 6 PM, Monday through Friday, after NEPSE market close.\n",
    "\n",
    "### **9.4.2 Prefect and Dagster (Modern Alternatives)**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Modern Orchestration Tools: Prefect and Dagster\n",
    "\n",
    "Prefect: Simpler than Airflow, better error handling, dynamic workflows\n",
    "Dagster: Asset-centric, emphasizes data quality and testing\n",
    "\"\"\"\n",
    "\n",
    "# Prefect Example (conceptual):\n",
    "\"\"\"\n",
    "from prefect import flow, task\n",
    "from prefect.tasks import task_input_hash\n",
    "import requests\n",
    "\n",
    "@task(cache_key_fn=task_input_hash)  # Cache results\n",
    "def fetch_nepse_data(date: str):\n",
    "    response = requests.get(f\"https://nepse-api.com/prices?date={date}\")\n",
    "    return response.json()\n",
    "\n",
    "@task\n",
    "def calculate_features(data: dict):\n",
    "    # Feature engineering\n",
    "    return processed_data\n",
    "\n",
    "@task\n",
    "def save_to_db(data: dict):\n",
    "    # Database insertion\n",
    "    pass\n",
    "\n",
    "@flow(name=\"nepse_daily_flow\")\n",
    "def nepse_pipeline(date: str = None):\n",
    "    if date is None:\n",
    "        date = datetime.now().strftime(\"%Y-%m-%d\")\n",
    "    \n",
    "    raw_data = fetch_nepse_data(date)\n",
    "    features = calculate_features(raw_data)\n",
    "    save_to_db(features)\n",
    "\n",
    "# Run: nepse_pipeline(\"2024-01-15\")\n",
    "\"\"\"\n",
    "\n",
    "# Dagster Example (conceptual):\n",
    "\"\"\"\n",
    "from dagster import asset, Definitions\n",
    "import pandas as pd\n",
    "\n",
    "@asset  # An asset is a data object (table, file, ML model)\n",
    "def nepse_raw_data():\n",
    "    # Asset 1: Raw data from API\n",
    "    return fetch_from_api()\n",
    "\n",
    "@asset\n",
    "def nepse_cleaned(nepse_raw_data):  # Dependency injection\n",
    "    # Asset 2: Cleaned data (depends on raw)\n",
    "    return clean_data(nepse_raw_data)\n",
    "\n",
    "@asset\n",
    "def nepse_features(nepse_cleaned):\n",
    "    # Asset 3: Feature engineered data\n",
    "    return calculate_features(nepse_cleaned)\n",
    "\n",
    "defs = Definitions(assets=[nepse_raw_data, nepse_cleaned, nepse_features])\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.5 Data Quality Gates**\n",
    "\n",
    "Data quality gates prevent bad data from polluting downstream systems.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Data Quality Gates and Validation\n",
    "\n",
    "Implement checks at multiple stages:\n",
    "1. Ingestion: Schema validation, completeness\n",
    "2. Transformation: Business rule validation\n",
    "3. Loading: Referential integrity, uniqueness\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from typing import List, Dict, Callable, Any\n",
    "from dataclasses import dataclass\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class ValidationResult:\n",
    "    \"\"\"Result of a data quality check.\"\"\"\n",
    "    check_name: str\n",
    "    passed: bool\n",
    "    details: Dict[str, Any]\n",
    "    severity: str  # 'error', 'warning'\n",
    "\n",
    "\n",
    "class DataQualitySuite:\n",
    "    \"\"\"\n",
    "    Comprehensive data quality validation for NEPSE data.\n",
    "    \n",
    "    Implements the Great Expectations pattern (without the library\n",
    "    for simplicity, but consider using great_expectations in production).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.checks: List[Callable] = []\n",
    "        self.results: List[ValidationResult] = []\n",
    "    \n",
    "    def add_check(self, check_fn: Callable):\n",
    "        \"\"\"Add a validation check.\"\"\"\n",
    "        self.checks.append(check_fn)\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"\n",
    "        Run all validation checks.\n",
    "        \n",
    "        Returns:\n",
    "            True if all critical checks pass, False otherwise\n",
    "        \"\"\"\n",
    "        self.results = []\n",
    "        \n",
    "        for check in self.checks:\n",
    "            try:\n",
    "                result = check(df)\n",
    "                self.results.append(result)\n",
    "                \n",
    "                if not result.passed and result.severity == 'error':\n",
    "                    logger.error(f\"Validation failed: {result.check_name}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                self.results.append(ValidationResult(\n",
    "                    check_name=check.__name__,\n",
    "                    passed=False,\n",
    "                    details={'error': str(e)},\n",
    "                    severity='error'\n",
    "                ))\n",
    "        \n",
    "        # Return True only if no errors\n",
    "        critical_failures = [r for r in self.results \n",
    "                           if not r.passed and r.severity == 'error']\n",
    "        \n",
    "        return len(critical_failures) == 0\n",
    "    \n",
    "    def get_report(self) -> Dict:\n",
    "        \"\"\"Generate validation report.\"\"\"\n",
    "        return {\n",
    "            'total_checks': len(self.results),\n",
    "            'passed': sum(1 for r in self.results if r.passed),\n",
    "            'failed': sum(1 for r in self.results if not r.passed),\n",
    "            'details': [\n",
    "                {\n",
    "                    'check': r.check_name,\n",
    "                    'status': 'PASS' if r.passed else 'FAIL',\n",
    "                    'severity': r.severity,\n",
    "                    'details': r.details\n",
    "                }\n",
    "                for r in self.results\n",
    "            ]\n",
    "        }\n",
    "\n",
    "\n",
    "# Pre-built validation checks for NEPSE data\n",
    "\n",
    "def check_no_missing_symbols(df: pd.DataFrame) -> ValidationResult:\n",
    "    \"\"\"Ensure all records have stock symbols.\"\"\"\n",
    "    missing = df['symbol'].isna().sum()\n",
    "    return ValidationResult(\n",
    "        check_name='no_missing_symbols',\n",
    "        passed=missing == 0,\n",
    "        details={'missing_count': missing},\n",
    "        severity='error'\n",
    "    )\n",
    "\n",
    "\n",
    "def check_price_positive(df: pd.DataFrame) -> ValidationResult:\n",
    "    \"\"\"Ensure all prices are positive.\"\"\"\n",
    "    invalid = (df['close'] <= 0).sum()\n",
    "    return ValidationResult(\n",
    "        check_name='price_positive',\n",
    "        passed=invalid == 0,\n",
    "        details={'invalid_prices': invalid},\n",
    "        severity='error'\n",
    "    )\n",
    "\n",
    "\n",
    "def check_volume_reasonable(df: pd.DataFrame) -> ValidationResult:\n",
    "    \"\"\"Check for suspicious volume (0 or extremely high).\"\"\"\n",
    "    zero_volume = (df['volume'] == 0).sum()\n",
    "    high_volume = (df['volume'] > 10_000_000).sum()\n",
    "    \n",
    "    return ValidationResult(\n",
    "        check_name='volume_reasonable',\n",
    "        passed=zero_volume == 0,  # Zero volume is error\n",
    "        details={\n",
    "            'zero_volume_rows': zero_volume,\n",
    "            'suspiciously_high': high_volume\n",
    "        },\n",
    "        severity='warning' if high_volume > 0 else 'error'\n",
    "    )\n",
    "\n",
    "\n",
    "def check_no_duplicates(df: pd.DataFrame) -> ValidationResult:\n",
    "    \"\"\"Check for duplicate symbol-date combinations.\"\"\"\n",
    "    dups = df.duplicated(subset=['symbol', 'date']).sum()\n",
    "    return ValidationResult(\n",
    "        check_name='no_duplicates',\n",
    "        passed=dups == 0,\n",
    "        details={'duplicate_rows': dups},\n",
    "        severity='error'\n",
    "    )\n",
    "\n",
    "\n",
    "def check_price_range_consistency(df: pd.DataFrame) -> ValidationResult:\n",
    "    \"\"\"Ensure high >= low, close within range.\"\"\"\n",
    "    invalid_high_low = (df['high'] < df['low']).sum()\n",
    "    invalid_close = ((df['close'] > df['high']) | (df['close'] < df['low'])).sum()\n",
    "    \n",
    "    return ValidationResult(\n",
    "        check_name='price_range_consistency',\n",
    "        passed=invalid_high_low == 0 and invalid_close == 0,\n",
    "        details={\n",
    "            'invalid_high_low': invalid_high_low,\n",
    "            'close_out_of_range': invalid_close\n",
    "        },\n",
    "        severity='error'\n",
    "    )\n",
    "\n",
    "\n",
    "def demonstrate_data_quality():\n",
    "    \"\"\"\n",
    "    Demonstrate data quality gates.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Data Quality Gates\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create test data with some issues\n",
    "    test_data = pd.DataFrame({\n",
    "        'symbol': ['NABIL', 'NICA', 'SCBL', None, 'NABIL'],  # Duplicate NABIL, None symbol\n",
    "        'date': ['2024-01-15'] * 5,\n",
    "        'open': [850.0, 780.0, 520.0, 340.0, 860.0],\n",
    "        'high': [870.0, 795.0, 535.0, 350.0, 875.0],\n",
    "        'low': [845.0, 775.0, 515.0, 335.0, 840.0],\n",
    "        'close': [865.0, 790.0, 530.0, -10.0, 865.0],  # Negative close\n",
    "        'volume': [125000, 0, 76000, 145000, 130000]  # Zero volume for NICA\n",
    "    })\n",
    "    \n",
    "    print(\"\\nTest Data Issues:\")\n",
    "    print(\"  - Row 3: Missing symbol, negative price\")\n",
    "    print(\"  - Row 4: Duplicate symbol (NABIL)\")\n",
    "    print(\"  - Row 1: Zero volume\")\n",
    "    \n",
    "    # Setup validation suite\n",
    "    suite = DataQualitySuite()\n",
    "    suite.add_check(check_no_missing_symbols)\n",
    "    suite.add_check(check_price_positive)\n",
    "    suite.add_check(check_volume_reasonable)\n",
    "    suite.add_check(check_no_duplicates)\n",
    "    suite.add_check(check_price_range_consistency)\n",
    "    \n",
    "    # Run validation\n",
    "    is_valid = suite.validate(test_data)\n",
    "    report = suite.get_report()\n",
    "    \n",
    "    print(f\"\\nValidation Result: {'PASS' if is_valid else 'FAIL'}\")\n",
    "    print(f\"Checks: {report['passed']}/{report['total_checks']} passed\")\n",
    "    \n",
    "    print(\"\\nDetailed Report:\")\n",
    "    for detail in report['details']:\n",
    "        status_icon = \"\u2713\" if detail['status'] == 'PASS' else \"\u2717\"\n",
    "        print(f\"  {status_icon} {detail['check']} ({detail['severity']})\")\n",
    "        if detail['details']:\n",
    "            print(f\"      Details: {detail['details']}\")\n",
    "    \n",
    "    return suite\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_data_quality()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Validation Suite**: Composable pattern where checks can be added/removed. Each check returns a `ValidationResult` with severity levels.\n",
    "\n",
    "2. **Severity Levels**:\n",
    "   - **Error**: Stop the pipeline (bad data)\n",
    "   - **Warning**: Log but continue (suspicious but possibly valid data)\n",
    "\n",
    "3. **Business Rules**: Checks enforce domain knowledge:\n",
    "   - Prices must be positive (stocks can't have negative price)\n",
    "   - High must be >= Low (definition of high/low)\n",
    "   - No duplicates (one record per symbol per day)\n",
    "\n",
    "---\n",
    "\n",
    "## **9.6 Pipeline Monitoring**\n",
    "\n",
    "Monitoring tracks pipeline health, performance, and data quality over time.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Pipeline Monitoring and Observability\n",
    "\n",
    "Key metrics to track:\n",
    "1. Operational: Runtime, success/failure rates, latency\n",
    "2. Data Quality: Row counts, null rates, distribution drift\n",
    "3. Business: Records processed, SLAs met\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "from dataclasses import dataclass, field\n",
    "import json\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PipelineMetrics:\n",
    "    \"\"\"Metrics collected during pipeline execution.\"\"\"\n",
    "    pipeline_id: str\n",
    "    run_id: str\n",
    "    start_time: datetime\n",
    "    end_time: datetime = None\n",
    "    status: str = \"running\"  # running, success, failed\n",
    "    records_processed: int = 0\n",
    "    duration_seconds: float = 0.0\n",
    "    custom_metrics: Dict[str, Any] = field(default_factory=dict)\n",
    "\n",
    "\n",
    "class PipelineMonitor:\n",
    "    \"\"\"\n",
    "    Monitor for tracking pipeline performance and health.\n",
    "    \n",
    "    In production, send metrics to:\n",
    "    - Prometheus (metrics)\n",
    "    - Grafana (visualization)\n",
    "    - ELK Stack (logs)\n",
    "    - PagerDuty (alerts)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, pipeline_id: str):\n",
    "        self.pipeline_id = pipeline_id\n",
    "        self.metrics_history: List[PipelineMetrics] = []\n",
    "        self.current_run: PipelineMetrics = None\n",
    "    \n",
    "    def start_run(self) -> str:\n",
    "        \"\"\"Start monitoring a new pipeline run.\"\"\"\n",
    "        run_id = f\"{self.pipeline_id}_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        self.current_run = PipelineMetrics(\n",
    "            pipeline_id=self.pipeline_id,\n",
    "            run_id=run_id,\n",
    "            start_time=datetime.now()\n",
    "        )\n",
    "        return run_id\n",
    "    \n",
    "    def record_metric(self, name: str, value: Any):\n",
    "        \"\"\"Record a custom metric.\"\"\"\n",
    "        if self.current_run:\n",
    "            self.current_run.custom_metrics[name] = value\n",
    "    \n",
    "    def end_run(self, status: str = \"success\"):\n",
    "        \"\"\"End current run and save metrics.\"\"\"\n",
    "        if not self.current_run:\n",
    "            return\n",
    "        \n",
    "        self.current_run.end_time = datetime.now()\n",
    "        self.current_run.status = status\n",
    "        self.current_run.duration_seconds = (\n",
    "            self.current_run.end_time - self.current_run.start_time\n",
    "        ).total_seconds()\n",
    "        \n",
    "        self.metrics_history.append(self.current_run)\n",
    "        \n",
    "        # Alert if failed\n",
    "        if status == \"failed\":\n",
    "            self._send_alert(self.current_run)\n",
    "    \n",
    "    def _send_alert(self, metrics: PipelineMetrics):\n",
    "        \"\"\"Send failure alert (simulated).\"\"\"\n",
    "        print(f\"\ud83d\udea8 ALERT: Pipeline {metrics.pipeline_id} failed!\")\n",
    "        print(f\"   Run ID: {metrics.run_id}\")\n",
    "        print(f\"   Duration: {metrics.duration_seconds}s\")\n",
    "    \n",
    "    def get_stats(self) -> Dict:\n",
    "        \"\"\"Calculate statistics over run history.\"\"\"\n",
    "        if not self.metrics_history:\n",
    "            return {}\n",
    "        \n",
    "        runs = self.metrics_history\n",
    "        success_runs = [r for r in runs if r.status == \"success\"]\n",
    "        \n",
    "        return {\n",
    "            'total_runs': len(runs),\n",
    "            'success_rate': len(success_runs) / len(runs),\n",
    "            'avg_duration': sum(r.duration_seconds for r in runs) / len(runs),\n",
    "            'avg_records': sum(r.records_processed for r in runs) / len(runs),\n",
    "            'last_run_status': runs[-1].status if runs else None\n",
    "        }\n",
    "\n",
    "\n",
    "class DataDriftMonitor:\n",
    "    \"\"\"\n",
    "    Monitor for data drift in time-series.\n",
    "    \n",
    "    Detects when incoming data distribution changes significantly\n",
    "    from historical patterns (indicates data quality issues or\n",
    "    fundamental market changes).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, reference_data: pd.DataFrame):\n",
    "        self.reference_stats = self._calculate_stats(reference_data)\n",
    "    \n",
    "    def _calculate_stats(self, df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"Calculate statistical profile of reference data.\"\"\"\n",
    "        return {\n",
    "            'close_mean': df['close'].mean(),\n",
    "            'close_std': df['close'].std(),\n",
    "            'volume_mean': df['volume'].mean(),\n",
    "            'volume_std': df['volume'].std()\n",
    "        }\n",
    "    \n",
    "    def check_drift(self, new_data: pd.DataFrame) -> Dict[str, bool]:\n",
    "        \"\"\"\n",
    "        Check if new data has drifted from reference.\n",
    "        \n",
    "        Uses z-score to detect significant deviations.\n",
    "        \"\"\"\n",
    "        drift_detected = {}\n",
    "        \n",
    "        # Check price drift\n",
    "        new_mean = new_data['close'].mean()\n",
    "        z_score = abs(new_mean - self.reference_stats['close_mean']) / self.reference_stats['close_std']\n",
    "        drift_detected['price_drift'] = z_score > 3  # 3 sigma rule\n",
    "        \n",
    "        # Check volume drift\n",
    "        new_vol_mean = new_data['volume'].mean()\n",
    "        vol_z = abs(new_vol_mean - self.reference_stats['volume_mean']) / self.reference_stats['volume_std']\n",
    "        drift_detected['volume_drift'] = vol_z > 3\n",
    "        \n",
    "        return drift_detected\n",
    "\n",
    "\n",
    "def demonstrate_monitoring():\n",
    "    \"\"\"\n",
    "    Demonstrate pipeline monitoring.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Pipeline Monitoring\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Simulate pipeline runs\n",
    "    monitor = PipelineMonitor(\"nepse_daily_etl\")\n",
    "    \n",
    "    for i in range(5):\n",
    "        run_id = monitor.start_run()\n",
    "        \n",
    "        # Simulate work\n",
    "        time.sleep(0.1)\n",
    "        \n",
    "        monitor.record_metric(\"rows_extracted\", 200 + i * 10)\n",
    "        monitor.record_metric(\"null_percentage\", 0.02)\n",
    "        \n",
    "        # Simulate occasional failure\n",
    "        status = \"success\" if i != 3 else \"failed\"\n",
    "        monitor.end_run(status)\n",
    "        \n",
    "        print(f\"Run {i+1}: {status}\")\n",
    "    \n",
    "    stats = monitor.get_stats()\n",
    "    print(f\"\\nPipeline Statistics:\")\n",
    "    print(f\"  Total runs: {stats['total_runs']}\")\n",
    "    print(f\"  Success rate: {stats['success_rate']:.1%}\")\n",
    "    print(f\"  Avg duration: {stats['avg_duration']:.2f}s\")\n",
    "    \n",
    "    # Data drift example\n",
    "    print(\"\\nData Drift Detection:\")\n",
    "    reference = pd.DataFrame({\n",
    "        'close': np.random.normal(500, 20, 1000),\n",
    "        'volume': np.random.normal(100000, 10000, 1000)\n",
    "    })\n",
    "    \n",
    "    drift_monitor = DataDriftMonitor(reference)\n",
    "    \n",
    "    # Normal new data\n",
    "    normal_data = pd.DataFrame({\n",
    "        'close': np.random.normal(505, 22, 100),\n",
    "        'volume': np.random.normal(101000, 11000, 100)\n",
    "    })\n",
    "    \n",
    "    # Drifted data (market crash simulation)\n",
    "    drifted_data = pd.DataFrame({\n",
    "        'close': np.random.normal(300, 15, 100),  # Major drop\n",
    "        'volume': np.random.normal(500000, 50000, 100)  # Volume spike\n",
    "    })\n",
    "    \n",
    "    print(f\"  Normal data drift: {drift_monitor.check_drift(normal_data)}\")\n",
    "    print(f\"  Crashed market drift: {drift_monitor.check_drift(drifted_data)}\")\n",
    "    \n",
    "    return monitor\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_monitoring()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.7 Error Handling and Recovery**\n",
    "\n",
    "Robust pipelines handle failures gracefully and recover automatically.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Error Handling and Recovery Strategies\n",
    "\n",
    "Strategies:\n",
    "1. Retry with exponential backoff\n",
    "2. Dead letter queues (save failed records)\n",
    "3. Circuit breaker (stop trying if failing repeatedly)\n",
    "4. Checkpoint/resume\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "import random\n",
    "from functools import wraps\n",
    "from typing import Callable, Any, Optional\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "class RetryWithBackoff:\n",
    "    \"\"\"\n",
    "    Decorator for retrying failed operations.\n",
    "    \n",
    "    Implements exponential backoff: wait 2^attempt seconds between retries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_retries: int = 3, base_delay: float = 1.0):\n",
    "        self.max_retries = max_retries\n",
    "        self.base_delay = base_delay\n",
    "    \n",
    "    def __call__(self, func: Callable) -> Callable:\n",
    "        @wraps(func)\n",
    "        def wrapper(*args, **kwargs):\n",
    "            for attempt in range(self.max_retries):\n",
    "                try:\n",
    "                    return func(*args, **kwargs)\n",
    "                except Exception as e:\n",
    "                    if attempt == self.max_retries - 1:\n",
    "                        raise\n",
    "                    \n",
    "                    delay = self.base_delay * (2 ** attempt) + random.uniform(0, 1)\n",
    "                    logger.warning(f\"Attempt {attempt + 1} failed: {e}. Retrying in {delay:.1f}s...\")\n",
    "                    time.sleep(delay)\n",
    "            \n",
    "        return wrapper\n",
    "\n",
    "\n",
    "class CircuitBreaker:\n",
    "    \"\"\"\n",
    "    Circuit breaker pattern: Stop calling failing service.\n",
    "    \n",
    "    States:\n",
    "    - CLOSED: Normal operation\n",
    "    - OPEN: Failing fast (service down)\n",
    "    - HALF_OPEN: Testing if service recovered\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, failure_threshold: int = 5, recovery_timeout: int = 60):\n",
    "        self.failure_threshold = failure_threshold\n",
    "        self.recovery_timeout = recovery_timeout\n",
    "        self.failure_count = 0\n",
    "        self.last_failure_time = None\n",
    "        self.state = \"CLOSED\"  # CLOSED, OPEN, HALF_OPEN\n",
    "    \n",
    "    def call(self, func: Callable, *args, **kwargs):\n",
    "        \"\"\"Call function with circuit breaker protection.\"\"\"\n",
    "        if self.state == \"OPEN\":\n",
    "            if time.time() - self.last_failure_time > self.recovery_timeout:\n",
    "                self.state = \"HALF_OPEN\"\n",
    "                logger.info(\"Circuit breaker entering HALF_OPEN state\")\n",
    "            else:\n",
    "                raise Exception(\"Circuit breaker is OPEN\")\n",
    "        \n",
    "        try:\n",
    "            result = func(*args, **kwargs)\n",
    "            self._on_success()\n",
    "            return result\n",
    "        except Exception as e:\n",
    "            self._on_failure()\n",
    "            raise\n",
    "    \n",
    "    def _on_success(self):\n",
    "        \"\"\"Reset on success.\"\"\"\n",
    "        self.failure_count = 0\n",
    "        self.state = \"CLOSED\"\n",
    "    \n",
    "    def _on_failure(self):\n",
    "        \"\"\"Track failures.\"\"\"\n",
    "        self.failure_count += 1\n",
    "        self.last_failure_time = time.time()\n",
    "        \n",
    "        if self.failure_count >= self.failure_threshold:\n",
    "            self.state = \"OPEN\"\n",
    "            logger.error(\"Circuit breaker OPENED due to repeated failures\")\n",
    "\n",
    "\n",
    "class DeadLetterQueue:\n",
    "    \"\"\"\n",
    "    Save failed records for later inspection/reprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, queue_path: str = \"./dlq.json\"):\n",
    "        self.queue_path = queue_path\n",
    "        self.failed_records: List[Dict] = []\n",
    "    \n",
    "    def add(self, record: Any, error: str, context: Dict = None):\n",
    "        \"\"\"Add failed record to DLQ.\"\"\"\n",
    "        self.failed_records.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'record': record,\n",
    "            'error': error,\n",
    "            'context': context or {}\n",
    "        })\n",
    "    \n",
    "    def save(self):\n",
    "        \"\"\"Persist DLQ to disk.\"\"\"\n",
    "        import json\n",
    "        with open(self.queue_path, 'w') as f:\n",
    "            json.dump(self.failed_records, f, indent=2, default=str)\n",
    "    \n",
    "    def reprocess(self, processor: Callable) -> int:\n",
    "        \"\"\"\n",
    "        Attempt to reprocess failed records.\n",
    "        \n",
    "        Returns:\n",
    "            Number successfully reprocessed\n",
    "        \"\"\"\n",
    "        success_count = 0\n",
    "        still_failed = []\n",
    "        \n",
    "        for item in self.failed_records:\n",
    "            try:\n",
    "                processor(item['record'])\n",
    "                success_count += 1\n",
    "            except Exception as e:\n",
    "                item['retry_error'] = str(e)\n",
    "                still_failed.append(item)\n",
    "        \n",
    "        self.failed_records = still_failed\n",
    "        return success_count\n",
    "\n",
    "\n",
    "# Example usage with NEPSE API\n",
    "\n",
    "@RetryWithBackoff(max_retries=3, base_delay=2.0)\n",
    "def fetch_nepse_api(date: str) -> Dict:\n",
    "    \"\"\"\n",
    "    Fetch data from NEPSE API with retry logic.\n",
    "    \n",
    "    Simulates occasional failures.\n",
    "    \"\"\"\n",
    "    if random.random() < 0.3:  # 30% failure rate simulation\n",
    "        raise ConnectionError(\"NEPSE API timeout\")\n",
    "    \n",
    "    return {\"date\": date, \"prices\": [{\"symbol\": \"NABIL\", \"close\": 865.0}]}\n",
    "\n",
    "\n",
    "def demonstrate_error_handling():\n",
    "    \"\"\"\n",
    "    Demonstrate error handling patterns.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Error Handling and Recovery\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Test retry logic\n",
    "    print(\"\\n1. Retry with Exponential Backoff\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    try:\n",
    "        result = fetch_nepse_api(\"2024-01-15\")\n",
    "        print(f\"Success: {result}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Failed after retries: {e}\")\n",
    "    \n",
    "    # Test circuit breaker\n",
    "    print(\"\\n2. Circuit Breaker Pattern\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    cb = CircuitBreaker(failure_threshold=3, recovery_timeout=5)\n",
    "    \n",
    "    def unreliable_function():\n",
    "        if random.random() < 0.7:\n",
    "            raise Exception(\"Service unavailable\")\n",
    "        return \"Success\"\n",
    "    \n",
    "    for i in range(10):\n",
    "        try:\n",
    "            result = cb.call(unreliable_function)\n",
    "            print(f\"Call {i+1}: {result}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Call {i+1}: {str(e)[:50]}\")\n",
    "        \n",
    "        time.sleep(0.5)\n",
    "    \n",
    "    # Test DLQ\n",
    "    print(\"\\n3. Dead Letter Queue\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    dlq = DeadLetterQueue()\n",
    "    \n",
    "    # Simulate processing with some failures\n",
    "    records = [\n",
    "        {\"symbol\": \"NABIL\", \"price\": 865},\n",
    "        {\"symbol\": \"BAD_DATA\", \"price\": \"invalid\"},\n",
    "        {\"symbol\": \"NICA\", \"price\": 790}\n",
    "    ]\n",
    "    \n",
    "    def process_record(record):\n",
    "        if not isinstance(record['price'], (int, float)):\n",
    "            raise ValueError(\"Invalid price type\")\n",
    "        print(f\"  Processed {record['symbol']}\")\n",
    "    \n",
    "    for record in records:\n",
    "        try:\n",
    "            process_record(record)\n",
    "        except Exception as e:\n",
    "            dlq.add(record, str(e))\n",
    "            print(f\"  Failed {record['symbol']}: {e}\")\n",
    "    \n",
    "    print(f\"\\nDLQ has {len(dlq.failed_records)} failed records\")\n",
    "    print(f\"Attempting reprocess...\")\n",
    "    fixed_count = dlq.reprocess(process_record)\n",
    "    print(f\"Reprocessed {fixed_count} records\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_error_handling()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.8 Pipeline Testing**\n",
    "\n",
    "Testing ensures pipelines work correctly before production deployment.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Pipeline Testing Strategies\n",
    "\n",
    "Types of tests:\n",
    "1. Unit tests: Individual functions\n",
    "2. Integration tests: Database connections, API calls\n",
    "3. Data tests: Schema, values, distributions\n",
    "4. End-to-end tests: Full pipeline run\n",
    "\"\"\"\n",
    "\n",
    "import unittest\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "class TestNEPSEPipeline(unittest.TestCase):\n",
    "    \"\"\"Unit tests for NEPSE pipeline components.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test fixtures.\"\"\"\n",
    "        self.sample_data = pd.DataFrame({\n",
    "            'symbol': ['NABIL', 'NICA'],\n",
    "            'date': ['2024-01-15', '2024-01-15'],\n",
    "            'open': [850.0, 780.0],\n",
    "            'high': [870.0, 795.0],\n",
    "            'low': [845.0, 775.0],\n",
    "            'close': [865.0, 790.0],\n",
    "            'volume': [100000, 80000]\n",
    "        })\n",
    "    \n",
    "    def test_data_validation(self):\n",
    "        \"\"\"Test validation logic.\"\"\"\n",
    "        # Should pass with valid data\n",
    "        self.assertTrue(len(self.sample_data) > 0)\n",
    "        self.assertTrue(all(self.sample_data['close'] > 0))\n",
    "    \n",
    "    def test_feature_calculation(self):\n",
    "        \"\"\"Test technical indicator calculations.\"\"\"\n",
    "        # Calculate daily return\n",
    "        self.sample_data['return'] = self.sample_data['close'].pct_change()\n",
    "        \n",
    "        # First row should be NaN (no previous data)\n",
    "        self.assertTrue(pd.isna(self.sample_data['return'].iloc[0]))\n",
    "        \n",
    "        # Other rows should be numeric\n",
    "        self.assertIsInstance(self.sample_data['return'].iloc[1], float)\n",
    "    \n",
    "    def test_duplicate_detection(self):\n",
    "        \"\"\"Test that duplicates are caught.\"\"\"\n",
    "        # Create duplicate\n",
    "        dup_data = pd.concat([self.sample_data, self.sample_data.iloc[[0]]])\n",
    "        \n",
    "        duplicates = dup_data.duplicated(subset=['symbol', 'date']).sum()\n",
    "        self.assertEqual(duplicates, 1)\n",
    "\n",
    "\n",
    "class TestDataQuality(unittest.TestCase):\n",
    "    \"\"\"Tests for data quality checks.\"\"\"\n",
    "    \n",
    "    def test_schema_validation(self):\n",
    "        \"\"\"Ensure required columns exist.\"\"\"\n",
    "        required_cols = ['symbol', 'date', 'open', 'high', 'low', 'close', 'volume']\n",
    "        data = pd.DataFrame(columns=required_cols)\n",
    "        \n",
    "        for col in required_cols:\n",
    "            self.assertIn(col, data.columns)\n",
    "    \n",
    "    def test_price_consistency(self):\n",
    "        \"\"\"Ensure high >= low.\"\"\"\n",
    "        data = pd.DataFrame({\n",
    "            'high': [100, 100],\n",
    "            'low': [90, 110]  # Second row invalid\n",
    "        })\n",
    "        \n",
    "        valid = data['high'] >= data['low']\n",
    "        self.assertTrue(valid.iloc[0])\n",
    "        self.assertFalse(valid.iloc[1])\n",
    "\n",
    "\n",
    "def run_tests():\n",
    "    \"\"\"Run the test suite.\"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Pipeline Testing\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Create test suite\n",
    "    loader = unittest.TestLoader()\n",
    "    suite = unittest.TestSuite()\n",
    "    \n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestNEPSEPipeline))\n",
    "    suite.addTests(loader.loadTestsFromTestCase(TestDataQuality))\n",
    "    \n",
    "    # Run tests\n",
    "    runner = unittest.TextTestRunner(verbosity=2)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_tests()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.9 Scalability Considerations**\n",
    "\n",
    "Designing pipelines to handle growing data volumes.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Scalability Strategies for NEPSE Pipelines\n",
    "\n",
    "Horizontal Scaling:\n",
    "- Distribute work across multiple workers\n",
    "- Partition data by symbol or date\n",
    "- Use message queues (Kafka, RabbitMQ)\n",
    "\n",
    "Vertical Scaling:\n",
    "- Optimize code (vectorization, caching)\n",
    "- Use faster storage (SSD, memory)\n",
    "- Parallel processing within single machine\n",
    "\"\"\"\n",
    "\n",
    "import multiprocessing as mp\n",
    "from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "\n",
    "\n",
    "def process_symbol_chunk(symbol_data: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Process a single symbol's data (for parallel execution).\n",
    "    \n",
    "    This function is picklable and can be sent to worker processes.\n",
    "    \"\"\"\n",
    "    symbol = symbol_data['symbol'].iloc[0]\n",
    "    \n",
    "    # Calculate indicators\n",
    "    symbol_data = symbol_data.sort_values('date')\n",
    "    symbol_data['sma_20'] = symbol_data['close'].rolling(20, min_periods=1).mean()\n",
    "    symbol_data['returns'] = symbol_data['close'].pct_change()\n",
    "    \n",
    "    return symbol_data\n",
    "\n",
    "\n",
    "class ScalablePipeline:\n",
    "    \"\"\"\n",
    "    Pipeline that scales horizontally using multiprocessing.\n",
    "    \n",
    "    Useful when processing many symbols independently.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, max_workers: int = None):\n",
    "        self.max_workers = max_workers or mp.cpu_count()\n",
    "    \n",
    "    def process_parallel(self, data: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Process data in parallel by symbol.\n",
    "        \n",
    "        Each symbol is processed independently in a separate process,\n",
    "        utilizing all CPU cores.\n",
    "        \"\"\"\n",
    "        # Group by symbol\n",
    "        symbol_groups = [group for _, group in data.groupby('symbol')]\n",
    "        \n",
    "        # Process in parallel\n",
    "        with ProcessPoolExecutor(max_workers=self.max_workers) as executor:\n",
    "            results = list(executor.map(process_symbol_chunk, symbol_groups))\n",
    "        \n",
    "        # Combine results\n",
    "        return pd.concat(results, ignore_index=True)\n",
    "    \n",
    "    def process_distributed(self, \n",
    "                          data: pd.DataFrame,\n",
    "                          partition_col: str = 'date') -> List[pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Partition data for distributed processing (e.g., Spark, Dask).\n",
    "        \n",
    "        Returns list of partitions that can be processed on different nodes.\n",
    "        \"\"\"\n",
    "        partitions = []\n",
    "        \n",
    "        for partition_value, group in data.groupby(partition_col):\n",
    "            partitions.append({\n",
    "                'partition_key': partition_value,\n",
    "                'data': group,\n",
    "                'count': len(group)\n",
    "            })\n",
    "        \n",
    "        return partitions\n",
    "\n",
    "\n",
    "def demonstrate_scalability():\n",
    "    \"\"\"\n",
    "    Demonstrate scalability concepts.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Scalability Considerations\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate large dataset\n",
    "    print(\"\\nGenerating test data (100,000 records)...\")\n",
    "    dates = pd.date_range('2020-01-01', '2024-01-01', freq='B')\n",
    "    symbols = [f'STOCK_{i}' for i in range(100)]  # 100 symbols\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for symbol in symbols:\n",
    "            data.append({\n",
    "                'symbol': symbol,\n",
    "                'date': date,\n",
    "                'close': np.random.uniform(100, 1000),\n",
    "                'volume': np.random.randint(10000, 1000000)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"Total records: {len(df)}\")\n",
    "    \n",
    "    # Parallel processing\n",
    "    pipeline = ScalablePipeline(max_workers=4)\n",
    "    \n",
    "    print(\"\\nProcessing with 4 workers...\")\n",
    "    import time\n",
    "    start = time.time()\n",
    "    result = pipeline.process_parallel(df.head(10000))  # Process subset for demo\n",
    "    duration = time.time() - start\n",
    "    \n",
    "    print(f\"Processed in {duration:.2f} seconds\")\n",
    "    print(f\"CPU cores used: {pipeline.max_workers}\")\n",
    "    \n",
    "    # Partitioning strategy\n",
    "    print(\"\\nData Partitioning for Distributed Processing:\")\n",
    "    partitions = pipeline.process_distributed(df.head(1000), 'date')\n",
    "    print(f\"  Created {len(partitions)} daily partitions\")\n",
    "    print(f\"  Average partition size: {np.mean([p['count'] for p in partitions]):.0f} records\")\n",
    "    \n",
    "    print(\"\\nScaling Recommendations for NEPSE:\")\n",
    "    print(\"  1. Current scale (< 1M records): Single machine, pandas\")\n",
    "    print(\"  2. Medium scale (1M-100M): Dask or multiprocessing\")\n",
    "    print(\"  3. Large scale (> 100M): Apache Spark on cluster\")\n",
    "    print(\"  4. Real-time: Kafka + Flink streaming\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_scalability()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.10 Cost Optimization**\n",
    "\n",
    "Managing infrastructure costs while maintaining performance.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Cost Optimization Strategies\n",
    "\n",
    "1. Storage Tiers: Hot (SSD) -> Warm (Disk) -> Cold (S3 Glacier)\n",
    "2. Compute: Spot instances, auto-scaling, serverless\n",
    "3. Data Lifecycle: Archive old data, compression\n",
    "4. Query Optimization: Partition pruning, column selection\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "\n",
    "class CostOptimizer:\n",
    "    \"\"\"\n",
    "    Strategies to minimize pipeline operating costs.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.costs = {\n",
    "            'storage_gb_month': 0.023,  # S3 Standard per GB\n",
    "            'compute_hour': 0.05,       # EC2 spot instance\n",
    "            'api_call': 0.001           # Per 1000 API calls\n",
    "        }\n",
    "    \n",
    "    def calculate_storage_cost(self, \n",
    "                              data_size_gb: float,\n",
    "                              storage_class: str = 'standard') -> float:\n",
    "        \"\"\"\n",
    "        Calculate monthly storage cost.\n",
    "        \n",
    "        Storage classes:\n",
    "        - standard: $0.023/GB (frequent access)\n",
    "        - infrequent: $0.0125/GB (monthly access)\n",
    "        - glacier: $0.004/GB (archive, rare access)\n",
    "        \"\"\"\n",
    "        rates = {\n",
    "            'standard': 0.023,\n",
    "            'infrequent': 0.0125,\n",
    "            'glacier': 0.004\n",
    "        }\n",
    "        \n",
    "        rate = rates.get(storage_class, 0.023)\n",
    "        return data_size_gb * rate\n",
    "    \n",
    "    def recommend_storage_tier(self, \n",
    "                              last_access_days: int,\n",
    "                              access_frequency: str) -> str:\n",
    "        \"\"\"\n",
    "        Recommend storage tier based on access patterns.\n",
    "        \"\"\"\n",
    "        if last_access_days > 365:\n",
    "            return 'glacier'\n",
    "        elif last_access_days > 30 or access_frequency == 'rare':\n",
    "            return 'infrequent'\n",
    "        else:\n",
    "            return 'standard'\n",
    "    \n",
    "    def calculate_processing_cost(self,\n",
    "                                 run_time_hours: float,\n",
    "                                 vcpu_count: int,\n",
    "                                 memory_gb: int,\n",
    "                                 use_spot: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        Calculate compute cost for pipeline run.\n",
    "        \n",
    "        Spot instances can save up to 90% but can be interrupted.\n",
    "        \"\"\"\n",
    "        base_rate = 0.05  # per hour per vCPU\n",
    "        \n",
    "        if use_spot:\n",
    "            base_rate *= 0.3  # 70% discount\n",
    "        \n",
    "        return run_time_hours * vcpu_count * base_rate\n",
    "    \n",
    "    def optimize_query_cost(self, \n",
    "                           data_scanned_gb: float,\n",
    "                           column_optimization: bool = True) -> float:\n",
    "        \"\"\"\n",
    "        Calculate query cost (BigQuery model: $5 per TB scanned).\n",
    "        \n",
    "        Column optimization (SELECT only needed columns) reduces cost.\n",
    "        \"\"\"\n",
    "        cost_per_tb = 5.0\n",
    "        \n",
    "        if column_optimization:\n",
    "            # Typical reduction: 80% less data scanned\n",
    "            data_scanned_gb *= 0.2\n",
    "        \n",
    "        return (data_scanned_gb / 1024) * cost_per_tb\n",
    "\n",
    "\n",
    "def demonstrate_cost_optimization():\n",
    "    \"\"\"\n",
    "    Demonstrate cost calculations.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Cost Optimization\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    optimizer = CostOptimizer()\n",
    "    \n",
    "    # NEPSE data cost calculation\n",
    "    data_size_gb = 0.5  # 500 MB historical data\n",
    "    growth_rate = 0.001  # 1 MB per day\n",
    "    \n",
    "    print(\"\\nStorage Cost Analysis (Monthly):\")\n",
    "    print(f\"Current data size: {data_size_gb} GB\")\n",
    "    \n",
    "    # Current month\n",
    "    standard_cost = optimizer.calculate_storage_cost(data_size_gb, 'standard')\n",
    "    glacier_cost = optimizer.calculate_storage_cost(data_size_gb, 'glacier')\n",
    "    \n",
    "    print(f\"  Standard storage: ${standard_cost:.2f}/month\")\n",
    "    print(f\"  Glacier archive: ${glacier_cost:.2f}/month\")\n",
    "    print(f\"  Potential savings: ${standard_cost - glacier_cost:.2f}/month\")\n",
    "    \n",
    "    # Processing cost\n",
    "    print(\"\\nProcessing Cost (Daily Pipeline):\")\n",
    "    daily_cost = optimizer.calculate_processing_cost(\n",
    "        run_time_hours=0.1,  # 6 minutes\n",
    "        vcpu_count=2,\n",
    "        memory_gb=4,\n",
    "        use_spot=True\n",
    "    )\n",
    "    print(f\"  Cost per run: ${daily_cost:.3f}\")\n",
    "    print(f\"  Monthly cost (22 trading days): ${daily_cost * 22:.2f}\")\n",
    "    \n",
    "    # Query optimization\n",
    "    print(\"\\nQuery Cost Optimization:\")\n",
    "    full_scan_cost = optimizer.optimize_query_cost(0.5, column_optimization=False)\n",
    "    optimized_cost = optimizer.optimize_query_cost(0.5, column_optimization=True)\n",
    "    \n",
    "    print(f\"  Full table scan: ${full_scan_cost:.4f} per query\")\n",
    "    print(f\"  Column-optimized: ${optimized_cost:.4f} per query\")\n",
    "    print(f\"  Savings per query: {(1 - optimized_cost/full_scan_cost)*100:.0f}%\")\n",
    "    \n",
    "    print(\"\\nCost Optimization Recommendations:\")\n",
    "    print(\"  1. Move data > 1 year old to Glacier (saves 80%)\")\n",
    "    print(\"  2. Use Spot instances for batch processing (saves 70%)\")\n",
    "    print(\"  3. Partition data by date (reduces query scan by 90%)\")\n",
    "    print(\"  4. Compress files (Parquet with Snappy)\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_cost_optimization()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **9.11 Building Production Pipelines**\n",
    "\n",
    "Putting it all together into a production-ready system.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Production Pipeline for NEPSE\n",
    "\n",
    "Complete integration of all concepts:\n",
    "- Orchestration with Airflow\n",
    "- Data quality gates\n",
    "- Error handling with retries\n",
    "- Monitoring and alerting\n",
    "- Cost optimization\n",
    "\"\"\"\n",
    "\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# Configure production logging\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s [%(levelname)s] %(name)s: %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('nepse_pipeline.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "\n",
    "\n",
    "class ProductionNEPSEPipeline:\n",
    "    \"\"\"\n",
    "    Production-ready pipeline for NEPSE daily data ingestion.\n",
    "    \n",
    "    Features:\n",
    "    - Comprehensive error handling\n",
    "    - Data quality validation\n",
    "    - Performance monitoring\n",
    "    - Idempotent operations\n",
    "    - Automatic retries\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.logger = logging.getLogger('ProductionNEPSEPipeline')\n",
    "        self.monitor = PipelineMonitor('nepse_production')\n",
    "        self.dlq = DeadLetterQueue('./production_dlq.json')\n",
    "        self.quality_suite = DataQualitySuite()\n",
    "        \n",
    "        # Setup quality checks\n",
    "        self.quality_suite.add_check(check_no_missing_symbols)\n",
    "        self.quality_suite.add_check(check_price_positive)\n",
    "        self.quality_suite.add_check(check_no_duplicates)\n",
    "        self.quality_suite.add_check(check_price_range_consistency)\n",
    "    \n",
    "    @RetryWithBackoff(max_retries=3, base_delay=60)\n",
    "    def extract(self, date: str):\n",
    "        \"\"\"Extract with retry logic.\"\"\"\n",
    "        self.logger.info(f\"Extracting data for {date}\")\n",
    "        # API call here\n",
    "        return pd.DataFrame()  # Placeholder\n",
    "    \n",
    "    def validate(self, df: pd.DataFrame) -> bool:\n",
    "        \"\"\"Run quality gates.\"\"\"\n",
    "        self.logger.info(\"Running data quality checks\")\n",
    "        is_valid = self.quality_suite.validate(df)\n",
    "        \n",
    "        report = self.quality_suite.get_report()\n",
    "        self.monitor.record_metric('quality_checks_passed', report['passed'])\n",
    "        self.monitor.record_metric('quality_checks_failed', report['failed'])\n",
    "        \n",
    "        return is_valid\n",
    "    \n",
    "    def run(self, date: str = None):\n",
    "        \"\"\"Execute full pipeline.\"\"\"\n",
    "        if date is None:\n",
    "            date = datetime.now().strftime('%Y-%m-%d')\n",
    "        \n",
    "        run_id = self.monitor.start_run()\n",
    "        self.logger.info(f\"Starting production run: {run_id}\")\n",
    "        \n",
    "        try:\n",
    "            # Extract\n",
    "            raw_data = self.extract(date)\n",
    "            self.monitor.record_metric('records_extracted', len(raw_data))\n",
    "            \n",
    "            # Validate\n",
    "            if not self.validate(raw_data):\n",
    "                raise ValueError(\"Data quality validation failed\")\n",
    "            \n",
    "            # Transform\n",
    "            processed = self.transform_data(raw_data)\n",
    "            \n",
    "            # Load\n",
    "            self.load_to_production_warehouse(processed)\n",
    "            self.monitor.record_metric('records_loaded', len(processed))\n",
    "            \n",
    "            # Success\n",
    "            self.monitor.end_run('success')\n",
    "            self.logger.info(f\"Pipeline completed successfully: {run_id}\")\n",
    "            \n",
    "            return {'status': 'success', 'run_id': run_id}\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Pipeline failed: {str(e)}\")\n",
    "            self.monitor.end_run('failed')\n",
    "            \n",
    "            # Save to DLQ for investigation\n",
    "            self.dlq.add({'date': date}, str(e))\n",
    "            self.dlq.save()\n",
    "            \n",
    "            raise\n",
    "\n",
    "\n",
    "def demonstrate_production_pipeline():\n",
    "    \"\"\"\n",
    "    Demonstrate production pipeline structure.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Production Pipeline Architecture\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    Production Pipeline Components:\n",
    "    \n",
    "    1. Orchestration: Apache Airflow\n",
    "       - Schedule: Daily at 18:00 (after market close)\n",
    "       - Retries: 3 attempts with exponential backoff\n",
    "       - Alerts: Email on failure\n",
    "    \n",
    "    2. Data Quality: Great Expectations\n",
    "       - Schema validation\n",
    "       - Statistical profiling\n",
    "       - Automatic documentation\n",
    "    \n",
    "    3. Storage: Tiered approach\n",
    "       - Hot: Last 30 days (SSD)\n",
    "       - Warm: 1 month to 1 year (Standard S3)\n",
    "       - Cold: > 1 year (Glacier)\n",
    "    \n",
    "    4. Monitoring: Prometheus + Grafana\n",
    "       - Pipeline duration\n",
    "       - Success rates\n",
    "       - Data quality metrics\n",
    "       - Cost tracking\n",
    "    \n",
    "    5. Disaster Recovery:\n",
    "       - Daily backups to S3\n",
    "       - Cross-region replication\n",
    "       - 4-hour RPO (Recovery Point Objective)\n",
    "    \n",
    "    6. Security:\n",
    "       - Encryption at rest (AES-256)\n",
    "       - TLS in transit\n",
    "       - IAM roles for service accounts\n",
    "       - Audit logging\n",
    "    \"\"\")\n",
    "    \n",
    "    # Initialize production pipeline\n",
    "    pipeline = ProductionNEPSEPipeline()\n",
    "    print(f\"\\nProduction pipeline initialized: {pipeline.monitor.pipeline_id}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_production_pipeline()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we covered comprehensive data pipeline strategies:\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Architecture Patterns**:\n",
    "   - **ETL**: Transform before load (protects warehouse)\n",
    "   - **ELT**: Load then transform (leverages DB power)\n",
    "   - **Lambda**: Batch + speed layers for different latencies\n",
    "   - **Medallion**: Bronze (raw) \u2192 Silver (clean) \u2192 Gold (features)\n",
    "\n",
    "2. **Batch Processing**:\n",
    "   - Idempotent operations (safe to retry)\n",
    "   - Checkpointing (resume from failures)\n",
    "   - Validation gates (stop bad data early)\n",
    "\n",
    "3. **Stream Processing**:\n",
    "   - Real-time tick processing\n",
    "   - Windowed aggregations (moving averages)\n",
    "   - Anomaly detection with cooldowns\n",
    "\n",
    "4. **Orchestration**:\n",
    "   - **Airflow**: Industry standard, DAG-based\n",
    "   - **Prefect**: Modern, Pythonic, better error handling\n",
    "   - **Dagster**: Asset-centric, strong typing\n",
    "\n",
    "5. **Data Quality**:\n",
    "   - Schema validation (types, ranges)\n",
    "   - Business rules (high >= low, positive prices)\n",
    "   - Great Expectations library for comprehensive checks\n",
    "\n",
    "6. **Monitoring**:\n",
    "   - Operational metrics (runtime, success rate)\n",
    "   - Data drift detection (distribution changes)\n",
    "   - Alerting on failures and anomalies\n",
    "\n",
    "7. **Error Handling**:\n",
    "   - Exponential backoff for retries\n",
    "   - Circuit breakers (stop hammering failing services)\n",
    "   - Dead letter queues (save failed records)\n",
    "\n",
    "8. **Testing**:\n",
    "   - Unit tests for transformations\n",
    "   - Integration tests for databases\n",
    "   - Data quality tests (schema, duplicates)\n",
    "\n",
    "9. **Scalability**:\n",
    "   - Horizontal scaling (multiprocessing by symbol)\n",
    "   - Partitioning strategies (date-based)\n",
    "   - Tools: Dask (medium), Spark (large)\n",
    "\n",
    "10. **Cost Optimization**:\n",
    "    - Storage tiers (hot/warm/cold)\n",
    "    - Spot instances for batch jobs\n",
    "    - Column pruning for queries\n",
    "\n",
    "11. **Production Readiness**:\n",
    "    - Comprehensive logging\n",
    "    - Automated retries\n",
    "    - Quality gates\n",
    "    - Disaster recovery\n",
    "    - Security (encryption, IAM)\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "Chapter 10 will cover **Feature Engineering**, including:\n",
    "- Creating technical indicators (SMA, RSI, MACD)\n",
    "- Lag features and rolling windows\n",
    "- Domain-specific features for finance\n",
    "- Feature selection and importance\n",
    "- Automated feature engineering with tsfresh\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 9**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='8. data_storage_and_management.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../3. feature_engineering/10. introduction_to_feature_engineering.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}