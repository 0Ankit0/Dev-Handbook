{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 8: Data Storage and Management**\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "- Make informed decisions about storage architecture for time-series data\n",
    "- Implement file-based storage solutions (CSV, Parquet, HDF5)\n",
    "- Design and query relational databases for time-series data\n",
    "- Utilize specialized time-series databases (InfluxDB, TimescaleDB)\n",
    "- Choose appropriate NoSQL solutions for different use cases\n",
    "- Implement data partitioning and archival strategies\n",
    "- Ensure data security and compliance\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "- Completed Chapter 7: Exploratory Data Analysis\n",
    "- Understanding of Python file I/O operations\n",
    "- Basic knowledge of SQL\n",
    "- Familiarity with pandas DataFrames\n",
    "\n",
    "---\n",
    "\n",
    "## **8.1 Storage Architecture Decisions**\n",
    "\n",
    "Choosing the right storage architecture is critical for any time-series prediction system. The decision impacts performance, scalability, cost, and maintainability. Let's explore the factors that influence this decision.\n",
    "\n",
    "### **Understanding Storage Requirements**\n",
    "\n",
    "Before selecting a storage solution, you need to analyze your specific requirements:\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Storage Requirements Analysis for Time-Series Systems\n",
    "\n",
    "This module helps analyze and document storage requirements\n",
    "for time-series prediction systems.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "from enum import Enum\n",
    "import os\n",
    "\n",
    "class AccessPattern(Enum):\n",
    "    \"\"\"Enumeration of common data access patterns\"\"\"\n",
    "    SEQUENTIAL = \"sequential\"      # Read data in time order\n",
    "    RANDOM = \"random\"              # Access arbitrary time points\n",
    "    AGGREGATION = \"aggregation\"    # Frequent aggregations/rollups\n",
    "    REAL_TIME = \"real_time\"        # Continuous writes and reads\n",
    "\n",
    "class DataVolume(Enum):\n",
    "    \"\"\"Classification of data volume categories\"\"\"\n",
    "    SMALL = \"small\"       # < 1 GB\n",
    "    MEDIUM = \"medium\"     # 1 GB - 100 GB\n",
    "    LARGE = \"large\"       # 100 GB - 10 TB\n",
    "    VERY_LARGE = \"very_large\"  # > 10 TB\n",
    "\n",
    "@dataclass\n",
    "class StorageRequirements:\n",
    "    \"\"\"\n",
    "    Data class to capture storage requirements for analysis.\n",
    "    \n",
    "    Using a dataclass provides automatic __init__, __repr__, and __eq__\n",
    "    methods, making it easy to create and compare requirement objects.\n",
    "    \"\"\"\n",
    "    estimated_size_gb: float          # Expected data size in gigabytes\n",
    "    write_frequency: str              # How often data is written\n",
    "    read_frequency: str               # How often data is read\n",
    "    access_pattern: AccessPattern     # Primary access pattern\n",
    "    query_complexity: str             # Simple, moderate, or complex queries\n",
    "    retention_period_days: int        # How long to keep data\n",
    "    requires_transaction: bool        # Need for ACID transactions\n",
    "    concurrent_users: int             # Number of simultaneous users\n",
    "    budget_constraint: str            # low, medium, high\n",
    "    \n",
    "    def analyze_requirements(self) -> dict:\n",
    "        \"\"\"\n",
    "        Analyze the storage requirements and provide recommendations.\n",
    "        \n",
    "        Returns a dictionary with analysis results and recommendations.\n",
    "        \"\"\"\n",
    "        # Determine volume category based on size\n",
    "        if self.estimated_size_gb < 1:\n",
    "            volume = DataVolume.SMALL\n",
    "        elif self.estimated_size_gb < 100:\n",
    "            volume = DataVolume.MEDIUM\n",
    "        elif self.estimated_size_gb < 10000:\n",
    "            volume = DataVolume.LARGE\n",
    "        else:\n",
    "            volume = DataVolume.VERY_LARGE\n",
    "        \n",
    "        # Calculate write-to-read ratio\n",
    "        # This helps determine if the system is write-heavy or read-heavy\n",
    "        write_patterns = {\n",
    "            'continuous': 10,\n",
    "            'hourly': 5,\n",
    "            'daily': 1,\n",
    "            'weekly': 0.5\n",
    "        }\n",
    "        read_patterns = {\n",
    "            'continuous': 10,\n",
    "            'frequent': 5,\n",
    "            'moderate': 2,\n",
    "            'occasional': 1\n",
    "        }\n",
    "        \n",
    "        write_score = write_patterns.get(self.write_frequency, 1)\n",
    "        read_score = read_patterns.get(self.read_frequency, 1)\n",
    "        write_read_ratio = write_score / read_score if read_score > 0 else 0\n",
    "        \n",
    "        # Determine system type\n",
    "        if write_read_ratio > 2:\n",
    "            system_type = \"write-heavy\"\n",
    "        elif write_read_ratio < 0.5:\n",
    "            system_type = \"read-heavy\"\n",
    "        else:\n",
    "            system_type = \"balanced\"\n",
    "        \n",
    "        return {\n",
    "            'volume_category': volume.value,\n",
    "            'write_read_ratio': write_read_ratio,\n",
    "            'system_type': system_type,\n",
    "            'recommendations': self._get_recommendations(volume, system_type)\n",
    "        }\n",
    "    \n",
    "    def _get_recommendations(self, volume: DataVolume, system_type: str) -> list:\n",
    "        \"\"\"\n",
    "        Generate storage recommendations based on volume and system type.\n",
    "        \n",
    "        Args:\n",
    "            volume: DataVolume enum value\n",
    "            system_type: String describing the system type\n",
    "            \n",
    "        Returns:\n",
    "            List of recommended storage solutions\n",
    "        \"\"\"\n",
    "        recommendations = []\n",
    "        \n",
    "        # File-based storage is good for small to medium datasets\n",
    "        # especially for development and prototyping\n",
    "        if volume in [DataVolume.SMALL, DataVolume.MEDIUM]:\n",
    "            recommendations.extend([\n",
    "                \"Parquet files for efficient columnar storage\",\n",
    "                \"HDF5 for numerical data with complex queries\",\n",
    "                \"SQLite for transactional requirements\"\n",
    "            ])\n",
    "        \n",
    "        # Relational databases work well for medium datasets with\n",
    "        # complex queries and transaction requirements\n",
    "        if volume == DataVolume.MEDIUM and self.requires_transaction:\n",
    "            recommendations.extend([\n",
    "                \"PostgreSQL with TimescaleDB extension\",\n",
    "                \"MySQL with partitioning\"\n",
    "            ])\n",
    "        \n",
    "        # Time-series databases are optimal for large datasets\n",
    "        # with time-based queries\n",
    "        if volume in [DataVolume.LARGE, DataVolume.VERY_LARGE]:\n",
    "            recommendations.extend([\n",
    "                \"TimescaleDB for SQL compatibility\",\n",
    "                \"InfluxDB for high write throughput\",\n",
    "                \"ClickHouse for analytical queries\"\n",
    "            ])\n",
    "        \n",
    "        # Cloud solutions for scalability\n",
    "        if self.concurrent_users > 10 or volume == DataVolume.VERY_LARGE:\n",
    "            recommendations.extend([\n",
    "                \"AWS Redshift for data warehousing\",\n",
    "                \"Google BigQuery for serverless analytics\",\n",
    "                \"Azure Time Series Insights for IoT scenarios\"\n",
    "            ])\n",
    "        \n",
    "        return recommendations\n",
    "\n",
    "\n",
    "def analyze_nepse_requirements():\n",
    "    \"\"\"\n",
    "    Analyze storage requirements for the NEPSE prediction system.\n",
    "    \n",
    "    This function demonstrates how to analyze requirements for a real\n",
    "    stock market prediction system using the StorageRequirements class.\n",
    "    \"\"\"\n",
    "    # Create a requirements object based on NEPSE characteristics\n",
    "    # NEPSE data characteristics:\n",
    "    # - Daily data for ~200 stocks\n",
    "    # - Approximately 250 trading days per year\n",
    "    # - Each record has 21 columns\n",
    "    # - Historical data going back multiple years\n",
    "    nepse_requirements = StorageRequirements(\n",
    "        estimated_size_gb=0.5,           # ~500 MB for historical data\n",
    "        write_frequency='daily',          # Data updated once per trading day\n",
    "        read_frequency='frequent',        # Model training runs frequently\n",
    "        access_pattern=AccessPattern.SEQUENTIAL,  # Time-series access\n",
    "        query_complexity='moderate',      # Date range queries, aggregations\n",
    "        retention_period_days=3650,       # 10 years of historical data\n",
    "        requires_transaction=False,       # No complex transactions needed\n",
    "        concurrent_users=5,               # Multiple analysts/models\n",
    "        budget_constraint='medium'\n",
    "    )\n",
    "    \n",
    "    # Perform analysis\n",
    "    analysis = nepse_requirements.analyze_requirements()\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"NEPSE Time-Series Prediction System - Storage Analysis\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"\\nData Volume Category: {analysis['volume_category']}\")\n",
    "    print(f\"Write/Read Ratio: {analysis['write_read_ratio']:.2f}\")\n",
    "    print(f\"System Type: {analysis['system_type']}\")\n",
    "    print(\"\\nRecommended Storage Solutions:\")\n",
    "    for i, rec in enumerate(analysis['recommendations'], 1):\n",
    "        print(f\"  {i}. {rec}\")\n",
    "    \n",
    "    return analysis\n",
    "\n",
    "\n",
    "# Example usage\n",
    "if __name__ == \"__main__\":\n",
    "    analyze_nepse_requirements()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **AccessPattern Enum**: Defines how your application accesses data. Time-series systems typically use sequential access (reading data in time order) or aggregation patterns (computing statistics over time windows).\n",
    "\n",
    "2. **DataVolume Enum**: Categorizes data size, which heavily influences storage choice. Small datasets can use simple file-based solutions, while large datasets need specialized databases.\n",
    "\n",
    "3. **StorageRequirements Dataclass**: Captures all relevant factors for storage decisions:\n",
    "   - `estimated_size_gb`: The expected size of your data. For NEPSE, with ~200 stocks and 21 columns per record, storing 10 years of daily data is approximately 500 MB.\n",
    "   - `write_frequency`: How often new data arrives. NEPSE updates daily after market close.\n",
    "   - `read_frequency`: How often data is accessed. Prediction systems read frequently for training.\n",
    "   - `access_pattern`: The primary way data is accessed. Time-series models need sequential access.\n",
    "   - `query_complexity`: Simple queries fetch specific records; complex queries involve joins and aggregations.\n",
    "   - `retention_period_days`: How long to keep historical data. Financial data often requires long retention.\n",
    "   - `requires_transaction`: Whether you need ACID guarantees. For pure analytics, this is often unnecessary.\n",
    "   - `concurrent_users`: Number of simultaneous readers/writers.\n",
    "   - `budget_constraint`: Financial limitations affect cloud vs. on-premise decisions.\n",
    "\n",
    "4. **analyze_requirements Method**: Computes a write-to-read ratio and classifies the system. A write-heavy system (ratio > 2) needs optimized write paths; a read-heavy system (ratio < 0.5) benefits from read replicas and caching.\n",
    "\n",
    "### **Storage Decision Matrix**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Storage Decision Matrix for Time-Series Systems\n",
    "\n",
    "This module provides a decision matrix to help choose the appropriate\n",
    "storage solution based on specific requirements.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Tuple\n",
    "\n",
    "class StorageDecisionMatrix:\n",
    "    \"\"\"\n",
    "    A decision matrix class that evaluates storage options against\n",
    "    multiple criteria to recommend the best solution.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the decision matrix with storage options and criteria.\"\"\"\n",
    "        # Define storage options available for time-series data\n",
    "        self.storage_options = [\n",
    "            'CSV Files',\n",
    "            'Parquet Files',\n",
    "            'HDF5 Files',\n",
    "            'SQLite',\n",
    "            'PostgreSQL',\n",
    "            'TimescaleDB',\n",
    "            'InfluxDB',\n",
    "            'MongoDB',\n",
    "            'Redis'\n",
    "        ]\n",
    "        \n",
    "        # Define evaluation criteria with weights\n",
    "        # Higher weight means the criterion is more important\n",
    "        self.criteria = {\n",
    "            'query_performance': 0.20,      # Speed of data retrieval\n",
    "            'write_performance': 0.15,      # Speed of data insertion\n",
    "            'storage_efficiency': 0.15,     # Compression and space usage\n",
    "            'scalability': 0.15,            # Ability to handle growth\n",
    "            'ease_of_use': 0.10,            # Developer experience\n",
    "            'cost': 0.10,                   # Licensing and operational costs\n",
    "            'ecosystem': 0.10,              # Community support and tools\n",
    "            'time_series_features': 0.05    # Built-in time-series support\n",
    "        }\n",
    "        \n",
    "        # Score matrix: each storage option scored 1-10 on each criterion\n",
    "        # These scores are based on typical use cases and may vary\n",
    "        self.scores = self._initialize_scores()\n",
    "    \n",
    "    def _initialize_scores(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Initialize the scoring matrix with default values.\n",
    "        \n",
    "        Returns a DataFrame with storage options as rows and criteria as columns.\n",
    "        Each cell contains a score from 1 (poor) to 10 (excellent).\n",
    "        \"\"\"\n",
    "        # Score data based on typical performance characteristics\n",
    "        # These scores are based on common benchmarks and use cases\n",
    "        data = {\n",
    "            'query_performance': {\n",
    "                'CSV Files': 3,      # Slow for large files, no indexing\n",
    "                'Parquet Files': 7,  # Good columnar access, predicate pushdown\n",
    "                'HDF5 Files': 8,     # Excellent for numerical data, indexed\n",
    "                'SQLite': 6,         # Good for moderate datasets\n",
    "                'PostgreSQL': 8,     # Excellent with proper indexing\n",
    "                'TimescaleDB': 9,    # Optimized for time-series queries\n",
    "                'InfluxDB': 9,       # Excellent time-series query engine\n",
    "                'MongoDB': 7,        # Good for document queries\n",
    "                'Redis': 10          # In-memory, extremely fast\n",
    "            },\n",
    "            'write_performance': {\n",
    "                'CSV Files': 7,      # Simple append operations\n",
    "                'Parquet Files': 5,  # Slower writes due to compression\n",
    "                'HDF5 Files': 6,     # Moderate write performance\n",
    "                'SQLite': 6,         # Good for moderate write loads\n",
    "                'PostgreSQL': 7,     # Good with proper configuration\n",
    "                'TimescaleDB': 9,    # Optimized for high write throughput\n",
    "                'InfluxDB': 10,      # Designed for high-frequency writes\n",
    "                'MongoDB': 8,        # Good write performance\n",
    "                'Redis': 10          # In-memory writes are fast\n",
    "            },\n",
    "            'storage_efficiency': {\n",
    "                'CSV Files': 2,      # No compression, text format\n",
    "                'Parquet Files': 9,  # Excellent compression ratios\n",
    "                'HDF5 Files': 8,     # Good compression for numerical data\n",
    "                'SQLite': 5,         # Moderate, page-based storage\n",
    "                'PostgreSQL': 6,     # TOAST compression available\n",
    "                'TimescaleDB': 7,    # Good with columnar compression\n",
    "                'InfluxDB': 8,       # Built-in compression\n",
    "                'MongoDB': 5,        # BSON can be verbose\n",
    "                'Redis': 1           # In-memory, no persistence efficiency\n",
    "            },\n",
    "            'scalability': {\n",
    "                'CSV Files': 2,      # Manual sharding, no built-in support\n",
    "                'Parquet Files': 5,  # Can partition files manually\n",
    "                'HDF5 Files': 4,     # Limited to single file/node\n",
    "                'SQLite': 2,         # Single file, no clustering\n",
    "                'PostgreSQL': 7,     # Read replicas, partitioning\n",
    "                'TimescaleDB': 8,    # Native partitioning, multi-node\n",
    "                'InfluxDB': 8,       # Clustering available\n",
    "                'MongoDB': 9,        # Built-in sharding\n",
    "                'Redis': 7           # Clustering available\n",
    "            },\n",
    "            'ease_of_use': {\n",
    "                'CSV Files': 10,     # Simplest format, universal support\n",
    "                'Parquet Files': 8,  # Easy with pandas/PyArrow\n",
    "                'HDF5 Files': 6,     # More complex API\n",
    "                'SQLite': 9,         # Simple setup, file-based\n",
    "                'PostgreSQL': 7,     # Requires setup and maintenance\n",
    "                'TimescaleDB': 7,    # PostgreSQL-based, familiar\n",
    "                'InfluxDB': 7,       # New query language (InfluxQL/Flux)\n",
    "                'MongoDB': 8,        # JSON-like documents, intuitive\n",
    "                'Redis': 6           # Different paradigm (key-value)\n",
    "            },\n",
    "            'cost': {\n",
    "                'CSV Files': 10,     # Free, no infrastructure\n",
    "                'Parquet Files': 10, # Free, no infrastructure\n",
    "                'HDF5 Files': 10,    # Free, no infrastructure\n",
    "                'SQLite': 10,        # Free, embedded\n",
    "                'PostgreSQL': 9,     # Open source, self-hosted\n",
    "                'TimescaleDB': 8,    # Open source with enterprise tier\n",
    "                'InfluxDB': 8,       # Open source with cloud offering\n",
    "                'MongoDB': 7,        # Open source with cloud costs\n",
    "                'Redis': 9           # Open source, self-hosted\n",
    "            },\n",
    "            'ecosystem': {\n",
    "                'CSV Files': 10,     # Universal support\n",
    "                'Parquet Files': 9,  # Strong big data ecosystem\n",
    "                'HDF5 Files': 7,     # Scientific computing focus\n",
    "                'SQLite': 9,         # Widely supported\n",
    "                'PostgreSQL': 9,     # Mature ecosystem\n",
    "                'TimescaleDB': 7,    # Growing ecosystem\n",
    "                'InfluxDB': 8,       # Strong monitoring ecosystem\n",
    "                'MongoDB': 9,        # Large community\n",
    "                'Redis': 9           # Widely adopted\n",
    "            },\n",
    "            'time_series_features': {\n",
    "                'CSV Files': 1,      # No built-in support\n",
    "                'Parquet Files': 2,  # Partitioning by time possible\n",
    "                'HDF5 Files': 3,     # Time dimension support\n",
    "                'SQLite': 2,         # Basic date functions\n",
    "                'PostgreSQL': 5,     # Date functions, can optimize\n",
    "                'TimescaleDB': 10,   # Purpose-built for time-series\n",
    "                'InfluxDB': 10,      # Purpose-built for time-series\n",
    "                'MongoDB': 3,        # Can store time-series but not optimized\n",
    "                'Redis': 2           # Time-series module available\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(data)\n",
    "    \n",
    "    def calculate_weighted_scores(self, \n",
    "                                   custom_weights: Dict[str, float] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate weighted scores for each storage option.\n",
    "        \n",
    "        Args:\n",
    "            custom_weights: Optional dictionary of custom criteria weights.\n",
    "                           If provided, overrides default weights.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with weighted scores for each storage option.\n",
    "        \"\"\"\n",
    "        weights = custom_weights if custom_weights else self.criteria\n",
    "        \n",
    "        # Normalize weights to ensure they sum to 1\n",
    "        total_weight = sum(weights.values())\n",
    "        normalized_weights = {k: v/total_weight for k, v in weights.items()}\n",
    "        \n",
    "        # Calculate weighted scores\n",
    "        weighted_scores = pd.DataFrame(index=self.storage_options)\n",
    "        \n",
    "        for criterion, weight in normalized_weights.items():\n",
    "            weighted_scores[f'{criterion}_weighted'] = (\n",
    "                self.scores[criterion] * weight\n",
    "            )\n",
    "        \n",
    "        # Calculate total weighted score\n",
    "        weighted_scores['total_score'] = weighted_scores.sum(axis=1)\n",
    "        \n",
    "        # Sort by total score\n",
    "        weighted_scores = weighted_scores.sort_values(\n",
    "            'total_score', ascending=False\n",
    "        )\n",
    "        \n",
    "        return weighted_scores\n",
    "    \n",
    "    def get_recommendation(self, \n",
    "                           use_case: str = 'general',\n",
    "                           custom_weights: Dict[str, float] = None) -> Tuple[str, str]:\n",
    "        \"\"\"\n",
    "        Get a storage recommendation based on use case.\n",
    "        \n",
    "        Args:\n",
    "            use_case: The primary use case. Options:\n",
    "                     'general', 'high_write', 'high_read', 'analytics', \n",
    "                     'real_time', 'archival'\n",
    "            custom_weights: Optional custom weights for criteria.\n",
    "        \n",
    "        Returns:\n",
    "            Tuple of (recommended storage, explanation)\n",
    "        \"\"\"\n",
    "        # Predefined weight configurations for different use cases\n",
    "        use_case_weights = {\n",
    "            'general': {\n",
    "                'query_performance': 0.20,\n",
    "                'write_performance': 0.15,\n",
    "                'storage_efficiency': 0.15,\n",
    "                'scalability': 0.15,\n",
    "                'ease_of_use': 0.10,\n",
    "                'cost': 0.10,\n",
    "                'ecosystem': 0.10,\n",
    "                'time_series_features': 0.05\n",
    "            },\n",
    "            'high_write': {\n",
    "                'query_performance': 0.10,\n",
    "                'write_performance': 0.35,  # Prioritize write speed\n",
    "                'storage_efficiency': 0.15,\n",
    "                'scalability': 0.20,\n",
    "                'ease_of_use': 0.05,\n",
    "                'cost': 0.05,\n",
    "                'ecosystem': 0.05,\n",
    "                'time_series_features': 0.05\n",
    "            },\n",
    "            'high_read': {\n",
    "                'query_performance': 0.35,  # Prioritize read speed\n",
    "                'write_performance': 0.10,\n",
    "                'storage_efficiency': 0.10,\n",
    "                'scalability': 0.15,\n",
    "                'ease_of_use': 0.10,\n",
    "                'cost': 0.05,\n",
    "                'ecosystem': 0.10,\n",
    "                'time_series_features': 0.05\n",
    "            },\n",
    "            'analytics': {\n",
    "                'query_performance': 0.20,\n",
    "                'write_performance': 0.10,\n",
    "                'storage_efficiency': 0.25,  # Important for large datasets\n",
    "                'scalability': 0.15,\n",
    "                'ease_of_use': 0.10,\n",
    "                'cost': 0.10,\n",
    "                'ecosystem': 0.05,\n",
    "                'time_series_features': 0.05\n",
    "            },\n",
    "            'real_time': {\n",
    "                'query_performance': 0.25,\n",
    "                'write_performance': 0.25,  # Both important\n",
    "                'storage_efficiency': 0.10,\n",
    "                'scalability': 0.15,\n",
    "                'ease_of_use': 0.05,\n",
    "                'cost': 0.05,\n",
    "                'ecosystem': 0.05,\n",
    "                'time_series_features': 0.10\n",
    "            },\n",
    "            'archival': {\n",
    "                'query_performance': 0.10,\n",
    "                'write_performance': 0.05,\n",
    "                'storage_efficiency': 0.35,  # Most important for archival\n",
    "                'scalability': 0.20,\n",
    "                'ease_of_use': 0.10,\n",
    "                'cost': 0.15,\n",
    "                'ecosystem': 0.05,\n",
    "                'time_series_features': 0.00\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        weights = custom_weights if custom_weights else use_case_weights.get(\n",
    "            use_case, use_case_weights['general']\n",
    "        )\n",
    "        \n",
    "        scores = self.calculate_weighted_scores(weights)\n",
    "        best_option = scores.index[0]\n",
    "        best_score = scores.loc[best_option, 'total_score']\n",
    "        \n",
    "        explanation = self._generate_explanation(best_option, scores)\n",
    "        \n",
    "        return best_option, explanation\n",
    "    \n",
    "    def _generate_explanation(self, \n",
    "                              best_option: str, \n",
    "                              scores: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Generate an explanation for why a storage option was recommended.\n",
    "        \n",
    "        Args:\n",
    "            best_option: The recommended storage option\n",
    "            scores: DataFrame with all scores\n",
    "        \n",
    "        Returns:\n",
    "            Explanation string\n",
    "        \"\"\"\n",
    "        explanations = {\n",
    "            'CSV Files': \"CSV files are simple and universally supported. \"\n",
    "                        \"Best for small datasets, prototyping, or when you need \"\n",
    "                        \"maximum compatibility with different tools.\",\n",
    "            \n",
    "            'Parquet Files': \"Parquet provides excellent compression and \"\n",
    "                            \"columnar access patterns. Ideal for analytical \"\n",
    "                            \"workloads on medium to large datasets where \"\n",
    "                            \"query performance and storage efficiency matter.\",\n",
    "            \n",
    "            'HDF5 Files': \"HDF5 excels at storing numerical scientific data \"\n",
    "                         \"with fast random access. Good for medium-sized \"\n",
    "                         \"datasets where you need efficient slicing and \"\n",
    "                         \"complex data structures.\",\n",
    "            \n",
    "            'SQLite': \"SQLite is a simple, file-based relational database. \"\n",
    "                     \"Great for applications that need SQL capabilities \"\n",
    "                     \"without a separate server, ideal for embedded or \"\n",
    "                     \"desktop applications.\",\n",
    "            \n",
    "            'PostgreSQL': \"PostgreSQL is a robust relational database with \"\n",
    "                         \"excellent query optimization and extensibility. \"\n",
    "                         \"Good for applications requiring complex queries, \"\n",
    "                         \"transactions, and data integrity.\",\n",
    "            \n",
    "            'TimescaleDB': \"TimescaleDB is PostgreSQL optimized for time-series \"\n",
    "                          \"data. It provides automatic partitioning, time-based \"\n",
    "                          \"functions, and excellent performance for time-series \"\n",
    "                          \"workloads. Ideal for production time-series systems.\",\n",
    "            \n",
    "            'InfluxDB': \"InfluxDB is purpose-built for time-series data with \"\n",
    "                       \"excellent write throughput and time-based queries. \"\n",
    "                       \"Great for monitoring, IoT, and real-time analytics.\",\n",
    "            \n",
    "            'MongoDB': \"MongoDB offers flexible document storage with good \"\n",
    "                      \"scalability. Suitable when your data schema evolves \"\n",
    "                      \"frequently or you need horizontal scaling.\",\n",
    "            \n",
    "            'Redis': \"Redis provides in-memory storage for extremely fast \"\n",
    "                    \"access. Perfect for caching, real-time analytics, or \"\n",
    "                    \"when latency is critical. Data size limited by memory.\"\n",
    "        }\n",
    "        \n",
    "        return explanations.get(best_option, \"No explanation available.\")\n",
    "\n",
    "\n",
    "def demonstrate_decision_matrix():\n",
    "    \"\"\"\n",
    "    Demonstrate the storage decision matrix for NEPSE system.\n",
    "    \"\"\"\n",
    "    matrix = StorageDecisionMatrix()\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Storage Decision Matrix - NEPSE Time-Series Prediction System\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Get recommendations for different use cases relevant to NEPSE\n",
    "    use_cases = ['general', 'high_read', 'analytics', 'archival']\n",
    "    \n",
    "    for use_case in use_cases:\n",
    "        recommendation, explanation = matrix.get_recommendation(use_case)\n",
    "        print(f\"\\n{'-' * 70}\")\n",
    "        print(f\"Use Case: {use_case.upper()}\")\n",
    "        print(f\"{'-' * 70}\")\n",
    "        print(f\"Recommended Storage: {recommendation}\")\n",
    "        print(f\"\\nExplanation: {explanation}\")\n",
    "    \n",
    "    # Show the scoring matrix\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"Complete Scoring Matrix\")\n",
    "    print(\"=\" * 70)\n",
    "    print(matrix.scores.transpose())\n",
    "    \n",
    "    # Calculate and show weighted scores for general use case\n",
    "    print(f\"\\n{'=' * 70}\")\n",
    "    print(\"Weighted Scores (General Use Case)\")\n",
    "    print(\"=\" * 70)\n",
    "    weighted = matrix.calculate_weighted_scores()\n",
    "    print(weighted[['total_score']].round(3))\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_decision_matrix()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **StorageDecisionMatrix Class**: A comprehensive decision-making tool that evaluates storage options against multiple criteria. Each criterion has a weight reflecting its importance.\n",
    "\n",
    "2. **Criteria and Weights**:\n",
    "   - `query_performance` (0.20): How fast can you retrieve data? Critical for prediction systems that need to load historical data frequently.\n",
    "   - `write_performance` (0.15): How fast can you write data? Important for systems with high-frequency data ingestion.\n",
    "   - `storage_efficiency` (0.15): How much disk space does the data occupy? Important for long-term storage of historical data.\n",
    "   - `scalability` (0.15): Can the system grow with your data? Important for systems that will accumulate data over time.\n",
    "   - `ease_of_use` (0.10): How quickly can developers become productive?\n",
    "   - `cost` (0.10): Licensing and operational costs.\n",
    "   - `ecosystem` (0.10): Community support, documentation, and available tools.\n",
    "   - `time_series_features` (0.05): Built-in support for time-series operations.\n",
    "\n",
    "3. **Scoring System**: Each storage option is scored 1-10 on each criterion. These scores are based on typical benchmarks and use cases. For example:\n",
    "   - CSV scores low on storage_efficiency (2) because it's uncompressed text.\n",
    "   - InfluxDB scores high on time_series_features (10) because it's purpose-built for time-series.\n",
    "   - Redis scores high on query_performance (10) because it's in-memory.\n",
    "\n",
    "4. **Use Case Configurations**: Different use cases have different priorities:\n",
    "   - `high_write`: Weights write performance higher for ingestion-heavy systems.\n",
    "   - `high_read`: Weights query performance for analytics-heavy systems.\n",
    "   - `analytics`: Weights storage efficiency for large analytical datasets.\n",
    "   - `archival`: Weights storage efficiency and cost for long-term storage.\n",
    "\n",
    "---\n",
    "\n",
    "## **8.2 File-Based Storage**\n",
    "\n",
    "File-based storage is the simplest and most portable approach for storing time-series data. While not suitable for all use cases, it's often the best starting point and remains relevant for many applications.\n",
    "\n",
    "### **8.2.1 CSV and Flat Files**\n",
    "\n",
    "CSV (Comma-Separated Values) files are the most universal format for tabular data. They're human-readable and supported by virtually every data tool.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "CSV Storage Module for Time-Series Data\n",
    "\n",
    "This module provides comprehensive CSV file operations for time-series\n",
    "data, specifically designed for the NEPSE stock prediction system.\n",
    "\n",
    "CSV Format for NEPSE Data:\n",
    "S.No,Symbol,Conf.,Open,High,Low,Close,LTP,Close - LTP,Close - LTP %,\n",
    "VWAP,Vol,Prev. Close,Turnover,Trans.,Diff,Range,Diff %,Range %,\n",
    "VWAP %,52 Weeks High,52 Weeks Low\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Optional, Dict, Union\n",
    "from pathlib import Path\n",
    "import csv\n",
    "import os\n",
    "\n",
    "\n",
    "class CSVTimeSeriesStorage:\n",
    "    \"\"\"\n",
    "    A class to handle CSV storage operations for time-series data.\n",
    "    \n",
    "    This class provides methods for reading, writing, and managing\n",
    "    CSV files with a focus on time-series data from the NEPSE system.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = './data'):\n",
    "        \"\"\"\n",
    "        Initialize the CSV storage handler.\n",
    "        \n",
    "        Args:\n",
    "            base_path: The base directory for storing CSV files.\n",
    "                      Will be created if it doesn't exist.\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Define the NEPSE column structure\n",
    "        # This maps the CSV column names to internal names and types\n",
    "        self.nepse_columns = {\n",
    "            'S.No': 'int64',\n",
    "            'Symbol': 'str',\n",
    "            'Conf.': 'str',           # Confirmation status\n",
    "            'Open': 'float64',        # Opening price\n",
    "            'High': 'float64',        # High price of the day\n",
    "            'Low': 'float64',         # Low price of the day\n",
    "            'Close': 'float64',       # Closing price\n",
    "            'LTP': 'float64',         # Last Traded Price\n",
    "            'Close - LTP': 'float64', # Difference between close and LTP\n",
    "            'Close - LTP %': 'float64', # Percentage difference\n",
    "            'VWAP': 'float64',        # Volume Weighted Average Price\n",
    "            'Vol': 'int64',           # Volume (number of shares)\n",
    "            'Prev. Close': 'float64', # Previous day's closing price\n",
    "            'Turnover': 'float64',    # Total turnover in NPR\n",
    "            'Trans.': 'int64',        # Number of transactions\n",
    "            'Diff': 'float64',        # Price difference from previous close\n",
    "            'Range': 'float64',       # High - Low\n",
    "            'Diff %': 'float64',      # Percentage change from previous close\n",
    "            'Range %': 'float64',     # Range as percentage\n",
    "            'VWAP %': 'float64',      # VWAP percentage\n",
    "            '52 Weeks High': 'float64',\n",
    "            '52 Weeks Low': 'float64'\n",
    "        }\n",
    "    \n",
    "    def write_data(self, \n",
    "                   data: pd.DataFrame, \n",
    "                   filename: str,\n",
    "                   mode: str = 'w',\n",
    "                   include_index: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Write DataFrame to a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            data: The DataFrame to write\n",
    "            filename: Name of the CSV file (without path)\n",
    "            mode: Write mode - 'w' for write (overwrite), \n",
    "                  'a' for append\n",
    "            include_index: Whether to include the DataFrame index\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = CSVTimeSeriesStorage('./data')\n",
    "            >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "            >>> storage.write_data(df, 'test.csv')\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        # Determine if we need to write the header\n",
    "        # For append mode, check if file exists and has content\n",
    "        header = True\n",
    "        if mode == 'a' and filepath.exists():\n",
    "            # Check if file has content\n",
    "            if filepath.stat().st_size > 0:\n",
    "                header = False\n",
    "        \n",
    "        # Write to CSV\n",
    "        # encoding='utf-8' ensures proper handling of Nepali characters\n",
    "        # if present in stock symbols or company names\n",
    "        data.to_csv(\n",
    "            filepath,\n",
    "            mode=mode,\n",
    "            header=header,\n",
    "            index=include_index,\n",
    "            encoding='utf-8'\n",
    "        )\n",
    "        \n",
    "        print(f\"Data written to {filepath}\")\n",
    "    \n",
    "    def read_data(self, \n",
    "                  filename: str,\n",
    "                  parse_dates: bool = True,\n",
    "                  date_column: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read CSV file into a DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the CSV file\n",
    "            parse_dates: Whether to parse date columns\n",
    "            date_column: Name of the date column to parse\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame containing the CSV data\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = CSVTimeSeriesStorage('./data')\n",
    "            >>> df = storage.read_data('nepse_data.csv')\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            raise FileNotFoundError(f\"File not found: {filepath}\")\n",
    "        \n",
    "        # Read CSV with proper handling\n",
    "        df = pd.read_csv(\n",
    "            filepath,\n",
    "            encoding='utf-8',\n",
    "            parse_dates=[date_column] if date_column and parse_dates else None\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def read_nepse_data(self, \n",
    "                        filename: str,\n",
    "                        add_date_column: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read NEPSE stock data with proper data type handling.\n",
    "        \n",
    "        This method specifically handles the NEPSE CSV format with\n",
    "        all its columns and ensures proper data types.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the NEPSE CSV file\n",
    "            add_date_column: Whether to add a date column based on filename\n",
    "                            (assumes filename contains date like 'nepse_2024-01-15.csv')\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with NEPSE stock data properly typed\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = CSVTimeSeriesStorage('./data')\n",
    "            >>> df = storage.read_nepse_data('nepse_2024-01-15.csv')\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        # Read the CSV file\n",
    "        # na_values specifies strings that should be treated as NaN\n",
    "        # This is important for NEPSE data where missing values might be\n",
    "        # represented as '-', 'N/A', or empty strings\n",
    "        df = pd.read_csv(\n",
    "            filepath,\n",
    "            encoding='utf-8',\n",
    "            na_values=['-', 'N/A', 'NA', '', ' '],\n",
    "            thousands=','  # Handle comma-separated numbers like \"1,234,567\"\n",
    "        )\n",
    "        \n",
    "        # Extract date from filename if pattern matches\n",
    "        # NEPSE data files are often named with dates like:\n",
    "        # nepse_2024-01-15.csv or NEPSE_20240115.csv\n",
    "        if add_date_column:\n",
    "            import re\n",
    "            # Try different date patterns\n",
    "            patterns = [\n",
    "                r'(\\d{4}-\\d{2}-\\d{2})',      # YYYY-MM-DD\n",
    "                r'(\\d{8})',                   # YYYYMMDD\n",
    "                r'(\\d{2}-\\d{2}-\\d{4})',       # DD-MM-YYYY\n",
    "            ]\n",
    "            \n",
    "            for pattern in patterns:\n",
    "                match = re.search(pattern, filename)\n",
    "                if match:\n",
    "                    date_str = match.group(1)\n",
    "                    try:\n",
    "                        if len(date_str) == 8:  # YYYYMMDD\n",
    "                            date = datetime.strptime(date_str, '%Y%m%d')\n",
    "                        elif '-' in date_str and len(date_str.split('-')[0]) == 4:\n",
    "                            date = datetime.strptime(date_str, '%Y-%m-%d')\n",
    "                        else:\n",
    "                            date = datetime.strptime(date_str, '%d-%m-%Y')\n",
    "                        \n",
    "                        df['Date'] = date\n",
    "                        break\n",
    "                    except ValueError:\n",
    "                        continue\n",
    "        \n",
    "        # Apply proper data types to known columns\n",
    "        for col, dtype in self.nepse_columns.items():\n",
    "            if col in df.columns:\n",
    "                if dtype == 'float64':\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce')\n",
    "                elif dtype == 'int64':\n",
    "                    df[col] = pd.to_numeric(df[col], errors='coerce').astype('Int64')\n",
    "                # 'str' type is already handled by default\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def append_daily_data(self, \n",
    "                          new_data: pd.DataFrame,\n",
    "                          master_file: str = 'nepse_master.csv') -> None:\n",
    "        \"\"\"\n",
    "        Append new daily data to a master CSV file.\n",
    "        \n",
    "        This method is designed for incremental updates to the\n",
    "        NEPSE historical data file. It handles duplicates by keeping\n",
    "        the most recent data.\n",
    "        \n",
    "        Args:\n",
    "            new_data: New data to append\n",
    "            master_file: Name of the master file to append to\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = CSVTimeSeriesStorage('./data')\n",
    "            >>> new_data = pd.read_csv('today_nepse.csv')\n",
    "            >>> storage.append_daily_data(new_data)\n",
    "        \"\"\"\n",
    "        master_path = self.base_path / master_file\n",
    "        \n",
    "        if master_path.exists():\n",
    "            # Read existing data\n",
    "            existing_data = self.read_nepse_data(master_file)\n",
    "            \n",
    "            # Combine with new data\n",
    "            combined = pd.concat([existing_data, new_data], ignore_index=True)\n",
    "            \n",
    "            # Remove duplicates, keeping the last occurrence\n",
    "            # This ensures that if the same day's data is uploaded twice,\n",
    "            # we keep the most recent version\n",
    "            if 'Date' in combined.columns and 'Symbol' in combined.columns:\n",
    "                combined = combined.drop_duplicates(\n",
    "                    subset=['Date', 'Symbol'],\n",
    "                    keep='last'\n",
    "                )\n",
    "        else:\n",
    "            combined = new_data\n",
    "        \n",
    "        # Write combined data\n",
    "        self.write_data(combined, master_file)\n",
    "    \n",
    "    def create_partitioned_files(self,\n",
    "                                  data: pd.DataFrame,\n",
    "                                  partition_by: str = 'Symbol',\n",
    "                                  directory: str = 'partitioned') -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Create partitioned CSV files based on a column value.\n",
    "        \n",
    "        Partitioning is useful for:\n",
    "        - Faster queries on specific symbols\n",
    "        - Parallel processing of different stocks\n",
    "        - Organized data storage\n",
    "        \n",
    "        Args:\n",
    "            data: DataFrame to partition\n",
    "            partition_by: Column to partition by (e.g., 'Symbol')\n",
    "            directory: Subdirectory name for partitioned files\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping partition values to file paths\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = CSVTimeSeriesStorage('./data')\n",
    "            >>> df = storage.read_nepse_data('nepse_master.csv')\n",
    "            >>> partitions = storage.create_partitioned_files(df)\n",
    "            >>> print(partitions)\n",
    "            {'NABIL': 'partitioned/NABIL.csv', 'NICA': 'partitioned/NICA.csv', ...}\n",
    "        \"\"\"\n",
    "        partition_dir = self.base_path / directory\n",
    "        partition_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        partitions = {}\n",
    "        \n",
    "        # Group by the partition column\n",
    "        for partition_value, group in data.groupby(partition_by):\n",
    "            # Create safe filename (replace invalid characters)\n",
    "            safe_name = str(partition_value).replace('/', '_').replace('\\\\', '_')\n",
    "            filename = f\"{safe_name}.csv\"\n",
    "            filepath = partition_dir / filename\n",
    "            \n",
    "            # Write partition file\n",
    "            group.to_csv(filepath, index=False, encoding='utf-8')\n",
    "            \n",
    "            partitions[partition_value] = str(filepath)\n",
    "        \n",
    "        print(f\"Created {len(partitions)} partitioned files in {partition_dir}\")\n",
    "        return partitions\n",
    "    \n",
    "    def get_file_info(self, filename: str) -> Dict:\n",
    "        \"\"\"\n",
    "        Get information about a CSV file.\n",
    "        \n",
    "        Args:\n",
    "            filename: Name of the CSV file\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with file information\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        if not filepath.exists():\n",
    "            return {'exists': False}\n",
    "        \n",
    "        # Get file stats\n",
    "        stat = filepath.stat()\n",
    "        \n",
    "        # Read first few lines to get column info\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            reader = csv.reader(f)\n",
    "            header = next(reader)\n",
    "            first_row = next(reader, None)\n",
    "        \n",
    "        # Count total rows (excluding header)\n",
    "        with open(filepath, 'r', encoding='utf-8') as f:\n",
    "            row_count = sum(1 for _ in f) - 1  # Subtract header\n",
    "        \n",
    "        return {\n",
    "            'exists': True,\n",
    "            'path': str(filepath),\n",
    "            'size_bytes': stat.st_size,\n",
    "            'size_mb': stat.st_size / (1024 * 1024),\n",
    "            'modified': datetime.fromtimestamp(stat.st_mtime),\n",
    "            'columns': header,\n",
    "            'column_count': len(header),\n",
    "            'row_count': row_count,\n",
    "            'first_row': first_row\n",
    "        }\n",
    "    \n",
    "    def merge_csv_files(self, \n",
    "                        file_pattern: str = '*.csv',\n",
    "                        output_file: str = 'merged.csv') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Merge multiple CSV files into a single file.\n",
    "        \n",
    "        This is useful for combining daily NEPSE data files into\n",
    "        a single historical file.\n",
    "        \n",
    "        Args:\n",
    "            file_pattern: Glob pattern to match files\n",
    "            output_file: Name of the output merged file\n",
    "        \n",
    "        Returns:\n",
    "            Merged DataFrame\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = CSVTimeSeriesStorage('./data')\n",
    "            >>> merged = storage.merge_csv_files('nepse_*.csv', 'all_nepse.csv')\n",
    "        \"\"\"\n",
    "        files = list(self.base_path.glob(file_pattern))\n",
    "        \n",
    "        if not files:\n",
    "            print(f\"No files matching pattern: {file_pattern}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Read all files\n",
    "        dfs = []\n",
    "        for file in files:\n",
    "            try:\n",
    "                df = self.read_nepse_data(file.name)\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Merge all DataFrames\n",
    "        merged = pd.concat(dfs, ignore_index=True)\n",
    "        \n",
    "        # Sort by date and symbol if columns exist\n",
    "        if 'Date' in merged.columns:\n",
    "            merged = merged.sort_values(['Date', 'Symbol'] \n",
    "                                        if 'Symbol' in merged.columns \n",
    "                                        else ['Date'])\n",
    "        \n",
    "        # Write merged file\n",
    "        self.write_data(merged, output_file)\n",
    "        \n",
    "        print(f\"Merged {len(files)} files into {output_file}\")\n",
    "        print(f\"Total rows: {len(merged)}\")\n",
    "        \n",
    "        return merged\n",
    "\n",
    "\n",
    "def demonstrate_csv_operations():\n",
    "    \"\"\"\n",
    "    Demonstrate CSV storage operations with NEPSE example data.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"CSV Storage Operations Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize storage\n",
    "    storage = CSVTimeSeriesStorage('./nepse_data')\n",
    "    \n",
    "    # Create sample NEPSE data for demonstration\n",
    "    # This simulates the actual NEPSE data format\n",
    "    sample_data = pd.DataFrame({\n",
    "        'S.No': range(1, 6),\n",
    "        'Symbol': ['NABIL', 'NICA', 'SCBL', 'ADBL', 'EBL'],\n",
    "        'Conf.': ['Confirmed'] * 5,\n",
    "        'Open': [850.0, 780.0, 520.0, 340.0, 290.0],\n",
    "        'High': [870.0, 795.0, 535.0, 350.0, 305.0],\n",
    "        'Low': [845.0, 775.0, 515.0, 335.0, 285.0],\n",
    "        'Close': [865.0, 790.0, 530.0, 345.0, 300.0],\n",
    "        'LTP': [865.0, 790.0, 530.0, 345.0, 300.0],\n",
    "        'Close - LTP': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        'Close - LTP %': [0.0, 0.0, 0.0, 0.0, 0.0],\n",
    "        'VWAP': [860.5, 785.2, 525.8, 342.3, 295.6],\n",
    "        'Vol': [125000, 98000, 76000, 145000, 89000],\n",
    "        'Prev. Close': [850.0, 775.0, 520.0, 335.0, 288.0],\n",
    "        'Turnover': [107562500.0, 76949600.0, 39960800.0, 49633500.0, 26308400.0],\n",
    "        'Trans.': [850, 620, 480, 920, 580],\n",
    "        'Diff': [15.0, 15.0, 10.0, 10.0, 12.0],\n",
    "        'Range': [25.0, 20.0, 20.0, 15.0, 20.0],\n",
    "        'Diff %': [1.76, 1.94, 1.92, 2.99, 4.17],\n",
    "        'Range %': [2.94, 2.58, 3.85, 4.48, 6.90],\n",
    "        'VWAP %': [1.24, 1.32, 1.12, 2.18, 2.64],\n",
    "        '52 Weeks High': [920.0, 850.0, 580.0, 380.0, 340.0],\n",
    "        '52 Weeks Low': [650.0, 580.0, 380.0, 240.0, 195.0]\n",
    "    })\n",
    "    \n",
    "    # Add date column for time-series context\n",
    "    sample_data['Date'] = pd.Timestamp('2024-01-15')\n",
    "    \n",
    "    print(\"\\n1. Writing NEPSE Data to CSV\")\n",
    "    print(\"-\" * 40)\n",
    "    storage.write_data(sample_data, 'nepse_2024-01-15.csv')\n",
    "    \n",
    "    print(\"\\n2. Reading and Displaying File Info\")\n",
    "    print(\"-\" * 40)\n",
    "    info = storage.get_file_info('nepse_2024-01-15.csv')\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    print(\"\\n3. Reading NEPSE Data\")\n",
    "    print(\"-\" * 40)\n",
    "    df = storage.read_nepse_data('nepse_2024-01-15.csv')\n",
    "    print(df.head())\n",
    "    print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "    \n",
    "    print(\"\\n4. Creating Partitioned Files by Symbol\")\n",
    "    print(\"-\" * 40)\n",
    "    partitions = storage.create_partitioned_files(sample_data, partition_by='Symbol')\n",
    "    print(f\"Created partitions: {list(partitions.keys())[:5]}...\")\n",
    "    \n",
    "    return storage, sample_data\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    storage, data = demonstrate_csv_operations()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **CSVTimeSeriesStorage Class**: A comprehensive class for managing CSV files for time-series data. It provides methods for reading, writing, partitioning, and merging CSV files.\n",
    "\n",
    "2. **Column Type Handling**: The `nepse_columns` dictionary defines the expected data types for each column in the NEPSE data format. This ensures:\n",
    "   - Price columns (`Open`, `High`, `Low`, `Close`) are `float64`\n",
    "   - Volume columns (`Vol`, `Trans.`) are `int64`\n",
    "   - Symbol column is string type\n",
    "\n",
    "3. **Date Extraction from Filename**: The `read_nepse_data` method extracts dates from filenames using regular expressions. This handles different date formats:\n",
    "   - `nepse_2024-01-15.csv` (YYYY-MM-DD)\n",
    "   - `nepse_20240115.csv` (YYYYMMDD)\n",
    "   - `nepse_15-01-2024.csv` (DD-MM-YYYY)\n",
    "\n",
    "4. **Handling Missing Values**: The `na_values` parameter in `pd.read_csv` converts various representations of missing data ('-', 'N/A', empty strings) to NaN values.\n",
    "\n",
    "5. **Thousands Separator**: The `thousands=','` parameter handles numbers formatted with commas (e.g., \"1,234,567\").\n",
    "\n",
    "6. **Partitioning**: The `create_partitioned_files` method creates separate CSV files for each stock symbol. This is useful because:\n",
    "   - Queries for a single stock only need to read one small file\n",
    "   - Parallel processing is easier\n",
    "   - Data management is simpler\n",
    "\n",
    "7. **Duplicate Handling**: The `append_daily_data` method handles duplicates by keeping the most recent entry. This prevents data duplication when the same day's data is uploaded multiple times.\n",
    "\n",
    "### **8.2.2 Parquet and Feather**\n",
    "\n",
    "Parquet and Feather are columnar storage formats that provide better compression and faster read performance compared to CSV, especially for analytical workloads.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Parquet and Feather Storage Module for Time-Series Data\n",
    "\n",
    "This module demonstrates efficient columnar storage formats for\n",
    "time-series data, particularly suited for the NEPSE prediction system.\n",
    "\n",
    "Key advantages over CSV:\n",
    "- Better compression (smaller files)\n",
    "- Faster reads (column pruning)\n",
    "- Preserves data types (no type inference needed)\n",
    "- Better for analytical queries\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import pyarrow.feather as feather\n",
    "import os\n",
    "\n",
    "\n",
    "class ColumnarStorage:\n",
    "    \"\"\"\n",
    "    A class to handle Parquet and Feather file operations for time-series data.\n",
    "    \n",
    "    Both formats are columnar storage formats optimized for analytical workloads.\n",
    "    - Parquet: Compressed, good for long-term storage and sharing\n",
    "    - Feather: Uncompressed (or lightly compressed), good for temporary storage\n",
    "               and inter-process communication\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = './data'):\n",
    "        \"\"\"\n",
    "        Initialize the columnar storage handler.\n",
    "        \n",
    "        Args:\n",
    "            base_path: Base directory for storing files\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def write_parquet(self,\n",
    "                      df: pd.DataFrame,\n",
    "                      filename: str,\n",
    "                      compression: str = 'snappy',\n",
    "                      partition_cols: List[str] = None,\n",
    "                      engine: str = 'pyarrow') -> None:\n",
    "        \"\"\"\n",
    "        Write DataFrame to Parquet format.\n",
    "        \n",
    "        Parquet is a columnar storage format that provides:\n",
    "        - Efficient compression (reduces file size by 70-90% vs CSV)\n",
    "        - Column pruning (only reads needed columns)\n",
    "        - Predicate pushdown (filters data at file level)\n",
    "        - Schema preservation (data types are stored)\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to write\n",
    "            filename: Output filename (should end with .parquet)\n",
    "            compression: Compression algorithm\n",
    "                        'snappy' - Fast compression/decompression (default)\n",
    "                        'gzip' - Better compression, slower\n",
    "                        'brotli' - Best compression, slowest\n",
    "                        'none' - No compression\n",
    "            partition_cols: Columns to partition by (creates directory structure)\n",
    "            engine: Parquet engine ('pyarrow' or 'fastparquet')\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = ColumnarStorage('./data')\n",
    "            >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "            >>> storage.write_parquet(df, 'data.parquet')\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        if partition_cols:\n",
    "            # Write partitioned dataset\n",
    "            # This creates a directory structure like:\n",
    "            # data.parquet/Symbol=NABIL/part-0.parquet\n",
    "            # data.parquet/Symbol=NICA/part-0.parquet\n",
    "            pq.write_to_dataset(\n",
    "                pa.Table.from_pandas(df),\n",
    "                root_path=str(filepath),\n",
    "                partition_cols=partition_cols,\n",
    "                compression=compression\n",
    "            )\n",
    "            print(f\"Partitioned Parquet dataset written to {filepath}\")\n",
    "        else:\n",
    "            # Write single file\n",
    "            df.to_parquet(\n",
    "                filepath,\n",
    "                engine=engine,\n",
    "                compression=compression,\n",
    "                index=False\n",
    "            )\n",
    "            print(f\"Parquet file written to {filepath}\")\n",
    "    \n",
    "    def read_parquet(self,\n",
    "                     filename: str,\n",
    "                     columns: List[str] = None,\n",
    "                     filters: List[tuple] = None,\n",
    "                     engine: str = 'pyarrow') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read DataFrame from Parquet format.\n",
    "        \n",
    "        Args:\n",
    "            filename: Parquet file or directory name\n",
    "            columns: List of columns to read (column pruning)\n",
    "                    If None, reads all columns\n",
    "            filters: Row filters to apply (predicate pushdown)\n",
    "                    Format: [('column', 'operator', value), ...]\n",
    "                    Example: [('Symbol', '==', 'NABIL'), ('Close', '>', 800)]\n",
    "            engine: Parquet engine ('pyarrow' or 'fastparquet')\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with the requested data\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = ColumnarStorage('./data')\n",
    "            >>> df = storage.read_parquet('data.parquet', \n",
    "            ...                           columns=['Symbol', 'Close'],\n",
    "            ...                           filters=[('Symbol', '==', 'NABIL')])\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        if filters:\n",
    "            # Convert filters to PyArrow format\n",
    "            # PyArrow uses expressions like: (pa.dataset.field('col') == value)\n",
    "            # The filters parameter in read_table accepts list of tuples\n",
    "            table = pq.read_table(\n",
    "                filepath,\n",
    "                columns=columns,\n",
    "                filters=filters\n",
    "            )\n",
    "            return table.to_pandas()\n",
    "        else:\n",
    "            return pd.read_parquet(\n",
    "                filepath,\n",
    "                engine=engine,\n",
    "                columns=columns\n",
    "            )\n",
    "    \n",
    "    def write_feather(self,\n",
    "                      df: pd.DataFrame,\n",
    "                      filename: str,\n",
    "                      compression: bool = True) -> None:\n",
    "        \"\"\"\n",
    "        Write DataFrame to Feather format.\n",
    "        \n",
    "        Feather (Arrow IPC) is designed for:\n",
    "        - Fast read/write speeds\n",
    "        - Minimal CPU overhead\n",
    "        - Inter-process communication\n",
    "        - Temporary data storage\n",
    "        \n",
    "        Feather is ideal when you need to quickly save data and read it back\n",
    "        in the same or another Python process. It's faster than Parquet but\n",
    "        typically produces larger files.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to write\n",
    "            filename: Output filename (should end with .feather or .arrow)\n",
    "            compression: Whether to use compression\n",
    "                        True - Uses LZ4 compression (fast, moderate compression)\n",
    "                        False - No compression (fastest, largest files)\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = ColumnarStorage('./data')\n",
    "            >>> df = pd.DataFrame({'A': [1, 2], 'B': [3, 4]})\n",
    "            >>> storage.write_feather(df, 'data.feather')\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        # Feather format using pyarrow\n",
    "        # compression='lz4' provides fast compression with good speed\n",
    "        df.to_feather(\n",
    "            filepath,\n",
    "            compression='lz4' if compression else None\n",
    "        )\n",
    "        print(f\"Feather file written to {filepath}\")\n",
    "    \n",
    "    def read_feather(self,\n",
    "                     filename: str,\n",
    "                     columns: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read DataFrame from Feather format.\n",
    "        \n",
    "        Args:\n",
    "            filename: Feather file name\n",
    "            columns: List of columns to read (column pruning)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with the requested data\n",
    "        \n",
    "        Example:\n",
    "            >>> storage = ColumnarStorage('./data')\n",
    "            >>> df = storage.read_feather('data.feather')\n",
    "        \"\"\"\n",
    "        filepath = self.base_path / filename\n",
    "        \n",
    "        # Feather supports column pruning for efficient reading\n",
    "        return pd.read_feather(filepath, columns=columns)\n",
    "    \n",
    "    def compare_formats(self,\n",
    "                        df: pd.DataFrame,\n",
    "                        name: str = 'data') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Compare CSV, Parquet, and Feather formats for the given DataFrame.\n",
    "        \n",
    "        This method saves the same data in all three formats and compares:\n",
    "        - File size\n",
    "        - Write time\n",
    "        - Read time\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to use for comparison\n",
    "            name: Base name for files\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with comparison results\n",
    "        \"\"\"\n",
    "        import time\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        # CSV\n",
    "        csv_path = self.base_path / f\"{name}.csv\"\n",
    "        start = time.time()\n",
    "        df.to_csv(csv_path, index=False)\n",
    "        csv_write_time = time.time() - start\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = pd.read_csv(csv_path)\n",
    "        csv_read_time = time.time() - start\n",
    "        \n",
    "        csv_size = os.path.getsize(csv_path) / 1024  # KB\n",
    "        \n",
    "        results.append({\n",
    "            'format': 'CSV',\n",
    "            'size_kb': csv_size,\n",
    "            'write_time_ms': csv_write_time * 1000,\n",
    "            'read_time_ms': csv_read_time * 1000,\n",
    "            'compression_ratio': 1.0\n",
    "        })\n",
    "        \n",
    "        # Parquet (snappy compression)\n",
    "        parquet_path = self.base_path / f\"{name}.parquet\"\n",
    "        start = time.time()\n",
    "        df.to_parquet(parquet_path, compression='snappy', index=False)\n",
    "        parquet_write_time = time.time() - start\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = pd.read_parquet(parquet_path)\n",
    "        parquet_read_time = time.time() - start\n",
    "        \n",
    "        parquet_size = os.path.getsize(parquet_path) / 1024\n",
    "        \n",
    "        results.append({\n",
    "            'format': 'Parquet (snappy)',\n",
    "            'size_kb': parquet_size,\n",
    "            'write_time_ms': parquet_write_time * 1000,\n",
    "            'read_time_ms': parquet_read_time * 1000,\n",
    "            'compression_ratio': csv_size / parquet_size\n",
    "        })\n",
    "        \n",
    "        # Feather (with compression)\n",
    "        feather_path = self.base_path / f\"{name}.feather\"\n",
    "        start = time.time()\n",
    "        df.to_feather(feather_path, compression='lz4')\n",
    "        feather_write_time = time.time() - start\n",
    "        \n",
    "        start = time.time()\n",
    "        _ = pd.read_feather(feather_path)\n",
    "        feather_read_time = time.time() - start\n",
    "        \n",
    "        feather_size = os.path.getsize(feather_path) / 1024\n",
    "        \n",
    "        results.append({\n",
    "            'format': 'Feather (lz4)',\n",
    "            'size_kb': feather_size,\n",
    "            'write_time_ms': feather_write_time * 1000,\n",
    "            'read_time_ms': feather_read_time * 1000,\n",
    "            'compression_ratio': csv_size / feather_size\n",
    "        })\n",
    "        \n",
    "        return pd.DataFrame(results)\n",
    "\n",
    "\n",
    "class NEPSEParquetManager:\n",
    "    \"\"\"\n",
    "    Specialized class for managing NEPSE stock data in Parquet format.\n",
    "    \n",
    "    This class provides methods optimized for the NEPSE data structure,\n",
    "    including efficient querying by symbol, date range, and other criteria.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = './nepse_parquet'):\n",
    "        \"\"\"\n",
    "        Initialize the NEPSE Parquet manager.\n",
    "        \n",
    "        Args:\n",
    "            base_path: Base directory for Parquet files\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "        self.storage = ColumnarStorage(base_path)\n",
    "        \n",
    "        # Define the schema for NEPSE data\n",
    "        # This ensures consistent data types across all files\n",
    "        self.schema = pa.schema([\n",
    "            ('S.No', pa.int64()),\n",
    "            ('Symbol', pa.string()),\n",
    "            ('Conf.', pa.string()),\n",
    "            ('Open', pa.float64()),\n",
    "            ('High', pa.float64()),\n",
    "            ('Low', pa.float64()),\n",
    "            ('Close', pa.float64()),\n",
    "            ('LTP', pa.float64()),\n",
    "            ('Close_LTP', pa.float64()),\n",
    "            ('Close_LTP_Pct', pa.float64()),\n",
    "            ('VWAP', pa.float64()),\n",
    "            ('Vol', pa.int64()),\n",
    "            ('Prev_Close', pa.float64()),\n",
    "            ('Turnover', pa.float64()),\n",
    "            ('Trans', pa.int64()),\n",
    "            ('Diff', pa.float64()),\n",
    "            ('Range', pa.float64()),\n",
    "            ('Diff_Pct', pa.float64()),\n",
    "            ('Range_Pct', pa.float64()),\n",
    "            ('VWAP_Pct', pa.float64()),\n",
    "            ('High_52W', pa.float64()),\n",
    "            ('Low_52W', pa.float64()),\n",
    "            ('Date', pa.date64())\n",
    "        ])\n",
    "    \n",
    "    def save_daily_data(self,\n",
    "                        df: pd.DataFrame,\n",
    "                        date: str,\n",
    "                        mode: str = 'append') -> None:\n",
    "        \"\"\"\n",
    "        Save daily NEPSE data to partitioned Parquet files.\n",
    "        \n",
    "        This method partitions data by:\n",
    "        - Year (for efficient date range queries)\n",
    "        - Symbol (for efficient symbol-specific queries)\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with daily stock data\n",
    "            date: Date string (YYYY-MM-DD format)\n",
    "            mode: 'append' to add to existing, 'overwrite' to replace\n",
    "        \"\"\"\n",
    "        # Parse date and extract year\n",
    "        date_obj = pd.to_datetime(date)\n",
    "        year = date_obj.year\n",
    "        \n",
    "        # Add date column if not present\n",
    "        if 'Date' not in df.columns:\n",
    "            df = df.copy()\n",
    "            df['Date'] = date_obj\n",
    "        \n",
    "        # Clean column names (replace special characters)\n",
    "        df.columns = df.columns.str.replace(' ', '_')\n",
    "        df.columns = df.columns.str.replace('-', '_')\n",
    "        df.columns = df.columns.str.replace('.', '')\n",
    "        df.columns = df.columns.str.replace('%', 'Pct')\n",
    "        \n",
    "        # Define partition path\n",
    "        partition_path = self.base_path / f\"year={year}\"\n",
    "        \n",
    "        if mode == 'overwrite' and partition_path.exists():\n",
    "            import shutil\n",
    "            shutil.rmtree(partition_path)\n",
    "        \n",
    "        partition_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Write to Parquet with symbol partitioning\n",
    "        # This creates files like: year=2024/Symbol=NABIL/data.parquet\n",
    "        self.storage.write_parquet(\n",
    "            df,\n",
    "            f\"year={year}/data.parquet\",\n",
    "            compression='snappy',\n",
    "            partition_cols=['Symbol']\n",
    "        )\n",
    "    \n",
    "    def query_by_symbol(self,\n",
    "                        symbol: str,\n",
    "                        columns: List[str] = None,\n",
    "                        start_date: str = None,\n",
    "                        end_date: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query historical data for a specific symbol.\n",
    "        \n",
    "        This method efficiently retrieves data for a single stock by:\n",
    "        1. Using column pruning to read only needed columns\n",
    "        2. Using predicate pushdown to filter at the file level\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol (e.g., 'NABIL')\n",
    "            columns: Columns to retrieve (None for all)\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with historical data for the symbol\n",
    "        \"\"\"\n",
    "        # Build filters\n",
    "        filters = [('Symbol', '==', symbol)]\n",
    "        \n",
    "        if start_date:\n",
    "            start = pd.to_datetime(start_date)\n",
    "            filters.append(('Date', '>=', start))\n",
    "        \n",
    "        if end_date:\n",
    "            end = pd.to_datetime(end_date)\n",
    "            filters.append(('Date', '<=', end))\n",
    "        \n",
    "        # Read all parquet files\n",
    "        dfs = []\n",
    "        for year_dir in sorted(self.base_path.glob('year=*')):\n",
    "            try:\n",
    "                df = self.storage.read_parquet(\n",
    "                    year_dir.name + '/data.parquet',\n",
    "                    columns=columns,\n",
    "                    filters=filters\n",
    "                )\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Warning: Could not read {year_dir}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if not dfs:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        result = pd.concat(dfs, ignore_index=True)\n",
    "        return result.sort_values('Date')\n",
    "    \n",
    "    def query_by_date_range(self,\n",
    "                            start_date: str,\n",
    "                            end_date: str,\n",
    "                            columns: List[str] = None,\n",
    "                            symbols: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query data for a date range across all or specific symbols.\n",
    "        \n",
    "        Args:\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            columns: Columns to retrieve (None for all)\n",
    "            symbols: List of symbols to filter (None for all)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with data for the specified date range\n",
    "        \"\"\"\n",
    "        start = pd.to_datetime(start_date)\n",
    "        end = pd.to_datetime(end_date)\n",
    "        \n",
    "        # Determine which year directories to read\n",
    "        years = range(start.year, end.year + 1)\n",
    "        \n",
    "        dfs = []\n",
    "        for year in years:\n",
    "            year_dir = self.base_path / f\"year={year}\"\n",
    "            if year_dir.exists():\n",
    "                try:\n",
    "                    df = self.storage.read_parquet(\n",
    "                        f\"year={year}/data.parquet\",\n",
    "                        columns=columns\n",
    "                    )\n",
    "                    \n",
    "                    # Filter by date range\n",
    "                    df['Date'] = pd.to_datetime(df['Date'])\n",
    "                    df = df[(df['Date'] >= start) & (df['Date'] <= end)]\n",
    "                    \n",
    "                    # Filter by symbols if specified\n",
    "                    if symbols:\n",
    "                        df = df[df['Symbol'].isin(symbols)]\n",
    "                    \n",
    "                    dfs.append(df)\n",
    "                except Exception as e:\n",
    "                    print(f\"Warning: Could not read {year_dir}: {e}\")\n",
    "                    continue\n",
    "        \n",
    "        if not dfs:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        result = pd.concat(dfs, ignore_index=True)\n",
    "        return result.sort_values(['Date', 'Symbol'])\n",
    "    \n",
    "    def get_all_symbols(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Get list of all unique symbols in the dataset.\n",
    "        \n",
    "        Returns:\n",
    "            Sorted list of stock symbols\n",
    "        \"\"\"\n",
    "        symbols = set()\n",
    "        \n",
    "        for year_dir in self.base_path.glob('year=*/Symbol=*'):\n",
    "            # Extract symbol from directory name\n",
    "            symbol = year_dir.name.split('=')[1]\n",
    "            symbols.add(symbol)\n",
    "        \n",
    "        return sorted(list(symbols))\n",
    "    \n",
    "    def get_statistics(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get statistics about the stored data.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with statistics\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'years': [],\n",
    "            'total_files': 0,\n",
    "            'total_size_mb': 0,\n",
    "            'symbols': self.get_all_symbols()\n",
    "        }\n",
    "        \n",
    "        for year_dir in sorted(self.base_path.glob('year=*')):\n",
    "            year = year_dir.name.split('=')[1]\n",
    "            stats['years'].append(year)\n",
    "            \n",
    "            # Count files and calculate size\n",
    "            for parquet_file in year_dir.rglob('*.parquet'):\n",
    "                stats['total_files'] += 1\n",
    "                stats['total_size_mb'] += parquet_file.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        stats['total_size_mb'] = round(stats['total_size_mb'], 2)\n",
    "        \n",
    "        return stats\n",
    "\n",
    "\n",
    "def demonstrate_parquet_operations():\n",
    "    \"\"\"\n",
    "    Demonstrate Parquet and Feather operations with NEPSE data.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Parquet and Feather Storage Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize storage\n",
    "    storage = ColumnarStorage('./nepse_columnar')\n",
    "    \n",
    "    # Create sample NEPSE data (larger dataset for meaningful comparison)\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Generate 1 year of daily data for 5 stocks\n",
    "    dates = pd.date_range('2023-01-01', '2023-12-31', freq='B')  # Business days\n",
    "    symbols = ['NABIL', 'NICA', 'SCBL', 'ADBL', 'EBL']\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for i, symbol in enumerate(symbols):\n",
    "            base_price = 500 + i * 100 + np.random.randint(-50, 50)\n",
    "            high = base_price * (1 + np.random.uniform(0, 0.05))\n",
    "            low = base_price * (1 - np.random.uniform(0, 0.05))\n",
    "            open_price = np.random.uniform(low, high)\n",
    "            close = np.random.uniform(low, high)\n",
    "            \n",
    "            data.append({\n",
    "                'Symbol': symbol,\n",
    "                'Date': date,\n",
    "                'Open': round(open_price, 2),\n",
    "                'High': round(high, 2),\n",
    "                'Low': round(low, 2),\n",
    "                'Close': round(close, 2),\n",
    "                'Volume': np.random.randint(10000, 500000),\n",
    "                'Turnover': round(close * np.random.randint(10000, 500000), 2)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"\\nGenerated {len(df)} records for {len(symbols)} stocks over {len(dates)} trading days\")\n",
    "    \n",
    "    # Compare formats\n",
    "    print(\"\\n1. Comparing Storage Formats\")\n",
    "    print(\"-\" * 40)\n",
    "    comparison = storage.compare_formats(df, 'nepse_sample')\n",
    "    print(comparison.to_string(index=False))\n",
    "    \n",
    "    print(\"\\n2. Writing Partitioned Parquet\")\n",
    "    print(\"-\" * 40)\n",
    "    storage.write_parquet(df, 'nepse_partitioned.parquet', \n",
    "                          partition_cols=['Symbol'])\n",
    "    \n",
    "    print(\"\\n3. Reading Specific Columns (Column Pruning)\")\n",
    "    print(\"-\" * 40)\n",
    "    # Only read needed columns - much faster for wide tables\n",
    "    selected_cols = ['Symbol', 'Date', 'Close', 'Volume']\n",
    "    df_selected = storage.read_parquet('nepse_sample.parquet', \n",
    "                                        columns=selected_cols)\n",
    "    print(f\"Read only {len(selected_cols)} columns:\")\n",
    "    print(df_selected.head())\n",
    "    \n",
    "    print(\"\\n4. Reading with Filters (Predicate Pushdown)\")\n",
    "    print(\"-\" * 40)\n",
    "    # Filter at file level - only reads matching rows\n",
    "    filtered = storage.read_parquet(\n",
    "        'nepse_sample.parquet',\n",
    "        columns=['Symbol', 'Date', 'Close'],\n",
    "        filters=[('Symbol', '==', 'NABIL')]\n",
    "    )\n",
    "    print(f\"Filtered data for NABIL ({len(filtered)} rows):\")\n",
    "    print(filtered.head())\n",
    "    \n",
    "    # Demonstrate NEPSE manager\n",
    "    print(\"\\n5. NEPSE Parquet Manager\")\n",
    "    print(\"-\" * 40)\n",
    "    manager = NEPSEParquetManager('./nepse_parquet_data')\n",
    "    \n",
    "    # Save data partitioned by year and symbol\n",
    "    for date in df['Date'].unique()[:5]:  # Save first 5 days as example\n",
    "        daily_df = df[df['Date'] == date]\n",
    "        manager.save_daily_data(daily_df, date.strftime('%Y-%m-%d'))\n",
    "    \n",
    "    stats = manager.get_statistics()\n",
    "    print(f\"Statistics: {stats}\")\n",
    "    \n",
    "    return storage, df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    storage, df = demonstrate_parquet_operations()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Parquet Format**: A columnar storage format developed by the Apache project. Key features:\n",
    "   - **Column pruning**: When you only need certain columns, Parquet reads only those columns from disk, skipping others entirely.\n",
    "   - **Predicate pushdown**: Filters are applied at the file level before reading data, dramatically reducing I/O.\n",
    "   - **Compression**: Parquet uses efficient compression algorithms (snappy, gzip, brotli). Each column is compressed separately, leading to better compression ratios because data in a column is often similar.\n",
    "   - **Schema preservation**: Data types are stored in the file, eliminating the need for type inference.\n",
    "\n",
    "2. **Feather Format**: Also known as Apache Arrow IPC format:\n",
    "   - Designed for fast read/write speeds\n",
    "   - Uses memory-mapped files for efficient reading\n",
    "   - Minimal CPU overhead\n",
    "   - Ideal for temporary storage and inter-process communication\n",
    "   - Less compressed than Parquet but faster to read/write\n",
    "\n",
    "3. **Partitioning**: The code demonstrates two levels of partitioning:\n",
    "   - **Year partitioning**: Data is organized by year (e.g., `year=2024/`), making date range queries efficient.\n",
    "   - **Symbol partitioning**: Within each year, data is partitioned by stock symbol, making symbol-specific queries very fast.\n",
    "\n",
    "4. **Comparison Results**: Typically you'll see:\n",
    "   - CSV is largest (no compression)\n",
    "   - Parquet is smallest (good compression, typically 3-5x smaller than CSV)\n",
    "   - Feather is medium-sized (light compression, fastest read/write)\n",
    "\n",
    "5. **NEPSEParquetManager**: A specialized class for managing NEPSE data:\n",
    "   - `save_daily_data`: Saves daily data with automatic partitioning\n",
    "   - `query_by_symbol`: Efficiently retrieves historical data for one stock\n",
    "   - `query_by_date_range`: Retrieves data across multiple symbols for a time period\n",
    "   - `get_statistics`: Provides metadata about stored data\n",
    "\n",
    "---\n",
    "\n",
    "## **8.2.3 HDF5 and NetCDF**\n",
    "\n",
    "HDF5 (Hierarchical Data Format version 5) and NetCDF (Network Common Data Form) are binary file formats designed for scientific and numerical data. They excel at storing large multidimensional arrays with metadata.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "HDF5 and NetCDF Storage Module for Time-Series Data\n",
    "\n",
    "HDF5 is particularly well-suited for time-series data because:\n",
    "- Efficient storage of numerical arrays\n",
    "- Supports hierarchical organization (like a file system)\n",
    "- Allows metadata storage alongside data\n",
    "- Supports chunking and compression\n",
    "- Fast random access to subsets of data\n",
    "\n",
    "For NEPSE data, HDF5 provides an excellent balance between:\n",
    "- File size (good compression)\n",
    "- Read performance (fast partial reads)\n",
    "- Query capability (can read specific time ranges)\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import h5py\n",
    "from pathlib import Path\n",
    "from typing import List, Optional, Dict, Any, Union, Tuple\n",
    "from datetime import datetime, timedelta\n",
    "import warnings\n",
    "\n",
    "\n",
    "class HDF5TimeSeriesStorage:\n",
    "    \"\"\"\n",
    "    A class to handle HDF5 storage operations for time-series data.\n",
    "    \n",
    "    HDF5 organizes data in a hierarchical structure similar to a file system:\n",
    "    - Groups (like directories) - can contain other groups or datasets\n",
    "    - Datasets (like files) - contain the actual data arrays\n",
    "    - Attributes (like metadata) - can be attached to groups or datasets\n",
    "    \n",
    "    For NEPSE data, we organize the HDF5 file as:\n",
    "    /nepse/\n",
    "        /metadata/           - Global metadata about the dataset\n",
    "        /prices/             - Price data for all stocks\n",
    "            /NABIL/          - Per-stock group\n",
    "            /NICA/\n",
    "            ...\n",
    "        /volumes/            - Volume data organized similarly\n",
    "        /index/              - Time index for all data\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath: str = './nepse_data.h5'):\n",
    "        \"\"\"\n",
    "        Initialize the HDF5 storage handler.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the HDF5 file (.h5 or .hdf5 extension)\n",
    "        \"\"\"\n",
    "        self.filepath = Path(filepath)\n",
    "        self.filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Compression settings for HDF5\n",
    "        # 'gzip' provides good compression and is widely supported\n",
    "        # 'lzf' is faster but lower compression\n",
    "        # 'szip' is fast but requires special installation\n",
    "        self.compression = 'gzip'\n",
    "        self.compression_level = 4  # 1-9, higher = more compression but slower\n",
    "    \n",
    "    def _get_file(self, mode: str = 'a') -> h5py.File:\n",
    "        \"\"\"\n",
    "        Get an HDF5 file handle.\n",
    "        \n",
    "        Args:\n",
    "            mode: File mode\n",
    "                  'r' - read only\n",
    "                  'a' - read/write, create if doesn't exist (default)\n",
    "                  'w' - write, overwrite if exists\n",
    "                  'r+' - read/write, file must exist\n",
    "        \n",
    "        Returns:\n",
    "            HDF5 file object\n",
    "        \"\"\"\n",
    "        return h5py.File(self.filepath, mode)\n",
    "    \n",
    "    def store_stock_data(self,\n",
    "                         df: pd.DataFrame,\n",
    "                         group_path: str = '/nepse/prices',\n",
    "                         overwrite: bool = False) -> None:\n",
    "        \"\"\"\n",
    "        Store stock price data in HDF5 format.\n",
    "        \n",
    "        This method organizes data by stock symbol, storing each stock's\n",
    "        time-series as a separate dataset within the group.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with columns including 'Symbol', 'Date', and price data\n",
    "            group_path: HDF5 group path for storing data\n",
    "            overwrite: Whether to overwrite existing data\n",
    "        \n",
    "        The HDF5 structure created:\n",
    "        /nepse/\n",
    "            /prices/\n",
    "                /NABIL/\n",
    "                    /open      - 1D array of open prices\n",
    "                    /high      - 1D array of high prices\n",
    "                    /low       - 1D array of low prices\n",
    "                    /close     - 1D array of close prices\n",
    "                    /volume    - 1D array of volumes\n",
    "                    /dates     - 1D array of dates (as integers)\n",
    "                    (attributes: symbol, start_date, end_date, count)\n",
    "                /NICA/\n",
    "                    ...\n",
    "        \"\"\"\n",
    "        with self._get_file('a') as f:\n",
    "            # Create the group structure if it doesn't exist\n",
    "            # require_group creates the group only if it doesn't exist\n",
    "            base_group = f.require_group(group_path)\n",
    "            \n",
    "            # Get unique symbols\n",
    "            symbols = df['Symbol'].unique()\n",
    "            \n",
    "            for symbol in symbols:\n",
    "                # Filter data for this symbol\n",
    "                stock_df = df[df['Symbol'] == symbol].copy()\n",
    "                stock_df = stock_df.sort_values('Date')\n",
    "                \n",
    "                # Create or get the group for this symbol\n",
    "                if overwrite and symbol in base_group:\n",
    "                    del base_group[symbol]\n",
    "                \n",
    "                stock_group = base_group.require_group(symbol)\n",
    "                \n",
    "                # Convert dates to integers for efficient storage\n",
    "                # We use Unix timestamp (seconds since 1970-01-01)\n",
    "                # This allows efficient date range queries\n",
    "                dates = pd.to_datetime(stock_df['Date'])\n",
    "                date_ints = dates.astype(np.int64) // 10**9  # Convert to seconds\n",
    "                \n",
    "                # Store numerical columns as datasets\n",
    "                # Each dataset is a 1D array\n",
    "                columns_to_store = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "                \n",
    "                for col in columns_to_store:\n",
    "                    if col in stock_df.columns:\n",
    "                        # Create dataset with compression\n",
    "                        # chunks=True enables chunked storage for efficient partial reads\n",
    "                        stock_group.create_dataset(\n",
    "                            col.lower(),\n",
    "                            data=stock_df[col].values,\n",
    "                            compression=self.compression,\n",
    "                            compression_opts=self.compression_level,\n",
    "                            chunks=True  # Enable chunking for partial reads\n",
    "                        )\n",
    "                \n",
    "                # Store dates as integers\n",
    "                stock_group.create_dataset(\n",
    "                    'dates',\n",
    "                    data=date_ints.values,\n",
    "                    compression=self.compression,\n",
    "                    compression_opts=self.compression_level,\n",
    "                    chunks=True\n",
    "                )\n",
    "                \n",
    "                # Store metadata as attributes\n",
    "                # Attributes are small pieces of metadata attached to groups/datasets\n",
    "                stock_group.attrs['symbol'] = symbol\n",
    "                stock_group.attrs['start_date'] = str(dates.min().date())\n",
    "                stock_group.attrs['end_date'] = str(dates.max().date())\n",
    "                stock_group.attrs['count'] = len(stock_df)\n",
    "                stock_group.attrs['columns'] = ', '.join(columns_to_store)\n",
    "            \n",
    "            # Store global metadata\n",
    "            base_group.attrs['last_updated'] = datetime.now().isoformat()\n",
    "            base_group.attrs['total_symbols'] = len(symbols)\n",
    "    \n",
    "    def store_aligned_data(self,\n",
    "                           df: pd.DataFrame,\n",
    "                           dataset_name: str = 'aligned_prices') -> None:\n",
    "        \"\"\"\n",
    "        Store all stock data in a single aligned 2D array.\n",
    "        \n",
    "        This approach is more efficient for:\n",
    "        - Cross-sectional analysis (comparing stocks at the same time)\n",
    "        - Matrix operations\n",
    "        - Machine learning models that need consistent array shapes\n",
    "        \n",
    "        Structure:\n",
    "        /nepse/\n",
    "            /aligned_prices/\n",
    "                /data        - 2D array: (time x stocks)\n",
    "                /dates       - 1D array of dates\n",
    "                /symbols     - 1D array of symbol names\n",
    "                /mapping/    - Metadata about the structure\n",
    "                    /symbol_to_idx  - JSON mapping symbol -> column index\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with Date, Symbol, and price columns\n",
    "            dataset_name: Name for the dataset group\n",
    "        \"\"\"\n",
    "        with self._get_file('a') as f:\n",
    "            # Pivot the data to create a 2D matrix\n",
    "            # Rows = dates, Columns = symbols\n",
    "            pivot_df = df.pivot_table(\n",
    "                index='Date',\n",
    "                columns='Symbol',\n",
    "                values='Close',\n",
    "                aggfunc='first'  # Take first value if duplicates\n",
    "            )\n",
    "            \n",
    "            # Sort by date\n",
    "            pivot_df = pivot_df.sort_index()\n",
    "            \n",
    "            # Create group\n",
    "            group = f.require_group(f'/nepse/{dataset_name}')\n",
    "            \n",
    "            # Store the 2D price matrix\n",
    "            group.create_dataset(\n",
    "                'close_matrix',\n",
    "                data=pivot_df.values,\n",
    "                compression=self.compression,\n",
    "                compression_opts=self.compression_level,\n",
    "                chunks=True\n",
    "            )\n",
    "            \n",
    "            # Store dates\n",
    "            dates = pd.to_datetime(pivot_df.index)\n",
    "            date_ints = dates.astype(np.int64) // 10**9\n",
    "            \n",
    "            group.create_dataset(\n",
    "                'dates',\n",
    "                data=date_ints.values,\n",
    "                compression=self.compression,\n",
    "                compression_opts=self.compression_level\n",
    "            )\n",
    "            \n",
    "            # Store symbols\n",
    "            symbols = pivot_df.columns.tolist()\n",
    "            # HDF5 can store variable-length strings\n",
    "            dt = h5py.special_dtype(vlen=str)\n",
    "            group.create_dataset('symbols', data=symbols, dtype=dt)\n",
    "            \n",
    "            # Store metadata\n",
    "            group.attrs['shape'] = str(pivot_df.shape)\n",
    "            group.attrs['num_dates'] = len(pivot_df)\n",
    "            group.attrs['num_symbols'] = len(symbols)\n",
    "            group.attrs['start_date'] = str(dates.min().date())\n",
    "            group.attrs['end_date'] = str(dates.max().date())\n",
    "    \n",
    "    def read_stock_data(self,\n",
    "                        symbol: str,\n",
    "                        start_date: Optional[str] = None,\n",
    "                        end_date: Optional[str] = None,\n",
    "                        columns: Optional[List[str]] = None,\n",
    "                        group_path: str = '/nepse/prices') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read stock data from HDF5 with optional date filtering.\n",
    "        \n",
    "        This method demonstrates the efficiency of HDF5:\n",
    "        - Only reads the requested columns (like Parquet column pruning)\n",
    "        - Can filter by date range without reading all data (chunking)\n",
    "        - Fast random access to any part of the dataset\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol to read\n",
    "            start_date: Start date filter (YYYY-MM-DD)\n",
    "            end_date: End date filter (YYYY-MM-DD)\n",
    "            columns: List of columns to read (None for all)\n",
    "            group_path: HDF5 group path where data is stored\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with the requested data\n",
    "        \"\"\"\n",
    "        with self._get_file('r') as f:\n",
    "            # Navigate to the stock's group\n",
    "            if group_path not in f:\n",
    "                raise ValueError(f\"Group {group_path} not found in HDF5 file\")\n",
    "            \n",
    "            base_group = f[group_path]\n",
    "            \n",
    "            if symbol not in base_group:\n",
    "                raise ValueError(f\"Symbol {symbol} not found in {group_path}\")\n",
    "            \n",
    "            stock_group = base_group[symbol]\n",
    "            \n",
    "            # Get dates\n",
    "            date_ints = stock_group['dates'][:]\n",
    "            dates = pd.to_datetime(date_ints * 10**9)  # Convert back to nanoseconds\n",
    "            \n",
    "            # Apply date filter\n",
    "            mask = np.ones(len(dates), dtype=bool)\n",
    "            \n",
    "            if start_date:\n",
    "                start = pd.to_datetime(start_date)\n",
    "                mask &= (dates >= start)\n",
    "            \n",
    "            if end_date:\n",
    "                end = pd.to_datetime(end_date)\n",
    "                mask &= (dates <= end)\n",
    "            \n",
    "            # Determine which columns to read\n",
    "            all_columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "            if columns:\n",
    "                # Convert to lowercase for matching\n",
    "                columns = [c.lower() for c in columns]\n",
    "                read_columns = [c for c in all_columns if c in columns]\n",
    "            else:\n",
    "                read_columns = all_columns\n",
    "            \n",
    "            # Build DataFrame\n",
    "            data = {'Date': dates[mask]}\n",
    "            \n",
    "            for col in read_columns:\n",
    "                if col in stock_group:\n",
    "                    # Only read the data that matches our mask\n",
    "                    data[col.capitalize()] = stock_group[col][mask]\n",
    "            \n",
    "            df = pd.DataFrame(data)\n",
    "            \n",
    "            # Add symbol column\n",
    "            df['Symbol'] = symbol\n",
    "            \n",
    "            return df\n",
    "    \n",
    "    def read_aligned_data(self,\n",
    "                          dataset_name: str = 'aligned_prices',\n",
    "                          symbols: Optional[List[str]] = None,\n",
    "                          start_date: Optional[str] = None,\n",
    "                          end_date: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read aligned stock data from HDF5.\n",
    "        \n",
    "        Args:\n",
    "            dataset_name: Name of the dataset group\n",
    "            symbols: List of symbols to read (None for all)\n",
    "            start_date: Start date filter\n",
    "            end_date: End date filter\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with aligned price data\n",
    "        \"\"\"\n",
    "        with self._get_file('r') as f:\n",
    "            group_path = f'/nepse/{dataset_name}'\n",
    "            \n",
    "            if group_path not in f:\n",
    "                raise ValueError(f\"Dataset {dataset_name} not found\")\n",
    "            \n",
    "            group = f[group_path]\n",
    "            \n",
    "            # Read dates and filter\n",
    "            date_ints = group['dates'][:]\n",
    "            dates = pd.to_datetime(date_ints * 10**9)\n",
    "            \n",
    "            mask = np.ones(len(dates), dtype=bool)\n",
    "            \n",
    "            if start_date:\n",
    "                start = pd.to_datetime(start_date)\n",
    "                mask &= (dates >= start)\n",
    "            \n",
    "            if end_date:\n",
    "                end = pd.to_datetime(end_date)\n",
    "                mask &= (dates <= end)\n",
    "            \n",
    "            # Read price matrix\n",
    "            price_matrix = group['close_matrix'][mask]\n",
    "            \n",
    "            # Read symbols\n",
    "            all_symbols = list(group['symbols'][:])\n",
    "            \n",
    "            # Filter symbols\n",
    "            if symbols:\n",
    "                symbol_mask = [s in symbols for s in all_symbols]\n",
    "                price_matrix = price_matrix[:, symbol_mask]\n",
    "                selected_symbols = [s for s in all_symbols if s in symbols]\n",
    "            else:\n",
    "                selected_symbols = all_symbols\n",
    "            \n",
    "            # Create DataFrame\n",
    "            df = pd.DataFrame(\n",
    "                price_matrix,\n",
    "                index=dates[mask],\n",
    "                columns=selected_symbols\n",
    "            )\n",
    "            df.index.name = 'Date'\n",
    "            \n",
    "            return df\n",
    "    \n",
    "    def get_metadata(self, symbol: str, group_path: str = '/nepse/prices') -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get metadata for a specific stock.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            group_path: HDF5 group path\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with metadata\n",
    "        \"\"\"\n",
    "        with self._get_file('r') as f:\n",
    "            if group_path not in f:\n",
    "                raise ValueError(f\"Group {group_path} not found\")\n",
    "            \n",
    "            stock_group = f[f'{group_path}/{symbol}']\n",
    "            \n",
    "            # Read all attributes\n",
    "            metadata = dict(stock_group.attrs)\n",
    "            \n",
    "            return metadata\n",
    "    \n",
    "    def list_symbols(self, group_path: str = '/nepse/prices') -> List[str]:\n",
    "        \"\"\"\n",
    "        List all symbols stored in the HDF5 file.\n",
    "        \n",
    "        Args:\n",
    "            group_path: HDF5 group path\n",
    "        \n",
    "        Returns:\n",
    "            List of symbol names\n",
    "        \"\"\"\n",
    "        with self._get_file('r') as f:\n",
    "            if group_path not in f:\n",
    "                return []\n",
    "            \n",
    "            base_group = f[group_path]\n",
    "            return list(base_group.keys())\n",
    "    \n",
    "    def get_file_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get information about the HDF5 file.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with file information\n",
    "        \"\"\"\n",
    "        info = {\n",
    "            'filepath': str(self.filepath),\n",
    "            'exists': self.filepath.exists(),\n",
    "            'size_mb': 0,\n",
    "            'groups': [],\n",
    "            'total_stocks': 0,\n",
    "            'datasets': []\n",
    "        }\n",
    "        \n",
    "        if not self.filepath.exists():\n",
    "            return info\n",
    "        \n",
    "        info['size_mb'] = self.filepath.stat().st_size / (1024 * 1024)\n",
    "        \n",
    "        with self._get_file('r') as f:\n",
    "            # Recursively list all groups and datasets\n",
    "            def explore_group(group, path=''):\n",
    "                for key in group.keys():\n",
    "                    item = group[key]\n",
    "                    full_path = f'{path}/{key}'\n",
    "                    \n",
    "                    if isinstance(item, h5py.Group):\n",
    "                        info['groups'].append(full_path)\n",
    "                        explore_group(item, full_path)\n",
    "                    elif isinstance(item, h5py.Dataset):\n",
    "                        info['datasets'].append({\n",
    "                            'path': full_path,\n",
    "                            'shape': item.shape,\n",
    "                            'dtype': str(item.dtype),\n",
    "                            'size_bytes': item.nbytes\n",
    "                        })\n",
    "            \n",
    "            explore_group(f)\n",
    "            \n",
    "            # Count stocks\n",
    "            if '/nepse/prices' in f:\n",
    "                info['total_stocks'] = len(f['/nepse/prices'].keys())\n",
    "        \n",
    "        return info\n",
    "    \n",
    "    def append_data(self,\n",
    "                    df: pd.DataFrame,\n",
    "                    symbol: str,\n",
    "                    group_path: str = '/nepse/prices') -> None:\n",
    "        \"\"\"\n",
    "        Append new data to an existing stock's dataset.\n",
    "        \n",
    "        HDF5 supports efficient appending through chunking.\n",
    "        However, resizing datasets requires careful handling.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with new data to append\n",
    "            symbol: Stock symbol\n",
    "            group_path: HDF5 group path\n",
    "        \"\"\"\n",
    "        with self._get_file('a') as f:\n",
    "            stock_group = f[f'{group_path}/{symbol}']\n",
    "            \n",
    "            # Read existing dates\n",
    "            existing_dates = stock_group['dates'][:]\n",
    "            existing_set = set(existing_dates)\n",
    "            \n",
    "            # Filter out duplicates\n",
    "            df = df.copy()\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            new_date_ints = df['Date'].astype(np.int64) // 10**9\n",
    "            mask = ~new_date_ints.isin(existing_set)\n",
    "            df = df[mask]\n",
    "            \n",
    "            if len(df) == 0:\n",
    "                print(f\"No new data to append for {symbol}\")\n",
    "                return\n",
    "            \n",
    "            # Append to each dataset\n",
    "            columns = ['open', 'high', 'low', 'close', 'volume']\n",
    "            \n",
    "            for col in columns:\n",
    "                if col in stock_group and col.capitalize() in df.columns:\n",
    "                    dataset = stock_group[col]\n",
    "                    \n",
    "                    # Get current size\n",
    "                    current_size = dataset.shape[0]\n",
    "                    new_size = current_size + len(df)\n",
    "                    \n",
    "                    # Resize and append\n",
    "                    dataset.resize(new_size, axis=0)\n",
    "                    dataset[current_size:] = df[col.capitalize()].values\n",
    "            \n",
    "            # Append dates\n",
    "            dates_dataset = stock_group['dates']\n",
    "            current_size = dates_dataset.shape[0]\n",
    "            new_size = current_size + len(df)\n",
    "            dates_dataset.resize(new_size, axis=0)\n",
    "            dates_dataset[current_size:] = new_date_ints[mask].values\n",
    "            \n",
    "            # Update metadata\n",
    "            all_dates = stock_group['dates'][:]\n",
    "            dates_dt = pd.to_datetime(all_dates * 10**9)\n",
    "            stock_group.attrs['end_date'] = str(dates_dt.max().date())\n",
    "            stock_group.attrs['count'] = len(all_dates)\n",
    "            \n",
    "            print(f\"Appended {len(df)} records to {symbol}\")\n",
    "\n",
    "\n",
    "class NetCDFTimeSeriesStorage:\n",
    "    \"\"\"\n",
    "    A class to handle NetCDF storage for time-series data.\n",
    "    \n",
    "    NetCDF (Network Common Data Form) is a set of software libraries\n",
    "    and machine-independent data formats for array-oriented scientific data.\n",
    "    \n",
    "    While HDF5 is more general-purpose, NetCDF is particularly well-suited for:\n",
    "    - Meteorological and climate data\n",
    "    - Multi-dimensional gridded data\n",
    "    - Data that needs CF (Climate and Forecast) conventions\n",
    "    - Interoperability with scientific software (MATLAB, R, etc.)\n",
    "    \n",
    "    For NEPSE data, we can use NetCDF to store:\n",
    "    - Daily prices across multiple stocks (2D: time x stocks)\n",
    "    - Multiple variables (open, high, low, close, volume)\n",
    "    - Geographic data if we add location information\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, filepath: str = './nepse_data.nc'):\n",
    "        \"\"\"\n",
    "        Initialize NetCDF storage.\n",
    "        \n",
    "        Args:\n",
    "            filepath: Path to the NetCDF file (.nc extension)\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import netCDF4 as nc\n",
    "            self.nc = nc\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"netCDF4 package is required. Install with: pip install netCDF4\"\n",
    "            )\n",
    "        \n",
    "        self.filepath = Path(filepath)\n",
    "        self.filepath.parent.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def store_time_series(self,\n",
    "                          df: pd.DataFrame,\n",
    "                          time_dim: str = 'time',\n",
    "                          symbol_dim: str = 'symbol') -> None:\n",
    "        \"\"\"\n",
    "        Store time-series data in NetCDF format.\n",
    "        \n",
    "        NetCDF uses dimensions and variables:\n",
    "        - Dimensions define the shape of data (time, symbol)\n",
    "        - Variables hold the actual data with attributes\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with Date, Symbol, and price columns\n",
    "            time_dim: Name for the time dimension\n",
    "            symbol_dim: Name for the symbol dimension\n",
    "        \"\"\"\n",
    "        with self.nc.Dataset(self.filepath, 'w') as ds:\n",
    "            # Get unique dates and symbols\n",
    "            dates = pd.to_datetime(df['Date'].unique())\n",
    "            dates = dates.sort_values()\n",
    "            symbols = sorted(df['Symbol'].unique())\n",
    "            \n",
    "            # Create dimensions\n",
    "            # 'UNLIMITED' allows appending new time steps\n",
    "            ds.createDimension(time_dim, None)  # Unlimited dimension\n",
    "            ds.createDimension(symbol_dim, len(symbols))\n",
    "            \n",
    "            # Create coordinate variables\n",
    "            # Time is stored as days since a reference date\n",
    "            time_var = ds.createVariable(\n",
    "                time_dim,\n",
    "                'f8',  # 64-bit float\n",
    "                (time_dim,)\n",
    "            )\n",
    "            time_var.units = 'days since 2020-01-01'\n",
    "            time_var.calendar = 'standard'\n",
    "            time_var.long_name = 'Time'\n",
    "            \n",
    "            # Calculate days since reference\n",
    "            ref_date = pd.Timestamp('2020-01-01')\n",
    "            days_since = (dates - ref_date).days\n",
    "            time_var[:] = days_since\n",
    "            \n",
    "            # Symbol is stored as strings using a character array\n",
    "            # NetCDF doesn't directly support variable-length strings\n",
    "            # So we use a character dimension\n",
    "            max_symbol_len = max(len(s) for s in symbols)\n",
    "            ds.createDimension('symbol_strlen', max_symbol_len)\n",
    "            \n",
    "            symbol_var = ds.createVariable(\n",
    "                symbol_dim,\n",
    "                'S1',  # Single character\n",
    "                (symbol_dim, 'symbol_strlen')\n",
    "            )\n",
    "            symbol_var.long_name = 'Stock Symbol'\n",
    "            \n",
    "            # Store symbols as character arrays\n",
    "            for i, symbol in enumerate(symbols):\n",
    "                symbol_var[i, :] = symbol.ljust(max_symbol_len).encode('utf-8')\n",
    "            \n",
    "            # Create data variables\n",
    "            # Each variable has the dimensions (time, symbol)\n",
    "            variable_configs = [\n",
    "                ('open', 'f8', 'Opening Price', 'NPR'),\n",
    "                ('high', 'f8', 'High Price', 'NPR'),\n",
    "                ('low', 'f8', 'Low Price', 'NPR'),\n",
    "                ('close', 'f8', 'Closing Price', 'NPR'),\n",
    "                ('volume', 'i8', 'Trading Volume', 'shares'),\n",
    "            ]\n",
    "            \n",
    "            # Create a mapping from symbol to index\n",
    "            symbol_to_idx = {s: i for i, s in enumerate(symbols)}\n",
    "            \n",
    "            for var_name, dtype, long_name, units in variable_configs:\n",
    "                var = ds.createVariable(\n",
    "                    var_name,\n",
    "                    dtype,\n",
    "                    (time_dim, symbol_dim),\n",
    "                    zlib=True,  # Enable compression\n",
    "                    complevel=4  # Compression level (1-9)\n",
    "                )\n",
    "                var.long_name = long_name\n",
    "                var.units = units\n",
    "                \n",
    "                # Initialize with missing value\n",
    "                var[:] = np.nan if dtype == 'f8' else -9999\n",
    "            \n",
    "            # Fill in the data\n",
    "            # Create a date to index mapping\n",
    "            date_to_idx = {d: i for i, d in enumerate(dates)}\n",
    "            \n",
    "            for _, row in df.iterrows():\n",
    "                date = pd.to_datetime(row['Date'])\n",
    "                symbol = row['Symbol']\n",
    "                \n",
    "                if date in date_to_idx and symbol in symbol_to_idx:\n",
    "                    t_idx = date_to_idx[date]\n",
    "                    s_idx = symbol_to_idx[symbol]\n",
    "                    \n",
    "                    # Store each variable\n",
    "                    if 'Open' in row:\n",
    "                        ds.variables['open'][t_idx, s_idx] = row['Open']\n",
    "                    if 'High' in row:\n",
    "                        ds.variables['high'][t_idx, s_idx] = row['High']\n",
    "                    if 'Low' in row:\n",
    "                        ds.variables['low'][t_idx, s_idx] = row['Low']\n",
    "                    if 'Close' in row:\n",
    "                        ds.variables['close'][t_idx, s_idx] = row['Close']\n",
    "                    if 'Volume' in row:\n",
    "                        ds.variables['volume'][t_idx, s_idx] = row['Volume']\n",
    "            \n",
    "            # Add global attributes\n",
    "            ds.title = 'NEPSE Stock Price Data'\n",
    "            ds.institution = 'Nepal Stock Exchange'\n",
    "            ds.source = 'NEPSE Historical Data'\n",
    "            ds.history = f'Created {datetime.now().isoformat()}'\n",
    "            ds.Conventions = 'CF-1.8'\n",
    "    \n",
    "    def read_time_series(self,\n",
    "                         variables: Optional[List[str]] = None,\n",
    "                         symbols: Optional[List[str]] = None,\n",
    "                         start_date: Optional[str] = None,\n",
    "                         end_date: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read time-series data from NetCDF.\n",
    "        \n",
    "        Args:\n",
    "            variables: List of variables to read (None for all)\n",
    "            symbols: List of symbols to read (None for all)\n",
    "            start_date: Start date filter\n",
    "            end_date: End date filter\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with the requested data\n",
    "        \"\"\"\n",
    "        with self.nc.Dataset(self.filepath, 'r') as ds:\n",
    "            # Read time coordinate\n",
    "            time_var = ds.variables['time']\n",
    "            days_since = time_var[:]\n",
    "            ref_date = pd.Timestamp('2020-01-01')\n",
    "            dates = ref_date + pd.to_timedelta(days_since, unit='D')\n",
    "            \n",
    "            # Read symbols\n",
    "            symbol_data = ds.variables['symbol'][:]\n",
    "            symbols_all = [''.join(s.astype(str)).strip() for s in symbol_data]\n",
    "            \n",
    "            # Determine time range\n",
    "            time_mask = np.ones(len(dates), dtype=bool)\n",
    "            if start_date:\n",
    "                start = pd.to_datetime(start_date)\n",
    "                time_mask &= (dates >= start)\n",
    "            if end_date:\n",
    "                end = pd.to_datetime(end_date)\n",
    "                time_mask &= (dates <= end)\n",
    "            \n",
    "            # Determine symbols\n",
    "            if symbols:\n",
    "                symbol_mask = [s in symbols for s in symbols_all]\n",
    "            else:\n",
    "                symbol_mask = np.ones(len(symbols_all), dtype=bool)\n",
    "            \n",
    "            # Determine variables\n",
    "            if variables is None:\n",
    "                variables = ['open', 'high', 'low', 'close', 'volume']\n",
    "            \n",
    "            # Build data array\n",
    "            data = []\n",
    "            for t_idx, date in enumerate(dates):\n",
    "                if not time_mask[t_idx]:\n",
    "                    continue\n",
    "                \n",
    "                for s_idx, symbol in enumerate(symbols_all):\n",
    "                    if not symbol_mask[s_idx]:\n",
    "                        continue\n",
    "                    \n",
    "                    row = {'Date': date, 'Symbol': symbol}\n",
    "                    for var in variables:\n",
    "                        if var in ds.variables:\n",
    "                            value = ds.variables[var][t_idx, s_idx]\n",
    "                            # Convert missing values to NaN\n",
    "                            if value == -9999 or np.isnan(value):\n",
    "                                row[var.capitalize()] = np.nan\n",
    "                            else:\n",
    "                                row[var.capitalize()] = value\n",
    "                    \n",
    "                    data.append(row)\n",
    "            \n",
    "            return pd.DataFrame(data)\n",
    "    \n",
    "    def get_variable_info(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get information about variables in the NetCDF file.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with variable information\n",
    "        \"\"\"\n",
    "        with self.nc.Dataset(self.filepath, 'r') as ds:\n",
    "            info = {\n",
    "                'dimensions': {name: len(dim) for name, dim in ds.dimensions.items()},\n",
    "                'variables': {},\n",
    "                'global_attributes': {name: getattr(ds, name) for name in ds.ncattrs()}\n",
    "            }\n",
    "            \n",
    "            for name, var in ds.variables.items():\n",
    "                info['variables'][name] = {\n",
    "                    'dimensions': var.dimensions,\n",
    "                    'shape': var.shape,\n",
    "                    'dtype': str(var.dtype),\n",
    "                    'attributes': {attr: var.getncattr(attr) for attr in var.ncattrs()}\n",
    "                }\n",
    "            \n",
    "            return info\n",
    "\n",
    "\n",
    "def demonstrate_hdf5_storage():\n",
    "    \"\"\"\n",
    "    Demonstrate HDF5 storage operations with NEPSE data.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"HDF5 Storage Demonstration for NEPSE Data\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = pd.date_range('2023-01-01', '2023-12-31', freq='B')\n",
    "    symbols = ['NABIL', 'NICA', 'SCBL', 'ADBL', 'EBL', 'GBIME', 'HBL', 'NBL']\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for symbol in symbols:\n",
    "            base_price = 300 + np.random.randint(0, 600)\n",
    "            high = base_price * (1 + np.random.uniform(0, 0.05))\n",
    "            low = base_price * (1 - np.random.uniform(0, 0.05))\n",
    "            open_price = np.random.uniform(low, high)\n",
    "            close = np.random.uniform(low, high)\n",
    "            \n",
    "            data.append({\n",
    "                'Date': date,\n",
    "                'Symbol': symbol,\n",
    "                'Open': round(open_price, 2),\n",
    "                'High': round(high, 2),\n",
    "                'Low': round(low, 2),\n",
    "                'Close': round(close, 2),\n",
    "                'Volume': np.random.randint(10000, 500000)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"\\nGenerated {len(df)} records for {len(symbols)} stocks\")\n",
    "    \n",
    "    # Initialize HDF5 storage\n",
    "    hdf5_storage = HDF5TimeSeriesStorage('./nepse_analysis.h5')\n",
    "    \n",
    "    # Store data by symbol (individual time-series)\n",
    "    print(\"\\n1. Storing Data by Symbol\")\n",
    "    print(\"-\" * 40)\n",
    "    hdf5_storage.store_stock_data(df, group_path='/nepse/prices')\n",
    "    \n",
    "    # Store aligned data (matrix format)\n",
    "    print(\"\\n2. Storing Aligned Data (Matrix Format)\")\n",
    "    print(\"-\" * 40)\n",
    "    hdf5_storage.store_aligned_data(df)\n",
    "    \n",
    "    # Get file info\n",
    "    print(\"\\n3. File Information\")\n",
    "    print(\"-\" * 40)\n",
    "    info = hdf5_storage.get_file_info()\n",
    "    print(f\"File: {info['filepath']}\")\n",
    "    print(f\"Size: {info['size_mb']:.2f} MB\")\n",
    "    print(f\"Total stocks: {info['total_stocks']}\")\n",
    "    print(f\"Groups: {len(info['groups'])}\")\n",
    "    print(f\"Datasets: {len(info['datasets'])}\")\n",
    "    \n",
    "    # Read specific stock data\n",
    "    print(\"\\n4. Reading Single Stock Data\")\n",
    "    print(\"-\" * 40)\n",
    "    nabil_data = hdf5_storage.read_stock_data(\n",
    "        symbol='NABIL',\n",
    "        start_date='2023-06-01',\n",
    "        end_date='2023-06-30',\n",
    "        columns=['open', 'high', 'low', 'close', 'volume']\n",
    "    )\n",
    "    print(f\"NABIL data for June 2023 ({len(nabil_data)} rows):\")\n",
    "    print(nabil_data.head())\n",
    "    \n",
    "    # Read aligned data\n",
    "    print(\"\\n5. Reading Aligned Data\")\n",
    "    print(\"-\" * 40)\n",
    "    aligned = hdf5_storage.read_aligned_data(\n",
    "        symbols=['NABIL', 'NICA', 'SCBL'],\n",
    "        start_date='2023-06-01',\n",
    "        end_date='2023-06-30'\n",
    "    )\n",
    "    print(f\"Aligned data ({aligned.shape}):\")\n",
    "    print(aligned.head())\n",
    "    \n",
    "    # Get metadata\n",
    "    print(\"\\n6. Stock Metadata\")\n",
    "    print(\"-\" * 40)\n",
    "    metadata = hdf5_storage.get_metadata('NABIL')\n",
    "    print(\"NABIL metadata:\")\n",
    "    for key, value in metadata.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "    \n",
    "    return hdf5_storage, df\n",
    "\n",
    "\n",
    "def demonstrate_netcdf_storage():\n",
    "    \"\"\"\n",
    "    Demonstrate NetCDF storage operations.\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"NetCDF Storage Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = pd.date_range('2023-01-01', '2023-12-31', freq='B')\n",
    "    symbols = ['NABIL', 'NICA', 'SCBL']\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for symbol in symbols:\n",
    "            base_price = 300 + np.random.randint(0, 500)\n",
    "            data.append({\n",
    "                'Date': date,\n",
    "                'Symbol': symbol,\n",
    "                'Open': round(base_price * (1 + np.random.uniform(-0.02, 0.02)), 2),\n",
    "                'High': round(base_price * 1.02, 2),\n",
    "                'Low': round(base_price * 0.98, 2),\n",
    "                'Close': round(base_price * (1 + np.random.uniform(-0.01, 0.01)), 2),\n",
    "                'Volume': np.random.randint(10000, 100000)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    try:\n",
    "        # Initialize NetCDF storage\n",
    "        netcdf_storage = NetCDFTimeSeriesStorage('./nepse_data.nc')\n",
    "        \n",
    "        print(\"\\n1. Storing Data in NetCDF Format\")\n",
    "        print(\"-\" * 40)\n",
    "        netcdf_storage.store_time_series(df)\n",
    "        print(\"Data stored successfully\")\n",
    "        \n",
    "        print(\"\\n2. Reading NetCDF Data\")\n",
    "        print(\"-\" * 40)\n",
    "        read_df = netcdf_storage.read_time_series(\n",
    "            symbols=['NABIL'],\n",
    "            start_date='2023-06-01',\n",
    "            end_date='2023-06-30'\n",
    "        )\n",
    "        print(f\"Read {len(read_df)} rows:\")\n",
    "        print(read_df.head())\n",
    "        \n",
    "        print(\"\\n3. Variable Information\")\n",
    "        print(\"-\" * 40)\n",
    "        var_info = netcdf_storage.get_variable_info()\n",
    "        print(\"Dimensions:\")\n",
    "        for name, size in var_info['dimensions'].items():\n",
    "            print(f\"  {name}: {size}\")\n",
    "        print(\"\\nVariables:\")\n",
    "        for name, info in var_info['variables'].items():\n",
    "            print(f\"  {name}: {info['shape']} - {info['attributes'].get('long_name', 'N/A')}\")\n",
    "        \n",
    "        return netcdf_storage, df\n",
    "    \n",
    "    except ImportError as e:\n",
    "        print(f\"\\nNetCDF demonstration skipped: {e}\")\n",
    "        print(\"Install netCDF4 with: pip install netCDF4\")\n",
    "        return None, df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    hdf5_storage, df = demonstrate_hdf5_storage()\n",
    "    netcdf_storage, df2 = demonstrate_netcdf_storage()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **HDF5 Structure**: HDF5 organizes data hierarchically:\n",
    "   - **Groups** act like directories, organizing data into logical collections\n",
    "   - **Datasets** are multidimensional arrays that store the actual data\n",
    "   - **Attributes** are small metadata attached to groups or datasets\n",
    "\n",
    "2. **Storing by Symbol**: The `store_stock_data` method creates a group for each stock symbol, with separate datasets for each price field (open, high, low, close, volume). This structure:\n",
    "   - Enables fast retrieval of a single stock's data\n",
    "   - Supports efficient appending of new data\n",
    "   - Stores metadata like start/end dates\n",
    "\n",
    "3. **Storing Aligned Data**: The `store_aligned_data` method creates a 2D matrix where rows are dates and columns are symbols. This is ideal for:\n",
    "   - Cross-sectional analysis (comparing stocks at the same time)\n",
    "   - Correlation calculations\n",
    "   - Machine learning models requiring consistent array shapes\n",
    "\n",
    "4. **Date Storage**: Dates are converted to Unix timestamps (integers) for efficient storage and comparison. This allows:\n",
    "   - Efficient date range queries using boolean masking\n",
    "   - Compact storage (8 bytes per date)\n",
    "   - Fast arithmetic operations\n",
    "\n",
    "5. **Chunking**: HDF5's chunking feature splits data into fixed-size blocks:\n",
    "   - Each chunk can be read/written independently\n",
    "   - Enables efficient partial reads\n",
    "   - Improves performance for time-range queries\n",
    "\n",
    "6. **NetCDF Differences**: While HDF5 is general-purpose, NetCDF is designed for scientific data:\n",
    "   - Follows CF (Climate and Forecast) conventions\n",
    "   - Built-in coordinate handling (time as \"days since reference\")\n",
    "   - Widely used in climate science, meteorology, oceanography\n",
    "   - Better interoperability with scientific software\n",
    "\n",
    "7. **Performance Considerations**:\n",
    "   - HDF5 excels at random access and partial reads\n",
    "   - NetCDF is better for gridded scientific data\n",
    "   - Both support compression (gzip/lzf for HDF5, zlib for NetCDF)\n",
    "\n",
    "---\n",
    "\n",
    "## **8.3 Relational Databases**\n",
    "\n",
    "Relational databases provide structured storage with ACID guarantees, making them ideal for transactional data and complex queries. For time-series data, they offer SQL's powerful query capabilities.\n",
    "\n",
    "### **8.3.1 Schema Design**\n",
    "\n",
    "Proper schema design is critical for time-series data in relational databases. Let's explore various approaches.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Relational Database Schema Design for Time-Series Data\n",
    "\n",
    "This module demonstrates schema designs optimized for time-series data,\n",
    "specifically for the NEPSE stock prediction system.\n",
    "\n",
    "Key considerations for time-series schema design:\n",
    "1. Indexing strategy for time-based queries\n",
    "2. Partitioning for large datasets\n",
    "3. Normalization vs. denormalization tradeoffs\n",
    "4. Handling updates and inserts efficiently\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Optional, Dict, Any, Tuple\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "\n",
    "class DatabaseSchema:\n",
    "    \"\"\"\n",
    "    A class to define and manage database schemas for time-series data.\n",
    "    \n",
    "    This class provides schema definitions and SQL statements for creating\n",
    "    tables optimized for time-series data storage.\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_nepse_schema_sqlite() -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Get SQLite schema definition for NEPSE data.\n",
    "        \n",
    "        SQLite is a lightweight, file-based database that's excellent for:\n",
    "        - Development and prototyping\n",
    "        - Embedded applications\n",
    "        - Desktop applications\n",
    "        - Small to medium datasets\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping table names to CREATE TABLE statements\n",
    "        \"\"\"\n",
    "        schema = {\n",
    "            # Main stock prices table\n",
    "            # This uses a normalized design with separate tables for\n",
    "            # stocks and prices\n",
    "            'stocks': '''\n",
    "                CREATE TABLE IF NOT EXISTS stocks (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    symbol TEXT UNIQUE NOT NULL,\n",
    "                    company_name TEXT,\n",
    "                    sector TEXT,\n",
    "                    listed_date DATE,\n",
    "                    is_active BOOLEAN DEFAULT 1,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "                \n",
    "                -- Index on symbol for fast lookups\n",
    "                CREATE INDEX IF NOT EXISTS idx_stocks_symbol ON stocks(symbol);\n",
    "            ''',\n",
    "            \n",
    "            'stock_prices': '''\n",
    "                CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    stock_id INTEGER NOT NULL,\n",
    "                    trade_date DATE NOT NULL,\n",
    "                    open_price REAL,\n",
    "                    high_price REAL,\n",
    "                    low_price REAL,\n",
    "                    close_price REAL,\n",
    "                    ltp REAL,\n",
    "                    vwap REAL,\n",
    "                    volume INTEGER,\n",
    "                    turnover REAL,\n",
    "                    transactions INTEGER,\n",
    "                    prev_close REAL,\n",
    "                    diff REAL,\n",
    "                    price_range REAL,\n",
    "                    diff_percent REAL,\n",
    "                    range_percent REAL,\n",
    "                    vwap_percent REAL,\n",
    "                    high_52_week REAL,\n",
    "                    low_52_week REAL,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    \n",
    "                    -- Foreign key to stocks table\n",
    "                    FOREIGN KEY (stock_id) REFERENCES stocks(id),\n",
    "                    \n",
    "                    -- Unique constraint to prevent duplicate entries\n",
    "                    -- This ensures one price record per stock per day\n",
    "                    UNIQUE(stock_id, trade_date)\n",
    "                );\n",
    "                \n",
    "                -- Composite index for time-range queries\n",
    "                -- This is the most important index for time-series queries\n",
    "                -- It allows efficient queries like:\n",
    "                -- WHERE stock_id = ? AND trade_date BETWEEN ? AND ?\n",
    "                CREATE INDEX IF NOT EXISTS idx_prices_stock_date \n",
    "                    ON stock_prices(stock_id, trade_date);\n",
    "                \n",
    "                -- Index for date-only queries (cross-sectional)\n",
    "                CREATE INDEX IF NOT EXISTS idx_prices_date \n",
    "                    ON stock_prices(trade_date);\n",
    "                \n",
    "                -- Index for price queries (e.g., finding stocks in price range)\n",
    "                CREATE INDEX IF NOT EXISTS idx_prices_close \n",
    "                    ON stock_prices(close_price);\n",
    "            ''',\n",
    "            \n",
    "            # Daily market summary table\n",
    "            # Stores aggregate statistics for each trading day\n",
    "            'market_summary': '''\n",
    "                CREATE TABLE IF NOT EXISTS market_summary (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    trade_date DATE UNIQUE NOT NULL,\n",
    "                    total_turnover REAL,\n",
    "                    total_volume INTEGER,\n",
    "                    total_transactions INTEGER,\n",
    "                    advancing_stocks INTEGER,\n",
    "                    declining_stocks INTEGER,\n",
    "                    unchanged_stocks INTEGER,\n",
    "                    index_value REAL,\n",
    "                    index_change REAL,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_market_summary_date \n",
    "                    ON market_summary(trade_date);\n",
    "            ''',\n",
    "            \n",
    "            # Technical indicators table\n",
    "            # Pre-calculated indicators for faster queries\n",
    "            'technical_indicators': '''\n",
    "                CREATE TABLE IF NOT EXISTS technical_indicators (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    stock_id INTEGER NOT NULL,\n",
    "                    trade_date DATE NOT NULL,\n",
    "                    sma_5 REAL,\n",
    "                    sma_10 REAL,\n",
    "                    sma_20 REAL,\n",
    "                    sma_50 REAL,\n",
    "                    ema_12 REAL,\n",
    "                    ema_26 REAL,\n",
    "                    rsi_14 REAL,\n",
    "                    macd REAL,\n",
    "                    macd_signal REAL,\n",
    "                    macd_histogram REAL,\n",
    "                    bollinger_upper REAL,\n",
    "                    bollinger_middle REAL,\n",
    "                    bollinger_lower REAL,\n",
    "                    atr_14 REAL,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    \n",
    "                    FOREIGN KEY (stock_id) REFERENCES stocks(id),\n",
    "                    UNIQUE(stock_id, trade_date)\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_indicators_stock_date \n",
    "                    ON technical_indicators(stock_id, trade_date);\n",
    "            ''',\n",
    "            \n",
    "            # Prediction results table\n",
    "            # Stores model predictions for tracking and analysis\n",
    "            'predictions': '''\n",
    "                CREATE TABLE IF NOT EXISTS predictions (\n",
    "                    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "                    stock_id INTEGER NOT NULL,\n",
    "                    prediction_date DATE NOT NULL,\n",
    "                    target_date DATE NOT NULL,\n",
    "                    model_name TEXT NOT NULL,\n",
    "                    model_version TEXT,\n",
    "                    predicted_close REAL,\n",
    "                    prediction_interval_lower REAL,\n",
    "                    prediction_interval_upper REAL,\n",
    "                    actual_close REAL,\n",
    "                    prediction_error REAL,\n",
    "                    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,\n",
    "                    \n",
    "                    FOREIGN KEY (stock_id) REFERENCES stocks(id),\n",
    "                    UNIQUE(stock_id, prediction_date, target_date, model_name)\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_predictions_stock \n",
    "                    ON predictions(stock_id);\n",
    "                CREATE INDEX IF NOT EXISTS idx_predictions_model \n",
    "                    ON predictions(model_name);\n",
    "            '''\n",
    "        }\n",
    "        \n",
    "        return schema\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_nepse_schema_postgresql() -> Dict[str, str]:\n",
    "        \"\"\"\n",
    "        Get PostgreSQL schema definition for NEPSE data.\n",
    "        \n",
    "        PostgreSQL offers advanced features for time-series data:\n",
    "        - Table partitioning for large datasets\n",
    "        - Advanced indexing (BRIN, GiST)\n",
    "        - JSONB for flexible data storage\n",
    "        - Powerful date/time functions\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping table names to CREATE TABLE statements\n",
    "        \"\"\"\n",
    "        schema = {\n",
    "            'stocks': '''\n",
    "                CREATE TABLE IF NOT EXISTS stocks (\n",
    "                    id SERIAL PRIMARY KEY,\n",
    "                    symbol VARCHAR(20) UNIQUE NOT NULL,\n",
    "                    company_name VARCHAR(255),\n",
    "                    sector VARCHAR(100),\n",
    "                    listed_date DATE,\n",
    "                    is_active BOOLEAN DEFAULT TRUE,\n",
    "                    metadata JSONB,\n",
    "                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n",
    "                    updated_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP\n",
    "                );\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_stocks_symbol ON stocks(symbol);\n",
    "                CREATE INDEX IF NOT EXISTS idx_stocks_sector ON stocks(sector);\n",
    "            ''',\n",
    "            \n",
    "            'stock_prices': '''\n",
    "                -- Partitioned table for stock prices\n",
    "                -- Partitioning by date range improves query performance\n",
    "                -- and makes data management easier (e.g., dropping old partitions)\n",
    "                CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "                    id BIGSERIAL,\n",
    "                    stock_id INTEGER NOT NULL REFERENCES stocks(id),\n",
    "                    trade_date DATE NOT NULL,\n",
    "                    open_price NUMERIC(12, 4),\n",
    "                    high_price NUMERIC(12, 4),\n",
    "                    low_price NUMERIC(12, 4),\n",
    "                    close_price NUMERIC(12, 4),\n",
    "                    ltp NUMERIC(12, 4),\n",
    "                    vwap NUMERIC(12, 4),\n",
    "                    volume BIGINT,\n",
    "                    turnover NUMERIC(18, 2),\n",
    "                    transactions INTEGER,\n",
    "                    prev_close NUMERIC(12, 4),\n",
    "                    diff NUMERIC(12, 4),\n",
    "                    price_range NUMERIC(12, 4),\n",
    "                    diff_percent NUMERIC(8, 4),\n",
    "                    range_percent NUMERIC(8, 4),\n",
    "                    vwap_percent NUMERIC(8, 4),\n",
    "                    high_52_week NUMERIC(12, 4),\n",
    "                    low_52_week NUMERIC(12, 4),\n",
    "                    created_at TIMESTAMP WITH TIME ZONE DEFAULT CURRENT_TIMESTAMP,\n",
    "                    PRIMARY KEY (id, trade_date)\n",
    "                ) PARTITION BY RANGE (trade_date);\n",
    "                \n",
    "                -- Create partitions for each year\n",
    "                -- This should be done dynamically as data grows\n",
    "                CREATE TABLE IF NOT EXISTS stock_prices_2023 \n",
    "                    PARTITION OF stock_prices\n",
    "                    FOR VALUES FROM ('2023-01-01') TO ('2024-01-01');\n",
    "                \n",
    "                CREATE TABLE IF NOT EXISTS stock_prices_2024 \n",
    "                    PARTITION OF stock_prices\n",
    "                    FOR VALUES FROM ('2024-01-01') TO ('2025-01-01');\n",
    "                \n",
    "                CREATE TABLE IF NOT EXISTS stock_prices_2025 \n",
    "                    PARTITION OF stock_prices\n",
    "                    FOR VALUES FROM ('2025-01-01') TO ('2026-01-01');\n",
    "                \n",
    "                -- BRIN index for time-series data\n",
    "                -- BRIN (Block Range INdex) is very efficient for large\n",
    "                -- time-series tables with naturally ordered data\n",
    "                CREATE INDEX IF NOT EXISTS idx_prices_stock_date_brin \n",
    "                    ON stock_prices USING BRIN (stock_id, trade_date);\n",
    "                \n",
    "                -- Standard B-tree index for exact lookups\n",
    "                CREATE INDEX IF NOT EXISTS idx_prices_stock_date_btree \n",
    "                    ON stock_prices(stock_id, trade_date);\n",
    "            ''',\n",
    "            \n",
    "            'stock_prices_denormalized': '''\n",
    "                -- Denormalized table for faster reads\n",
    "                -- Includes stock symbol directly for simpler queries\n",
    "                -- Trade-off: larger table size, redundancy\n",
    "                CREATE TABLE IF NOT EXISTS stock_prices_denormalized (\n",
    "                    id BIGSERIAL,\n",
    "                    symbol VARCHAR(20) NOT NULL,\n",
    "                    trade_date DATE NOT NULL,\n",
    "                    open_price NUMERIC(12, 4),\n",
    "                    high_price NUMERIC(12, 4),\n",
    "                    low_price NUMERIC(12, 4),\n",
    "                    close_price NUMERIC(12, 4),\n",
    "                    volume BIGINT,\n",
    "                    turnover NUMERIC(18, 2),\n",
    "                    sector VARCHAR(100),\n",
    "                    PRIMARY KEY (id, trade_date)\n",
    "                ) PARTITION BY RANGE (trade_date);\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_denorm_symbol_date \n",
    "                    ON stock_prices_denormalized(symbol, trade_date);\n",
    "            ''',\n",
    "            \n",
    "            'ohlc_materialized_view': '''\n",
    "                -- Materialized view for common OHLC queries\n",
    "                -- Pre-computes data for faster access\n",
    "                CREATE MATERIALIZED VIEW IF NOT EXISTS ohlc_daily AS\n",
    "                SELECT \n",
    "                    s.symbol,\n",
    "                    s.company_name,\n",
    "                    sp.trade_date,\n",
    "                    sp.open_price,\n",
    "                    sp.high_price,\n",
    "                    sp.low_price,\n",
    "                    sp.close_price,\n",
    "                    sp.volume,\n",
    "                    sp.turnover\n",
    "                FROM stock_prices sp\n",
    "                JOIN stocks s ON sp.stock_id = s.id\n",
    "                ORDER BY s.symbol, sp.trade_date;\n",
    "                \n",
    "                -- Refresh the materialized view periodically\n",
    "                -- REFRESH MATERIALIZED VIEW ohlc_daily;\n",
    "                \n",
    "                CREATE INDEX IF NOT EXISTS idx_ohlc_symbol_date \n",
    "                    ON ohlc_daily(symbol, trade_date);\n",
    "            '''\n",
    "        }\n",
    "        \n",
    "        return schema\n",
    "\n",
    "\n",
    "class SQLiteTimeSeriesDB:\n",
    "    \"\"\"\n",
    "    A class to manage SQLite database operations for time-series data.\n",
    "    \n",
    "    SQLite is ideal for:\n",
    "    - Development and testing\n",
    "    - Single-user applications\n",
    "    - Embedded systems\n",
    "    - Prototyping database schemas\n",
    "    - Desktop applications\n",
    "    \n",
    "    For the NEPSE prediction system, SQLite provides:\n",
    "    - Zero configuration (no server setup)\n",
    "    - Single file for easy backup and transfer\n",
    "    - Full SQL support\n",
    "    - Good performance for moderate datasets\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = './nepse.db'):\n",
    "        \"\"\"\n",
    "        Initialize the SQLite database connection.\n",
    "        \n",
    "        Args:\n",
    "            db_path: Path to the SQLite database file.\n",
    "                    If it doesn't exist, it will be created.\n",
    "        \"\"\"\n",
    "        self.db_path = Path(db_path)\n",
    "        self.db_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        self.conn = None\n",
    "        self.cursor = None\n",
    "        \n",
    "        # Connect and initialize schema\n",
    "        self._connect()\n",
    "        self._initialize_schema()\n",
    "    \n",
    "    def _connect(self):\n",
    "        \"\"\"\n",
    "        Establish connection to the SQLite database.\n",
    "        \n",
    "        SQLite connection options:\n",
    "        - check_same_thread: Allows sharing connection across threads\n",
    "        - isolation_level: Controls transaction behavior\n",
    "        - timeout: Wait time for locks\n",
    "        \"\"\"\n",
    "        self.conn = sqlite3.connect(\n",
    "            str(self.db_path),\n",
    "            check_same_thread=False,\n",
    "            isolation_level='DEFERRED',\n",
    "            timeout=30.0\n",
    "        )\n",
    "        \n",
    "        # Enable foreign key support (disabled by default in SQLite)\n",
    "        self.conn.execute('PRAGMA foreign_keys = ON')\n",
    "        \n",
    "        # Enable WAL mode for better concurrent access\n",
    "        # WAL (Write-Ahead Logging) allows readers and writers to work\n",
    "        # simultaneously, improving performance for read-heavy workloads\n",
    "        self.conn.execute('PRAGMA journal_mode = WAL')\n",
    "        \n",
    "        # Return rows as dictionaries for easier access\n",
    "        self.conn.row_factory = sqlite3.Row\n",
    "        \n",
    "        self.cursor = self.conn.cursor()\n",
    "    \n",
    "    def _initialize_schema(self):\n",
    "        \"\"\"\n",
    "        Initialize the database schema by creating all tables.\n",
    "        \"\"\"\n",
    "        schema = DatabaseSchema.get_nepse_schema_sqlite()\n",
    "        \n",
    "        for table_name, sql in schema.items():\n",
    "            try:\n",
    "                # Execute the SQL (may contain multiple statements)\n",
    "                # Split by semicolon and execute each statement\n",
    "                statements = [s.strip() for s in sql.split(';') if s.strip()]\n",
    "                for stmt in statements:\n",
    "                    self.cursor.execute(stmt)\n",
    "                self.conn.commit()\n",
    "                print(f\"Created/verified table: {table_name}\")\n",
    "            except sqlite3.Error as e:\n",
    "                print(f\"Error creating table {table_name}: {e}\")\n",
    "    \n",
    "    def insert_stock(self, \n",
    "                     symbol: str, \n",
    "                     company_name: str = None,\n",
    "                     sector: str = None,\n",
    "                     listed_date: str = None) -> int:\n",
    "        \"\"\"\n",
    "        Insert a new stock into the stocks table.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol (e.g., 'NABIL')\n",
    "            company_name: Full company name\n",
    "            sector: Business sector\n",
    "            listed_date: Date the stock was listed\n",
    "        \n",
    "        Returns:\n",
    "            The stock_id of the inserted or existing stock\n",
    "        \"\"\"\n",
    "        # Check if stock already exists\n",
    "        self.cursor.execute(\n",
    "            'SELECT id FROM stocks WHERE symbol = ?',\n",
    "            (symbol,)\n",
    "        )\n",
    "        row = self.cursor.fetchone()\n",
    "        \n",
    "        if row:\n",
    "            return row['id']\n",
    "        \n",
    "        # Insert new stock\n",
    "        self.cursor.execute('''\n",
    "            INSERT INTO stocks (symbol, company_name, sector, listed_date)\n",
    "            VALUES (?, ?, ?, ?)\n",
    "        ''', (symbol, company_name, sector, listed_date))\n",
    "        \n",
    "        self.conn.commit()\n",
    "        return self.cursor.lastrowid\n",
    "    \n",
    "    def insert_price_data(self, \n",
    "                          df: pd.DataFrame,\n",
    "                          batch_size: int = 1000) -> int:\n",
    "        \"\"\"\n",
    "        Insert price data from a DataFrame.\n",
    "        \n",
    "        This method handles the complete insertion process:\n",
    "        1. Ensures stocks exist in the stocks table\n",
    "        2. Inserts price data in batches for efficiency\n",
    "        3. Handles duplicates using INSERT OR IGNORE\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with columns matching the stock_prices table\n",
    "               Must include 'Symbol' and 'Date' columns\n",
    "            batch_size: Number of rows to insert per transaction\n",
    "        \n",
    "        Returns:\n",
    "            Number of rows inserted\n",
    "        \"\"\"\n",
    "        # First, ensure all stocks exist\n",
    "        symbols = df['Symbol'].unique()\n",
    "        stock_ids = {}\n",
    "        \n",
    "        for symbol in symbols:\n",
    "            stock_ids[symbol] = self.insert_stock(symbol)\n",
    "        \n",
    "        # Prepare data for insertion\n",
    "        rows_inserted = 0\n",
    "        \n",
    "        for i in range(0, len(df), batch_size):\n",
    "            batch = df.iloc[i:i + batch_size]\n",
    "            \n",
    "            for _, row in batch.iterrows():\n",
    "                try:\n",
    "                    stock_id = stock_ids.get(row['Symbol'])\n",
    "                    if stock_id is None:\n",
    "                        continue\n",
    "                    \n",
    "                    # Use INSERT OR IGNORE to handle duplicates\n",
    "                    # This silently skips rows that would violate unique constraints\n",
    "                    self.cursor.execute('''\n",
    "                        INSERT OR IGNORE INTO stock_prices (\n",
    "                            stock_id, trade_date,\n",
    "                            open_price, high_price, low_price, close_price,\n",
    "                            ltp, vwap, volume, turnover, transactions,\n",
    "                            prev_close, diff, price_range,\n",
    "                            diff_percent, range_percent, vwap_percent,\n",
    "                            high_52_week, low_52_week\n",
    "                        ) VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?, ?)\n",
    "                    ''', (\n",
    "                        stock_id,\n",
    "                        row.get('Date') or row.get('trade_date'),\n",
    "                        row.get('Open') or row.get('open_price'),\n",
    "                        row.get('High') or row.get('high_price'),\n",
    "                        row.get('Low') or row.get('low_price'),\n",
    "                        row.get('Close') or row.get('close_price'),\n",
    "                        row.get('LTP') or row.get('ltp'),\n",
    "                        row.get('VWAP') or row.get('vwap'),\n",
    "                        row.get('Vol') or row.get('Volume') or row.get('volume'),\n",
    "                        row.get('Turnover') or row.get('turnover'),\n",
    "                        row.get('Trans.') or row.get('transactions'),\n",
    "                        row.get('Prev. Close') or row.get('prev_close'),\n",
    "                        row.get('Diff') or row.get('diff'),\n",
    "                        row.get('Range') or row.get('price_range'),\n",
    "                        row.get('Diff %') or row.get('diff_percent'),\n",
    "                        row.get('Range %') or row.get('range_percent'),\n",
    "                        row.get('VWAP %') or row.get('vwap_percent'),\n",
    "                        row.get('52 Weeks High') or row.get('high_52_week'),\n",
    "                        row.get('52 Weeks Low') or row.get('low_52_week')\n",
    "                    ))\n",
    "                    \n",
    "                    if self.cursor.rowcount > 0:\n",
    "                        rows_inserted += 1\n",
    "                        \n",
    "                except sqlite3.Error as e:\n",
    "                    print(f\"Error inserting row: {e}\")\n",
    "                    continue\n",
    "            \n",
    "            # Commit after each batch\n",
    "            self.conn.commit()\n",
    "        \n",
    "        print(f\"Inserted {rows_inserted} rows\")\n",
    "        return rows_inserted\n",
    "    \n",
    "    def query_stock_data(self,\n",
    "                         symbol: str,\n",
    "                         start_date: str = None,\n",
    "                         end_date: str = None,\n",
    "                         columns: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query stock price data.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol to query\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "            columns: Specific columns to retrieve (None for all)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with the query results\n",
    "        \"\"\"\n",
    "        # Build the SELECT clause\n",
    "        if columns:\n",
    "            # Map common column names\n",
    "            column_mapping = {\n",
    "                'date': 'trade_date',\n",
    "                'open': 'open_price',\n",
    "                'high': 'high_price',\n",
    "                'low': 'low_price',\n",
    "                'close': 'close_price',\n",
    "                'volume': 'volume'\n",
    "            }\n",
    "            select_cols = [column_mapping.get(c.lower(), c) for c in columns]\n",
    "            select_clause = ', '.join(select_cols)\n",
    "        else:\n",
    "            select_clause = '''\n",
    "                sp.trade_date, sp.open_price, sp.high_price, sp.low_price, \n",
    "                sp.close_price, sp.volume, sp.turnover, sp.vwap,\n",
    "                sp.transactions, sp.diff, sp.price_range,\n",
    "                sp.diff_percent, sp.range_percent\n",
    "            '''\n",
    "        \n",
    "        # Build the WHERE clause\n",
    "        conditions = ['s.symbol = ?']\n",
    "        params = [symbol]\n",
    "        \n",
    "        if start_date:\n",
    "            conditions.append('sp.trade_date >= ?')\n",
    "            params.append(start_date)\n",
    "        \n",
    "        if end_date:\n",
    "            conditions.append('sp.trade_date <= ?')\n",
    "            params.append(end_date)\n",
    "        \n",
    "        where_clause = ' AND '.join(conditions)\n",
    "        \n",
    "        # Execute query\n",
    "        query = f'''\n",
    "            SELECT {select_clause}\n",
    "            FROM stock_prices sp\n",
    "            JOIN stocks s ON sp.stock_id = s.id\n",
    "            WHERE {where_clause}\n",
    "            ORDER BY sp.trade_date\n",
    "        '''\n",
    "        \n",
    "        self.cursor.execute(query, params)\n",
    "        rows = self.cursor.fetchall()\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        if rows:\n",
    "            df = pd.DataFrame([dict(row) for row in rows])\n",
    "            return df\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def query_cross_sectional(self,\n",
    "                              date: str,\n",
    "                              columns: List[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query all stocks' data for a specific date (cross-sectional).\n",
    "        \n",
    "        This query retrieves data for all stocks on a single day,\n",
    "        useful for:\n",
    "        - Market analysis\n",
    "        - Relative performance comparison\n",
    "        - Portfolio construction\n",
    "        \n",
    "        Args:\n",
    "            date: The date to query (YYYY-MM-DD)\n",
    "            columns: Specific columns to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with all stocks' data for the date\n",
    "        \"\"\"\n",
    "        if columns:\n",
    "            select_clause = ', '.join(columns)\n",
    "        else:\n",
    "            select_clause = '''\n",
    "                s.symbol, s.company_name, s.sector,\n",
    "                sp.trade_date, sp.open_price, sp.high_price, \n",
    "                sp.low_price, sp.close_price, sp.volume, sp.turnover\n",
    "            '''\n",
    "        \n",
    "        query = f'''\n",
    "            SELECT {select_clause}\n",
    "            FROM stock_prices sp\n",
    "            JOIN stocks s ON sp.stock_id = s.id\n",
    "            WHERE sp.trade_date = ?\n",
    "            ORDER BY sp.turnover DESC\n",
    "        '''\n",
    "        \n",
    "        self.cursor.execute(query, (date,))\n",
    "        rows = self.cursor.fetchall()\n",
    "        \n",
    "        if rows:\n",
    "            return pd.DataFrame([dict(row) for row in rows])\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def execute_query(self, query: str, params: tuple = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Execute a raw SQL query and return results as DataFrame.\n",
    "        \n",
    "        Args:\n",
    "            query: SQL query string\n",
    "            params: Query parameters (for parameterized queries)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with query results\n",
    "        \"\"\"\n",
    "        if params:\n",
    "            self.cursor.execute(query, params)\n",
    "        else:\n",
    "            self.cursor.execute(query)\n",
    "        \n",
    "        rows = self.cursor.fetchall()\n",
    "        \n",
    "        if rows:\n",
    "            return pd.DataFrame([dict(row) for row in rows])\n",
    "        else:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def get_table_info(self, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get information about a table's structure.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with column information\n",
    "        \"\"\"\n",
    "        query = f\"PRAGMA table_info({table_name})\"\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_index_info(self, table_name: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get information about indexes on a table.\n",
    "        \n",
    "        Args:\n",
    "            table_name: Name of the table\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with index information\n",
    "        \"\"\"\n",
    "        query = f\"PRAGMA index_list({table_name})\"\n",
    "        return self.execute_query(query)\n",
    "    \n",
    "    def get_database_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get statistics about the database.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with database statistics\n",
    "        \"\"\"\n",
    "        stats = {\n",
    "            'file_size_mb': self.db_path.stat().st_size / (1024 * 1024),\n",
    "            'tables': {}\n",
    "        }\n",
    "        \n",
    "        # Get stats for each table\n",
    "        tables = ['stocks', 'stock_prices', 'market_summary', 'technical_indicators']\n",
    "        \n",
    "        for table in tables:\n",
    "            try:\n",
    "                row_count = self.execute_query(\n",
    "                    f\"SELECT COUNT(*) as count FROM {table}\"\n",
    "                )\n",
    "                stats['tables'][table] = {\n",
    "                    'row_count': row_count['count'].iloc[0] if len(row_count) > 0 else 0\n",
    "                }\n",
    "            except:\n",
    "                stats['tables'][table] = {'row_count': 0}\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the database connection.\"\"\"\n",
    "        if self.conn:\n",
    "            self.conn.close()\n",
    "\n",
    "\n",
    "def demonstrate_sqlite_storage():\n",
    "    \"\"\"\n",
    "    Demonstrate SQLite storage operations with NEPSE data.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"SQLite Relational Database Storage Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Initialize database\n",
    "    db = SQLiteTimeSeriesDB('./nepse_analysis.db')\n",
    "    \n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = pd.date_range('2023-01-01', '2023-12-31', freq='B')\n",
    "    symbols = ['NABIL', 'NICA', 'SCBL', 'ADBL', 'EBL']\n",
    "    sectors = ['Banking', 'Banking', 'Banking', 'Banking', 'Banking']\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for symbol, sector in zip(symbols, sectors):\n",
    "            base_price = 300 + np.random.randint(0, 500)\n",
    "            data.append({\n",
    "                'Date': date,\n",
    "                'Symbol': symbol,\n",
    "                'Open': round(base_price * (1 + np.random.uniform(-0.02, 0.02)), 2),\n",
    "                'High': round(base_price * 1.03, 2),\n",
    "                'Low': round(base_price * 0.97, 2),\n",
    "                'Close': round(base_price * (1 + np.random.uniform(-0.01, 0.01)), 2),\n",
    "                'Volume': np.random.randint(10000, 500000),\n",
    "                'Turnover': round(base_price * np.random.randint(10000, 500000), 2),\n",
    "                'VWAP': round(base_price, 2),\n",
    "                'Trans.': np.random.randint(100, 1000),\n",
    "                'Diff': round(base_price * np.random.uniform(-0.02, 0.02), 2),\n",
    "                'Range': round(base_price * 0.06, 2),\n",
    "                'Diff %': round(np.random.uniform(-2, 2), 2),\n",
    "                'Range %': round(np.random.uniform(4, 8), 2),\n",
    "                'VWAP %': round(np.random.uniform(-1, 1), 2),\n",
    "                '52 Weeks High': round(base_price * 1.2, 2),\n",
    "                '52 Weeks Low': round(base_price * 0.8, 2)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"\\nGenerated {len(df)} records\")\n",
    "    \n",
    "    # Insert data\n",
    "    print(\"\\n1. Inserting Data\")\n",
    "    print(\"-\" * 40)\n",
    "    rows_inserted = db.insert_price_data(df)\n",
    "    \n",
    "    # Query data\n",
    "    print(\"\\n2. Querying Single Stock Data\")\n",
    "    print(\"-\" * 40)\n",
    "    nabil_data = db.query_stock_data(\n",
    "        symbol='NABIL',\n",
    "        start_date='2023-06-01',\n",
    "        end_date='2023-06-30'\n",
    "    )\n",
    "    print(f\"NABIL data for June 2023 ({len(nabil_data)} rows):\")\n",
    "    print(nabil_data.head())\n",
    "    \n",
    "    # Cross-sectional query\n",
    "    print(\"\\n3. Cross-Sectional Query (All Stocks for a Date)\")\n",
    "    print(\"-\" * 40)\n",
    "    cross_section = db.query_cross_sectional('2023-06-15')\n",
    "    print(f\"All stocks on 2023-06-15 ({len(cross_section)} rows):\")\n",
    "    print(cross_section[['symbol', 'close_price', 'volume', 'turnover']])\n",
    "    \n",
    "    # Custom query\n",
    "    print(\"\\n4. Custom SQL Query - Top Performers\")\n",
    "    print(\"-\" * 40)\n",
    "    top_performers = db.execute_query('''\n",
    "        SELECT \n",
    "            s.symbol,\n",
    "            COUNT(*) as trading_days,\n",
    "            AVG(sp.close_price) as avg_close,\n",
    "            MAX(sp.close_price) as high_close,\n",
    "            MIN(sp.close_price) as low_close,\n",
    "            SUM(sp.volume) as total_volume,\n",
    "            SUM(sp.turnover) as total_turnover\n",
    "        FROM stock_prices sp\n",
    "        JOIN stocks s ON sp.stock_id = s.id\n",
    "        GROUP BY s.symbol\n",
    "        ORDER BY total_turnover DESC\n",
    "    ''')\n",
    "    print(top_performers)\n",
    "    \n",
    "    # Database stats\n",
    "    print(\"\\n5. Database Statistics\")\n",
    "    print(\"-\" * 40)\n",
    "    stats = db.get_database_stats()\n",
    "    print(f\"Database size: {stats['file_size_mb']:.2f} MB\")\n",
    "    print(f\"Stocks table: {stats['tables']['stocks']['row_count']} rows\")\n",
    "    print(f\"Prices table: {stats['tables']['stock_prices']['row_count']} rows\")\n",
    "    \n",
    "    # Table structure\n",
    "    print(\"\\n6. Table Structure\")\n",
    "    print(\"-\" * 40)\n",
    "    table_info = db.get_table_info('stock_prices')\n",
    "    print(table_info[['name', 'type', 'notnull', 'pk']])\n",
    "    \n",
    "    # Indexes\n",
    "    print(\"\\n7. Indexes\")\n",
    "    print(\"-\" * 40)\n",
    "    index_info = db.get_index_info('stock_prices')\n",
    "    print(index_info)\n",
    "    \n",
    "    db.close()\n",
    "    return db, df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    db, df = demonstrate_sqlite_storage()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Schema Design Choices**:\n",
    "   - **Normalized Design**: The `stocks` table stores stock metadata separately from price data. This:\n",
    "     - Reduces redundancy (symbol, company_name stored once)\n",
    "     - Enables easy updates to stock metadata\n",
    "     - Requires joins for most queries\n",
    "   \n",
    "   - **Foreign Keys**: The `stock_id` in `stock_prices` references `stocks(id)`. This:\n",
    "     - Ensures referential integrity\n",
    "     - Prevents orphaned records\n",
    "     - Enables cascading updates/deletes\n",
    "\n",
    "   - **Unique Constraints**: `UNIQUE(stock_id, trade_date)` prevents duplicate entries for the same stock on the same day.\n",
    "\n",
    "2. **Indexing Strategy**:\n",
    "   - **Composite Index** `idx_prices_stock_date`: The most important index for time-series queries. It allows efficient queries filtering by both stock and date range.\n",
    "   - **Single-column Index** `idx_prices_date`: For cross-sectional queries that need all stocks for a specific date.\n",
    "   - **Column Index** `idx_prices_close`: For queries filtering by price (e.g., stocks in a price range).\n",
    "\n",
    "3. **PostgreSQL-Specific Features**:\n",
    "   - **Table Partitioning**: The `PARTITION BY RANGE (trade_date)` splits the table by year. Benefits:\n",
    "     - Queries on a date range only scan relevant partitions\n",
    "     - Old data can be archived by dropping partitions\n",
    "     - Each partition can have its own storage settings\n",
    "   \n",
    "   - **BRIN Index**: Block Range INdex is very efficient for large, naturally ordered datasets like time-series. It stores summary information about blocks of data, making it much smaller than B-tree indexes.\n",
    "\n",
    "   - **Materialized Views**: Pre-computed queries that can be refreshed periodically. Great for expensive aggregations that are queried frequently.\n",
    "\n",
    "4. **SQLite Features**:\n",
    "   - **WAL Mode**: Write-Ahead Logging allows concurrent readers during writes, improving performance for read-heavy workloads.\n",
    "   - **Foreign Keys**: Must be explicitly enabled with `PRAGMA foreign_keys = ON`.\n",
    "   - **INSERT OR IGNORE**: Handles duplicates gracefully without raising errors.\n",
    "\n",
    "5. **Batch Insertion**: The `insert_price_data` method inserts data in batches:\n",
    "   - Reduces transaction overhead\n",
    "   - Provides progress feedback\n",
    "   - Allows partial success if errors occur\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### **8.3.2 Indexing Strategies**\n",
    "\n",
    "Proper indexing is crucial for time-series database performance. Without appropriate indexes, queries on large time-series datasets can take minutes or hours instead of milliseconds.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Database Indexing Strategies for Time-Series Data\n",
    "\n",
    "This module demonstrates various indexing strategies optimized for\n",
    "time-series queries, using the NEPSE stock data as an example.\n",
    "\n",
    "Key concepts:\n",
    "1. B-Tree indexes (default) - good for equality and range queries\n",
    "2. BRIN indexes (PostgreSQL) - efficient for large, naturally ordered data\n",
    "3. Partial indexes - index only relevant data (e.g., active stocks only)\n",
    "4. Covering indexes - include all columns needed for a query\n",
    "5. Composite indexes - multi-column indexes for specific query patterns\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Tuple\n",
    "import time\n",
    "\n",
    "\n",
    "class IndexingStrategy:\n",
    "    \"\"\"\n",
    "    Demonstrates various indexing strategies for time-series data.\n",
    "    \n",
    "    For the NEPSE prediction system, we need indexes that support:\n",
    "    1. Time-range queries: \"Get prices for NABIL from Jan to Mar\"\n",
    "    2. Cross-sectional queries: \"Get all stocks for 2024-01-15\"\n",
    "    3. Aggregate queries: \"Average closing price by month\"\n",
    "    4. Point queries: \"Get specific date's closing price\"\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = './nepse_indexing.db'):\n",
    "        self.db_path = db_path\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.conn.execute('PRAGMA foreign_keys = ON')\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self._setup_tables()\n",
    "    \n",
    "    def _setup_tables(self):\n",
    "        \"\"\"Create tables with various indexing strategies.\"\"\"\n",
    "        # Table without indexes (baseline for comparison)\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS prices_no_index (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                symbol TEXT,\n",
    "                trade_date DATE,\n",
    "                close_price REAL,\n",
    "                volume INTEGER\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Table with basic single-column indexes\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS prices_basic_index (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                symbol TEXT,\n",
    "                trade_date DATE,\n",
    "                close_price REAL,\n",
    "                volume INTEGER\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create basic indexes\n",
    "        self.cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_basic_symbol \n",
    "            ON prices_basic_index(symbol)\n",
    "        ''')\n",
    "        self.cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_basic_date \n",
    "            ON prices_basic_index(trade_date)\n",
    "        ''')\n",
    "        \n",
    "        # Table with composite index (optimal for time-series)\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS prices_composite (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                symbol TEXT,\n",
    "                trade_date DATE,\n",
    "                close_price REAL,\n",
    "                volume INTEGER\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Composite index: symbol first, then date\n",
    "        # This is optimal for queries filtering by symbol and date range\n",
    "        self.cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_composite_symbol_date \n",
    "            ON prices_composite(symbol, trade_date)\n",
    "        ''')\n",
    "        \n",
    "        # Covering index: includes all columns needed for common queries\n",
    "        # This allows \"index-only scans\" where the database never touches the table\n",
    "        self.cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_composite_covering \n",
    "            ON prices_composite(symbol, trade_date, close_price, volume)\n",
    "        ''')\n",
    "        \n",
    "        # Partial index: only index high-volume trading days\n",
    "        # This reduces index size and maintenance cost\n",
    "        self.cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_composite_high_volume \n",
    "            ON prices_composite(symbol, trade_date) \n",
    "            WHERE volume > 100000\n",
    "        ''')\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    def generate_test_data(self, num_records: int = 100000):\n",
    "        \"\"\"Generate test data for performance comparison.\"\"\"\n",
    "        symbols = ['NABIL', 'NICA', 'SCBL', 'ADBL', 'EBL']\n",
    "        start_date = datetime(2020, 1, 1)\n",
    "        \n",
    "        print(f\"Generating {num_records} test records...\")\n",
    "        \n",
    "        # Generate data\n",
    "        data = []\n",
    "        for i in range(num_records):\n",
    "            symbol = symbols[i % len(symbols)]\n",
    "            days_offset = i // len(symbols)\n",
    "            trade_date = start_date + timedelta(days=days_offset)\n",
    "            \n",
    "            data.append((\n",
    "                symbol,\n",
    "                trade_date.strftime('%Y-%m-%d'),\n",
    "                100 + np.random.random() * 900,  # Price between 100-1000\n",
    "                np.random.randint(1000, 1000000)  # Volume\n",
    "            ))\n",
    "        \n",
    "        # Insert into all tables\n",
    "        for table in ['prices_no_index', 'prices_basic_index', 'prices_composite']:\n",
    "            self.cursor.executemany(f'''\n",
    "                INSERT INTO {table} (symbol, trade_date, close_price, volume)\n",
    "                VALUES (?, ?, ?, ?)\n",
    "            ''', data)\n",
    "        \n",
    "        self.conn.commit()\n",
    "        print(\"Data inserted successfully\")\n",
    "    \n",
    "    def benchmark_query(self, query: str, params: tuple = (), \n",
    "                       iterations: int = 10) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Benchmark a query across different table configurations.\n",
    "        \n",
    "        Args:\n",
    "            query: SQL query template with {table} placeholder\n",
    "            params: Query parameters\n",
    "            iterations: Number of times to run each query\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with timing results\n",
    "        \"\"\"\n",
    "        results = {}\n",
    "        \n",
    "        tables = {\n",
    "            'No Index': 'prices_no_index',\n",
    "            'Basic Index': 'prices_basic_index',\n",
    "            'Composite Index': 'prices_composite'\n",
    "        }\n",
    "        \n",
    "        for config_name, table_name in tables.items():\n",
    "            # Replace table name in query\n",
    "            actual_query = query.format(table=table_name)\n",
    "            \n",
    "            # Warm up\n",
    "            self.cursor.execute(actual_query, params)\n",
    "            self.cursor.fetchall()\n",
    "            \n",
    "            # Benchmark\n",
    "            times = []\n",
    "            for _ in range(iterations):\n",
    "                start = time.time()\n",
    "                self.cursor.execute(actual_query, params)\n",
    "                self.cursor.fetchall()\n",
    "                times.append(time.time() - start)\n",
    "            \n",
    "            results[config_name] = {\n",
    "                'avg_time_ms': np.mean(times) * 1000,\n",
    "                'min_time_ms': np.min(times) * 1000,\n",
    "                'max_time_ms': np.max(times) * 1000\n",
    "            }\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def demonstrate_time_range_query(self):\n",
    "        \"\"\"\n",
    "        Benchmark time-range queries (most common in time-series).\n",
    "        \n",
    "        Query pattern: Get all prices for a specific symbol in a date range\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"Benchmark: Time-Range Query\")\n",
    "        print(\"Pattern: SELECT * FROM table WHERE symbol = ? AND trade_date BETWEEN ? AND ?\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        query = '''\n",
    "            SELECT * FROM {table} \n",
    "            WHERE symbol = ? AND trade_date BETWEEN ? AND ?\n",
    "        '''\n",
    "        \n",
    "        results = self.benchmark_query(\n",
    "            query,\n",
    "            params=('NABIL', '2023-01-01', '2023-12-31'),\n",
    "            iterations=50\n",
    "        )\n",
    "        \n",
    "        for config, metrics in results.items():\n",
    "            print(f\"{config:20s}: {metrics['avg_time_ms']:.2f} ms \"\n",
    "                  f\"(min: {metrics['min_time_ms']:.2f}, \"\n",
    "                  f\"max: {metrics['max_time_ms']:.2f})\")\n",
    "        \n",
    "        # Explain query plans\n",
    "        print(\"\\nQuery Plans:\")\n",
    "        for config, table in [('No Index', 'prices_no_index'), \n",
    "                              ('Composite', 'prices_composite')]:\n",
    "            print(f\"\\n{config}:\")\n",
    "            cursor = self.conn.execute(f'EXPLAIN QUERY PLAN {query.format(table=table)}',\n",
    "                                       ('NABIL', '2023-01-01', '2023-12-31'))\n",
    "            for row in cursor:\n",
    "                print(f\"  {row}\")\n",
    "    \n",
    "    def demonstrate_cross_sectional_query(self):\n",
    "        \"\"\"\n",
    "        Benchmark cross-sectional queries (all stocks for one date).\n",
    "        \n",
    "        Query pattern: Get all stocks for a specific trading day\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"Benchmark: Cross-Sectional Query\")\n",
    "        print(\"Pattern: SELECT * FROM table WHERE trade_date = ?\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        query = 'SELECT * FROM {table} WHERE trade_date = ?'\n",
    "        \n",
    "        results = self.benchmark_query(\n",
    "            query,\n",
    "            params=('2023-06-15',),\n",
    "            iterations=50\n",
    "        )\n",
    "        \n",
    "        for config, metrics in results.items():\n",
    "            print(f\"{config:20s}: {metrics['avg_time_ms']:.2f} ms\")\n",
    "    \n",
    "    def demonstrate_aggregation_query(self):\n",
    "        \"\"\"\n",
    "        Benchmark aggregation queries (analytical workloads).\n",
    "        \n",
    "        Query pattern: Monthly average closing price by symbol\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"Benchmark: Aggregation Query\")\n",
    "        print(\"Pattern: SELECT symbol, AVG(close_price) GROUP BY symbol\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        query = '''\n",
    "            SELECT symbol, AVG(close_price), MAX(close_price), MIN(close_price)\n",
    "            FROM {table}\n",
    "            WHERE trade_date BETWEEN ? AND ?\n",
    "            GROUP BY symbol\n",
    "        '''\n",
    "        \n",
    "        results = self.benchmark_query(\n",
    "            query,\n",
    "            params=('2023-01-01', '2023-12-31'),\n",
    "            iterations=20\n",
    "        )\n",
    "        \n",
    "        for config, metrics in results.items():\n",
    "            print(f\"{config:20s}: {metrics['avg_time_ms']:.2f} ms\")\n",
    "    \n",
    "    def explain_index_usage(self):\n",
    "        \"\"\"\n",
    "        Demonstrate how to check if indexes are being used.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"Index Usage Analysis\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # SQLite EXPLAIN QUERY PLAN\n",
    "        print(\"\\n1. Query Plan for Symbol Lookup:\")\n",
    "        cursor = self.conn.execute('''\n",
    "            EXPLAIN QUERY PLAN\n",
    "            SELECT * FROM prices_composite \n",
    "            WHERE symbol = 'NABIL' AND trade_date > '2023-06-01'\n",
    "        ''')\n",
    "        \n",
    "        for row in cursor:\n",
    "            print(f\"  {row}\")\n",
    "        \n",
    "        # Show index info\n",
    "        print(\"\\n2. Available Indexes on prices_composite:\")\n",
    "        cursor = self.conn.execute(\n",
    "            \"SELECT name, sql FROM sqlite_master WHERE type='index' AND tbl_name='prices_composite'\"\n",
    "        )\n",
    "        for row in cursor:\n",
    "            print(f\"  {row['name']}: {row['sql']}\")\n",
    "    \n",
    "    def get_index_statistics(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get statistics about indexes (SQLite specific).\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with index statistics\n",
    "        \"\"\"\n",
    "        # Query sqlite_stat tables if they exist\n",
    "        try:\n",
    "            cursor = self.conn.execute(\n",
    "                \"SELECT * FROM sqlite_master WHERE type='index'\"\n",
    "            )\n",
    "            indexes = cursor.fetchall()\n",
    "            \n",
    "            data = []\n",
    "            for idx in indexes:\n",
    "                data.append({\n",
    "                    'name': idx['name'],\n",
    "                    'table': idx['tbl_name'],\n",
    "                    'sql': idx['sql'][:100] + '...' if len(idx['sql']) > 100 else idx['sql']\n",
    "                })\n",
    "            \n",
    "            return pd.DataFrame(data)\n",
    "        except:\n",
    "            return pd.DataFrame()\n",
    "    \n",
    "    def close(self):\n",
    "        self.conn.close()\n",
    "\n",
    "\n",
    "def demonstrate_indexing_strategies():\n",
    "    \"\"\"Run the indexing strategy demonstration.\"\"\"\n",
    "    indexer = IndexingStrategy()\n",
    "    \n",
    "    # Generate data\n",
    "    indexer.generate_test_data(50000)\n",
    "    \n",
    "    # Run benchmarks\n",
    "    indexer.demonstrate_time_range_query()\n",
    "    indexer.demonstrate_cross_sectional_query()\n",
    "    indexer.demonstrate_aggregation_query()\n",
    "    indexer.explain_index_usage()\n",
    "    \n",
    "    # Show indexes\n",
    "    print(\"\\nIndex Statistics:\")\n",
    "    stats = indexer.get_index_statistics()\n",
    "    print(stats)\n",
    "    \n",
    "    indexer.close()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_indexing_strategies()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **B-Tree Indexes**: The default index type in most databases. They balance lookup speed, insertion speed, and storage efficiency. For time-series:\n",
    "   - Single-column index on `symbol`: Fast for symbol lookups but requires separate index for dates\n",
    "   - Single-column index on `trade_date`: Fast for date lookups but not for symbol-specific queries\n",
    "\n",
    "2. **Composite Indexes**: Multi-column indexes where order matters. For `INDEX(symbol, trade_date)`:\n",
    "   - Optimized for `WHERE symbol = ? AND trade_date BETWEEN ? AND ?`\n",
    "   - Can also satisfy `WHERE symbol = ?` (leftmost prefix rule)\n",
    "   - Cannot efficiently satisfy `WHERE trade_date = ?` alone (symbol comes first)\n",
    "\n",
    "3. **Covering Indexes**: Include all columns needed for a query. For example, if you always query `symbol`, `trade_date`, `close_price`, and `volume`, a covering index on all four columns allows the database to answer the query using only the index, without touching the table (index-only scan).\n",
    "\n",
    "4. **Partial Indexes**: Index only a subset of data. For example, indexing only high-volume days (`WHERE volume > 100000`):\n",
    "   - Smaller index size\n",
    "   - Faster index maintenance\n",
    "   - Optimized for specific query patterns\n",
    "\n",
    "5. **Query Plan Analysis**: The `EXPLAIN QUERY PLAN` command shows how the database executes queries:\n",
    "   - \"SCAN TABLE\" means full table scan (slow for large tables)\n",
    "   - \"SEARCH TABLE USING INDEX\" means index lookup (fast)\n",
    "   - \"USING COVERING INDEX\" means index-only scan (fastest)\n",
    "\n",
    "### **8.3.3 Query Optimization**\n",
    "\n",
    "Optimizing queries for time-series data involves understanding execution plans, using appropriate query patterns, and leveraging database-specific features.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Query Optimization Techniques for Time-Series Data\n",
    "\n",
    "This module demonstrates optimization techniques for common time-series\n",
    "queries in the NEPSE prediction system.\n",
    "\"\"\"\n",
    "\n",
    "import sqlite3\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any\n",
    "\n",
    "\n",
    "class QueryOptimizer:\n",
    "    \"\"\"\n",
    "    Demonstrates query optimization techniques for time-series data.\n",
    "    \n",
    "    Key optimization strategies:\n",
    "    1. Use appropriate indexes (covered in 8.3.2)\n",
    "    2. Filter early (WHERE clauses before JOINs)\n",
    "    3. Use covering indexes for aggregations\n",
    "    4. Limit result sets when possible\n",
    "    5. Use window functions for time-series calculations\n",
    "    6. Materialize expensive calculations\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, db_path: str = './nepse_optimized.db'):\n",
    "        self.conn = sqlite3.connect(db_path)\n",
    "        self.conn.execute('PRAGMA foreign_keys = ON')\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self._setup_schema()\n",
    "    \n",
    "    def _setup_schema(self):\n",
    "        \"\"\"Set up optimized schema with materialized views.\"\"\"\n",
    "        # Main prices table\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "                id INTEGER PRIMARY KEY,\n",
    "                symbol TEXT,\n",
    "                trade_date DATE,\n",
    "                open_price REAL,\n",
    "                high_price REAL,\n",
    "                low_price REAL,\n",
    "                close_price REAL,\n",
    "                volume INTEGER\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Create optimized indexes\n",
    "        self.cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_prices_sym_date_vol \n",
    "            ON stock_prices(symbol, trade_date, volume)\n",
    "        ''')\n",
    "        \n",
    "        # Create a summary table (materialized view pattern)\n",
    "        # This pre-computes daily aggregates for fast querying\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS daily_summary (\n",
    "                trade_date DATE PRIMARY KEY,\n",
    "                total_volume INTEGER,\n",
    "                total_turnover REAL,\n",
    "                num_stocks INTEGER,\n",
    "                advancers INTEGER,\n",
    "                decliners INTEGER\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    def optimized_time_range_query(self, \n",
    "                                   symbol: str, \n",
    "                                   start_date: str, \n",
    "                                   end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Optimized query for time-range data retrieval.\n",
    "        \n",
    "        Optimization techniques used:\n",
    "        1. Specific column selection (avoid SELECT *)\n",
    "        2. Proper index usage (symbol, date)\n",
    "        3. Parameterized queries (prevent re-parsing)\n",
    "        4. Fetching in batches for large results\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with price data\n",
    "        \"\"\"\n",
    "        # OPTIMIZED: Select only needed columns\n",
    "        # Avoids memory overhead of unused columns\n",
    "        query = '''\n",
    "            SELECT \n",
    "                trade_date,\n",
    "                open_price,\n",
    "                high_price,\n",
    "                low_price,\n",
    "                close_price,\n",
    "                volume\n",
    "            FROM stock_prices\n",
    "            WHERE symbol = ?\n",
    "              AND trade_date >= ?\n",
    "              AND trade_date <= ?\n",
    "            ORDER BY trade_date ASC\n",
    "        '''\n",
    "        \n",
    "        # Use parameterized query for security and performance\n",
    "        # Parameterized queries are cached by the query planner\n",
    "        df = pd.read_sql_query(\n",
    "            query, \n",
    "            self.conn, \n",
    "            params=(symbol, start_date, end_date),\n",
    "            parse_dates=['trade_date']\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def optimized_cross_sectional(self, \n",
    "                                  trade_date: str,\n",
    "                                  min_volume: int = 10000) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Optimized cross-sectional query with filtering.\n",
    "        \n",
    "        Args:\n",
    "            trade_date: Date to query\n",
    "            min_volume: Minimum volume filter\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with all stocks for the date\n",
    "        \"\"\"\n",
    "        # OPTIMIZED: Filter on indexed columns first\n",
    "        # The database can use the index to quickly find the date range\n",
    "        query = '''\n",
    "            SELECT \n",
    "                symbol,\n",
    "                close_price,\n",
    "                volume,\n",
    "                (close_price - LAG(close_price) OVER (ORDER BY symbol)) / \n",
    "                    LAG(close_price) OVER (ORDER BY symbol) * 100 as price_change_pct\n",
    "            FROM stock_prices\n",
    "            WHERE trade_date = ?\n",
    "              AND volume >= ?\n",
    "            ORDER BY volume DESC\n",
    "        '''\n",
    "        \n",
    "        return pd.read_sql_query(\n",
    "            query,\n",
    "            self.conn,\n",
    "            params=(trade_date, min_volume)\n",
    "        )\n",
    "    \n",
    "    def optimized_rolling_average(self,\n",
    "                                   symbol: str,\n",
    "                                   window: int = 20) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate rolling average using SQL window functions.\n",
    "        \n",
    "        Window functions are more efficient than self-joins or\n",
    "        fetching all data and calculating in Python.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            window: Rolling window size (days)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with rolling averages\n",
    "        \"\"\"\n",
    "        # Use SQLite window functions (available in SQLite 3.25+)\n",
    "        query = f'''\n",
    "            SELECT \n",
    "                trade_date,\n",
    "                close_price,\n",
    "                AVG(close_price) OVER (\n",
    "                    ORDER BY trade_date \n",
    "                    ROWS BETWEEN {window-1} PRECEDING AND CURRENT ROW\n",
    "                ) as sma_{window},\n",
    "                AVG(volume) OVER (\n",
    "                    ORDER BY trade_date \n",
    "                    ROWS BETWEEN {window-1} PRECEDING AND CURRENT ROW\n",
    "                ) as avg_volume\n",
    "            FROM stock_prices\n",
    "            WHERE symbol = ?\n",
    "            ORDER BY trade_date\n",
    "        '''\n",
    "        \n",
    "        return pd.read_sql_query(query, self.conn, params=(symbol,))\n",
    "    \n",
    "    def batch_insert_optimization(self, \n",
    "                                   df: pd.DataFrame,\n",
    "                                   batch_size: int = 1000) -> int:\n",
    "        \"\"\"\n",
    "        Optimized batch insertion with transaction management.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to insert\n",
    "            batch_size: Number of rows per batch\n",
    "        \n",
    "        Returns:\n",
    "            Number of rows inserted\n",
    "        \"\"\"\n",
    "        # Convert DataFrame to list of tuples for executemany\n",
    "        data = [(\n",
    "            row['symbol'],\n",
    "            row['trade_date'],\n",
    "            row['open_price'],\n",
    "            row['high_price'],\n",
    "            row['low_price'],\n",
    "            row['close_price'],\n",
    "            row['volume']\n",
    "        ) for _, row in df.iterrows()]\n",
    "        \n",
    "        total_inserted = 0\n",
    "        \n",
    "        # Use single transaction for entire batch\n",
    "        try:\n",
    "            self.cursor.execute('BEGIN TRANSACTION')\n",
    "            \n",
    "            for i in range(0, len(data), batch_size):\n",
    "                batch = data[i:i+batch_size]\n",
    "                self.cursor.executemany('''\n",
    "                    INSERT INTO stock_prices \n",
    "                    (symbol, trade_date, open_price, high_price, \n",
    "                     low_price, close_price, volume)\n",
    "                    VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "                ''', batch)\n",
    "                total_inserted += len(batch)\n",
    "            \n",
    "            self.conn.commit()\n",
    "            \n",
    "        except Exception as e:\n",
    "            self.conn.rollback()\n",
    "            raise e\n",
    "        \n",
    "        return total_inserted\n",
    "    \n",
    "    def close(self):\n",
    "        self.conn.close()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Column Selection**: Always specify columns in SELECT statements. `SELECT *` forces the database to read all columns, even if you only need a few, increasing I/O and memory usage.\n",
    "\n",
    "2. **Parameterized Queries**: Using `?` placeholders instead of string formatting:\n",
    "   - Prevents SQL injection attacks\n",
    "   - Allows query plan caching (the database reuses the execution plan)\n",
    "   - Handles data type conversion automatically\n",
    "\n",
    "3. **Window Functions**: SQL window functions (`OVER`, `PARTITION BY`, `ROWS BETWEEN`) calculate rolling statistics efficiently in the database:\n",
    "   - Avoids transferring large datasets to Python\n",
    "   - Uses optimized algorithms in the database engine\n",
    "   - Handles edge cases (beginning of series) automatically\n",
    "\n",
    "4. **Transaction Management**: Wrapping batch inserts in explicit transactions:\n",
    "   - Reduces disk I/O (commit once instead of per row)\n",
    "   - Ensures atomicity (all or nothing)\n",
    "   - Improves performance by orders of magnitude\n",
    "\n",
    "---\n",
    "\n",
    "## **8.4 Time-Series Databases**\n",
    "\n",
    "Specialized time-series databases are optimized for handling time-stamped data. They provide superior performance for ingestion, storage, and querying of time-series data compared to general-purpose databases.\n",
    "\n",
    "### **8.4.1 InfluxDB**\n",
    "\n",
    "InfluxDB is a purpose-built time-series database designed for high write loads and time-range queries. It's particularly popular for monitoring, IoT, and financial data.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "InfluxDB Storage Module for NEPSE Time-Series Data\n",
    "\n",
    "InfluxDB is optimized for:\n",
    "- High write throughput (millions of points per second)\n",
    "- Time-range queries\n",
    "- Downsampling and retention policies\n",
    "- Tag-based indexing (inverted index)\n",
    "\n",
    "Key concepts:\n",
    "- Measurement: Similar to a table (e.g., \"stock_prices\")\n",
    "- Tags: Indexed metadata (e.g., symbol, sector) - use for filtering\n",
    "- Fields: Non-indexed data (e.g., price, volume) - use for values\n",
    "- Timestamp: The time field (nanosecond precision)\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Note: Install influxdb-client: pip install influxdb-client\n",
    "try:\n",
    "    from influxdb_client import InfluxDBClient, Point\n",
    "    from influxdb_client.client.write_api import SYNCHRONOUS\n",
    "    INFLUX_AVAILABLE = True\n",
    "except ImportError:\n",
    "    INFLUX_AVAILABLE = False\n",
    "    print(\"Warning: influxdb-client not installed. Install with: pip install influxdb-client\")\n",
    "\n",
    "\n",
    "class InfluxDBTimeSeriesStorage:\n",
    "    \"\"\"\n",
    "    A class to handle InfluxDB operations for NEPSE stock data.\n",
    "    \n",
    "    Schema Design for NEPSE in InfluxDB:\n",
    "    Measurement: stock_prices\n",
    "    Tags: symbol, sector (indexed, low cardinality)\n",
    "    Fields: open, high, low, close, volume, turnover (non-indexed values)\n",
    "    Timestamp: trade_date\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 url: str = \"http://localhost:8086\",\n",
    "                 token: str = \"your-token\",\n",
    "                 org: str = \"nepse-org\",\n",
    "                 bucket: str = \"nepse-bucket\"):\n",
    "        \"\"\"\n",
    "        Initialize InfluxDB connection.\n",
    "        \n",
    "        Args:\n",
    "            url: InfluxDB server URL\n",
    "            token: Authentication token\n",
    "            org: Organization name\n",
    "            bucket: Bucket (database) name\n",
    "        \"\"\"\n",
    "        if not INFLUX_AVAILABLE:\n",
    "            raise ImportError(\"influxdb-client required\")\n",
    "        \n",
    "        self.url = url\n",
    "        self.token = token\n",
    "        self.org = org\n",
    "        self.bucket = bucket\n",
    "        \n",
    "        self.client = InfluxDBClient(url=url, token=token, org=org)\n",
    "        self.write_api = self.client.write_api(write_options=SYNCHRONOUS)\n",
    "        self.query_api = self.client.query_api()\n",
    "    \n",
    "    def write_stock_data(self, df: pd.DataFrame, \n",
    "                         measurement: str = \"stock_prices\") -> None:\n",
    "        \"\"\"\n",
    "        Write stock data to InfluxDB.\n",
    "        \n",
    "        Data Model:\n",
    "        - Measurement: stock_prices\n",
    "        - Tags: symbol (indexed for fast filtering)\n",
    "        - Fields: open, high, low, close, volume\n",
    "        - Timestamp: trade_date\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with columns: Symbol, Date, Open, High, Low, Close, Volume\n",
    "            measurement: InfluxDB measurement name\n",
    "        \"\"\"\n",
    "        points = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Create a Point (InfluxDB data structure)\n",
    "            point = Point(measurement)\\\n",
    "                .tag(\"symbol\", row['Symbol'])\\\n",
    "                .field(\"open\", float(row['Open']))\\\n",
    "                .field(\"high\", float(row['High']))\\\n",
    "                .field(\"low\", float(row['Low']))\\\n",
    "                .field(\"close\", float(row['Close']))\\\n",
    "                .field(\"volume\", int(row['Volume']))\n",
    "            \n",
    "            # Add optional fields if present\n",
    "            if 'Turnover' in row:\n",
    "                point = point.field(\"turnover\", float(row['Turnover']))\n",
    "            if 'VWAP' in row:\n",
    "                point = point.field(\"vwap\", float(row['VWAP']))\n",
    "            \n",
    "            # Set timestamp\n",
    "            if isinstance(row['Date'], str):\n",
    "                timestamp = datetime.strptime(row['Date'], '%Y-%m-%d')\n",
    "            else:\n",
    "                timestamp = row['Date']\n",
    "            \n",
    "            point = point.time(timestamp)\n",
    "            points.append(point)\n",
    "        \n",
    "        # Write in batch\n",
    "        self.write_api.write(bucket=self.bucket, record=points)\n",
    "        print(f\"Written {len(points)} points to InfluxDB\")\n",
    "    \n",
    "    def query_time_range(self, \n",
    "                        symbol: str,\n",
    "                        start: str,\n",
    "                        stop: str,\n",
    "                        measurement: str = \"stock_prices\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query stock data for a specific time range using Flux query language.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol to query\n",
    "            start: Start time (ISO format or relative like \"-30d\")\n",
    "            stop: Stop time (ISO format or relative like \"now()\")\n",
    "            measurement: Measurement name\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with query results\n",
    "        \n",
    "        Flux Query Explanation:\n",
    "        from(): Select the bucket\n",
    "        |> range(): Filter by time range\n",
    "        |> filter(): Filter by measurement and tags\n",
    "        |> aggregateWindow(): Optional downsampling\n",
    "        |> yield(): Return results\n",
    "        \"\"\"\n",
    "        query = f'''\n",
    "        from(bucket: \"{self.bucket}\")\n",
    "            |> range(start: {start}, stop: {stop})\n",
    "            |> filter(fn: (r) => r._measurement == \"{measurement}\")\n",
    "            |> filter(fn: (r) => r.symbol == \"{symbol}\")\n",
    "            |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "            |> sort(columns: [\"_time\"])\n",
    "        '''\n",
    "        \n",
    "        # Execute query and convert to DataFrame\n",
    "        result = self.query_api.query_data_frame(query)\n",
    "        \n",
    "        if result.empty:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Rename columns for clarity\n",
    "        result = result.rename(columns={\n",
    "            '_time': 'timestamp',\n",
    "            'close': 'close_price',\n",
    "            'open': 'open_price',\n",
    "            'high': 'high_price',\n",
    "            'low': 'low_price'\n",
    "        })\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def query_downsampled(self,\n",
    "                         symbol: str,\n",
    "                         start: str,\n",
    "                         window: str = \"1w\",\n",
    "                         aggregate: str = \"mean\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query downsampled (aggregated) data.\n",
    "        \n",
    "        InfluxDB is excellent at downsampling on the fly.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start: Start time\n",
    "            window: Window duration (1h, 1d, 1w, 1mo)\n",
    "            aggregate: Aggregation function (mean, sum, min, max, first, last)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with downsampled data\n",
    "        \"\"\"\n",
    "        query = f'''\n",
    "        from(bucket: \"{self.bucket}\")\n",
    "            |> range(start: {start})\n",
    "            |> filter(fn: (r) => r._measurement == \"stock_prices\")\n",
    "            |> filter(fn: (r) => r.symbol == \"{symbol}\")\n",
    "            |> aggregateWindow(every: {window}, fn: {aggregate}, createEmpty: false)\n",
    "            |> pivot(rowKey:[\"_time\"], columnKey: [\"_field\"], valueColumn: \"_value\")\n",
    "        '''\n",
    "        \n",
    "        return self.query_api.query_data_frame(query)\n",
    "    \n",
    "    def query_multiple_symbols(self,\n",
    "                                symbols: List[str],\n",
    "                                start: str,\n",
    "                                field: str = \"close\") -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query multiple symbols simultaneously (cross-sectional).\n",
    "        \n",
    "        Args:\n",
    "            symbols: List of stock symbols\n",
    "            start: Start time\n",
    "            field: Field to retrieve (close, volume, etc.)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with data for all symbols\n",
    "        \"\"\"\n",
    "        # Build symbol filter\n",
    "        symbol_filter = \" or \".join([f'r.symbol == \"{s}\"' for s in symbols])\n",
    "        \n",
    "        query = f'''\n",
    "        from(bucket: \"{self.bucket}\")\n",
    "            |> range(start: {start})\n",
    "            |> filter(fn: (r) => r._measurement == \"stock_prices\")\n",
    "            |> filter(fn: (r) => {symbol_filter})\n",
    "            |> filter(fn: (r) => r._field == \"{field}\")\n",
    "            |> pivot(rowKey:[\"_time\"], columnKey: [\"symbol\"], valueColumn: \"_value\")\n",
    "        '''\n",
    "        \n",
    "        return self.query_api.query_data_frame(query)\n",
    "    \n",
    "    def continuous_query_example(self):\n",
    "        \"\"\"\n",
    "        Example of setting up a continuous query (Task in InfluxDB 2.x).\n",
    "        \n",
    "        Continuous queries automatically downsample high-frequency data\n",
    "        to lower retention buckets for long-term storage.\n",
    "        \"\"\"\n",
    "        # This would be set up via the InfluxDB API or CLI\n",
    "        # Example Flux task for daily aggregation:\n",
    "        task_flux = '''\n",
    "        option task = {{\n",
    "            name: \"daily_stock_aggregation\",\n",
    "            every: 1d\n",
    "        }}\n",
    "        \n",
    "        from(bucket: \"nepse-bucket\")\n",
    "            |> range(start: -task.every)\n",
    "            |> filter(fn: (r) => r._measurement == \"stock_prices\")\n",
    "            |> aggregateWindow(every: 1d, fn: mean)\n",
    "            |> to(bucket: \"nepse-daily\", org: \"nepse-org\")\n",
    "        '''\n",
    "        \n",
    "        print(\"Task Flux script for daily aggregation:\")\n",
    "        print(task_flux)\n",
    "        return task_flux\n",
    "    \n",
    "    def get_measurement_stats(self) -> Dict[str, Any]:\n",
    "        \"\"\"\n",
    "        Get statistics about the measurement.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with statistics\n",
    "        \"\"\"\n",
    "        query = f'''\n",
    "        from(bucket: \"{self.bucket}\")\n",
    "            |> range(start: -30d)\n",
    "            |> filter(fn: (r) => r._measurement == \"stock_prices\")\n",
    "            |> group(columns: [\"symbol\"])\n",
    "            |> count()\n",
    "        '''\n",
    "        \n",
    "        result = self.query_api.query(query)\n",
    "        \n",
    "        stats = {}\n",
    "        for table in result:\n",
    "            for record in table.records:\n",
    "                symbol = record.values.get('symbol')\n",
    "                count = record.get_value()\n",
    "                stats[symbol] = count\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the InfluxDB client.\"\"\"\n",
    "        self.client.close()\n",
    "\n",
    "\n",
    "class InfluxDBSchemaDesign:\n",
    "    \"\"\"\n",
    "    Documentation and examples of InfluxDB schema design for NEPSE.\n",
    "    \n",
    "    Schema Design Best Practices:\n",
    "    \n",
    "    1. Tags vs Fields:\n",
    "       - Tags: Use for metadata that you filter/group by (symbol, sector)\n",
    "              Tags are indexed (inverted index) - fast filtering\n",
    "              But high cardinality (many unique values) costs memory\n",
    "       - Fields: Use for actual data values (price, volume)\n",
    "                Not indexed - efficient for storage\n",
    "    \n",
    "    2. Measurement Design:\n",
    "       Option A: Single measurement \"stock_prices\" with symbol as tag\n",
    "       Option B: Separate measurement per stock (avoid - too many measurements)\n",
    "       \n",
    "       Recommended: Option A (single measurement with symbol tag)\n",
    "    \n",
    "    3. Retention Policies:\n",
    "       - Raw data: 1 year retention\n",
    "       - Daily aggregates: 5 years retention\n",
    "       - Monthly aggregates: Infinite retention\n",
    "    \"\"\"\n",
    "    \n",
    "    @staticmethod\n",
    "    def get_schema_recommendations():\n",
    "        \"\"\"\n",
    "        Print schema recommendations for NEPSE data.\n",
    "        \"\"\"\n",
    "        recommendations = \"\"\"\n",
    "        InfluxDB Schema Design for NEPSE:\n",
    "        \n",
    "        Measurement: stock_prices\n",
    "        Tags:\n",
    "          - symbol: Stock ticker (NABIL, NICA, etc.) [cardinality: ~200]\n",
    "          - sector: Banking, Hydro, etc. [cardinality: ~20]\n",
    "        \n",
    "        Fields:\n",
    "          - open: Opening price (float)\n",
    "          - high: Daily high (float)\n",
    "          - low: Daily low (float)\n",
    "          - close: Closing price (float)\n",
    "          - volume: Trading volume (integer)\n",
    "          - turnover: Total turnover (float)\n",
    "          - vwap: Volume weighted average price (float)\n",
    "        \n",
    "        Timestamp: trade_date (midnight UTC or market close time)\n",
    "        \n",
    "        Retention Policies:\n",
    "        - autogen (raw data): 52 weeks\n",
    "        - daily_aggregates: 260 weeks (5 years)\n",
    "        - monthly_aggregates: INF (forever)\n",
    "        \n",
    "        Why this design?\n",
    "        - Symbol as tag allows fast queries: SELECT * WHERE symbol='NABIL'\n",
    "        - Low cardinality (~200 symbols) fits well in memory\n",
    "        - Fields store the actual time-series values\n",
    "        - Timestamp allows efficient time-range queries\n",
    "        \"\"\"\n",
    "        print(recommendations)\n",
    "\n",
    "\n",
    "def demonstrate_influxdb():\n",
    "    \"\"\"\n",
    "    Demonstrate InfluxDB operations (if available).\n",
    "    \"\"\"\n",
    "    if not INFLUX_AVAILABLE:\n",
    "        print(\"InfluxDB demonstration skipped (influxdb-client not installed)\")\n",
    "        print(\"To install: pip install influxdb-client\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"InfluxDB Time-Series Database Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Show schema design\n",
    "    InfluxDBSchemaDesign.get_schema_recommendations()\n",
    "    \n",
    "    # Note: This requires a running InfluxDB instance\n",
    "    # For demonstration, we'll show the code structure\n",
    "    print(\"\\nExample code structure:\")\n",
    "    print(\"\"\"\n",
    "    # Initialize\n",
    "    storage = InfluxDBTimeSeriesStorage(\n",
    "        url=\"http://localhost:8086\",\n",
    "        token=\"your-token\",\n",
    "        org=\"nepse-org\",\n",
    "        bucket=\"nepse-bucket\"\n",
    "    )\n",
    "    \n",
    "    # Write data\n",
    "    storage.write_stock_data(df)\n",
    "    \n",
    "    # Query time range\n",
    "    result = storage.query_time_range(\n",
    "        symbol=\"NABIL\",\n",
    "        start=\"-30d\",\n",
    "        stop=\"now()\"\n",
    "    )\n",
    "    \n",
    "    # Query downsampled data\n",
    "    weekly = storage.query_downsampled(\n",
    "        symbol=\"NABIL\",\n",
    "        start=\"-1y\",\n",
    "        window=\"1w\",\n",
    "        aggregate=\"mean\"\n",
    "    )\n",
    "    \"\"\")\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Data Model**: InfluxDB uses a specific data model:\n",
    "   - **Measurement**: Like a table name (`stock_prices`)\n",
    "   - **Tags**: Indexed metadata (symbol, sector) - used in WHERE clauses\n",
    "   - **Fields**: Non-indexed values (price, volume) - the actual data\n",
    "   - **Timestamp**: The time field (nanosecond precision)\n",
    "\n",
    "2. **Tag vs Field Decision**: \n",
    "   - Use **tags** for fields you filter by (symbol = 'NABIL') or group by (GROUP BY sector)\n",
    "   - Use **fields** for values you aggregate (AVG(close), SUM(volume))\n",
    "   - Tags are stored in an inverted index (fast lookup but memory intensive)\n",
    "   - Fields are compressed and stored separately (space efficient)\n",
    "\n",
    "3. **Flux Query Language**: InfluxDB 2.x uses Flux (functional query language):\n",
    "   - Pipe-forward operator `|>` passes data between functions\n",
    "   - `from()` selects the bucket\n",
    "   - `range()` filters by time\n",
    "   - `filter()` filters by tags/fields\n",
    "   - `aggregateWindow()` downsamples data\n",
    "   - `pivot()` transforms from long format (one row per field) to wide format (one row per timestamp)\n",
    "\n",
    "4. **Retention Policies**: InfluxDB can automatically downsample and expire data:\n",
    "   - Keep high-resolution data for short periods (e.g., 1 year of daily data)\n",
    "   - Keep low-resolution aggregates for long periods (e.g., weekly averages for 5 years)\n",
    "   - Reduces storage costs while maintaining historical trends\n",
    "\n",
    "### **8.4.2 TimescaleDB**\n",
    "\n",
    "TimescaleDB is a PostgreSQL extension that adds time-series capabilities to PostgreSQL. It combines SQL's flexibility with time-series optimizations.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "TimescaleDB Storage Module for NEPSE Time-Series Data\n",
    "\n",
    "TimescaleDB transforms PostgreSQL into a time-series database by:\n",
    "1. Automatic partitioning by time (hypertables)\n",
    "2. Time-series specific functions (time_bucket, first, last, etc.)\n",
    "3. Compression for historical data\n",
    "4. Continuous aggregates (materialized views for time-series)\n",
    "\n",
    "Advantages over vanilla PostgreSQL:\n",
    "- Automatic partitioning improves query performance\n",
    "- Better ingestion rates for time-series data\n",
    "- Built-in time-series functions\n",
    "- Data retention policies\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "try:\n",
    "    import psycopg2\n",
    "    from psycopg2.extras import execute_values\n",
    "    POSTGRES_AVAILABLE = True\n",
    "except ImportError:\n",
    "    POSTGRES_AVAILABLE = False\n",
    "\n",
    "\n",
    "class TimescaleDBStorage:\n",
    "    \"\"\"\n",
    "    A class to handle TimescaleDB operations for NEPSE data.\n",
    "    \n",
    "    TimescaleDB uses \"hypertables\" - tables that are automatically\n",
    "    partitioned by time. They look like regular tables but perform\n",
    "    like time-series databases.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 host: str = \"localhost\",\n",
    "                 port: int = 5432,\n",
    "                 database: str = \"nepse\",\n",
    "                 user: str = \"postgres\",\n",
    "                 password: str = \"password\"):\n",
    "        \"\"\"\n",
    "        Initialize TimescaleDB connection.\n",
    "        \n",
    "        Args:\n",
    "            host: Database host\n",
    "            port: Database port\n",
    "            database: Database name\n",
    "            user: Username\n",
    "            password: Password\n",
    "        \"\"\"\n",
    "        if not POSTGRES_AVAILABLE:\n",
    "            raise ImportError(\"psycopg2 required. Install: pip install psycopg2-binary\")\n",
    "        \n",
    "        self.conn = psycopg2.connect(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            database=database,\n",
    "            user=user,\n",
    "            password=password\n",
    "        )\n",
    "        self.cursor = self.conn.cursor()\n",
    "        self._initialize_schema()\n",
    "    \n",
    "    def _initialize_schema(self):\n",
    "        \"\"\"Create hypertables and indexes.\"\"\"\n",
    "        # Enable TimescaleDB extension\n",
    "        self.cursor.execute(\"CREATE EXTENSION IF NOT EXISTS timescaledb;\")\n",
    "        \n",
    "        # Create stock prices table\n",
    "        # This will be converted to a hypertable\n",
    "        self.cursor.execute('''\n",
    "            CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "                time TIMESTAMPTZ NOT NULL,\n",
    "                symbol VARCHAR(20) NOT NULL,\n",
    "                open_price NUMERIC(12, 4),\n",
    "                high_price NUMERIC(12, 4),\n",
    "                low_price NUMERIC(12, 4),\n",
    "                close_price NUMERIC(12, 4),\n",
    "                volume BIGINT,\n",
    "                turnover NUMERIC(18, 2),\n",
    "                PRIMARY KEY (time, symbol)\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Convert to hypertable\n",
    "        # chunk_time_interval determines partition size\n",
    "        # For daily stock data, 7 days (1 week) per chunk is reasonable\n",
    "        try:\n",
    "            self.cursor.execute('''\n",
    "                SELECT create_hypertable('stock_prices', 'time', \n",
    "                                        chunk_time_interval => INTERVAL '7 days',\n",
    "                                        if_not_exists => TRUE)\n",
    "            ''')\n",
    "        except Exception as e:\n",
    "            print(f\"Hypertable may already exist: {e}\")\n",
    "        \n",
    "        # Create indexes optimized for time-series queries\n",
    "        self.cursor.execute('''\n",
    "            CREATE INDEX IF NOT EXISTS idx_prices_symbol_time \n",
    "            ON stock_prices(symbol, time DESC)\n",
    "        ''')\n",
    "        \n",
    "        self.conn.commit()\n",
    "    \n",
    "    def insert_data(self, df: pd.DataFrame) -> int:\n",
    "        \"\"\"\n",
    "        Insert stock data into TimescaleDB.\n",
    "        \n",
    "        Uses batch insertion for performance.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with stock data\n",
    "        \n",
    "        Returns:\n",
    "            Number of rows inserted\n",
    "        \"\"\"\n",
    "        # Prepare data\n",
    "        data = []\n",
    "        for _, row in df.iterrows():\n",
    "            if isinstance(row['Date'], str):\n",
    "                dt = datetime.strptime(row['Date'], '%Y-%m-%d')\n",
    "            else:\n",
    "                dt = row['Date']\n",
    "            \n",
    "            data.append((\n",
    "                dt,\n",
    "                row['Symbol'],\n",
    "                row.get('Open'),\n",
    "                row.get('High'),\n",
    "                row.get('Low'),\n",
    "                row.get('Close'),\n",
    "                row.get('Volume'),\n",
    "                row.get('Turnover')\n",
    "            ))\n",
    "        \n",
    "        # Use execute_values for efficient batch insertion\n",
    "        # This is much faster than individual INSERT statements\n",
    "        execute_values(\n",
    "            self.cursor,\n",
    "            '''\n",
    "            INSERT INTO stock_prices \n",
    "            (time, symbol, open_price, high_price, low_price, close_price, volume, turnover)\n",
    "            VALUES %s\n",
    "            ON CONFLICT (time, symbol) DO NOTHING\n",
    "            ''',\n",
    "            data,\n",
    "            page_size=1000\n",
    "        )\n",
    "        \n",
    "        self.conn.commit()\n",
    "        return len(data)\n",
    "    \n",
    "    def query_time_range(self,\n",
    "                        symbol: str,\n",
    "                        start_date: str,\n",
    "                        end_date: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query stock prices for a time range.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start_date: Start date (YYYY-MM-DD)\n",
    "            end_date: End date (YYYY-MM-DD)\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with price data\n",
    "        \"\"\"\n",
    "        query = '''\n",
    "            SELECT \n",
    "                time,\n",
    "                symbol,\n",
    "                open_price,\n",
    "                high_price,\n",
    "                low_price,\n",
    "                close_price,\n",
    "                volume,\n",
    "                turnover\n",
    "            FROM stock_prices\n",
    "            WHERE symbol = %s\n",
    "              AND time BETWEEN %s AND %s\n",
    "            ORDER BY time ASC\n",
    "        '''\n",
    "        \n",
    "        df = pd.read_sql_query(\n",
    "            query,\n",
    "            self.conn,\n",
    "            params=(symbol, start_date, end_date),\n",
    "            parse_dates=['time']\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def query_aggregates(self,\n",
    "                        symbol: str,\n",
    "                        bucket_size: str = '1 month',\n",
    "                        start_date: str = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query aggregated data using time_bucket.\n",
    "        \n",
    "        TimescaleDB's time_bucket function is like GROUP BY for time.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            bucket_size: Time bucket (1 day, 1 week, 1 month, etc.)\n",
    "            start_date: Start date for query\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with aggregated data (OHLCV)\n",
    "        \"\"\"\n",
    "        query = '''\n",
    "            SELECT \n",
    "                time_bucket(%s, time) as bucket,\n",
    "                first(open_price, time) as open_price,\n",
    "                max(high_price) as high_price,\n",
    "                min(low_price) as low_price,\n",
    "                last(close_price, time) as close_price,\n",
    "                sum(volume) as total_volume,\n",
    "                avg(close_price) as avg_close\n",
    "            FROM stock_prices\n",
    "            WHERE symbol = %s\n",
    "              AND time > %s\n",
    "            GROUP BY bucket\n",
    "            ORDER BY bucket ASC\n",
    "        '''\n",
    "        \n",
    "        if start_date is None:\n",
    "            start_date = '2020-01-01'\n",
    "        \n",
    "        df = pd.read_sql_query(\n",
    "            query,\n",
    "            self.conn,\n",
    "            params=(bucket_size, symbol, start_date)\n",
    "        )\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_continuous_aggregate(self):\n",
    "        \"\"\"\n",
    "        Create a continuous aggregate (materialized view for time-series).\n",
    "        \n",
    "        Continuous aggregates automatically refresh and maintain\n",
    "        pre-computed aggregations for fast querying.\n",
    "        \"\"\"\n",
    "        # Create daily aggregates view\n",
    "        self.cursor.execute('''\n",
    "            CREATE MATERIALIZED VIEW IF NOT EXISTS daily_stock_stats\n",
    "            WITH (timescaledb.continuous) AS\n",
    "            SELECT \n",
    "                time_bucket('1 day', time) as day,\n",
    "                symbol,\n",
    "                first(open_price, time) as open_price,\n",
    "                max(high_price) as high_price,\n",
    "                min(low_price) as low_price,\n",
    "                last(close_price, time) as close_price,\n",
    "                sum(volume) as total_volume,\n",
    "                avg(close_price) as avg_price\n",
    "            FROM stock_prices\n",
    "            GROUP BY day, symbol\n",
    "            WITH DATA\n",
    "        ''')\n",
    "        \n",
    "        # Add policy to drop raw data after 1 year\n",
    "        # but keep aggregates\n",
    "        try:\n",
    "            self.cursor.execute('''\n",
    "                SELECT add_retention_policy('stock_prices', INTERVAL '1 year')\n",
    "            ''')\n",
    "        except:\n",
    "            pass  # Policy may already exist\n",
    "        \n",
    "        self.conn.commit()\n",
    "        print(\"Continuous aggregate created\")\n",
    "    \n",
    "    def query_with_window_functions(self, symbol: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query using advanced window functions.\n",
    "        \n",
    "        TimescaleDB supports all PostgreSQL window functions,\n",
    "        optimized for time-series.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with calculated indicators\n",
    "        \"\"\"\n",
    "        query = '''\n",
    "            SELECT \n",
    "                time,\n",
    "                close_price,\n",
    "                volume,\n",
    "                -- Moving averages\n",
    "                AVG(close_price) OVER (ORDER BY time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) as sma_5,\n",
    "                AVG(close_price) OVER (ORDER BY time ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) as sma_20,\n",
    "                -- Previous day close (for daily returns)\n",
    "                LAG(close_price) OVER (ORDER BY time) as prev_close,\n",
    "                -- Daily return percentage\n",
    "                (close_price - LAG(close_price) OVER (ORDER BY time)) / \n",
    "                    LAG(close_price) OVER (ORDER BY time) * 100 as daily_return_pct,\n",
    "                -- Running total volume\n",
    "                SUM(volume) OVER (ORDER BY time ROWS BETWEEN 4 PRECEDING AND CURRENT ROW) as volume_5d_sum,\n",
    "                -- Rank by volume in last 20 days\n",
    "                RANK() OVER (ORDER BY volume DESC ROWS BETWEEN 19 PRECEDING AND CURRENT ROW) as volume_rank\n",
    "            FROM stock_prices\n",
    "            WHERE symbol = %s\n",
    "            ORDER BY time DESC\n",
    "            LIMIT 100\n",
    "        '''\n",
    "        \n",
    "        return pd.read_sql_query(query, self.conn, params=(symbol,))\n",
    "    \n",
    "    def compress_old_data(self, older_than: str = '1 month'):\n",
    "        \"\"\"\n",
    "        Enable compression for old data.\n",
    "        \n",
    "        TimescaleDB can compress chunks (partitions) to save space.\n",
    "        Compressed chunks are still queryable but use less disk space.\n",
    "        \n",
    "        Args:\n",
    "            older_than: Compress chunks older than this interval\n",
    "        \"\"\"\n",
    "        self.cursor.execute(f'''\n",
    "            ALTER TABLE stock_prices SET (\n",
    "                timescaledb.compress,\n",
    "                timescaledb.compress_segmentby = 'symbol'\n",
    "            )\n",
    "        ''')\n",
    "        \n",
    "        # Add compression policy\n",
    "        try:\n",
    "            self.cursor.execute(f'''\n",
    "                SELECT add_compression_policy('stock_prices', INTERVAL '{older_than}')\n",
    "            ''')\n",
    "            self.conn.commit()\n",
    "            print(f\"Compression policy added for data older than {older_than}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Compression policy may already exist: {e}\")\n",
    "    \n",
    "    def get_hypertable_info(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get information about the hypertable.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with hypertable statistics\n",
    "        \"\"\"\n",
    "        query = '''\n",
    "            SELECT \n",
    "                hypertable_name,\n",
    "                chunk_count,\n",
    "                total_bytes,\n",
    "                index_bytes,\n",
    "                toast_bytes,\n",
    "                compression_enabled\n",
    "            FROM hypertable_detailed_size('stock_prices')\n",
    "        '''\n",
    "        \n",
    "        return pd.read_sql_query(query, self.conn)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close the database connection.\"\"\"\n",
    "        self.cursor.close()\n",
    "        self.conn.close()\n",
    "\n",
    "\n",
    "def demonstrate_timescaledb():\n",
    "    \"\"\"\n",
    "    Demonstrate TimescaleDB features.\n",
    "    \"\"\"\n",
    "    if not POSTGRES_AVAILABLE:\n",
    "        print(\"TimescaleDB demonstration skipped (psycopg2 not installed)\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"TimescaleDB Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    TimescaleDB Setup for NEPSE:\n",
    "    \n",
    "    1. Install TimescaleDB extension in PostgreSQL\n",
    "    2. Create hypertable from regular table:\n",
    "       SELECT create_hypertable('stock_prices', 'time');\n",
    "    \n",
    "    3. Key benefits for NEPSE data:\n",
    "       - Automatic partitioning by time (7-day chunks)\n",
    "       - Efficient time-range queries\n",
    "       - Built-in time_bucket for aggregations\n",
    "       - Continuous aggregates for pre-computed indicators\n",
    "       - Compression for old data\n",
    "    \"\"\")\n",
    "    \n",
    "    # Example code structure\n",
    "    print(\"\"\"\n",
    "    Example Usage:\n",
    "    \n",
    "    # Initialize\n",
    "    db = TimescaleDBStorage(\n",
    "        host=\"localhost\",\n",
    "        database=\"nepse\",\n",
    "        user=\"postgres\",\n",
    "        password=\"password\"\n",
    "    )\n",
    "    \n",
    "    # Insert data\n",
    "    db.insert_data(df)\n",
    "    \n",
    "    # Query with time_bucket aggregation\n",
    "    monthly = db.query_aggregates(\n",
    "        symbol=\"NABIL\",\n",
    "        bucket_size=\"1 month\"\n",
    "    )\n",
    "    \n",
    "    # Create continuous aggregate\n",
    "    db.create_continuous_aggregate()\n",
    "    \n",
    "    # Enable compression\n",
    "    db.compress_old_data(\"3 months\")\n",
    "    \"\"\")\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Hypertables**: TimescaleDB's core feature. They look like regular tables but are automatically partitioned by time:\n",
    "   - **Chunks**: Partitions of the hypertable (e.g., 7 days of data per chunk)\n",
    "   - **Automatic creation**: New chunks are created automatically as data is inserted\n",
    "   - **Efficient queries**: Queries that filter by time only scan relevant chunks\n",
    "\n",
    "2. **time_bucket Function**: Like GROUP BY but for time:\n",
    "   - `time_bucket('1 day', time)` groups data into daily buckets\n",
    "   - `time_bucket('1 week', time)` groups into weekly buckets\n",
    "   - Works with `first()`, `last()`, `max()`, `min()` for OHLC calculations\n",
    "\n",
    "3. **Continuous Aggregates**: Materialized views that automatically refresh:\n",
    "   - Pre-compute daily/weekly aggregates from raw data\n",
    "   - Queries on aggregates are instant\n",
    "   - Raw data can be dropped while keeping aggregates (retention policies)\n",
    "\n",
    "4. **Compression**: TimescaleDB can compress old chunks:\n",
    "   - Uses less disk space (often 90%+ reduction)\n",
    "   - Still queryable (decompresses on the fly)\n",
    "   - Configurable by age (e.g., compress data older than 3 months)\n",
    "\n",
    "### **8.4.3 Prometheus**\n",
    "\n",
    "Prometheus is primarily a monitoring system but can be used for time-series data with appropriate exporters. It's less suitable for financial data than InfluxDB or TimescaleDB but worth mentioning for monitoring ML model metrics.\n",
    "\n",
    "# **Chapter 8: Data Storage and Management (Continued - Final Part)**\n",
    "\n",
    "---\n",
    "\n",
    "### **8.4.3 Prometheus (Continued)**\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Prometheus Integration for NEPSE Model Monitoring (Continued)\n",
    "\n",
    "Prometheus is primarily used for monitoring operational metrics\n",
    "of the prediction system rather than storing the actual stock data.\n",
    "\"\"\"\n",
    "\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Any\n",
    "\n",
    "try:\n",
    "    from prometheus_client import Counter, Histogram, Gauge, start_http_server, Info\n",
    "    PROMETHEUS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    PROMETHEUS_AVAILABLE = False\n",
    "\n",
    "\n",
    "class PrometheusModelMonitor:\n",
    "    \"\"\"\n",
    "    Monitor NEPSE prediction system metrics using Prometheus.\n",
    "    \n",
    "    Use cases:\n",
    "    - Track prediction latency (how long models take to predict)\n",
    "    - Monitor prediction accuracy over time\n",
    "    - Track data ingestion rates\n",
    "    - Alert on system errors\n",
    "    - Monitor model drift\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, port: int = 8000):\n",
    "        \"\"\"\n",
    "        Initialize Prometheus metrics.\n",
    "        \n",
    "        Args:\n",
    "            port: Port for Prometheus metrics endpoint\n",
    "        \"\"\"\n",
    "        if not PROMETHEUS_AVAILABLE:\n",
    "            raise ImportError(\"prometheus-client required. Install: pip install prometheus-client\")\n",
    "        \n",
    "        self.port = port\n",
    "        \n",
    "        # Counter for total predictions made\n",
    "        # Labels allow us to track by symbol and model version\n",
    "        self.prediction_counter = Counter(\n",
    "            'nepse_predictions_total',\n",
    "            'Total number of predictions made',\n",
    "            ['symbol', 'model_version', 'prediction_type']\n",
    "        )\n",
    "        \n",
    "        # Histogram for prediction latency\n",
    "        # Buckets appropriate for ML inference (milliseconds)\n",
    "        self.prediction_latency = Histogram(\n",
    "            'nepse_prediction_latency_seconds',\n",
    "            'Time spent making predictions',\n",
    "            ['symbol', 'model_version'],\n",
    "            buckets=[.001, .005, .01, .025, .05, .075, .1, .25, .5, 1.0]\n",
    "        )\n",
    "        \n",
    "        # Gauge for model accuracy (updated periodically)\n",
    "        self.model_accuracy = Gauge(\n",
    "            'nepse_model_accuracy',\n",
    "            'Current model accuracy metrics',\n",
    "            ['metric_type', 'symbol']\n",
    "        )\n",
    "        \n",
    "        # Gauge for data freshness\n",
    "        self.data_freshness_hours = Gauge(\n",
    "            'nepse_data_freshness_hours',\n",
    "            'Hours since last data update',\n",
    "            ['data_source']\n",
    "        )\n",
    "        \n",
    "        # Counter for data ingestion\n",
    "        self.ingestion_counter = Counter(\n",
    "            'nepse_data_ingestion_total',\n",
    "            'Total records ingested',\n",
    "            ['source', 'status']\n",
    "        )\n",
    "        \n",
    "        # Info about the model\n",
    "        self.model_info = Info('nepse_model', 'Model metadata')\n",
    "    \n",
    "    def start_server(self):\n",
    "        \"\"\"Start the Prometheus metrics HTTP server.\"\"\"\n",
    "        start_http_server(self.port)\n",
    "        print(f\"Prometheus metrics server started on port {self.port}\")\n",
    "        print(f\"Access metrics at: http://localhost:{self.port}/metrics\")\n",
    "    \n",
    "    def record_prediction(self, \n",
    "                         symbol: str, \n",
    "                         model_version: str,\n",
    "                         prediction_type: str = 'close_price',\n",
    "                         latency_seconds: float = 0):\n",
    "        \"\"\"\n",
    "        Record a prediction event.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            model_version: Model version string\n",
    "            prediction_type: Type of prediction (close_price, direction, etc.)\n",
    "            latency_seconds: Time taken to make prediction\n",
    "        \"\"\"\n",
    "        # Increment counter\n",
    "        self.prediction_counter.labels(\n",
    "            symbol=symbol,\n",
    "            model_version=model_version,\n",
    "            prediction_type=prediction_type\n",
    "        ).inc()\n",
    "        \n",
    "        # Record latency\n",
    "        self.prediction_latency.labels(\n",
    "            symbol=symbol,\n",
    "            model_version=model_version\n",
    "        ).observe(latency_seconds)\n",
    "    \n",
    "    def update_accuracy(self, \n",
    "                       metric_type: str,\n",
    "                       value: float,\n",
    "                       symbol: str = 'all'):\n",
    "        \"\"\"\n",
    "        Update model accuracy metrics.\n",
    "        \n",
    "        Args:\n",
    "            metric_type: Type of metric (mae, rmse, mape, direction_accuracy)\n",
    "            value: Metric value\n",
    "            symbol: Stock symbol or 'all' for aggregate\n",
    "        \"\"\"\n",
    "        self.model_accuracy.labels(\n",
    "            metric_type=metric_type,\n",
    "            symbol=symbol\n",
    "        ).set(value)\n",
    "    \n",
    "    def record_ingestion(self, \n",
    "                         source: str,\n",
    "                         count: int,\n",
    "                         success: bool = True):\n",
    "        \"\"\"\n",
    "        Record data ingestion metrics.\n",
    "        \n",
    "        Args:\n",
    "            source: Data source (nepse_api, csv_import, etc.)\n",
    "            count: Number of records\n",
    "            success: Whether ingestion was successful\n",
    "        \"\"\"\n",
    "        status = 'success' if success else 'error'\n",
    "        self.ingestion_counter.labels(\n",
    "            source=source,\n",
    "            status=status\n",
    "        ).inc(count)\n",
    "    \n",
    "    def set_model_info(self, \n",
    "                       model_name: str,\n",
    "                       version: str,\n",
    "                       training_date: str,\n",
    "                       features: str):\n",
    "        \"\"\"\n",
    "        Set model metadata.\n",
    "        \n",
    "        Args:\n",
    "            model_name: Name of the model\n",
    "            version: Version string\n",
    "            training_date: Date model was trained\n",
    "            features: Comma-separated list of features\n",
    "        \"\"\"\n",
    "        self.model_info.info({\n",
    "            'model_name': model_name,\n",
    "            'version': version,\n",
    "            'training_date': training_date,\n",
    "            'features': features\n",
    "        })\n",
    "\n",
    "\n",
    "def demonstrate_prometheus():\n",
    "    \"\"\"\n",
    "    Demonstrate Prometheus monitoring setup.\n",
    "    \"\"\"\n",
    "    if not PROMETHEUS_AVAILABLE:\n",
    "        print(\"Prometheus demonstration skipped (prometheus-client not installed)\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Prometheus Model Monitoring Setup\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    monitor = PrometheusModelMonitor(port=8000)\n",
    "    \n",
    "    # Set model info\n",
    "    monitor.set_model_info(\n",
    "        model_name=\"LSTM_Price_Predictor\",\n",
    "        version=\"v2.1.0\",\n",
    "        training_date=\"2024-01-15\",\n",
    "        features=\"sma_20,rsi,macd,volume\"\n",
    "    )\n",
    "    \n",
    "    # Start server\n",
    "    monitor.start_server()\n",
    "    \n",
    "    # Simulate predictions\n",
    "    print(\"\\nSimulating predictions...\")\n",
    "    for i in range(10):\n",
    "        symbol = ['NABIL', 'NICA', 'SCBL'][i % 3]\n",
    "        \n",
    "        # Simulate prediction latency\n",
    "        start = time.time()\n",
    "        time.sleep(0.01)  # Simulate work\n",
    "        latency = time.time() - start\n",
    "        \n",
    "        monitor.record_prediction(\n",
    "            symbol=symbol,\n",
    "            model_version=\"v2.1.0\",\n",
    "            prediction_type=\"close_price\",\n",
    "            latency_seconds=latency\n",
    "        )\n",
    "        \n",
    "        # Update accuracy periodically\n",
    "        if i % 5 == 0:\n",
    "            monitor.update_accuracy('mae', 15.5, symbol)\n",
    "            monitor.update_accuracy('direction_accuracy', 0.65, symbol)\n",
    "    \n",
    "    print(\"Metrics recorded. Access at http://localhost:8000/metrics\")\n",
    "    \n",
    "    return monitor\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Metric Types**:\n",
    "   - **Counters**: Monotonically increasing values (total predictions, total errors). Used for rates (predictions per second).\n",
    "   - **Histograms**: Distribution of values (prediction latency). Automatically calculates percentiles.\n",
    "   - **Gauges**: Values that can go up or down (current accuracy, queue size).\n",
    "   - **Info**: Static metadata (model version, training date).\n",
    "\n",
    "2. **Labels**: Key-value pairs that add dimensions to metrics:\n",
    "   - `symbol='NABIL'` allows filtering by stock\n",
    "   - `model_version='v2.1.0'` allows comparing versions\n",
    "   - Without labels, you'd need separate metrics for each combination\n",
    "\n",
    "3. **Use Cases for NEPSE**:\n",
    "   - Monitor prediction latency (should be < 100ms)\n",
    "   - Track model accuracy degradation (data drift)\n",
    "   - Alert when data ingestion fails\n",
    "   - Monitor API rate limits\n",
    "\n",
    "---\n",
    "\n",
    "## **8.5 NoSQL Solutions**\n",
    "\n",
    "NoSQL databases offer flexible schemas and horizontal scalability, making them suitable for certain time-series use cases.\n",
    "\n",
    "### **8.5.1 Document Stores (MongoDB)**\n",
    "\n",
    "Document stores like MongoDB are useful when data structure varies or when you need to store complex nested data with time-series.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "MongoDB Storage for Time-Series Data\n",
    "\n",
    "MongoDB is a document database that stores data in JSON-like format.\n",
    "For time-series, MongoDB 5.0+ introduced specific optimizations:\n",
    "- Time-series collections\n",
    "- Automatic bucketing\n",
    "- Improved compression\n",
    "\n",
    "Use cases for NEPSE:\n",
    "- Storing unstructured news/sentiment data alongside prices\n",
    "- Flexible schema for different data sources\n",
    "- Storing model metadata and experiment tracking\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    from pymongo import MongoClient, ASCENDING, DESCENDING\n",
    "    from pymongo.collection import Collection\n",
    "    MONGODB_AVAILABLE = True\n",
    "except ImportError:\n",
    "    MONGODB_AVAILABLE = False\n",
    "\n",
    "\n",
    "class MongoDBTimeSeriesStorage:\n",
    "    \"\"\"\n",
    "    MongoDB storage handler for NEPSE time-series data.\n",
    "    \n",
    "    Two approaches:\n",
    "    1. Time-series collections (MongoDB 5.0+) - optimized for time-series\n",
    "    2. Regular collections with proper indexing - more flexible\n",
    "    \n",
    "    Schema Design:\n",
    "    {\n",
    "        \"timestamp\": ISODate(\"2024-01-15T00:00:00Z\"),\n",
    "        \"symbol\": \"NABIL\",\n",
    "        \"metadata\": {\n",
    "            \"sector\": \"Banking\",\n",
    "            \"market_cap\": \"Large\"\n",
    "        },\n",
    "        \"prices\": {\n",
    "            \"open\": 850.0,\n",
    "            \"high\": 870.0,\n",
    "            \"low\": 845.0,\n",
    "            \"close\": 865.0\n",
    "        },\n",
    "        \"volume\": 125000,\n",
    "        \"indicators\": {\n",
    "            \"sma_20\": 860.5,\n",
    "            \"rsi\": 65.3\n",
    "        }\n",
    "    }\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 connection_string: str = \"mongodb://localhost:27017\",\n",
    "                 database_name: str = \"nepse\"):\n",
    "        \"\"\"\n",
    "        Initialize MongoDB connection.\n",
    "        \n",
    "        Args:\n",
    "            connection_string: MongoDB connection URI\n",
    "            database_name: Database name\n",
    "        \"\"\"\n",
    "        if not MONGODB_AVAILABLE:\n",
    "            raise ImportError(\"pymongo required. Install: pip install pymongo\")\n",
    "        \n",
    "        self.client = MongoClient(connection_string)\n",
    "        self.db = self.client[database_name]\n",
    "        self._setup_collections()\n",
    "    \n",
    "    def _setup_collections(self):\n",
    "        \"\"\"Set up time-series collections with proper indexing.\"\"\"\n",
    "        # Check if time-series collection exists, create if not\n",
    "        collections = self.db.list_collection_names()\n",
    "        \n",
    "        if 'stock_prices' not in collections:\n",
    "            try:\n",
    "                # Create time-series collection (MongoDB 5.0+)\n",
    "                self.db.create_collection(\n",
    "                    'stock_prices',\n",
    "                    timeseries={\n",
    "                        'timeField': 'timestamp',\n",
    "                        'metaField': 'metadata',\n",
    "                        'granularity': 'hours'\n",
    "                    }\n",
    "                )\n",
    "                print(\"Created time-series collection: stock_prices\")\n",
    "            except Exception as e:\n",
    "                # Fallback to regular collection for older MongoDB versions\n",
    "                print(f\"Time-series collection not supported ({e}), using regular collection\")\n",
    "                self.db.create_collection('stock_prices')\n",
    "        \n",
    "        # Create indexes for efficient querying\n",
    "        # Compound index: symbol + timestamp (optimal for time-range queries)\n",
    "        self.db.stock_prices.create_index([\n",
    "            ('symbol', ASCENDING),\n",
    "            ('timestamp', DESCENDING)\n",
    "        ], name='symbol_time_idx')\n",
    "        \n",
    "        # Index for cross-sectional queries (specific date)\n",
    "        self.db.stock_prices.create_index([\n",
    "            ('timestamp', ASCENDING),\n",
    "            ('symbol', ASCENDING)\n",
    "        ], name='time_symbol_idx')\n",
    "        \n",
    "        # Text index for searching\n",
    "        self.db.stock_prices.create_index([\n",
    "            ('metadata.sector', ASCENDING)\n",
    "        ])\n",
    "    \n",
    "    def insert_data(self, df: pd.DataFrame) -> int:\n",
    "        \"\"\"\n",
    "        Insert stock data into MongoDB.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with stock data\n",
    "        \n",
    "        Returns:\n",
    "            Number of documents inserted\n",
    "        \"\"\"\n",
    "        documents = []\n",
    "        \n",
    "        for _, row in df.iterrows():\n",
    "            # Parse date\n",
    "            if isinstance(row['Date'], str):\n",
    "                timestamp = datetime.strptime(row['Date'], '%Y-%m-%d')\n",
    "            else:\n",
    "                timestamp = row['Date']\n",
    "            \n",
    "            # Create document\n",
    "            doc = {\n",
    "                'timestamp': timestamp,\n",
    "                'symbol': row['Symbol'],\n",
    "                'metadata': {\n",
    "                    'sector': row.get('Sector', 'Unknown'),\n",
    "                    'source': 'nepse_api'\n",
    "                },\n",
    "                'prices': {\n",
    "                    'open': float(row.get('Open', 0)),\n",
    "                    'high': float(row.get('High', 0)),\n",
    "                    'low': float(row.get('Low', 0)),\n",
    "                    'close': float(row.get('Close', 0)),\n",
    "                    'vwap': float(row.get('VWAP', 0))\n",
    "                },\n",
    "                'volume': int(row.get('Volume', 0)),\n",
    "                'turnover': float(row.get('Turnover', 0)),\n",
    "                'transactions': int(row.get('Trans.', 0))\n",
    "            }\n",
    "            \n",
    "            # Add calculated fields if present\n",
    "            if 'Diff' in row:\n",
    "                doc['change'] = {\n",
    "                    'absolute': float(row['Diff']),\n",
    "                    'percent': float(row.get('Diff %', 0))\n",
    "                }\n",
    "            \n",
    "            # Add technical indicators if present\n",
    "            indicators = {}\n",
    "            for col in ['sma_20', 'sma_50', 'rsi', 'macd']:\n",
    "                if col in row:\n",
    "                    indicators[col] = float(row[col])\n",
    "            \n",
    "            if indicators:\n",
    "                doc['indicators'] = indicators\n",
    "            \n",
    "            documents.append(doc)\n",
    "        \n",
    "        # Insert in bulk\n",
    "        if documents:\n",
    "            result = self.db.stock_prices.insert_many(documents, ordered=False)\n",
    "            return len(result.inserted_ids)\n",
    "        \n",
    "        return 0\n",
    "    \n",
    "    def query_time_range(self, \n",
    "                        symbol: str,\n",
    "                        start_date: datetime,\n",
    "                        end_date: datetime) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Query stock data for a time range.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start_date: Start datetime\n",
    "            end_date: End datetime\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with results\n",
    "        \"\"\"\n",
    "        query = {\n",
    "            'symbol': symbol,\n",
    "            'timestamp': {\n",
    "                '$gte': start_date,\n",
    "                '$lte': end_date\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # Projection to select only needed fields\n",
    "        projection = {\n",
    "            'timestamp': 1,\n",
    "            'symbol': 1,\n",
    "            'prices.open': 1,\n",
    "            'prices.high': 1,\n",
    "            'prices.low': 1,\n",
    "            'prices.close': 1,\n",
    "            'volume': 1,\n",
    "            '_id': 0\n",
    "        }\n",
    "        \n",
    "        cursor = self.db.stock_prices.find(query, projection).sort('timestamp', ASCENDING)\n",
    "        \n",
    "        # Convert to DataFrame\n",
    "        data = list(cursor)\n",
    "        if not data:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Flatten nested structure\n",
    "        flattened = []\n",
    "        for doc in data:\n",
    "            flat = {\n",
    "                'timestamp': doc['timestamp'],\n",
    "                'symbol': doc['symbol'],\n",
    "                'open': doc['prices']['open'],\n",
    "                'high': doc['prices']['high'],\n",
    "                'low': doc['prices']['low'],\n",
    "                'close': doc['prices']['close'],\n",
    "                'volume': doc['volume']\n",
    "            }\n",
    "            flattened.append(flat)\n",
    "        \n",
    "        return pd.DataFrame(flattened)\n",
    "    \n",
    "    def query_with_aggregation(self,\n",
    "                                symbol: str,\n",
    "                                start_date: datetime) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Query with MongoDB aggregation pipeline.\n",
    "        \n",
    "        Useful for complex analytics like:\n",
    "        - Moving averages\n",
    "        - Resampling\n",
    "        - Statistical aggregations\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start_date: Start date\n",
    "        \n",
    "        Returns:\n",
    "            List of aggregated results\n",
    "        \"\"\"\n",
    "        pipeline = [\n",
    "            # Match stage: Filter documents\n",
    "            {\n",
    "                '$match': {\n",
    "                    'symbol': symbol,\n",
    "                    'timestamp': {'$gte': start_date}\n",
    "                }\n",
    "            },\n",
    "            # Group stage: Aggregate by month\n",
    "            {\n",
    "                '$group': {\n",
    "                    '_id': {\n",
    "                        'year': {'$year': '$timestamp'},\n",
    "                        'month': {'$month': '$timestamp'}\n",
    "                    },\n",
    "                    'avg_close': {'$avg': '$prices.close'},\n",
    "                    'max_high': {'$max': '$prices.high'},\n",
    "                    'min_low': {'$min': '$prices.low'},\n",
    "                    'total_volume': {'$sum': '$volume'},\n",
    "                    'count': {'$sum': 1}\n",
    "                }\n",
    "            },\n",
    "            # Sort stage\n",
    "            {\n",
    "                '$sort': {'_id.year': -1, '_id.month': -1}\n",
    "            }\n",
    "        ]\n",
    "        \n",
    "        results = list(self.db.stock_prices.aggregate(pipeline))\n",
    "        return results\n",
    "    \n",
    "    def update_with_indicators(self,\n",
    "                               symbol: str,\n",
    "                               date: datetime,\n",
    "                               indicators: Dict[str, float]):\n",
    "        \"\"\"\n",
    "        Update documents with calculated indicators.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            date: Date\n",
    "            indicators: Dictionary of indicator values\n",
    "        \"\"\"\n",
    "        self.db.stock_prices.update_one(\n",
    "            {\n",
    "                'symbol': symbol,\n",
    "                'timestamp': date\n",
    "            },\n",
    "            {\n",
    "                '$set': {\n",
    "                    'indicators': indicators,\n",
    "                    'updated_at': datetime.now()\n",
    "                }\n",
    "            }\n",
    "        )\n",
    "    \n",
    "    def get_distinct_symbols(self) -> List[str]:\n",
    "        \"\"\"Get list of all symbols in the collection.\"\"\"\n",
    "        return self.db.stock_prices.distinct('symbol')\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close MongoDB connection.\"\"\"\n",
    "        self.client.close()\n",
    "\n",
    "\n",
    "def demonstrate_mongodb():\n",
    "    \"\"\"\n",
    "    Demonstrate MongoDB time-series storage.\n",
    "    \"\"\"\n",
    "    if not MONGODB_AVAILABLE:\n",
    "        print(\"MongoDB demonstration skipped (pymongo not installed)\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"MongoDB Time-Series Storage Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        storage = MongoDBTimeSeriesStorage()\n",
    "        \n",
    "        # Generate sample data\n",
    "        import numpy as np\n",
    "        dates = pd.date_range('2024-01-01', '2024-01-31', freq='B')\n",
    "        \n",
    "        data = []\n",
    "        for date in dates:\n",
    "            data.append({\n",
    "                'Date': date,\n",
    "                'Symbol': 'NABIL',\n",
    "                'Sector': 'Banking',\n",
    "                'Open': 850 + np.random.randint(-20, 20),\n",
    "                'High': 870 + np.random.randint(-20, 20),\n",
    "                'Low': 840 + np.random.randint(-20, 20),\n",
    "                'Close': 860 + np.random.randint(-20, 20),\n",
    "                'Volume': np.random.randint(100000, 200000),\n",
    "                'Turnover': 150000000 + np.random.randint(-10000000, 10000000),\n",
    "                'Trans.': 500 + np.random.randint(-100, 100),\n",
    "                'Diff': np.random.randint(-10, 10),\n",
    "                'Diff %': np.random.uniform(-1, 1)\n",
    "            })\n",
    "        \n",
    "        df = pd.DataFrame(data)\n",
    "        \n",
    "        print(\"\\n1. Inserting data...\")\n",
    "        count = storage.insert_data(df)\n",
    "        print(f\"Inserted {count} documents\")\n",
    "        \n",
    "        print(\"\\n2. Querying time range...\")\n",
    "        result = storage.query_time_range(\n",
    "            'NABIL',\n",
    "            datetime(2024, 1, 1),\n",
    "            datetime(2024, 1, 15)\n",
    "        )\n",
    "        print(f\"Retrieved {len(result)} records\")\n",
    "        print(result.head())\n",
    "        \n",
    "        print(\"\\n3. Aggregation query...\")\n",
    "        agg_results = storage.query_with_aggregation(\n",
    "            'NABIL',\n",
    "            datetime(2024, 1, 1)\n",
    "        )\n",
    "        print(f\"Monthly aggregations: {len(agg_results)}\")\n",
    "        \n",
    "        storage.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"MongoDB connection failed (is MongoDB running?): {e}\")\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Time-Series Collections**: MongoDB 5.0+ has native time-series support:\n",
    "   - `timeField`: The timestamp field (required)\n",
    "   - `metaField`: Metadata that doesn't change often (symbol, sector)\n",
    "   - `granularity`: Hint for bucketing (seconds, minutes, hours)\n",
    "   - Automatic compression and indexing\n",
    "\n",
    "2. **Schema Flexibility**: Documents can have varying structures:\n",
    "   - Some documents have `indicators` field, others don't\n",
    "   - Can add new fields without schema migration\n",
    "   - Nested documents (`prices.open`) organize related data\n",
    "\n",
    "3. **Aggregation Pipeline**: MongoDB's powerful query language:\n",
    "   - `$match`: Filter documents (like WHERE)\n",
    "   - `$group`: Aggregate (like GROUP BY)\n",
    "   - `$sort`: Order results\n",
    "   - Supports complex time-series calculations\n",
    "\n",
    "### **8.5.2 Key-Value Stores (Redis)**\n",
    "\n",
    "Redis is an in-memory data structure store, excellent for caching and real-time applications.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Redis Storage for Time-Series Data\n",
    "\n",
    "Redis is an in-memory key-value store with several time-series use cases:\n",
    "1. Caching recent data for fast access\n",
    "2. Real-time streaming data\n",
    "3. Session storage for prediction API\n",
    "4. Rate limiting\n",
    "\n",
    "RedisTimeSeries module (available in Redis Enterprise) provides\n",
    "native time-series capabilities.\n",
    "\n",
    "For NEPSE:\n",
    "- Cache latest prices for quick API responses\n",
    "- Store real-time model predictions\n",
    "- Queue system for async processing\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Union\n",
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "try:\n",
    "    import redis\n",
    "    REDIS_AVAILABLE = True\n",
    "except ImportError:\n",
    "    REDIS_AVAILABLE = False\n",
    "\n",
    "\n",
    "class RedisTimeSeriesCache:\n",
    "    \"\"\"\n",
    "    Redis cache for NEPSE time-series data.\n",
    "    \n",
    "    Key patterns:\n",
    "    - price:{symbol}:latest -> Latest price data (hash)\n",
    "    - price:{symbol}:history -> Recent price history (sorted set)\n",
    "    - predictions:{symbol} -> Model predictions (hash)\n",
    "    - metadata:{symbol} -> Stock metadata (hash)\n",
    "    - queue:ingestion -> Data ingestion queue (list)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 host: str = 'localhost',\n",
    "                 port: int = 6379,\n",
    "                 db: int = 0,\n",
    "                 password: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize Redis connection.\n",
    "        \n",
    "        Args:\n",
    "            host: Redis host\n",
    "            port: Redis port\n",
    "            db: Database number (0-15)\n",
    "            password: Authentication password\n",
    "        \"\"\"\n",
    "        if not REDIS_AVAILABLE:\n",
    "            raise ImportError(\"redis required. Install: pip install redis\")\n",
    "        \n",
    "        self.r = redis.Redis(\n",
    "            host=host,\n",
    "            port=port,\n",
    "            db=db,\n",
    "            password=password,\n",
    "            decode_responses=True  # Auto-decode bytes to strings\n",
    "        )\n",
    "    \n",
    "    def cache_latest_price(self, \n",
    "                          symbol: str,\n",
    "                          price_data: Dict[str, Any],\n",
    "                          ttl_seconds: int = 3600):\n",
    "        \"\"\"\n",
    "        Cache latest price data with TTL (time-to-live).\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            price_data: Dictionary with price information\n",
    "            ttl_seconds: Time to live in seconds (default 1 hour)\n",
    "        \"\"\"\n",
    "        key = f\"price:{symbol}:latest\"\n",
    "        \n",
    "        # Convert to hash (field-value pairs)\n",
    "        # Convert non-string values to JSON\n",
    "        data = {}\n",
    "        for k, v in price_data.items():\n",
    "            if isinstance(v, (dict, list)):\n",
    "                data[k] = json.dumps(v)\n",
    "            else:\n",
    "                data[k] = str(v)\n",
    "        \n",
    "        self.r.hset(key, mapping=data)\n",
    "        self.r.expire(key, ttl_seconds)  # Set expiration\n",
    "    \n",
    "    def get_latest_price(self, symbol: str) -> Optional[Dict[str, Any]]:\n",
    "        \"\"\"\n",
    "        Get cached latest price for a symbol.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with price data or None if not cached\n",
    "        \"\"\"\n",
    "        key = f\"price:{symbol}:latest\"\n",
    "        data = self.r.hgetall(key)\n",
    "        \n",
    "        if not data:\n",
    "            return None\n",
    "        \n",
    "        # Convert string values back to appropriate types\n",
    "        result = {}\n",
    "        for k, v in data.items():\n",
    "            # Try to parse as JSON first\n",
    "            try:\n",
    "                result[k] = json.loads(v)\n",
    "            except json.JSONDecodeError:\n",
    "                # Try to convert to number\n",
    "                try:\n",
    "                    if '.' in v:\n",
    "                        result[k] = float(v)\n",
    "                    else:\n",
    "                        result[k] = int(v)\n",
    "                except ValueError:\n",
    "                    result[k] = v\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def add_to_history(self,\n",
    "                      symbol: str,\n",
    "                      timestamp: datetime,\n",
    "                      close_price: float,\n",
    "                      max_entries: int = 1000):\n",
    "        \"\"\"\n",
    "        Add price to historical sorted set.\n",
    "        \n",
    "        Uses Redis Sorted Sets (ZADD) where score is timestamp.\n",
    "        Allows efficient range queries by time.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            timestamp: Price timestamp\n",
    "            close_price: Closing price\n",
    "            max_entries: Maximum history entries to keep\n",
    "        \"\"\"\n",
    "        key = f\"price:{symbol}:history\"\n",
    "        \n",
    "        # Convert timestamp to Unix timestamp (float) for scoring\n",
    "        score = timestamp.timestamp()\n",
    "        \n",
    "        # Add to sorted set\n",
    "        # Member is the price (or can be JSON with more data)\n",
    "        member = json.dumps({\n",
    "            'timestamp': timestamp.isoformat(),\n",
    "            'close': close_price\n",
    "        })\n",
    "        \n",
    "        self.r.zadd(key, {member: score})\n",
    "        \n",
    "        # Trim to max entries (keep most recent)\n",
    "        self.r.zremrangebyrank(key, 0, -(max_entries + 1))\n",
    "        \n",
    "        # Set expiration (e.g., 7 days for history)\n",
    "        self.r.expire(key, 7 * 24 * 3600)\n",
    "    \n",
    "    def get_price_history(self,\n",
    "                         symbol: str,\n",
    "                         start_date: Optional[datetime] = None,\n",
    "                         end_date: Optional[datetime] = None) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Get price history from Redis.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start_date: Start date (None for all)\n",
    "            end_date: End date (None for all)\n",
    "        \n",
    "        Returns:\n",
    "            List of price records\n",
    "        \"\"\"\n",
    "        key = f\"price:{symbol}:history\"\n",
    "        \n",
    "        # Convert dates to Unix timestamps\n",
    "        min_score = start_date.timestamp() if start_date else '-inf'\n",
    "        max_score = end_date.timestamp() if end_date else '+inf'\n",
    "        \n",
    "        # Get members by score range\n",
    "        members = self.r.zrangebyscore(\n",
    "            key,\n",
    "            min_score,\n",
    "            max_score,\n",
    "            withscores=False\n",
    "        )\n",
    "        \n",
    "        # Parse JSON members\n",
    "        results = []\n",
    "        for member in members:\n",
    "            try:\n",
    "                data = json.loads(member)\n",
    "                results.append(data)\n",
    "            except json.JSONDecodeError:\n",
    "                continue\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def cache_prediction(self,\n",
    "                         symbol: str,\n",
    "                         prediction_date: str,\n",
    "                         predicted_value: float,\n",
    "                         confidence: float,\n",
    "                         model_version: str,\n",
    "                         ttl_hours: int = 24):\n",
    "        \"\"\"\n",
    "        Cache model prediction.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            prediction_date: Target date for prediction\n",
    "            predicted_value: Predicted price\n",
    "            confidence: Prediction confidence (0-1)\n",
    "            model_version: Model version string\n",
    "            ttl_hours: Cache duration\n",
    "        \"\"\"\n",
    "        key = f\"prediction:{symbol}:{prediction_date}\"\n",
    "        \n",
    "        data = {\n",
    "            'symbol': symbol,\n",
    "            'prediction_date': prediction_date,\n",
    "            'predicted_value': str(predicted_value),\n",
    "            'confidence': str(confidence),\n",
    "            'model_version': model_version,\n",
    "            'created_at': datetime.now().isoformat()\n",
    "        }\n",
    "        \n",
    "        self.r.hset(key, mapping=data)\n",
    "        self.r.expire(key, ttl_hours * 3600)\n",
    "    \n",
    "    def get_cached_prediction(self, \n",
    "                              symbol: str, \n",
    "                              prediction_date: str) -> Optional[Dict]:\n",
    "        \"\"\"Get cached prediction if available.\"\"\"\n",
    "        key = f\"prediction:{symbol}:{prediction_date}\"\n",
    "        return self.r.hgetall(key) or None\n",
    "    \n",
    "    def enqueue_data_ingestion(self, data: Dict[str, Any]):\n",
    "        \"\"\"\n",
    "        Add data to ingestion queue.\n",
    "        \n",
    "        Uses Redis List (LPUSH/RPOP) for queue semantics.\n",
    "        \"\"\"\n",
    "        self.r.lpush('queue:ingestion', json.dumps(data))\n",
    "    \n",
    "    def dequeue_data_ingestion(self, timeout: int = 5) -> Optional[Dict]:\n",
    "        \"\"\"\n",
    "        Pop data from ingestion queue (blocking).\n",
    "        \n",
    "        Args:\n",
    "            timeout: Block for up to N seconds\n",
    "        \n",
    "        Returns:\n",
    "            Data dictionary or None if timeout\n",
    "        \"\"\"\n",
    "        result = self.r.brpop('queue:ingestion', timeout=timeout)\n",
    "        if result:\n",
    "            _, data = result\n",
    "            return json.loads(data)\n",
    "        return None\n",
    "    \n",
    "    def increment_counter(self, \n",
    "                         metric_name: str,\n",
    "                         amount: int = 1):\n",
    "        \"\"\"\n",
    "        Increment a counter (for metrics).\n",
    "        \n",
    "        Args:\n",
    "            metric_name: Metric name\n",
    "            amount: Amount to increment\n",
    "        \"\"\"\n",
    "        self.r.incr(f\"counter:{metric_name}\", amount)\n",
    "    \n",
    "    def get_counter(self, metric_name: str) -> int:\n",
    "        \"\"\"Get current counter value.\"\"\"\n",
    "        value = self.r.get(f\"counter:{metric_name}\")\n",
    "        return int(value) if value else 0\n",
    "    \n",
    "    def clear_cache(self, pattern: str = \"*\"):\n",
    "        \"\"\"\n",
    "        Clear cached data matching pattern.\n",
    "        \n",
    "        Args:\n",
    "            pattern: Key pattern to clear (e.g., \"price:NABIL:*\")\n",
    "        \"\"\"\n",
    "        keys = self.r.keys(pattern)\n",
    "        if keys:\n",
    "            self.r.delete(*keys)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Redis connection.\"\"\"\n",
    "        self.r.close()\n",
    "\n",
    "\n",
    "def demonstrate_redis():\n",
    "    \"\"\"\n",
    "    Demonstrate Redis caching for NEPSE data.\n",
    "    \"\"\"\n",
    "    if not REDIS_AVAILABLE:\n",
    "        print(\"Redis demonstration skipped (redis not installed)\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Redis Cache Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    try:\n",
    "        cache = RedisTimeSeriesCache()\n",
    "        \n",
    "        print(\"\\n1. Caching latest price...\")\n",
    "        cache.cache_latest_price('NABIL', {\n",
    "            'symbol': 'NABIL',\n",
    "            'date': '2024-01-15',\n",
    "            'close': 865.0,\n",
    "            'open': 850.0,\n",
    "            'high': 870.0,\n",
    "            'low': 845.0,\n",
    "            'volume': 125000,\n",
    "            'change_pct': 1.76\n",
    "        })\n",
    "        \n",
    "        latest = cache.get_latest_price('NABIL')\n",
    "        print(f\"Cached price: {latest}\")\n",
    "        \n",
    "        print(\"\\n2. Adding to price history...\")\n",
    "        import numpy as np\n",
    "        base_date = datetime(2024, 1, 1)\n",
    "        for i in range(10):\n",
    "            date = base_date + timedelta(days=i)\n",
    "            price = 850 + np.random.randint(-20, 20)\n",
    "            cache.add_to_history('NABIL', date, float(price))\n",
    "        \n",
    "        history = cache.get_price_history('NABIL')\n",
    "        print(f\"History entries: {len(history)}\")\n",
    "        print(f\"First entry: {history[0] if history else 'None'}\")\n",
    "        \n",
    "        print(\"\\n3. Caching prediction...\")\n",
    "        cache.cache_prediction('NABIL', '2024-01-16', 870.5, 0.85, 'v2.1.0')\n",
    "        pred = cache.get_cached_prediction('NABIL', '2024-01-16')\n",
    "        print(f\"Cached prediction: {pred}\")\n",
    "        \n",
    "        print(\"\\n4. Queue operations...\")\n",
    "        cache.enqueue_data_ingestion({'symbol': 'NICA', 'price': 780.0})\n",
    "        item = cache.dequeue_data_ingestion()\n",
    "        print(f\"Dequeued: {item}\")\n",
    "        \n",
    "        print(\"\\n5. Counters...\")\n",
    "        cache.increment_counter('predictions_made', 5)\n",
    "        count = cache.get_counter('predictions_made')\n",
    "        print(f\"Predictions made: {count}\")\n",
    "        \n",
    "        cache.close()\n",
    "        print(\"\\nRedis demonstration completed successfully\")\n",
    "        \n",
    "    except redis.ConnectionError as e:\n",
    "        print(f\"Redis connection failed (is Redis running?): {e}\")\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Key Patterns**: Redis keys follow a naming convention:\n",
    "   - `price:NABIL:latest` - Hash with latest price fields\n",
    "   - `price:NABIL:history` - Sorted set with historical prices\n",
    "   - `prediction:NABIL:2024-01-16` - Specific prediction\n",
    "   - `queue:ingestion` - List for queue operations\n",
    "\n",
    "2. **Data Structures**:\n",
    "   - **Hash**: Store object-like data (latest price with multiple fields)\n",
    "   - **Sorted Set**: Time-series data with automatic sorting by timestamp\n",
    "   - **List**: Queue implementation (LPUSH to add, BRPOP to consume)\n",
    "   - **String**: Simple counters and values\n",
    "\n",
    "3. **TTL (Time-To-Live)**: Automatic expiration:\n",
    "   - Latest prices: 1 hour (refresh frequently)\n",
    "   - History: 7 days (keep recent data)\n",
    "   - Predictions: 24 hours (stale predictions are useless)\n",
    "\n",
    "4. **Use Cases**:\n",
    "   - **Caching**: Store frequently accessed data in memory (< 1ms access)\n",
    "   - **Rate Limiting**: Track API usage per client\n",
    "   - **Pub/Sub**: Real-time updates to connected clients\n",
    "   - **Session Store**: User sessions for web dashboard\n",
    "\n",
    "### **8.5.3 Wide-Column Stores (Cassandra)**\n",
    "\n",
    "Wide-column stores like Apache Cassandra excel at handling massive write loads and providing high availability.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Apache Cassandra Storage for Time-Series Data\n",
    "\n",
    "Cassandra is designed for:\n",
    "- Massive scale (petabytes of data)\n",
    "- High write throughput\n",
    "- Distributed architecture (no single point of failure)\n",
    "- Time-series data (used by Apple, Netflix for time-series)\n",
    "\n",
    "Data Model for NEPSE:\n",
    "- Partition key: symbol (distributes data across nodes)\n",
    "- Clustering key: trade_date (orders data within partition)\n",
    "- Columns: open, high, low, close, volume\n",
    "\n",
    "Trade-offs:\n",
    "+ Excellent write performance\n",
    "+ Linear scalability\n",
    "+ High availability\n",
    "- No JOINs (must denormalize)\n",
    "- Eventually consistent (not ACID)\n",
    "- Complex to operate\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from typing import List, Dict, Any, Optional\n",
    "\n",
    "try:\n",
    "    from cassandra.cluster import Cluster\n",
    "    from cassandra.query import SimpleStatement, BatchStatement\n",
    "    CASSANDRA_AVAILABLE = True\n",
    "except ImportError:\n",
    "    CASSANDRA_AVAILABLE = False\n",
    "\n",
    "\n",
    "class CassandraTimeSeriesStorage:\n",
    "    \"\"\"\n",
    "    Cassandra storage for NEPSE time-series data.\n",
    "    \n",
    "    Schema:\n",
    "    CREATE KEYSPACE nepse WITH replication = {\n",
    "        'class': 'SimpleStrategy', \n",
    "        'replication_factor': 3\n",
    "    };\n",
    "    \n",
    "    CREATE TABLE stock_prices (\n",
    "        symbol text,\n",
    "        trade_date date,\n",
    "        open_price double,\n",
    "        high_price double,\n",
    "        low_price double,\n",
    "        close_price double,\n",
    "        volume bigint,\n",
    "        PRIMARY KEY (symbol, trade_date)\n",
    "    ) WITH CLUSTERING ORDER BY (trade_date DESC);\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 hosts: List[str] = ['127.0.0.1'],\n",
    "                 keyspace: str = 'nepse'):\n",
    "        \"\"\"\n",
    "        Initialize Cassandra connection.\n",
    "        \n",
    "        Args:\n",
    "            hosts: List of Cassandra node IPs\n",
    "            keyspace: Keyspace (database) name\n",
    "        \"\"\"\n",
    "        if not CASSANDRA_AVAILABLE:\n",
    "            raise ImportError(\"cassandra-driver required. Install: pip install cassandra-driver\")\n",
    "        \n",
    "        self.cluster = Cluster(hosts)\n",
    "        self.session = self.cluster.connect()\n",
    "        self.keyspace = keyspace\n",
    "        \n",
    "        self._setup_schema()\n",
    "    \n",
    "    def _setup_schema(self):\n",
    "        \"\"\"Create keyspace and tables if they don't exist.\"\"\"\n",
    "        # Create keyspace with SimpleStrategy (for single datacenter)\n",
    "        # NetworkTopologyStrategy for multiple datacenters\n",
    "        self.session.execute(f\"\"\"\n",
    "            CREATE KEYSPACE IF NOT EXISTS {self.keyspace}\n",
    "            WITH replication = {{\n",
    "                'class': 'SimpleStrategy',\n",
    "                'replication_factor': 3\n",
    "            }}\n",
    "        \"\"\")\n",
    "        \n",
    "        self.session.set_keyspace(self.keyspace)\n",
    "        \n",
    "        # Create stock_prices table\n",
    "        # Partition key: symbol (data distributed by symbol)\n",
    "        # Clustering key: trade_date (sorted within each symbol partition)\n",
    "        self.session.execute(\"\"\"\n",
    "            CREATE TABLE IF NOT EXISTS stock_prices (\n",
    "                symbol text,\n",
    "                trade_date date,\n",
    "                open_price double,\n",
    "                high_price double,\n",
    "                low_price double,\n",
    "                close_price double,\n",
    "                volume bigint,\n",
    "                turnover double,\n",
    "                PRIMARY KEY (symbol, trade_date)\n",
    "            ) WITH CLUSTERING ORDER BY (trade_date DESC)\n",
    "              AND compaction = {{\n",
    "                'class': 'TimeWindowCompactionStrategy',\n",
    "                'compaction_window_unit': 'DAYS',\n",
    "                'compaction_window_size': 7\n",
    "              }}\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create materialized view for time-range queries across symbols\n",
    "        # Cassandra doesn't allow querying by date only (need partition key)\n",
    "        # Materialized view duplicates data but allows different query patterns\n",
    "        self.session.execute(\"\"\"\n",
    "            CREATE MATERIALIZED VIEW IF NOT EXISTS stock_prices_by_date AS\n",
    "            SELECT symbol, trade_date, close_price, volume\n",
    "            FROM stock_prices\n",
    "            WHERE trade_date IS NOT NULL AND symbol IS NOT NULL\n",
    "            PRIMARY KEY (trade_date, symbol)\n",
    "            WITH CLUSTERING ORDER BY (symbol ASC)\n",
    "        \"\"\")\n",
    "    \n",
    "    def insert_data(self, \n",
    "                    symbol: str,\n",
    "                    trade_date: datetime,\n",
    "                    open_p: float,\n",
    "                    high_p: float,\n",
    "                    low_p: float,\n",
    "                    close_p: float,\n",
    "                    volume: int,\n",
    "                    turnover: float = 0.0):\n",
    "        \"\"\"\n",
    "        Insert a single price record.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            trade_date: Trading date\n",
    "            open_p: Opening price\n",
    "            high_p: High price\n",
    "            low_p: Low price\n",
    "            close_p: Closing price\n",
    "            volume: Trading volume\n",
    "            turnover: Total turnover\n",
    "        \"\"\"\n",
    "        query = \"\"\"\n",
    "            INSERT INTO stock_prices \n",
    "            (symbol, trade_date, open_price, high_price, low_price, close_price, volume, turnover)\n",
    "            VALUES (?, ?, ?, ?, ?, ?, ?, ?)\n",
    "        \"\"\"\n",
    "        \n",
    "        self.session.execute(query, (\n",
    "            symbol, trade_date.date(), open_p, high_p, low_p, close_p, volume, turnover\n",
    "        ))\n",
    "    \n",
    "    def batch_insert(self, records: List[Dict[str, Any]]):\n",
    "        \"\"\"\n",
    "        Insert multiple records using batch for efficiency.\n",
    "        \n",
    "        Note: Cassandra batches are for atomicity within partition,\n",
    "        not for performance. For high throughput, use async inserts.\n",
    "        \n",
    "        Args:\n",
    "            records: List of record dictionaries\n",
    "        \"\"\"\n",
    "        from cassandra.concurrent import execute_concurrent\n",
    "        \n",
    "        queries = []\n",
    "        for record in records:\n",
    "            query = \"\"\"\n",
    "                INSERT INTO stock_prices \n",
    "                (symbol, trade_date, open_price, high_price, low_price, close_price, volume)\n",
    "                VALUES (?, ?, ?, ?, ?, ?, ?)\n",
    "            \"\"\"\n",
    "            params = (\n",
    "                record['symbol'],\n",
    "                record['trade_date'],\n",
    "                record['open'],\n",
    "                record['high'],\n",
    "                record['low'],\n",
    "                record['close'],\n",
    "                record['volume']\n",
    "            )\n",
    "            queries.append((query, params))\n",
    "        \n",
    "        # Execute concurrently for better performance\n",
    "        results = execute_concurrent(self.session, queries)\n",
    "        \n",
    "        # Check for errors\n",
    "        errors = [r for r in results if not r.success]\n",
    "        if errors:\n",
    "            print(f\"Errors during batch insert: {len(errors)}\")\n",
    "    \n",
    "    def query_by_symbol(self,\n",
    "                        symbol: str,\n",
    "                        start_date: Optional[datetime] = None,\n",
    "                        end_date: Optional[datetime] = None,\n",
    "                        limit: int = 1000) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Query stock prices by symbol.\n",
    "        \n",
    "        This is efficient because symbol is the partition key.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            start_date: Start date filter\n",
    "            end_date: End date filter\n",
    "            limit: Maximum results\n",
    "        \n",
    "        Returns:\n",
    "            List of price records\n",
    "        \"\"\"\n",
    "        if start_date and end_date:\n",
    "            query = \"\"\"\n",
    "                SELECT * FROM stock_prices \n",
    "                WHERE symbol = ? AND trade_date >= ? AND trade_date <= ?\n",
    "                LIMIT ?\n",
    "            \"\"\"\n",
    "            params = (symbol, start_date.date(), end_date.date(), limit)\n",
    "        else:\n",
    "            query = \"\"\"\n",
    "                SELECT * FROM stock_prices \n",
    "                WHERE symbol = ?\n",
    "                LIMIT ?\n",
    "            \"\"\"\n",
    "            params = (symbol, limit)\n",
    "        \n",
    "        rows = self.session.execute(query, params)\n",
    "        \n",
    "        results = []\n",
    "        for row in rows:\n",
    "            results.append({\n",
    "                'symbol': row.symbol,\n",
    "                'trade_date': row.trade_date,\n",
    "                'open': row.open_price,\n",
    "                'high': row.high_price,\n",
    "                'low': row.low_price,\n",
    "                'close': row.close_price,\n",
    "                'volume': row.volume\n",
    "            })\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def query_recent(self, symbol: str, days: int = 30) -> List[Dict]:\n",
    "        \"\"\"\n",
    "        Query recent data for a symbol.\n",
    "        \n",
    "        Takes advantage of CLUSTERING ORDER BY (trade_date DESC)\n",
    "        to efficiently get most recent data.\n",
    "        \n",
    "        Args:\n",
    "            symbol: Stock symbol\n",
    "            days: Number of days to retrieve\n",
    "        \n",
    "        Returns:\n",
    "            List of recent price records\n",
    "        \"\"\"\n",
    "        # Since data is ordered DESC by date, we can just LIMIT\n",
    "        query = \"\"\"\n",
    "            SELECT * FROM stock_prices \n",
    "            WHERE symbol = ?\n",
    "            LIMIT ?\n",
    "        \"\"\"\n",
    "        \n",
    "        # Estimate rows (approximately 1 per day)\n",
    "        rows = self.session.execute(query, (symbol, days))\n",
    "        return list(rows)\n",
    "    \n",
    "    def close(self):\n",
    "        \"\"\"Close Cassandra connection.\"\"\"\n",
    "        self.cluster.shutdown()\n",
    "\n",
    "\n",
    "def demonstrate_cassandra():\n",
    "    \"\"\"\n",
    "    Demonstrate Cassandra storage (requires running Cassandra instance).\n",
    "    \"\"\"\n",
    "    if not CASSANDRA_AVAILABLE:\n",
    "        print(\"Cassandra demonstration skipped (cassandra-driver not installed)\")\n",
    "        return\n",
    "    \n",
    "    print(\"=\" * 70)\n",
    "    print(\"Apache Cassandra Storage Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    Cassandra Setup for NEPSE:\n",
    "    \n",
    "    1. Data Model:\n",
    "       - Partition Key: symbol (distributes across nodes)\n",
    "       - Clustering Key: trade_date (sorts within partition)\n",
    "       - Columns: OHLCV data\n",
    "    \n",
    "    2. Compaction Strategy:\n",
    "       - TimeWindowCompactionStrategy (TWCS) optimized for time-series\n",
    "       - Groups data into 7-day windows\n",
    "       - Efficient for time-range queries and TTL\n",
    "    \n",
    "    3. Replication:\n",
    "       - replication_factor: 3 (3 copies for high availability)\n",
    "       - Tunable consistency (ONE, QUORUM, ALL)\n",
    "    \n",
    "    4. Use Cases:\n",
    "       - High-frequency trading data (tick data)\n",
    "       - Large scale (billions of records)\n",
    "       - Multi-datacenter replication\n",
    "    \"\"\")\n",
    "    \n",
    "    try:\n",
    "        storage = CassandraTimeSeriesStorage()\n",
    "        print(\"\\nCassandra connected successfully\")\n",
    "        \n",
    "        # Example operations\n",
    "        print(\"\\nExample: Insert and query data\")\n",
    "        print(\"storage.insert_data('NABIL', datetime.now(), 850.0, 870.0, 845.0, 865.0, 125000)\")\n",
    "        print(\"results = storage.query_by_symbol('NABIL')\")\n",
    "        \n",
    "        storage.close()\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Cassandra connection failed (is Cassandra running?): {e}\")\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Partition Key**: `symbol` distributes data across cluster nodes:\n",
    "   - All data for 'NABIL' is on the same node (or replica set)\n",
    "   - Allows efficient queries for single symbol\n",
    "   - Prevents cross-node queries for symbol-specific data\n",
    "\n",
    "2. **Clustering Key**: `trade_date` orders data within each partition:\n",
    "   - `WITH CLUSTERING ORDER BY (trade_date DESC)` stores newest first\n",
    "   - Allows efficient \"latest N records\" queries\n",
    "   - Enables time-range filtering within partition\n",
    "\n",
    "3. **TimeWindowCompactionStrategy**: Optimized compaction for time-series:\n",
    "   - Groups data into time windows (7 days)\n",
    "   - Expires entire windows at once (efficient TTL)\n",
    "   - Optimizes for time-range queries\n",
    "\n",
    "4. **Materialized Views**: Allow querying by different keys:\n",
    "   - Main table: Query by symbol (partition key)\n",
    "   - View: Query by date (for cross-sectional queries)\n",
    "   - Cassandra automatically maintains consistency\n",
    "\n",
    "---\n",
    "\n",
    "## **8.6 Cloud Storage Solutions**\n",
    "\n",
    "Cloud providers offer managed storage services that eliminate operational overhead.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Cloud Storage Solutions for NEPSE Time-Series Data\n",
    "\n",
    "This module demonstrates integration with major cloud storage services:\n",
    "- AWS S3: Object storage for files\n",
    "- AWS RDS: Managed relational databases\n",
    "- Google BigQuery: Analytics data warehouse\n",
    "- Azure Blob Storage: Object storage\n",
    "\n",
    "Benefits:\n",
    "- No infrastructure management\n",
    "- Automatic scaling\n",
    "- Built-in durability (99.999999999%)\n",
    "- Pay-as-you-go pricing\n",
    "- Global availability\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any\n",
    "import pandas as pd\n",
    "import io\n",
    "\n",
    "\n",
    "class AWSStorage:\n",
    "    \"\"\"\n",
    "    AWS Storage integration for NEPSE data.\n",
    "    \n",
    "    Services:\n",
    "    - S3: Store CSV, Parquet, HDF5 files\n",
    "    - RDS: Managed PostgreSQL/TimescaleDB\n",
    "    - DynamoDB: NoSQL for metadata\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 aws_access_key: Optional[str] = None,\n",
    "                 aws_secret_key: Optional[str] = None,\n",
    "                 region: str = 'us-east-1'):\n",
    "        \"\"\"\n",
    "        Initialize AWS storage.\n",
    "        \n",
    "        Args:\n",
    "            aws_access_key: AWS access key (or use IAM role)\n",
    "            aws_secret_key: AWS secret key\n",
    "            region: AWS region\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import boto3\n",
    "            self.boto3 = boto3\n",
    "        except ImportError:\n",
    "            raise ImportError(\"boto3 required. Install: pip install boto3\")\n",
    "        \n",
    "        # Initialize sessions\n",
    "        if aws_access_key and aws_secret_key:\n",
    "            self.session = self.boto3.Session(\n",
    "                aws_access_key_id=aws_access_key,\n",
    "                aws_secret_access_key=aws_secret_key,\n",
    "                region_name=region\n",
    "            )\n",
    "        else:\n",
    "            # Use default credential chain (IAM role, ~/.aws/credentials, etc.)\n",
    "            self.session = self.boto3.Session(region_name=region)\n",
    "        \n",
    "        self.s3 = self.session.client('s3')\n",
    "        self.rds = self.session.client('rds')\n",
    "    \n",
    "    def upload_to_s3(self,\n",
    "                     local_path: str,\n",
    "                     bucket: str,\n",
    "                     s3_key: Optional[str] = None,\n",
    "                     storage_class: str = 'STANDARD'):\n",
    "        \"\"\"\n",
    "        Upload file to S3.\n",
    "        \n",
    "        Args:\n",
    "            local_path: Local file path\n",
    "            bucket: S3 bucket name\n",
    "            s3_key: S3 object key (path in bucket)\n",
    "            storage_class: S3 storage class\n",
    "                          STANDARD, STANDARD_IA, GLACIER, etc.\n",
    "        \"\"\"\n",
    "        if s3_key is None:\n",
    "            s3_key = Path(local_path).name\n",
    "        \n",
    "        self.s3.upload_file(\n",
    "            local_path,\n",
    "            bucket,\n",
    "            s3_key,\n",
    "            ExtraArgs={'StorageClass': storage_class}\n",
    "        )\n",
    "        \n",
    "        print(f\"Uploaded {local_path} to s3://{bucket}/{s3_key}\")\n",
    "    \n",
    "    def download_from_s3(self,\n",
    "                        bucket: str,\n",
    "                        s3_key: str,\n",
    "                        local_path: str):\n",
    "        \"\"\"\n",
    "        Download file from S3.\n",
    "        \n",
    "        Args:\n",
    "            bucket: S3 bucket name\n",
    "            s3_key: S3 object key\n",
    "            local_path: Local destination path\n",
    "        \"\"\"\n",
    "        self.s3.download_file(bucket, s3_key, local_path)\n",
    "        print(f\"Downloaded s3://{bucket}/{s3_key} to {local_path}\")\n",
    "    \n",
    "    def upload_dataframe_s3(self,\n",
    "                            df: pd.DataFrame,\n",
    "                            bucket: str,\n",
    "                            s3_key: str,\n",
    "                            format: str = 'parquet'):\n",
    "        \"\"\"\n",
    "        Upload DataFrame directly to S3 without saving locally.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to upload\n",
    "            bucket: S3 bucket name\n",
    "            s3_key: S3 object key\n",
    "            format: 'parquet', 'csv', or 'json'\n",
    "        \"\"\"\n",
    "        buffer = io.BytesIO()\n",
    "        \n",
    "        if format == 'parquet':\n",
    "            df.to_parquet(buffer, index=False)\n",
    "            content_type = 'application/octet-stream'\n",
    "        elif format == 'csv':\n",
    "            df.to_csv(buffer, index=False)\n",
    "            content_type = 'text/csv'\n",
    "        elif format == 'json':\n",
    "            df.to_json(buffer, orient='records')\n",
    "            content_type = 'application/json'\n",
    "        \n",
    "        buffer.seek(0)\n",
    "        \n",
    "        self.s3.put_object(\n",
    "            Bucket=bucket,\n",
    "            Key=s3_key,\n",
    "            Body=buffer.getvalue(),\n",
    "            ContentType=content_type\n",
    "        )\n",
    "        \n",
    "        print(f\"Uploaded DataFrame to s3://{bucket}/{s3_key}\")\n",
    "    \n",
    "    def list_s3_objects(self,\n",
    "                         bucket: str,\n",
    "                         prefix: str = '') -> List[Dict]:\n",
    "        \"\"\"\n",
    "        List objects in S3 bucket.\n",
    "        \n",
    "        Args:\n",
    "            bucket: S3 bucket name\n",
    "            prefix: Key prefix filter\n",
    "        \n",
    "        Returns:\n",
    "            List of object metadata\n",
    "        \"\"\"\n",
    "        response = self.s3.list_objects_v2(\n",
    "            Bucket=bucket,\n",
    "            Prefix=prefix\n",
    "        )\n",
    "        \n",
    "        return response.get('Contents', [])\n",
    "    \n",
    "    def get_s3_object_url(self,\n",
    "                          bucket: str,\n",
    "                          s3_key: str,\n",
    "                          expiration: int = 3600) -> str:\n",
    "        \"\"\"\n",
    "        Generate presigned URL for temporary access.\n",
    "        \n",
    "        Args:\n",
    "            bucket: S3 bucket name\n",
    "            s3_key: S3 object key\n",
    "            expiration: URL expiration in seconds\n",
    "        \n",
    "        Returns:\n",
    "            Presigned URL string\n",
    "        \"\"\"\n",
    "        url = self.s3.generate_presigned_url(\n",
    "            'get_object',\n",
    "            Params={'Bucket': bucket, 'Key': s3_key},\n",
    "            ExpiresIn=expiration\n",
    "        )\n",
    "        return url\n",
    "\n",
    "\n",
    "class GoogleCloudStorage:\n",
    "    \"\"\"\n",
    "    Google Cloud Storage integration.\n",
    "    \n",
    "    Services:\n",
    "    - Cloud Storage: Object storage (like S3)\n",
    "    - BigQuery: Analytics data warehouse\n",
    "    - Cloud SQL: Managed PostgreSQL\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, project_id: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Initialize Google Cloud Storage.\n",
    "        \n",
    "        Args:\n",
    "            project_id: GCP project ID\n",
    "        \"\"\"\n",
    "        try:\n",
    "            from google.cloud import storage, bigquery\n",
    "            self.storage = storage\n",
    "            self.bigquery = bigquery\n",
    "        except ImportError:\n",
    "            raise ImportError(\"google-cloud-storage and google-cloud-bigquery required\")\n",
    "        \n",
    "        self.project_id = project_id\n",
    "        self.storage_client = self.storage.Client(project=project_id)\n",
    "        self.bq_client = self.bigquery.Client(project=project_id)\n",
    "    \n",
    "    def upload_to_gcs(self,\n",
    "                      local_path: str,\n",
    "                      bucket_name: str,\n",
    "                      blob_name: Optional[str] = None):\n",
    "        \"\"\"\n",
    "        Upload file to Google Cloud Storage.\n",
    "        \n",
    "        Args:\n",
    "            local_path: Local file path\n",
    "            bucket_name: GCS bucket name\n",
    "            blob_name: Destination blob name\n",
    "        \"\"\"\n",
    "        bucket = self.storage_client.bucket(bucket_name)\n",
    "        blob = bucket.blob(blob_name or Path(local_path).name)\n",
    "        \n",
    "        blob.upload_from_filename(local_path)\n",
    "        print(f\"Uploaded to gs://{bucket_name}/{blob.name}\")\n",
    "    \n",
    "    def upload_dataframe_bigquery(self,\n",
    "                                   df: pd.DataFrame,\n",
    "                                   dataset: str,\n",
    "                                   table: str,\n",
    "                                   write_disposition: str = 'WRITE_APPEND'):\n",
    "        \"\"\"\n",
    "        Upload DataFrame to BigQuery.\n",
    "        \n",
    "        BigQuery is excellent for analytics queries on large datasets.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame to upload\n",
    "            dataset: BigQuery dataset name\n",
    "            table: Table name\n",
    "            write_disposition: 'WRITE_TRUNCATE', 'WRITE_APPEND', 'WRITE_EMPTY'\n",
    "        \"\"\"\n",
    "        table_ref = f\"{self.project_id}.{dataset}.{table}\"\n",
    "        \n",
    "        job = self.bq_client.load_table_from_dataframe(\n",
    "            df,\n",
    "            table_ref,\n",
    "            job_config=self.bigquery.LoadJobConfig(\n",
    "                write_disposition=write_disposition\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        job.result()  # Wait for completion\n",
    "        print(f\"Loaded {len(df)} rows to {table_ref}\")\n",
    "    \n",
    "    def query_bigquery(self, query: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Execute BigQuery SQL query.\n",
    "        \n",
    "        Args:\n",
    "            query: SQL query string\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with results\n",
    "        \"\"\"\n",
    "        query_job = self.bq_client.query(query)\n",
    "        return query_job.to_dataframe()\n",
    "\n",
    "\n",
    "def demonstrate_cloud_storage():\n",
    "    \"\"\"\n",
    "    Demonstrate cloud storage concepts.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Cloud Storage Solutions Overview\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    AWS S3 for NEPSE Data:\n",
    "    \n",
    "    Storage Classes:\n",
    "    - S3 Standard: Frequently accessed data\n",
    "    - S3 Standard-IA: Infrequent access (cheaper storage, higher retrieval)\n",
    "    - S3 Glacier: Archive (very cheap, slow retrieval)\n",
    "    - S3 Intelligent-Tiering: Automatic cost optimization\n",
    "    \n",
    "    Organization:\n",
    "    s3://nepse-bucket/\n",
    "      raw/\n",
    "        YYYY/MM/DD/\n",
    "          nepse_data_YYYYMMDD.csv\n",
    "      processed/\n",
    "        parquet/\n",
    "          stock_prices/\n",
    "            year=2024/\n",
    "              month=01/\n",
    "                part-00001.parquet\n",
    "      models/\n",
    "        model_v1.0.pkl\n",
    "        model_v2.0.pkl\n",
    "    \n",
    "    Lifecycle Policies:\n",
    "    - Move to IA after 30 days\n",
    "    - Move to Glacier after 1 year\n",
    "    - Delete after 7 years (compliance)\n",
    "    \n",
    "    Google BigQuery for Analytics:\n",
    "    \n",
    "    Schema:\n",
    "    Dataset: nepse_analytics\n",
    "    Table: stock_prices\n",
    "      - symbol: STRING\n",
    "      - trade_date: DATE\n",
    "      - close_price: FLOAT\n",
    "      - volume: INTEGER\n",
    "    \n",
    "    Advantages:\n",
    "    - Serverless (no infrastructure)\n",
    "    - Petabyte-scale queries\n",
    "    - Standard SQL support\n",
    "    - Integration with ML (BigQuery ML)\n",
    "    \n",
    "    Cost Optimization:\n",
    "    - Partition by date (query only relevant partitions)\n",
    "    - Cluster by symbol (faster filtering)\n",
    "    - Use materialized views for common aggregations\n",
    "    \"\"\")\n",
    "    \n",
    "    # Example code structure\n",
    "    print(\"\"\"\n",
    "    Example Usage:\n",
    "    \n",
    "    # AWS S3\n",
    "    aws = AWSStorage()\n",
    "    aws.upload_dataframe_s3(df, 'nepse-bucket', 'data/2024/01/prices.parquet')\n",
    "    \n",
    "    # Google BigQuery\n",
    "    gcp = GoogleCloudStorage(project_id='nepse-project')\n",
    "    gcp.upload_dataframe_bigquery(df, 'nepse_dataset', 'stock_prices')\n",
    "    \n",
    "    # Query in BigQuery\n",
    "    results = gcp.query_bigquery('''\n",
    "        SELECT symbol, AVG(close_price) as avg_price\n",
    "        FROM `nepse-project.nepse_dataset.stock_prices`\n",
    "        WHERE trade_date >= '2024-01-01'\n",
    "        GROUP BY symbol\n",
    "    ''')\n",
    "    \"\"\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_cloud_storage()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **AWS S3 Storage Classes**:\n",
    "   - **Standard**: Hot data, frequent access\n",
    "   - **Standard-IA**: 40% cheaper, retrieval fee\n",
    "   - **Glacier**: 90% cheaper, minutes/hours retrieval\n",
    "   - **Intelligent-Tiering**: Automatic based on access patterns\n",
    "\n",
    "2. **BigQuery**: Analytics data warehouse:\n",
    "   - **Columnar storage**: Fast aggregations\n",
    "   - **Partitioning**: Divide table by date (query only needed partitions)\n",
    "   - **Clustering**: Sort within partitions by symbol (faster filtering)\n",
    "   - **Pricing**: Pay per query (not storage), so optimize queries\n",
    "\n",
    "3. **Data Lifecycle**: Automate cost optimization:\n",
    "   - Raw data: S3 Standard (30 days)\n",
    "   - Processed data: S3 Standard-IA (90 days)\n",
    "   - Old data: Glacier (7 years)\n",
    "   - Compliance: Delete after regulatory period\n",
    "\n",
    "---\n",
    "\n",
    "## **8.7 Data Partitioning Strategies**\n",
    "\n",
    "Partitioning divides large datasets into smaller, manageable pieces for improved query performance and maintenance.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Data Partitioning Strategies for Time-Series Data\n",
    "\n",
    "Partitioning improves:\n",
    "- Query performance (scan only relevant partitions)\n",
    "- Data management (drop old partitions vs. delete rows)\n",
    "- Maintenance (rebuild indexes per partition)\n",
    "- Parallel processing (process partitions concurrently)\n",
    "\n",
    "Strategies:\n",
    "1. Time-based partitioning (most common for time-series)\n",
    "2. Hash partitioning (distribute evenly)\n",
    "3. List partitioning (by category)\n",
    "4. Composite partitioning (time + symbol)\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "class PartitioningManager:\n",
    "    \"\"\"\n",
    "    Manage data partitioning for NEPSE time-series data.\n",
    "    \n",
    "    Partitioning approaches:\n",
    "    1. File-based: Directory structure (year=2024/month=01/day=15/)\n",
    "    2. Database: Table partitioning (PostgreSQL/TimescaleDB)\n",
    "    3. DataFrame: In-memory partitioning\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, base_path: str = './partitioned_data'):\n",
    "        \"\"\"\n",
    "        Initialize partitioning manager.\n",
    "        \n",
    "        Args:\n",
    "            base_path: Base directory for partitioned files\n",
    "        \"\"\"\n",
    "        self.base_path = Path(base_path)\n",
    "        self.base_path.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def partition_by_time(self,\n",
    "                          df: pd.DataFrame,\n",
    "                          date_column: str = 'Date',\n",
    "                          frequency: str = 'month',\n",
    "                          output_format: str = 'parquet') -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Partition DataFrame by time periods.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with time-series data\n",
    "            date_column: Name of date column\n",
    "            frequency: 'day', 'week', 'month', 'year'\n",
    "            output_format: 'csv', 'parquet', 'feather'\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping partition keys to file paths\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        \n",
    "        # Create time partition key\n",
    "        if frequency == 'day':\n",
    "            df['partition_key'] = df[date_column].dt.strftime('%Y-%m-%d')\n",
    "            dir_pattern = 'year={}/month={}/day={}'\n",
    "        elif frequency == 'week':\n",
    "            df['partition_key'] = df[date_column].dt.strftime('%Y-W%U')\n",
    "            dir_pattern = 'year={}/week={}'\n",
    "        elif frequency == 'month':\n",
    "            df['partition_key'] = df[date_column].dt.strftime('%Y-%m')\n",
    "            dir_pattern = 'year={}/month={}'\n",
    "        elif frequency == 'year':\n",
    "            df['partition_key'] = df[date_column].dt.strftime('%Y')\n",
    "            dir_pattern = 'year={}'\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown frequency: {frequency}\")\n",
    "        \n",
    "        partitions = {}\n",
    "        \n",
    "        # Group by partition key\n",
    "        for partition_key, group in df.groupby('partition_key'):\n",
    "            # Create directory structure\n",
    "            date = group[date_column].iloc[0]\n",
    "            \n",
    "            if frequency == 'day':\n",
    "                path = self.base_path / dir_pattern.format(\n",
    "                    date.year, \n",
    "                    f\"{date.month:02d}\", \n",
    "                    f\"{date.day:02d}\"\n",
    "                )\n",
    "            elif frequency == 'week':\n",
    "                path = self.base_path / dir_pattern.format(\n",
    "                    date.year,\n",
    "                    date.strftime('%U')\n",
    "                )\n",
    "            elif frequency == 'month':\n",
    "                path = self.base_path / dir_pattern.format(\n",
    "                    date.year,\n",
    "                    f\"{date.month:02d}\"\n",
    "                )\n",
    "            else:\n",
    "                path = self.base_path / dir_pattern.format(date.year)\n",
    "            \n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Write file\n",
    "            filename = f\"data.{output_format}\"\n",
    "            filepath = path / filename\n",
    "            \n",
    "            if output_format == 'csv':\n",
    "                group.to_csv(filepath, index=False)\n",
    "            elif output_format == 'parquet':\n",
    "                group.to_parquet(filepath, index=False)\n",
    "            elif output_format == 'feather':\n",
    "                group.to_feather(filepath)\n",
    "            \n",
    "            partitions[partition_key] = filepath\n",
    "        \n",
    "        print(f\"Created {len(partitions)} partitions\")\n",
    "        return partitions\n",
    "    \n",
    "    def partition_by_symbol(self,\n",
    "                            df: pd.DataFrame,\n",
    "                            symbol_column: str = 'Symbol',\n",
    "                            chunks_per_symbol: int = 1) -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Partition data by stock symbol.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with stock data\n",
    "            symbol_column: Symbol column name\n",
    "            chunks_per_symbol: Split large symbols into chunks\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping symbols to file paths\n",
    "        \"\"\"\n",
    "        partitions = {}\n",
    "        \n",
    "        for symbol, group in df.groupby(symbol_column):\n",
    "            # Clean symbol for filename\n",
    "            safe_symbol = str(symbol).replace('/', '_')\n",
    "            \n",
    "            # Create directory\n",
    "            symbol_dir = self.base_path / f\"symbol={safe_symbol}\"\n",
    "            symbol_dir.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            # Split into chunks if needed\n",
    "            if chunks_per_symbol > 1 and len(group) > 1000:\n",
    "                chunk_size = len(group) // chunks_per_symbol\n",
    "                \n",
    "                for i in range(chunks_per_symbol):\n",
    "                    start = i * chunk_size\n",
    "                    end = start + chunk_size if i < chunks_per_symbol - 1 else len(group)\n",
    "                    chunk = group.iloc[start:end]\n",
    "                    \n",
    "                    filepath = symbol_dir / f\"part_{i:04d}.parquet\"\n",
    "                    chunk.to_parquet(filepath, index=False)\n",
    "                    partitions[f\"{symbol}_{i}\"] = filepath\n",
    "            else:\n",
    "                filepath = symbol_dir / \"data.parquet\"\n",
    "                group.to_parquet(filepath, index=False)\n",
    "                partitions[symbol] = filepath\n",
    "        \n",
    "        return partitions\n",
    "    \n",
    "    def partition_composite(self,\n",
    "                            df: pd.DataFrame,\n",
    "                            date_column: str = 'Date',\n",
    "                            symbol_column: str = 'Symbol',\n",
    "                            time_freq: str = 'month') -> Dict[str, Path]:\n",
    "        \"\"\"\n",
    "        Composite partitioning: Time + Symbol.\n",
    "        \n",
    "        Best for:\n",
    "        - Large datasets (billions of rows)\n",
    "        - Queries that filter by both time and symbol\n",
    "        - Parallel processing by symbol\n",
    "        \n",
    "        Structure:\n",
    "        year=2024/month=01/\n",
    "          symbol=NABIL/\n",
    "            data.parquet\n",
    "          symbol=NICA/\n",
    "            data.parquet\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        df[date_column] = pd.to_datetime(df[date_column])\n",
    "        \n",
    "        if time_freq == 'month':\n",
    "            df['time_partition'] = df[date_column].dt.strftime('%Y-%m')\n",
    "        elif time_freq == 'day':\n",
    "            df['time_partition'] = df[date_column].dt.strftime('%Y-%m-%d')\n",
    "        elif time_freq == 'year':\n",
    "            df['time_partition'] = df[date_column].dt.strftime('%Y')\n",
    "        \n",
    "        partitions = {}\n",
    "        \n",
    "        for (time_part, symbol), group in df.groupby(['time_partition', symbol_column]):\n",
    "            # Create path: time_partition/symbol=XXX/\n",
    "            safe_symbol = str(symbol).replace('/', '_')\n",
    "            path = self.base_path / f\"time={time_part}\" / f\"symbol={safe_symbol}\"\n",
    "            path.mkdir(parents=True, exist_ok=True)\n",
    "            \n",
    "            filepath = path / \"data.parquet\"\n",
    "            group.to_parquet(filepath, index=False)\n",
    "            \n",
    "            partitions[f\"{time_part}_{symbol}\"] = filepath\n",
    "        \n",
    "        return partitions\n",
    "    \n",
    "    def read_partition(self, partition_path: Path) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read a single partition file.\n",
    "        \n",
    "        Args:\n",
    "            partition_path: Path to partition file\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with partition data\n",
    "        \"\"\"\n",
    "        if partition_path.suffix == '.csv':\n",
    "            return pd.read_csv(partition_path)\n",
    "        elif partition_path.suffix == '.parquet':\n",
    "            return pd.read_parquet(partition_path)\n",
    "        elif partition_path.suffix == '.feather':\n",
    "            return pd.read_feather(partition_path)\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown file type: {partition_path.suffix}\")\n",
    "    \n",
    "    def read_partitions(self,\n",
    "                        pattern: str = \"**/*.parquet\",\n",
    "                        filters: Optional[Dict[str, Any]] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Read multiple partitions with optional filtering.\n",
    "        \n",
    "        Args:\n",
    "            pattern: Glob pattern for files\n",
    "            filters: Dictionary of partition key filters\n",
    "                    e.g., {'year': '2024', 'month': '01'}\n",
    "        \n",
    "        Returns:\n",
    "            Combined DataFrame\n",
    "        \"\"\"\n",
    "        if filters:\n",
    "            # Build path from filters\n",
    "            path_parts = []\n",
    "            for key in ['year', 'month', 'day', 'symbol']:\n",
    "                if key in filters:\n",
    "                    path_parts.append(f\"{key}={filters[key]}\")\n",
    "            \n",
    "            search_path = self.base_path / Path(*path_parts)\n",
    "            files = list(search_path.glob(\"*.parquet\"))\n",
    "        else:\n",
    "            files = list(self.base_path.glob(pattern))\n",
    "        \n",
    "        if not files:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        # Read and combine\n",
    "        dfs = []\n",
    "        for file in files:\n",
    "            try:\n",
    "                df = self.read_partition(file)\n",
    "                dfs.append(df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error reading {file}: {e}\")\n",
    "        \n",
    "        if not dfs:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        return pd.concat(dfs, ignore_index=True)\n",
    "    \n",
    "    def drop_partition(self, partition_key: str):\n",
    "        \"\"\"\n",
    "        Drop (delete) a specific partition.\n",
    "        \n",
    "        Much faster than deleting rows in a large table.\n",
    "        \n",
    "        Args:\n",
    "            partition_key: Key of partition to drop (e.g., '2023-01')\n",
    "        \"\"\"\n",
    "        partition_path = self.base_path / f\"time={partition_key}\"\n",
    "        if partition_path.exists():\n",
    "            shutil.rmtree(partition_path)\n",
    "            print(f\"Dropped partition: {partition_key}\")\n",
    "        else:\n",
    "            print(f\"Partition not found: {partition_key}\")\n",
    "    \n",
    "    def get_partition_statistics(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get statistics about partitions.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with partition info\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for partition_dir in self.base_path.rglob(\"*.parquet\"):\n",
    "            stat = partition_dir.stat()\n",
    "            stats.append({\n",
    "                'partition': str(partition_dir.parent.relative_to(self.base_path)),\n",
    "                'filename': partition_dir.name,\n",
    "                'size_mb': stat.st_size / (1024 * 1024),\n",
    "                'modified': datetime.fromtimestamp(stat.st_mtime)\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "def demonstrate_partitioning():\n",
    "    \"\"\"\n",
    "    Demonstrate partitioning strategies.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Data Partitioning Strategies Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Generate sample data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', '2023-12-31', freq='B')\n",
    "    symbols = ['NABIL', 'NICA', 'SCBL', 'ADBL']\n",
    "    \n",
    "    data = []\n",
    "    for date in dates:\n",
    "        for symbol in symbols:\n",
    "            data.append({\n",
    "                'Date': date,\n",
    "                'Symbol': symbol,\n",
    "                'Close': np.random.uniform(100, 1000),\n",
    "                'Volume': np.random.randint(10000, 100000)\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(data)\n",
    "    print(f\"\\nGenerated {len(df)} records\")\n",
    "    \n",
    "    manager = PartitioningManager('./demo_partitions')\n",
    "    \n",
    "    # Time partitioning\n",
    "    print(\"\\n1. Time Partitioning (by month)\")\n",
    "    print(\"-\" * 40)\n",
    "    time_parts = manager.partition_by_time(df, frequency='month')\n",
    "    print(f\"Created {len(time_parts)} monthly partitions\")\n",
    "    print(f\"Example path: {list(time_parts.values())[0]}\")\n",
    "    \n",
    "    # Symbol partitioning\n",
    "    print(\"\\n2. Symbol Partitioning\")\n",
    "    print(\"-\" * 40)\n",
    "    symbol_parts = manager.partition_by_symbol(df)\n",
    "    print(f\"Created {len(symbol_parts)} symbol partitions\")\n",
    "    \n",
    "    # Composite partitioning\n",
    "    print(\"\\n3. Composite Partitioning (Month + Symbol)\")\n",
    "    print(\"-\" * 40)\n",
    "    # Clean up first\n",
    "    import shutil\n",
    "    if Path('./demo_partitions').exists():\n",
    "        shutil.rmtree('./demo_partitions')\n",
    "    \n",
    "    manager2 = PartitioningManager('./demo_partitions')\n",
    "    composite_parts = manager2.partition_composite(df, time_freq='month')\n",
    "    print(f\"Created {len(composite_parts)} composite partitions\")\n",
    "    \n",
    "    # Read specific partition\n",
    "    print(\"\\n4. Reading Specific Partition\")\n",
    "    print(\"-\" * 40)\n",
    "    specific = manager2.read_partitions(filters={'year': '2023', 'month': '01', 'symbol': 'NABIL'})\n",
    "    print(f\"Read {len(specific)} records for 2023-01 NABIL\")\n",
    "    \n",
    "    # Statistics\n",
    "    print(\"\\n5. Partition Statistics\")\n",
    "    print(\"-\" * 40)\n",
    "    stats = manager2.get_partition_statistics()\n",
    "    print(f\"Total partitions: {len(stats)}\")\n",
    "    print(f\"Total size: {stats['size_mb'].sum():.2f} MB\")\n",
    "    print(f\"Average size: {stats['size_mb'].mean():.2f} MB\")\n",
    "    \n",
    "    return manager, df\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_partitioning()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Time Partitioning**: Divide by date (year/month/day)\n",
    "   - Query `WHERE date >= '2024-01-01'` only scans 2024 partitions\n",
    "   - Drop old data: `DROP PARTITION year=2022` (instant vs. DELETE)\n",
    "\n",
    "2. **Symbol Partitioning**: Divide by stock symbol\n",
    "   - Query for single symbol only reads one partition\n",
    "   - Enables parallel processing (process NABIL and NICA concurrently)\n",
    "\n",
    "3. **Composite Partitioning**: Time + Symbol\n",
    "   - Best for very large datasets\n",
    "   - Query for specific symbol in time range: scan one small partition\n",
    "   - Trade-off: More files, higher metadata overhead\n",
    "\n",
    "4. **Partition Pruning**: Query engines skip irrelevant partitions:\n",
    "   - Hive/Spark: Use partition columns in WHERE clause\n",
    "   - PostgreSQL: Constraint exclusion\n",
    "   - Manual: Only read files matching filters\n",
    "\n",
    "---\n",
    "\n",
    "## **8.8 Data Archival and Retention**\n",
    "\n",
    "Managing data lifecycle from hot (frequently accessed) to cold (rarely accessed) to deleted.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Data Archival and Retention Policies\n",
    "\n",
    "Strategies for managing data lifecycle:\n",
    "- Hot data: Recent, frequently accessed (SSD, memory)\n",
    "- Warm data: Occasionally accessed (standard disk)\n",
    "- Cold data: Rarely accessed (archive storage)\n",
    "- Deleted: Beyond regulatory requirement\n",
    "\n",
    "For NEPSE:\n",
    "- Last 1 year: Hot (fast queries for model training)\n",
    "- 1-5 years: Warm (occasional backtesting)\n",
    "- 5+ years: Cold (regulatory compliance)\n",
    "- 7+ years: Delete (unless required by law)\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime, timedelta\n",
    "from typing import Dict, Any, List, Optional\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "class DataRetentionManager:\n",
    "    \"\"\"\n",
    "    Manage data retention policies for NEPSE time-series data.\n",
    "    \n",
    "    Policies:\n",
    "    - Raw data: Keep 7 years (regulatory)\n",
    "    - Processed features: Keep 3 years\n",
    "    - Model predictions: Keep 2 years\n",
    "    - Logs: Keep 90 days\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 data_root: str = './data',\n",
    "                 archive_root: str = './archive'):\n",
    "        \"\"\"\n",
    "        Initialize retention manager.\n",
    "        \n",
    "        Args:\n",
    "            data_root: Root directory for active data\n",
    "            archive_root: Root directory for archived data\n",
    "        \"\"\"\n",
    "        self.data_root = Path(data_root)\n",
    "        self.archive_root = Path(archive_root)\n",
    "        self.archive_root.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Define retention policies (days)\n",
    "        self.policies = {\n",
    "            'raw_data': 7 * 365,        # 7 years\n",
    "            'processed_features': 3 * 365,  # 3 years\n",
    "            'predictions': 2 * 365,     # 2 years\n",
    "            'logs': 90,                 # 90 days\n",
    "            'temp': 7                   # 7 days\n",
    "        }\n",
    "    \n",
    "    def should_archive(self, \n",
    "                       file_path: Path,\n",
    "                       data_type: str = 'raw_data') -> bool:\n",
    "        \"\"\"\n",
    "        Determine if file should be archived based on age.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to file\n",
    "            data_type: Type of data (determines retention period)\n",
    "        \n",
    "        Returns:\n",
    "            True if file should be archived\n",
    "        \"\"\"\n",
    "        if not file_path.exists():\n",
    "            return False\n",
    "        \n",
    "        # Get file modification time\n",
    "        stat = file_path.stat()\n",
    "        file_date = datetime.fromtimestamp(stat.st_mtime)\n",
    "        age_days = (datetime.now() - file_date).days\n",
    "        \n",
    "        threshold = self.policies.get(data_type, 365)\n",
    "        \n",
    "        return age_days > threshold\n",
    "    \n",
    "    def should_delete(self,\n",
    "                      file_path: Path,\n",
    "                      data_type: str = 'raw_data') -> bool:\n",
    "        \"\"\"\n",
    "        Determine if file should be deleted based on retention policy.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to file\n",
    "            data_type: Type of data\n",
    "        \n",
    "        Returns:\n",
    "            True if file should be deleted\n",
    "        \"\"\"\n",
    "        if not file_path.exists():\n",
    "            return False\n",
    "        \n",
    "        stat = file_path.stat()\n",
    "        file_date = datetime.fromtimestamp(stat.st_mtime)\n",
    "        age_days = (datetime.now() - file_date).days\n",
    "        \n",
    "        # Delete after 2x retention (grace period)\n",
    "        threshold = self.policies.get(data_type, 365) * 2\n",
    "        \n",
    "        return age_days > threshold\n",
    "    \n",
    "    def archive_file(self, \n",
    "                     file_path: Path,\n",
    "                     compress: bool = True) -> Path:\n",
    "        \"\"\"\n",
    "        Move file to archive storage.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Original file path\n",
    "            compress: Whether to compress during archival\n",
    "        \n",
    "        Returns:\n",
    "            Path to archived file\n",
    "        \"\"\"\n",
    "        # Create archive directory structure\n",
    "        relative_path = file_path.relative_to(self.data_root)\n",
    "        archive_path = self.archive_root / relative_path\n",
    "        archive_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        if compress and file_path.suffix not in ['.gz', '.zip', '.bz2']:\n",
    "            # Compress to save space\n",
    "            import gzip\n",
    "            archive_path = archive_path.with_suffix(file_path.suffix + '.gz')\n",
    "            \n",
    "            with open(file_path, 'rb') as f_in:\n",
    "                with gzip.open(archive_path, 'wb') as f_out:\n",
    "                    shutil.copyfileobj(f_in, f_out)\n",
    "            \n",
    "            # Remove original after successful archive\n",
    "            file_path.unlink()\n",
    "        else:\n",
    "            # Just move\n",
    "            shutil.move(str(file_path), str(archive_path))\n",
    "        \n",
    "        print(f\"Archived: {file_path} -> {archive_path}\")\n",
    "        return archive_path\n",
    "    \n",
    "    def apply_retention_policy(self, \n",
    "                               data_type: str,\n",
    "                               dry_run: bool = True) -> Dict[str, List[str]]:\n",
    "        \"\"\"\n",
    "        Apply retention policy to a data type.\n",
    "        \n",
    "        Args:\n",
    "            data_type: Type of data (raw_data, predictions, etc.)\n",
    "            dry_run: If True, only report what would be done\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary with actions taken\n",
    "        \"\"\"\n",
    "        actions = {\n",
    "            'archived': [],\n",
    "            'deleted': [],\n",
    "            'kept': []\n",
    "        }\n",
    "        \n",
    "        # Determine directory based on data type\n",
    "        if data_type == 'raw_data':\n",
    "            search_dir = self.data_root / 'raw'\n",
    "        elif data_type == 'predictions':\n",
    "            search_dir = self.data_root / 'predictions'\n",
    "        else:\n",
    "            search_dir = self.data_root / data_type\n",
    "        \n",
    "        if not search_dir.exists():\n",
    "            return actions\n",
    "        \n",
    "        # Find all files\n",
    "        files = list(search_dir.rglob('*'))\n",
    "        files = [f for f in files if f.is_file()]\n",
    "        \n",
    "        for file_path in files:\n",
    "            if self.should_delete(file_path, data_type):\n",
    "                actions['deleted'].append(str(file_path))\n",
    "                if not dry_run:\n",
    "                    file_path.unlink()\n",
    "                    \n",
    "            elif self.should_archive(file_path, data_type):\n",
    "                actions['archived'].append(str(file_path))\n",
    "                if not dry_run:\n",
    "                    self.archive_file(file_path)\n",
    "            else:\n",
    "                actions['kept'].append(str(file_path))\n",
    "        \n",
    "        return actions\n",
    "    \n",
    "    def get_storage_statistics(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Get statistics about data storage.\n",
    "        \n",
    "        Returns:\n",
    "            DataFrame with storage stats by data type\n",
    "        \"\"\"\n",
    "        stats = []\n",
    "        \n",
    "        for data_type in self.policies.keys():\n",
    "            search_dir = self.data_root / data_type\n",
    "            if not search_dir.exists():\n",
    "                continue\n",
    "            \n",
    "            files = list(search_dir.rglob('*'))\n",
    "            files = [f for f in files if f.is_file()]\n",
    "            \n",
    "            total_size = sum(f.stat().st_size for f in files)\n",
    "            total_files = len(files)\n",
    "            \n",
    "            # Calculate age distribution\n",
    "            ages = []\n",
    "            for f in files:\n",
    "                age = (datetime.now() - datetime.fromtimestamp(f.stat().st_mtime)).days\n",
    "                ages.append(age)\n",
    "            \n",
    "            if ages:\n",
    "                avg_age = sum(ages) / len(ages)\n",
    "                max_age = max(ages)\n",
    "                min_age = min(ages)\n",
    "            else:\n",
    "                avg_age = max_age = min_age = 0\n",
    "            \n",
    "            stats.append({\n",
    "                'data_type': data_type,\n",
    "                'file_count': total_files,\n",
    "                'total_size_mb': total_size / (1024 * 1024),\n",
    "                'avg_age_days': avg_age,\n",
    "                'max_age_days': max_age,\n",
    "                'min_age_days': min_age,\n",
    "                'retention_days': self.policies[data_type]\n",
    "            })\n",
    "        \n",
    "        return pd.DataFrame(stats)\n",
    "\n",
    "\n",
    "def demonstrate_retention():\n",
    "    \"\"\"\n",
    "    Demonstrate retention policies.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Data Retention and Archival Policies\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    manager = DataRetentionManager('./data', './archive')\n",
    "    \n",
    "    print(\"\\nRetention Policies:\")\n",
    "    for data_type, days in manager.policies.items():\n",
    "        years = days / 365\n",
    "        print(f\"  {data_type:20s}: {days:4d} days ({years:.1f} years)\")\n",
    "    \n",
    "    # Simulate applying policy\n",
    "    print(\"\\nApplying retention policy (dry run):\")\n",
    "    # This would normally scan actual files\n",
    "    print(\"  - Files older than retention period: archive\")\n",
    "    print(\"  - Files older than 2x retention period: delete\")\n",
    "    print(\"  - Recent files: keep in hot storage\")\n",
    "    \n",
    "    return manager\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_retention()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Tiered Storage**:\n",
    "   - **Hot**: Last 1 year, SSD, instant access (active trading)\n",
    "   - **Warm**: 1-5 years, standard disk, occasional access (backtesting)\n",
    "   - **Cold**: 5+ years, archive storage (Glacier), compliance only\n",
    "   - **Delete**: 7+ years, unless legally required\n",
    "\n",
    "2. **Automation**: Scripts run daily/weekly:\n",
    "   - Identify files exceeding retention thresholds\n",
    "   - Move to archive (compressed)\n",
    "   - Delete after grace period\n",
    "   - Generate compliance reports\n",
    "\n",
    "3. **Compliance**: Financial data often regulated:\n",
    "   - SEC requires 7 years of trade data\n",
    "   - GDPR requires deletion of personal data\n",
    "   - Audit trails for all deletions\n",
    "\n",
    "---\n",
    "\n",
    "## **8.9 Backup and Recovery**\n",
    "\n",
    "Ensuring data durability and ability to recover from failures.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Backup and Recovery Strategies\n",
    "\n",
    "Types of backups:\n",
    "1. Full backup: Complete copy of all data\n",
    "2. Incremental backup: Only changes since last backup\n",
    "3. Differential backup: Changes since last full backup\n",
    "\n",
    "Strategies:\n",
    "- 3-2-1 rule: 3 copies, 2 different media, 1 offsite\n",
    "- Daily incremental, weekly full\n",
    "- Point-in-time recovery\n",
    "- Disaster recovery testing\n",
    "\"\"\"\n",
    "\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "import hashlib\n",
    "import json\n",
    "from typing import List, Dict, Optional\n",
    "\n",
    "\n",
    "class BackupManager:\n",
    "    \"\"\"\n",
    "    Manage backups for NEPSE data.\n",
    "    \n",
    "    Backup schedule:\n",
    "    - Daily: Incremental (changed files only)\n",
    "    - Weekly: Full backup (all data)\n",
    "    - Monthly: Archive to cold storage\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 source_dir: str = './data',\n",
    "                 backup_dir: str = './backups'):\n",
    "        \"\"\"\n",
    "        Initialize backup manager.\n",
    "        \n",
    "        Args:\n",
    "            source_dir: Directory to backup\n",
    "            backup_dir: Directory for backups\n",
    "        \"\"\"\n",
    "        self.source_dir = Path(source_dir)\n",
    "        self.backup_dir = Path(backup_dir)\n",
    "        self.backup_dir.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Track backup history\n",
    "        self.manifest_path = self.backup_dir / 'backup_manifest.json'\n",
    "        self.manifest = self._load_manifest()\n",
    "    \n",
    "    def _load_manifest(self) -> Dict:\n",
    "        \"\"\"Load backup manifest.\"\"\"\n",
    "        if self.manifest_path.exists():\n",
    "            with open(self.manifest_path) as f:\n",
    "                return json.load(f)\n",
    "        return {'backups': []}\n",
    "    \n",
    "    def _save_manifest(self):\n",
    "        \"\"\"Save backup manifest.\"\"\"\n",
    "        with open(self.manifest_path, 'w') as f:\n",
    "            json.dump(self.manifest, f, indent=2, default=str)\n",
    "    \n",
    "    def _calculate_checksum(self, file_path: Path) -> str:\n",
    "        \"\"\"Calculate MD5 checksum of file.\"\"\"\n",
    "        hash_md5 = hashlib.md5()\n",
    "        with open(file_path, \"rb\") as f:\n",
    "            for chunk in iter(lambda: f.read(4096), b\"\"):\n",
    "                hash_md5.update(chunk)\n",
    "        return hash_md5.hexdigest()\n",
    "    \n",
    "    def full_backup(self, name: Optional[str] = None) -> Path:\n",
    "        \"\"\"\n",
    "        Perform full backup of all data.\n",
    "        \n",
    "        Args:\n",
    "            name: Backup name (timestamp if not provided)\n",
    "        \n",
    "        Returns:\n",
    "            Path to backup directory\n",
    "        \"\"\"\n",
    "        if name is None:\n",
    "            name = f\"full_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        backup_path = self.backup_dir / name\n",
    "        backup_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        files_backed = 0\n",
    "        total_size = 0\n",
    "        \n",
    "        # Copy all files\n",
    "        for file_path in self.source_dir.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                relative_path = file_path.relative_to(self.source_dir)\n",
    "                dest_path = backup_path / relative_path\n",
    "                dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                \n",
    "                shutil.copy2(file_path, dest_path)\n",
    "                files_backed += 1\n",
    "                total_size += file_path.stat().st_size\n",
    "        \n",
    "        # Record in manifest\n",
    "        backup_info = {\n",
    "            'name': name,\n",
    "            'type': 'full',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'files': files_backed,\n",
    "            'size_bytes': total_size,\n",
    "            'path': str(backup_path)\n",
    "        }\n",
    "        self.manifest['backups'].append(backup_info)\n",
    "        self._save_manifest()\n",
    "        \n",
    "        print(f\"Full backup completed: {name}\")\n",
    "        print(f\"  Files: {files_backed}\")\n",
    "        print(f\"  Size: {total_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return backup_path\n",
    "    \n",
    "    def incremental_backup(self, name: Optional[str] = None) -> Path:\n",
    "        \"\"\"\n",
    "        Perform incremental backup (only changed files).\n",
    "        \n",
    "        Args:\n",
    "            name: Backup name\n",
    "        \n",
    "        Returns:\n",
    "            Path to backup directory\n",
    "        \"\"\"\n",
    "        if name is None:\n",
    "            name = f\"incr_{datetime.now().strftime('%Y%m%d_%H%M%S')}\"\n",
    "        \n",
    "        backup_path = self.backup_dir / name\n",
    "        backup_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # Get last full backup timestamp\n",
    "        last_full = None\n",
    "        for backup in reversed(self.manifest['backups']):\n",
    "            if backup['type'] == 'full':\n",
    "                last_full = datetime.fromisoformat(backup['timestamp'])\n",
    "                break\n",
    "        \n",
    "        if last_full is None:\n",
    "            print(\"No full backup found, performing full backup instead\")\n",
    "            return self.full_backup(name)\n",
    "        \n",
    "        files_backed = 0\n",
    "        total_size = 0\n",
    "        \n",
    "        # Only copy files modified since last full backup\n",
    "        for file_path in self.source_dir.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                stat = file_path.stat()\n",
    "                mod_time = datetime.fromtimestamp(stat.st_mtime)\n",
    "                \n",
    "                if mod_time > last_full:\n",
    "                    relative_path = file_path.relative_to(self.source_dir)\n",
    "                    dest_path = backup_path / relative_path\n",
    "                    dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                    \n",
    "                    shutil.copy2(file_path, dest_path)\n",
    "                    files_backed += 1\n",
    "                    total_size += stat.st_size\n",
    "        \n",
    "        # Record in manifest\n",
    "        backup_info = {\n",
    "            'name': name,\n",
    "            'type': 'incremental',\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'files': files_backed,\n",
    "            'size_bytes': total_size,\n",
    "            'path': str(backup_path),\n",
    "            'base_backup': last_full.isoformat()\n",
    "        }\n",
    "        self.manifest['backups'].append(backup_info)\n",
    "        self._save_manifest()\n",
    "        \n",
    "        print(f\"Incremental backup completed: {name}\")\n",
    "        print(f\"  Files: {files_backed}\")\n",
    "        print(f\"  Size: {total_size / (1024*1024):.2f} MB\")\n",
    "        \n",
    "        return backup_path\n",
    "    \n",
    "    def verify_backup(self, backup_name: str) -> bool:\n",
    "        \"\"\"\n",
    "        Verify backup integrity by checking checksums.\n",
    "        \n",
    "        Args:\n",
    "            backup_name: Name of backup to verify\n",
    "        \n",
    "        Returns:\n",
    "            True if backup is valid\n",
    "        \"\"\"\n",
    "        backup_path = self.backup_dir / backup_name\n",
    "        \n",
    "        if not backup_path.exists():\n",
    "            print(f\"Backup not found: {backup_name}\")\n",
    "            return False\n",
    "        \n",
    "        # Simple verification: check all files exist and are readable\n",
    "        errors = []\n",
    "        for file_path in backup_path.rglob('*'):\n",
    "            if file_path.is_file():\n",
    "                try:\n",
    "                    with open(file_path, 'rb') as f:\n",
    "                        f.read(1)  # Try to read first byte\n",
    "                except Exception as e:\n",
    "                    errors.append(f\"Error reading {file_path}: {e}\")\n",
    "        \n",
    "        if errors:\n",
    "            print(f\"Backup verification failed with {len(errors)} errors\")\n",
    "            return False\n",
    "        \n",
    "        print(f\"Backup verified successfully: {backup_name}\")\n",
    "        return True\n",
    "    \n",
    "    def restore(self, \n",
    "                backup_name: str,\n",
    "                target_dir: Optional[str] = None,\n",
    "                dry_run: bool = True):\n",
    "        \"\"\"\n",
    "        Restore from backup.\n",
    "        \n",
    "        Args:\n",
    "            backup_name: Name of backup to restore\n",
    "            target_dir: Directory to restore to (default: source_dir)\n",
    "            dry_run: If True, only show what would be restored\n",
    "        \"\"\"\n",
    "        backup_path = self.backup_dir / backup_name\n",
    "        \n",
    "        if not backup_path.exists():\n",
    "            print(f\"Backup not found: {backup_name}\")\n",
    "            return\n",
    "        \n",
    "        if target_dir is None:\n",
    "            target_dir = self.source_dir\n",
    "        else:\n",
    "            target_dir = Path(target_dir)\n",
    "        \n",
    "        print(f\"Restoring from {backup_name} to {target_dir}\")\n",
    "        \n",
    "        # List all files in backup\n",
    "        files = [f for f in backup_path.rglob('*') if f.is_file()]\n",
    "        \n",
    "        if dry_run:\n",
    "            print(f\"Would restore {len(files)} files:\")\n",
    "            for f in files[:10]:  # Show first 10\n",
    "                rel = f.relative_to(backup_path)\n",
    "                print(f\"  {rel}\")\n",
    "            if len(files) > 10:\n",
    "                print(f\"  ... and {len(files) - 10} more\")\n",
    "        else:\n",
    "            # Perform restore\n",
    "            for file_path in files:\n",
    "                relative_path = file_path.relative_to(backup_path)\n",
    "                dest_path = target_dir / relative_path\n",
    "                dest_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                shutil.copy2(file_path, dest_path)\n",
    "            \n",
    "            print(f\"Restored {len(files)} files\")\n",
    "    \n",
    "    def list_backups(self) -> List[Dict]:\n",
    "        \"\"\"List all available backups.\"\"\"\n",
    "        return self.manifest['backups']\n",
    "    \n",
    "    def cleanup_old_backups(self, keep_days: int = 30):\n",
    "        \"\"\"\n",
    "        Remove backups older than specified days.\n",
    "        \n",
    "        Args:\n",
    "            keep_days: Number of days to keep backups\n",
    "        \"\"\"\n",
    "        cutoff = datetime.now() - timedelta(days=keep_days)\n",
    "        \n",
    "        to_remove = []\n",
    "        for backup in self.manifest['backups']:\n",
    "            backup_time = datetime.fromisoformat(backup['timestamp'])\n",
    "            if backup_time < cutoff:\n",
    "                backup_path = Path(backup['path'])\n",
    "                if backup_path.exists():\n",
    "                    shutil.rmtree(backup_path)\n",
    "                to_remove.append(backup)\n",
    "        \n",
    "        # Update manifest\n",
    "        self.manifest['backups'] = [b for b in self.manifest['backups'] if b not in to_remove]\n",
    "        self._save_manifest()\n",
    "        \n",
    "        print(f\"Removed {len(to_remove)} old backups\")\n",
    "\n",
    "\n",
    "def demonstrate_backup():\n",
    "    \"\"\"\n",
    "    Demonstrate backup operations.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Backup and Recovery Demonstration\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    manager = BackupManager('./data', './backups')\n",
    "    \n",
    "    print(\"\\nBackup Strategy:\")\n",
    "    print(\"  - Full backup: Weekly (all data)\")\n",
    "    print(\"  - Incremental: Daily (changes only)\")\n",
    "    print(\"  - Retention: 30 days\")\n",
    "    print(\"  - Verification: Checksums on completion\")\n",
    "    \n",
    "    print(\"\\nExample commands:\")\n",
    "    print(\"  manager.full_backup('weekly_2024_01_15')\")\n",
    "    print(\"  manager.incremental_backup('daily_2024_01_16')\")\n",
    "    print(\"  manager.verify_backup('weekly_2024_01_15')\")\n",
    "    print(\"  manager.restore('weekly_2024_01_15', dry_run=False)\")\n",
    "    \n",
    "    return manager\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_backup()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **3-2-1 Rule**:\n",
    "   - **3 copies**: Original + 2 backups\n",
    "   - **2 different media**: Local disk + cloud/offsite\n",
    "   - **1 offsite**: Protect against site disaster\n",
    "\n",
    "2. **Backup Types**:\n",
    "   - **Full**: Complete copy, slow, large\n",
    "   - **Incremental**: Changes since last backup, fast, small\n",
    "   - **Differential**: Changes since last full, medium size\n",
    "\n",
    "3. **Recovery**:\n",
    "   - **Point-in-time**: Restore to specific moment\n",
    "   - **Granular**: Restore single file or table\n",
    "   - **Testing**: Regular restore drills to verify backups work\n",
    "\n",
    "---\n",
    "\n",
    "## **8.10 Data Security and Compliance**\n",
    "\n",
    "Protecting sensitive financial data and meeting regulatory requirements.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Data Security and Compliance for NEPSE Data\n",
    "\n",
    "Security measures:\n",
    "1. Encryption at rest (disk/files)\n",
    "2. Encryption in transit (TLS/SSL)\n",
    "3. Access control (authentication/authorization)\n",
    "4. Audit logging (who accessed what)\n",
    "5. Data masking (for non-prod environments)\n",
    "\n",
    "Compliance:\n",
    "- Data privacy laws (GDPR, CCPA)\n",
    "- Financial regulations (SEC, FINRA)\n",
    "- Audit requirements\n",
    "\"\"\"\n",
    "\n",
    "from pathlib import Path\n",
    "from typing import Optional, Dict, Any\n",
    "import hashlib\n",
    "import secrets\n",
    "\n",
    "\n",
    "class DataSecurityManager:\n",
    "    \"\"\"\n",
    "    Manage data security for NEPSE storage.\n",
    "    \n",
    "    Security layers:\n",
    "    - Network: TLS, VPN, firewalls\n",
    "    - Application: Authentication, authorization\n",
    "    - Data: Encryption, masking, tokenization\n",
    "    - Physical: Data center security\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.audit_log = []\n",
    "    \n",
    "    def encrypt_file(self,\n",
    "                     file_path: Path,\n",
    "                     password: str,\n",
    "                     output_path: Optional[Path] = None) -> Path:\n",
    "        \"\"\"\n",
    "        Encrypt file using Fernet (symmetric encryption).\n",
    "        \n",
    "        Args:\n",
    "            file_path: File to encrypt\n",
    "            password: Encryption password\n",
    "            output_path: Output file path\n",
    "        \n",
    "        Returns:\n",
    "            Path to encrypted file\n",
    "        \"\"\"\n",
    "        from cryptography.fernet import Fernet\n",
    "        from cryptography.hazmat.primitives import hashes\n",
    "        from cryptography.hazmat.primitives.kdf.pbkdf2 import PBKDF2HMAC\n",
    "        import base64\n",
    "        \n",
    "        # Derive key from password\n",
    "        kdf = PBKDF2HMAC(\n",
    "            algorithm=hashes.SHA256(),\n",
    "            length=32,\n",
    "            salt=secrets.token_bytes(16),\n",
    "            iterations=480000,\n",
    "        )\n",
    "        key = base64.urlsafe_b64encode(kdf.derive(password.encode()))\n",
    "        \n",
    "        f = Fernet(key)\n",
    "        \n",
    "        # Read and encrypt\n",
    "        with open(file_path, 'rb') as file:\n",
    "            file_data = file.read()\n",
    "        \n",
    "        encrypted_data = f.encrypt(file_data)\n",
    "        \n",
    "        # Write encrypted file\n",
    "        if output_path is None:\n",
    "            output_path = file_path.with_suffix(file_path.suffix + '.encrypted')\n",
    "        \n",
    "        with open(output_path, 'wb') as file:\n",
    "            file.write(encrypted_data)\n",
    "        \n",
    "        self.log_audit('ENCRYPT', str(file_path), str(output_path))\n",
    "        return output_path\n",
    "    \n",
    "    def hash_sensitive_data(self, data: str) -> str:\n",
    "        \"\"\"\n",
    "        One-way hash for sensitive data (PII).\n",
    "        \n",
    "        Args:\n",
    "            data: String to hash\n",
    "        \n",
    "        Returns:\n",
    "            SHA-256 hash\n",
    "        \"\"\"\n",
    "        return hashlib.sha256(data.encode()).hexdigest()\n",
    "    \n",
    "    def mask_data(self, \n",
    "                  df: pd.DataFrame,\n",
    "                  columns: Dict[str, str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Mask sensitive data for non-production use.\n",
    "        \n",
    "        Args:\n",
    "            df: DataFrame with sensitive data\n",
    "            columns: Dict of column -> mask_type\n",
    "                    mask_types: 'hash', 'partial', 'random', 'null'\n",
    "        \n",
    "        Returns:\n",
    "            Masked DataFrame\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        for col, mask_type in columns.items():\n",
    "            if col not in df.columns:\n",
    "                continue\n",
    "            \n",
    "            if mask_type == 'hash':\n",
    "                df[col] = df[col].astype(str).apply(self.hash_sensitive_data)\n",
    "            \n",
    "            elif mask_type == 'partial':\n",
    "                # Show first 2 and last 2 characters only\n",
    "                df[col] = df[col].astype(str).apply(\n",
    "                    lambda x: x[:2] + '***' + x[-2:] if len(x) > 4 else '****'\n",
    "                )\n",
    "            \n",
    "            elif mask_type == 'random':\n",
    "                # Replace with random values of same type\n",
    "                if df[col].dtype == 'int64':\n",
    "                    df[col] = np.random.randint(0, 100, size=len(df))\n",
    "                else:\n",
    "                    df[col] = ['RANDOM'] * len(df)\n",
    "            \n",
    "            elif mask_type == 'null':\n",
    "                df[col] = None\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def log_audit(self, \n",
    "                  action: str, \n",
    "                  resource: str,\n",
    "                  details: str = ''):\n",
    "        \"\"\"\n",
    "        Log security-relevant actions.\n",
    "        \n",
    "        Args:\n",
    "            action: Action performed (READ, WRITE, DELETE, etc.)\n",
    "            resource: Resource affected\n",
    "            details: Additional details\n",
    "        \"\"\"\n",
    "        from datetime import datetime\n",
    "        import getpass\n",
    "        \n",
    "        entry = {\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'user': getpass.getuser(),\n",
    "            'action': action,\n",
    "            'resource': resource,\n",
    "            'details': details\n",
    "        }\n",
    "        self.audit_log.append(entry)\n",
    "    \n",
    "    def get_audit_log(self) -> pd.DataFrame:\n",
    "        \"\"\"Get audit log as DataFrame.\"\"\"\n",
    "        return pd.DataFrame(self.audit_log)\n",
    "\n",
    "\n",
    "def demonstrate_security():\n",
    "    \"\"\"\n",
    "    Demonstrate security features.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Data Security and Compliance\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    security = DataSecurityManager()\n",
    "    \n",
    "    print(\"\\nSecurity Measures:\")\n",
    "    print(\"  1. Encryption at rest (AES-256)\")\n",
    "    print(\"  2. Encryption in transit (TLS 1.3)\")\n",
    "    print(\"  3. Access control (RBAC)\")\n",
    "    print(\"  4. Audit logging (all access)\")\n",
    "    print(\"  5. Data masking (non-prod)\")\n",
    "    \n",
    "    print(\"\\nCompliance Requirements:\")\n",
    "    print(\"  - Data retention: 7 years\")\n",
    "    print(\"  - Access logs: Indefinite\")\n",
    "    print(\"  - Encryption: Required for PII\")\n",
    "    print(\"  - Backup testing: Quarterly\")\n",
    "    \n",
    "    return security\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_security()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Encryption**:\n",
    "   - **At rest**: Files encrypted on disk (AES-256)\n",
    "   - **In transit**: TLS for network communication\n",
    "   - **In use**: Memory encryption (advanced)\n",
    "\n",
    "2. **Access Control**:\n",
    "   - **Authentication**: Verify identity (passwords, keys, MFA)\n",
    "   - **Authorization**: Verify permissions (RBAC - Role-Based Access Control)\n",
    "   - **Principle of least privilege**: Minimum necessary access\n",
    "\n",
    "3. **Compliance**:\n",
    "   - **GDPR**: Right to deletion, data portability\n",
    "   - **SEC Rules 17a-3/4**: Record keeping for brokers\n",
    "   - **Audit trails**: Immutable logs of all access\n",
    "\n",
    "---\n",
    "\n",
    "## **8.11 Choosing the Right Storage**\n",
    "\n",
    "Decision framework for selecting the appropriate storage solution.\n",
    "\n",
    "```python\n",
    "\"\"\"\n",
    "Storage Decision Framework for NEPSE Prediction System\n",
    "\n",
    "This module provides a decision tree and scoring system to help\n",
    "choose the right storage solution based on specific requirements.\n",
    "\"\"\"\n",
    "\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Any\n",
    "from enum import Enum\n",
    "\n",
    "\n",
    "class StorageOption(Enum):\n",
    "    CSV = \"CSV Files\"\n",
    "    PARQUET = \"Parquet Files\"\n",
    "    HDF5 = \"HDF5 Files\"\n",
    "    SQLITE = \"SQLite\"\n",
    "    POSTGRESQL = \"PostgreSQL\"\n",
    "    TIMESCALEDB = \"TimescaleDB\"\n",
    "    INFLUXDB = \"InfluxDB\"\n",
    "    MONGODB = \"MongoDB\"\n",
    "    CASSANDRA = \"Apache Cassandra\"\n",
    "    REDIS = \"Redis\"\n",
    "    S3 = \"AWS S3 / Cloud Storage\"\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Requirements:\n",
    "    \"\"\"System requirements for storage selection.\"\"\"\n",
    "    data_size_gb: float\n",
    "    write_frequency: str  # 'high', 'medium', 'low'\n",
    "    query_pattern: str    # 'time_range', 'point', 'analytical', 'mixed'\n",
    "    concurrency: int      # Number of concurrent users\n",
    "    budget: str          # 'low', 'medium', 'high'\n",
    "    team_size: int       # For operational complexity\n",
    "    latency_requirement: str  # 'real_time', 'batch', 'analytical'\n",
    "    data_retention_years: int\n",
    "    need_sql: bool\n",
    "    need_scaling: bool\n",
    "\n",
    "\n",
    "class StorageRecommender:\n",
    "    \"\"\"\n",
    "    Recommend storage solution based on requirements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.scoring = {\n",
    "            StorageOption.CSV: {\n",
    "                'small_data': 10, 'large_data': 2,\n",
    "                'low_write': 10, 'high_write': 3,\n",
    "                'simple_query': 10, 'complex_query': 3,\n",
    "                'low_concurrency': 10, 'high_concurrency': 2,\n",
    "                'low_budget': 10, 'high_budget': 5,\n",
    "                'small_team': 10, 'large_team': 5,\n",
    "                'batch_latency': 10, 'realtime_latency': 1,\n",
    "                'short_retention': 10, 'long_retention': 5,\n",
    "                'sql_needed': 3, 'sql_not_needed': 10,\n",
    "                'scaling_needed': 1, 'scaling_not_needed': 10\n",
    "            },\n",
    "            StorageOption.PARQUET: {\n",
    "                'small_data': 8, 'large_data': 9,\n",
    "                'low_write': 8, 'high_write': 6,\n",
    "                'simple_query': 7, 'complex_query': 9,\n",
    "                'low_concurrency': 8, 'high_concurrency': 6,\n",
    "                'low_budget': 9, 'high_budget': 8,\n",
    "                'small_team': 9, 'large_team': 7,\n",
    "                'batch_latency': 9, 'realtime_latency': 3,\n",
    "                'short_retention': 8, 'long_retention': 9,\n",
    "                'sql_needed': 5, 'sql_not_needed': 10,\n",
    "                'scaling_needed': 5, 'scaling_not_needed': 10\n",
    "            },\n",
    "            StorageOption.TIMESCALEDB: {\n",
    "                'small_data': 7, 'large_data': 10,\n",
    "                'low_write': 7, 'high_write': 9,\n",
    "                'simple_query': 9, 'complex_query': 10,\n",
    "                'low_concurrency': 8, 'high_concurrency': 9,\n",
    "                'low_budget': 6, 'high_budget': 9,\n",
    "                'small_team': 6, 'large_team': 9,\n",
    "                'batch_latency': 9, 'realtime_latency': 8,\n",
    "                'short_retention': 8, 'long_retention': 10,\n",
    "                'sql_needed': 10, 'sql_not_needed': 6,\n",
    "                'scaling_needed': 8, 'scaling_not_needed': 9\n",
    "            },\n",
    "            StorageOption.INFLUXDB: {\n",
    "                'small_data': 6, 'large_data': 9,\n",
    "                'low_write': 6, 'high_write': 10,\n",
    "                'simple_query': 8, 'complex_query': 7,\n",
    "                'low_concurrency': 7, 'high_concurrency': 9,\n",
    "                'low_budget': 5, 'high_budget': 8,\n",
    "                'small_team': 5, 'large_team': 8,\n",
    "                'batch_latency': 7, 'realtime_latency': 10,\n",
    "                'short_retention': 7, 'long_retention': 9,\n",
    "                'sql_needed': 3, 'sql_not_needed': 10,\n",
    "                'scaling_needed': 8, 'scaling_not_needed': 8\n",
    "            }\n",
    "        }\n",
    "    \n",
    "    def recommend(self, req: Requirements) -> Dict[StorageOption, float]:\n",
    "        \"\"\"\n",
    "        Score each storage option based on requirements.\n",
    "        \n",
    "        Returns:\n",
    "            Dictionary mapping options to scores (higher is better)\n",
    "        \"\"\"\n",
    "        scores = {}\n",
    "        \n",
    "        for option in StorageOption:\n",
    "            if option not in self.scoring:\n",
    "                continue\n",
    "            \n",
    "            score = 0\n",
    "            weights = self.scoring[option]\n",
    "            \n",
    "            # Data size\n",
    "            if req.data_size_gb < 1:\n",
    "                score += weights.get('small_data', 5)\n",
    "            else:\n",
    "                score += weights.get('large_data', 5)\n",
    "            \n",
    "            # Write frequency\n",
    "            if req.write_frequency == 'high':\n",
    "                score += weights.get('high_write', 5)\n",
    "            else:\n",
    "                score += weights.get('low_write', 5)\n",
    "            \n",
    "            # Query pattern\n",
    "            if req.query_pattern in ['analytical', 'mixed']:\n",
    "                score += weights.get('complex_query', 5)\n",
    "            else:\n",
    "                score += weights.get('simple_query', 5)\n",
    "            \n",
    "            # Concurrency\n",
    "            if req.concurrency > 10:\n",
    "                score += weights.get('high_concurrency', 5)\n",
    "            else:\n",
    "                score += weights.get('low_concurrency', 5)\n",
    "            \n",
    "            # Budget\n",
    "            if req.budget == 'low':\n",
    "                score += weights.get('low_budget', 5)\n",
    "            else:\n",
    "                score += weights.get('high_budget', 5)\n",
    "            \n",
    "            # Latency\n",
    "            if req.latency_requirement == 'real_time':\n",
    "                score += weights.get('realtime_latency', 5)\n",
    "            else:\n",
    "                score += weights.get('batch_latency', 5)\n",
    "            \n",
    "            # SQL need\n",
    "            if req.need_sql:\n",
    "                score += weights.get('sql_needed', 5)\n",
    "            else:\n",
    "                score += weights.get('sql_not_needed', 5)\n",
    "            \n",
    "            # Scaling\n",
    "            if req.need_scaling:\n",
    "                score += weights.get('scaling_needed', 5)\n",
    "            else:\n",
    "                score += weights.get('scaling_not_needed', 5)\n",
    "            \n",
    "            scores[option] = score\n",
    "        \n",
    "        return dict(sorted(scores.items(), key=lambda x: x[1], reverse=True))\n",
    "\n",
    "\n",
    "def demonstrate_decision_framework():\n",
    "    \"\"\"\n",
    "    Demonstrate the storage decision framework.\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"Storage Decision Framework\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    recommender = StorageRecommender()\n",
    "    \n",
    "    # Define NEPSE requirements\n",
    "    nepse_reqs = Requirements(\n",
    "        data_size_gb=0.5,           # 500 MB historical data\n",
    "        write_frequency='low',     # Daily updates\n",
    "        query_pattern='time_range', # Time-series analysis\n",
    "        concurrency=5,              # Small team\n",
    "        budget='low',               # Limited budget\n",
    "        team_size=2,                # Small team\n",
    "        latency_requirement='batch', # Not real-time\n",
    "        data_retention_years=10,    # Long history\n",
    "        need_sql=True,              # SQL familiarity\n",
    "        need_scaling=False          # Not massive scale\n",
    "    )\n",
    "    \n",
    "    print(\"\\nNEPSE System Requirements:\")\n",
    "    print(f\"  Data size: {nepse_reqs.data_size_gb} GB\")\n",
    "    print(f\"  Write frequency: {nepse_reqs.write_frequency}\")\n",
    "    print(f\"  Query pattern: {nepse_reqs.query_pattern}\")\n",
    "    print(f\"  Concurrency: {nepse_reqs.concurrency}\")\n",
    "    print(f\"  Budget: {nepse_reqs.budget}\")\n",
    "    print(f\"  Team size: {nepse_reqs.team_size}\")\n",
    "    print(f\"  Latency: {nepse_reqs.latency_requirement}\")\n",
    "    print(f\"  Retention: {nepse_reqs.data_retention_years} years\")\n",
    "    print(f\"  SQL needed: {nepse_reqs.need_sql}\")\n",
    "    print(f\"  Scaling needed: {nepse_reqs.need_scaling}\")\n",
    "    \n",
    "    print(\"\\nRecommendations (ranked by score):\")\n",
    "    scores = recommender.recommend(nepse_reqs)\n",
    "    \n",
    "    for i, (option, score) in enumerate(scores.items(), 1):\n",
    "        print(f\"  {i}. {option.value:20s} (Score: {score:.1f})\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Final Recommendation for NEPSE:\")\n",
    "    print(\"=\" * 70)\n",
    "    print(\"\"\"\n",
    "    Primary: Parquet files with partitioning\n",
    "      - Cost-effective\n",
    "      - Good performance for time-series\n",
    "      - Easy to manage with small team\n",
    "      - Compatible with pandas/NumPy\n",
    "    \n",
    "    Secondary: SQLite for metadata\n",
    "      - Simple relational data\n",
    "      - Zero configuration\n",
    "      - Single file backup\n",
    "    \n",
    "    Future (if scaling needed): TimescaleDB\n",
    "      - Easy migration from PostgreSQL\n",
    "      - Time-series optimizations\n",
    "      - When data grows beyond 10GB\n",
    "    \"\"\")\n",
    "    \n",
    "    return recommender, nepse_reqs\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    demonstrate_decision_framework()\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "1. **Decision Criteria**:\n",
    "   - **Data size**: Small (<1GB) vs Large (>100GB)\n",
    "   - **Write pattern**: Batch (daily) vs Streaming (real-time)\n",
    "   - **Query complexity**: Simple lookups vs Analytical aggregations\n",
    "   - **Team expertise**: SQL familiarity vs NoSQL learning curve\n",
    "   - **Operational capacity**: Managed services vs Self-hosted\n",
    "\n",
    "2. **NEPSE Recommendation**:\n",
    "   - **Current (Small scale)**: Parquet files + SQLite\n",
    "     - 500MB data fits in memory\n",
    "     - Daily batch updates\n",
    "     - Simple time-range queries\n",
    "     - Zero operational overhead\n",
    "   \n",
    "   - **Future (Scale up)**: TimescaleDB\n",
    "     - When data exceeds 10GB\n",
    "     - Need concurrent users\n",
    "     - Complex SQL analytics\n",
    "     - Managed service available\n",
    "\n",
    "3. **Hybrid Approach**: Most production systems use multiple storage types:\n",
    "   - **Hot data**: Redis (recent prices)\n",
    "   - **Warm data**: TimescaleDB (historical prices)\n",
    "   - **Cold data**: Parquet on S3 (archived data)\n",
    "   - **Metadata**: PostgreSQL (stock info, users)\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we covered comprehensive data storage and management strategies for time-series prediction systems:\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Storage Architecture Decisions**: Analyze requirements (size, access patterns, latency) before choosing storage.\n",
    "\n",
    "2. **File-Based Storage**:\n",
    "   - **CSV**: Universal compatibility, human-readable, poor performance\n",
    "   - **Parquet**: Columnar, compressed, ideal for analytics\n",
    "   - **HDF5**: Scientific data, hierarchical organization\n",
    "   - **Feather**: Fast I/O for temporary storage\n",
    "\n",
    "3. **Relational Databases**:\n",
    "   - **SQLite**: Zero-config, embedded, single-user\n",
    "   - **PostgreSQL**: Advanced features, extensibility\n",
    "   - **TimescaleDB**: PostgreSQL extension for time-series\n",
    "   - **Indexing**: Composite indexes on (symbol, date) critical for performance\n",
    "\n",
    "4. **Time-Series Databases**:\n",
    "   - **InfluxDB**: High write throughput, purpose-built for time-series\n",
    "   - **TimescaleDB**: SQL compatibility with time-series optimizations\n",
    "   - **Prometheus**: Monitoring and metrics (not primary storage)\n",
    "\n",
    "5. **NoSQL Solutions**:\n",
    "   - **MongoDB**: Flexible schema, document storage\n",
    "   - **Redis**: In-memory cache, ultra-low latency\n",
    "   - **Cassandra**: Massive scale, high availability\n",
    "\n",
    "6. **Cloud Storage**: S3 for files, BigQuery for analytics, managed databases for operational data.\n",
    "\n",
    "7. **Partitioning**: Time-based partitioning essential for large datasets; enables efficient querying and data lifecycle management.\n",
    "\n",
    "8. **Retention & Archival**: Automate data lifecycle from hot (SSD) to warm (disk) to cold (glacier) to deleted.\n",
    "\n",
    "9. **Backup & Recovery**: 3-2-1 rule, regular testing, point-in-time recovery capability.\n",
    "\n",
    "10. **Security**: Encryption at rest and in transit, access control, audit logging, compliance with financial regulations.\n",
    "\n",
    "11. **Decision Framework**: For NEPSE (small scale), start with Parquet + SQLite; migrate to TimescaleDB as data grows.\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "Chapter 9 will cover **Data Pipelines and Automation**, including:\n",
    "- Batch and streaming pipeline architectures\n",
    "- Workflow orchestration with Apache Airflow\n",
    "- Data quality gates and validation\n",
    "- Monitoring and error handling\n",
    "- Building production-ready data pipelines\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 8**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='7. exploratory_data_analysis.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='9. data_pipelines_and_automation.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}