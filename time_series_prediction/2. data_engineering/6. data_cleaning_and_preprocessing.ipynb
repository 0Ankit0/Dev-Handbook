{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 6: Data Cleaning and Preprocessing**\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Develop systematic data cleaning strategies for time-series data\n",
    "- Identify and handle duplicate records effectively\n",
    "- Standardize data types and formats across datasets\n",
    "- Analyze missing data patterns (MCAR, MAR, MNAR) and choose appropriate imputation strategies\n",
    "- Apply advanced imputation techniques including KNN and iterative methods\n",
    "- Detect and treat outliers using statistical, ML, and domain-specific approaches\n",
    "- Implement data smoothing and noise reduction techniques\n",
    "- Build automated preprocessing pipelines for production systems\n",
    "- Ensure reproducibility in data preprocessing steps\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "- Completed Chapter 4: Data Fundamentals and Chapter 5: Data Collection\n",
    "- Understanding of pandas DataFrame operations\n",
    "- Basic statistical concepts (mean, median, standard deviation)\n",
    "- Familiarity with NEPSE data structure (OHLCV format)\n",
    "\n",
    "---\n",
    "\n",
    "## **6.1 Data Cleaning Strategy**\n",
    "\n",
    "Data cleaning is not just about fixing errors\u2014it's about understanding your data's limitations and ensuring it meets the requirements of your prediction models. A systematic approach prevents ad-hoc decisions that can introduce bias.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import logging\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "class DataCleaningStrategy:\n",
    "    \"\"\"\n",
    "    Framework for systematic data cleaning in time-series prediction systems.\n",
    "    \n",
    "    The strategy follows these principles:\n",
    "    1. Assess before acting (understand the data first)\n",
    "    2. Document everything (track what was changed and why)\n",
    "    3. Preserve raw data (never modify source files)\n",
    "    4. Validate after cleaning (ensure quality improved, not degraded)\n",
    "    5. Make reproducible (same cleaning steps every time)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, raw_data: pd.DataFrame, symbol: str):\n",
    "        self.raw_data = raw_data.copy()\n",
    "        self.symbol = symbol\n",
    "        self.cleaning_log = []\n",
    "        self.quality_metrics = {}\n",
    "        \n",
    "        # Calculate initial quality metrics\n",
    "        self._assess_initial_quality()\n",
    "    \n",
    "    def _assess_initial_quality(self):\n",
    "        \"\"\"\n",
    "        Baseline assessment before any cleaning.\n",
    "        \n",
    "        These metrics help determine if cleaning improved the data\n",
    "        or accidentally removed useful information.\n",
    "        \"\"\"\n",
    "        self.quality_metrics['initial'] = {\n",
    "            'total_rows': len(self.raw_data),\n",
    "            'missing_values': self.raw_data.isna().sum().sum(),\n",
    "            'duplicate_rows': self.raw_data.duplicated().sum(),\n",
    "            'date_range': (self.raw_data.index.min(), self.raw_data.index.max()) \n",
    "                          if isinstance(self.raw_data.index, pd.DatetimeIndex) else None,\n",
    "            'numeric_outliers': self._count_outliers(),\n",
    "            'memory_usage_mb': self.raw_data.memory_usage(deep=True).sum() / 1024 / 1024\n",
    "        }\n",
    "        \n",
    "        print(f\"Initial Quality Assessment for {self.symbol}:\")\n",
    "        print(f\"  Records: {self.quality_metrics['initial']['total_rows']}\")\n",
    "        print(f\"  Missing: {self.quality_metrics['initial']['missing_values']}\")\n",
    "        print(f\"  Duplicates: {self.quality_metrics['initial']['duplicate_rows']}\")\n",
    "        print(f\"  Outliers: {self.quality_metrics['initial']['numeric_outliers']}\")\n",
    "    \n",
    "    def _count_outliers(self) -> int:\n",
    "        \"\"\"Count outliers using IQR method for numeric columns.\"\"\"\n",
    "        numeric_cols = self.raw_data.select_dtypes(include=[np.number]).columns\n",
    "        outlier_count = 0\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            Q1 = self.raw_data[col].quantile(0.25)\n",
    "            Q3 = self.raw_data[col].quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - 1.5 * IQR\n",
    "            upper = Q3 + 1.5 * IQR\n",
    "            outlier_count += ((self.raw_data[col] < lower) | (self.raw_data[col] > upper)).sum()\n",
    "        \n",
    "        return outlier_count\n",
    "    \n",
    "    def log_action(self, action: str, details: str, rows_affected: int):\n",
    "        \"\"\"Log every cleaning action for audit trail.\"\"\"\n",
    "        self.cleaning_log.append({\n",
    "            'timestamp': datetime.now().isoformat(),\n",
    "            'action': action,\n",
    "            'details': details,\n",
    "            'rows_affected': rows_affected,\n",
    "            'symbol': self.symbol\n",
    "        })\n",
    "    \n",
    "    def get_cleaning_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate comprehensive cleaning report.\"\"\"\n",
    "        if not self.cleaning_log:\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        log_df = pd.DataFrame(self.cleaning_log)\n",
    "        \n",
    "        # Calculate final metrics if not done\n",
    "        if 'final' not in self.quality_metrics:\n",
    "            self.quality_metrics['final'] = {\n",
    "                'total_rows': len(self.raw_data),\n",
    "                'missing_values': self.raw_data.isna().sum().sum(),\n",
    "                'duplicate_rows': self.raw_data.duplicated().sum(),\n",
    "                'numeric_outliers': self._count_outliers()\n",
    "            }\n",
    "        \n",
    "        # Calculate changes\n",
    "        initial = self.quality_metrics['initial']\n",
    "        final = self.quality_metrics['final']\n",
    "        \n",
    "        summary = {\n",
    "            'metric': ['Total Rows', 'Missing Values', 'Duplicate Rows', 'Outliers'],\n",
    "            'initial': [initial['total_rows'], initial['missing_values'], \n",
    "                       initial['duplicate_rows'], initial['numeric_outliers']],\n",
    "            'final': [final['total_rows'], final['missing_values'],\n",
    "                     final['duplicate_rows'], final['numeric_outliers']],\n",
    "            'change': [\n",
    "                final['total_rows'] - initial['total_rows'],\n",
    "                final['missing_values'] - initial['missing_values'],\n",
    "                final['duplicate_rows'] - initial['duplicate_rows'],\n",
    "                final['numeric_outliers'] - initial['numeric_outliers']\n",
    "            ]\n",
    "        }\n",
    "        \n",
    "        return pd.DataFrame(summary)\n",
    "\n",
    "# Usage example with NEPSE data\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=20, freq='B')\n",
    "\n",
    "# Create sample data with intentional issues\n",
    "raw_nepse = pd.DataFrame({\n",
    "    'Open': [2850.50, 2875.25, np.nan, 2865.75, 2880.50] * 4,\n",
    "    'High': [2890.00, 2910.00, 2920.00, 2900.00, 2915.00] * 4,\n",
    "    'Low': [2840.00, 2860.00, 2880.00, 2850.00, 2870.00] * 4,\n",
    "    'Close': [2875.25, 2895.50, 2900.00, np.nan, 2905.00] * 4,\n",
    "    'Volume': [125000, 150000, 175000, 140000, np.nan] * 4\n",
    "}, index=dates)\n",
    "\n",
    "# Add duplicates\n",
    "duplicate_row = raw_nepse.iloc[0:1].copy()\n",
    "raw_nepse = pd.concat([raw_nepse, duplicate_row])\n",
    "\n",
    "# Initialize strategy\n",
    "strategy = DataCleaningStrategy(raw_nepse, 'NABIL')\n",
    "\n",
    "print(\"\\nCleaning log initialized\")\n",
    "print(\"Strategy: Document all changes, preserve raw data, validate results\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Systematic cleaning** requires a framework, not ad-hoc fixes.\n",
    "- **The strategy class** tracks:\n",
    "  - **Initial metrics**: Baseline quality before cleaning\n",
    "  - **Actions log**: Every modification documented with timestamp\n",
    "  - **Final metrics**: Quality after cleaning\n",
    "  - **Comparison**: Did we improve or degrade quality?\n",
    "- **Key principle**: Never modify raw data files. Always work on copies and save cleaned versions separately.\n",
    "- **Quality metrics**:\n",
    "  - Total rows: Did we accidentally delete valid data?\n",
    "  - Missing values: How complete is the data?\n",
    "  - Duplicates: Are there redundant records?\n",
    "  - Outliers: How many extreme values exist?\n",
    "- **For NEPSE data**, tracking these metrics is crucial because:\n",
    "  - Stock data should have no duplicates (each date-symbol pair is unique)\n",
    "  - Missing values might indicate trading halts (information, not just errors)\n",
    "  - Outliers might be genuine market events (earnings announcements, crashes)\n",
    "\n",
    "---\n",
    "\n",
    "## **6.2 Duplicate Detection and Removal**\n",
    "\n",
    "Duplicates in time-series data can skew statistical analyses and cause data leakage in train-test splits. However, not all duplicates are errors\u2014some represent legitimate repeated measurements.\n",
    "\n",
    "```python\n",
    "class DuplicateHandler:\n",
    "    \"\"\"\n",
    "    Handle duplicates in time-series data with awareness of context.\n",
    "    \n",
    "    Types of duplicates in financial data:\n",
    "    1. Exact duplicates: Same timestamp, same values (data entry error)\n",
    "    2. Partial duplicates: Same timestamp, different values (correction/update)\n",
    "    3. Near-duplicates: Slightly different timestamps (timezone issues)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.duplicate_analysis = {}\n",
    "    \n",
    "    def analyze_duplicates(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Comprehensive duplicate analysis.\n",
    "        \"\"\"\n",
    "        analysis = {}\n",
    "        \n",
    "        # 1. Exact duplicates (all columns identical)\n",
    "        exact_dups = self.df.duplicated(keep=False)\n",
    "        analysis['exact_duplicates'] = {\n",
    "            'count': exact_dups.sum(),\n",
    "            'indices': self.df[exact_dups].index.tolist()\n",
    "        }\n",
    "        \n",
    "        # 2. Index duplicates (same timestamp, any values)\n",
    "        idx_dups = self.df.index.duplicated(keep=False)\n",
    "        analysis['index_duplicates'] = {\n",
    "            'count': idx_dups.sum(),\n",
    "            'groups': self._analyze_index_duplicates()\n",
    "        }\n",
    "        \n",
    "        # 3. Subset duplicates (same date and symbol)\n",
    "        if 'Symbol' in self.df.columns:\n",
    "            subset_cols = [self.df.index.name or 'index', 'Symbol']\n",
    "            subset_dups = self.df.duplicated(subset=subset_cols, keep=False)\n",
    "            analysis['subset_duplicates'] = {\n",
    "                'count': subset_dups.sum(),\n",
    "                'note': 'Same date and symbol, different prices (corrections?)'\n",
    "            }\n",
    "        \n",
    "        self.duplicate_analysis = analysis\n",
    "        return analysis\n",
    "    \n",
    "    def _analyze_index_duplicates(self) -> List[Dict]:\n",
    "        \"\"\"Analyze groups of rows with same index.\"\"\"\n",
    "        grouped = self.df.groupby(self.df.index)\n",
    "        conflicts = []\n",
    "        \n",
    "        for idx, group in grouped:\n",
    "            if len(group) > 1:\n",
    "                # Check if values are identical or different\n",
    "                is_identical = group.duplicated().any()\n",
    "                conflicts.append({\n",
    "                    'index': idx,\n",
    "                    'count': len(group),\n",
    "                    'is_identical': is_identical,\n",
    "                    'variance': group.select_dtypes(include=[np.number]).var().mean() \n",
    "                               if not is_identical else 0\n",
    "                })\n",
    "        \n",
    "        return conflicts\n",
    "    \n",
    "    def remove_exact_duplicates(self, keep: str = 'first') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Remove rows where all values are identical.\n",
    "        \n",
    "        keep='first': Keep first occurrence\n",
    "        keep='last': Keep last occurrence (often more recent/corrected)\n",
    "        keep=False: Remove all duplicates (investigate manually)\n",
    "        \"\"\"\n",
    "        before_count = len(self.df)\n",
    "        cleaned = self.df.drop_duplicates(keep=keep)\n",
    "        removed = before_count - len(cleaned)\n",
    "        \n",
    "        print(f\"Removed {removed} exact duplicate rows\")\n",
    "        print(f\"Kept '{keep}' occurrence of each duplicate\")\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def handle_index_duplicates(self, strategy: str = 'keep_last') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Handle rows with same timestamp (index).\n",
    "        \n",
    "        Strategies:\n",
    "        - 'keep_first': Keep earliest record\n",
    "        - 'keep_last': Keep latest record (often corrected data)\n",
    "        - 'average': Average numeric values (for intraday aggregations)\n",
    "        - 'flag': Keep all but add flag column\n",
    "        \"\"\"\n",
    "        if strategy == 'keep_first':\n",
    "            return self.df[~self.df.index.duplicated(keep='first')]\n",
    "        \n",
    "        elif strategy == 'keep_last':\n",
    "            return self.df[~self.df.index.duplicated(keep='last')]\n",
    "        \n",
    "        elif strategy == 'average':\n",
    "            # Group by index and average numeric columns\n",
    "            # For financial data, this might be appropriate for intraday data\n",
    "            # where you want daily averages\n",
    "            \n",
    "            numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "            \n",
    "            # Aggregate: mean for numeric, first for others\n",
    "            agg_dict = {}\n",
    "            for col in self.df.columns:\n",
    "                if col in numeric_cols:\n",
    "                    agg_dict[col] = 'mean'\n",
    "                else:\n",
    "                    agg_dict[col] = 'first'\n",
    "            \n",
    "            return self.df.groupby(self.df.index).agg(agg_dict)\n",
    "        \n",
    "        elif strategy == 'flag':\n",
    "            # Keep all rows but add a flag column indicating duplicates\n",
    "            self.df['is_duplicate'] = self.df.index.duplicated(keep=False)\n",
    "            self.df['duplicate_count'] = self.df.groupby(self.df.index).transform('size')\n",
    "            return self.df\n",
    "        \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown strategy: {strategy}\")\n",
    "\n",
    "# Continue with the NEPSE example\n",
    "# First, let's create data with specific duplicate issues\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2024-01-01', periods=10, freq='B')\n",
    "\n",
    "# Create base NEPSE data\n",
    "nepse_base = pd.DataFrame({\n",
    "    'Symbol': 'NABIL',\n",
    "    'Open': np.random.uniform(2800, 2900, 10),\n",
    "    'High': np.random.uniform(2850, 2950, 10),\n",
    "    'Low': np.random.uniform(2750, 2850, 10),\n",
    "    'Close': np.random.uniform(2800, 2900, 10),\n",
    "    'Volume': np.random.randint(100000, 200000, 10)\n",
    "}, index=dates)\n",
    "\n",
    "# Introduce exact duplicates (data entry error)\n",
    "nepse_with_dups = pd.concat([nepse_base, nepse_base.iloc[0:2]])\n",
    "\n",
    "# Introduce index duplicates with different values (price correction scenario)\n",
    "correction_row = pd.DataFrame({\n",
    "    'Symbol': 'NABIL',\n",
    "    'Open': 2850.00,  # Corrected from original\n",
    "    'High': 2900.00,\n",
    "    'Low': 2840.00,\n",
    "    'Close': 2880.00,  # Different from original\n",
    "    'Volume': 150000\n",
    "}, index=[dates[2]])  # Same date as row 2\n",
    "\n",
    "nepse_with_dups = pd.concat([nepse_with_dups, correction_row])\n",
    "nepse_with_dups.sort_index(inplace=True)\n",
    "\n",
    "print(\"Data with duplicates:\")\n",
    "print(nepse_with_dups[nepse_with_dups.index.duplicated(keep=False)])\n",
    "\n",
    "# Analyze duplicates\n",
    "dup_handler = DuplicateHandler(nepse_with_dups)\n",
    "analysis = dup_handler.analyze_duplicates()\n",
    "\n",
    "print(f\"\\nDuplicate Analysis:\")\n",
    "print(f\"Exact duplicates: {analysis['exact_duplicates']['count']}\")\n",
    "print(f\"Index duplicates: {analysis['index_duplicates']['count']}\")\n",
    "\n",
    "# Handle exact duplicates\n",
    "cleaned_exact = dup_handler.remove_exact_duplicates(keep='last')\n",
    "print(f\"\\nAfter removing exact duplicates: {len(cleaned_exact)} rows\")\n",
    "\n",
    "# Handle index duplicates (keep last - assumes corrections are later)\n",
    "dup_handler2 = DuplicateHandler(cleaned_exact)\n",
    "final_clean = dup_handler2.handle_index_duplicates(strategy='keep_last')\n",
    "print(f\"After handling index duplicates: {len(final_clean)} rows\")\n",
    "print(\"\\nFinal cleaned data:\")\n",
    "print(final_clean.head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Exact duplicates** occur when the same row is accidentally inserted twice. This often happens during data collection retries or when merging datasets. We remove these while keeping either the first or last occurrence.\n",
    "- **Index duplicates** (same timestamp) are more complex in financial data. They might represent:\n",
    "  - **Corrections**: The exchange issued a correction for a price\n",
    "  - **Auction trades**: Separate auction session on the same day\n",
    "  - **Errors**: Data collection glitches\n",
    "- **Strategy selection**:\n",
    "  - `keep_last`: Assumes later data is more accurate (common for corrections)\n",
    "  - `average`: Useful for intraday data aggregated to daily level\n",
    "  - `flag`: When you want to preserve all data but mark potential issues\n",
    "- **For NEPSE specifically**: If you see the same date twice with different prices, it's likely a correction was issued by the exchange. The later record is usually the corrected one.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.3 Inconsistent Data Handling**\n",
    "\n",
    "Inconsistent data includes mixed formats, varying units, or categorical values that should be standardized (e.g., \"NABIL\" vs \"nabil\" vs \"Nabil Bank\").\n",
    "\n",
    "```python\n",
    "class InconsistencyHandler:\n",
    "    \"\"\"\n",
    "    Handle data inconsistencies common in NEPSE and financial datasets.\n",
    "    \n",
    "    Common inconsistencies:\n",
    "    - Symbol naming variations (NABIL vs Nabil)\n",
    "    - Date format variations (DD/MM/YYYY vs YYYY-MM-DD)\n",
    "    - Mixed units (Volume in thousands vs actual shares)\n",
    "    - Whitespace and special characters\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.changes_log = []\n",
    "    \n",
    "    def standardize_symbols(self, column: str = 'Symbol') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Standardize stock symbols to uppercase and remove whitespace.\n",
    "        \n",
    "        NEPSE symbols should be consistent: NABIL, NICA, SCBL, etc.\n",
    "        \"\"\"\n",
    "        if column not in self.df.columns:\n",
    "            return self.df\n",
    "        \n",
    "        original_unique = self.df[column].nunique()\n",
    "        \n",
    "        # Standardize: uppercase, strip whitespace, remove special chars\n",
    "        self.df[column] = (self.df[column]\n",
    "                          .astype(str)\n",
    "                          .str.upper()\n",
    "                          .str.strip()\n",
    "                          .str.replace(r'[^\\w]', '', regex=True))\n",
    "        \n",
    "        new_unique = self.df[column].nunique()\n",
    "        \n",
    "        if original_unique != new_unique:\n",
    "            self.changes_log.append(\n",
    "                f\"Symbol standardization: {original_unique} -> {new_unique} unique values\"\n",
    "            )\n",
    "            print(f\"Warning: Symbol consolidation reduced {original_unique} to {new_unique} unique symbols\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def standardize_dates(self, date_column: str = 'Date', \n",
    "                         format: Optional[str] = None) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert various date formats to standard datetime.\n",
    "        \n",
    "        Handles:\n",
    "        - Different separators (/, -, .)\n",
    "        - Different orders (DD/MM/YYYY vs MM/DD/YYYY)\n",
    "        - String vs datetime types\n",
    "        \"\"\"\n",
    "        if date_column not in self.df.columns:\n",
    "            return self.df\n",
    "        \n",
    "        # Convert to datetime with flexible parsing\n",
    "        if format:\n",
    "            self.df[date_column] = pd.to_datetime(self.df[date_column], format=format)\n",
    "        else:\n",
    "            self.df[date_column] = pd.to_datetime(self.df[date_column], infer_datetime_format=True)\n",
    "        \n",
    "        # Ensure business day frequency consistency\n",
    "        self.df[date_column] = pd.to_datetime(self.df[date_column]).dt.normalize()\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def normalize_units(self, volume_col: str = 'Volume', \n",
    "                       turnover_col: str = 'Turnover',\n",
    "                       unit_hint: str = 'actual') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normalize volume and turnover to consistent units.\n",
    "        \n",
    "        Some sources report volume in thousands or millions.\n",
    "        This standardizes everything to actual shares and NPR.\n",
    "        \"\"\"\n",
    "        if volume_col in self.df.columns:\n",
    "            # Detect if volume is in thousands (if max < 1000 for a liquid stock)\n",
    "            max_vol = self.df[volume_col].max()\n",
    "            \n",
    "            if max_vol < 10000 and unit_hint == 'auto':\n",
    "                # Likely in thousands\n",
    "                self.df[volume_col] = self.df[volume_col] * 1000\n",
    "                self.changes_log.append(f\"Converted {volume_col} from thousands to actual\")\n",
    "                print(f\"Converted {volume_col} from thousands to actual (detected small values)\")\n",
    "            elif unit_hint == 'thousands':\n",
    "                self.df[volume_col] = self.df[volume_col] * 1000\n",
    "                self.changes_log.append(f\"Converted {volume_col} from thousands to actual\")\n",
    "        \n",
    "        if turnover_col in self.df.columns:\n",
    "            # Turnover should be price * volume\n",
    "            # If values are too small, might be in millions or thousands\n",
    "            sample_calc = (self.df['Close'] * self.df[volume_col]).iloc[0] if 'Close' in self.df.columns else 0\n",
    "            actual_turnover = self.df[turnover_col].iloc[0] if len(self.df) > 0 else 0\n",
    "            \n",
    "            if actual_turnover > 0 and sample_calc / actual_turnover > 1000:\n",
    "                # Turnover seems to be in millions\n",
    "                self.df[turnover_col] = self.df[turnover_col] * 1000000\n",
    "                self.changes_log.append(f\"Converted {turnover_col} from millions to actual\")\n",
    "                print(f\"Converted {turnover_col} from millions to actual\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def remove_whitespace(self, columns: Optional[List[str]] = None) -> pd.DataFrame:\n",
    "        \"\"\"Remove leading/trailing whitespace from string columns.\"\"\"\n",
    "        if columns is None:\n",
    "            columns = self.df.select_dtypes(include=['object']).columns\n",
    "        \n",
    "        for col in columns:\n",
    "            if self.df[col].dtype == 'object':\n",
    "                self.df[col] = self.df[col].str.strip()\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "# Usage with NEPSE data\n",
    "# Create inconsistent data\n",
    "inconsistent_data = pd.DataFrame({\n",
    "    'Symbol': ['nabil', 'NABIL', 'Nabil ', 'nica', 'NICA'],\n",
    "    'Date': ['15/01/2024', '2024-01-16', '17-01-2024', '01/18/2024', '2024-01-19'],\n",
    "    'Volume': [125, 150, 175, 140, 160],  # In thousands?\n",
    "    'Turnover': [359062.5, 433200, 507237.5, 403630, 465280],  # In thousands?\n",
    "    'Close': [2875.25, 2895.50, 2900.00, 2880.50, 2905.00]\n",
    "})\n",
    "\n",
    "print(\"Original inconsistent data:\")\n",
    "print(inconsistent_data)\n",
    "print(f\"\\nUnique symbols (should be 2): {inconsistent_data['Symbol'].nunique()}\")\n",
    "\n",
    "# Fix inconsistencies\n",
    "consistency_handler = InconsistencyHandler(inconsistent_data)\n",
    "cleaned = (consistency_handler\n",
    "           .standardize_symbols('Symbol')\n",
    "           .standardize_dates('Date')\n",
    "           .normalize_units('Volume', 'Turnover', unit_hint='thousands')\n",
    "           .remove_whitespace())\n",
    "\n",
    "print(f\"\\nCleaned data:\")\n",
    "print(cleaned)\n",
    "print(f\"\\nUnique symbols after cleaning: {cleaned['Symbol'].nunique()}\")\n",
    "print(f\"Volume now in actual shares: {cleaned['Volume'].iloc[0]:,.0f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Symbol standardization** ensures that \"nabil\", \"NABIL\", and \"Nabil \" are treated as the same stock. This is crucial because string matching is case-sensitive and whitespace-sensitive.\n",
    "- **Date parsing** handles the common issue where different data sources use different date formats. NEPSE official data might use Nepali calendar or different separators than international sources.\n",
    "- **Unit normalization** is critical for financial data:\n",
    "  - Some APIs return volume in thousands (125 instead of 125,000)\n",
    "  - Turnover might be in millions or lakhs\n",
    "  - We detect this by comparing calculated turnover (Close \u00d7 Volume) with reported turnover\n",
    "- **Whitespace removal** prevents issues where \"NABIL \" (with space) doesn't match \"NABIL\" in lookup tables.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.4 Data Type Standardization**\n",
    "\n",
    "Ensuring each column has the correct data type saves memory and prevents calculation errors.\n",
    "\n",
    "```python\n",
    "class DataTypeStandardizer:\n",
    "    \"\"\"\n",
    "    Standardize data types for optimal memory usage and calculation accuracy.\n",
    "    \n",
    "    Financial data types:\n",
    "    - Prices: float32 (sufficient precision, half the memory of float64)\n",
    "    - Volumes: int32 or int64 (depending on market size)\n",
    "    - Dates: datetime64[ns]\n",
    "    - Symbols: category (repeated strings)\n",
    "    - Flags: int8 or boolean\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.type_changes = []\n",
    "    \n",
    "    def optimize_numeric_types(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Downcast numeric types to save memory without losing precision.\n",
    "        \n",
    "        For NEPSE:\n",
    "        - Prices: float32 (7 decimal digits precision, sufficient for NPR)\n",
    "        - Volume: int32 (max ~2 billion, sufficient for NEPSE daily volume)\n",
    "        - Turnover: int64 (can be large: 2 billion shares * 3000 NPR = 6 trillion)\n",
    "        \"\"\"\n",
    "        # Prices: float32 is sufficient (precise to ~0.01 NPR for values up to 10,000)\n",
    "        price_cols = ['Open', 'High', 'Low', 'Close', 'VWAP', 'Prev_Close', 'LTP']\n",
    "        for col in price_cols:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "                self.df[col] = self.df[col].astype('float32')\n",
    "        \n",
    "        # Volume: int32 for most cases, int64 if values exceed 2 billion\n",
    "        if 'Volume' in self.df.columns:\n",
    "            max_vol = self.df['Volume'].max()\n",
    "            if max_vol < 2_147_483_647:  # Max int32\n",
    "                self.df['Volume'] = self.df['Volume'].astype('int32')\n",
    "            else:\n",
    "                self.df['Volume'] = self.df['Volume'].astype('int64')\n",
    "        \n",
    "        # Turnover: usually needs int64\n",
    "        if 'Turnover' in self.df.columns:\n",
    "            self.df['Turnover'] = self.df['Turnover'].astype('int64')\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def categorize_symbols(self, symbol_col: str = 'Symbol') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert symbol column to category type for memory efficiency.\n",
    "        \n",
    "        If you have 1000 days of data for 100 stocks, that's 100,000 rows\n",
    "        but only 100 unique symbols. Category stores integers internally.\n",
    "        \"\"\"\n",
    "        if symbol_col in self.df.columns:\n",
    "            # Calculate memory before\n",
    "            mem_before = self.df[symbol_col].memory_usage(deep=True)\n",
    "            \n",
    "            self.df[symbol_col] = self.df[symbol_col].astype('category')\n",
    "            \n",
    "            mem_after = self.df[symbol_col].memory_usage(deep=True)\n",
    "            savings = (1 - mem_after/mem_before) * 100\n",
    "            \n",
    "            print(f\"Symbol column memory reduced by {savings:.1f}% using category type\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def standardize_datetime(self, date_col: str = 'Date', \n",
    "                           set_index: bool = False) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Ensure proper datetime type and optionally set as index.\n",
    "        \"\"\"\n",
    "        if date_col in self.df.columns:\n",
    "            self.df[date_col] = pd.to_datetime(self.df[date_col])\n",
    "            \n",
    "            if set_index:\n",
    "                self.df.set_index(date_col, inplace=True)\n",
    "                self.df.sort_index(inplace=True)\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def convert_boolean_flags(self, columns: List[str]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Convert string representations of boolean to actual boolean type.\n",
    "        \n",
    "        Handles: 'Yes'/'No', 'True'/'False', 1/0, '1'/'0'\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            if col in self.df.columns:\n",
    "                # Handle various representations\n",
    "                if self.df[col].dtype == 'object':\n",
    "                    self.df[col] = self.df[col].str.lower().map({\n",
    "                        'true': True, 'false': False,\n",
    "                        'yes': True, 'no': False,\n",
    "                        '1': True, '0': False,\n",
    "                        't': True, 'f': False\n",
    "                    }).fillna(self.df[col]).astype('boolean')\n",
    "                else:\n",
    "                    self.df[col] = self.df[col].astype('boolean')\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def get_memory_report(self) -> pd.DataFrame:\n",
    "        \"\"\"Generate memory usage report by column.\"\"\"\n",
    "        usage = self.df.memory_usage(deep=True)\n",
    "        usage_mb = usage / 1024 / 1024\n",
    "        \n",
    "        report = pd.DataFrame({\n",
    "            'Column': usage.index,\n",
    "            'Bytes': usage.values,\n",
    "            'MB': usage_mb.values,\n",
    "            'Dtype': [str(self.df[col].dtype) if col in self.df.columns else 'N/A' \n",
    "                     for col in usage.index]\n",
    "        })\n",
    "        \n",
    "        return report.sort_values('Bytes', ascending=False)\n",
    "\n",
    "# Usage with NEPSE data\n",
    "# Create sample with suboptimal types\n",
    "nepse_types = pd.DataFrame({\n",
    "    'Symbol': ['NABIL'] * 1000,  # Will benefit from category\n",
    "    'Date': pd.date_range('2020-01-01', periods=1000, freq='B'),\n",
    "    'Open': np.random.uniform(2800, 3000, 1000).astype('float64'),  # Overkill precision\n",
    "    'High': np.random.uniform(2800, 3000, 1000).astype('float64'),\n",
    "    'Low': np.random.uniform(2800, 3000, 1000).astype('float64'),\n",
    "    'Close': np.random.uniform(2800, 3000, 1000).astype('float64'),\n",
    "    'Volume': np.random.randint(100000, 200000, 1000).astype('int64'),  # Could be int32\n",
    "    'Is_Bullish': ['Yes', 'No'] * 500  # Should be boolean\n",
    "})\n",
    "\n",
    "print(\"Before optimization:\")\n",
    "print(nepse_types.dtypes)\n",
    "print(f\"\\nMemory usage: {nepse_types.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "\n",
    "# Optimize\n",
    "standardizer = DataTypeStandardizer(nepse_types)\n",
    "optimized = (standardizer\n",
    "             .optimize_numeric_types()\n",
    "             .categorize_symbols('Symbol')\n",
    "             .standardize_datetime('Date', set_index=True)\n",
    "             .convert_boolean_flags(['Is_Bullish']))\n",
    "\n",
    "print(\"\\nAfter optimization:\")\n",
    "print(optimized.dtypes)\n",
    "print(f\"\\nMemory usage: {optimized.memory_usage(deep=True).sum() / 1024:.2f} KB\")\n",
    "print(f\"\\nMemory savings: {(1 - optimized.memory_usage().sum() / nepse_types.memory_usage().sum()) * 100:.1f}%\")\n",
    "\n",
    "print(\"\\nDetailed memory report:\")\n",
    "print(standardizer.get_memory_report().head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Memory optimization** is crucial when dealing with years of tick data or hundreds of stocks.\n",
    "- **float32 vs float64**: \n",
    "  - float32: 7 decimal digits precision, 4 bytes\n",
    "  - float64: 15 decimal digits precision, 8 bytes\n",
    "  - For stock prices under 10,000 NPR, float32 gives precision to 0.01 NPR, which is sufficient\n",
    "- **int32 vs int64**:\n",
    "  - int32 max: ~2.1 billion (sufficient for NEPSE daily volume)\n",
    "  - int64 max: ~9 quintillion (needed for turnover calculations)\n",
    "- **Category type**: \n",
    "  - Stores repeated strings as integers with a lookup table\n",
    "  - For 1000 rows of \"NABIL\", stores [0,0,0...] + mapping {0: \"NABIL\"}\n",
    "  - Reduces memory from ~8KB to ~4KB (integer array + small mapping)\n",
    "- **Boolean type**: Proper boolean uses 1 byte vs object (string) which uses 50+ bytes per entry.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.5 Missing Data Patterns**\n",
    "\n",
    "Understanding why data is missing is as important as the missing data itself. The mechanism of missingness determines the appropriate handling strategy.\n",
    "\n",
    "```python\n",
    "class MissingDataAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze patterns of missing data to determine appropriate handling strategies.\n",
    "    \n",
    "    Types of missingness:\n",
    "    1. MCAR (Missing Completely At Random): No systematic reason\n",
    "    2. MAR (Missing At Random): Missingness related to observed data\n",
    "    3. MNAR (Missing Not At Random): Missingness related to the missing value itself\n",
    "    \n",
    "    For NEPSE:\n",
    "    - MCAR: Data collection system glitch\n",
    "    - MAR: Small cap stocks missing volume on low liquidity days (observable)\n",
    "    - MNAR: Delisted stocks missing data because company performed poorly\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.missing_report = {}\n",
    "    \n",
    "    def calculate_missing_statistics(self) -> pd.DataFrame:\n",
    "        \"\"\"Calculate comprehensive missing data statistics.\"\"\"\n",
    "        missing_stats = pd.DataFrame({\n",
    "            'Column': self.df.columns,\n",
    "            'Missing_Count': self.df.isnull().sum(),\n",
    "            'Missing_Percent': (self.df.isnull().sum() / len(self.df)) * 100,\n",
    "            'Data_Type': self.df.dtypes.values\n",
    "        })\n",
    "        \n",
    "        missing_stats = missing_stats[missing_stats['Missing_Count'] > 0]\n",
    "        missing_stats = missing_stats.sort_values('Missing_Percent', ascending=False)\n",
    "        \n",
    "        return missing_stats\n",
    "    \n",
    "    def analyze_temporal_patterns(self, date_col: str = 'Date') -> Dict:\n",
    "        \"\"\"\n",
    "        Analyze if missing data follows temporal patterns.\n",
    "        \n",
    "        Checks:\n",
    "        - Day of week patterns (weekends, holidays)\n",
    "        - Month-end effects\n",
    "        - Trend over time (getting worse/better?)\n",
    "        \"\"\"\n",
    "        if date_col not in self.df.columns:\n",
    "            return {}\n",
    "        \n",
    "        df_time = self.df.copy()\n",
    "        df_time['DayOfWeek'] = pd.to_datetime(df_time[date_col]).dt.dayofweek\n",
    "        df_time['Month'] = pd.to_datetime(df_time[date_col]).dt.month\n",
    "        df_time['Year'] = pd.to_datetime(df_time[date_col]).dt.year\n",
    "        \n",
    "        patterns = {}\n",
    "        \n",
    "        # By day of week (0=Monday, 6=Sunday)\n",
    "        dow_missing = df_time.groupby('DayOfWeek').apply(lambda x: x.isnull().sum().sum())\n",
    "        patterns['day_of_week'] = dow_missing.to_dict()\n",
    "        \n",
    "        # By month\n",
    "        month_missing = df_time.groupby('Month').apply(lambda x: x.isnull().sum().sum())\n",
    "        patterns['month'] = month_missing.to_dict()\n",
    "        \n",
    "        # Trend over time (by year)\n",
    "        year_missing = df_time.groupby('Year').apply(lambda x: x.isnull().mean().mean() * 100)\n",
    "        patterns['yearly_trend'] = year_missing.to_dict()\n",
    "        \n",
    "        return patterns\n",
    "    \n",
    "    def analyze_correlation_with_missingness(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Check if missingness in one column correlates with values in another.\n",
    "        \n",
    "        This helps determine if data is MAR (Missing At Random).\n",
    "        \"\"\"\n",
    "        # Create missingness indicators (1 if missing, 0 if not)\n",
    "        missing_indicators = self.df.isnull().astype(int)\n",
    "        \n",
    "        # Add suffix to distinguish from original data\n",
    "        missing_indicators.columns = [f\"{col}_missing\" for col in missing_indicators.columns]\n",
    "        \n",
    "        # Combine with numeric data\n",
    "        numeric_data = self.df.select_dtypes(include=[np.number])\n",
    "        combined = pd.concat([numeric_data, missing_indicators], axis=1)\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = combined.corr()\n",
    "        \n",
    "        # Extract correlations between data and missing indicators\n",
    "        missing_corr = pd.DataFrame()\n",
    "        for missing_col in missing_indicators.columns:\n",
    "            orig_col = missing_col.replace('_missing', '')\n",
    "            if orig_col in numeric_data.columns:\n",
    "                correlations = corr_matrix[missing_col].drop(missing_indicators.columns)\n",
    "                missing_corr[missing_col] = correlations\n",
    "        \n",
    "        return missing_corr\n",
    "    \n",
    "    def detect_mcar_little_test(self) -> Dict:\n",
    "        \"\"\"\n",
    "        Little's MCAR test (simplified version).\n",
    "        \n",
    "        H0: Data is Missing Completely At Random\n",
    "        If p-value < 0.05, reject H0 (data is not MCAR)\n",
    "        \n",
    "        Note: Full implementation requires complex statistical calculations.\n",
    "        This is a heuristic check.\n",
    "        \"\"\"\n",
    "        # Simplified check: If missingness correlates with observed values, not MCAR\n",
    "        corr_analysis = self.analyze_correlation_with_missingness()\n",
    "        \n",
    "        strong_correlations = (corr_analysis.abs() > 0.3).sum().sum()\n",
    "        \n",
    "        if strong_correlations > 0:\n",
    "            return {\n",
    "                'mcar_likely': False,\n",
    "                'reason': f'Found {strong_correlations} strong correlations between missingness and observed values',\n",
    "                'recommendation': 'Use MAR-appropriate methods (conditional imputation)'\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                'mcar_likely': True,\n",
    "                'reason': 'No strong correlations found between missingness and observed values',\n",
    "                'recommendation': 'Simple imputation methods may be adequate'\n",
    "            }\n",
    "    \n",
    "    def visualize_missing_pattern(self):\n",
    "        \"\"\"Create visualization of missing data pattern.\"\"\"\n",
    "        import matplotlib.pyplot as plt\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        \n",
    "        # Missingness heatmap\n",
    "        plt.subplot(1, 2, 1)\n",
    "        missing_matrix = self.df.isnull().astype(int)\n",
    "        plt.imshow(missing_matrix.T, cmap='viridis', aspect='auto')\n",
    "        plt.yticks(range(len(self.df.columns)), self.df.columns)\n",
    "        plt.xlabel('Time Index')\n",
    "        plt.title('Missing Data Pattern (Yellow = Missing)')\n",
    "        plt.colorbar(label='Missing (1) / Present (0)')\n",
    "        \n",
    "        # Missingness by column\n",
    "        plt.subplot(1, 2, 2)\n",
    "        missing_counts = self.df.isnull().sum().sort_values(ascending=True)\n",
    "        missing_counts[missing_counts > 0].plot(kind='barh')\n",
    "        plt.xlabel('Count of Missing Values')\n",
    "        plt.title('Missing Values by Column')\n",
    "        plt.tight_layout()\n",
    "        \n",
    "        plt.savefig('missing_data_pattern.png', dpi=150)\n",
    "        plt.close()\n",
    "        print(\"Missing data pattern visualization saved\")\n",
    "\n",
    "# Usage with NEPSE data\n",
    "# Create data with different missing patterns\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='B')\n",
    "np.random.seed(42)\n",
    "\n",
    "nepse_missing = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Symbol': 'NABIL',\n",
    "    'Open': np.where(np.random.random(100) > 0.9, np.nan, np.random.uniform(2800, 3000, 100)),\n",
    "    'High': np.where(np.random.random(100) > 0.95, np.nan, np.random.uniform(2800, 3000, 100)),\n",
    "    'Low': np.where(np.random.random(100) > 0.95, np.nan, np.random.uniform(2800, 3000, 100)),\n",
    "    'Close': np.where(np.random.random(100) > 0.9, np.nan, np.random.uniform(2800, 3000, 100)),\n",
    "    'Volume': np.where(np.random.random(100) > 0.8, np.nan, np.random.randint(100000, 200000, 100))\n",
    "})\n",
    "\n",
    "# Add systematic missingness (MAR): Low volume days more likely to have missing prices\n",
    "low_volume_mask = nepse_missing['Volume'] < 120000\n",
    "nepse_missing.loc[low_volume_mask, 'Close'] = np.nan\n",
    "\n",
    "print(\"Missing Data Analysis:\")\n",
    "analyzer = MissingDataAnalyzer(nepse_missing)\n",
    "\n",
    "# Basic statistics\n",
    "stats = analyzer.calculate_missing_statistics()\n",
    "print(\"\\nMissing Statistics:\")\n",
    "print(stats)\n",
    "\n",
    "# Temporal patterns\n",
    "patterns = analyzer.analyze_temporal_patterns()\n",
    "print(f\"\\nTemporal Patterns:\")\n",
    "print(f\"Missing by day of week: {patterns.get('day_of_week', {})}\")\n",
    "\n",
    "# MCAR test\n",
    "mcar_result = analyzer.detect_mcar_little_test()\n",
    "print(f\"\\nMCAR Test Result:\")\n",
    "print(f\"Likely MCAR: {mcar_result['mcar_likely']}\")\n",
    "print(f\"Reason: {mcar_result['reason']}\")\n",
    "print(f\"Recommendation: {mcar_result['recommendation']}\")\n",
    "\n",
    "# Visualize\n",
    "analyzer.visualize_missing_pattern()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **MCAR (Missing Completely At Random)**: The probability of missingness is the same for all observations. Example: A random data transmission error.\n",
    "- **MAR (Missing At Random)**: Missingness depends on observed data but not the missing value itself. Example: Small-cap stocks (observable characteristic) are more likely to have missing volume data.\n",
    "- **MNAR (Missing Not At Random)**: Missingness depends on the missing value itself. Example: A company delists (extremely low stock price) and data stops appearing.\n",
    "- **Why this matters**:\n",
    "  - MCAR: Simple imputation (mean, median) is unbiased\n",
    "  - MAR: Use conditional imputation based on observed values\n",
    "  - MNAR: Requires specialized models or the missingness itself is information\n",
    "- **Little's MCAR test**: Statistical test to check if data is MCAR. If rejected, you need more sophisticated methods than simple mean imputation.\n",
    "- **Temporal patterns**: Missing data on weekends is expected (markets closed). Missing data on Tuesdays might indicate a data collection issue.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.6 Advanced Imputation Techniques**\n",
    "\n",
    "When simple methods (mean, forward-fill) are insufficient, advanced techniques use relationships between variables and patterns in the data.\n",
    "\n",
    "```python\n",
    "from sklearn.impute import KNNImputer, IterativeImputer\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class AdvancedImputation:\n",
    "    \"\"\"\n",
    "    Advanced imputation methods for time-series financial data.\n",
    "    \n",
    "    Methods:\n",
    "    1. KNN Imputation: Use similar days to fill missing values\n",
    "    2. Iterative Imputation (MICE): Model each feature as function of others\n",
    "    3. Interpolation with constraints: Respect OHLC relationships\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.imputed_columns = []\n",
    "    \n",
    "    def knn_imputation(self, n_neighbors: int = 5) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        K-Nearest Neighbors imputation.\n",
    "        \n",
    "        For a row with missing values, find k most similar rows (based on\n",
    "        non-missing features) and use their average.\n",
    "        \n",
    "        Good for: Cross-sectional data where similar stocks/days exist\n",
    "        \"\"\"\n",
    "        # Select numeric columns\n",
    "        numeric_df = self.df.select_dtypes(include=[np.number])\n",
    "        \n",
    "        # Standardize for distance calculation\n",
    "        scaler = StandardScaler()\n",
    "        scaled_data = scaler.fit_transform(numeric_df)\n",
    "        \n",
    "        # Apply KNN imputation\n",
    "        imputer = KNNImputer(n_neighbors=n_neighbors, weights='distance')\n",
    "        imputed_scaled = imputer.fit_transform(scaled_data)\n",
    "        \n",
    "        # Inverse transform\n",
    "        imputed_data = scaler.inverse_transform(imputed_scaled)\n",
    "        \n",
    "        # Update dataframe\n",
    "        imputed_df = pd.DataFrame(imputed_data, \n",
    "                                 columns=numeric_df.columns, \n",
    "                                 index=numeric_df.index)\n",
    "        \n",
    "        # Only update missing values, preserve original where present\n",
    "        for col in numeric_df.columns:\n",
    "            mask = self.df[col].isnull()\n",
    "            if mask.any():\n",
    "                self.df.loc[mask, col] = imputed_df.loc[mask, col]\n",
    "                self.imputed_columns.append(f\"{col} (KNN)\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def iterative_imputation(self, max_iter: int = 10) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Multiple Imputation by Chained Equations (MICE).\n",
    "        \n",
    "        Models each feature with missing values as a function of other features.\n",
    "        Iterates until convergence.\n",
    "        \n",
    "        Good for: Data with strong correlations between features (OHLC)\n",
    "        \"\"\"\n",
    "        numeric_df = self.df.select_dtypes(include=[np.number])\n",
    "        \n",
    "        imputer = IterativeImputer(max_iter=max_iter, random_state=42)\n",
    "        imputed_data = imputer.fit_transform(numeric_df)\n",
    "        \n",
    "        imputed_df = pd.DataFrame(imputed_data,\n",
    "                                 columns=numeric_df.columns,\n",
    "                                 index=numeric_df.index)\n",
    "        \n",
    "        # Update only missing values\n",
    "        for col in numeric_df.columns:\n",
    "            mask = self.df[col].isnull()\n",
    "            if mask.any():\n",
    "                self.df.loc[mask, col] = imputed_df.loc[mask, col]\n",
    "                self.imputed_columns.append(f\"{col} (Iterative)\")\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def constrained_ohlc_imputation(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Impute OHLC data while maintaining financial constraints.\n",
    "        \n",
    "        Ensures after imputation:\n",
    "        - High >= max(Open, Close, Low)\n",
    "        - Low <= min(Open, Close, High)\n",
    "        \"\"\"\n",
    "        # First, forward fill for continuity\n",
    "        ohlc_cols = ['Open', 'High', 'Low', 'Close']\n",
    "        available_cols = [c for c in ohlc_cols if c in self.df.columns]\n",
    "        \n",
    "        # Initial fill with interpolation\n",
    "        for col in available_cols:\n",
    "            self.df[col] = self.df[col].interpolate(method='linear')\n",
    "        \n",
    "        # Enforce constraints\n",
    "        if all(c in self.df.columns for c in ['High', 'Low', 'Close', 'Open']):\n",
    "            # High should be >= Open, Close, Low\n",
    "            self.df['High'] = self.df[['High', 'Open', 'Close', 'Low']].max(axis=1)\n",
    "            \n",
    "            # Low should be <= Open, Close, High\n",
    "            self.df['Low'] = self.df[['Low', 'Open', 'Close', 'High']].min(axis=1)\n",
    "            \n",
    "            # If any are still missing, use close as reference\n",
    "            for col in ['Open', 'High', 'Low']:\n",
    "                mask = self.df[col].isnull()\n",
    "                if mask.any():\n",
    "                    self.df.loc[mask, col] = self.df.loc[mask, 'Close']\n",
    "        \n",
    "        return self.df\n",
    "    \n",
    "    def time_weighted_imputation(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Impute using time-weighted average (recent values weighted more).\n",
    "        \n",
    "        Unlike simple mean, this respects that recent prices are more relevant.\n",
    "        \"\"\"\n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            if self.df[col].isnull().any():\n",
    "                # Calculate exponential weighted moving average\n",
    "                ewma = self.df[col].ewm(span=10, adjust=False).mean()\n",
    "                \n",
    "                # Fill missing with EWMA\n",
    "                self.df[col] = self.df[col].fillna(ewma)\n",
    "                \n",
    "                # If still missing (at start of series), use forward/backward fill\n",
    "                self.df[col] = self.df[col].fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "# Usage example\n",
    "# Create NEPSE data with missing values\n",
    "dates = pd.date_range('2024-01-01', periods=20, freq='B')\n",
    "np.random.seed(42)\n",
    "\n",
    "nepse_impute = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Open': [2850.50, np.nan, 2890.00, 2865.75, np.nan, 2880.50, 2905.00, np.nan, 2900.00, 2920.00] * 2,\n",
    "    'High': [2890.00, 2910.00, np.nan, 2900.00, 2915.00, 2920.00, np.nan, 2925.00, 2930.00, 2940.00] * 2,\n",
    "    'Low': [2840.00, 2860.00, 2880.00, np.nan, 2870.00, 2875.00, 2890.00, 2900.00, np.nan, 2910.00] * 2,\n",
    "    'Close': [2875.25, 2895.50, 2900.00, 2880.50, np.nan, 2910.00, 2915.00, 2925.00, 2935.00, np.nan] * 2,\n",
    "    'Volume': [125000, 150000, np.nan, 140000, 160000, 180000, 155000, np.nan, 170000, 200000] * 2\n",
    "})\n",
    "\n",
    "print(\"Data with missing values:\")\n",
    "print(nepse_impute.isnull().sum())\n",
    "\n",
    "# Apply constrained imputation (best for OHLC data)\n",
    "imputer = AdvancedImputation(nepse_impute)\n",
    "imputed_data = imputer.constrained_ohlc_imputation()\n",
    "\n",
    "print(f\"\\nAfter constrained imputation:\")\n",
    "print(imputed_data.isnull().sum())\n",
    "print(f\"\\nConstraints maintained:\")\n",
    "print(f\"High >= Close: {(imputed_data['High'] >= imputed_data['Close']).all()}\")\n",
    "print(f\"Low <= Close: {(imputed_data['Low'] <= imputed_data['Close']).all()}\")\n",
    "\n",
    "# Show imputed values\n",
    "print(f\"\\nColumns imputed: {imputer.imputed_columns}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **KNN Imputation**: Finds the 5 most similar days (based on Volume, other prices) and averages their values. Good when you have cross-sectional data (multiple stocks) or features that correlate with the missing value.\n",
    "- **Iterative Imputation (MICE)**: Creates a model for each column using other columns as predictors. Iterates 10 times, each time using updated estimates. Excellent for OHLC data where Open, High, Low, Close are highly correlated.\n",
    "- **Constrained Imputation**: Financial data has rigid constraints (High \u2265 Low). Standard imputation might violate these. This method imputes first, then adjusts values to ensure High is actually the highest and Low is the lowest.\n",
    "- **Time-Weighted**: Uses Exponentially Weighted Moving Average (EWMA) which gives more weight to recent observations. This is more appropriate than simple mean for time-series where recent values are more relevant.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.7 Outlier Analysis and Treatment**\n",
    "\n",
    "Outliers in financial data can be errors (bad ticks) or genuine events (market crashes, earnings surprises). Distinguishing between them is crucial.\n",
    "\n",
    "```python\n",
    "class OutlierHandler:\n",
    "    \"\"\"\n",
    "    Comprehensive outlier handling for financial time-series.\n",
    "    \n",
    "    Approaches:\n",
    "    1. Statistical: Z-score, IQR (assumes normal distribution)\n",
    "    2. ML-based: Isolation Forest (unsupervised anomaly detection)\n",
    "    3. Domain-specific: Price limits, volatility checks\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.outlier_mask = pd.DataFrame(False, index=df.index, columns=df.columns)\n",
    "    \n",
    "    def statistical_outliers(self, column: str, method: str = 'iqr', \n",
    "                            threshold: float = 3.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect outliers using statistical methods.\n",
    "        \n",
    "        method: 'iqr' (Interquartile Range) or 'zscore'\n",
    "        threshold: For IQR, multiplier (1.5 = standard, 3.0 = extreme)\n",
    "                   For Z-score, number of standard deviations\n",
    "        \"\"\"\n",
    "        data = self.df[column].dropna()\n",
    "        \n",
    "        if method == 'iqr':\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            lower = Q1 - threshold * IQR\n",
    "            upper = Q3 + threshold * IQR\n",
    "            \n",
    "            outliers = (self.df[column] < lower) | (self.df[column] > upper)\n",
    "            \n",
    "        elif method == 'zscore':\n",
    "            mean = data.mean()\n",
    "            std = data.std()\n",
    "            z_scores = (self.df[column] - mean) / std\n",
    "            outliers = z_scores.abs() > threshold\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        self.outlier_mask[column] = outliers\n",
    "        return outliers\n",
    "    \n",
    "    def price_jump_outliers(self, price_col: str = 'Close', \n",
    "                           threshold_pct: float = 10.0) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect unusual price jumps (common in NEPSE during circuit breakers).\n",
    "        \n",
    "        threshold_pct: Percentage change considered unusual (e.g., 10%)\n",
    "        \"\"\"\n",
    "        returns = self.df[price_col].pct_change() * 100\n",
    "        outliers = returns.abs() > threshold_pct\n",
    "        \n",
    "        self.outlier_mask[f\"{price_col}_jump\"] = outliers\n",
    "        return outliers\n",
    "    \n",
    "    def ohlc_logic_outliers(self) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect rows where OHLC logic is violated (data errors).\n",
    "        \n",
    "        Valid: High >= Open, Close, Low and Low <= Open, Close, High\n",
    "        \"\"\"\n",
    "        violations = pd.Series(False, index=self.df.index)\n",
    "        \n",
    "        if all(c in self.df.columns for c in ['High', 'Low', 'Open', 'Close']):\n",
    "            # High should be maximum\n",
    "            high_violation = self.df['High'] < self.df[['Open', 'Close', 'Low']].max(axis=1)\n",
    "            \n",
    "            # Low should be minimum\n",
    "            low_violation = self.df['Low'] > self.df[['Open', 'Close', 'High']].min(axis=1)\n",
    "            \n",
    "            violations = high_violation | low_violation\n",
    "        \n",
    "        self.outlier_mask['ohlc_logic_error'] = violations\n",
    "        return violations\n",
    "    \n",
    "    def volume_outliers(self, volume_col: str = 'Volume', \n",
    "                       method: str = 'mad') -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect unusual volume spikes.\n",
    "        \n",
    "        method: 'mad' (Median Absolute Deviation) - more robust than IQR for volume\n",
    "        \"\"\"\n",
    "        data = self.df[volume_col].dropna()\n",
    "        \n",
    "        if method == 'mad':\n",
    "            median = data.median()\n",
    "            mad = (data - median).abs().median()\n",
    "            modified_z = 0.6745 * (self.df[volume_col] - median) / mad\n",
    "            outliers = modified_z.abs() > 3.5\n",
    "            \n",
    "        else:\n",
    "            # Use IQR method\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = (self.df[volume_col] < (Q1 - 1.5 * IQR)) | \\\n",
    "                      (self.df[volume_col] > (Q3 + 1.5 * IQR))\n",
    "        \n",
    "        self.outlier_mask[f\"{volume_col}_outlier\"] = outliers\n",
    "        return outliers\n",
    "    \n",
    "    def treat_outliers(self, column: str, method: str = 'clip', \n",
    "                      lower_quantile: float = 0.01,\n",
    "                      upper_quantile: float = 0.99) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Treat outliers using specified method.\n",
    "        \n",
    "        methods:\n",
    "        - 'clip': Cap at percentiles (Winsorization)\n",
    "        - 'remove': Delete rows with outliers\n",
    "        - 'transform': Apply log transformation\n",
    "        - 'flag': Keep but add indicator column\n",
    "        \"\"\"\n",
    "        if method == 'clip':\n",
    "            lower = self.df[column].quantile(lower_quantile)\n",
    "            upper = self.df[column].quantile(upper_quantile)\n",
    "            self.df[f\"{column}_original\"] = self.df[column]\n",
    "            self.df[column] = self.df[column].clip(lower, upper)\n",
    "            \n",
    "        elif method == 'remove':\n",
    "            mask = self.outlier_mask.get(column, pd.Series(False, index=self.df.index))\n",
    "            self.df = self.df[~mask]\n",
    "            \n",
    "        elif method == 'transform':\n",
    "            # Log transform (good for right-skewed data like volume)\n",
    "            self.df[f\"{column}_original\"] = self.df[column]\n",
    "            self.df[column] = np.log1p(self.df[column])\n",
    "            \n",
    "        elif method == 'flag':\n",
    "            mask = self.outlier_mask.get(column, pd.Series(False, index=self.df.index))\n",
    "            self.df[f\"{column}_is_outlier\"] = mask.astype(int)\n",
    "            \n",
    "        return self.df\n",
    "\n",
    "# Usage with NEPSE data\n",
    "# Create data with various outlier types\n",
    "dates = pd.date_range('2024-01-01', periods=50, freq='B')\n",
    "np.random.seed(42)\n",
    "\n",
    "nepse_outliers = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Open': np.random.uniform(2800, 3000, 50),\n",
    "    'High': np.random.uniform(2800, 3000, 50),\n",
    "    'Low': np.random.uniform(2800, 3000, 50),\n",
    "    'Close': np.random.uniform(2800, 3000, 50),\n",
    "    'Volume': np.random.randint(100000, 200000, 50)\n",
    "})\n",
    "\n",
    "# Add outliers\n",
    "nepse_outliers.loc[10, 'Close'] = 5000  # Price spike (error or news)\n",
    "nepse_outliers.loc[20, 'Volume'] = 2000000  # Volume spike\n",
    "nepse_outliers.loc[30, 'High'] = 2500  # Logic error (High < Low)\n",
    "\n",
    "# Ensure High/Low logic for non-outlier rows\n",
    "for i in range(len(nepse_outliers)):\n",
    "    if i != 30:  # Skip the outlier row\n",
    "        row = nepse_outliers.loc[i]\n",
    "        nepse_outliers.loc[i, 'High'] = max(row['High'], row['Open'], row['Close'], row['Low'])\n",
    "        nepse_outliers.loc[i, 'Low'] = min(row['Low'], row['Open'], row['Close'], row['High'])\n",
    "\n",
    "# Detect outliers\n",
    "handler = OutlierHandler(nepse_outliers)\n",
    "\n",
    "# Statistical outliers in Close price\n",
    "price_outliers = handler.statistical_outliers('Close', method='iqr', threshold=1.5)\n",
    "print(f\"Price outliers detected: {price_outliers.sum()}\")\n",
    "\n",
    "# Price jump outliers\n",
    "jump_outliers = handler.price_jump_outliers('Close', threshold_pct=5)\n",
    "print(f\"Price jump outliers: {jump_outliers.sum()}\")\n",
    "\n",
    "# Logic errors\n",
    "logic_errors = handler.ohlc_logic_outliers()\n",
    "print(f\"OHLC logic errors: {logic_errors.sum()}\")\n",
    "\n",
    "# Volume outliers\n",
    "vol_outliers = handler.volume_outliers('Volume', method='mad')\n",
    "print(f\"Volume outliers: {vol_outliers.sum()}\")\n",
    "\n",
    "# Treat outliers (clip extreme values)\n",
    "handler.treat_outliers('Close', method='clip', lower_quantile=0.01, upper_quantile=0.99)\n",
    "print(f\"\\nClose price range after treatment: {handler.df['Close'].min():.2f} - {handler.df['Close'].max():.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Statistical methods** (Z-score, IQR) assume roughly normal distribution. Financial returns are somewhat normal, but prices are not (they trend).\n",
    "- **Price jump detection**: A 10% daily move might be normal for crypto but is a huge outlier for NEPSE stocks. This is domain-specific.\n",
    "- **OHLC logic**: Violations (High < Close) are definitely errors and should be flagged or corrected immediately.\n",
    "- **MAD (Median Absolute Deviation)**: More robust than standard deviation for skewed distributions like trading volume (which has long right tail).\n",
    "- **Treatment strategies**:\n",
    "  - **Clip (Winsorize)**: Cap at 1st and 99th percentiles. Keeps data but reduces extreme influence.\n",
    "  - **Remove**: Delete rows. Use only if certain it's an error.\n",
    "  - **Transform**: Log-transform for skewed data. Reduces impact of large values.\n",
    "  - **Flag**: Keep data but mark it. Models can learn that flagged data is special.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.8 Data Smoothing Techniques**\n",
    "\n",
    "Smoothing reduces noise to reveal underlying trends. However, excessive smoothing removes genuine signals.\n",
    "\n",
    "```python\n",
    "class DataSmoother:\n",
    "    \"\"\"\n",
    "    Smoothing techniques for noisy financial data.\n",
    "    \n",
    "    Warning: Smoothing introduces lookahead bias if not handled carefully.\n",
    "    Always use causal (backward-looking) filters for prediction tasks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "    \n",
    "    def moving_average(self, column: str, window: int = 5, \n",
    "                      type: str = 'simple') -> pd.Series:\n",
    "        \"\"\"\n",
    "        Calculate moving average.\n",
    "        \n",
    "        type: 'simple' (SMA), 'exponential' (EMA), 'weighted' (WMA)\n",
    "        \"\"\"\n",
    "        if type == 'simple':\n",
    "            return self.df[column].rolling(window=window).mean()\n",
    "        \n",
    "        elif type == 'exponential':\n",
    "            # More weight to recent values\n",
    "            return self.df[column].ewm(span=window, adjust=False).mean()\n",
    "        \n",
    "        elif type == 'weighted':\n",
    "            # Linearly decreasing weights\n",
    "            weights = np.arange(1, window + 1)\n",
    "            return self.df[column].rolling(window=window).apply(\n",
    "                lambda x: np.dot(x, weights) / weights.sum(), raw=True\n",
    "            )\n",
    "    \n",
    "    def savgol_filter(self, column: str, window: int = 5, \n",
    "                     polyorder: int = 2) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Savitzky-Golay filter: preserves shape better than moving average.\n",
    "        \n",
    "        Fits polynomial to window and uses center point.\n",
    "        Good for preserving peaks while removing noise.\n",
    "        \"\"\"\n",
    "        from scipy.signal import savgol_filter\n",
    "        \n",
    "        # Handle NaNs by interpolation\n",
    "        data = self.df[column].interpolate()\n",
    "        \n",
    "        smoothed = savgol_filter(data, window_length=window, \n",
    "                                polyorder=polyorder, mode='nearest')\n",
    "        return pd.Series(smoothed, index=self.df.index)\n",
    "    \n",
    "    def kalman_filter(self, column: str) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Kalman filter: optimal recursive estimator.\n",
    "        \n",
    "        Adapts to changing volatility (good for regime changes in markets).\n",
    "        \"\"\"\n",
    "        from pykalman import KalmanFilter\n",
    "        \n",
    "        kf = KalmanFilter(transition_matrices=[1],\n",
    "                         observation_matrices=[1],\n",
    "                         initial_state_mean=self.df[column].iloc[0],\n",
    "                         initial_state_covariance=1,\n",
    "                         observation_covariance=1,\n",
    "                         transition_covariance=0.01)\n",
    "        \n",
    "        state_means, _ = kf.filter(self.df[column].values)\n",
    "        return pd.Series(state_means.flatten(), index=self.df.index)\n",
    "    \n",
    "    def hodrick_prescott(self, column: str, lambda_: float = 1600) -> Tuple[pd.Series, pd.Series]:\n",
    "        \"\"\"\n",
    "        Hodrick-Prescott filter: separate trend and cyclical components.\n",
    "        \n",
    "        lambda: smoothing parameter (1600 for quarterly data, 6.25 for yearly, 129600 for monthly)\n",
    "        \"\"\"\n",
    "        from statsmodels.tsa.filters.hp_filter import hpfilter\n",
    "        \n",
    "        cycle, trend = hpfilter(self.df[column], lamb=lambda_)\n",
    "        return trend, cycle\n",
    "\n",
    "# Usage\n",
    "# Generate noisy NEPSE price data\n",
    "dates = pd.date_range('2024-01-01', periods=100, freq='B')\n",
    "np.random.seed(42)\n",
    "trend = np.linspace(2800, 3000, 100)\n",
    "noise = np.random.randn(100) * 20\n",
    "price = trend + noise\n",
    "\n",
    "nepse_smooth = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Close': price\n",
    "})\n",
    "nepse_smooth.set_index('Date', inplace=True)\n",
    "\n",
    "# Apply smoothing\n",
    "smoother = DataSmoother(nepse_smooth)\n",
    "\n",
    "# Simple Moving Average (causal - only uses past data)\n",
    "nepse_smooth['SMA_5'] = smoother.moving_average('Close', window=5, type='simple')\n",
    "\n",
    "# Exponential Moving Average\n",
    "nepse_smooth['EMA_5'] = smoother.moving_average('Close', window=5, type='exponential')\n",
    "\n",
    "# Savitzky-Golay (non-causal, for analysis only)\n",
    "try:\n",
    "    nepse_smooth['SavGol'] = smoother.savgol_filter('Close', window=5, polyorder=2)\n",
    "except ImportError:\n",
    "    print(\"scipy not installed, skipping Savitzky-Golay\")\n",
    "\n",
    "print(\"Smoothing applied\")\n",
    "print(nepse_smooth.head(10))\n",
    "print(\"\\nNote: SMA and EMA use only past data (causal) - safe for prediction.\")\n",
    "print(\"Savitzky-Golay uses future data within window - only for visualization, not prediction features.\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Simple Moving Average (SMA)**: Average of last N periods. Lagging indicator, smooth but slow to react.\n",
    "- **Exponential Moving Average (EMA)**: Gives more weight to recent prices. Reacts faster to changes than SMA.\n",
    "- **Savitzky-Golay**: Fits polynomial to window. Preserves peaks and valleys better than averaging. **Warning**: Uses center of window, so for real-time prediction you need to shift it or use only past data (causal filter).\n",
    "- **Kalman Filter**: Statistical optimal filter that adapts to noise levels. Complex but powerful for financial data with changing volatility regimes.\n",
    "- **Hodrick-Prescott**: Decomposes into trend and cycle. Lambda parameter controls smoothness. Common in macroeconomics.\n",
    "- **Critical for prediction**: Only use causal filters (backward-looking) for feature engineering. Non-causal filters (centered) introduce future information (lookahead bias) which makes models look great in backtests but fail in production.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.9 Noise Reduction**\n",
    "\n",
    "Beyond smoothing, specific techniques reduce microstructure noise (bid-ask bounce, discrete pricing).\n",
    "\n",
    "```python\n",
    "class NoiseReducer:\n",
    "    \"\"\"\n",
    "    Reduce market microstructure noise from price data.\n",
    "    \n",
    "    NEPSE data may contain:\n",
    "    - Bid-ask bounce (prices oscillating between bid and ask)\n",
    "    - Discrete pricing (prices at tick increments)\n",
    "    - Outliers from block trades\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "    \n",
    "    def remove_bid_ask_bounce(self, price_col: str = 'Close', \n",
    "                              threshold: float = 0.001) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect and smooth bid-ask bounce using price clustering.\n",
    "        \n",
    "        If prices oscillate rapidly between two levels (bid/ask),\n",
    "        replace with midpoints.\n",
    "        \"\"\"\n",
    "        prices = self.df[price_col]\n",
    "        \n",
    "        # Detect rapid reversals (sign changes in returns)\n",
    "        returns = prices.pct_change()\n",
    "        reversals = (returns.shift(1) * returns < 0) & \\\n",
    "                   (returns.abs() < threshold)\n",
    "        \n",
    "        # Where reversals occur, use moving average\n",
    "        cleaned = prices.copy()\n",
    "        cleaned[reversals] = prices.rolling(3, center=True, min_periods=1).mean()[reversals]\n",
    "        \n",
    "        return cleaned\n",
    "    \n",
    "    def wavelet_denoising(self, column: str, wavelet: str = 'db1', \n",
    "                         level: int = 2) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Wavelet denoising: removes high-frequency noise while preserving jumps.\n",
    "        \n",
    "        Better than Fourier because it handles non-stationary signals (time-varying frequency).\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import pywt\n",
    "            \n",
    "            data = self.df[column].dropna().values\n",
    "            \n",
    "            # Wavelet decomposition\n",
    "            coeffs = pywt.wavedec(data, wavelet, level=level)\n",
    "            \n",
    "            # Threshold detail coefficients (noise is in high frequency details)\n",
    "            sigma = np.median(np.abs(coeffs[-1])) / 0.6745\n",
    "            uthresh = sigma * np.sqrt(2 * np.log(len(data)))\n",
    "            \n",
    "            # Soft thresholding\n",
    "            coeffs[1:] = [pywt.threshold(c, value=uthresh, mode='soft') \n",
    "                         for c in coeffs[1:]]\n",
    "            \n",
    "            # Reconstruct\n",
    "            denoised = pywt.waverec(coeffs, wavelet)\n",
    "            \n",
    "            # Handle length mismatch (wavelet output may be longer by 1)\n",
    "            if len(denoised) > len(self.df):\n",
    "                denoised = denoised[:len(self.df)]\n",
    "            \n",
    "            return pd.Series(denoised, index=self.df.index)\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"PyWavelets not installed, returning original\")\n",
    "            return self.df[column]\n",
    "    \n",
    "    def median_filter(self, column: str, window: int = 3) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Median filter: robust to outliers (unlike mean).\n",
    "        \n",
    "        Good for removing single-tick errors while preserving edges.\n",
    "        \"\"\"\n",
    "        from scipy.ndimage import median_filter\n",
    "        \n",
    "        filtered = median_filter(self.df[column].fillna(method='ffill'), \n",
    "                                size=window, mode='nearest')\n",
    "        return pd.Series(filtered, index=self.df.index)\n",
    "\n",
    "# Usage example\n",
    "print(\"Noise reduction techniques demonstrated\")\n",
    "print(\"Note: Wavelet denoising requires pywt installation\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Bid-ask bounce**: In tick data, prices alternate between bid (buy) and ask (sell) prices. This creates artificial volatility. Detection looks for rapid reversals small in magnitude.\n",
    "- **Wavelet denoising**: Unlike Fourier transforms which work on the whole signal, wavelets analyze local time-frequency. This preserves sharp jumps (earnings announcements) while removing high-frequency noise. Uses \"soft thresholding\" on detail coefficients.\n",
    "- **Median filter**: Replaces each point with median of neighbors. Unlike mean, median ignores outliers completely. Good for removing single bad ticks.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.10 Anomaly Detection**\n",
    "\n",
    "Anomalies are different from outliers\u2014anomalies are patterns that don't conform to expected behavior, often indicating important events.\n",
    "\n",
    "```python\n",
    "class AnomalyDetector:\n",
    "    \"\"\"\n",
    "    Detect anomalies in time-series using various methods.\n",
    "    \n",
    "    Anomalies vs Outliers:\n",
    "    - Outliers: Statistical extremes (single points)\n",
    "    - Anomalies: Pattern deviations (sequences, contextual)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "    \n",
    "    def isolation_forest(self, columns: List[str], \n",
    "                        contamination: float = 0.05) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Isolation Forest: ML-based anomaly detection.\n",
    "        \n",
    "        Isolates anomalies instead of profiling normal data.\n",
    "        Efficient for high-dimensional data.\n",
    "        \"\"\"\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        \n",
    "        data = self.df[columns].dropna()\n",
    "        \n",
    "        clf = IsolationForest(contamination=contamination, \n",
    "                             random_state=42,\n",
    "                             n_estimators=100)\n",
    "        predictions = clf.fit_predict(data)\n",
    "        \n",
    "        # -1 for anomaly, 1 for normal\n",
    "        anomalies = pd.Series(predictions == -1, index=data.index)\n",
    "        return anomalies\n",
    "    \n",
    "    def break_points(self, column: str, penalty: int = 10) -> List[int]:\n",
    "        \"\"\"\n",
    "        Detect structural break points (regime changes).\n",
    "        \n",
    "        Uses PELT (Pruned Exact Linear Time) algorithm.\n",
    "        Finds points where statistical properties change significantly.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            import ruptures as rpt\n",
    "            \n",
    "            data = self.df[column].dropna().values.reshape(-1, 1)\n",
    "            \n",
    "            # Binary segmentation\n",
    "            model = rpt.Binseg(model=\"l2\").fit(data)\n",
    "            break_points = model.predict(n_bkps=3)  # Find 3 breakpoints\n",
    "            \n",
    "            return break_points[:-1]  # Exclude end of series\n",
    "            \n",
    "        except ImportError:\n",
    "            print(\"ruptures not installed\")\n",
    "            return []\n",
    "    \n",
    "    def contextual_anomaly(self, column: str, \n",
    "                          context_window: int = 5) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Detect contextual anomalies: unusual given recent context.\n",
    "        \n",
    "        Example: Normal volume is 100k, but given recent news, \n",
    "        150k might be expected. 50k would be anomalously low.\n",
    "        \"\"\"\n",
    "        data = self.df[column]\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        rolling_mean = data.rolling(window=context_window).mean()\n",
    "        rolling_std = data.rolling(window=context_window).std()\n",
    "        \n",
    "        # Z-score relative to recent context\n",
    "        z_scores = (data - rolling_mean) / rolling_std\n",
    "        \n",
    "        # Anomaly if z-score > 3 or < -3\n",
    "        anomalies = z_scores.abs() > 3\n",
    "        \n",
    "        return anomalies\n",
    "\n",
    "# Usage would require scikit-learn and optionally ruptures\n",
    "print(\"Anomaly detection class defined\")\n",
    "print(\"Isolation Forest detects multivariate anomalies\")\n",
    "print(\"Breakpoint detection finds regime changes\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Isolation Forest**: Randomly selects features and split values. Anomalies are easier to isolate (fewer splits needed) than normal points. Unsupervised\u2014no labeled anomalies needed.\n",
    "- **Breakpoint detection**: Finds structural changes in the time-series (e.g., NEPSE market regime change after new regulations). Uses algorithms like PELT to find optimal change points.\n",
    "- **Contextual anomalies**: A volume of 1M might be normal during earnings week but anomalous during quiet periods. Compares to recent context rather than global statistics.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.11 Data Transformation Pipelines**\n",
    "\n",
    "Production systems require automated, reproducible pipelines that combine all preprocessing steps.\n",
    "\n",
    "```python\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "\n",
    "class NEPSEPreprocessor(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Custom sklearn transformer for NEPSE data preprocessing.\n",
    "    \n",
    "    Integrates with sklearn pipelines for cross-validation compatibility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, handle_missing: str = 'interpolate',\n",
    "                 outlier_method: str = 'clip',\n",
    "                 add_features: bool = True):\n",
    "        self.handle_missing = handle_missing\n",
    "        self.outlier_method = outlier_method\n",
    "        self.add_features = add_features\n",
    "        self.fitted_params = {}\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Learn parameters from training data (e.g., outlier thresholds).\"\"\"\n",
    "        # Store training statistics for consistent transformation\n",
    "        self.fitted_params['means'] = X.mean()\n",
    "        self.fitted_params['stds'] = X.std()\n",
    "        self.fitted_params['mins'] = X.quantile(0.01)\n",
    "        self.fitted_params['maxs'] = X.quantile(0.99)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        \"\"\"Apply transformations.\"\"\"\n",
    "        X = X.copy()\n",
    "        \n",
    "        # Handle missing values\n",
    "        if self.handle_missing == 'interpolate':\n",
    "            X = X.interpolate(method='linear')\n",
    "            X = X.fillna(method='ffill').fillna(method='bfill')\n",
    "        \n",
    "        # Handle outliers using training thresholds\n",
    "        if self.outlier_method == 'clip':\n",
    "            for col in X.columns:\n",
    "                if col in self.fitted_params['mins']:\n",
    "                    X[col] = X[col].clip(\n",
    "                        lower=self.fitted_params['mins'][col],\n",
    "                        upper=self.fitted_params['maxs'][col]\n",
    "                    )\n",
    "        \n",
    "        # Add engineered features\n",
    "        if self.add_features:\n",
    "            if 'Close' in X.columns:\n",
    "                X['Returns'] = X['Close'].pct_change()\n",
    "                X['Volatility'] = X['Returns'].rolling(5).std()\n",
    "                X['MA_Ratio'] = X['Close'] / X['Close'].rolling(5).mean()\n",
    "        \n",
    "        return X\n",
    "\n",
    "# Usage in a pipeline\n",
    "\"\"\"\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('preprocessor', NEPSEPreprocessor(handle_missing='interpolate')),\n",
    "    ('model', RandomForestRegressor())\n",
    "])\n",
    "\n",
    "# Fit on training data\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Predict on test data (preprocessing applied automatically)\n",
    "predictions = pipeline.predict(X_test)\n",
    "\"\"\"\n",
    "\n",
    "print(\"Sklearn-compatible preprocessor defined\")\n",
    "print(\"Can be used in cross-validation to prevent data leakage\")\n",
    "print(\"Fit learns parameters on train set, transform applies to test set\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Sklearn compatibility**: By inheriting from `BaseEstimator` and `TransformerMixin`, our preprocessor works with sklearn's `Pipeline`, `GridSearchCV`, etc.\n",
    "- **Fit vs Transform**: Critical distinction for preventing data leakage:\n",
    "  - `fit()`: Learns statistics (means, outlier thresholds) from training data only\n",
    "  - `transform()`: Applies those learned parameters to new data\n",
    "  - Never calculate statistics on test data\u2014always use training parameters\n",
    "- **Pipeline benefits**: Ensures preprocessing is part of model training, not separate. Prevents forgetting to apply preprocessing to validation sets.\n",
    "\n",
    "---\n",
    "\n",
    "## **6.12 Reproducible Preprocessing**\n",
    "\n",
    "Ensure that the same cleaning steps produce the same results every time, across different environments.\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import json\n",
    "\n",
    "class ReproducibleCleaner:\n",
    "    \"\"\"\n",
    "    Ensure preprocessing is fully reproducible.\n",
    "    \n",
    "    Tracks:\n",
    "    - Code version (git commit)\n",
    "    - Random seeds\n",
    "    - Hyperparameters\n",
    "    - Data hashes (input and output)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, config: Dict):\n",
    "        self.config = config\n",
    "        self.seed = config.get('random_seed', 42)\n",
    "        np.random.seed(self.seed)\n",
    "        \n",
    "        self.execution_log = {\n",
    "            'config': config,\n",
    "            'steps': [],\n",
    "            'input_hash': None,\n",
    "            'output_hash': None,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        }\n",
    "    \n",
    "    def log_step(self, name: str, params: Dict):\n",
    "        \"\"\"Log each preprocessing step.\"\"\"\n",
    "        self.execution_log['steps'].append({\n",
    "            'name': name,\n",
    "            'params': params,\n",
    "            'timestamp': datetime.now().isoformat()\n",
    "        })\n",
    "    \n",
    "    def hash_dataframe(self, df: pd.DataFrame) -> str:\n",
    "        \"\"\"Create deterministic hash of DataFrame.\"\"\"\n",
    "        # Sort columns and index for consistency\n",
    "        df_sorted = df.sort_index(axis=0).sort_index(axis=1)\n",
    "        \n",
    "        # Convert to string representation\n",
    "        data_string = df_sorted.to_json(sort_keys=True)\n",
    "        \n",
    "        # Create hash\n",
    "        return hashlib.sha256(data_string.encode()).hexdigest()[:16]\n",
    "    \n",
    "    def verify_reproducibility(self, df: pd.DataFrame, \n",
    "                              expected_hash: str) -> bool:\n",
    "        \"\"\"Verify that output matches expected hash.\"\"\"\n",
    "        actual_hash = self.hash_dataframe(df)\n",
    "        return actual_hash == expected_hash\n",
    "    \n",
    "    def save_report(self, filename: str):\n",
    "        \"\"\"Save execution log to JSON.\"\"\"\n",
    "        with open(filename, 'w') as f:\n",
    "            json.dump(self.execution_log, f, indent=2, default=str)\n",
    "        print(f\"Reproducibility report saved to {filename}\")\n",
    "\n",
    "# Usage example\n",
    "config = {\n",
    "    'random_seed': 42,\n",
    "    'outlier_threshold': 3.0,\n",
    "    'missing_strategy': 'interpolate',\n",
    "    'smoothing_window': 5\n",
    "}\n",
    "\n",
    "reproducible_cleaner = ReproducibleCleaner(config)\n",
    "\n",
    "# Load data\n",
    "data = nepse_outliers.copy()\n",
    "input_hash = reproducible_cleaner.hash_dataframe(data)\n",
    "reproducible_cleaner.execution_log['input_hash'] = input_hash\n",
    "\n",
    "# Apply cleaning steps (example)\n",
    "cleaned_data = data.interpolate()\n",
    "reproducible_cleaner.log_step('interpolate', {'method': 'linear'})\n",
    "\n",
    "cleaned_data = cleaned_data.fillna(method='ffill')\n",
    "reproducible_cleaner.log_step('fillna', {'method': 'ffill'})\n",
    "\n",
    "# Hash output\n",
    "output_hash = reproducible_cleaner.hash_dataframe(cleaned_data)\n",
    "reproducible_cleaner.execution_log['output_hash'] = output_hash\n",
    "\n",
    "# Save report\n",
    "reproducible_cleaner.save_report('cleaning_report.json')\n",
    "\n",
    "print(f\"\\nInput hash: {input_hash}\")\n",
    "print(f\"Output hash: {output_hash}\")\n",
    "print(f\"Config: {config}\")\n",
    "print(\"\\nThis report ensures the exact same cleaning can be reproduced later.\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Reproducibility** is crucial for scientific integrity and debugging. If your model performance drops, you need to know exactly what changed.\n",
    "- **Data hashing**: Creates a fingerprint of the dataset. If two datasets have the same hash, they are identical. This verifies that preprocessing produced the expected output.\n",
    "- **Execution log**: Records every step with parameters and timestamps. Like a lab notebook for data science.\n",
    "- **Random seeds**: NumPy and other libraries use random numbers. Setting the seed ensures \"random\" operations (like train-test splits) are identical across runs.\n",
    "- **Configuration**: All parameters externalized (not hardcoded) so they can be saved, versioned, and compared.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we covered comprehensive data cleaning and preprocessing for time-series:\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Systematic Strategy**: Always assess before cleaning, document changes, preserve raw data, and validate results. Use the `DataCleaningStrategy` framework.\n",
    "\n",
    "2. **Duplicate Handling**: Distinguish between exact duplicates (errors) and index duplicates (corrections). Use appropriate strategies (keep last for corrections, remove for errors).\n",
    "\n",
    "3. **Inconsistency Resolution**: Standardize symbols (uppercase), dates (ISO format), and units (actual shares, not thousands). Use regex and mapping dictionaries.\n",
    "\n",
    "4. **Type Optimization**: Use float32 for prices, int32 for volume, category for symbols, and datetime for dates. This reduces memory by 50-80%.\n",
    "\n",
    "5. **Missing Data Patterns**: Understand MCAR vs MAR vs MNAR. Use Little's test to check assumptions. Different mechanisms require different imputation strategies.\n",
    "\n",
    "6. **Advanced Imputation**: \n",
    "   - KNN for cross-sectional similarity\n",
    "   - Iterative (MICE) for correlated features like OHLC\n",
    "   - Constrained imputation to maintain financial logic (High \u2265 Low)\n",
    "\n",
    "7. **Outlier Treatment**: Use statistical (IQR, Z-score), domain-specific (price jumps), and logic-based (OHLC violations) detection. Treat by clipping, removing, or flagging based on context.\n",
    "\n",
    "8. **Smoothing**: Use causal filters (EMA, SMA with past data only) for prediction features. Non-causal filters (centered) are only for visualization.\n",
    "\n",
    "9. **Noise Reduction**: Wavelet denoising preserves jumps while removing noise. Median filters remove bad ticks without distorting trends.\n",
    "\n",
    "10. **Anomaly Detection**: Isolation Forest for multivariate anomalies, breakpoint detection for regime changes, contextual anomalies for unusual patterns given recent history.\n",
    "\n",
    "11. **Pipelines**: Build sklearn-compatible transformers to prevent data leakage and ensure preprocessing is part of model validation.\n",
    "\n",
    "12. **Reproducibility**: Hash inputs/outputs, log all steps, set random seeds, and save configurations. Science requires reproducibility.\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "In Chapter 7, we will cover **Exploratory Data Analysis**, including:\n",
    "- Statistical visualization techniques\n",
    "- Time-series decomposition (trend, seasonality)\n",
    "- Correlation analysis for feature selection\n",
    "- Distribution analysis and normality tests\n",
    "- Autocorrelation and partial autocorrelation functions\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 6**\n",
    "\n",
    "---\n",
    "\n",
    "*This chapter provided production-grade techniques for cleaning financial time-series data. The NEPSE examples demonstrate how to handle real-world issues like OHLC logic violations, missing trading days, and price spikes. Remember: cleaning is not just about removing bad data\u2014it's about understanding the data generation process and preserving the signal while removing noise.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='5. data_collection_and_ingestion.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='7. exploratory_data_analysis.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}