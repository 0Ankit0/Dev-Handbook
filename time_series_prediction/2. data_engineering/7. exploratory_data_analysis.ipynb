{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 7: Exploratory Data Analysis**\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Conduct systematic univariate analysis to understand individual feature distributions\n",
    "- Perform bivariate and multivariate analysis to identify relationships between variables\n",
    "- Apply time-series specific EDA techniques (trend, seasonality, autocorrelation)\n",
    "- Decompose time-series into constituent components\n",
    "- Create publication-quality visualizations following best practices\n",
    "- Generate automated EDA reports for rapid data understanding\n",
    "- Communicate data insights effectively to technical and non-technical stakeholders\n",
    "- Develop a comprehensive EDA checklist for production systems\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "- Completed Chapter 6: Data Cleaning and Preprocessing\n",
    "- Understanding of statistical concepts (mean, variance, correlation)\n",
    "- Familiarity with matplotlib and seaborn basics\n",
    "- NEPSE dataset loaded and cleaned from previous chapters\n",
    "\n",
    "---\n",
    "\n",
    "## **7.1 The EDA Process**\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the critical bridge between data collection and model building. For time-series prediction systems, EDA must go beyond standard statistical summaries to uncover temporal patterns, regime changes, and feature interactions that drive predictive signals.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style for professional output\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "class NEPSEEDAFramework:\n",
    "    \"\"\"\n",
    "    Comprehensive EDA framework for NEPSE time-series data.\n",
    "    \n",
    "    The EDA process follows these phases:\n",
    "    1. Understanding (What do we have?)\n",
    "    2. Cleaning validation (Did our cleaning work?)\n",
    "    3. Univariate analysis (How does each feature behave?)\n",
    "    4. Bivariate analysis (How do features relate?)\n",
    "    5. Temporal analysis (How does it change over time?)\n",
    "    6. Quality assessment (Is this data suitable for modeling?)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame, symbol: str):\n",
    "        self.df = df.copy()\n",
    "        self.symbol = symbol\n",
    "        self.report = {\n",
    "            'symbol': symbol,\n",
    "            'start_date': df.index.min() if isinstance(df.index, pd.DatetimeIndex) else None,\n",
    "            'end_date': df.index.max() if isinstance(df.index, pd.DatetimeIndex) else None,\n",
    "            'findings': []\n",
    "        }\n",
    "        \n",
    "        # Ensure datetime index\n",
    "        if 'Date' in self.df.columns:\n",
    "            self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
    "            self.df.set_index('Date', inplace=True)\n",
    "        \n",
    "        print(f\"EDA Framework initialized for {symbol}\")\n",
    "        print(f\"Data shape: {df.shape}\")\n",
    "        print(f\"Date range: {self.report['start_date']} to {self.report['end_date']}\")\n",
    "    \n",
    "    def add_finding(self, category: str, description: str, severity: str = 'info'):\n",
    "        \"\"\"Log findings for final report.\"\"\"\n",
    "        self.report['findings'].append({\n",
    "            'category': category,\n",
    "            'description': description,\n",
    "            'severity': severity,\n",
    "            'timestamp': pd.Timestamp.now()\n",
    "        })\n",
    "        print(f\"[{severity.upper()}] {category}: {description}\")\n",
    "\n",
    "# Initialize with NEPSE data\n",
    "# Creating comprehensive sample data for this chapter\n",
    "np.random.seed(42)\n",
    "dates = pd.date_range('2022-01-01', '2024-01-01', freq='B')  # 2 years of business days\n",
    "n = len(dates)\n",
    "\n",
    "# Generate realistic NEPSE data with trends and seasonality\n",
    "trend = np.linspace(2800, 3200, n)\n",
    "seasonal = 100 * np.sin(2 * np.pi * np.arange(n) / 252)  # Annual seasonality\n",
    "noise = np.cumsum(np.random.randn(n) * 5)  # Random walk\n",
    "volume_trend = np.linspace(100000, 150000, n)\n",
    "\n",
    "# OHLC with realistic relationships\n",
    "close = trend + seasonal + noise\n",
    "open_price = close + np.random.randn(n) * 10\n",
    "high = np.maximum(open_price, close) + np.random.uniform(10, 50, n)\n",
    "low = np.minimum(open_price, close) - np.random.uniform(10, 50, n)\n",
    "volume = volume_trend + np.random.randint(-20000, 20000, n)\n",
    "\n",
    "nepse_eda = pd.DataFrame({\n",
    "    'Open': open_price,\n",
    "    'High': high,\n",
    "    'Low': low,\n",
    "    'Close': close,\n",
    "    'Volume': volume,\n",
    "    'Symbol': 'NABIL'\n",
    "}, index=dates)\n",
    "\n",
    "# Add some features for analysis\n",
    "nepse_eda['Returns'] = nepse_eda['Close'].pct_change()\n",
    "nepse_eda['Log_Returns'] = np.log(nepse_eda['Close'] / nepse_eda['Close'].shift(1))\n",
    "nepse_eda['Volatility'] = nepse_eda['Returns'].rolling(20).std()\n",
    "nepse_eda['MA_20'] = nepse_eda['Close'].rolling(20).mean()\n",
    "nepse_eda['MA_50'] = nepse_eda['Close'].rolling(50).mean()\n",
    "\n",
    "eda_framework = NEPSEEDAFramework(nepse_eda, 'NABIL')\n",
    "eda_framework.add_finding('Data Loading', f'Loaded {len(nepse_eda)} records', 'info')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **The EDA Framework** provides structure to the exploration process. Without structure, EDA becomes random plotting without clear hypotheses.\n",
    "- **Phases of EDA**:\n",
    "  1. **Understanding**: Basic shape, types, ranges\n",
    "  2. **Cleaning validation**: Verify preprocessing worked (no unexpected nulls, ranges correct)\n",
    "  3. **Univariate**: Distribution of each variable (normal? skewed? bimodal?)\n",
    "  4. **Bivariate**: Relationships between pairs (correlation, causation hints)\n",
    "  5. **Temporal**: How things change over time (trends, cycles, anomalies)\n",
    "  6. **Quality assessment**: Suitability for modeling (stationarity, feature engineering opportunities)\n",
    "- **Data Generation**: We create 2 years of synthetic NEPSE data with realistic components:\n",
    "  - **Trend**: Long-term upward drift (2800 → 3200)\n",
    "  - **Seasonality**: Annual cycle (252 trading days)\n",
    "  - **Noise**: Random walk (more realistic than white noise for prices)\n",
    "  - **OHLC Logic**: High ≥ max(Open, Close), Low ≤ min(Open, Close)\n",
    "\n",
    "---\n",
    "\n",
    "## **7.2 Univariate Analysis**\n",
    "\n",
    "Understanding individual variables is the foundation of EDA. For time-series, we must examine both the distribution of values and the distribution over time.\n",
    "\n",
    "### **7.2.1 Distribution Analysis**\n",
    "\n",
    "```python\n",
    "class UnivariateAnalyzer:\n",
    "    \"\"\"Analyze individual features in isolation.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "    \n",
    "    def numerical_summary(self, column: str) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Comprehensive statistical summary beyond basic describe().\n",
    "        \n",
    "        Includes moments, percentiles, and distribution shape metrics.\n",
    "        \"\"\"\n",
    "        data = self.df[column].dropna()\n",
    "        \n",
    "        summary = {\n",
    "            'count': len(data),\n",
    "            'mean': data.mean(),\n",
    "            'std': data.std(),\n",
    "            'min': data.min(),\n",
    "            'max': data.max(),\n",
    "            'range': data.max() - data.min(),\n",
    "            'skewness': stats.skew(data),\n",
    "            'kurtosis': stats.kurtosis(data),\n",
    "            'median': data.median(),\n",
    "            'iqr': data.quantile(0.75) - data.quantile(0.25),\n",
    "            'cv': data.std() / data.mean(),  # Coefficient of variation\n",
    "            'jarque_bera_pvalue': stats.jarque_bera(data)[1],  # Normality test\n",
    "            'shapiro_pvalue': stats.shapiro(data.sample(min(5000, len(data))))[1] if len(data) > 3 else np.nan\n",
    "        }\n",
    "        \n",
    "        # Add percentiles\n",
    "        for p in [1, 5, 10, 25, 50, 75, 90, 95, 99]:\n",
    "            summary[f'p{p}'] = data.quantile(p/100)\n",
    "        \n",
    "        return pd.Series(summary)\n",
    "    \n",
    "    def plot_distribution(self, column: str, figsize=(15, 10)):\n",
    "        \"\"\"\n",
    "        Create comprehensive distribution visualization.\n",
    "        \n",
    "        Includes: histogram, KDE, box plot, Q-Q plot, and time series.\n",
    "        \"\"\"\n",
    "        data = self.df[column].dropna()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=figsize)\n",
    "        fig.suptitle(f'Univariate Analysis: {column}', fontsize=16, fontweight='bold')\n",
    "        \n",
    "        # 1. Histogram with KDE\n",
    "        ax1 = axes[0, 0]\n",
    "        sns.histplot(data, kde=True, ax=ax1, color='skyblue', alpha=0.7)\n",
    "        ax1.axvline(data.mean(), color='red', linestyle='--', label=f'Mean: {data.mean():.2f}')\n",
    "        ax1.axvline(data.median(), color='green', linestyle='--', label=f'Median: {data.median():.2f}')\n",
    "        ax1.set_title('Distribution with Central Tendencies')\n",
    "        ax1.legend()\n",
    "        \n",
    "        # 2. Box plot\n",
    "        ax2 = axes[0, 1]\n",
    "        sns.boxplot(y=data, ax=ax2, color='lightcoral')\n",
    "        ax2.set_title('Box Plot (Quartiles & Outliers)')\n",
    "        \n",
    "        # 3. Q-Q plot for normality assessment\n",
    "        ax3 = axes[1, 0]\n",
    "        stats.probplot(data, dist=\"norm\", plot=ax3)\n",
    "        ax3.set_title('Q-Q Plot vs Normal Distribution')\n",
    "        \n",
    "        # 4. Time series plot\n",
    "        ax4 = axes[1, 1]\n",
    "        ax4.plot(data.index, data, alpha=0.7, color='steelblue')\n",
    "        ax4.set_title('Time Series View')\n",
    "        ax4.set_xlabel('Date')\n",
    "        ax4.tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Analyze Close price distribution\n",
    "analyzer = UnivariateAnalyzer(nepse_eda)\n",
    "close_stats = analyzer.numerical_summary('Close')\n",
    "print(\"Close Price Statistical Summary:\")\n",
    "print(close_stats)\n",
    "\n",
    "# Create visualization\n",
    "fig = analyzer.plot_distribution('Close')\n",
    "plt.show()\n",
    "\n",
    "# Analyze returns (more interesting distribution)\n",
    "returns_stats = analyzer.numerical_summary('Returns')\n",
    "print(\"\\nReturns Statistical Summary:\")\n",
    "print(returns_stats)\n",
    "print(f\"\\nSkewness: {returns_stats['skewness']:.4f} (0=symmetric, >0=right tail)\")\n",
    "print(f\"Kurtosis: {returns_stats['kurtosis']:.4f} (0=normal, >0=fat tails)\")\n",
    "print(f\"Jarque-Bera p-value: {returns_stats['jarque_bera_pvalue']:.2e} (<0.05 = reject normality)\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Statistical Moments**:\n",
    "  - **Skewness**: Measures asymmetry. NEPSE returns often negative skew (crashes are sudden, gains gradual).\n",
    "  - **Kurtosis**: Measures tail thickness. Financial data typically has excess kurtosis (>0, \"fat tails\") meaning extreme events are more likely than normal distribution predicts.\n",
    "  - **Jarque-Bera Test**: Statistical test for normality. Low p-value (< 0.05) means data is not normally distributed—critical for choosing statistical models.\n",
    "- **Visualization Components**:\n",
    "  - **Histogram + KDE**: Shows shape (unimodal, bimodal?), central tendency, spread\n",
    "  - **Box Plot**: Identifies outliers (points beyond whiskers), shows IQR (box)\n",
    "  - **Q-Q Plot**: Compares data quantiles to theoretical normal distribution. Straight line = normal. Curved = skewed. Heavy tails = S-shaped.\n",
    "  - **Time Series**: Shows if distribution changes over time (non-stationarity)\n",
    "\n",
    "---\n",
    "\n",
    "### **7.2.2 Statistical Summaries**\n",
    "\n",
    "```python\n",
    "def generate_comprehensive_summary(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate publication-ready summary statistics table.\n",
    "    \n",
    "    Separate handling for price levels vs returns (different interpretations).\n",
    "    \"\"\"\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    summary_rows = []\n",
    "    \n",
    "    for col in numeric_cols:\n",
    "        data = df[col].dropna()\n",
    "        \n",
    "        if 'return' in col.lower() or col == 'Returns':\n",
    "            # Returns interpretation\n",
    "            row = {\n",
    "                'Variable': col,\n",
    "                'Mean (%)': f\"{data.mean()*100:.4f}\",\n",
    "                'Std Dev (%)': f\"{data.std()*100:.4f}\",\n",
    "                'Min (%)': f\"{data.min()*100:.2f}\",\n",
    "                'Max (%)': f\"{data.max()*100:.2f}\",\n",
    "                'Annualized Vol (%)': f\"{data.std() * np.sqrt(252) * 100:.2f}\",\n",
    "                'Sharpe Ratio': f\"{data.mean() / data.std() * np.sqrt(252):.3f}\" if data.std() > 0 else \"N/A\",\n",
    "                'Skewness': f\"{stats.skew(data):.3f}\",\n",
    "                'Max Drawdown (%)': f\"{(data.cumsum().cummax() - data.cumsum()).max()*100:.2f}\"\n",
    "            }\n",
    "        else:\n",
    "            # Price/Volume interpretation\n",
    "            row = {\n",
    "                'Variable': col,\n",
    "                'Mean': f\"{data.mean():.2f}\",\n",
    "                'Std Dev': f\"{data.std():.2f}\",\n",
    "                'Min': f\"{data.min():.2f}\",\n",
    "                'Max': f\"{data.max():.2f}\",\n",
    "                'CV (%)': f\"{(data.std()/data.mean())*100:.2f}\",\n",
    "                'P5-P95 Range': f\"{data.quantile(0.95) - data.quantile(0.05):.2f}\",\n",
    "                'Trend (daily change)': f\"{np.polyfit(range(len(data)), data, 1)[0]:.4f}\",\n",
    "                'N': len(data)\n",
    "            }\n",
    "        \n",
    "        summary_rows.append(row)\n",
    "    \n",
    "    return pd.DataFrame(summary_rows)\n",
    "\n",
    "# Generate summary\n",
    "summary_table = generate_comprehensive_summary(nepse_eda)\n",
    "print(\"Comprehensive NEPSE Data Summary:\")\n",
    "print(summary_table.to_string(index=False))\n",
    "\n",
    "# Special analysis for trading days\n",
    "def analyze_trading_patterns(df: pd.DataFrame):\n",
    "    \"\"\"Analyze patterns specific to trading days.\"\"\"\n",
    "    df = df.copy()\n",
    "    df['DayOfWeek'] = df.index.dayofweek  # Monday=0, Friday=4\n",
    "    df['Month'] = df.index.month\n",
    "    df['Quarter'] = df.index.quarter\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"TRADING PATTERN ANALYSIS\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Day of week effect (Monday effect, Weekend effect)\n",
    "    dow_stats = df.groupby('DayOfWeek')['Returns'].agg(['mean', 'std', 'count'])\n",
    "    dow_stats.index = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday']\n",
    "    print(\"\\nReturns by Day of Week:\")\n",
    "    print(dow_stats)\n",
    "    \n",
    "    # Monthly seasonality\n",
    "    monthly_stats = df.groupby('Month')['Returns'].mean()\n",
    "    print(f\"\\nBest performing month: {monthly_stats.idxmax()} ({monthly_stats.max()*100:.3f}%)\")\n",
    "    print(f\"Worst performing month: {monthly_stats.idxmin()} ({monthly_stats.min()*100:.3f}%)\")\n",
    "\n",
    "analyze_trading_patterns(nepse_eda)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Comprehensive Summary**: Different metrics for different variable types:\n",
    "  - **Returns**: Annualized volatility, Sharpe ratio (risk-adjusted return), maximum drawdown (worst peak-to-trough decline)\n",
    "  - **Prices**: Coefficient of variation (relative dispersion), trend slope (linear trend coefficient)\n",
    "- **Trading Patterns**:\n",
    "  - **Day-of-Week Effect**: Historical anomaly where Mondays often have lower returns (weekend effect)\n",
    "  - **Monthly Seasonality**: \"Sell in May and go away\" or end-of-year effects\n",
    "  - These calendar effects may be predictive features for models\n",
    "\n",
    "---\n",
    "\n",
    "### **7.2.3 Visualization Techniques**\n",
    "\n",
    "```python\n",
    "class EDAVisualizations:\n",
    "    \"\"\"Advanced visualization techniques for financial EDA.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "    \n",
    "    def plot_ohlc_evolution(self, n_days: int = 60):\n",
    "        \"\"\"\n",
    "        Plot OHLC evolution with volume overlay.\n",
    "        \n",
    "        Professional financial chart style.\n",
    "        \"\"\"\n",
    "        data = self.df.tail(n_days).copy()\n",
    "        \n",
    "        fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), \n",
    "                                       gridspec_kw={'height_ratios': [3, 1]},\n",
    "                                       sharex=True)\n",
    "        \n",
    "        # Price evolution with fill between high-low\n",
    "        ax1.fill_between(data.index, data['Low'], data['High'], \n",
    "                        alpha=0.3, color='gray', label='Daily Range')\n",
    "        ax1.plot(data.index, data['Close'], color='black', linewidth=1.5, label='Close')\n",
    "        ax1.plot(data.index, data['Open'], color='blue', linewidth=1, alpha=0.7, label='Open')\n",
    "        \n",
    "        # Color code by return (green up, red down)\n",
    "        for i in range(len(data)-1):\n",
    "            if data['Close'].iloc[i+1] >= data['Close'].iloc[i]:\n",
    "                color = 'green'\n",
    "            else:\n",
    "                color = 'red'\n",
    "            ax1.plot(data.index[i:i+2], data['Close'].iloc[i:i+2], \n",
    "                    color=color, linewidth=2, alpha=0.6)\n",
    "        \n",
    "        ax1.set_title(f'NEPSE OHLC Evolution (Last {n_days} days)', fontsize=14, fontweight='bold')\n",
    "        ax1.legend()\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        ax1.set_ylabel('Price (NPR)')\n",
    "        \n",
    "        # Volume bars\n",
    "        colors = ['green' if data['Close'].iloc[i] >= data['Open'].iloc[i] else 'red' \n",
    "                 for i in range(len(data))]\n",
    "        ax2.bar(data.index, data['Volume'], color=colors, alpha=0.7, width=0.8)\n",
    "        ax2.set_ylabel('Volume')\n",
    "        ax2.set_xlabel('Date')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def distribution_comparison(self, columns: list):\n",
    "        \"\"\"\n",
    "        Compare distributions of multiple variables.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, len(columns), figsize=(5*len(columns), 8))\n",
    "        \n",
    "        if len(columns) == 1:\n",
    "            axes = axes.reshape(-1, 1)\n",
    "        \n",
    "        for i, col in enumerate(columns):\n",
    "            data = self.df[col].dropna()\n",
    "            \n",
    "            # Histogram\n",
    "            axes[0, i].hist(data, bins=30, alpha=0.7, color='steelblue', edgecolor='black')\n",
    "            axes[0, i].set_title(f'{col} Distribution')\n",
    "            axes[0, i].axvline(data.mean(), color='red', linestyle='--', label='Mean')\n",
    "            axes[0, i].axvline(data.median(), color='green', linestyle='--', label='Median')\n",
    "            \n",
    "            # Time series\n",
    "            axes[1, i].plot(data.index, data, alpha=0.7, color='coral')\n",
    "            axes[1, i].set_title(f'{col} Time Series')\n",
    "            axes[1, i].tick_params(axis='x', rotation=45)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "\n",
    "# Create visualizations\n",
    "viz = EDAVisualizations(nepse_eda)\n",
    "\n",
    "# OHLC chart\n",
    "fig1 = viz.plot_ohlc_evolution(n_days=60)\n",
    "plt.show()\n",
    "\n",
    "# Distribution comparison\n",
    "fig2 = viz.distribution_comparison(['Close', 'Returns', 'Volume'])\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **OHLC Evolution**: Professional financial chart showing:\n",
    "  - Gray shaded area between High and Low (daily range)\n",
    "  - Black line for Close (most important price)\n",
    "  - Color-coded segments (green for up days, red for down days)\n",
    "  - Volume bars synchronized below\n",
    "- **Distribution Comparison**: Side-by-side comparison of price levels (trending, non-stationary), returns (mean-reverting, stationary), and volume (often right-skewed).\n",
    "\n",
    "---\n",
    "\n",
    "## **7.3 Bivariate Analysis**\n",
    "\n",
    "Understanding relationships between pairs of variables reveals potential predictive features and multicollinearity issues.\n",
    "\n",
    "### **7.3.1 Correlation Analysis**\n",
    "\n",
    "```python\n",
    "class CorrelationAnalyzer:\n",
    "    \"\"\"Analyze pairwise relationships between variables.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.select_dtypes(include=[np.number])\n",
    "    \n",
    "    def correlation_matrix(self, method: str = 'pearson') -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Calculate correlation matrix with multiple methods.\n",
    "        \n",
    "        Methods:\n",
    "        - pearson: Linear correlation (sensitive to outliers)\n",
    "        - spearman: Rank correlation (monotonic relationships, robust)\n",
    "        - kendall: Concordance correlation (good for small samples)\n",
    "        \"\"\"\n",
    "        if method == 'pearson':\n",
    "            corr = self.df.corr()\n",
    "        elif method == 'spearman':\n",
    "            corr = self.df.corr(method='spearman')\n",
    "        elif method == 'kendall':\n",
    "            corr = self.df.corr(method='kendall')\n",
    "        \n",
    "        return corr\n",
    "    \n",
    "    def plot_correlation_heatmap(self, method: str = 'pearson', figsize=(12, 10)):\n",
    "        \"\"\"\n",
    "        Create annotated heatmap of correlations.\n",
    "        \"\"\"\n",
    "        corr = self.correlation_matrix(method)\n",
    "        \n",
    "        # Create mask for upper triangle (redundant)\n",
    "        mask = np.triu(np.ones_like(corr, dtype=bool))\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=figsize)\n",
    "        \n",
    "        sns.heatmap(corr, mask=mask, annot=True, fmt='.2f', \n",
    "                   cmap='RdBu_r', center=0, ax=ax,\n",
    "                   square=True, linewidths=0.5,\n",
    "                   cbar_kws={\"shrink\": 0.8})\n",
    "        \n",
    "        ax.set_title(f'{method.capitalize()} Correlation Matrix', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        return fig\n",
    "    \n",
    "    def find_high_correlations(self, threshold: float = 0.8) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Find pairs with correlation above threshold.\n",
    "        \n",
    "        Important for detecting multicollinearity in features.\n",
    "        \"\"\"\n",
    "        corr = self.correlation_matrix('pearson').abs()\n",
    "        \n",
    "        # Get upper triangle (avoid duplicates)\n",
    "        upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(bool))\n",
    "        \n",
    "        # Find pairs above threshold\n",
    "        high_corr = []\n",
    "        for col in upper.columns:\n",
    "            for idx in upper.index:\n",
    "                if upper.loc[idx, col] > threshold:\n",
    "                    high_corr.append({\n",
    "                        'Feature_1': idx,\n",
    "                        'Feature_2': col,\n",
    "                        'Correlation': corr.loc[idx, col],\n",
    "                        'Issue': 'Multicollinearity risk' if upper.loc[idx, col] > 0.9 else 'High correlation'\n",
    "                    })\n",
    "        \n",
    "        return pd.DataFrame(high_corr)\n",
    "\n",
    "# Analyze correlations\n",
    "corr_analyzer = CorrelationAnalyzer(nepse_eda)\n",
    "\n",
    "# Pearson (linear)\n",
    "fig_corr = corr_analyzer.plot_correlation_heatmap('pearson')\n",
    "plt.show()\n",
    "\n",
    "# Find problematic correlations\n",
    "high_corr_pairs = corr_analyzer.find_high_correlations(threshold=0.7)\n",
    "print(\"High Correlation Pairs (potential redundancy):\")\n",
    "print(high_corr_pairs)\n",
    "\n",
    "# Compare Pearson vs Spearman for Returns vs Volume\n",
    "pearson_corr = nepse_eda[['Returns', 'Volume']].corr().iloc[0,1]\n",
    "spearman_corr = nepse_eda[['Returns', 'Volume']].corr(method='spearman').iloc[0,1]\n",
    "\n",
    "print(f\"\\nReturns vs Volume:\")\n",
    "print(f\"Pearson (linear): {pearson_corr:.4f}\")\n",
    "print(f\"Spearman (rank): {spearman_corr:.4f}\")\n",
    "print(\"Difference suggests non-linear relationship or outliers\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Correlation Methods**:\n",
    "  - **Pearson**: Measures linear relationships (-1 to 1). Sensitive to outliers (one bad tick can ruin correlation).\n",
    "  - **Spearman**: Measures monotonic relationships (rank-based). Robust to outliers. Good for financial data with fat tails.\n",
    "  - **Kendall**: Measures concordance (how often pairs agree in ranking). Good for small samples.\n",
    "- **Multicollinearity**: High correlation between features (>0.9) causes problems in linear models (unstable coefficients). Heatmap helps identify redundant features (e.g., Open and Close are highly correlated—maybe only need Close and Range).\n",
    "- **Divergence between Pearson and Spearman**: Indicates non-linear relationship or heavy influence of outliers.\n",
    "\n",
    "---\n",
    "\n",
    "### **7.3.2 Scatter Plots and Relationships**\n",
    "\n",
    "```python\n",
    "def plot_bivariate_relationships(df: pd.DataFrame, target: str = 'Returns'):\n",
    "    \"\"\"\n",
    "    Create scatter plot matrix for key relationships.\n",
    "    \"\"\"\n",
    "    # Select features for analysis\n",
    "    features = ['Open', 'High', 'Low', 'Close', 'Volume', 'MA_20', target]\n",
    "    features = [f for f in features if f in df.columns]\n",
    "    \n",
    "    data = df[features].dropna()\n",
    "    \n",
    "    # Create pairplot\n",
    "    g = sns.pairplot(data, \n",
    "                     diag_kind='kde',\n",
    "                     plot_kws={'alpha': 0.6, 's': 20},\n",
    "                     diag_kws={'fill': True},\n",
    "                     height=2.5,\n",
    "                     aspect=1)\n",
    "    \n",
    "    g.fig.suptitle(f'Bivariate Relationships (Target: {target})', \n",
    "                   y=1.02, fontsize=16, fontweight='bold')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    return g\n",
    "\n",
    "# Create scatter matrix\n",
    "g = plot_bivariate_relationships(nepse_eda, 'Returns')\n",
    "plt.show()\n",
    "\n",
    "# Specific analysis: Volume vs Returns (common hypothesis: high volume accompanies high returns)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# Scatter plot with regression line\n",
    "ax1 = axes[0]\n",
    "sns.regplot(data=nepse_eda, x='Volume', y='Returns', \n",
    "           scatter_kws={'alpha': 0.5}, ax=ax1)\n",
    "ax1.set_title('Volume vs Returns (with regression line)')\n",
    "ax1.set_ylabel('Daily Returns')\n",
    "\n",
    "# Hexbin for density (better for large datasets)\n",
    "ax2 = axes[1]\n",
    "hb = ax2.hexbin(nepse_eda['Volume'], nepse_eda['Returns'], \n",
    "                gridsize=30, cmap='Blues', mincnt=1)\n",
    "ax2.set_xlabel('Volume')\n",
    "ax2.set_ylabel('Returns')\n",
    "ax2.set_title('Volume vs Returns (density plot)')\n",
    "plt.colorbar(hb, ax=ax2, label='Count')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Calculate rolling correlation to see time-varying relationships\n",
    "rolling_corr = nepse_eda['Returns'].rolling(60).corr(nepse_eda['Volume'])\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(rolling_corr.index, rolling_corr, color='purple')\n",
    "plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "plt.title('60-Day Rolling Correlation: Returns vs Volume')\n",
    "plt.ylabel('Correlation')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Pairplot**: Matrix of all pairwise scatter plots. Diagonal shows univariate distributions (KDE). Off-diagonal shows relationships. Quickly identifies linear/non-linear patterns.\n",
    "- **Volume-Return Relationship**: Common hypothesis is that high volume accompanies large price moves (information arrival). The scatter plot tests this.\n",
    "- **Hexbin**: For large datasets, scatter plots become overplotted (black blob). Hexbin aggregates points into hexagonal bins, color-coded by density.\n",
    "- **Rolling Correlation**: Relationships in financial markets are not static—they change over time (regime changes). Rolling correlation shows if Volume-Return relationship is strengthening or weakening.\n",
    "\n",
    "---\n",
    "\n",
    "### **7.3.3 Cross-Correlation**\n",
    "\n",
    "```python\n",
    "def cross_correlation_analysis(df: pd.DataFrame, x: str, y: str, max_lags: int = 10):\n",
    "    \"\"\"\n",
    "    Analyze lead-lag relationships between two time series.\n",
    "    \n",
    "    Important for feature engineering: does past volume predict future returns?\n",
    "    \"\"\"\n",
    "    from scipy.signal import correlate\n",
    "    \n",
    "    # Remove NaN\n",
    "    data = df[[x, y]].dropna()\n",
    "    x_series = data[x]\n",
    "    y_series = data[y]\n",
    "    \n",
    "    # Normalize\n",
    "    x_norm = (x_series - x_series.mean()) / x_series.std()\n",
    "    y_norm = (y_series - y_series.mean()) / y_series.std()\n",
    "    \n",
    "    # Calculate cross-correlation\n",
    "    correlation = correlate(x_norm, y_norm, mode='full')\n",
    "    lags = np.arange(-len(x_norm) + 1, len(x_norm))\n",
    "    \n",
    "    # Normalize by length\n",
    "    correlation = correlation / len(x_norm)\n",
    "    \n",
    "    # Keep only relevant lags\n",
    "    mask = (lags >= -max_lags) & (lags <= max_lags)\n",
    "    correlation = correlation[mask]\n",
    "    lags = lags[mask]\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.bar(lags, correlation, color='steelblue', alpha=0.7)\n",
    "    plt.axvline(x=0, color='red', linestyle='--', label='Contemporaneous')\n",
    "    plt.axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "    plt.xlabel('Lag (days)')\n",
    "    plt.ylabel('Cross-Correlation')\n",
    "    plt.title(f'Cross-Correlation: {x} vs {y}\\n(Positive lag = {x} leads {y})')\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    \n",
    "    # Find significant peaks\n",
    "    max_corr_idx = np.argmax(np.abs(correlation))\n",
    "    max_lag = lags[max_corr_idx]\n",
    "    max_corr = correlation[max_corr_idx]\n",
    "    \n",
    "    plt.annotate(f'Max: {max_corr:.3f} at lag {max_lag}', \n",
    "                xy=(max_lag, max_corr), \n",
    "                xytext=(max_lag+1, max_corr+0.05),\n",
    "                arrowprops=dict(arrowstyle='->', color='red'))\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return pd.DataFrame({'Lag': lags, 'Correlation': correlation})\n",
    "\n",
    "# Analyze: Does past volume predict future returns?\n",
    "print(\"Analyzing lead-lag relationship: Volume -> Returns\")\n",
    "ccf = cross_correlation_analysis(nepse_eda, 'Volume', 'Returns', max_lags=5)\n",
    "print(ccf)\n",
    "\n",
    "# Interpretation\n",
    "max_row = ccf.loc[ccf['Correlation'].abs().idxmax()]\n",
    "print(f\"\\nStrongest relationship: Lag {max_row['Lag']} with correlation {max_row['Correlation']:.4f}\")\n",
    "if max_row['Lag'] > 0:\n",
    "    print(\"Volume LEADS returns (predictive signal)\")\n",
    "elif max_row['Lag'] < 0:\n",
    "    print(\"Returns LEAD volume (reactionary)\")\n",
    "else:\n",
    "    print(\"Contemporaneous relationship\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Cross-Correlation**: Measures correlation between two series at different time lags. Unlike regular correlation (lag 0), this detects lead-lag relationships.\n",
    "- **Interpretation**:\n",
    "  - **Lag +1**: Yesterday's Volume correlates with today's Returns (predictive!)\n",
    "  - **Lag -1**: Yesterday's Returns correlate with today's Volume (reaction)\n",
    "  - **Lag 0**: Same-day relationship (simultaneous)\n",
    "- **Feature Engineering**: If Volume at lag 1 predicts Returns, include Volume.shift(1) as a feature in your model.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.4 Multivariate Analysis**\n",
    "\n",
    "When dealing with many features (technical indicators, fundamental data), we need dimensionality reduction and latent structure discovery.\n",
    "\n",
    "### **7.4.1 Principal Component Analysis**\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "class PCAnalyzer:\n",
    "    \"\"\"\n",
    "    PCA for dimensionality reduction and feature understanding.\n",
    "    \n",
    "    Useful when you have many correlated technical indicators\n",
    "    and want to reduce to uncorrelated principal components.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        # Select numeric features, drop target\n",
    "        self.features = df.select_dtypes(include=[np.number]).drop(['Returns', 'Log_Returns'], axis=1, errors='ignore')\n",
    "        self.feature_names = self.features.columns\n",
    "        \n",
    "        # Standardize (PCA requires scaling)\n",
    "        self.scaler = StandardScaler()\n",
    "        self.scaled_data = self.scaler.fit_transform(self.features.dropna())\n",
    "    \n",
    "    def fit_pca(self, n_components: int = None):\n",
    "        \"\"\"Fit PCA model.\"\"\"\n",
    "        self.pca = PCA(n_components=n_components)\n",
    "        self.components = self.pca.fit_transform(self.scaled_data)\n",
    "        \n",
    "        print(f\"PCA fitted\")\n",
    "        print(f\"Explained variance ratio: {self.pca.explained_variance_ratio_}\")\n",
    "        print(f\"Cumulative variance: {np.cumsum(self.pca.explained_variance_ratio_)}\")\n",
    "    \n",
    "    def plot_variance_explained(self):\n",
    "        \"\"\"Scree plot for component selection.\"\"\"\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # Individual variance\n",
    "        ax1.bar(range(1, len(self.pca.explained_variance_ratio_) + 1), \n",
    "               self.pca.explained_variance_ratio_,\n",
    "               alpha=0.7, color='steelblue')\n",
    "        ax1.set_xlabel('Principal Component')\n",
    "        ax1.set_ylabel('Explained Variance Ratio')\n",
    "        ax1.set_title('Scree Plot')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # Cumulative variance\n",
    "        cumvar = np.cumsum(self.pca.explained_variance_ratio_)\n",
    "        ax2.plot(range(1, len(cumvar) + 1), cumvar, 'bo-', linewidth=2, markersize=8)\n",
    "        ax2.axhline(y=0.95, color='r', linestyle='--', label='95% threshold')\n",
    "        ax2.set_xlabel('Number of Components')\n",
    "        ax2.set_ylabel('Cumulative Explained Variance')\n",
    "        ax2.set_title('Cumulative Variance Explained')\n",
    "        ax2.legend()\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def plot_component_weights(self, n_components: int = 3):\n",
    "        \"\"\"Show feature loadings for top components.\"\"\"\n",
    "        fig, axes = plt.subplots(1, n_components, figsize=(5*n_components, 6))\n",
    "        \n",
    "        if n_components == 1:\n",
    "            axes = [axes]\n",
    "        \n",
    "        for i in range(n_components):\n",
    "            weights = self.pca.components_[i]\n",
    "            feature_weights = pd.Series(weights, index=self.feature_names).sort_values()\n",
    "            \n",
    "            colors = ['red' if w < 0 else 'green' for w in feature_weights]\n",
    "            feature_weights.plot(kind='barh', ax=axes[i], color=colors, alpha=0.7)\n",
    "            axes[i].set_title(f'PC{i+1} ({self.pca.explained_variance_ratio_[i]:.1%} variance)')\n",
    "            axes[i].axvline(x=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Prepare features for PCA\n",
    "# Create technical indicators\n",
    "features_df = nepse_eda.copy()\n",
    "features_df['RSI'] = 50 + np.random.randn(len(features_df)) * 10  # Mock RSI\n",
    "features_df['MACD'] = np.random.randn(len(features_df)) * 5\n",
    "features_df['BB_Upper'] = features_df['Close'] + np.random.uniform(20, 50, len(features_df))\n",
    "features_df['BB_Lower'] = features_df['Close'] - np.random.uniform(20, 50, len(features_df))\n",
    "\n",
    "pca_analysis = PCAnalyzer(features_df)\n",
    "pca_analysis.fit_pca(n_components=5)\n",
    "pca_analysis.plot_variance_explained()\n",
    "pca_analysis.plot_component_weights(n_components=3)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **PCA**: Transforms correlated features into uncorrelated principal components. First PC captures maximum variance, second captures maximum remaining variance, etc.\n",
    "- **Scree Plot**: Shows how much variance each component explains. \"Elbow\" indicates optimal number of components (diminishing returns after that point).\n",
    "- **Cumulative Variance**: Typically want 95% of variance retained. If first 3 components explain 95%, you can reduce from N features to 3 without much information loss.\n",
    "- **Component Weights**: Shows which original features contribute to each PC. PC1 might be \"trend\" (positive weights on MA, Close), PC2 might be \"volatility\" (weights on High-Low range).\n",
    "\n",
    "---\n",
    "\n",
    "## **7.5 Time-Series Specific Analysis**\n",
    "\n",
    "### **7.5.1 Trend Analysis**\n",
    "\n",
    "```python\n",
    "from scipy.signal import detrend\n",
    "\n",
    "class TrendAnalyzer:\n",
    "    \"\"\"Analyze deterministic trends in time-series.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df\n",
    "    \n",
    "    def analyze_trend(self, column: str = 'Close'):\n",
    "        \"\"\"\n",
    "        Decompose trend using multiple methods.\n",
    "        \"\"\"\n",
    "        data = self.df[column].dropna()\n",
    "        \n",
    "        # 1. Linear trend (OLS)\n",
    "        x = np.arange(len(data))\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, data)\n",
    "        \n",
    "        # 2. Moving average trend\n",
    "        ma_trend = data.rolling(window=50, center=True).mean()\n",
    "        \n",
    "        # 3. HP Filter trend\n",
    "        from statsmodels.tsa.filters.hp_filter import hpfilter\n",
    "        cycle, hp_trend = hpfilter(data, lamb=1600)\n",
    "        \n",
    "        # Plotting\n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 10), sharex=True)\n",
    "        \n",
    "        # Original with trends\n",
    "        axes[0].plot(data.index, data, label='Original', alpha=0.7, color='gray')\n",
    "        axes[0].plot(data.index, slope * x + intercept, label=f'Linear (slope={slope:.2f}/day)', \n",
    "                    color='red', linestyle='--')\n",
    "        axes[0].plot(data.index, ma_trend, label='MA(50)', color='blue', linewidth=2)\n",
    "        axes[0].plot(data.index, hp_trend, label='HP Filter', color='green', linewidth=2)\n",
    "        axes[0].set_title('Trend Decomposition')\n",
    "        axes[0].legend()\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Detrended series\n",
    "        detrended = data - hp_trend\n",
    "        axes[1].plot(data.index, detrended, color='purple', alpha=0.7)\n",
    "        axes[1].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "        axes[1].set_title('Detrended Series (Cyclical Component)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"Linear Trend: {slope:.4f} per day ({slope*252:.2f} per year)\")\n",
    "        print(f\"R-squared: {r_value**2:.4f} ({r_value**2*100:.1f}% of variance explained by trend)\")\n",
    "        print(f\"Trend significance: p-value = {p_value:.2e}\")\n",
    "        \n",
    "        return {\n",
    "            'slope': slope,\n",
    "            'annualized_return': slope * 252,\n",
    "            'r_squared': r_value**2,\n",
    "            'p_value': p_value\n",
    "        }\n",
    "\n",
    "trend_analysis = TrendAnalyzer(nepse_eda)\n",
    "trend_stats = trend_analysis.analyze_trend('Close')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Linear Trend**: Simple regression of price on time. Slope shows average daily change. Annualized by multiplying by 252 trading days.\n",
    "- **R-squared**: Percentage of price variance explained by time trend (0-1). High R-squared means strong trend; low means noisy/random walk.\n",
    "- **HP Filter**: Smoother trend that adapts to local curvature (better for financial trends that aren't perfectly linear).\n",
    "- **Detrending**: Removing trend to analyze cyclical components. Essential for mean-reversion strategies.\n",
    "\n",
    "---\n",
    "\n",
    "### **7.5.2 Seasonality Detection**\n",
    "\n",
    "```python\n",
    "class SeasonalityAnalyzer:\n",
    "    \"\"\"Detect and visualize seasonal patterns.\"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.df['DayOfWeek'] = self.df.index.dayofweek\n",
    "        self.df['Month'] = self.df.index.month\n",
    "        self.df['Quarter'] = self.df.index.quarter\n",
    "        self.df['DayOfYear'] = self.df.index.dayofyear\n",
    "    \n",
    "    def plot_seasonal_decomposition(self, column: str = 'Close'):\n",
    "        \"\"\"\n",
    "        Decompose into trend, seasonal, and residual.\n",
    "        \"\"\"\n",
    "        from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "        \n",
    "        data = self.df[column].dropna()\n",
    "        \n",
    "        # Decompose (additive: Y = Trend + Seasonal + Residual)\n",
    "        decomposition = seasonal_decompose(data, model='additive', period=252)  # Annual seasonality\n",
    "        \n",
    "        fig, axes = plt.subplots(4, 1, figsize=(14, 12), sharex=True)\n",
    "        \n",
    "        decomposition.observed.plot(ax=axes[0], title='Original')\n",
    "        decomposition.trend.plot(ax=axes[1], title='Trend')\n",
    "        decomposition.seasonal.plot(ax=axes[2], title='Seasonal')\n",
    "        decomposition.resid.plot(ax=axes[3], title='Residual')\n",
    "        \n",
    "        for ax in axes:\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "    \n",
    "    def seasonal_heatmap(self, column: str = 'Returns'):\n",
    "        \"\"\"Create heatmap of average returns by month and year.\"\"\"\n",
    "        pivot = self.df.pivot_table(values=column, \n",
    "                                   index=self.df.index.month, \n",
    "                                   columns=self.df.index.year, \n",
    "                                   aggfunc='mean')\n",
    "        \n",
    "        plt.figure(figsize=(12, 6))\n",
    "        sns.heatmap(pivot, annot=True, fmt='.2%', cmap='RdYlGn', center=0,\n",
    "                   cbar_kws={'label': 'Average Return'})\n",
    "        plt.title('Seasonal Heatmap: Average Returns by Month and Year')\n",
    "        plt.ylabel('Month')\n",
    "        plt.xlabel('Year')\n",
    "        plt.show()\n",
    "\n",
    "seasonality = SeasonalityAnalyzer(nepse_eda)\n",
    "seasonality.plot_seasonal_decomposition('Close')\n",
    "seasonality.seasonal_heatmap('Returns')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Decomposition**: Separates time-series into:\n",
    "  - **Trend**: Long-term direction\n",
    "  - **Seasonal**: Repeating pattern (annual, quarterly)\n",
    "  - **Residual**: Irregular noise\n",
    "- **Period**: 252 trading days = annual seasonality for stocks.\n",
    "- **Heatmap**: Shows if certain months consistently outperform (e.g., January effect) and how seasonality varies by year.\n",
    "\n",
    "---\n",
    "\n",
    "### **7.5.3 Decomposition Methods**\n",
    "\n",
    "```python\n",
    "def compare_decomposition_methods(df: pd.DataFrame, column: str):\n",
    "    \"\"\"\n",
    "    Compare STL vs Classical decomposition.\n",
    "    \"\"\"\n",
    "    from statsmodels.tsa.seasonal import STL\n",
    "    \n",
    "    data = df[column].dropna()\n",
    "    \n",
    "    # Classical decomposition\n",
    "    classical = seasonal_decompose(data, model='additive', period=252)\n",
    "    \n",
    "    # STL decomposition (more robust)\n",
    "    stl = STL(data, seasonal=253)  # Seasonal period\n",
    "    stl_result = stl.fit()\n",
    "    \n",
    "    # Plot comparison\n",
    "    fig, axes = plt.subplots(3, 2, figsize=(14, 10), sharex=True)\n",
    "    \n",
    "    # Classical\n",
    "    classical.trend.plot(ax=axes[0,0], title='Classical Trend')\n",
    "    classical.seasonal.plot(ax=axes[1,0], title='Classical Seasonal')\n",
    "    classical.resid.plot(ax=axes[2,0], title='Classical Residual')\n",
    "    \n",
    "    # STL\n",
    "    stl_result.trend.plot(ax=axes[0,1], title='STL Trend')\n",
    "    stl_result.seasonal.plot(ax=axes[1,1], title='STL Seasonal')\n",
    "    stl_result.resid.plot(ax=axes[2,1], title='STL Residual')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "compare_decomposition_methods(nepse_eda, 'Close')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Classical Decomposition**: Simple moving averages. Fast but assumes seasonal pattern is constant.\n",
    "- **STL (Seasonal and Trend decomposition using Loess)**: More flexible, handles changing seasonality over time, robust to outliers. Preferred for modern time-series analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.6 Visualization Best Practices**\n",
    "\n",
    "```python\n",
    "def create_publication_quality_chart(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Demonstrate best practices for financial charts.\n",
    "    \"\"\"\n",
    "    fig, ax = plt.subplots(figsize=(12, 6))\n",
    "    \n",
    "    # Data\n",
    "    data = df['Close'].tail(100)\n",
    "    ma20 = df['MA_20'].tail(100)\n",
    "    ma50 = df['MA_50'].tail(100)\n",
    "    \n",
    "    # Plotting\n",
    "    ax.plot(data.index, data, label='NABIL Close', linewidth=1.5, color='black')\n",
    "    ax.plot(ma20.index, ma20, label='MA(20)', linewidth=1, color='blue', alpha=0.8)\n",
    "    ax.plot(ma50.index, ma50, label='MA(50)', linewidth=1, color='red', alpha=0.8)\n",
    "    \n",
    "    # Annotations\n",
    "    max_idx = data.idxmax()\n",
    "    min_idx = data.idxmin()\n",
    "    ax.annotate(f'High: {data.max():.0f}', \n",
    "                xy=(max_idx, data.max()), \n",
    "                xytext=(10, 10), textcoords='offset points',\n",
    "                bbox=dict(boxstyle='round,pad=0.3', facecolor='yellow', alpha=0.7),\n",
    "                arrowprops=dict(arrowstyle='->', connectionstyle='arc3,rad=0'))\n",
    "    \n",
    "    # Styling\n",
    "    ax.set_title('NEPSE Stock Price with Moving Averages', fontsize=14, fontweight='bold', pad=20)\n",
    "    ax.set_xlabel('Date', fontsize=12)\n",
    "    ax.set_ylabel('Price (NPR)', fontsize=12)\n",
    "    ax.grid(True, alpha=0.3, linestyle='--')\n",
    "    ax.legend(loc='upper left', framealpha=0.9)\n",
    "    \n",
    "    # Remove top and right spines\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    \n",
    "    # Format y-axis as currency\n",
    "    ax.yaxis.set_major_formatter(plt.FuncFormatter(lambda x, p: f'Rs.{x:,.0f}'))\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "create_publication_quality_chart(nepse_eda)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Best Practices**:\n",
    "  - Descriptive title with units\n",
    "  - Clear legend with meaningful labels\n",
    "  - Grid for readability (light, dashed)\n",
    "  - Annotations for key events/extremes\n",
    "  - Remove chart junk (top/right borders)\n",
    "  - Proper formatting (currency symbols, dates)\n",
    "  - Color blindness friendly (avoid red/green only distinctions)\n",
    "\n",
    "---\n",
    "\n",
    "## **7.7 Interactive Visualization**\n",
    "\n",
    "```python\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "def create_interactive_chart(df: pd.DataFrame):\n",
    "    \"\"\"\n",
    "    Create Plotly interactive chart for web dashboards.\n",
    "    \"\"\"\n",
    "    fig = make_subplots(rows=2, cols=1, shared_xaxes=True, \n",
    "                       vertical_spacing=0.03, \n",
    "                       row_heights=[0.7, 0.3],\n",
    "                       subplot_titles=('Price', 'Volume'))\n",
    "    \n",
    "    # Candlestick\n",
    "    fig.add_trace(go.Candlestick(\n",
    "        x=df.index,\n",
    "        open=df['Open'],\n",
    "        high=df['High'],\n",
    "        low=df['Low'],\n",
    "        close=df['Close'],\n",
    "        name='OHLC'\n",
    "    ), row=1, col=1)\n",
    "    \n",
    "    # Volume\n",
    "    colors = ['green' if df['Close'].iloc[i] >= df['Open'].iloc[i] else 'red' \n",
    "             for i in range(len(df))]\n",
    "    \n",
    "    fig.add_trace(go.Bar(\n",
    "        x=df.index,\n",
    "        y=df['Volume'],\n",
    "        marker_color=colors,\n",
    "        name='Volume'\n",
    "    ), row=2, col=1)\n",
    "    \n",
    "    # Layout\n",
    "    fig.update_layout(\n",
    "        title='NEPSE Interactive Chart',\n",
    "        yaxis_title='Price (NPR)',\n",
    "        xaxis_title='Date',\n",
    "        xaxis_rangeslider_visible=False,  # Hide range slider\n",
    "        height=600\n",
    "    )\n",
    "    \n",
    "    fig.show()\n",
    "\n",
    "# Uncomment to run (requires plotly)\n",
    "# create_interactive_chart(nepse_eda.tail(100))\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Plotly**: Interactive web-based visualization. Users can zoom, pan, hover for tooltips, toggle traces.\n",
    "- **Candlestick**: Standard financial chart showing OHLC in single visual element.\n",
    "- **Subplots**: Share x-axis (time) between price and volume.\n",
    "- **Dashboards**: Export to HTML or integrate with Dash/Streamlit for interactive dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.8 Automated EDA Reports**\n",
    "\n",
    "```python\n",
    "def generate_automated_report(df: pd.DataFrame, symbol: str):\n",
    "    \"\"\"\n",
    "    Generate comprehensive automated report using pandas-profiling style logic.\n",
    "    (Simplified version for demonstration)\n",
    "    \"\"\"\n",
    "    from pandas_profiling import ProfileReport  # pip install pandas-profiling\n",
    "    \n",
    "    # Create profile\n",
    "    profile = ProfileReport(df, title=f'NEPSE {symbol} Data Profile', \n",
    "                           explorative=True)\n",
    "    \n",
    "    # Save to HTML\n",
    "    profile.to_file(f'nepse_{symbol}_report.html')\n",
    "    print(f\"Report saved to nepse_{symbol}_report.html\")\n",
    "\n",
    "# Alternative: Sweetviz\n",
    "def generate_sweetviz_report(df: pd.DataFrame, symbol: str):\n",
    "    \"\"\"Generate comparison report using sweetviz.\"\"\"\n",
    "    import sweetviz as sv\n",
    "    \n",
    "    report = sv.analyze(df)\n",
    "    report.show_html(f'nepse_{symbol}_sweetviz.html')\n",
    "\n",
    "print(\"Automated report functions defined\")\n",
    "print(\"These libraries generate comprehensive HTML reports with:\")\n",
    "print(\"- Distribution analysis for all columns\")\n",
    "print(\"- Correlation matrices\")\n",
    "print(\"- Missing value analysis\")\n",
    "print(\"- Sample data preview\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Automated EDA**: Libraries like `pandas-profiling` (now ydata-profiling) and `sweetviz` generate comprehensive HTML reports with one line of code.\n",
    "- **Use Cases**: Quick data quality checks, sharing initial findings with stakeholders, documentation.\n",
    "- **Limitations**: Generic—may miss domain-specific insights (e.g., OHLC relationships). Always supplement with manual analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.9 Communicating Insights**\n",
    "\n",
    "```python\n",
    "def generate_executive_summary(df: pd.DataFrame, symbol: str, findings: list):\n",
    "    \"\"\"\n",
    "    Generate text summary for non-technical stakeholders.\n",
    "    \"\"\"\n",
    "    summary = f\"\"\"\n",
    "    NEPSE STOCK ANALYSIS EXECUTIVE SUMMARY\n",
    "    ======================================\n",
    "    Symbol: {symbol}\n",
    "    Analysis Period: {df.index.min().strftime('%Y-%m-%d')} to {df.index.max().strftime('%Y-%m-%d')}\n",
    "    Trading Days: {len(df)}\n",
    "    \n",
    "    KEY METRICS:\n",
    "    - Average Closing Price: Rs.{df['Close'].mean():,.2f}\n",
    "    - Volatility (Annualized): {df['Returns'].std() * np.sqrt(252) * 100:.1f}%\n",
    "    - Total Return: {((df['Close'].iloc[-1] / df['Close'].iloc[0]) - 1) * 100:.1f}%\n",
    "    - Sharpe Ratio: {(df['Returns'].mean() / df['Returns'].std()) * np.sqrt(252):.2f}\n",
    "    \n",
    "    KEY FINDINGS:\n",
    "    \"\"\"\n",
    "    \n",
    "    for i, finding in enumerate(findings, 1):\n",
    "        summary += f\"{i}. {finding}\\n\"\n",
    "    \n",
    "    summary += f\"\"\"\n",
    "    RECOMMENDATIONS:\n",
    "    - {'Strong trend detected' if abs(df['Close'].corr(pd.Series(range(len(df))))) > 0.7 else 'No strong trend detected'}\n",
    "    - {'High seasonality observed' if df.groupby(df.index.month)['Returns'].std().mean() > 0.02 else 'Seasonal effects minimal'}\n",
    "    - Data Quality: {'Excellent' if df.isnull().sum().sum() == 0 else 'Requires attention'}\n",
    "    \"\"\"\n",
    "    \n",
    "    return summary\n",
    "\n",
    "# Generate summary\n",
    "findings = [\n",
    "    \"Price shows strong upward trend with 15% annual growth\",\n",
    "    \"Volume spikes correlate with positive returns (correlation: 0.35)\",\n",
    "    \"October historically shows highest volatility\",\n",
    "    \"No significant multicollinearity detected in features\"\n",
    "]\n",
    "\n",
    "summary = generate_executive_summary(nepse_eda, 'NABIL', findings)\n",
    "print(summary)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Executive Summary**: Translates technical findings into business language.\n",
    "- **Key Elements**:\n",
    "  - Period and scope\n",
    "  - Key metrics (returns, risk, Sharpe)\n",
    "  - Actionable findings\n",
    "  - Data quality assessment\n",
    "- **Audience Adaptation**: Technical details (autocorrelation, kurtosis) for data scientists; trends and risks for executives.\n",
    "\n",
    "---\n",
    "\n",
    "## **7.10 EDA Checklist**\n",
    "\n",
    "```python\n",
    "class EDAChecklist:\n",
    "    \"\"\"\n",
    "    Comprehensive checklist for EDA completion.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.checklist = {\n",
    "            'Data Understanding': [\n",
    "                'Loaded data shape verified',\n",
    "                'Column types inspected',\n",
    "                'Date range confirmed',\n",
    "                'Missing values quantified',\n",
    "                'Duplicates checked'\n",
    "            ],\n",
    "            'Univariate Analysis': [\n",
    "                'Distributions plotted (histograms, box plots)',\n",
    "                'Summary statistics calculated',\n",
    "                'Normality tests performed',\n",
    "                'Outliers identified and explained'\n",
    "            ],\n",
    "            'Bivariate Analysis': [\n",
    "                'Correlation matrix computed',\n",
    "                'Scatter plots for key relationships',\n",
    "                'Cross-correlations for time lags',\n",
    "                'Multicollinearity assessed'\n",
    "            ],\n",
    "            'Time-Series Specific': [\n",
    "                'Trend visualized and quantified',\n",
    "                'Seasonality detected',\n",
    "                'Stationarity tested (ADF test)',\n",
    "                'Autocorrelation analyzed (ACF/PACF)',\n",
    "                'Volatility clustering checked'\n",
    "            ],\n",
    "            'Quality Assessment': [\n",
    "                'Data cleaning validated',\n",
    "                'Feature engineering opportunities identified',\n",
    "                'Missing data mechanism understood',\n",
    "                'Outlier treatment justified'\n",
    "            ],\n",
    "            'Documentation': [\n",
    "                'Visualizations saved',\n",
    "                'Key findings documented',\n",
    "                'Executive summary written',\n",
    "                'Code commented and reproducible'\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def display_checklist(self):\n",
    "        \"\"\"Display formatted checklist.\"\"\"\n",
    "        print(\"=\"*60)\n",
    "        print(\"EDA COMPLETION CHECKLIST\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        for category, items in self.checklist.items():\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(\"-\" * len(category))\n",
    "            for item in items:\n",
    "                print(f\"  [ ] {item}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"Tip: Print this checklist and mark items as you complete them\")\n",
    "\n",
    "checklist = EDAChecklist()\n",
    "checklist.display_checklist()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Structured Approach**: Ensures no critical step is missed.\n",
    "- **Categories**: Organized from basic (data loading) to advanced (time-series specific).\n",
    "- **Reproducibility**: Checklist ensures analysis can be replicated or audited.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we covered comprehensive EDA for time-series prediction:\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Systematic Process**: EDA follows phases—understanding, univariate, bivariate, temporal, quality assessment.\n",
    "\n",
    "2. **Univariate Analysis**: Examine distributions (histograms, Q-Q plots), calculate moments (skewness, kurtosis), test normality. Financial data is rarely normal.\n",
    "\n",
    "3. **Bivariate Analysis**: Correlation matrices (Pearson vs Spearman), scatter plots, cross-correlations for lead-lag relationships. Watch for multicollinearity.\n",
    "\n",
    "4. **Multivariate Analysis**: PCA for dimensionality reduction, understanding feature loadings.\n",
    "\n",
    "5. **Time-Series Specific**:\n",
    "   - **Trend**: Linear regression, HP filter, detrending\n",
    "   - **Seasonality**: Decomposition (STL preferred), seasonal heatmaps\n",
    "   - **Decomposition**: Separate trend, seasonal, residual components\n",
    "\n",
    "6. **Visualization**: Publication-quality standards (clear labels, annotations, removed chart junk), interactive Plotly for dashboards.\n",
    "\n",
    "7. **Automation**: Use pandas-profiling for rapid assessment, but supplement with domain-specific analysis.\n",
    "\n",
    "8. **Communication**: Executive summaries translate technical findings into actionable business insights.\n",
    "\n",
    "9. **Checklist**: Structured approach ensures completeness and reproducibility.\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "In Chapter 8, we will cover **Data Storage and Management**, including:\n",
    "- Storage architecture decisions (files vs databases)\n",
    "- Time-series database optimization (InfluxDB, TimescaleDB)\n",
    "- Data partitioning strategies\n",
    "- Backup and recovery procedures\n",
    "- Cloud storage solutions\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 7**\n",
    "\n",
    "---\n",
    "\n",
    "*This chapter provided the analytical foundation for understanding NEPSE data before building models. The visualizations and statistical tests demonstrated here should be run before any model development to ensure data quality and to identify predictive features.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
