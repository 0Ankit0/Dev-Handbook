{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 93: Foundation Models for Time-Series\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand what foundation models are and how they differ from traditional task\u2011specific models.\n",
    "- Identify the emerging landscape of time\u2011series foundation models (Chronos, Lag\u2011Llama, Moirai, etc.).\n",
    "- Explain the pre\u2011training strategies used for time\u2011series foundation models (e.g., contrastive learning, masked modeling).\n",
    "- Apply fine\u2011 tuning techniques to adapt a foundation model to a specific dataset like NEPSE.\n",
    "- Leverage zero\u2011shot forecasting capabilities of foundation models for quick prototyping.\n",
    "- Evaluate the strengths and limitations of foundation models compared to traditional approaches.\n",
    "- Understand the computational requirements and practical considerations for deploying foundation models.\n",
    "- Explore future directions and ongoing research in this rapidly evolving field.\n",
    "\n",
    "---\n",
    "\n",
    "## **93.1 Introduction to Foundation Models**\n",
    "\n",
    "Foundation models are large\u2011scale machine learning models pre\u2011trained on vast amounts of data, which can then be adapted (fine\u2011tuned) for a wide range of downstream tasks. The term was popularised by models like GPT (for text) and DALL\u2011E (for images). These models capture general patterns and representations that transfer effectively to new tasks with minimal additional training.\n",
    "\n",
    "In the time\u2011series domain, foundation models are an emerging paradigm. Instead of training a model from scratch for each new forecasting task (e.g., NEPSE stock prediction, retail sales, energy demand), we can leverage a model pre\u2011trained on a diverse collection of time series from many domains. This offers several potential benefits:\n",
    "\n",
    "- **Reduced training time and data requirements**: Fine\u2011tuning a foundation model often requires less data and compute than training from scratch.\n",
    "- **Improved performance on small datasets**: The model brings prior knowledge that helps generalise.\n",
    "- **Zero\u2011shot forecasting**: In some cases, the model can make reasonable predictions on a new dataset without any fine\u2011tuning.\n",
    "- **Unified architecture**: A single model can handle many different time\u2011series tasks (forecasting, classification, anomaly detection).\n",
    "\n",
    "However, foundation models also come with challenges: they are computationally expensive to pre\u2011train, may require careful fine\u2011tuning, and can be opaque in their reasoning.\n",
    "\n",
    "In this chapter, we will explore the current state of time\u2011series foundation models, how to use them for the NEPSE prediction task, and what the future might hold.\n",
    "\n",
    "---\n",
    "\n",
    "## **93.2 The Landscape of Time\u2011Series Foundation Models**\n",
    "\n",
    "Several foundation models have recently been proposed for time series. Let's briefly review the most prominent ones.\n",
    "\n",
    "### **93.2.1 Chronos (Amazon)**\n",
    "Chronos is a family of pre\u2011trained time\u2011series forecasting models based on the T5 architecture (encoder\u2011decoder). It is trained on a large corpus of public time\u2011series data from various domains. Chronos tokenises time series by scaling and quantising values into a fixed vocabulary, then treats forecasting as a language modeling task.\n",
    "\n",
    "**Key features**:\n",
    "- Available in different sizes (tiny, mini, small, base, large).\n",
    "- Supports probabilistic forecasting (generates multiple samples).\n",
    "- Can be used zero\u2011shot or fine\u2011tuned.\n",
    "\n",
    "### **93.2.2 Lag\u2011Llama (University of Oxford)**\n",
    "Lag\u2011Llama is a foundation model for univariate probabilistic forecasting. It is based on the LLaMA architecture (decoder\u2011only) and uses lagged features as inputs. It is pre\u2011trained on a large collection of time series and can produce predictions with uncertainty estimates.\n",
    "\n",
    "**Key features**:\n",
    "- Decoder\u2011only transformer.\n",
    "- Uses lagged values as covariates.\n",
    "- Strong zero\u2011shot performance on many benchmarks.\n",
    "\n",
    "### **93.2.3 Moirai (Salesforce)**\n",
    "Moirai is a multivariate time\u2011series foundation model that can handle multiple frequencies and missing values. It uses a transformer architecture with a novel \"masked encoder\" pre\u2011training objective.\n",
    "\n",
    "**Key features**:\n",
    "- Supports multivariate forecasting.\n",
    "- Handles irregularly sampled data.\n",
    "- Pre\u2011trained on a massive dataset (Lotka\u2011Volterra).\n",
    "\n",
    "### **93.2.4 Others**\n",
    "- **TimesNet**: A task\u2011specific model that has inspired foundation approaches.\n",
    "- **UniTime**: A unified model for multiple time\u2011series tasks.\n",
    "- **GPT\u20114 for time series**: Researchers have explored prompting large language models with numerical data, though results are mixed.\n",
    "\n",
    "For the NEPSE system, we will focus on Chronos and Lag\u2011Llama as they are publicly available and well\u2011documented.\n",
    "\n",
    "---\n",
    "\n",
    "## **93.3 Pre\u2011training Strategies**\n",
    "\n",
    "Understanding how these models are pre\u2011trained helps in using them effectively.\n",
    "\n",
    "### **93.3.1 Data Curation**\n",
    "Foundation models are trained on massive, diverse collections of time series. For example, Chronos uses data from:\n",
    "\n",
    "- Monash Time Series Forecasting Repository\n",
    "- M4, M5 competitions\n",
    "- UCR Time Series Classification Archive\n",
    "- Synthetic data\n",
    "\n",
    "The diversity ensures the model learns general temporal patterns: trends, seasonality, noise, etc.\n",
    "\n",
    "### **93.3.2 Tokenization**\n",
    "Time series are continuous, but transformers expect discrete tokens. Chronos addresses this by:\n",
    "\n",
    "1. Scaling each time series (e.g., by its mean absolute value).\n",
    "2. Quantising the scaled values into a fixed number of bins (e.g., 4096 bins).\n",
    "3. Representing each observation as a token (the bin index).\n",
    "\n",
    "This transforms forecasting into a next\u2011token prediction task, similar to language modeling.\n",
    "\n",
    "### **93.3.3 Masked Modeling (Moirai)**\n",
    "Moirai uses a masked autoencoder approach: random patches of the time series are masked, and the model learns to reconstruct them. This forces the model to capture dependencies across time.\n",
    "\n",
    "### **93.3.4 Contrastive Learning**\n",
    "Some models use contrastive objectives: positive pairs (e.g., different views of the same series) are pulled together, while negative pairs are pushed apart. This learns robust representations.\n",
    "\n",
    "### **93.3.5 Next\u2011Step Prediction (Lag\u2011Llama)**\n",
    "Lag\u2011Llama is trained to predict the next value given a context window of lagged values, using a causal transformer.\n",
    "\n",
    "The result of pre\u2011training is a set of weights that capture general time\u2011series patterns. These weights can then be fine\u2011tuned on specific datasets like NEPSE.\n",
    "\n",
    "---\n",
    "\n",
    "## **93.4 Zero\u2011Shot Forecasting**\n",
    "\n",
    "One of the most exciting capabilities of foundation models is zero\u2011shot forecasting: making predictions on a new dataset without any additional training.\n",
    "\n",
    "### **93.4.1 When Zero\u2011Shot Works**\n",
    "Zero\u2011shot works well when the new dataset's patterns are similar to those seen during pre\u2011training. For example, if the model has seen many retail sales time series, it may generalise to a new retail dataset. For NEPSE, if the pre\u2011training data included stock prices, zero\u2011shot might be reasonable.\n",
    "\n",
    "### **93.4.2 Example: Zero\u2011Shot with Chronos**\n",
    "\n",
    "```python\n",
    "# pip install git+https://github.com/amazon-science/chronos-forecasting.git\n",
    "import torch\n",
    "from chronos import ChronosPipeline\n",
    "\n",
    "# Load the pre-trained Chronos model (small version for demo)\n",
    "pipeline = ChronosPipeline.from_pretrained(\n",
    "    \"amazon/chronos-t5-small\",\n",
    "    device_map=\"cuda\" if torch.cuda.is_available() else \"cpu\",\n",
    "    torch_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# Prepare NEPSE data (a single time series)\n",
    "# Assume we have a pandas Series 'close_prices' with daily close prices\n",
    "context = torch.tensor(close_prices.values[-100:], dtype=torch.float32)  # last 100 days\n",
    "\n",
    "# Generate 24 forecast samples (24 days ahead)\n",
    "forecast_samples = pipeline.predict(\n",
    "    context=context,\n",
    "    prediction_length=24,\n",
    "    num_samples=20,  # number of samples for probabilistic forecast\n",
    ")\n",
    "\n",
    "# forecast_samples shape: (num_samples, prediction_length)\n",
    "# Take median as point forecast\n",
    "median_forecast = np.median(forecast_samples.numpy(), axis=0)\n",
    "\n",
    "# Plot\n",
    "import matplotlib.pyplot as plt\n",
    "plt.plot(range(-100, 0), context, label=\"History\")\n",
    "plt.plot(range(1, 25), median_forecast, label=\"Chronos forecast\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- Chronos is loaded as a pipeline. The model expects a context window (the last N observations) and outputs samples of future values.\n",
    "- The `predict` method returns multiple samples, allowing us to compute quantiles for probabilistic forecasting.\n",
    "- In zero\u2011shot mode, we use the model as is, with no fine\u2011tuning. The quality of the forecast depends on how similar NEPSE is to the pre\u2011training data.\n",
    "\n",
    "---\n",
    "\n",
    "## **93.5 Fine\u2011Tuning for NEPSE**\n",
    "\n",
    "If zero\u2011shot performance is insufficient, we can fine\u2011tune the foundation model on historical NEPSE data. Fine\u2011tuning adapts the model's weights to the specific characteristics of our dataset.\n",
    "\n",
    "### **93.5.1 When to Fine\u2011Tune**\n",
    "- You have enough historical data (at least a few hundred observations).\n",
    "- The data distribution differs from the pre\u2011training distribution (e.g., unique patterns in NEPSE).\n",
    "- You need higher accuracy than zero\u2011shot provides.\n",
    "\n",
    "### **93.5.2 Fine\u2011Tuning Chronos**\n",
    "\n",
    "Chronos supports fine\u2011tuning through its Hugging Face integration.\n",
    "\n",
    "```python\n",
    "from chronos import ChronosPipeline\n",
    "from chronos.utils import ChronosDataset\n",
    "from transformers import TrainingArguments, Trainer\n",
    "import torch\n",
    "\n",
    "# Load the base model\n",
    "model = ChronosPipeline.from_pretrained(\"amazon/chronos-t5-small\")\n",
    "\n",
    "# Prepare dataset (list of time series)\n",
    "# For NEPSE, we might have multiple stocks, each as a separate time series\n",
    "train_series = [df[df['symbol'] == sym]['close'].values for sym in symbols]\n",
    "\n",
    "# Chronos expects a specific format\n",
    "dataset = ChronosDataset(\n",
    "    series=train_series,\n",
    "    context_length=512,  # max context length\n",
    "    prediction_length=24, # forecast horizon\n",
    "    tokenizer=model.tokenizer,\n",
    "    freq='D'  # daily frequency\n",
    ")\n",
    "\n",
    "# Define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./chronos-nepse\",\n",
    "    per_device_train_batch_size=4,\n",
    "    learning_rate=5e-5,\n",
    "    num_train_epochs=3,\n",
    "    logging_steps=10,\n",
    "    save_steps=100,\n",
    "    evaluation_strategy=\"no\",\n",
    "    save_total_limit=2,\n",
    "    remove_unused_columns=False,\n",
    ")\n",
    "\n",
    "# Trainer\n",
    "trainer = Trainer(\n",
    "    model=model.model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset,\n",
    ")\n",
    "\n",
    "# Fine\u2011tune\n",
    "trainer.train()\n",
    "\n",
    "# Save the fine\u2011tuned model\n",
    "model.save_pretrained(\"./chronos-nepse-finetuned\")\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- The `ChronosDataset` handles tokenization and formatting.\n",
    "- We train for a few epochs with a low learning rate (typical for fine\u2011tuning).\n",
    "- After fine\u2011tuning, the model is saved and can be loaded for inference as before.\n",
    "\n",
    "### **93.5.3 Fine\u2011Tuning Lag\u2011Llama**\n",
    "\n",
    "Lag\u2011Llama can be fine\u2011tuned using its PyTorch implementation.\n",
    "\n",
    "```python\n",
    "# Lag\u2011Llama fine\u2011tuning (simplified)\n",
    "from lag_llama import LagLlamaModel, LagLlamaConfig\n",
    "import pytorch_lightning as pl\n",
    "\n",
    "config = LagLlamaConfig(\n",
    "    context_length=256,\n",
    "    prediction_length=24,\n",
    "    num_layers=6,\n",
    "    d_model=256,\n",
    "    n_heads=8,\n",
    ")\n",
    "\n",
    "model = LagLlamaModel(config)\n",
    "\n",
    "# Prepare data loader for NEPSE\n",
    "train_loader = ...  # yields (context, target) pairs\n",
    "\n",
    "# Trainer\n",
    "trainer = pl.Trainer(max_epochs=10, gpus=1 if torch.cuda.is_available() else 0)\n",
    "trainer.fit(model, train_loader)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **93.6 Evaluation and Comparison**\n",
    "\n",
    "When using a foundation model, we should compare its performance against traditional approaches (e.g., XGBoost, ARIMA, LSTM trained from scratch).\n",
    "\n",
    "### **93.6.1 Metrics**\n",
    "Use the same metrics as before: MAE, RMSE, MAPE. For probabilistic forecasts, use CRPS or pinball loss.\n",
    "\n",
    "### **93.6.2 Experimental Setup**\n",
    "- **Zero\u2011shot**: Evaluate directly on test data (no training).\n",
    "- **Fine\u2011tuned**: Train on training period, evaluate on test period.\n",
    "- **Baseline**: Train XGBoost/LSTM on same training period.\n",
    "\n",
    "**Example evaluation**:\n",
    "\n",
    "```python\n",
    "def evaluate_model(model_fn, X_train, y_train, X_test, y_test):\n",
    "    # model_fn returns predictions\n",
    "    y_pred = model_fn(X_train, y_train, X_test)\n",
    "    return mean_absolute_error(y_test, y_pred)\n",
    "\n",
    "# Zero\u2011shot Chronos\n",
    "def chronos_zero_shot(context):\n",
    "    # use Chronos as above\n",
    "    ...\n",
    "\n",
    "# Fine\u2011tuned Chronos\n",
    "def chronos_finetuned(context):\n",
    "    # load fine\u2011tuned model\n",
    "    ...\n",
    "\n",
    "# XGBoost baseline\n",
    "def xgboost_baseline(X_train, y_train, X_test):\n",
    "    model = xgb.XGBRegressor()\n",
    "    model.fit(X_train, y_train)\n",
    "    return model.predict(X_test)\n",
    "\n",
    "# Compare\n",
    "mae_zs = evaluate_model(chronos_zero_shot, ...)\n",
    "mae_ft = evaluate_model(chronos_finetuned, ...)\n",
    "mae_xgb = evaluate_model(xgboost_baseline, ...)\n",
    "\n",
    "print(f\"Zero\u2011shot MAE: {mae_zs:.2f}\")\n",
    "print(f\"Fine\u2011tuned MAE: {mae_ft:.2f}\")\n",
    "print(f\"XGBoost MAE: {mae_xgb:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **93.7 Computational Requirements and Practical Considerations**\n",
    "\n",
    "### **93.7.1 Hardware**\n",
    "Foundation models are large. Even the \"small\" Chronos model has ~50 million parameters; the large version has billions. Inference requires a GPU for reasonable speed, and fine\u2011tuning definitely requires one (or multiple). For the NEPSE system, a single T4 GPU (available on many cloud platforms) is sufficient for the small/medium models.\n",
    "\n",
    "### **93.7.2 Latency**\n",
    "Inference with foundation models is slower than with a simple XGBoost. For real\u2011time APIs, you may need to batch requests or use a smaller model. For daily batch predictions, latency is less critical.\n",
    "\n",
    "### **93.7.3 Cold Start**\n",
    "Zero\u2011shot eliminates the need for training data, which is useful for new stocks with little history. However, the model may not capture stock\u2011specific nuances.\n",
    "\n",
    "### **93.7.4 Interpretability**\n",
    "Foundation models are black boxes. If interpretability is required (e.g., for regulatory reasons), they may not be suitable. However, techniques like attention visualisation can provide some insight.\n",
    "\n",
    "### **93.7.5 Licensing**\n",
    "Check the license of each foundation model. Some are for research use only; others are Apache 2.0. For a commercial NEPSE system, ensure compliance.\n",
    "\n",
    "---\n",
    "\n",
    "## **93.8 Limitations and Challenges**\n",
    "\n",
    "Foundation models are not a silver bullet. Be aware of:\n",
    "\n",
    "- **Domain shift**: If NEPSE behaves very differently from the pre\u2011training data (e.g., extreme volatility, circuit breakers), zero\u2011shot may fail.\n",
    "- **Fine\u2011tuning data requirements**: Fine\u2011tuning still needs sufficient data; if you have very little, you may overfit.\n",
    "- **Catastrophic forgetting**: Fine\u2011tuning can cause the model to forget general knowledge. Use a low learning rate and possibly freeze early layers.\n",
    "- **Cost**: Pre\u2011training is prohibitively expensive for most organisations. We rely on publicly released models.\n",
    "- **Evaluation complexity**: With multiple foundation models emerging, choosing the right one requires careful benchmarking.\n",
    "\n",
    "---\n",
    "\n",
    "## **93.9 Future Directions**\n",
    "\n",
    "The field of time\u2011series foundation models is evolving rapidly. Expect to see:\n",
    "\n",
    "- **Larger, more diverse pre\u2011training datasets**: Including more financial, economic, and climate data.\n",
    "- **Multivariate foundation models**: Handling multiple correlated time series (e.g., all NEPSE stocks together).\n",
    "- **Integration with LLMs**: Using language models to incorporate textual information (news, reports) alongside time series.\n",
    "- **Efficient fine\u2011tuning techniques**: Like LoRA (Low\u2011Rank Adaptation) to adapt models with minimal parameters.\n",
    "- **On\u2011device deployment**: Smaller, distilled versions for edge devices.\n",
    "- **Standardised benchmarks**: To fairly compare foundation models.\n",
    "\n",
    "For the NEPSE system, keeping an eye on these developments will help you decide when to upgrade.\n",
    "\n",
    "---\n",
    "\n",
    "## **93.10 Best Practices**\n",
    "\n",
    "1. **Start with zero\u2011shot**: It's quick and gives a baseline. If performance is acceptable, you may not need fine\u2011tuning.\n",
    "2. **Benchmark against simple models**: Ensure the added complexity is justified.\n",
    "3. **Use fine\u2011tuning judiciously**: Monitor for overfitting; use a validation set.\n",
    "4. **Monitor for drift**: Even foundation models can suffer from concept drift. Retrain or fine\u2011tune periodically.\n",
    "5. **Consider ensemble**: Combine a foundation model with a traditional model for robustness.\n",
    "6. **Document everything**: Which model version, fine\u2011tuning data, and hyperparameters were used.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored the emerging paradigm of foundation models for time\u2011series forecasting. We introduced several models (Chronos, Lag\u2011Llama, Moirai) and explained their pre\u2011training strategies. Using the NEPSE prediction system as an example, we demonstrated zero\u2011shot forecasting and fine\u2011tuning with Chronos. We discussed the computational requirements, limitations, and future directions.\n",
    "\n",
    "Foundation models represent a significant shift in how we approach time\u2011series prediction. They offer the promise of general\u2011purpose forecasting models that can be adapted to new tasks with minimal effort. For the NEPSE system, they provide a powerful new tool in the forecasting toolbox, complementing the traditional approaches we've built throughout this handbook.\n",
    "\n",
    "In the next chapter, we will explore **Large Language Models for Time\u2011Series**, diving deeper into how models like GPT can be used for numerical prediction and reasoning about time\u2011series data.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 93**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../12. industry_best_practices_and_standards/92. troubleshooting_and_debugging.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='94. large_language_models_for_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}