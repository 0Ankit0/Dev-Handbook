{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 98: Ethical AI and Responsible ML\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the core principles of ethical AI: fairness, accountability, transparency, and explainability (FATE).\n",
    "- Identify potential sources of bias in time‑series prediction systems, from data collection to model deployment.\n",
    "- Apply fairness metrics to evaluate models and detect disparities across groups (e.g., stock sectors, market capitalisation).\n",
    "- Implement techniques to mitigate bias, such as reweighting, adversarial debiasing, and fairness constraints.\n",
    "- Ensure transparency by documenting models thoroughly (model cards) and providing explanations for predictions (e.g., SHAP, LIME).\n",
    "- Navigate the regulatory landscape for AI in finance (e.g., GDPR, SEC guidelines, algorithmic accountability).\n",
    "- Build accountability into the system through audits, logging, and human oversight.\n",
    "- Consider the broader societal impact of predictive systems, including potential for market manipulation or unfair advantage.\n",
    "- Develop a framework for responsible AI that evolves with the system and the field.\n",
    "\n",
    "---\n",
    "\n",
    "## **98.1 Introduction to Ethical AI and Responsible ML**\n",
    "\n",
    "As machine learning systems become more integrated into high‑stakes domains like finance, healthcare, and criminal justice, the ethical implications of their decisions come to the forefront. The NEPSE stock prediction system, while seemingly benign, can have significant real‑world impact: inaccurate or biased predictions could lead to financial losses, unfair market advantages, or even systemic risk.\n",
    "\n",
    "Ethical AI is not just about avoiding harm; it is about building systems that are **fair, accountable, transparent, and explainable**. These principles are often abbreviated as **FATE**. In this chapter, we will explore what these principles mean in the context of time‑series prediction, and how to embed them into the development lifecycle.\n",
    "\n",
    "We will use the NEPSE system as a concrete example to discuss:\n",
    "\n",
    "- **Fairness**: Does the model perform equally well for all stocks, or does it favour certain sectors (e.g., banking vs. hydropower)?\n",
    "- **Accountability**: Who is responsible when a prediction leads to a poor investment decision?\n",
    "- **Transparency**: Can we explain why the model predicted a price increase?\n",
    "- **Privacy**: Does the system protect sensitive data?\n",
    "\n",
    "Addressing these questions is not only ethical but increasingly a legal requirement. Regulations like the EU's General Data Protection Regulation (GDPR) include provisions for algorithmic decision‑making, and financial regulators are scrutinising automated trading systems.\n",
    "\n",
    "---\n",
    "\n",
    "## **98.2 Bias in Time‑Series Prediction Systems**\n",
    "\n",
    "Bias can enter a system at any stage. Let's examine common sources.\n",
    "\n",
    "### **98.2.1 Data Bias**\n",
    "\n",
    "The data used to train the model may not be representative of the real world. For the NEPSE system:\n",
    "\n",
    "- **Historical bias**: The training data may cover only certain market conditions (e.g., bull markets). The model may perform poorly in bear markets or during crises.\n",
    "- **Sector imbalance**: If the dataset contains more data for banking stocks than for hydropower stocks, the model may be less accurate for the latter.\n",
    "- **Survivorship bias**: If we only include stocks that are currently listed, we ignore those that were delisted (e.g., due to bankruptcy). This can lead to overly optimistic predictions.\n",
    "\n",
    "### **98.2.2 Feature Bias**\n",
    "\n",
    "Features themselves can encode bias. For example:\n",
    "\n",
    "- Using past returns as a feature may perpetuate momentum effects that favour large‑cap stocks.\n",
    "- Volume features may be less reliable for illiquid stocks, leading to different prediction quality.\n",
    "\n",
    "### **98.2.3 Model Bias**\n",
    "\n",
    "The choice of algorithm and hyperparameters can also introduce bias. Some models may systematically under‑ or over‑predict for certain groups.\n",
    "\n",
    "### **98.2.4 Deployment Bias**\n",
    "\n",
    "Even a fair model can be used unfairly. For example, if the predictions are only accessible to a subset of traders, it creates an unfair advantage.\n",
    "\n",
    "---\n",
    "\n",
    "## **98.3 Fairness Metrics**\n",
    "\n",
    "To detect bias, we need quantitative metrics. In classification tasks, common fairness metrics include:\n",
    "\n",
    "- **Demographic parity**: The prediction rate (e.g., positive outcome) should be similar across groups.\n",
    "- **Equal opportunity**: The true positive rate should be similar across groups.\n",
    "- **Predictive parity**: The positive predictive value should be similar across groups.\n",
    "\n",
    "For regression tasks (like price prediction), we can adapt these ideas:\n",
    "\n",
    "- **Group‑wise error**: Compare MAE or RMSE for different groups (e.g., by sector). Significant differences indicate bias.\n",
    "- **Distributional parity**: The distribution of predictions should be similar across groups.\n",
    "\n",
    "Let's implement a fairness evaluation for the NEPSE system.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "def evaluate_fairness(y_true, y_pred, groups):\n",
    "    \"\"\"\n",
    "    Compute per‑group MAE and return a fairness report.\n",
    "    \n",
    "    Parameters:\n",
    "    y_true : array-like, true values\n",
    "    y_pred : array-like, predicted values\n",
    "    groups : array-like, group labels (e.g., sector)\n",
    "    \n",
    "    Returns:\n",
    "    DataFrame with group sizes, MAE, and ratio to overall MAE.\n",
    "    \"\"\"\n",
    "    df = pd.DataFrame({'true': y_true, 'pred': y_pred, 'group': groups})\n",
    "    overall_mae = mean_absolute_error(y_true, y_pred)\n",
    "    \n",
    "    group_stats = df.groupby('group').apply(\n",
    "        lambda g: pd.Series({\n",
    "            'count': len(g),\n",
    "            'mae': mean_absolute_error(g['true'], g['pred']),\n",
    "            'mae_ratio': mean_absolute_error(g['true'], g['pred']) / overall_mae\n",
    "        })\n",
    "    ).reset_index()\n",
    "    \n",
    "    return group_stats\n",
    "\n",
    "# Example usage\n",
    "# Assume we have test data with predictions and sector labels\n",
    "# y_test, y_pred, sectors from NEPSE data\n",
    "fairness_report = evaluate_fairness(y_test, y_pred, sectors)\n",
    "print(fairness_report)\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- We compute MAE for each group (e.g., stock sector). If one group has significantly higher MAE (e.g., >1.2× overall), it suggests the model is less accurate for that group.\n",
    "- This can guide further investigation: is the group underrepresented in training? Do its features behave differently?\n",
    "\n",
    "---\n",
    "\n",
    "## **98.4 Mitigating Bias**\n",
    "\n",
    "If bias is detected, we can take steps to mitigate it.\n",
    "\n",
    "### **98.4.1 Data‑Level Mitigation**\n",
    "\n",
    "- **Resampling**: Oversample underrepresented groups or undersample overrepresented ones. For time‑series, this must be done carefully to preserve temporal order.\n",
    "- **Reweighting**: Assign higher weights to samples from disadvantaged groups during training.\n",
    "- **Data augmentation**: Generate synthetic samples for underrepresented groups (e.g., using time‑series GANs).\n",
    "\n",
    "### **98.4.2 Algorithm‑Level Mitigation**\n",
    "\n",
    "- **Adversarial debiasing**: Train a model that predicts the target while fooling an adversary that tries to predict the sensitive attribute (e.g., sector). This removes information about the group from the representations.\n",
    "- **Fairness constraints**: Incorporate fairness metrics into the loss function (e.g., penalise differences in group error).\n",
    "\n",
    "### **98.4.3 Post‑Processing Mitigation**\n",
    "\n",
    "- Adjust predictions to meet fairness criteria (e.g., by adding a group‑specific bias term). This is simple but may reduce overall accuracy.\n",
    "\n",
    "**Example: Reweighting in XGBoost**\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "# Compute sample weights inversely proportional to group representation\n",
    "group_counts = sectors.value_counts()\n",
    "sample_weights = 1.0 / group_counts[sectors].values\n",
    "\n",
    "model = xgb.XGBRegressor()\n",
    "model.fit(X_train, y_train, sample_weight=sample_weights)\n",
    "```\n",
    "\n",
    "**Example: Adversarial Debiasing (conceptual)**\n",
    "\n",
    "```python\n",
    "# Using a framework like AIF360\n",
    "from aif360.algorithms.inprocessing import AdversarialDebiasing\n",
    "\n",
    "# Prepare data in AIF360 format\n",
    "# ...\n",
    "\n",
    "debias_model = AdversarialDebiasing(...)\n",
    "debias_model.fit(train_dataset)\n",
    "predictions = debias_model.predict(test_dataset)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **98.5 Transparency and Explainability**\n",
    "\n",
    "Stakeholders need to understand why a prediction was made. This is especially important in finance, where decisions may be challenged.\n",
    "\n",
    "### **98.5.1 Model Documentation (Model Cards)**\n",
    "\n",
    "As introduced in Chapter 77, model cards summarise a model's purpose, performance, limitations, and ethical considerations. For the NEPSE system, a model card should include:\n",
    "\n",
    "- **Model details**: Type, version, training data, hyperparameters.\n",
    "- **Intended use**: What it can and cannot be used for.\n",
    "- **Factors**: Which groups were considered in evaluation (e.g., sectors, market cap).\n",
    "- **Metrics**: Overall and per‑group performance.\n",
    "- **Ethical considerations**: Potential biases, limitations.\n",
    "- **Caveats and recommendations**: How to use the model responsibly.\n",
    "\n",
    "We can generate model cards programmatically from evaluation results.\n",
    "\n",
    "### **98.5.2 Local Explanations (SHAP, LIME)**\n",
    "\n",
    "For individual predictions, we can use SHAP or LIME to show feature contributions.\n",
    "\n",
    "```python\n",
    "import shap\n",
    "\n",
    "# Train a model (e.g., XGBoost)\n",
    "model = xgb.XGBRegressor().fit(X_train, y_train)\n",
    "\n",
    "# Explain a single prediction\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_test.iloc[[0]])\n",
    "\n",
    "shap.force_plot(explainer.expected_value, shap_values[0], X_test.iloc[0])\n",
    "```\n",
    "\n",
    "This visualisation shows which features pushed the prediction up or down. For a trader, this can build trust and provide insight.\n",
    "\n",
    "### **98.5.3 Global Explanations**\n",
    "\n",
    "Feature importance (permutation or built‑in) gives a global view of which features are most influential.\n",
    "\n",
    "```python\n",
    "importances = model.feature_importances_\n",
    "feature_names = X_train.columns\n",
    "indices = np.argsort(importances)[::-1]\n",
    "\n",
    "print(\"Feature ranking:\")\n",
    "for i in range(10):\n",
    "    print(f\"{i+1}. {feature_names[indices[i]]} ({importances[indices[i]]:.4f})\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **98.6 Accountability**\n",
    "\n",
    "Accountability means assigning responsibility for the system's outcomes. This involves:\n",
    "\n",
    "- **Audit trails**: Log all predictions, including input features, model version, and timestamp. This allows post‑hoc analysis if something goes wrong.\n",
    "- **Human oversight**: For critical decisions, a human should be able to review and override the model.\n",
    "- **Clear ownership**: Designate a person or team responsible for the model's performance and ethical compliance.\n",
    "\n",
    "**Example logging**:\n",
    "\n",
    "```python\n",
    "import logging\n",
    "import json\n",
    "\n",
    "logger = logging.getLogger('prediction_log')\n",
    "\n",
    "def log_prediction(symbol, date, features, prediction, model_version):\n",
    "    log_entry = {\n",
    "        'timestamp': datetime.now().isoformat(),\n",
    "        'symbol': symbol,\n",
    "        'date': date.isoformat(),\n",
    "        'features': features.to_dict(),\n",
    "        'prediction': float(prediction),\n",
    "        'model_version': model_version\n",
    "    }\n",
    "    logger.info(json.dumps(log_entry))\n",
    "```\n",
    "\n",
    "These logs can be stored in a database and used for audits.\n",
    "\n",
    "---\n",
    "\n",
    "## **98.7 Regulatory Compliance**\n",
    "\n",
    "Different jurisdictions have regulations affecting AI systems.\n",
    "\n",
    "### **98.7.1 GDPR (Europe)**\n",
    "- **Right to explanation**: Individuals have the right to an explanation of algorithmic decisions that affect them.\n",
    "- **Data protection**: Personal data must be handled with care. In the NEPSE system, if we use personally identifiable information (e.g., trader IDs), we must comply.\n",
    "\n",
    "### **98.7.2 SEC and FINRA (US)**\n",
    "For algorithmic trading systems, the SEC requires:\n",
    "- **Supervision**: Firms must supervise algorithmic trading.\n",
    "- **Testing**: Algorithms must be tested before deployment.\n",
    "- **Record‑keeping**: Records of algorithms and their performance must be kept.\n",
    "\n",
    "### **98.7.3 Future Regulations**\n",
    "The EU's proposed AI Act will categorise AI systems by risk and impose requirements for high‑risk systems (which may include financial applications).\n",
    "\n",
    "Staying compliant requires:\n",
    "\n",
    "- Documenting the model development process.\n",
    "- Conducting impact assessments.\n",
    "- Ensuring human oversight.\n",
    "- Maintaining audit trails.\n",
    "\n",
    "---\n",
    "\n",
    "## **98.8 Privacy**\n",
    "\n",
    "Privacy is a key concern, especially if the system uses personal data. For the NEPSE system, if we only use publicly available stock data, privacy is less of an issue. However, if we incorporate user portfolios or trading history, privacy becomes critical.\n",
    "\n",
    "Techniques to protect privacy:\n",
    "\n",
    "- **Anonymisation**: Remove personally identifiable information.\n",
    "- **Differential privacy**: Add noise to training to prevent leakage of individual records.\n",
    "- **Federated learning**: Train models across decentralised data without sharing raw data.\n",
    "\n",
    "---\n",
    "\n",
    "## **98.9 Societal Impact**\n",
    "\n",
    "Beyond individual fairness, consider the broader impact of the system.\n",
    "\n",
    "- **Market stability**: Could the model contribute to herding behaviour or flash crashes?\n",
    "- **Inequality**: Does the system benefit only those with access to it, exacerbating inequality?\n",
    "- **Transparency of markets**: If many traders use similar models, markets may become less diverse and more fragile.\n",
    "\n",
    "These are difficult questions, but responsible AI practitioners should at least be aware of them and engage with stakeholders.\n",
    "\n",
    "---\n",
    "\n",
    "## **98.10 Building a Responsible AI Framework**\n",
    "\n",
    "To embed ethics into the development process, consider the following framework:\n",
    "\n",
    "1. **Define principles**: Establish team‑wide principles (e.g., fairness, transparency).\n",
    "2. **Risk assessment**: Before starting a project, assess potential ethical risks.\n",
    "3. **Design with ethics**: Include fairness metrics in model evaluation from the start.\n",
    "4. **Test for bias**: Regularly evaluate models on subgroups.\n",
    "5. **Document**: Maintain model cards and audit logs.\n",
    "6. **Monitor in production**: Continuously check for drift and unfairness.\n",
    "7. **Review and update**: Periodically review the system's impact and update as needed.\n",
    "\n",
    "This framework should be integrated into the development lifecycle (Chapter 86) and project management (Chapter 88).\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored the critical topic of ethical AI and responsible machine learning in the context of time‑series prediction systems. Using the NEPSE system as an example, we covered:\n",
    "\n",
    "- Sources of bias in data, features, models, and deployment.\n",
    "- Fairness metrics to quantify bias across groups.\n",
    "- Mitigation techniques at the data, algorithm, and post‑processing levels.\n",
    "- Transparency through model cards and local explanations (SHAP).\n",
    "- Accountability via audit trails and human oversight.\n",
    "- Regulatory landscape (GDPR, SEC) and privacy considerations.\n",
    "- Broader societal impact and the importance of a responsible AI framework.\n",
    "\n",
    "Building ethical AI is not a one‑time task but an ongoing commitment. By embedding these practices into your workflow, you ensure that your prediction system not only performs well but also earns the trust of users and regulators.\n",
    "\n",
    "In the next and final chapter, we will look to the future of time‑series prediction, exploring emerging trends and how to prepare for what's next.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 98**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
