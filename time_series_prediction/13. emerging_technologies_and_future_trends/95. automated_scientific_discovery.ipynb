{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 95: Automated Scientific Discovery\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the concept of automated scientific discovery and its relevance to time\u2011series analysis.\n",
    "- Apply symbolic regression techniques to discover mathematical relationships in time\u2011series data.\n",
    "- Use equation discovery algorithms to uncover underlying physical or economic laws from observations.\n",
    "- Implement causal discovery methods to infer causal structures from time\u2011series (e.g., Granger causality, PC algorithm).\n",
    "- Leverage AI to generate hypotheses and guide scientific experimentation.\n",
    "- Apply these techniques to the NEPSE dataset to discover relationships between stocks or between stocks and external factors.\n",
    "- Evaluate the discovered equations and causal models for validity and interpretability.\n",
    "- Recognize the limitations and challenges of automated discovery, including overfitting and confounding.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.1 Introduction to Automated Scientific Discovery**\n",
    "\n",
    "Automated scientific discovery is the use of artificial intelligence to uncover new knowledge, relationships, and laws from data. Traditionally, science has progressed through human intuition, hypothesis formulation, and experimentation. AI can accelerate this process by:\n",
    "\n",
    "- **Discovering equations** that fit observed data (symbolic regression).\n",
    "- **Inferring causal relationships** from observational time\u2011series (causal discovery).\n",
    "- **Generating hypotheses** that can be tested experimentally.\n",
    "- **Automating the scientific method** in closed\u2011loop systems.\n",
    "\n",
    "In the context of time\u2011series, automated discovery can help us understand the underlying dynamics of a system. For the NEPSE stock market, we might discover:\n",
    "\n",
    "- A mathematical relationship between the prices of two stocks (e.g., cointegration).\n",
    "- A causal relationship: \u201cchanges in interest rates cause changes in banking stock prices.\u201d\n",
    "- A physical\u2011like law: \u201cprice movement is proportional to trading volume squared.\u201d\n",
    "\n",
    "These discoveries can lead to better forecasting models, improved trading strategies, and deeper understanding of market behaviour.\n",
    "\n",
    "This chapter will introduce key techniques in automated scientific discovery, with practical examples using Python libraries like `gplearn` (symbolic regression), `causalnex`, and `statsmodels` (causal inference).\n",
    "\n",
    "---\n",
    "\n",
    "## **95.2 Symbolic Regression**\n",
    "\n",
    "Symbolic regression is a type of regression analysis that searches the space of mathematical expressions to find a model that best fits a given dataset. Unlike traditional regression, which assumes a fixed form (e.g., linear), symbolic regression can discover arbitrary equations.\n",
    "\n",
    "### **95.2.1 How It Works**\n",
    "Symbolic regression typically uses genetic programming:\n",
    "\n",
    "1. **Initialisation**: Create a population of random mathematical expressions (trees) from a set of operators (+, -, \u00d7, \u00f7, sin, exp, etc.) and variables.\n",
    "2. **Fitness evaluation**: Evaluate each expression on the training data (e.g., mean squared error).\n",
    "3. **Selection**: Choose the fittest expressions to breed.\n",
    "4. **Crossover and mutation**: Combine and modify expressions to create new ones.\n",
    "5. **Repeat** for many generations.\n",
    "\n",
    "The result is a set of equations that trade off complexity and accuracy.\n",
    "\n",
    "### **95.2.2 Symbolic Regression with `gplearn`**\n",
    "\n",
    "We'll use `gplearn` to discover a relationship between NEPSE stock prices and volume.\n",
    "\n",
    "```python\n",
    "# pip install gplearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from gplearn.genetic import SymbolicRegressor\n",
    "from gplearn.functions import make_function\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic NEPSE-like data (or use real)\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "price = 1000 + np.cumsum(np.random.randn(n) * 5)\n",
    "volume = np.random.lognormal(12, 1, n)\n",
    "# Create a known relationship: price_change = 0.01 * volume - 0.5 * lagged_price + noise\n",
    "price_change = 0.01 * volume - 0.5 * np.roll(price, 1) + np.random.randn(n) * 2\n",
    "df = pd.DataFrame({\n",
    "    'price': price,\n",
    "    'volume': volume,\n",
    "    'price_change': price_change,\n",
    "    'lag_price': np.roll(price, 1)\n",
    "})\n",
    "df = df.dropna()\n",
    "\n",
    "# Define features and target\n",
    "X = df[['price', 'volume', 'lag_price']].values\n",
    "y = df['price_change'].values\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Define function set (operators)\n",
    "function_set = ['add', 'sub', 'mul', 'div', 'sqrt', 'log', 'neg', 'sin', 'cos']\n",
    "\n",
    "# Create symbolic regressor\n",
    "est = SymbolicRegressor(\n",
    "    population_size=5000,\n",
    "    generations=20,\n",
    "    stopping_criteria=0.01,\n",
    "    p_crossover=0.7,\n",
    "    p_subtree_mutation=0.1,\n",
    "    p_hoist_mutation=0.05,\n",
    "    p_point_mutation=0.1,\n",
    "    max_samples=0.9,\n",
    "    verbose=1,\n",
    "    parsimony_coefficient=0.01,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train (this may take a few minutes)\n",
    "est.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "y_pred = est.predict(X_test)\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "print(f\"Test MSE: {mse:.4f}\")\n",
    "\n",
    "# Print the discovered equation\n",
    "print(\"Discovered equation:\")\n",
    "print(est._program)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.subplot(1,2,1)\n",
    "plt.scatter(y_test, y_pred, alpha=0.5)\n",
    "plt.plot([y_test.min(), y_test.max()], [y_test.min(), y_test.max()], 'r--')\n",
    "plt.xlabel('True')\n",
    "plt.ylabel('Predicted')\n",
    "plt.title('Symbolic Regression Performance')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(est._program, label='Discovered Program')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation**:\n",
    "\n",
    "- We create synthetic data with a known (but noisy) relationship between price change, volume, and lagged price.\n",
    "- `SymbolicRegressor` evolves a population of expressions. The `parsimony_coefficient` penalises complex expressions, encouraging simpler equations.\n",
    "- The best program is printed. It might be something like `add(mul(0.01, volume), mul(-0.5, lag_price))`, closely approximating the true relationship.\n",
    "- This demonstrates how symbolic regression can recover underlying physical laws from data.\n",
    "\n",
    "### **95.2.3 Applying to Real NEPSE Data**\n",
    "\n",
    "For real NEPSE data, you might try to discover relationships between:\n",
    "\n",
    "- Stock returns and trading volume.\n",
    "- Price movements of related stocks (e.g., banking sector).\n",
    "- Price and external factors (e.g., exchange rate, oil prices).\n",
    "\n",
    "However, beware of overfitting: the discovered equations may not generalise out of sample. Use cross\u2011validation and test on a hold\u2011out period.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.3 Equation Discovery Algorithms**\n",
    "\n",
    "Symbolic regression is one form of equation discovery. Other approaches include:\n",
    "\n",
    "- **SINDy** (Sparse Identification of Nonlinear Dynamics): Uses sparse regression to identify governing equations from time\u2011series.\n",
    "- **Eureqa**: Commercial software for symbolic regression.\n",
    "- **PySR**: A high\u2011performance symbolic regression library in Python.\n",
    "\n",
    "### **95.3.1 SINDy Example**\n",
    "\n",
    "SINDy is particularly suited for discovering dynamical systems from time\u2011series.\n",
    "\n",
    "```python\n",
    "# pip install pysindy\n",
    "import pysindy as ps\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate data from a simple dynamical system: dx/dt = -x\n",
    "t = np.linspace(0, 10, 100)\n",
    "x = np.exp(-t)[:, np.newaxis]\n",
    "x_dot = -x  # derivative\n",
    "\n",
    "# Create SINDy model\n",
    "model = ps.SINDy()\n",
    "model.fit(x, t=t, x_dot=x_dot)\n",
    "model.print()\n",
    "\n",
    "# This should discover: x' = -1.000 x\n",
    "```\n",
    "\n",
    "For NEPSE, you could model stock prices as a dynamical system, though financial data is notoriously noisy and non\u2011stationary.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.4 Causal Discovery**\n",
    "\n",
    "Causal discovery aims to infer causal relationships from observational data. In time\u2011series, this is often done using:\n",
    "\n",
    "- **Granger causality**: A statistical test for whether one time series helps predict another.\n",
    "- **PC algorithm**: A constraint\u2011based method that uses conditional independence tests.\n",
    "- **LiNGAM**: Assumes linear non\u2011Gaussian relationships.\n",
    "- **VAR models**: Vector autoregression with causality tests.\n",
    "\n",
    "Understanding causality is crucial for making interventions (e.g., \u201cif we change interest rates, what happens to stock prices?\u201d) and for building robust forecasting models.\n",
    "\n",
    "### **95.4.1 Granger Causality**\n",
    "\n",
    "Granger causality tests whether past values of one time series improve the prediction of another, beyond its own past.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import grangercausalitytests\n",
    "import pandas as pd\n",
    "\n",
    "# Example: test if volume Granger\u2011causes price\n",
    "# Prepare data: two columns, price and volume\n",
    "df_g = df[['price', 'volume']].dropna()\n",
    "\n",
    "# Test with up to 5 lags\n",
    "gc_result = grangercausalitytests(df_g, maxlag=5, verbose=True)\n",
    "\n",
    "# Interpret p-values; if p < 0.05, reject null hypothesis (no causality)\n",
    "```\n",
    "\n",
    "**Interpretation**: If volume Granger\u2011causes price, it means past volume helps predict future price, suggesting a directional relationship (but not necessarily true causality).\n",
    "\n",
    "### **95.4.2 PC Algorithm for Time\u2011Series**\n",
    "\n",
    "The PC algorithm can be applied to time\u2011series by treating each lag as a separate variable. Libraries like `causalnex` or `causallearn` implement this.\n",
    "\n",
    "```python\n",
    "# pip install causalnex\n",
    "from causalnex.structure import StructureModel\n",
    "from causalnex.structure.notears import from_pandas\n",
    "import pandas as pd\n",
    "\n",
    "# Create a DataFrame with multiple lags\n",
    "df_lags = pd.DataFrame({\n",
    "    'price_t': df['price'],\n",
    "    'price_t1': df['price'].shift(1),\n",
    "    'volume_t': df['volume'],\n",
    "    'volume_t1': df['volume'].shift(1),\n",
    "}).dropna()\n",
    "\n",
    "# Learn structure using NOTEARS (continuous optimisation)\n",
    "sm = from_pandas(df_lags, tabu_edges=[], tabu_parent_nodes=[])\n",
    "\n",
    "# Visualise\n",
    "sm.plot('causal_graph.png')\n",
    "```\n",
    "\n",
    "The resulting graph shows directed edges, e.g., `volume_t1 -> price_t`, suggesting that past volume influences current price.\n",
    "\n",
    "### **95.4.3 Limitations of Causal Discovery**\n",
    "\n",
    "- **Confounding**: Unobserved variables can create spurious relationships.\n",
    "- **Feedback loops**: In financial markets, causality is often bidirectional (price and volume influence each other).\n",
    "- **Non\u2011stationarity**: Causal structures may change over time.\n",
    "- **Data requirements**: Large samples are needed for reliable inference.\n",
    "\n",
    "Despite these challenges, causal discovery can generate hypotheses for further investigation.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.5 Hypothesis Generation with AI**\n",
    "\n",
    "AI can also generate hypotheses by identifying surprising patterns or anomalies. For example, if a stock's price movement deviates significantly from its historical relationship with another stock, an LLM could suggest potential causes (e.g., merger rumours, regulatory changes).\n",
    "\n",
    "```python\n",
    "def generate_hypothesis(stock_a, stock_b, correlation_break):\n",
    "    prompt = f\"\"\"\n",
    "    Historically, stocks {stock_a} and {stock_b} in the NEPSE market have had a correlation of 0.8.\n",
    "    Over the last week, their correlation has dropped to 0.2.\n",
    "    What are some possible hypotheses for this decoupling? Consider market news, sector-specific events, or company announcements.\n",
    "    \"\"\"\n",
    "    # Call LLM (as in Chapter 94)\n",
    "    # ...\n",
    "```\n",
    "\n",
    "This combines data analysis with LLM reasoning to suggest plausible explanations.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.6 Applications to NEPSE**\n",
    "\n",
    "Let's apply these techniques to the NEPSE dataset to discover meaningful relationships.\n",
    "\n",
    "### **95.6.1 Discovering a Relationship Between Banking Stocks**\n",
    "\n",
    "We might hypothesise that banking stocks move together. Symbolic regression could find an equation linking their returns.\n",
    "\n",
    "```python\n",
    "# Assume we have data for NABIL and EBL (two major banks)\n",
    "df_banks = pd.DataFrame({\n",
    "    'nabil_return': nabil_returns,\n",
    "    'ebl_return': ebl_returns,\n",
    "})\n",
    "\n",
    "X = df_banks[['nabil_return']].values\n",
    "y = df_banks['ebl_return'].values\n",
    "\n",
    "est = SymbolicRegressor(...)\n",
    "est.fit(X, y)\n",
    "print(est._program)\n",
    "```\n",
    "\n",
    "The discovered equation might be something like `ebl_return = 0.95 * nabil_return + 0.02`, indicating a strong linear relationship.\n",
    "\n",
    "### **95.6.2 Causal Discovery Between Volume and Price**\n",
    "\n",
    "Using Granger causality, we might find that volume Granger\u2011causes price for some stocks, but not others. This could inform feature selection for prediction models.\n",
    "\n",
    "### **95.6.3 Discovering Market Regimes**\n",
    "\n",
    "Symbolic regression could be applied piecewise to discover different equations for different market regimes (bull, bear, sideways). This aligns with the regime\u2011switching models from Chapter 83.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.7 Evaluation and Validation**\n",
    "\n",
    "Discovered equations and causal models must be validated:\n",
    "\n",
    "- **Out\u2011of\u2011sample testing**: Does the equation hold on unseen data?\n",
    "- **Cross\u2011validation**: Use rolling window evaluation for time\u2011series.\n",
    "- **Simulation**: Can the discovered dynamics simulate realistic behaviour?\n",
    "- **Domain knowledge**: Does the discovered relationship make sense to a domain expert?\n",
    "\n",
    "For the NEPSE system, a discovered relationship should be reviewed by a financial analyst before being used in trading decisions.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.8 Tools and Libraries**\n",
    "\n",
    "- **gplearn**: Symbolic regression with genetic programming.\n",
    "- **PySR**: Faster symbolic regression with multi\u2011language support.\n",
    "- **pysindy**: Sparse identification of nonlinear dynamics.\n",
    "- **causalnex**: Causal discovery and Bayesian networks.\n",
    "- **causallearn**: Collection of causal discovery algorithms.\n",
    "- **statsmodels**: Granger causality, VAR models.\n",
    "- **DoWhy**: Causal inference library.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.9 Challenges and Limitations**\n",
    "\n",
    "- **Overfitting**: Complex equations can fit noise. Use parsimony penalties and validation.\n",
    "- **Interpretability**: Some discovered equations may be too complex to understand.\n",
    "- **Non\u2011identifiability**: Multiple equations may fit the data equally well.\n",
    "- **Computational cost**: Symbolic regression is computationally expensive.\n",
    "- **Causal discovery assumptions**: Many algorithms assume no hidden confounders, linearity, etc.\n",
    "\n",
    "---\n",
    "\n",
    "## **95.10 Future Directions**\n",
    "\n",
    "- **Integration with LLMs**: Use LLMs to guide the search for equations or to interpret discovered models.\n",
    "- **Automated experimentation**: AI that designs experiments to test hypotheses.\n",
    "- **Discovering physical laws from video**: Extending to spatiotemporal data.\n",
    "- **Causal representation learning**: Learning causal variables directly from time\u2011series.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored automated scientific discovery and its application to time\u2011series data. We covered:\n",
    "\n",
    "- Symbolic regression using genetic programming to discover mathematical relationships.\n",
    "- Equation discovery with SINDy for dynamical systems.\n",
    "- Causal discovery techniques including Granger causality and the PC algorithm.\n",
    "- Hypothesis generation using LLMs.\n",
    "- Practical applications to the NEPSE dataset.\n",
    "- Evaluation methods and challenges.\n",
    "\n",
    "Automated discovery can uncover hidden patterns and relationships in financial time\u2011series, leading to better models and deeper understanding. While not a replacement for human expertise, it is a powerful tool in the data scientist's arsenal.\n",
    "\n",
    "In the next chapter, we will explore **Edge AI and TinyML**, focusing on deploying time\u2011series models on resource\u2011constrained devices.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 95**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='94. large_language_models_for_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='96. edge_ai_and_tinyml.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}