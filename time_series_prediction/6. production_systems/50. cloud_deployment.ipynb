{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 50: Cloud Deployment\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the core offerings of major cloud providers (AWS, Google Cloud, Azure) relevant to machine learning systems\n",
    "- Design cloud architectures for time\u2011series prediction systems that are scalable, cost\u2011effective, and resilient\n",
    "- Deploy your NEPSE prediction model using managed ML services like Amazon SageMaker, Google Vertex AI, and Azure Machine Learning\n",
    "- Leverage serverless computing (AWS Lambda, Google Cloud Functions) for lightweight inference tasks\n",
    "- Orchestrate containerised applications on Kubernetes using cloud\u2011managed services (EKS, GKE, AKS)\n",
    "- Choose the right data storage solutions (object storage, relational databases, data warehouses) for different components of your pipeline\n",
    "- Implement cost management strategies to avoid unexpected bills and optimise cloud spending\n",
    "- Consider multi\u2011cloud and hybrid cloud approaches for redundancy and avoiding vendor lock\u2011in\n",
    "- Apply cloud best practices for security, scalability, and monitoring as discussed in previous chapters\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "So far, we have built a comprehensive NEPSE stock prediction system that can ingest data, engineer features, train models, and serve predictions. We have run everything locally or on a single server. But in the real world, the system must be **deployed** to the cloud to handle unpredictable traffic, store large amounts of data reliably, and ensure high availability. Cloud platforms provide virtually unlimited resources on demand, but they also introduce complexity: you must choose the right services, configure them correctly, and manage costs.\n",
    "\n",
    "In this chapter, we will explore how to deploy the NEPSE prediction system on the three major cloud providers: **Amazon Web Services (AWS)**, **Google Cloud Platform (GCP)**, and **Microsoft Azure**. We will cover various deployment patterns, from fully managed ML services to custom containerised solutions, and discuss the trade\u2011offs. By the end, you will be equipped to make informed decisions about cloud deployment for your own time\u2011series prediction systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 50.1 Cloud Providers Overview\n",
    "\n",
    "Each cloud provider offers a vast portfolio of services. For an ML system, the most relevant categories are:\n",
    "\n",
    "- **Compute**: Virtual machines (EC2, Compute Engine, VMs), containers (EKS, GKE, AKS), and serverless (Lambda, Cloud Functions, Functions).\n",
    "- **Storage**: Object storage (S3, GCS, Blob Storage), block storage (EBS, Persistent Disk), file storage (EFS, Filestore).\n",
    "- **Databases**: Relational (RDS, Cloud SQL, Azure SQL), NoSQL (DynamoDB, Firestore, Cosmos DB), data warehouses (Redshift, BigQuery, Synapse).\n",
    "- **Machine Learning**: Managed training and serving (SageMaker, Vertex AI, Azure ML), pre\u2011built AI services (Rekognition, Vision API, Cognitive Services).\n",
    "- **Networking**: VPC, load balancers, API Gateway, CloudFront/CDN.\n",
    "- **Monitoring**: CloudWatch, Stackdriver, Azure Monitor.\n",
    "\n",
    "We will focus on the services that are most relevant to a time\u2011series prediction system like NEPSE.\n",
    "\n",
    "### 50.1.1 Choosing a Provider\n",
    "\n",
    "The choice often comes down to existing organisational expertise, specific service offerings, and cost. For example:\n",
    "\n",
    "- **AWS** has the broadest and most mature ML ecosystem, with SageMaker being a comprehensive platform.\n",
    "- **GCP** excels in data analytics and has strong integration with BigQuery, a serverless data warehouse that can handle large time\u2011series datasets.\n",
    "- **Azure** is popular in enterprises with heavy Microsoft investments and offers good integration with tools like Power BI.\n",
    "\n",
    "For the NEPSE system, any of the three could work. We will provide examples for all, but you may choose based on your preferences.\n",
    "\n",
    "---\n",
    "\n",
    "## 50.2 Cloud Architecture Patterns\n",
    "\n",
    "A typical cloud\u2011based prediction system consists of several components:\n",
    "\n",
    "1. **Data Ingestion Layer**: Collects raw data from sources (APIs, databases) and lands it in cloud storage.\n",
    "2. **Data Lake / Warehouse**: Stores raw and processed data (e.g., S3 + Glue, BigQuery).\n",
    "3. **Feature Store**: Stores pre\u2011computed features for reuse (e.g., Feast on cloud, or a simple database).\n",
    "4. **Training Pipeline**: Periodically trains models using historical data (e.g., SageMaker training jobs, Vertex AI training).\n",
    "5. **Model Registry**: Stores trained models and metadata (e.g., SageMaker Model Registry, MLflow on cloud).\n",
    "6. **Inference Service**: Serves predictions via API (e.g., SageMaker endpoints, Vertex AI endpoints, custom containers on Kubernetes).\n",
    "7. **Monitoring and Alerting**: Tracks system health and model drift (e.g., CloudWatch, Stackdriver, Prometheus on Kubernetes).\n",
    "\n",
    "![Cloud Architecture Diagram](images/cloud_arch.png)\n",
    "\n",
    "For the NEPSE system, we can implement each component using cloud services. We will walk through a concrete architecture on AWS, then highlight equivalent services on GCP and Azure.\n",
    "\n",
    "---\n",
    "\n",
    "## 50.3 Managed ML Services\n",
    "\n",
    "Managed ML services abstract away the infrastructure, allowing you to focus on the model. They handle provisioning, scaling, and maintenance.\n",
    "\n",
    "### 50.3.1 Amazon SageMaker\n",
    "\n",
    "Amazon SageMaker is a fully managed service covering the entire ML workflow. For our NEPSE predictor, we can use:\n",
    "\n",
    "- **SageMaker Notebooks** for exploration (similar to Jupyter).\n",
    "- **SageMaker Training** for distributed training.\n",
    "- **SageMaker Model Registry** to version models.\n",
    "- **SageMaker Endpoints** for real\u2011time inference.\n",
    "- **SageMaker Batch Transform** for offline predictions.\n",
    "\n",
    "**Example: Training an XGBoost model on SageMaker**\n",
    "\n",
    "```python\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker.xgboost.estimator import XGBoost\n",
    "\n",
    "role = get_execution_role()\n",
    "session = sagemaker.Session()\n",
    "\n",
    "# Specify the S3 location of training data\n",
    "train_data_uri = 's3://nepse-data/train/'\n",
    "\n",
    "# Create an XGBoost estimator\n",
    "xgb_estimator = XGBoost(\n",
    "    entry_point='train.py',           # custom training script\n",
    "    hyperparameters={\n",
    "        'max_depth': 5,\n",
    "        'eta': 0.2,\n",
    "        'gamma': 4,\n",
    "        'min_child_weight': 6,\n",
    "        'subsample': 0.8,\n",
    "        'objective': 'binary:logistic',\n",
    "        'num_round': 100\n",
    "    },\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    framework_version='1.3-1',\n",
    "    role=role,\n",
    "    output_path='s3://nepse-models/'\n",
    ")\n",
    "\n",
    "# Launch training\n",
    "xgb_estimator.fit({'train': TrainingInput(train_data_uri, content_type='csv')})\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We define an estimator with hyperparameters and instance type. SageMaker spins up a training instance, runs the script `train.py` (which should read data from the input channel), and saves the model artifact to S3.\n",
    "\n",
    "**Deploying to a real\u2011time endpoint:**\n",
    "\n",
    "```python\n",
    "predictor = xgb_estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    endpoint_name='nepse-predictor'\n",
    ")\n",
    "\n",
    "# Now you can call the endpoint:\n",
    "result = predictor.predict(data)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`deploy` creates a scalable endpoint behind a load balancer. SageMaker handles health checks, auto\u2011scaling, and updates.\n",
    "\n",
    "### 50.3.2 Google Vertex AI\n",
    "\n",
    "Vertex AI is Google\u2019s unified ML platform. It offers similar capabilities:\n",
    "\n",
    "- **Vertex AI Workbench** for notebooks.\n",
    "- **Vertex AI Training** for custom and pre\u2011built containers.\n",
    "- **Vertex AI Model Registry**.\n",
    "- **Vertex AI Endpoints** for prediction.\n",
    "\n",
    "**Example: Training an XGBoost model on Vertex AI**\n",
    "\n",
    "```python\n",
    "from google.cloud import aiplatform\n",
    "\n",
    "aiplatform.init(project='my-project', location='us-central1')\n",
    "\n",
    "# Define training job\n",
    "job = aiplatform.CustomTrainingJob(\n",
    "    display_name='nepse-xgboost',\n",
    "    script_path='trainer.py',\n",
    "    container_uri='gcr.io/cloud-aiplatform/training/xgboost-cpu.0-90:latest',\n",
    "    requirements=['pandas', 'scikit-learn'],\n",
    "    model_serving_container_image_uri='gcr.io/cloud-aiplatform/prediction/xgboost-cpu.0-90:latest'\n",
    ")\n",
    "\n",
    "# Run training\n",
    "model = job.run(\n",
    "    dataset=None,  # we'll pass data via arguments\n",
    "    args=['--data-uri', 'gs://nepse-data/train/'],\n",
    "    replica_count=1,\n",
    "    machine_type='n1-standard-4'\n",
    ")\n",
    "\n",
    "# Deploy\n",
    "endpoint = model.deploy(machine_type='n1-standard-2')\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Vertex AI uses custom containers; we specify the XGBoost training container and a script. The trained model is automatically registered and can be deployed to an endpoint.\n",
    "\n",
    "### 50.3.3 Azure Machine Learning\n",
    "\n",
    "Azure ML provides a similar workflow:\n",
    "\n",
    "```python\n",
    "from azureml.core import Workspace, Experiment, Environment, ScriptRunConfig\n",
    "from azureml.core.compute import ComputeTarget, AmlCompute\n",
    "\n",
    "ws = Workspace.from_config()\n",
    "\n",
    "# Create compute cluster\n",
    "compute_cluster = AmlCompute.create_or_attach(ws, name='cpu-cluster', min_nodes=0, max_nodes=4)\n",
    "\n",
    "# Define environment\n",
    "env = Environment.from_conda_specification(name='xgboost-env', file_path='conda.yml')\n",
    "\n",
    "# Configure training script\n",
    "config = ScriptRunConfig(\n",
    "    source_directory='.',\n",
    "    script='train.py',\n",
    "    arguments=['--data-folder', 'wasbs://...'],\n",
    "    compute_target=compute_cluster,\n",
    "    environment=env\n",
    ")\n",
    "\n",
    "# Submit experiment\n",
    "run = Experiment(ws, 'nepse-training').submit(config)\n",
    "run.wait_for_completion()\n",
    "\n",
    "# Register model\n",
    "model = run.register_model(model_name='nepse-xgboost', model_path='outputs/model.pkl')\n",
    "\n",
    "# Deploy to endpoint\n",
    "from azureml.core.model import InferenceConfig\n",
    "inference_config = InferenceConfig(entry_script='score.py', environment=env)\n",
    "deployment_config = AciWebservice.deploy_configuration(cpu_cores=1, memory_gb=1)\n",
    "service = Model.deploy(ws, 'nepse-service', [model], inference_config, deployment_config)\n",
    "service.wait_for_deployment()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Azure ML uses a workspace, compute targets, and environments. The model is registered and can be deployed to Azure Container Instances (ACI) or Kubernetes (AKS).\n",
    "\n",
    "---\n",
    "\n",
    "## 50.4 Serverless ML\n",
    "\n",
    "For low\u2011traffic or sporadic prediction workloads, serverless functions can be cost\u2011effective. They scale to zero when not in use, but have cold\u2011start latency.\n",
    "\n",
    "### 50.4.1 AWS Lambda with Container Support\n",
    "\n",
    "AWS Lambda now supports packaging models as container images (up to 10 GB). You can deploy a lightweight inference function.\n",
    "\n",
    "**Example: Lambda function for NEPSE prediction**\n",
    "\n",
    "```python\n",
    "# Dockerfile\n",
    "FROM public.ecr.aws/lambda/python:3.9\n",
    "COPY app.py requirements.txt ./\n",
    "RUN pip install -r requirements.txt\n",
    "CMD [\"app.handler\"]\n",
    "```\n",
    "\n",
    "```python\n",
    "# app.py\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "model = joblib.load('model.pkl')  # loaded at cold start\n",
    "\n",
    "def handler(event, context):\n",
    "    body = json.loads(event['body'])\n",
    "    features = np.array(body['features']).reshape(1, -1)\n",
    "    pred = model.predict_proba(features)[0, 1]\n",
    "    return {\n",
    "        'statusCode': 200,\n",
    "        'body': json.dumps({'probability': pred})\n",
    "    }\n",
    "```\n",
    "\n",
    "Deploy using the AWS CLI or SAM. Lambda auto\u2011scales with concurrency, but each new instance loads the model (cold start). To reduce cold starts, you can enable **provisioned concurrency**.\n",
    "\n",
    "### 50.4.2 Google Cloud Functions\n",
    "\n",
    "Cloud Functions has a shorter timeout (9 minutes) and smaller memory, suitable for lightweight models.\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "model = joblib.load('model.pkl')\n",
    "\n",
    "def predict(request):\n",
    "    request_json = request.get_json()\n",
    "    features = np.array(request_json['features']).reshape(1, -1)\n",
    "    prob = model.predict_proba(features)[0, 1]\n",
    "    return {'probability': prob}\n",
    "```\n",
    "\n",
    "### 50.4.3 Azure Functions\n",
    "\n",
    "Similar to AWS Lambda, Azure Functions supports custom containers and can be triggered via HTTP.\n",
    "\n",
    "---\n",
    "\n",
    "## 50.5 Container Orchestration with Kubernetes\n",
    "\n",
    "For more control and portability, you can run your own containers on Kubernetes. Cloud providers offer managed Kubernetes:\n",
    "\n",
    "- **Amazon EKS** (Elastic Kubernetes Service)\n",
    "- **Google GKE** (Google Kubernetes Engine)\n",
    "- **Azure AKS** (Azure Kubernetes Service)\n",
    "\n",
    "You can deploy your prediction service as a deployment, expose it via a load balancer, and scale automatically.\n",
    "\n",
    "**Example: Deploying NEPSE predictor on EKS**\n",
    "\n",
    "First, build and push your Docker image to Amazon ECR. Then create a Kubernetes deployment:\n",
    "\n",
    "```yaml\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nepse-predictor\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nepse-predictor\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nepse-predictor\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: predictor\n",
    "        image: 123456789012.dkr.ecr.us-east-1.amazonaws.com/nepse-predictor:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: MODEL_PATH\n",
    "          value: /app/model.pkl\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"1000m\"\n",
    "```\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: nepse-predictor\n",
    "spec:\n",
    "  selector:\n",
    "    app: nepse-predictor\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "```\n",
    "\n",
    "Apply with `kubectl apply -f deployment.yaml -f service.yaml`. The cloud provider provisions a load balancer and assigns a public IP.\n",
    "\n",
    "**Auto\u2011scaling with Horizontal Pod Autoscaler:**\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: nepse-predictor-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: nepse-predictor\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "```\n",
    "\n",
    "Kubernetes will scale the number of pods based on CPU usage.\n",
    "\n",
    "---\n",
    "\n",
    "## 50.6 Data Storage in the Cloud\n",
    "\n",
    "Choosing the right storage for each component is crucial.\n",
    "\n",
    "### 50.6.1 Object Storage (S3, GCS, Blob)\n",
    "\n",
    "Use object storage for:\n",
    "\n",
    "- Raw data (CSV files, Parquet)\n",
    "- Model artifacts\n",
    "- Feature store backup\n",
    "\n",
    "**Example: Storing NEPSE data in S3 and reading with pandas**\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import pandas as pd\n",
    "from io import StringIO\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "obj = s3.get_object(Bucket='nepse-data', Key='raw/nepse_2024.csv')\n",
    "data = obj['Body'].read().decode('utf-8')\n",
    "df = pd.read_csv(StringIO(data))\n",
    "```\n",
    "\n",
    "### 50.6.2 Relational Databases (RDS, Cloud SQL, Azure SQL)\n",
    "\n",
    "Use for:\n",
    "\n",
    "- Metadata about models, experiments\n",
    "- User data, API keys\n",
    "- Small transactional data\n",
    "\n",
    "**Example: Connecting to PostgreSQL on RDS**\n",
    "\n",
    "```python\n",
    "import psycopg2\n",
    "import os\n",
    "\n",
    "conn = psycopg2.connect(\n",
    "    host=os.environ['DB_HOST'],\n",
    "    database='nepsedb',\n",
    "    user=os.environ['DB_USER'],\n",
    "    password=os.environ['DB_PASSWORD']\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"SELECT * FROM predictions WHERE symbol='NABIL'\")\n",
    "rows = cur.fetchall()\n",
    "```\n",
    "\n",
    "### 50.6.3 Data Warehouses (Redshift, BigQuery, Synapse)\n",
    "\n",
    "For analytical queries on large historical datasets, a data warehouse is ideal. BigQuery, in particular, is serverless and can query terabytes of data in seconds.\n",
    "\n",
    "**Example: Querying NEPSE data in BigQuery**\n",
    "\n",
    "```sql\n",
    "SELECT symbol, AVG(close) as avg_close\n",
    "FROM `my-project.nepse_dataset.prices`\n",
    "WHERE date >= '2024-01-01'\n",
    "GROUP BY symbol\n",
    "```\n",
    "\n",
    "You can load data from GCS into BigQuery periodically.\n",
    "\n",
    "### 50.6.4 Feature Stores (Feast on Cloud)\n",
    "\n",
    "For production ML, a feature store ensures consistent feature computation between training and serving. Feast can be deployed on Kubernetes and use Redis (for online) and BigQuery (for offline).\n",
    "\n",
    "---\n",
    "\n",
    "## 50.7 Cost Management\n",
    "\n",
    "Cloud costs can spiral if not managed. Here are key strategies:\n",
    "\n",
    "- **Right\u2011sizing instances**: Use monitoring to identify under\u2011utilised resources and downsize.\n",
    "- **Spot/preemptible instances**: For non\u2011critical batch jobs (training, backtesting), use spot instances (AWS) or preemptible VMs (GCP) at 60\u201190% discount.\n",
    "- **Auto\u2011scaling**: Scale down to zero when not in use (e.g., development environments).\n",
    "- **Storage lifecycle**: Move old data to cheaper tiers (S3 Glacier, GCS Coldline).\n",
    "- **Reserved instances / savings plans**: Commit to 1\u20113 years for steady workloads to get significant discounts.\n",
    "- **Monitor and alert**: Set up budgets and alerts to notify when costs exceed thresholds.\n",
    "\n",
    "**Example: Using AWS Budgets to alert on cost**\n",
    "\n",
    "```bash\n",
    "aws budgets create-budget \\\n",
    "    --account-id 123456789012 \\\n",
    "    --budget file://budget.json \\\n",
    "    --notifications-with-subscribers file://subscribers.json\n",
    "```\n",
    "\n",
    "Budget JSON defines the amount and time period; subscribers get email alerts.\n",
    "\n",
    "---\n",
    "\n",
    "## 50.8 Multi\u2011Cloud and Hybrid Cloud\n",
    "\n",
    "Some organisations adopt a multi\u2011cloud strategy to avoid vendor lock\u2011in or for redundancy. However, it adds complexity.\n",
    "\n",
    "- **Multi\u2011cloud**: Run parts of the system on different clouds. For example, use AWS for training and GCP for BigQuery analytics.\n",
    "- **Hybrid cloud**: Connect on\u2011premises data centres with cloud services. This might be necessary if data cannot leave a certain jurisdiction.\n",
    "\n",
    "**Tools for multi\u2011cloud/hybrid:**\n",
    "\n",
    "- **Kubernetes** (with federation) can run anywhere.\n",
    "- **Terraform** can manage infrastructure across clouds.\n",
    "- **Istio** can create a service mesh spanning clouds.\n",
    "\n",
    "For the NEPSE system, unless you have specific requirements, starting with a single cloud is simpler and cheaper.\n",
    "\n",
    "---\n",
    "\n",
    "## 50.9 Cloud Best Practices\n",
    "\n",
    "1. **Infrastructure as Code (IaC)**: Use Terraform, CloudFormation, or Deployment Manager to define your infrastructure. This makes it repeatable and auditable.\n",
    "2. **Security**: Follow the principle of least privilege for IAM roles. Use VPCs to isolate resources.\n",
    "3. **Monitoring**: Enable detailed monitoring for all services. Use cloud\u2011native tools (CloudWatch, Stackdriver) and integrate with your central observability stack.\n",
    "4. **Backup and disaster recovery**: Regularly back up databases and model artifacts to another region. Test recovery procedures.\n",
    "5. **Tagging**: Tag all resources (e.g., `Project=NEPSE`, `Environment=Production`) for cost allocation and management.\n",
    "6. **Use managed services where possible**: They reduce operational overhead. Only run your own Kubernetes if you need customisation.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored the deployment of the NEPSE prediction system on major cloud platforms. We covered:\n",
    "\n",
    "- The core services offered by AWS, GCP, and Azure for ML workloads.\n",
    "- Architecture patterns for cloud\u2011based prediction systems.\n",
    "- Using managed ML services (SageMaker, Vertex AI, Azure ML) to train and deploy models with minimal infrastructure.\n",
    "- Serverless options for lightweight, intermittent inference.\n",
    "- Running containerised applications on managed Kubernetes (EKS, GKE, AKS).\n",
    "- Choosing appropriate data storage solutions for different parts of the pipeline.\n",
    "- Cost management strategies to keep cloud bills under control.\n",
    "- Considerations for multi\u2011cloud and hybrid cloud approaches.\n",
    "- Best practices for security, monitoring, and infrastructure as code.\n",
    "\n",
    "By leveraging the cloud, your NEPSE prediction system can scale to handle any load, store petabytes of data reliably, and remain highly available. The same principles apply to any time\u2011series prediction system you build.\n",
    "\n",
    "This chapter concludes **Part XI: Advanced Implementation Patterns**. In the next part, we will discuss **Industry Best Practices and Standards**, covering topics like development workflows, team collaboration, and project management for ML systems.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 50**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='49. security_and_compliance.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../7. advanced_topics/51. ensemble_methods.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}