{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 43: Model Deployment Strategies\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand different deployment patterns (canary, blue\u2011green, shadow) and when to use each\n",
    "- Containerize a machine learning model using Docker for consistent and portable deployment\n",
    "- Use Docker Compose to orchestrate multi\u2011service applications locally\n",
    "- Deploy models to Kubernetes for automated scaling, rolling updates, and self\u2011healing\n",
    "- Evaluate serverless and edge deployment options for low\u2011latency and cost\u2011sensitive scenarios\n",
    "- Design deployment pipelines that integrate with CI/CD systems\n",
    "- Implement rollback strategies to revert to a previous model version safely\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "After you have trained, validated, and packaged a model (like our NEPSE stock predictor), the next critical step is to **deploy** it into a production environment where it can serve predictions. Model deployment is not a one\u2011time event; it is an ongoing process that must handle updates, scaling, failures, and varying load. The strategy you choose affects the system\u2019s reliability, latency, cost, and ability to evolve.\n",
    "\n",
    "In this chapter, we will explore various deployment patterns and technologies, using our NEPSE prediction system as a concrete example. We will start with simple containerization, then move to orchestration with Kubernetes, and finally touch on serverless and edge deployments. By the end, you will be equipped to design a deployment pipeline that safely rolls out new models while minimising downtime and risk.\n",
    "\n",
    "---\n",
    "\n",
    "## 43.1 Deployment Patterns\n",
    "\n",
    "Deployment patterns describe how a new version of a model is introduced and how traffic is shifted from the old version to the new one. The goal is to reduce risk: if the new version behaves poorly, we want to minimise the impact on users and be able to revert quickly.\n",
    "\n",
    "### 43.1.1 Canary Deployment\n",
    "\n",
    "In a **canary deployment**, the new model version is initially exposed to a small subset of users or requests. If it performs well (according to monitoring metrics), the traffic is gradually increased until it reaches 100%. If errors spike, the canary can be rolled back without affecting most users.\n",
    "\n",
    "**NEPSE Example:**  \n",
    "Suppose we have a new model that predicts intra\u2011day price movements. We might route 5% of the incoming prediction requests to the new model and 95% to the old one. After observing no degradation in latency or accuracy over a few hours, we increase the canary to 25%, then 50%, and finally 100%.\n",
    "\n",
    "**Implementation Approach:**  \n",
    "Canary deployments can be implemented at the load balancer level (e.g., using Kubernetes `Service` with weighted routing) or within the application (e.g., by reading a feature flag from a configuration server).\n",
    "\n",
    "```yaml\n",
    "# Kubernetes service with weighted routing (using Istio or similar)\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: nepse-predictor\n",
    "spec:\n",
    "  hosts:\n",
    "  - nepse-predictor\n",
    "  http:\n",
    "  - match:\n",
    "    - headers:\n",
    "        x-canary:\n",
    "          exact: \"true\"\n",
    "    route:\n",
    "    - destination:\n",
    "        host: nepse-predictor-v2\n",
    "      weight: 100\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: nepse-predictor-v1\n",
    "      weight: 95\n",
    "    - destination:\n",
    "        host: nepse-predictor-v2\n",
    "      weight: 5\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This Istio `VirtualService` routes 5% of all traffic to version 2 of the predictor and 95% to version 1. Optionally, requests with the header `x-canary: true` can be forced to the new version for internal testing. As confidence grows, the weights can be adjusted gradually.\n",
    "\n",
    "### 43.1.2 Blue\u2011Green Deployment\n",
    "\n",
    "**Blue\u2011green deployment** maintains two identical production environments: the **blue** (current) and the **green** (new). At any time, only one environment serves live traffic. When a new version is ready, it is deployed to the green environment, thoroughly tested, and then the router is switched to send all traffic to green. Blue becomes the standby for the next deployment.\n",
    "\n",
    "**NEPSE Example:**  \n",
    "We have two sets of servers running our prediction API: blue (v1.0) and green (v1.1). After validating green internally, we flip the load balancer from blue to green. If problems arise, we flip back immediately.\n",
    "\n",
    "**Implementation:**  \n",
    "Blue\u2011green requires a load balancer or router that can switch between two backend groups.\n",
    "\n",
    "```yaml\n",
    "# Kubernetes service pointing to blue deployment\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: nepse-predictor\n",
    "spec:\n",
    "  selector:\n",
    "    app: nepse-predictor\n",
    "    version: blue   # initially blue\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "```\n",
    "\n",
    "To switch to green, we simply update the `selector` to `version: green`. Kubernetes will immediately start routing traffic to the green pods.\n",
    "\n",
    "**Advantages:**  \n",
    "- Instant rollback (just change selector back).  \n",
    "- No mixed versions during the switch.\n",
    "\n",
    "**Disadvantages:**  \n",
    "- Requires double the resources during deployment.  \n",
    "- Database schema changes must be backward\u2011compatible if both environments share the database.\n",
    "\n",
    "### 43.1.3 Shadow Deployment\n",
    "\n",
    "**Shadow (or mirroring) deployment** sends a copy of production traffic to the new model while the old model continues to serve the live responses. The new model\u2019s predictions are logged and compared, but never returned to users. This allows you to validate performance under real\u2011world load without any risk.\n",
    "\n",
    "**NEPSE Example:**  \n",
    "We deploy a new model as a shadow service. Every prediction request is duplicated and sent to both the old and the new model. The old model\u2019s response is returned to the user, while the new model\u2019s output is stored for offline analysis. After a week of validation, we may decide to promote it.\n",
    "\n",
    "**Implementation:**  \n",
    "Many service meshes (like Istio) support traffic mirroring.\n",
    "\n",
    "```yaml\n",
    "apiVersion: networking.istio.io/v1beta1\n",
    "kind: VirtualService\n",
    "metadata:\n",
    "  name: nepse-predictor\n",
    "spec:\n",
    "  hosts:\n",
    "  - nepse-predictor\n",
    "  http:\n",
    "  - route:\n",
    "    - destination:\n",
    "        host: nepse-predictor-v1\n",
    "      weight: 100\n",
    "    mirror:\n",
    "      host: nepse-predictor-v2\n",
    "    mirrorPercentage:\n",
    "      value: 100.0\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This configuration sends all requests to `v1` but mirrors 100% of them to `v2`. Responses from `v2` are ignored, but the traffic allows us to measure latency and error rates under production conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## 43.2 Containerization\n",
    "\n",
    "Containerization packages a model and its dependencies into a lightweight, portable unit that runs consistently across different environments. Docker is the most popular container platform.\n",
    "\n",
    "### 43.2.1 Docker Fundamentals\n",
    "\n",
    "A Docker **image** contains everything needed to run an application: code, runtime, system tools, libraries, settings. A **container** is a running instance of an image. Images are built from a **Dockerfile**.\n",
    "\n",
    "For our NEPSE prediction API (built with FastAPI, for example), we need a Dockerfile that:\n",
    "\n",
    "- Starts from a base Python image.\n",
    "- Copies the application code and the trained model.\n",
    "- Installs dependencies.\n",
    "- Exposes the port on which the API listens.\n",
    "- Defines the command to run the application.\n",
    "\n",
    "**Example Dockerfile:**\n",
    "\n",
    "```dockerfile\n",
    "# Use official Python image\n",
    "FROM python:3.9-slim\n",
    "\n",
    "# Set working directory\n",
    "WORKDIR /app\n",
    "\n",
    "# Copy requirements first (for better layer caching)\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "\n",
    "# Copy the rest of the application\n",
    "COPY app/ ./app/\n",
    "COPY models/ ./models/\n",
    "\n",
    "# Expose port 8000\n",
    "EXPOSE 8000\n",
    "\n",
    "# Command to run the FastAPI app with uvicorn\n",
    "CMD [\"uvicorn\", \"app.main:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- We use a slim Python image to keep the image size small.  \n",
    "- Copying `requirements.txt` first allows Docker to cache the `pip install` layer; if only the code changes, we don\u2019t reinstall dependencies.  \n",
    "- The final `CMD` launches the FastAPI application using Uvicorn, binding to all interfaces so that it can be reached from outside the container.\n",
    "\n",
    "**Building and running:**\n",
    "\n",
    "```bash\n",
    "docker build -t nepse-predictor:v1 .\n",
    "docker run -p 8000:8000 nepse-predictor:v1\n",
    "```\n",
    "\n",
    "Now the API is accessible at `http://localhost:8000`.\n",
    "\n",
    "### 43.2.2 Docker Compose\n",
    "\n",
    "When our system consists of multiple containers (e.g., prediction API, database, message queue), we can use **Docker Compose** to define and run them together.\n",
    "\n",
    "**Example `docker-compose.yml` for NEPSE system:**\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  predictor:\n",
    "    build: .\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "      - MODEL_PATH=/app/models/nepse_model.pkl\n",
    "    depends_on:\n",
    "      - redis\n",
    "    volumes:\n",
    "      - ./models:/app/models  # mount for live model updates\n",
    "\n",
    "  redis:\n",
    "    image: redis:alpine\n",
    "    ports:\n",
    "      - \"6379:6379\"\n",
    "    volumes:\n",
    "      - redis_data:/data\n",
    "\n",
    "volumes:\n",
    "  redis_data:\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- The `predictor` service is built from the current directory (using the Dockerfile above).  \n",
    "- It depends on `redis`, which is started first.  \n",
    "- Environment variables configure the predictor to connect to Redis.  \n",
    "- A volume mounts the local `models` directory into the container, allowing us to update the model file without rebuilding the image.  \n",
    "- The Redis service uses a named volume to persist data.\n",
    "\n",
    "Running `docker-compose up` starts both containers, and they can communicate via their service names (`redis` resolves to the Redis container).\n",
    "\n",
    "### 43.2.3 Best Practices for Containerizing ML Models\n",
    "\n",
    "- **Keep images small**: Use slim base images, multi\u2011stage builds, and remove unnecessary packages. Smaller images reduce pull times and attack surface.\n",
    "- **Never bake secrets**: Use environment variables or secret management tools (e.g., Docker secrets, Kubernetes secrets) for API keys, database passwords.\n",
    "- **Version your models**: Model files should be versioned and possibly pulled from a model registry at runtime, not baked into the image (unless the model rarely changes).\n",
    "- **Health checks**: Define `HEALTHCHECK` in the Dockerfile or in the orchestration to let the platform know when the container is ready.\n",
    "- **Log to stdout/stderr**: Containers should log to standard output; the orchestrator collects these logs.\n",
    "\n",
    "---\n",
    "\n",
    "## 43.3 Orchestration\n",
    "\n",
    "When you have multiple containers running across several machines, you need an **orchestrator** to manage them: deploy, scale, network, and heal automatically.\n",
    "\n",
    "### 43.3.1 Kubernetes\n",
    "\n",
    "Kubernetes (K8s) is the de facto standard for container orchestration. It provides:\n",
    "\n",
    "- **Pods**: The smallest deployable units (one or more containers).\n",
    "- **Deployments**: Declarative updates for Pods (supports rolling updates and rollbacks).\n",
    "- **Services**: Stable network endpoints to access a set of Pods.\n",
    "- **Ingress**: External access, often with load balancing and SSL termination.\n",
    "- **ConfigMaps and Secrets**: Configuration and sensitive data.\n",
    "- **Horizontal Pod Autoscaler**: Automatically scales the number of Pods based on CPU/memory or custom metrics.\n",
    "\n",
    "**Deploying the NEPSE predictor to Kubernetes:**\n",
    "\n",
    "First, we create a **Deployment** manifest:\n",
    "\n",
    "```yaml\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nepse-predictor-v1\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nepse-predictor\n",
    "      version: v1\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nepse-predictor\n",
    "        version: v1\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: predictor\n",
    "        image: myregistry/nepse-predictor:v1\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "        env:\n",
    "        - name: REDIS_HOST\n",
    "          value: redis-service\n",
    "        - name: MODEL_PATH\n",
    "          value: /app/models/nepse_model.pkl\n",
    "        resources:\n",
    "          requests:\n",
    "            memory: \"512Mi\"\n",
    "            cpu: \"500m\"\n",
    "          limits:\n",
    "            memory: \"1Gi\"\n",
    "            cpu: \"1000m\"\n",
    "        livenessProbe:\n",
    "          httpGet:\n",
    "            path: /health\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 30\n",
    "          periodSeconds: 10\n",
    "        readinessProbe:\n",
    "          httpGet:\n",
    "            path: /ready\n",
    "            port: 8000\n",
    "          initialDelaySeconds: 5\n",
    "          periodSeconds: 5\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- The Deployment ensures three replicas are always running.  \n",
    "- The container image is pulled from `myregistry`.  \n",
    "- Environment variables configure the connection to Redis (via a Service named `redis-service`).  \n",
    "- Resource requests guarantee minimum resources; limits prevent a container from consuming all node resources.  \n",
    "- Liveness and readiness probes let Kubernetes know when the container is healthy and ready to serve traffic.\n",
    "\n",
    "Next, a **Service** to expose the Deployment internally:\n",
    "\n",
    "```yaml\n",
    "# service.yaml\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: nepse-predictor\n",
    "spec:\n",
    "  selector:\n",
    "    app: nepse-predictor\n",
    "    version: v1\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "  type: ClusterIP\n",
    "```\n",
    "\n",
    "For external access, an **Ingress**:\n",
    "\n",
    "```yaml\n",
    "# ingress.yaml\n",
    "apiVersion: networking.k8s.io/v1\n",
    "kind: Ingress\n",
    "metadata:\n",
    "  name: nepse-predictor-ingress\n",
    "spec:\n",
    "  rules:\n",
    "  - host: predict.nepse.example.com\n",
    "    http:\n",
    "      paths:\n",
    "      - path: /\n",
    "        pathType: Prefix\n",
    "        backend:\n",
    "          service:\n",
    "            name: nepse-predictor\n",
    "            port:\n",
    "              number: 80\n",
    "```\n",
    "\n",
    "**Deploying with `kubectl`:**\n",
    "\n",
    "```bash\n",
    "kubectl apply -f deployment.yaml\n",
    "kubectl apply -f service.yaml\n",
    "kubectl apply -f ingress.yaml\n",
    "```\n",
    "\n",
    "Kubernetes will pull the image, start the Pods, and expose them through the Service and Ingress.\n",
    "\n",
    "### 43.3.2 Docker Swarm\n",
    "\n",
    "Docker Swarm is Docker\u2019s native clustering and orchestration solution. It is simpler to set up than Kubernetes but offers fewer features. It uses the same Docker Compose file format with some extensions.\n",
    "\n",
    "**Example stack file for NEPSE predictor:**\n",
    "\n",
    "```yaml\n",
    "version: '3.8'\n",
    "\n",
    "services:\n",
    "  predictor:\n",
    "    image: myregistry/nepse-predictor:v1\n",
    "    ports:\n",
    "      - \"8000:8000\"\n",
    "    environment:\n",
    "      - REDIS_HOST=redis\n",
    "    deploy:\n",
    "      replicas: 3\n",
    "      update_config:\n",
    "        parallelism: 1\n",
    "        delay: 10s\n",
    "      restart_policy:\n",
    "        condition: on-failure\n",
    "    networks:\n",
    "      - nepse-net\n",
    "\n",
    "  redis:\n",
    "    image: redis:alpine\n",
    "    networks:\n",
    "      - nepse-net\n",
    "\n",
    "networks:\n",
    "  nepse-net:\n",
    "```\n",
    "\n",
    "Deploy with `docker stack deploy -c stack.yml nepse`.\n",
    "\n",
    "### 43.3.3 Cloud Orchestration\n",
    "\n",
    "All major cloud providers offer managed Kubernetes services:\n",
    "\n",
    "- **Amazon EKS** (Elastic Kubernetes Service)\n",
    "- **Google GKE** (Google Kubernetes Engine)\n",
    "- **Azure AKS** (Azure Kubernetes Service)\n",
    "\n",
    "These services handle the control plane (master nodes) for you, making it easier to run Kubernetes without managing the underlying infrastructure.\n",
    "\n",
    "---\n",
    "\n",
    "## 43.4 Serverless Deployment\n",
    "\n",
    "Serverless platforms allow you to run code without provisioning or managing servers. They automatically scale to zero when idle, which can be cost\u2011effective for sporadic workloads. However, they typically impose cold\u2011start latencies, which may be unacceptable for real\u2011time predictions.\n",
    "\n",
    "**Options for serverless ML:**\n",
    "\n",
    "- **AWS Lambda** with container support (up to 10 GB memory, 15 min timeout).\n",
    "- **Google Cloud Run** \u2013 runs stateless containers, scales automatically, pay\u2011per\u2011request.\n",
    "- **Azure Functions** \u2013 similar.\n",
    "\n",
    "**NEPSE Example with Cloud Run:**\n",
    "\n",
    "Package your FastAPI app in a container as before. Then deploy to Cloud Run:\n",
    "\n",
    "```bash\n",
    "gcloud builds submit --tag gcr.io/myproject/nepse-predictor\n",
    "gcloud run deploy nepse-predictor --image gcr.io/myproject/nepse-predictor --platform managed --allow-unauthenticated\n",
    "```\n",
    "\n",
    "Cloud Run will provide a HTTPS endpoint. It scales from zero to many instances based on traffic. However, if a request arrives when no instance is running, there is a cold start (a few hundred milliseconds to a few seconds). For many prediction workloads, this is acceptable; for high\u2011frequency trading, it is not.\n",
    "\n",
    "---\n",
    "\n",
    "## 43.5 Edge Deployment\n",
    "\n",
    "Edge deployment runs the model on devices close to the data source (e.g., on a broker\u2019s trading workstation, or on an IoT device). Benefits include ultra\u2011low latency and offline operation. Challenges include limited compute resources and model updates.\n",
    "\n",
    "For NEPSE, edge deployment might mean running a lightweight model on a trader\u2019s laptop that makes predictions based on local data, or on a Raspberry Pi at a branch office.\n",
    "\n",
    "**Tools for edge ML:**\n",
    "\n",
    "- **TensorFlow Lite** \u2013 optimised for mobile and embedded devices.\n",
    "- **ONNX Runtime** \u2013 cross\u2011platform inference.\n",
    "- **AWS IoT Greengrass** \u2013 runs Lambda functions on edge devices.\n",
    "- **Azure IoT Edge** \u2013 similar.\n",
    "\n",
    "**Example: Converting an XGBoost model to a format that runs on a Raspberry Pi using ONNX:**\n",
    "\n",
    "```python\n",
    "import onnxmltools\n",
    "from onnxmltools.convert import convert_xgboost\n",
    "from skl2onnx.common.data_types import FloatTensorType\n",
    "\n",
    "# Load model\n",
    "model = joblib.load('nepse_xgboost.pkl')\n",
    "\n",
    "# Convert to ONNX\n",
    "initial_type = [('float_input', FloatTensorType([None, 4]))]  # 4 features\n",
    "onnx_model = convert_xgboost(model, initial_types=initial_type)\n",
    "\n",
    "# Save\n",
    "with open(\"nepse_xgboost.onnx\", \"wb\") as f:\n",
    "    f.write(onnx_model.SerializeToString())\n",
    "```\n",
    "\n",
    "On the edge device, you can load the ONNX model and run inference using ONNX Runtime.\n",
    "\n",
    "---\n",
    "\n",
    "## 43.6 Hybrid Deployment\n",
    "\n",
    "A hybrid deployment combines multiple strategies. For example, you might use Kubernetes for the core prediction service but serverless for batch processing, or edge for real\u2011time alerts and cloud for retraining.\n",
    "\n",
    "For NEPSE, a hybrid approach could be:\n",
    "\n",
    "- **Edge** on traders\u2019 workstations for ultra\u2011low latency signals.\n",
    "- **Cloud** (Kubernetes) for the main API that powers dashboards and mobile apps.\n",
    "- **Serverless** for ad\u2011hoc backtesting requests.\n",
    "\n",
    "---\n",
    "\n",
    "## 43.7 Deployment Pipelines\n",
    "\n",
    "A deployment pipeline automates the steps from code commit to production. For ML, this includes training, validation, packaging, deployment, and monitoring. Tools like Jenkins, GitLab CI, GitHub Actions, and Argo CD help orchestrate these steps.\n",
    "\n",
    "**Example GitHub Actions workflow for deploying to Kubernetes:**\n",
    "\n",
    "```yaml\n",
    "name: Deploy to Kubernetes\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  build-and-deploy:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "\n",
    "    - name: Build Docker image\n",
    "      run: docker build -t myregistry/nepse-predictor:${{ github.sha }} .\n",
    "\n",
    "    - name: Push to registry\n",
    "      run: docker push myregistry/nepse-predictor:${{ github.sha }}\n",
    "\n",
    "    - name: Set up kubectl\n",
    "      uses: azure/setup-kubectl@v1\n",
    "      with:\n",
    "        version: 'latest'\n",
    "\n",
    "    - name: Deploy to Kubernetes\n",
    "      run: |\n",
    "        kubectl set image deployment/nepse-predictor predictor=myregistry/nepse-predictor:${{ github.sha }}\n",
    "        kubectl rollout status deployment/nepse-predictor\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- On every push to main, the workflow builds a Docker image tagged with the commit SHA.  \n",
    "- It pushes the image to a container registry.  \n",
    "- It updates the Kubernetes deployment with the new image using `kubectl set image`.  \n",
    "- `rollout status` waits for the rollout to complete.\n",
    "\n",
    "This pipeline implements a **continuous deployment** strategy where every commit potentially goes to production (after tests). For more control, you might add a manual approval step.\n",
    "\n",
    "---\n",
    "\n",
    "## 43.8 Rollback Strategies\n",
    "\n",
    "No matter how careful you are, a new model version may introduce errors. You must be able to roll back quickly.\n",
    "\n",
    "**Rollback mechanisms:**\n",
    "\n",
    "- **Kubernetes rollback**: `kubectl rollout undo deployment/nepse-predictor` reverts to the previous revision.\n",
    "- **Blue\u2011green**: Simply switch the router back to blue.\n",
    "- **Canary**: Reduce the weight of the new version to zero and shift all traffic back to the old version.\n",
    "- **Database rollbacks**: If the new model writes predictions to a database, you might need to mark or delete them. Ideally, model writes are idempotent and easily reversible.\n",
    "\n",
    "**Testing rollbacks:** Practice them in a staging environment. Measure the time to revert; it should be within your service level objectives.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we covered the essential strategies and technologies for deploying machine learning models into production, using the NEPSE prediction system as a running example.\n",
    "\n",
    "- We explored **deployment patterns** (canary, blue\u2011green, shadow) that allow safe introduction of new model versions.\n",
    "- We learned to **containerize** our model with Docker, creating portable images that run anywhere.\n",
    "- We used **Docker Compose** to orchestrate multi\u2011container applications locally.\n",
    "- We dived into **Kubernetes** for production\u2011grade orchestration, with examples of Deployments, Services, and Ingress.\n",
    "- We touched on **serverless** and **edge** deployment options for different latency and cost requirements.\n",
    "- We designed a **deployment pipeline** using GitHub Actions to automate builds and deployments.\n",
    "- Finally, we discussed **rollback strategies** to recover from failures.\n",
    "\n",
    "With these tools and patterns, you can deploy your NEPSE model\u2014or any time\u2011series prediction model\u2014reliably and safely. The next chapter will cover **Monitoring and Observability**, ensuring that once deployed, you can keep track of your model\u2019s health and performance in real time.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 43**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='42. real_time_prediction_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='44. monitoring_and_observability.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}