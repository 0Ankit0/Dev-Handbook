{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 44: Monitoring and Observability\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the three pillars of observability—metrics, logs, and traces—and how they apply to a prediction system\n",
    "- Set up monitoring for system‑level metrics such as CPU, memory, latency, and throughput of your NEPSE prediction service\n",
    "- Instrument your application to emit custom metrics (e.g., number of predictions, model inference time, prediction distribution)\n",
    "- Implement structured logging to capture detailed events for debugging and audit trails\n",
    "- Use distributed tracing to follow a request through multiple microservices\n",
    "- Configure alerting rules to notify you of anomalies or service degradation\n",
    "- Build real‑time dashboards with Grafana to visualise system health and model performance\n",
    "- Detect and respond to model drift (data drift and concept drift) using statistical tests and monitoring tools\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Once your NEPSE stock prediction model is deployed into production, the journey is far from over. In fact, it has just begun. A model that performs well today may degrade tomorrow because of changing market conditions, data distribution shifts, or infrastructure issues. **Monitoring and observability** are what allow you to detect, diagnose, and remediate these problems before they affect users.\n",
    "\n",
    "Observability goes beyond traditional monitoring. It gives you the ability to ask arbitrary questions about your system’s internal state based on the data it emits—metrics, logs, and traces. In this chapter, we will build a comprehensive observability stack for our real‑time prediction system. We will instrument the Python service, collect metrics with Prometheus, visualise them in Grafana, implement structured logging with Elasticsearch and Kibana (or a lighter stack), and add distributed tracing with Jaeger. Finally, we will discuss how to monitor for model drift, a critical concern for any machine learning system.\n",
    "\n",
    "---\n",
    "\n",
    "## 44.1 The Three Pillars of Observability\n",
    "\n",
    "Observability is built on three complementary data types:\n",
    "\n",
    "1. **Metrics** – Numerical measurements aggregated over time (e.g., requests per second, error rate, latency percentiles). Metrics are lightweight and ideal for alerting and dashboards.\n",
    "2. **Logs** – Discrete events with timestamps and structured or unstructured messages. Logs provide detailed context for debugging.\n",
    "3. **Traces** – Records of a request’s journey through distributed services, showing where time is spent and where errors occur.\n",
    "\n",
    "A mature observability practice uses all three in concert. For our NEPSE system, we will:\n",
    "\n",
    "- **Metrics**: Count predictions per symbol, measure inference latency, track CPU and memory usage.\n",
    "- **Logs**: Log each prediction request with input features, predicted probability, and any warnings.\n",
    "- **Traces**: Trace a prediction request from the API gateway through the model inference to the database.\n",
    "\n",
    "---\n",
    "\n",
    "## 44.2 System Metrics\n",
    "\n",
    "System metrics tell us about the health of the infrastructure: are the servers overloaded? Is the network saturated? These metrics are typically collected by agents like **Prometheus Node Exporter** (for host metrics) and **cAdvisor** (for container metrics). For our Python service, we can expose custom application metrics via the Prometheus client library.\n",
    "\n",
    "### 44.2.1 Exposing Metrics from a Python Service\n",
    "\n",
    "First, install the Prometheus client:\n",
    "\n",
    "```bash\n",
    "pip install prometheus-client\n",
    "```\n",
    "\n",
    "Then, in your FastAPI application, create a metrics endpoint that Prometheus can scrape.\n",
    "\n",
    "```python\n",
    "# app/metrics.py\n",
    "from prometheus_client import Counter, Histogram, generate_latest, REGISTRY\n",
    "from fastapi import Response\n",
    "import time\n",
    "\n",
    "# Define metrics\n",
    "PREDICTIONS = Counter('predictions_total', 'Total number of predictions', ['symbol'])\n",
    "LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency in seconds', buckets=[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5])\n",
    "ERRORS = Counter('prediction_errors_total', 'Total prediction errors', ['error_type'])\n",
    "\n",
    "# Instrument the prediction function\n",
    "@app.post(\"/predict\")\n",
    "async def predict(symbol: str, features: dict):\n",
    "    start = time.time()\n",
    "    try:\n",
    "        # ... actual prediction logic ...\n",
    "        PREDICTIONS.labels(symbol=symbol).inc()\n",
    "        latency = time.time() - start\n",
    "        LATENCY.observe(latency)\n",
    "        return {\"probability\": prob}\n",
    "    except Exception as e:\n",
    "        ERRORS.labels(error_type=type(e).__name__).inc()\n",
    "        raise\n",
    "\n",
    "# Expose metrics endpoint\n",
    "@app.get(\"/metrics\")\n",
    "async def get_metrics():\n",
    "    return Response(content=generate_latest(REGISTRY), media_type=\"text/plain\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- We define a `Counter` for predictions, labelled by stock symbol. This allows us to see prediction volume per symbol.  \n",
    "- A `Histogram` tracks latency distribution. The buckets are chosen to capture the typical range (from a few milliseconds to a couple of seconds).  \n",
    "- An error counter helps track failure rates.  \n",
    "- The `/metrics` endpoint returns all metrics in the format Prometheus expects. Prometheus will scrape this endpoint periodically.\n",
    "\n",
    "### 44.2.2 Running Prometheus\n",
    "\n",
    "Prometheus is a time‑series database that scrapes metrics from configured targets. A basic `prometheus.yml` configuration:\n",
    "\n",
    "```yaml\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'nepse-predictor'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:8000']\n",
    "```\n",
    "\n",
    "Start Prometheus with:\n",
    "\n",
    "```bash\n",
    "prometheus --config.file=prometheus.yml\n",
    "```\n",
    "\n",
    "### 44.2.3 Visualising with Grafana\n",
    "\n",
    "Grafana connects to Prometheus and builds dashboards. After installing Grafana, add Prometheus as a data source. Then create a dashboard with panels for:\n",
    "\n",
    "- Prediction rate (rate(predictions_total[5m]))\n",
    "- Error rate (rate(prediction_errors_total[5m]))\n",
    "- Latency p99 (histogram_quantile(0.99, sum(rate(prediction_latency_seconds_bucket[5m])) by (le)))\n",
    "- CPU and memory (from Node Exporter)\n",
    "\n",
    "**Example query for latency p99:**  \n",
    "```\n",
    "histogram_quantile(0.99, sum(rate(prediction_latency_seconds_bucket[5m])) by (le))\n",
    "```\n",
    "\n",
    "This gives the 99th percentile latency over the last 5 minutes.\n",
    "\n",
    "---\n",
    "\n",
    "## 44.3 Application Logging\n",
    "\n",
    "While metrics give aggregate numbers, logs provide individual events. For a prediction service, you might log:\n",
    "\n",
    "- Each prediction request and response\n",
    "- Model version used\n",
    "- Any anomalies (e.g., input features out of expected range)\n",
    "- Errors with stack traces\n",
    "\n",
    "### 44.3.1 Structured Logging with Python\n",
    "\n",
    "Using the `structlog` library, we can output JSON‑formatted logs that are easy to ingest into systems like Elasticsearch.\n",
    "\n",
    "```python\n",
    "import structlog\n",
    "import logging\n",
    "\n",
    "# Configure structlog to output JSON\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.processors.JSONRenderer()\n",
    "    ],\n",
    "    context_class=dict,\n",
    "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
    ")\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(symbol: str, features: dict):\n",
    "    log = logger.bind(symbol=symbol, model_version=\"v1.2\")\n",
    "    log.info(\"prediction_start\", features=features)\n",
    "    try:\n",
    "        prob = model.predict_proba([features])[0][1]\n",
    "        log.info(\"prediction_success\", probability=prob)\n",
    "        return {\"probability\": prob}\n",
    "    except Exception as e:\n",
    "        log.error(\"prediction_error\", error=str(e), exc_info=True)\n",
    "        raise\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- `structlog` enriches logs with timestamps and allows key‑value pairs.  \n",
    "- We bind common fields (symbol, model version) to a logger instance so they appear in every log line from that request.  \n",
    "- The final output is a JSON line, e.g.,  \n",
    "  `{\"event\": \"prediction_success\", \"timestamp\": \"2025-03-15T10:30:00Z\", \"symbol\": \"NEPSE\", \"model_version\": \"v1.2\", \"probability\": 0.87}`.\n",
    "\n",
    "### 44.3.2 Log Aggregation\n",
    "\n",
    "For a production system, logs should be collected centrally. A common stack is **Elasticsearch, Logstash, and Kibana (ELK)** or the lighter **Loki** from Grafana Labs.\n",
    "\n",
    "**Fluentd** or **Fluent Bit** can be deployed as a daemonset in Kubernetes to ship logs to Elasticsearch or Loki.\n",
    "\n",
    "With Loki, you can query logs using LogQL and correlate them with metrics in Grafana.\n",
    "\n",
    "---\n",
    "\n",
    "## 44.4 Distributed Tracing\n",
    "\n",
    "When your system consists of multiple services (e.g., prediction API, feature store, database), a single user request may span several components. Distributed tracing helps you understand the end‑to‑end flow and pinpoint bottlenecks.\n",
    "\n",
    "**OpenTelemetry** is the emerging standard for instrumenting applications. It supports multiple backends like Jaeger, Zipkin, and Tempo.\n",
    "\n",
    "### 44.4.1 Instrumenting with OpenTelemetry\n",
    "\n",
    "Install the required packages:\n",
    "\n",
    "```bash\n",
    "pip install opentelemetry-distro opentelemetry-exporter-jaeger\n",
    "```\n",
    "\n",
    "Set up tracing in your FastAPI app:\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n",
    "from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\n",
    "from opentelemetry.sdk.resources import SERVICE_NAME, Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "\n",
    "# Configure tracer provider\n",
    "resource = Resource(attributes={\n",
    "    SERVICE_NAME: \"nepse-predictor\"\n",
    "})\n",
    "provider = TracerProvider(resource=resource)\n",
    "\n",
    "# Configure Jaeger exporter\n",
    "jaeger_exporter = JaegerExporter(\n",
    "    agent_host_name=\"localhost\",\n",
    "    agent_port=6831,\n",
    ")\n",
    "\n",
    "# Add span processor\n",
    "provider.add_span_processor(BatchSpanProcessor(jaeger_exporter))\n",
    "trace.set_tracer_provider(provider)\n",
    "\n",
    "# Instrument FastAPI\n",
    "app = FastAPI()\n",
    "FastAPIInstrumentor.instrument_app(app)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- The tracer provider creates spans for each request.  \n",
    "- The Jaeger exporter sends spans to a Jaeger agent (running locally or in the cluster).  \n",
    "- `FastAPIInstrumentor` automatically traces incoming requests and creates spans for each endpoint.\n",
    "\n",
    "Now you can view traces in the Jaeger UI, seeing how long each part of the request took.\n",
    "\n",
    "---\n",
    "\n",
    "## 44.5 Alerting\n",
    "\n",
    "Metrics are useless if no one looks at them. Alerting notifies you when something goes wrong. With Prometheus, you define alerting rules, and **Alertmanager** handles routing to channels like email, Slack, PagerDuty.\n",
    "\n",
    "### 44.5.1 Defining Alert Rules\n",
    "\n",
    "Create a file `alerts.yml`:\n",
    "\n",
    "```yaml\n",
    "groups:\n",
    "  - name: nepse_alerts\n",
    "    rules:\n",
    "      - alert: HighErrorRate\n",
    "        expr: rate(prediction_errors_total[5m]) > 0.01\n",
    "        for: 2m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"High error rate on {{ $labels.job }}\"\n",
    "          description: \"Error rate is {{ $value }} errors/s for job {{ $labels.job }}\"\n",
    "\n",
    "      - alert: HighLatency\n",
    "        expr: histogram_quantile(0.99, sum(rate(prediction_latency_seconds_bucket[5m])) by (le)) > 0.5\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"High latency for predictions\"\n",
    "```\n",
    "\n",
    "Then include this file in your Prometheus configuration under `rule_files`.\n",
    "\n",
    "### 44.5.2 Configuring Alertmanager\n",
    "\n",
    "Alertmanager receives alerts from Prometheus and sends notifications. A minimal configuration to send to Slack:\n",
    "\n",
    "```yaml\n",
    "route:\n",
    "  group_by: ['alertname']\n",
    "  group_wait: 10s\n",
    "  group_interval: 10s\n",
    "  repeat_interval: 1h\n",
    "  receiver: 'slack'\n",
    "\n",
    "receivers:\n",
    "  - name: 'slack'\n",
    "    slack_configs:\n",
    "      - api_url: 'https://hooks.slack.com/services/...'\n",
    "        channel: '#alerts'\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 44.6 Dashboard Design\n",
    "\n",
    "Dashboards should provide a high‑level overview of system health and drill‑down capabilities. For the NEPSE system, consider these panels:\n",
    "\n",
    "- **Request Rate** – total predictions per second, coloured by symbol.\n",
    "- **Latency** – p50, p95, p99 latency over time.\n",
    "- **Error Rate** – percentage of failed predictions.\n",
    "- **Model Drift** – plots of feature distributions over time (from logs or feature store).\n",
    "- **Resource Usage** – CPU, memory of each pod.\n",
    "\n",
    "Grafana allows you to create variables (e.g., `$symbol`) to filter panels dynamically.\n",
    "\n",
    "**Example panel for prediction rate by symbol:**  \n",
    "Query: `sum(rate(predictions_total[$__rate_interval])) by (symbol)`\n",
    "\n",
    "**Dashboard screenshot** (described textually):  \n",
    "A line chart showing lines for each symbol, a latency heatmap, a table of recent errors.\n",
    "\n",
    "---\n",
    "\n",
    "## 44.7 Model Drift Detection\n",
    "\n",
    "Beyond system health, we must monitor the model itself. **Drift** occurs when the statistical properties of the input data or the relationship between inputs and outputs change over time. Two main types:\n",
    "\n",
    "- **Data drift**: The distribution of input features shifts (e.g., average traded volume changes).\n",
    "- **Concept drift**: The relationship between features and target changes (e.g., previously strong indicators become weak).\n",
    "\n",
    "### 44.7.1 Detecting Data Drift\n",
    "\n",
    "We can compare the current feature distribution with a reference distribution (e.g., training data) using statistical tests.\n",
    "\n",
    "**Example using the `evidently` library:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import DataDriftPreset\n",
    "\n",
    "# Reference data (training set)\n",
    "reference = pd.read_csv('nepse_training_features.csv')\n",
    "\n",
    "# Current production data (last 1000 predictions)\n",
    "current = get_production_features(1000)\n",
    "\n",
    "# Generate drift report\n",
    "report = Report(metrics=[DataDriftPreset()])\n",
    "report.run(reference_data=reference, current_data=current)\n",
    "report.save_html('drift_report.html')\n",
    "```\n",
    "\n",
    "Evidently computes drift for each feature using statistical tests (e.g., Kolmogorov‑Smirnov for numerical, chi‑square for categorical) and produces an HTML report.\n",
    "\n",
    "### 44.7.2 Detecting Concept Drift\n",
    "\n",
    "Concept drift is harder to detect because it requires ground truth labels, which may be delayed (e.g., next day’s actual price). We can monitor prediction error over time on a held‑out validation set or on production data when labels become available.\n",
    "\n",
    "**Implementation with Prometheus:**  \n",
    "Emit a metric `prediction_error` that records the absolute error when the true value is known. Then alert if the rolling average exceeds a threshold.\n",
    "\n",
    "```python\n",
    "ERROR = Histogram('prediction_error', 'Prediction absolute error', buckets=[0.01, 0.05, 0.1, 0.2, 0.5])\n",
    "...\n",
    "# When true value arrives (e.g., next day)\n",
    "error = abs(true_value - predicted_value)\n",
    "ERROR.observe(error)\n",
    "```\n",
    "\n",
    "Then create an alert if the recent error rate is too high:\n",
    "\n",
    "```yaml\n",
    "- alert: HighPredictionError\n",
    "  expr: rate(prediction_error_sum[1d]) / rate(prediction_error_count[1d]) > 0.1\n",
    "  for: 1h\n",
    "```\n",
    "\n",
    "### 44.7.3 Automated Response to Drift\n",
    "\n",
    "When drift is detected, you may want to trigger actions:\n",
    "\n",
    "- Log a warning\n",
    "- Send an alert\n",
    "- Automatically trigger model retraining\n",
    "- Roll back to a previous model version\n",
    "\n",
    "This can be orchestrated with a tool like **Apache Airflow** or **Kubeflow**.\n",
    "\n",
    "---\n",
    "\n",
    "## 44.8 Putting It All Together\n",
    "\n",
    "A complete observability stack for the NEPSE prediction system might look like this:\n",
    "\n",
    "- **Prometheus** for metrics (application and infrastructure).\n",
    "- **Grafana** for dashboards (visualisation) and alerting.\n",
    "- **Loki** for log aggregation (or Elasticsearch).\n",
    "- **Jaeger** for distributed tracing.\n",
    "- **Evidently** (or custom scripts) for periodic drift analysis.\n",
    "\n",
    "All components can run in Kubernetes using Helm charts (e.g., kube‑prometheus‑stack, loki‑stack, jaeger‑operator).\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we built a comprehensive observability framework for our NEPSE real‑time prediction system. We covered:\n",
    "\n",
    "- The three pillars of observability—metrics, logs, traces—and why each is necessary.\n",
    "- Instrumenting a Python FastAPI service with Prometheus metrics, including custom counters and histograms.\n",
    "- Configuring Prometheus to scrape metrics and Grafana to visualise them.\n",
    "- Implementing structured logging with `structlog` and sending logs to a central aggregator.\n",
    "- Adding distributed tracing with OpenTelemetry and Jaeger.\n",
    "- Setting up alerting rules in Prometheus and routing notifications via Alertmanager.\n",
    "- Detecting data drift and concept drift using statistical tests and error monitoring.\n",
    "- Designing dashboards that give both operational and business visibility.\n",
    "\n",
    "With these tools in place, you can ensure that your NEPSE prediction system remains reliable, performant, and accurate over time. In the next chapter, we will explore **Model Drift Detection** in greater depth, focusing on automated retraining strategies to keep your model fresh.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 44**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
