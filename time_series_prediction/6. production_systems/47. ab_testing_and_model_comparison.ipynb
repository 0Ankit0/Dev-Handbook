{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 47: A/B Testing and Model Comparison\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamentals of A/B testing and why it is essential for validating model improvements in production\n",
    "- Design an A/B test for a time‑series prediction system, including proper traffic splitting and metric selection\n",
    "- Calculate sample sizes required to achieve statistically significant results\n",
    "- Apply statistical significance tests (t‑test, chi‑square, Mann–Whitney U) to compare model performance\n",
    "- Implement multi‑armed bandit algorithms to dynamically allocate traffic while testing\n",
    "- Conduct multivariate tests when multiple factors change simultaneously\n",
    "- Build a model comparison framework that tracks multiple models over time\n",
    "- Measure business impact (e.g., trading returns) rather than just technical metrics\n",
    "- Interpret test results and avoid common pitfalls like p‑hacking and peeking\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Imagine you have trained a new model for predicting NEPSE stock movements—perhaps it uses a more advanced feature set or a deeper neural network. You believe it will outperform the current production model. But before you replace the old model, you must be certain that the new one is genuinely better, not just luck on a particular test set. **A/B testing** (also known as split testing) provides a rigorous framework for comparing two (or more) versions of a model by exposing them to real user traffic and measuring their performance.\n",
    "\n",
    "In the context of the NEPSE prediction system, an A/B test might compare two models that serve predictions to a downstream trading algorithm. The key challenge is that the true labels (whether the price actually went up the next day) are only available after the fact. Moreover, financial data is noisy, and even a genuinely superior model may appear worse over short periods due to random fluctuations.\n",
    "\n",
    "This chapter will guide you through the design, execution, and analysis of A/B tests for machine learning models. We will cover sample size calculation, metric selection, statistical significance, and more advanced techniques like multi‑armed bandits that can accelerate learning. Using the NEPSE example, we will also discuss how to measure business impact—such as simulated trading returns—rather than just accuracy metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## 47.1 A/B Testing Fundamentals\n",
    "\n",
    "A/B testing is a controlled experiment with two variants: **A** (the control, usually the current production model) and **B** (the treatment, the candidate model). Users or prediction requests are randomly assigned to one of the variants, and their outcomes are tracked. At the end of the experiment, we compare the performance metrics between the two groups to decide if the difference is statistically significant.\n",
    "\n",
    "### 47.1.1 Key Concepts\n",
    "\n",
    "- **Randomisation**: Essential to ensure that any differences are due to the model, not confounding factors.\n",
    "- **Sample size**: The number of observations needed to detect a meaningful effect with sufficient power.\n",
    "- **Metric**: The quantitative measure we use to compare models (e.g., accuracy, F1 score, profit).\n",
    "- **Statistical significance**: The probability that the observed difference is not due to random chance (usually p‑value < 0.05).\n",
    "- **Practical significance**: The effect size is large enough to matter in the business context.\n",
    "\n",
    "For the NEPSE system, the unit of randomisation could be each prediction request, or it could be each stock symbol if we want to avoid mixing predictions for the same symbol across models (which could confuse downstream logic). We’ll discuss this trade‑off.\n",
    "\n",
    "---\n",
    "\n",
    "## 47.2 Experimental Design for Time‑Series Models\n",
    "\n",
    "Time‑series data introduces unique challenges for A/B testing because observations are not independent. If you assign each request randomly, a single symbol might receive predictions from both models on different days, which is acceptable as long as the assignment is independent. However, if the model is used for trading decisions, consistency may matter. Often, it’s simpler to randomise by time periods (e.g., odd days use model A, even days use model B), but this can be confounded by day‑of‑week effects. A better approach is to randomise by symbol: assign each stock symbol to either control or treatment for the entire duration of the test. This ensures that each symbol's predictions are always from the same model, making the comparison cleaner.\n",
    "\n",
    "### 47.2.1 Randomisation by Symbol\n",
    "\n",
    "```python\n",
    "import hashlib\n",
    "import pandas as pd\n",
    "\n",
    "def assign_symbol_to_group(symbol, salt=\"nepse_test_v1\", split=0.5):\n",
    "    \"\"\"Deterministic assignment based on symbol hash.\"\"\"\n",
    "    hash_val = int(hashlib.md5((symbol + salt).encode()).hexdigest(), 16)\n",
    "    return 'treatment' if (hash_val % 100) < split * 100 else 'control'\n",
    "\n",
    "# Example: NEPSE stocks\n",
    "symbols = ['NABIL', 'NTC', 'SBI', 'HRL', 'NICA']\n",
    "for sym in symbols:\n",
    "    group = assign_symbol_to_group(sym, split=0.5)\n",
    "    print(f\"{sym}: {group}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Using a hash of the symbol plus a random salt ensures that the assignment is stable across runs. The salt can be changed for a new test. This method guarantees that the same symbol always receives the same model during the experiment.\n",
    "\n",
    "### 47.2.2 Metric Selection\n",
    "\n",
    "What should we measure? Common choices:\n",
    "\n",
    "- **Accuracy / F1 score** – If the task is classification (up/down).\n",
    "- **Mean Absolute Error (MAE)** – If predicting exact price.\n",
    "- **Profit from a trading strategy** – The ultimate business metric. For example, simulate a simple strategy: buy if predicted probability > 0.6, sell otherwise, and compute total return.\n",
    "\n",
    "Using a trading simulation is more realistic because it accounts for the magnitude of moves, not just direction.\n",
    "\n",
    "**Example of computing daily return from predictions:**\n",
    "\n",
    "```python\n",
    "def simulate_trading(predictions_df):\n",
    "    \"\"\"\n",
    "    predictions_df has columns: date, symbol, predicted_prob, actual_return\n",
    "    Strategy: if predicted_prob > 0.6, take long position (expect up),\n",
    "              if predicted_prob < 0.4, take short position (expect down),\n",
    "              else stay out.\n",
    "    \"\"\"\n",
    "    positions = 0\n",
    "    returns = []\n",
    "    for _, row in predictions_df.iterrows():\n",
    "        if row['predicted_prob'] > 0.6:\n",
    "            returns.append(row['actual_return'])   # long\n",
    "        elif row['predicted_prob'] < 0.4:\n",
    "            returns.append(-row['actual_return'])  # short\n",
    "        else:\n",
    "            returns.append(0)                       # out\n",
    "    return sum(returns)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 47.3 Sample Size Calculation\n",
    "\n",
    "Before running an A/B test, you must determine how many observations you need to detect a meaningful difference. This depends on:\n",
    "\n",
    "- **Baseline metric** (e.g., current model's accuracy)\n",
    "- **Minimum detectable effect** (MDE) – the smallest improvement you care about\n",
    "- **Significance level** (α) – usually 0.05\n",
    "- **Statistical power** (1‑β) – usually 0.8\n",
    "\n",
    "For a two‑sample test of proportions (e.g., accuracy), you can use a formula or a library like `statsmodels`.\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.power import NormalIndPower\n",
    "from statsmodels.stats.proportion import proportion_effectsize\n",
    "\n",
    "# Current accuracy\n",
    "p1 = 0.55\n",
    "# Desired improvement (MDE)\n",
    "p2 = 0.58\n",
    "effect_size = proportion_effectsize(p1, p2)\n",
    "\n",
    "power_analysis = NormalIndPower()\n",
    "sample_size = power_analysis.solve_power(\n",
    "    effect_size=effect_size,\n",
    "    alpha=0.05,\n",
    "    power=0.8,\n",
    "    alternative='larger'\n",
    ")\n",
    "print(f\"Required sample size per group: {int(sample_size)}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`proportion_effectsize` computes Cohen's h for proportions. `solve_power` returns the number of observations needed in each group. For NEPSE, if you have ~250 trading days per year and many symbols, you might need several months of data to reach the required sample size.\n",
    "\n",
    "---\n",
    "\n",
    "## 47.4 Statistical Significance Testing\n",
    "\n",
    "After collecting data, you need to test whether the observed difference is statistically significant. The choice of test depends on the metric:\n",
    "\n",
    "- **Binary outcomes** (e.g., correct/incorrect prediction): use a chi‑square test or a two‑proportion z‑test.\n",
    "- **Continuous outcomes** (e.g., trading profit, absolute error): use a t‑test (if normally distributed) or Mann‑Whitney U test (non‑parametric).\n",
    "\n",
    "### 47.4.1 Example: Comparing Accuracies with Chi‑Square\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Suppose we have:\n",
    "# Control group: 1000 predictions, 550 correct (55%)\n",
    "# Treatment group: 1000 predictions, 580 correct (58%)\n",
    "control_correct = 550\n",
    "control_total = 1000\n",
    "treatment_correct = 580\n",
    "treatment_total = 1000\n",
    "\n",
    "# Create contingency table\n",
    "table = [[control_correct, control_total - control_correct],\n",
    "         [treatment_correct, treatment_total - treatment_correct]]\n",
    "\n",
    "chi2, p_value, dof, expected = chi2_contingency(table)\n",
    "print(f\"Chi-square = {chi2:.4f}, p-value = {p_value:.4f}\")\n",
    "if p_value < 0.05:\n",
    "    print(\"Statistically significant difference.\")\n",
    "else:\n",
    "    print(\"Not significant.\")\n",
    "```\n",
    "\n",
    "### 47.4.2 Example: Comparing Trading Returns with t‑test\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# daily_returns_control: list of daily returns for control group\n",
    "# daily_returns_treatment: list for treatment group\n",
    "t_stat, p_value = ttest_ind(daily_returns_control, daily_returns_treatment, equal_var=False)\n",
    "print(f\"t-statistic = {t_stat:.4f}, p-value = {p_value:.4f}\")\n",
    "```\n",
    "\n",
    "### 47.4.3 Multiple Testing and Peeking\n",
    "\n",
    "If you check the p‑value every day and stop as soon as it becomes significant, you inflate the false positive rate. This is called **peeking**. To avoid this, you should decide the sample size in advance and not look at the results until the experiment is complete. Alternatively, use sequential testing methods that adjust for multiple looks.\n",
    "\n",
    "---\n",
    "\n",
    "## 47.5 Multi‑Armed Bandit\n",
    "\n",
    "Traditional A/B testing is static: you split traffic equally and wait until the end. A **multi‑armed bandit** dynamically allocates more traffic to the better‑performing variant as data accumulates, reducing the opportunity cost of a bad model. It also can produce a conclusion faster, though with more complexity.\n",
    "\n",
    "### 47.5.1 Epsilon‑Greedy Algorithm\n",
    "\n",
    "A simple bandit: with probability ε, explore (choose a random model); with probability 1‑ε, exploit (choose the model with the highest average reward so far).\n",
    "\n",
    "```python\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "class EpsilonGreedy:\n",
    "    def __init__(self, n_models, epsilon=0.1):\n",
    "        self.epsilon = epsilon\n",
    "        self.counts = [0] * n_models   # number of times each model was used\n",
    "        self.values = [0.0] * n_models # average reward for each model\n",
    "\n",
    "    def select_model(self):\n",
    "        if random.random() < self.epsilon:\n",
    "            return random.randint(0, len(self.values)-1)  # explore\n",
    "        else:\n",
    "            return np.argmax(self.values)                 # exploit\n",
    "\n",
    "    def update(self, chosen_model, reward):\n",
    "        self.counts[chosen_model] += 1\n",
    "        n = self.counts[chosen_model]\n",
    "        value = self.values[chosen_model]\n",
    "        # Incremental average\n",
    "        self.values[chosen_model] = ((n-1) * value + reward) / n\n",
    "\n",
    "# Simulate\n",
    "bandit = EpsilonGreedy(2, epsilon=0.1)\n",
    "for _ in range(1000):\n",
    "    model = bandit.select_model()\n",
    "    # Simulate reward (1 for correct prediction, 0 otherwise)\n",
    "    # Assume model 0 has true reward 0.55, model 1 has 0.58\n",
    "    reward = 1 if (model == 0 and random.random() < 0.55) or (model == 1 and random.random() < 0.58) else 0\n",
    "    bandit.update(model, reward)\n",
    "\n",
    "print(f\"Model 0 average: {bandit.values[0]:.4f}, pulls: {bandit.counts[0]}\")\n",
    "print(f\"Model 1 average: {bandit.values[1]:.4f}, pulls: {bandit.counts[1]}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The bandit will eventually allocate most pulls to the better model (model 1) while still occasionally exploring. This can be used in production to serve predictions while learning.\n",
    "\n",
    "### 47.5.2 Thompson Sampling\n",
    "\n",
    "Thompson Sampling is a Bayesian approach that maintains a belief distribution for each model's reward and samples from that distribution to choose the model. It often converges faster than epsilon‑greedy.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class ThompsonSampling:\n",
    "    def __init__(self, n_models):\n",
    "        # Beta distribution parameters (successes, failures)\n",
    "        self.alphas = [1] * n_models\n",
    "        self.betas = [1] * n_models\n",
    "\n",
    "    def select_model(self):\n",
    "        samples = [np.random.beta(a, b) for a, b in zip(self.alphas, self.betas)]\n",
    "        return np.argmax(samples)\n",
    "\n",
    "    def update(self, chosen_model, reward):\n",
    "        if reward == 1:\n",
    "            self.alphas[chosen_model] += 1\n",
    "        else:\n",
    "            self.betas[chosen_model] += 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 47.6 Multivariate Testing\n",
    "\n",
    "Sometimes you want to test multiple changes simultaneously, e.g., different feature sets and different algorithms. A multivariate test (also called factorial experiment) can evaluate several factors at once. For example:\n",
    "\n",
    "- Factor 1: Model architecture (LSTM vs. XGBoost)\n",
    "- Factor 2: Feature set (with vs. without sentiment features)\n",
    "\n",
    "This gives 2 × 2 = 4 combinations. You can then analyse main effects and interactions.\n",
    "\n",
    "### 47.6.1 Analysing Multivariate Results\n",
    "\n",
    "Use ANOVA (Analysis of Variance) to test whether factors have significant effects. However, multivariate tests require larger sample sizes because you are testing multiple hypotheses.\n",
    "\n",
    "---\n",
    "\n",
    "## 47.7 Model Comparison Framework\n",
    "\n",
    "Beyond one‑off A/B tests, you may want to continuously compare multiple models in production. This can be done with a **model registry** that tracks performance metrics over time, and a **champion/challenger** system where one model serves most traffic and a few challengers get a small slice.\n",
    "\n",
    "### 47.7.1 Building a Champion/Challenger System\n",
    "\n",
    "```python\n",
    "class ChampionChallenger:\n",
    "    def __init__(self, champion_model, challenger_models, traffic_split):\n",
    "        self.champion = champion_model\n",
    "        self.challengers = challenger_models\n",
    "        self.split = traffic_split  # e.g., [0.8, 0.1, 0.1] for champion + 2 challengers\n",
    "\n",
    "    def predict(self, features):\n",
    "        # Choose model based on probabilities\n",
    "        choice = np.random.choice(len(self.split), p=self.split)\n",
    "        if choice == 0:\n",
    "            return self.champion.predict(features)\n",
    "        else:\n",
    "            return self.challengers[choice-1].predict(features)\n",
    "```\n",
    "\n",
    "Periodically, you evaluate the performance of each challenger against the champion using a statistical test. If a challenger is significantly better, it becomes the new champion.\n",
    "\n",
    "---\n",
    "\n",
    "## 47.8 Business Impact Analysis\n",
    "\n",
    "Technical metrics (accuracy, AUC) are useful but do not always translate to business value. For the NEPSE system, the ultimate measure might be **profit** from a trading strategy. When analysing an A/B test, you should compute the business impact directly.\n",
    "\n",
    "### 47.8.1 Example: Profit per Trade\n",
    "\n",
    "Suppose you have a trading algorithm that uses the model's predictions. You can simulate its performance on the historical data for both control and treatment groups. Then compare the total profit or Sharpe ratio.\n",
    "\n",
    "**Caveat:** The trading strategy itself may introduce variance. You might need to run multiple simulations or use bootstrap to get confidence intervals.\n",
    "\n",
    "```python\n",
    "def bootstrap_profit(predictions_df, n_bootstrap=1000):\n",
    "    profits = []\n",
    "    for _ in range(n_bootstrap):\n",
    "        sample = predictions_df.sample(frac=1.0, replace=True)\n",
    "        profit = simulate_trading(sample)\n",
    "        profits.append(profit)\n",
    "    return np.percentile(profits, [2.5, 97.5])  # 95% CI\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 47.9 Best Practices\n",
    "\n",
    "1. **Pre‑register the experiment** – Define your hypothesis, metrics, and sample size before starting.\n",
    "2. **Randomise properly** – Avoid systematic biases.\n",
    "3. **Monitor for side effects** – The new model might be better on your primary metric but worse on others (e.g., latency). Track multiple metrics.\n",
    "4. **Use both statistical and practical significance** – A tiny improvement may not be worth the deployment cost.\n",
    "5. **Segment analysis** – The new model might work better for some symbols or market conditions. Check for heterogeneity.\n",
    "6. **Avoid peeking** – Stick to the planned duration, or use sequential testing.\n",
    "7. **Document results** – Save all experiment data for future reference.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored how to rigorously compare models in production using A/B testing and related techniques, all within the context of the NEPSE prediction system. We covered:\n",
    "\n",
    "- The fundamentals of A/B testing and its importance for validating model improvements.\n",
    "- Experimental design considerations for time‑series data, including randomisation by symbol and metric selection.\n",
    "- Sample size calculation to ensure experiments are adequately powered.\n",
    "- Statistical significance tests for different metric types (chi‑square, t‑test, Mann‑Whitney).\n",
    "- The dangers of peeking and how to avoid inflated false positives.\n",
    "- Multi‑armed bandit algorithms (epsilon‑greedy, Thompson sampling) that dynamically allocate traffic while testing.\n",
    "- Multivariate testing for evaluating multiple factors simultaneously.\n",
    "- Building a champion/challenger framework for continuous model comparison.\n",
    "- Translating technical metrics into business impact, such as trading profits.\n",
    "\n",
    "By applying these principles, you can make data‑driven decisions about model updates, ensuring that only genuinely better models reach production. In the next chapter, we will discuss **Scalability and Performance Optimization**, focusing on how to handle increasing data volumes and prediction loads in your NEPSE system.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 47**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
