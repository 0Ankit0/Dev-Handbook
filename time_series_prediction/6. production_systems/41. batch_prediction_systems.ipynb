{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 41: Batch Prediction Systems\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the architecture of a batch prediction system and when to use it\n",
    "- Design and implement a robust batch processing pipeline for daily NEPSE predictions\n",
    "- Schedule batch jobs using cron, Apache Airflow, and cloud schedulers\n",
    "- Build scalable data preparation pipelines that handle multiple stocks efficiently\n",
    "- Compute features at scale using pandas, Dask, or Spark\n",
    "- Implement batch inference for hundreds of models (one per stock) efficiently\n",
    "- Store prediction results in databases, data lakes, or files for downstream consumption\n",
    "- Set up notification systems (email, Slack) to alert on job completion or failures\n",
    "- Monitor batch jobs with logging and metrics\n",
    "- Handle errors gracefully with retries and fallback mechanisms\n",
    "- Optimize performance for large\u2011scale batch processing\n",
    "- Integrate batch predictions into trading or reporting workflows\n",
    "\n",
    "---\n",
    "\n",
    "## **41.1 Introduction to Batch Prediction Systems**\n",
    "\n",
    "Batch prediction systems generate forecasts on a regular schedule (e.g., daily, hourly) for a large set of inputs (e.g., all NEPSE stocks). Unlike real\u2011time prediction services (Chapter 40), which respond to individual requests, batch systems process data in bulk, store the results, and make them available for later querying.\n",
    "\n",
    "For the NEPSE prediction system, a batch approach is natural: after the market closes each day, we can fetch the day's data, compute features, run predictions for all stocks, and store the next\u2011day return forecasts in a database. Traders can then query these predictions the next morning.\n",
    "\n",
    "**Advantages of batch prediction:**\n",
    "- Efficient for large numbers of predictions (e.g., hundreds of stocks).\n",
    "- Easier to manage and monitor (scheduled jobs).\n",
    "- Can leverage big data tools (Spark, Dask) for scalability.\n",
    "- Results are persistent and auditable.\n",
    "\n",
    "**Disadvantages:**\n",
    "- Not real\u2011time; predictions are only as fresh as the last batch.\n",
    "- Requires infrastructure to schedule and run jobs reliably.\n",
    "\n",
    "---\n",
    "\n",
    "## **41.2 Batch Processing Architecture**\n",
    "\n",
    "A typical batch prediction pipeline consists of the following stages:\n",
    "\n",
    "1. **Data Ingestion:** Fetch raw data from sources (CSV files, databases, APIs).\n",
    "2. **Data Validation:** Check data quality and completeness.\n",
    "3. **Feature Engineering:** Compute features for each stock using historical data.\n",
    "4. **Model Loading:** Load pre\u2011trained models (one per stock or a single global model).\n",
    "5. **Inference:** Generate predictions for the target period.\n",
    "6. **Result Storage:** Write predictions to a database, data lake, or file.\n",
    "7. **Notification:** Alert on success/failure.\n",
    "8. **Monitoring:** Track job duration, data volumes, and prediction quality.\n",
    "\n",
    "These stages are often implemented as a **DAG** (Directed Acyclic Graph) using workflow orchestrators like Apache Airflow.\n",
    "\n",
    "---\n",
    "\n",
    "## **41.3 Data Preparation Pipelines**\n",
    "\n",
    "Data preparation for batch prediction must handle multiple stocks and ensure that features are computed correctly without look\u2011ahead bias.\n",
    "\n",
    "### **41.3.1 Loading Raw Data**\n",
    "\n",
    "Assume we receive a daily CSV file with all stocks' OHLCV data. We'll load it into a pandas DataFrame.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "def load_raw_data(date):\n",
    "    \"\"\"\n",
    "    Load raw NEPSE data for a given date.\n",
    "    For simplicity, we assume files are named like 'nepse_YYYYMMDD.csv'\n",
    "    \"\"\"\n",
    "    filename = f\"data/raw/nepse_{date.strftime('%Y%m%d')}.csv\"\n",
    "    df = pd.read_csv(filename)\n",
    "    df['Date'] = pd.to_datetime(df['Date'])\n",
    "    return df\n",
    "```\n",
    "\n",
    "### **41.3.2 Feature Computation at Scale**\n",
    "\n",
    "If we have many stocks and need to compute rolling features (e.g., 20\u2011day moving average), we must do this per stock. Using pandas `groupby` is efficient for moderate data (e.g., a few thousand stocks \u00d7 few years of history). For larger datasets, we might use Dask or Spark.\n",
    "\n",
    "```python\n",
    "def compute_features(df):\n",
    "    \"\"\"\n",
    "    Compute features for all stocks.\n",
    "    Assumes df is sorted by Date within each Symbol.\n",
    "    \"\"\"\n",
    "    # Sort by Symbol and Date\n",
    "    df = df.sort_values(['Symbol', 'Date'])\n",
    "    \n",
    "    # Compute returns\n",
    "    df['Return'] = df.groupby('Symbol')['Close'].pct_change() * 100\n",
    "    \n",
    "    # Lag features\n",
    "    for lag in [1, 2, 3, 5]:\n",
    "        df[f'Return_Lag{lag}'] = df.groupby('Symbol')['Return'].shift(lag)\n",
    "    \n",
    "    # Rolling statistics (20-day)\n",
    "    df['MA_20'] = df.groupby('Symbol')['Close'].transform(lambda x: x.rolling(20, min_periods=1).mean())\n",
    "    df['Volatility_20'] = df.groupby('Symbol')['Return'].transform(lambda x: x.rolling(20, min_periods=1).std())\n",
    "    \n",
    "    # RSI (simplified)\n",
    "    def rsi(series, period=14):\n",
    "        delta = series.diff()\n",
    "        gain = delta.where(delta > 0, 0)\n",
    "        loss = -delta.where(delta < 0, 0)\n",
    "        avg_gain = gain.rolling(period).mean()\n",
    "        avg_loss = loss.rolling(period).mean()\n",
    "        rs = avg_gain / avg_loss\n",
    "        rsi = 100 - (100 / (1 + rs))\n",
    "        return rsi\n",
    "    df['RSI'] = df.groupby('Symbol')['Close'].transform(lambda x: rsi(x))\n",
    "    \n",
    "    # Drop rows with NaN (first few rows of each stock)\n",
    "    df = df.dropna()\n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We use `groupby` and `transform` to apply rolling functions per stock. The `min_periods=1` ensures we get a value even at the beginning, but we drop NaN later to avoid using incomplete windows. In a production batch job, we would compute features incrementally (e.g., only for the new day) rather than recomputing everything.\n",
    "\n",
    "### **41.3.3 Incremental Feature Computation**\n",
    "\n",
    "For daily updates, we can store intermediate states (e.g., last 20 days of returns) and update rolling features incrementally. This avoids reprocessing all historical data each day.\n",
    "\n",
    "```python\n",
    "def update_features(existing_features, new_data):\n",
    "    \"\"\"\n",
    "    Update feature set with new day's data.\n",
    "    This is a simplified example; real implementation would use a feature store.\n",
    "    \"\"\"\n",
    "    # Combine existing and new data\n",
    "    combined = pd.concat([existing_features, new_data]).sort_values(['Symbol', 'Date'])\n",
    "    # Recompute rolling features for the affected stocks (could be optimized)\n",
    "    # For simplicity, we recompute all\n",
    "    return compute_features(combined)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **41.4 Scheduling Systems**\n",
    "\n",
    "Batch jobs need to run at specific times. We'll explore several scheduling options.\n",
    "\n",
    "### **41.4.1 Cron (Simple)**\n",
    "\n",
    "On a Unix server, you can use cron to run a Python script daily.\n",
    "\n",
    "```bash\n",
    "# Edit crontab: crontab -e\n",
    "# Run at 6 PM every day\n",
    "0 18 * * * cd /path/to/project && python run_batch_prediction.py >> logs/batch.log 2>&1\n",
    "```\n",
    "\n",
    "**Pros:** Simple, no extra dependencies.  \n",
    "**Cons:** No monitoring, no retries, no dependency management.\n",
    "\n",
    "### **41.4.2 Apache Airflow (Enterprise)**\n",
    "\n",
    "Airflow is a workflow orchestrator that allows you to define DAGs in Python, with built\u2011in monitoring, retries, and alerting.\n",
    "\n",
    "```python\n",
    "# dags/batch_prediction_dag.py\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "from batch_prediction import run_batch_pipeline\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'data_science',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2025, 1, 1),\n",
    "    'email_on_failure': True,\n",
    "    'email_on_retry': False,\n",
    "    'retries': 3,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'nepse_batch_prediction',\n",
    "    default_args=default_args,\n",
    "    description='Daily NEPSE prediction batch job',\n",
    "    schedule_interval='0 18 * * *',  # daily at 6 PM\n",
    "    catchup=False,\n",
    ")\n",
    "\n",
    "run_pipeline = PythonOperator(\n",
    "    task_id='run_prediction_pipeline',\n",
    "    python_callable=run_batch_pipeline,\n",
    "    dag=dag,\n",
    ")\n",
    "```\n",
    "\n",
    "**Pros:** Robust scheduling, retries, monitoring, UI, dependency management.  \n",
    "**Cons:** Requires setup (database, web server).\n",
    "\n",
    "### **41.4.3 Cloud Schedulers**\n",
    "\n",
    "- **AWS CloudWatch Events / EventBridge** can trigger a Lambda function.\n",
    "- **Google Cloud Scheduler** can trigger a Cloud Function or a job on Compute Engine.\n",
    "- **Azure Logic Apps / Scheduler** similar.\n",
    "\n",
    "These are serverless options, good for lightweight jobs.\n",
    "\n",
    "---\n",
    "\n",
    "## **41.5 Feature Computation at Scale**\n",
    "\n",
    "If you have many stocks and long history, pandas may become slow or memory\u2011intensive. Consider:\n",
    "\n",
    "- **Dask:** Parallelizes pandas operations across cores or clusters.\n",
    "- **PySpark:** For very large datasets (billions of rows).\n",
    "- **Feature Store:** A centralized system that stores pre\u2011computed features (see Chapter 63).\n",
    "\n",
    "### **41.5.1 Using Dask for Parallel Feature Engineering**\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Read data with Dask (lazy)\n",
    "ddf = dd.read_csv('data/raw/nepse_*.csv', parse_dates=['Date'])\n",
    "\n",
    "# Groupby and compute rolling features (Dask supports rolling but with limitations)\n",
    "# For complex rolling, you might need to repartition and map partitions.\n",
    "def compute_features_on_partition(partition):\n",
    "    # pandas function applied to each partition\n",
    "    return compute_features(partition)\n",
    "\n",
    "# Apply per partition\n",
    "ddf = ddf.map_partitions(compute_features_on_partition)\n",
    "\n",
    "# Compute result (triggers execution)\n",
    "df_result = ddf.compute()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Dask splits data into partitions and processes them in parallel. For rolling operations that require cross\u2011partition data, you may need to use more advanced techniques (e.g., `rolling` with `groupby` is supported in recent Dask versions).\n",
    "\n",
    "---\n",
    "\n",
    "## **41.6 Batch Inference**\n",
    "\n",
    "After features are ready, we need to generate predictions. If we have a single model that works for all stocks, we simply call `model.predict(X)`. If we have one model per stock, we need to load each model and predict for that stock's data.\n",
    "\n",
    "### **41.6.1 Single Model for All Stocks**\n",
    "\n",
    "```python\n",
    "def predict_all_stocks(features_df, model, feature_cols):\n",
    "    X = features_df[feature_cols]\n",
    "    predictions = model.predict(X)\n",
    "    features_df['Prediction'] = predictions\n",
    "    return features_df[['Symbol', 'Date', 'Prediction']]\n",
    "```\n",
    "\n",
    "### **41.6.2 One Model per Stock**\n",
    "\n",
    "```python\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def predict_per_stock(features_df, model_dir, feature_cols):\n",
    "    results = []\n",
    "    for symbol in features_df['Symbol'].unique():\n",
    "        model_path = os.path.join(model_dir, f\"{symbol}\", \"model.joblib\")\n",
    "        if not os.path.exists(model_path):\n",
    "            logger.warning(f\"Model for {symbol} not found, skipping.\")\n",
    "            continue\n",
    "        model = joblib.load(model_path)\n",
    "        symbol_data = features_df[features_df['Symbol'] == symbol]\n",
    "        X = symbol_data[feature_cols]\n",
    "        preds = model.predict(X)\n",
    "        symbol_data = symbol_data.copy()\n",
    "        symbol_data['Prediction'] = preds\n",
    "        results.append(symbol_data[['Symbol', 'Date', 'Prediction']])\n",
    "    return pd.concat(results)\n",
    "```\n",
    "\n",
    "**Performance consideration:** Loading hundreds of models one by one can be slow. Consider:\n",
    "- Caching models in memory (e.g., using a dictionary) if the batch job runs repeatedly.\n",
    "- Using a model serving layer (e.g., MLflow) to load models on demand.\n",
    "- Parallelizing the prediction loop (see below).\n",
    "\n",
    "### **41.6.3 Parallelizing Predictions**\n",
    "\n",
    "You can use `concurrent.futures` to predict for multiple stocks in parallel.\n",
    "\n",
    "```python\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "def predict_stock(symbol, data, model_dir, feature_cols):\n",
    "    model_path = os.path.join(model_dir, f\"{symbol}\", \"model.joblib\")\n",
    "    if not os.path.exists(model_path):\n",
    "        return None\n",
    "    model = joblib.load(model_path)\n",
    "    X = data[feature_cols]\n",
    "    preds = model.predict(X)\n",
    "    data = data.copy()\n",
    "    data['Prediction'] = preds\n",
    "    return data[['Symbol', 'Date', 'Prediction']]\n",
    "\n",
    "def predict_parallel(features_df, model_dir, feature_cols, max_workers=4):\n",
    "    results = []\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        futures = []\n",
    "        for symbol, group in features_df.groupby('Symbol'):\n",
    "            futures.append(executor.submit(predict_stock, symbol, group, model_dir, feature_cols))\n",
    "        for future in as_completed(futures):\n",
    "            result = future.result()\n",
    "            if result is not None:\n",
    "                results.append(result)\n",
    "    return pd.concat(results)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This uses a thread pool to load models and predict concurrently. Since loading models and predicting are I/O\u2011bound (disk read) and CPU\u2011bound (prediction), threads are appropriate. For CPU\u2011intensive prediction, consider `ProcessPoolExecutor`.\n",
    "\n",
    "---\n",
    "\n",
    "## **41.7 Result Storage**\n",
    "\n",
    "Predictions must be stored for downstream use. Options:\n",
    "\n",
    "- **CSV files:** Simple, but not queryable.\n",
    "- **Database (PostgreSQL, MySQL):** Structured, queryable.\n",
    "- **Data warehouse (Redshift, BigQuery):** For large\u2011scale analytics.\n",
    "- **Feature store:** For low\u2011latency access.\n",
    "\n",
    "### **41.7.1 Storing in PostgreSQL**\n",
    "\n",
    "```python\n",
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "def store_predictions(df):\n",
    "    engine = create_engine('postgresql://user:pass@localhost/nepse')\n",
    "    df.to_sql('predictions', engine, if_exists='append', index=False)\n",
    "```\n",
    "\n",
    "### **41.7.2 Storing in Parquet Files (Data Lake)**\n",
    "\n",
    "```python\n",
    "def store_predictions_parquet(df, date):\n",
    "    filename = f\"data/predictions/{date.strftime('%Y%m%d')}_predictions.parquet\"\n",
    "    df.to_parquet(filename, index=False)\n",
    "```\n",
    "\n",
    "This is suitable for later analysis with Spark or Dask.\n",
    "\n",
    "### **41.7.3 Storing in a Feature Store**\n",
    "\n",
    "For real\u2011time access, you might push predictions to a Redis cache or a feature store (see Chapter 63).\n",
    "\n",
    "---\n",
    "\n",
    "## **41.8 Notification Systems**\n",
    "\n",
    "After the batch job completes (or fails), notify stakeholders.\n",
    "\n",
    "### **41.8.1 Email Notifications**\n",
    "\n",
    "```python\n",
    "import smtplib\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "def send_email(subject, body, to_emails):\n",
    "    msg = MIMEText(body)\n",
    "    msg['Subject'] = subject\n",
    "    msg['From'] = 'batch@nepse-predictor.com'\n",
    "    msg['To'] = ', '.join(to_emails)\n",
    "    with smtplib.SMTP('smtp.gmail.com', 587) as server:\n",
    "        server.starttls()\n",
    "        server.login('user', 'password')\n",
    "        server.send_message(msg)\n",
    "```\n",
    "\n",
    "### **41.8.2 Slack Notifications**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def send_slack_message(message, webhook_url):\n",
    "    payload = {'text': message}\n",
    "    requests.post(webhook_url, json=payload)\n",
    "```\n",
    "\n",
    "### **41.8.3 Integration with Airflow**\n",
    "\n",
    "Airflow automatically sends emails on failure if configured. You can also add Slack operators.\n",
    "\n",
    "---\n",
    "\n",
    "## **41.9 Monitoring and Alerting**\n",
    "\n",
    "Monitor the health of batch jobs:\n",
    "\n",
    "- **Job duration:** Alert if it takes too long.\n",
    "- **Data volume:** Alert if number of stocks or rows is abnormal.\n",
    "- **Model performance:** Compare predictions to actuals when they arrive (next day) and alert if error exceeds threshold.\n",
    "- **Infrastructure metrics:** CPU, memory, disk usage.\n",
    "\n",
    "Use logging and metrics (e.g., Prometheus, CloudWatch) to track these.\n",
    "\n",
    "### **41.9.1 Logging**\n",
    "\n",
    "```python\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "def run_pipeline():\n",
    "    logger.info(\"Starting batch prediction pipeline\")\n",
    "    try:\n",
    "        # ... steps\n",
    "        logger.info(\"Pipeline completed successfully\")\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Pipeline failed: {e}\", exc_info=True)\n",
    "        raise\n",
    "```\n",
    "\n",
    "### **41.9.2 Metrics**\n",
    "\n",
    "You can log custom metrics to a time\u2011series database.\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Gauge, push_to_gateway\n",
    "\n",
    "job_duration = Gauge('batch_job_duration_seconds', 'Duration of batch job')\n",
    "job_success = Counter('batch_job_success_total', 'Number of successful runs')\n",
    "\n",
    "start = time.time()\n",
    "# ... run job\n",
    "duration = time.time() - start\n",
    "job_duration.set(duration)\n",
    "job_success.inc()\n",
    "push_to_gateway('localhost:9091', job='batch_prediction', registry=...)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **41.10 Error Handling**\n",
    "\n",
    "Batch jobs should handle errors gracefully.\n",
    "\n",
    "### **41.10.1 Retries**\n",
    "\n",
    "If a step fails (e.g., data download), retry a few times with exponential backoff.\n",
    "\n",
    "```python\n",
    "import time\n",
    "from tenacity import retry, stop_after_attempt, wait_exponential\n",
    "\n",
    "@retry(stop=stop_after_attempt(3), wait=wait_exponential(multiplier=1, min=4, max=10))\n",
    "def download_data(date):\n",
    "    # may raise exception\n",
    "    pass\n",
    "```\n",
    "\n",
    "### **41.10.2 Fallback Models**\n",
    "\n",
    "If the main model for a stock is missing or fails, fall back to a simpler model (e.g., historical mean).\n",
    "\n",
    "```python\n",
    "def predict_with_fallback(symbol, data):\n",
    "    try:\n",
    "        model = joblib.load(f\"models/{symbol}/model.joblib\")\n",
    "        return model.predict(data)\n",
    "    except FileNotFoundError:\n",
    "        # fallback: predict mean of last 20 returns\n",
    "        return data['Return'].tail(20).mean()\n",
    "```\n",
    "\n",
    "### **41.10.3 Partial Success**\n",
    "\n",
    "If some stocks fail, still record successes and log failures. Continue the pipeline.\n",
    "\n",
    "```python\n",
    "results = []\n",
    "failed_stocks = []\n",
    "for symbol in symbols:\n",
    "    try:\n",
    "        pred = predict_stock(symbol, data)\n",
    "        results.append(pred)\n",
    "    except Exception as e:\n",
    "        failed_stocks.append(symbol)\n",
    "        logger.error(f\"Failed for {symbol}: {e}\")\n",
    "\n",
    "if failed_stocks:\n",
    "    send_alert(f\"Failed for stocks: {failed_stocks}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **41.11 Performance Optimization**\n",
    "\n",
    "- **Use vectorized operations** (pandas) instead of loops.\n",
    "- **Parallelize** where possible (per stock or per partition).\n",
    "- **Cache intermediate results** (e.g., pre\u2011computed features) to avoid recomputation.\n",
    "- **Use efficient file formats** like Parquet (columnar, compressed) instead of CSV.\n",
    "- **Consider incremental processing** for daily updates to avoid full history recomputation.\n",
    "\n",
    "### **41.11.1 Incremental Processing Example**\n",
    "\n",
    "```python\n",
    "def incremental_update(last_date, new_date):\n",
    "    # Load only new data since last_date\n",
    "    new_data = load_data_since(last_date)\n",
    "    # Load last known features (e.g., from a feature store)\n",
    "    last_features = load_last_features()\n",
    "    # Update features incrementally\n",
    "    updated_features = update_features(last_features, new_data)\n",
    "    # Predict only on new_data's target dates\n",
    "    predictions = predict(updated_features[updated_features['Date'] == new_date])\n",
    "    return predictions\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **41.12 Integration with Downstream Systems**\n",
    "\n",
    "Predictions are only useful if they reach traders or automated systems.\n",
    "\n",
    "- **Database:** Traders query via dashboards.\n",
    "- **Message queue:** Push predictions to Kafka for real\u2011time consumption.\n",
    "- **File share:** Deliver CSV to a shared folder.\n",
    "\n",
    "Choose based on your architecture.\n",
    "\n",
    "---\n",
    "\n",
    "## **41.13 Chapter Summary**\n",
    "\n",
    "In this chapter, we designed a robust batch prediction system for the NEPSE dataset.\n",
    "\n",
    "- **Architecture:** Data ingestion \u2192 feature engineering \u2192 inference \u2192 storage \u2192 notification.\n",
    "- **Scheduling:** Cron for simple jobs, Airflow for enterprise workflows.\n",
    "- **Feature computation:** Handled per stock using pandas; scaled with Dask if needed.\n",
    "- **Inference:** Single or per\u2011stock models; parallelized for efficiency.\n",
    "- **Result storage:** Database, Parquet, or feature store.\n",
    "- **Notifications:** Email, Slack on job completion/failure.\n",
    "- **Monitoring:** Logging, metrics, alerts.\n",
    "- **Error handling:** Retries, fallbacks, partial success.\n",
    "- **Performance:** Incremental updates, parallelization, efficient formats.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Use Airflow to schedule daily predictions at market close.\n",
    "- Compute features incrementally to save time.\n",
    "- Store predictions in a PostgreSQL database for easy querying.\n",
    "- Set up Slack alerts for job failures.\n",
    "- Monitor prediction accuracy by comparing with actual returns the next day.\n",
    "\n",
    "In the next chapter, **Chapter 42: Real\u2011Time Prediction Systems**, we will explore how to build low\u2011latency streaming prediction services using technologies like Kafka and Flink.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 41**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='40. building_prediction_services.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='42. real_time_prediction_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}