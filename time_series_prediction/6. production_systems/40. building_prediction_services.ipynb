{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 40: Building Prediction Services\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Design a RESTful API to serve predictions from your trained model\n",
    "- Choose between synchronous and asynchronous prediction services based on latency requirements\n",
    "- Implement a prediction service using FastAPI and Flask\n",
    "- Handle request validation, error responses, and logging\n",
    "- Add authentication and rate limiting to secure your API\n",
    "- Document your API using OpenAPI/Swagger\n",
    "- Package the service as a standalone application for deployment\n",
    "- Test the API locally and with automated tests\n",
    "- Understand the differences between online (real\u2011time) and batch prediction services\n",
    "- Deploy the service using containers and orchestration tools\n",
    "\n",
    "---\n",
    "\n",
    "## **40.1 Introduction to Prediction Services**\n",
    "\n",
    "A model that sits on disk is not useful until it can make predictions on new data. In a production environment, we typically expose the model through a **prediction service** \u2013 an API that accepts input data and returns predictions. This decouples the model from the applications that consume its predictions (e.g., trading dashboards, mobile apps, other microservices).\n",
    "\n",
    "For the NEPSE prediction system, we might build a service that:\n",
    "\n",
    "- Accepts a stock symbol and returns the predicted next\u2011day return.\n",
    "- Accepts a batch of recent market data for multiple stocks and returns predictions.\n",
    "- Runs as a scheduled batch job that pushes predictions to a database.\n",
    "\n",
    "In this chapter, we focus on building a **real\u2011time prediction API** using FastAPI, a modern Python web framework. We'll also discuss batch prediction services briefly.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.2 Service Architecture Patterns**\n",
    "\n",
    "Before coding, we need to decide on the architecture. Common patterns include:\n",
    "\n",
    "### **40.2.1 Online (Real\u2011Time) Prediction**\n",
    "\n",
    "- Client sends a request with input data.\n",
    "- Service loads the model, preprocesses the input, runs inference, and returns the prediction.\n",
    "- Latency is critical (milliseconds to seconds).\n",
    "- Suitable for interactive applications or low\u2011latency trading signals.\n",
    "\n",
    "### **40.2.2 Batch Prediction**\n",
    "\n",
    "- Predictions are generated periodically (e.g., once per day) for all stocks.\n",
    "- Results are stored in a database or file.\n",
    "- Clients query the pre\u2011computed results.\n",
    "- Simpler to implement, no real\u2011time infrastructure required.\n",
    "\n",
    "### **40.2.3 Streaming Prediction**\n",
    "\n",
    "- Input data arrives as a stream (e.g., tick data).\n",
    "- Predictions are generated on the fly and emitted to a stream.\n",
    "- Requires stream processing frameworks (Kafka, Flink). We'll touch on this in Chapter 42.\n",
    "\n",
    "For the NEPSE system, a daily batch prediction might suffice, but we'll build an API for on\u2011demand predictions as a learning exercise.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.3 REST API Design**\n",
    "\n",
    "A well\u2011designed REST API follows conventions:\n",
    "\n",
    "- Use nouns for resources (e.g., `/predict` is an action, better to use `/predictions`).\n",
    "- Use HTTP methods appropriately: `POST` for creating a prediction (since we are sending data).\n",
    "- Accept JSON in the request body, return JSON.\n",
    "- Use meaningful status codes: 200 for success, 400 for bad input, 404 for not found, 500 for server error.\n",
    "- Version your API (e.g., `/api/v1/predictions`).\n",
    "\n",
    "For our prediction service, we'll design two endpoints:\n",
    "\n",
    "- `POST /api/v1/predict` \u2013 single prediction for one stock.\n",
    "- `POST /api/v1/predict/batch` \u2013 batch predictions for multiple stocks.\n",
    "\n",
    "Request body example for single prediction:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"symbol\": \"NEPSE\",\n",
    "  \"date\": \"2025-03-16\",\n",
    "  \"features\": {\n",
    "    \"Open\": 1200.5,\n",
    "    \"High\": 1210.0,\n",
    "    \"Low\": 1195.0,\n",
    "    \"Close\": 1205.0,\n",
    "    \"Vol\": 1500000,\n",
    "    \"VWAP\": 1202.3,\n",
    "    \"Prev_Close\": 1198.0\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "Response:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"symbol\": \"NEPSE\",\n",
    "  \"prediction_date\": \"2025-03-16\",\n",
    "  \"predicted_return\": 0.35,\n",
    "  \"model_version\": \"v1.2.0\"\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **40.4 FastAPI Implementation**\n",
    "\n",
    "FastAPI is a modern, high\u2011performance web framework for building APIs with Python. It automatically generates OpenAPI documentation and supports asynchronous programming.\n",
    "\n",
    "### **40.4.1 Setting Up FastAPI**\n",
    "\n",
    "First, install FastAPI and an ASGI server like Uvicorn:\n",
    "\n",
    "```bash\n",
    "pip install fastapi uvicorn\n",
    "```\n",
    "\n",
    "Create a file `app.py`:\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel, Field\n",
    "import joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import List, Optional\n",
    "\n",
    "app = FastAPI(title=\"NEPSE Prediction API\", version=\"1.0.0\")\n",
    "\n",
    "# Load model and preprocessor at startup\n",
    "model = joblib.load(\"models/pipeline.joblib\")\n",
    "feature_names = joblib.load(\"models/feature_names.joblib\")  # list of expected features\n",
    "\n",
    "class PredictionRequest(BaseModel):\n",
    "    symbol: str\n",
    "    date: str\n",
    "    features: dict\n",
    "\n",
    "class BatchPredictionRequest(BaseModel):\n",
    "    requests: List[PredictionRequest]\n",
    "\n",
    "class PredictionResponse(BaseModel):\n",
    "    symbol: str\n",
    "    prediction_date: str\n",
    "    predicted_return: float\n",
    "    model_version: str = \"v1.2.0\"\n",
    "\n",
    "@app.get(\"/\")\n",
    "def root():\n",
    "    return {\"message\": \"NEPSE Prediction API\"}\n",
    "\n",
    "@app.post(\"/api/v1/predict\", response_model=PredictionResponse)\n",
    "async def predict(request: PredictionRequest):\n",
    "    \"\"\"\n",
    "    Predict next day's return for a single stock.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Convert features dict to array in correct order\n",
    "        features_array = np.array([[request.features.get(f, 0) for f in feature_names]])\n",
    "        # Predict\n",
    "        pred = model.predict(features_array)[0]\n",
    "        return PredictionResponse(\n",
    "            symbol=request.symbol,\n",
    "            prediction_date=request.date,\n",
    "            predicted_return=float(pred)\n",
    "        )\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "@app.post(\"/api/v1/predict/batch\", response_model=List[PredictionResponse])\n",
    "async def predict_batch(request: BatchPredictionRequest):\n",
    "    \"\"\"\n",
    "    Predict next day's return for multiple stocks.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build feature matrix\n",
    "        X = []\n",
    "        for req in request.requests:\n",
    "            X.append([req.features.get(f, 0) for f in feature_names])\n",
    "        X = np.array(X)\n",
    "        preds = model.predict(X)\n",
    "        responses = []\n",
    "        for req, pred in zip(request.requests, preds):\n",
    "            responses.append(PredictionResponse(\n",
    "                symbol=req.symbol,\n",
    "                prediction_date=req.date,\n",
    "                predicted_return=float(pred)\n",
    "            ))\n",
    "        return responses\n",
    "    except Exception as e:\n",
    "        raise HTTPException(status_code=400, detail=str(e))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    import uvicorn\n",
    "    uvicorn.run(app, host=\"0.0.0.0\", port=8000)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Pydantic models** (`PredictionRequest`, etc.) define the expected request/response structure and provide automatic validation.\n",
    "- The model is loaded once at startup (outside the endpoint functions) to avoid reloading on every request.\n",
    "- The endpoint is `async def` \u2013 FastAPI can handle async, but our prediction is CPU\u2011bound, so it will run in a thread pool. This is fine.\n",
    "- We extract features in the order expected by the model (`feature_names`). If a feature is missing, we default to 0.\n",
    "- If an error occurs, we raise `HTTPException` with a 400 status code.\n",
    "- The API documentation is automatically available at `/docs`.\n",
    "\n",
    "### **40.4.2 Running the Service**\n",
    "\n",
    "```bash\n",
    "uvicorn app:app --reload --host 0.0.0.0 --port 8000\n",
    "```\n",
    "\n",
    "Then visit `http://localhost:8000/docs` to see the interactive Swagger UI.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.5 Flask Implementation (Alternative)**\n",
    "\n",
    "Flask is another popular choice, though it is synchronous by default. Here's a minimal Flask version:\n",
    "\n",
    "```python\n",
    "from flask import Flask, request, jsonify\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = Flask(__name__)\n",
    "model = joblib.load(\"models/pipeline.joblib\")\n",
    "feature_names = joblib.load(\"models/feature_names.joblib\")\n",
    "\n",
    "@app.route('/api/v1/predict', methods=['POST'])\n",
    "def predict():\n",
    "    data = request.get_json()\n",
    "    try:\n",
    "        features = data['features']\n",
    "        X = np.array([[features.get(f, 0) for f in feature_names]])\n",
    "        pred = model.predict(X)[0]\n",
    "        return jsonify({\n",
    "            'symbol': data['symbol'],\n",
    "            'prediction_date': data['date'],\n",
    "            'predicted_return': float(pred),\n",
    "            'model_version': 'v1.2.0'\n",
    "        })\n",
    "    except Exception as e:\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(host='0.0.0.0', port=8000)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Flask is simpler but lacks automatic validation and async support. You would need to add request validation manually (e.g., using `marshmallow`). FastAPI is generally preferred for new projects.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.6 Asynchronous Processing**\n",
    "\n",
    "If predictions take a long time (e.g., large deep learning models), you may want to process them asynchronously to avoid blocking the request thread. This involves:\n",
    "\n",
    "- Accepting the request and immediately returning a `202 Accepted` with a task ID.\n",
    "- Processing the prediction in the background.\n",
    "- Allowing the client to poll a status endpoint for the result.\n",
    "\n",
    "FastAPI can integrate with background tasks:\n",
    "\n",
    "```python\n",
    "from fastapi import BackgroundTasks\n",
    "import uuid\n",
    "\n",
    "tasks = {}\n",
    "\n",
    "def run_prediction(task_id, features):\n",
    "    # long-running prediction\n",
    "    result = model.predict(features)\n",
    "    tasks[task_id] = result\n",
    "\n",
    "@app.post(\"/api/v1/predict/async\")\n",
    "async def predict_async(request: PredictionRequest, background_tasks: BackgroundTasks):\n",
    "    task_id = str(uuid.uuid4())\n",
    "    features = ...  # prepare features\n",
    "    background_tasks.add_task(run_prediction, task_id, features)\n",
    "    return {\"task_id\": task_id, \"status\": \"processing\"}\n",
    "\n",
    "@app.get(\"/api/v1/predict/async/{task_id}\")\n",
    "async def get_result(task_id: str):\n",
    "    if task_id not in tasks:\n",
    "        raise HTTPException(status_code=404)\n",
    "    result = tasks[task_id]\n",
    "    return {\"task_id\": task_id, \"result\": result}\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This pattern is useful for long\u2011running inferences. The client can poll the status endpoint until the result is ready.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.7 Batch Prediction Services**\n",
    "\n",
    "For daily batch predictions, we might not need a REST API. Instead, we can write a script that:\n",
    "\n",
    "1. Fetches the latest data for all stocks.\n",
    "2. Computes features.\n",
    "3. Loads the model (or multiple models).\n",
    "4. Generates predictions.\n",
    "5. Saves predictions to a database or CSV.\n",
    "\n",
    "This script can be scheduled with cron, Airflow, or a cloud scheduler.\n",
    "\n",
    "```python\n",
    "# batch_predict.py\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from datetime import datetime\n",
    "\n",
    "def run_batch_predictions():\n",
    "    # Load data\n",
    "    df = pd.read_csv(f\"data/raw/nepse_{datetime.today().strftime('%Y%m%d')}.csv\")\n",
    "    \n",
    "    # Feature engineering (reuse feature engineering module)\n",
    "    from src.data.features import FeatureEngineer\n",
    "    engineer = FeatureEngineer(config)\n",
    "    df_features = engineer.create_features(df)\n",
    "    \n",
    "    # For each symbol, load its model and predict\n",
    "    predictions = []\n",
    "    for symbol in df_features['Symbol'].unique():\n",
    "        model = joblib.load(f\"models/{symbol}/model.joblib\")\n",
    "        symbol_data = df_features[df_features['Symbol'] == symbol]\n",
    "        X = symbol_data[feature_cols]\n",
    "        preds = model.predict(X)\n",
    "        # Store prediction (e.g., for the last row, or for each row)\n",
    "        predictions.append({'symbol': symbol, 'date': df['Date'].iloc[-1], 'prediction': preds[-1]})\n",
    "    \n",
    "    # Save to CSV or database\n",
    "    pred_df = pd.DataFrame(predictions)\n",
    "    pred_df.to_csv(f\"data/predictions/{datetime.today().strftime('%Y%m%d')}_predictions.csv\", index=False)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    run_batch_predictions()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **40.8 Authentication and Authorization**\n",
    "\n",
    "If your API is exposed to the internet, you need to secure it. Common methods:\n",
    "\n",
    "- **API Key:** Client includes a key in the header.\n",
    "- **OAuth2:** More complex, suitable for user authentication.\n",
    "- **JWT:** Token\u2011based authentication.\n",
    "\n",
    "FastAPI provides built\u2011in support for OAuth2 and API keys.\n",
    "\n",
    "### **40.8.1 API Key Example**\n",
    "\n",
    "```python\n",
    "from fastapi import Security, HTTPException\n",
    "from fastapi.security import APIKeyHeader\n",
    "\n",
    "API_KEY = \"secret-key-123\"\n",
    "api_key_header = APIKeyHeader(name=\"X-API-Key\")\n",
    "\n",
    "def verify_api_key(api_key: str = Security(api_key_header)):\n",
    "    if api_key != API_KEY:\n",
    "        raise HTTPException(status_code=403, detail=\"Invalid API Key\")\n",
    "    return api_key\n",
    "\n",
    "@app.post(\"/api/v1/predict\", dependencies=[Security(verify_api_key)])\n",
    "async def predict(request: PredictionRequest):\n",
    "    # ... endpoint code\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The client must include the header `X-API-Key: secret-key-123`. The key should be stored securely (environment variable) and not hard\u2011coded.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.9 Rate Limiting**\n",
    "\n",
    "To prevent abuse, you may want to limit the number of requests per client. FastAPI does not include rate limiting out of the box, but you can use middleware or a library like `slowapi`.\n",
    "\n",
    "```python\n",
    "from slowapi import Limiter, _rate_limit_exceeded_handler\n",
    "from slowapi.util import get_remote_address\n",
    "\n",
    "limiter = Limiter(key_func=get_remote_address)\n",
    "app.state.limiter = limiter\n",
    "app.add_exception_handler(429, _rate_limit_exceeded_handler)\n",
    "\n",
    "@app.post(\"/api/v1/predict\")\n",
    "@limiter.limit(\"5/minute\")\n",
    "async def predict(request: PredictionRequest):\n",
    "    # ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **40.10 API Documentation**\n",
    "\n",
    "FastAPI automatically generates OpenAPI documentation. You can add descriptions to endpoints and models using docstrings and Pydantic field descriptions.\n",
    "\n",
    "```python\n",
    "class PredictionRequest(BaseModel):\n",
    "    symbol: str = Field(..., description=\"NEPSE stock symbol\")\n",
    "    date: str = Field(..., description=\"Date of the input data (YYYY-MM-DD)\")\n",
    "    features: dict = Field(..., description=\"Dictionary of feature values\")\n",
    "```\n",
    "\n",
    "The interactive docs at `/docs` allow users to try out the API.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.11 Service Testing**\n",
    "\n",
    "Test your API endpoints using `pytest` and `httpx` (for FastAPI).\n",
    "\n",
    "```python\n",
    "# test_api.py\n",
    "from fastapi.testclient import TestClient\n",
    "from app import app\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_predict():\n",
    "    response = client.post(\"/api/v1/predict\", json={\n",
    "        \"symbol\": \"NEPSE\",\n",
    "        \"date\": \"2025-03-16\",\n",
    "        \"features\": {\"Open\": 1200, \"High\": 1210, \"Low\": 1195, \"Close\": 1205, \"Vol\": 1500000}\n",
    "    })\n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert \"predicted_return\" in data\n",
    "```\n",
    "\n",
    "Run tests with `pytest`.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.12 Deployment**\n",
    "\n",
    "Once the API is ready, you can deploy it using:\n",
    "\n",
    "- **Container** (Docker) \u2013 as shown in Chapter 38.\n",
    "- **Serverless** \u2013 using AWS Lambda with API Gateway (may require adapting for async).\n",
    "- **Cloud Run** / **Azure Container Instances** for simple container hosting.\n",
    "\n",
    "Example Dockerfile:\n",
    "\n",
    "```dockerfile\n",
    "FROM python:3.9-slim\n",
    "\n",
    "WORKDIR /app\n",
    "COPY requirements.txt .\n",
    "RUN pip install --no-cache-dir -r requirements.txt\n",
    "COPY app.py .\n",
    "COPY models/ ./models/\n",
    "\n",
    "CMD [\"uvicorn\", \"app:app\", \"--host\", \"0.0.0.0\", \"--port\", \"8000\"]\n",
    "```\n",
    "\n",
    "Then deploy to a cloud service.\n",
    "\n",
    "---\n",
    "\n",
    "## **40.13 Chapter Summary**\n",
    "\n",
    "In this chapter, we built a prediction service for the NEPSE system using FastAPI.\n",
    "\n",
    "- **REST API design:** We defined endpoints for single and batch predictions.\n",
    "- **FastAPI implementation:** We created a service that loads a model, validates input, and returns predictions.\n",
    "- **Flask alternative:** A simpler, synchronous version.\n",
    "- **Asynchronous processing:** For long\u2011running predictions, we used background tasks.\n",
    "- **Batch prediction:** A script for daily scheduled predictions.\n",
    "- **Security:** API key authentication and rate limiting.\n",
    "- **Documentation:** Auto\u2011generated Swagger UI.\n",
    "- **Testing:** Unit tests for endpoints.\n",
    "- **Deployment:** Docker and cloud options.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- For real\u2011time needs, use FastAPI to serve predictions with low latency.\n",
    "- Store models and preprocessing artifacts in a known location, loaded at startup.\n",
    "- Validate input using Pydantic models.\n",
    "- Secure the API with an API key if exposed externally.\n",
    "- For batch predictions, schedule a script using cron/Airflow.\n",
    "- Containerize the service for reproducible deployment.\n",
    "\n",
    "In the next chapter, **Chapter 41: Batch Prediction Systems**, we will explore batch processing in more detail, including scheduling, data preparation at scale, and integration with data warehouses.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 40**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='39. model_serialization_and_storage.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='41. batch_prediction_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}