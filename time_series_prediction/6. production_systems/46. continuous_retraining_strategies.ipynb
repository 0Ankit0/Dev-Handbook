{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 46: Continuous Retraining Strategies\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Determine when and why to retrain a time\u2011series prediction model in production\n",
    "- Distinguish between time\u2011based, performance\u2011based, and drift\u2011based retraining triggers\n",
    "- Implement batch retraining pipelines that periodically update models using historical data\n",
    "- Apply incremental learning techniques to update models without full retraining\n",
    "- Explore online learning algorithms that adapt continuously to new data streams\n",
    "- Use active learning to selectively label and incorporate the most informative new examples\n",
    "- Design continuous training pipelines with orchestration tools like Apache Airflow\n",
    "- Compare model versions using A/B testing and shadow deployments\n",
    "- Build automated retraining systems that monitor drift and trigger updates without manual intervention\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A model deployed to production is never truly finished. Markets evolve, user behaviour changes, and the underlying data generating process shifts. For a time\u2011series prediction system like our NEPSE stock predictor, the statistical properties of financial data are constantly in flux. A model that performed well last month may underperform today because of new market regimes, regulatory changes, or unexpected events.\n",
    "\n",
    "**Continuous retraining** is the practice of updating your model on a regular basis\u2014or in response to specific triggers\u2014to maintain its accuracy and relevance. In this chapter, we will explore the spectrum of retraining strategies, from simple scheduled retraining to sophisticated online learning algorithms. We will also discuss how to build automated pipelines that monitor model performance, detect drift, and retrain without human intervention, all while ensuring that new model versions are thoroughly validated before they serve traffic.\n",
    "\n",
    "Using the NEPSE prediction system as a running example, we will implement practical retraining workflows and discuss the trade\u2011offs between freshness, computational cost, and stability.\n",
    "\n",
    "---\n",
    "\n",
    "## 46.1 When to Retrain\n",
    "\n",
    "Deciding when to retrain a model is a fundamental design question. There are three primary drivers:\n",
    "\n",
    "1. **Time\u2011based retraining** \u2013 Retrain on a fixed schedule (e.g., every week, every month). This is simple to implement and ensures the model is periodically refreshed with the most recent data.\n",
    "2. **Performance\u2011based retraining** \u2013 Retrain when the model\u2019s prediction error exceeds a threshold on recent data (once ground truth is available). This directly targets degradation but may react late.\n",
    "3. **Drift\u2011based retraining** \u2013 Retrain when statistical tests detect significant data drift or concept drift. This can be a leading indicator of future performance drop.\n",
    "\n",
    "For the NEPSE system, a combination is often best. You might retrain every week (time\u2011based) but also trigger an extra retraining cycle if a critical feature like `Volume` drifts beyond a certain PSI threshold (drift\u2011based), or if the daily error rate spikes (performance\u2011based).\n",
    "\n",
    "---\n",
    "\n",
    "## 46.2 Batch Retraining\n",
    "\n",
    "Batch retraining is the most common approach: periodically (e.g., nightly) you collect all available data up to the present, preprocess it, engineer features, and train a new model from scratch. The new model is then validated and, if it passes acceptance tests, deployed to replace the old one.\n",
    "\n",
    "### 46.2.1 A Simple Batch Retraining Pipeline\n",
    "\n",
    "Consider a daily retraining script for our NEPSE model:\n",
    "\n",
    "```python\n",
    "# retrain_daily.py\n",
    "import pandas as pd\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import logging\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "\n",
    "def load_and_prepare_data():\n",
    "    \"\"\"Load all available NEPSE data up to yesterday.\"\"\"\n",
    "    df = pd.read_csv('nepse_all.csv', parse_dates=['Date'])\n",
    "    # Assume we have features already engineered in the CSV\n",
    "    # Or we could run feature engineering here\n",
    "    return df\n",
    "\n",
    "def train_model(df):\n",
    "    \"\"\"Train a new model on all data.\"\"\"\n",
    "    # Use data up to yesterday for training\n",
    "    train_data = df[df['Date'] < pd.Timestamp.now().normalize()]\n",
    "    X = train_data.drop(columns=['Date', 'Close', 'Symbol'])  # features\n",
    "    y = train_data['Close'] > train_data['Close'].shift(-1)   # binary target: next day up?\n",
    "\n",
    "    model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "    model.fit(X, y)\n",
    "    return model, X, y\n",
    "\n",
    "def validate_model(model, X, y):\n",
    "    \"\"\"Quick validation on last month's data.\"\"\"\n",
    "    split_date = pd.Timestamp.now().normalize() - pd.DateOffset(months=1)\n",
    "    X_val = X[X.index >= split_date]\n",
    "    y_val = y[y.index >= split_date]\n",
    "    preds = model.predict(X_val)\n",
    "    acc = accuracy_score(y_val, preds)\n",
    "    logging.info(f\"Validation accuracy on last month: {acc:.4f}\")\n",
    "    return acc\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    df = load_and_prepare_data()\n",
    "    model, X, y = train_model(df)\n",
    "    acc = validate_model(model, X, y)\n",
    "    if acc > 0.55:  # threshold\n",
    "        joblib.dump(model, 'models/nepse_model_latest.pkl')\n",
    "        logging.info(\"New model saved.\")\n",
    "    else:\n",
    "        logging.warning(\"New model did not meet accuracy threshold; not deployed.\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- The script loads all historical data, trains a random forest on everything up to yesterday, and evaluates on the last month.  \n",
    "- If the validation accuracy exceeds a simple threshold (55%, since random guessing would be 50%), it saves the model.  \n",
    "- This script could be run daily via a cron job or an orchestrator like Airflow.\n",
    "\n",
    "### 46.2.2 Challenges with Batch Retraining\n",
    "\n",
    "- **Computational cost**: Retraining from scratch every day on a growing dataset becomes expensive. Incremental approaches may be needed.\n",
    "- **Data consistency**: When using all historical data, the model may be slow to adapt to recent changes because old data dominates.\n",
    "- **Validation**: You must ensure the new model does not regress on important segments (e.g., high\u2011volatility periods).\n",
    "\n",
    "To address the dominance of old data, many practitioners use a **sliding window**: train only on the last N months (e.g., 12 months). This makes the model more responsive to recent patterns and reduces training time.\n",
    "\n",
    "```python\n",
    "window_size = 365  # last year of trading days\n",
    "train_data = df[df['Date'] >= (pd.Timestamp.now() - pd.DateOffset(days=window_size))]\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 46.3 Incremental Learning\n",
    "\n",
    "Incremental learning (also called online learning or streaming learning) updates the model with each new batch of data without retraining from scratch. This is much more efficient and can adapt faster. Many algorithms support incremental updates: linear models (SGD), Naive Bayes, and some tree ensembles (e.g., `river` library's Hoeffding Tree).\n",
    "\n",
    "### 46.3.1 Incremental Learning with River\n",
    "\n",
    "`river` is a Python library for online machine learning. It provides estimators with a `learn_one` method that updates the model with a single example.\n",
    "\n",
    "**Example: Incremental logistic regression for NEPSE direction prediction.**\n",
    "\n",
    "```python\n",
    "from river import linear_model, preprocessing, metrics\n",
    "import pandas as pd\n",
    "\n",
    "# Assume we have a stream of feature vectors (X_dict) and labels (y)\n",
    "model = preprocessing.StandardScaler() | linear_model.LogisticRegression()\n",
    "\n",
    "metric = metrics.Accuracy()\n",
    "\n",
    "# Simulate streaming data\n",
    "for index, row in df.iterrows():\n",
    "    # Convert row to dict of features (excluding target)\n",
    "    x = row.drop('target').to_dict()\n",
    "    y = row['target']\n",
    "    \n",
    "    # Make prediction (optional, for monitoring)\n",
    "    y_pred = model.predict_one(x)\n",
    "    if y_pred is not None:\n",
    "        metric.update(y, y_pred)\n",
    "    \n",
    "    # Update model with this example\n",
    "    model.learn_one(x, y)\n",
    "    \n",
    "    # Every 1000 examples, log current accuracy\n",
    "    if index % 1000 == 0:\n",
    "        print(f\"Iteration {index}, accuracy: {metric.get():.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- The model is wrapped in a pipeline that first scales features (online) and then trains a logistic regression.  \n",
    "- For each new trading day, we call `learn_one` to update the model. We can also monitor its accuracy on the fly.  \n",
    "- This approach uses very little memory and can run indefinitely.\n",
    "\n",
    "### 46.3.2 When to Use Incremental Learning\n",
    "\n",
    "Incremental learning is ideal when:\n",
    "- Data arrives continuously and you need near\u2011real\u2011time adaptation.\n",
    "- You cannot afford to retrain from scratch frequently.\n",
    "- The underlying process changes gradually.\n",
    "\n",
    "For NEPSE, incremental learning could be used to update the model after each trading day. However, care must be taken because financial data often exhibits non\u2011stationarity and sudden shifts; incremental models may overreact to noise if the learning rate is too high.\n",
    "\n",
    "---\n",
    "\n",
    "## 46.4 Online Learning\n",
    "\n",
    "Online learning is a subset of incremental learning where the model is updated after each individual instance. It is particularly useful for real\u2011time systems where predictions must be made immediately and the model can adapt on the fly.\n",
    "\n",
    "### 46.4.1 Online Learning with Vowpal Wabbit\n",
    "\n",
    "Vowpal Wabbit (VW) is a fast online learning system that supports various algorithms and is widely used in production.\n",
    "\n",
    "**Example training a logistic regression model with VW via command line:**\n",
    "\n",
    "```bash\n",
    "# Convert data to VW format\n",
    "# Each line: label |f feature1:value feature2:value ...\n",
    "vw --data train.vw --loss_function logistic --passes 1 --learning_rate 0.1 -f model.vw\n",
    "```\n",
    "\n",
    "For Python integration, you can use `vw` Python wrapper or `vowpalwabbit` package.\n",
    "\n",
    "### 46.4.2 Challenges of Online Learning\n",
    "\n",
    "- **Choice of learning rate**: Too high and the model oscillates; too low and it adapts slowly.\n",
    "- **Catastrophic forgetting**: The model may forget old patterns if it only sees recent data.\n",
    "- **Evaluation**: Standard cross\u2011validation doesn't apply; you must use progressive validation.\n",
    "\n",
    "---\n",
    "\n",
    "## 46.5 Active Learning\n",
    "\n",
    "Active learning is a strategy where the model selectively requests labels for the most informative unlabeled examples. In a prediction system, you might not receive immediate feedback (the true next\u2011day price is only known the next day). Active learning can help you choose which examples to store for later retraining, reducing the amount of labeled data needed.\n",
    "\n",
    "For NEPSE, you could decide to only retrain on days where the model was highly uncertain (e.g., prediction probability near 0.5) or where the market exhibited unusual behaviour.\n",
    "\n",
    "**Example: Uncertainty sampling**\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "import numpy as np\n",
    "\n",
    "model = RandomForestClassifier()\n",
    "# Initially train on some data\n",
    "model.fit(X_initial, y_initial)\n",
    "\n",
    "# Stream of unlabeled examples\n",
    "for x_unlabeled in stream:\n",
    "    proba = model.predict_proba([x_unlabeled])[0]\n",
    "    uncertainty = 1 - np.max(proba)   # lower max probability = higher uncertainty\n",
    "    if uncertainty > 0.4:  # threshold\n",
    "        # Ask for label (in practice, wait for next day's price)\n",
    "        # Store this example for the next retraining batch\n",
    "        store_for_retraining(x_unlabeled)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 46.6 Continuous Training Pipelines\n",
    "\n",
    "In a mature MLOps setup, retraining is not a one\u2011off script but a continuous pipeline integrated with your CI/CD and monitoring systems. Tools like Apache Airflow, Prefect, or Kubeflow Pipelines allow you to orchestrate complex workflows that include:\n",
    "\n",
    "- Data extraction and validation\n",
    "- Feature engineering\n",
    "- Model training and hyperparameter tuning\n",
    "- Model validation (e.g., against a baseline)\n",
    "- Model deployment (if validation passes)\n",
    "\n",
    "### 46.6.1 Example Airflow DAG for Weekly Retraining\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ds-team',\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "def extract_data(**context):\n",
    "    # Pull new data from database or data lake\n",
    "    pass\n",
    "\n",
    "def engineer_features(**context):\n",
    "    # Run feature engineering scripts\n",
    "    pass\n",
    "\n",
    "def train_model(**context):\n",
    "    # Train model and save artifact\n",
    "    pass\n",
    "\n",
    "def validate_model(**context):\n",
    "    # Compare new model with current production model\n",
    "    pass\n",
    "\n",
    "def deploy_model(**context):\n",
    "    # Update production endpoint with new model\n",
    "    pass\n",
    "\n",
    "with DAG(\n",
    "    'nepse_retrain',\n",
    "    schedule_interval='0 2 * * 0',  # 2 AM every Sunday\n",
    "    start_date=datetime(2024, 1, 1),\n",
    "    catchup=False,\n",
    "    default_args=default_args\n",
    ") as dag:\n",
    "\n",
    "    t1 = PythonOperator(task_id='extract_data', python_callable=extract_data)\n",
    "    t2 = PythonOperator(task_id='engineer_features', python_callable=engineer_features)\n",
    "    t3 = PythonOperator(task_id='train_model', python_callable=train_model)\n",
    "    t4 = PythonOperator(task_id='validate_model', python_callable=validate_model)\n",
    "    t5 = PythonOperator(task_id='deploy_model', python_callable=deploy_model)\n",
    "\n",
    "    t1 >> t2 >> t3 >> t4 >> t5\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- This DAG runs weekly on Sunday at 2 AM.  \n",
    "- It chains tasks: data extraction \u2192 feature engineering \u2192 training \u2192 validation \u2192 deployment.  \n",
    "- If validation fails, you could add a branch that sends an alert instead of deploying.\n",
    "\n",
    "---\n",
    "\n",
    "## 46.7 A/B Testing Models\n",
    "\n",
    "Before fully deploying a new model, you should test it against the current production model to ensure it actually improves performance in the live environment. A/B testing (or champion/challenger) is the standard method.\n",
    "\n",
    "### 46.7.1 Setting Up an A/B Test for NEPSE\n",
    "\n",
    "- **Champion**: Current production model.\n",
    "- **Challenger**: Newly trained candidate model.\n",
    "- **Traffic split**: 90% champion, 10% challenger (or 50/50 if you have enough traffic).\n",
    "- **Metric**: Compare prediction accuracy (once ground truth arrives), business metrics (e.g., hypothetical trading returns), or system metrics (latency).\n",
    "\n",
    "You need to ensure that the assignment is consistent (e.g., by user ID or by symbol) to avoid contamination. For a stock\u2011level predictor, you might split by symbol: some symbols use champion, others challenger.\n",
    "\n",
    "**Implementation using a feature flag or routing service:**\n",
    "\n",
    "```python\n",
    "import random\n",
    "\n",
    "def get_model_for_symbol(symbol):\n",
    "    if random.random() < 0.1:  # 10% of requests\n",
    "        return challenger_model\n",
    "    else:\n",
    "        return champion_model\n",
    "```\n",
    "\n",
    "But this random split per request may cause the same symbol to switch models, making comparison noisy. A better approach: hash the symbol and use modulo to assign it consistently.\n",
    "\n",
    "```python\n",
    "def get_model_for_symbol(symbol):\n",
    "    # Consistent hashing\n",
    "    if hash(symbol) % 10 == 0:\n",
    "        return challenger_model\n",
    "    else:\n",
    "        return champion_model\n",
    "```\n",
    "\n",
    "### 46.7.2 Statistical Significance\n",
    "\n",
    "After a test period, you must determine if the challenger is statistically significantly better. Use a statistical test appropriate for your metric (e.g., t\u2011test for continuous metrics, chi\u2011square for proportions). Be mindful of multiple testing if you compare many metrics.\n",
    "\n",
    "```python\n",
    "from scipy.stats import ttest_ind\n",
    "\n",
    "# champion_accuracies: list of daily accuracies for champion\n",
    "# challenger_accuracies: list of daily accuracies for challenger\n",
    "t_stat, p_value = ttest_ind(champion_accuracies, challenger_accuracies)\n",
    "\n",
    "if p_value < 0.05 and challenger_mean > champion_mean:\n",
    "    print(\"Challenger is significantly better; deploy.\")\n",
    "else:\n",
    "    print(\"No significant improvement; keep champion.\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 46.8 Model Comparison Framework\n",
    "\n",
    "Beyond simple A/B testing, you may want to maintain a **model registry** that tracks all versions, their performance metrics, and their deployment status. Tools like MLflow, Weights & Biases, or a simple database can serve this purpose.\n",
    "\n",
    "### 46.8.1 Tracking Model Performance Over Time\n",
    "\n",
    "Store metrics for each model version, and compare them not only at training time but also as they serve production.\n",
    "\n",
    "```python\n",
    "# Example record in model registry database\n",
    "{\n",
    "    \"model_id\": \"rf_20250315\",\n",
    "    \"training_date\": \"2025-03-15\",\n",
    "    \"training_accuracy\": 0.62,\n",
    "    \"validation_accuracy\": 0.59,\n",
    "    \"production_accuracy_last_week\": 0.58,\n",
    "    \"data_drift_detected\": False,\n",
    "    \"deployment_status\": \"champion\"\n",
    "}\n",
    "```\n",
    "\n",
    "You can build a dashboard that shows the performance of all models over time, helping you spot when a champion starts degrading and a challenger might be ready.\n",
    "\n",
    "---\n",
    "\n",
    "## 46.9 Automated Retraining\n",
    "\n",
    "The ultimate goal is to automate the entire retraining loop: monitor \u2192 detect \u2192 retrain \u2192 validate \u2192 deploy \u2192 monitor again. This is sometimes called **auto\u2011pilot** mode.\n",
    "\n",
    "### 46.9.1 Triggering Retraining from Drift Detection\n",
    "\n",
    "We can extend the drift detection system from Chapter 45 to trigger a retraining pipeline when drift exceeds a threshold.\n",
    "\n",
    "**Simplified logic:**\n",
    "\n",
    "```python\n",
    "def monitor_and_retrain():\n",
    "    drift_detected = check_for_drift()\n",
    "    if drift_detected:\n",
    "        # Trigger retraining pipeline (e.g., via Airflow API)\n",
    "        trigger_retraining_dag()\n",
    "```\n",
    "\n",
    "In practice, you might use a tool like **Argo Events** or **AWS Lambda** to respond to drift metrics.\n",
    "\n",
    "### 46.9.2 Safe Rollout of Automatically Retrained Models\n",
    "\n",
    "Automated retraining carries risk: a model trained on anomalous data could be worse than the current one. Therefore, any automatically trained model should go through validation gates:\n",
    "\n",
    "- **Unit tests**: Ensure feature schemas match.\n",
    "- **Performance on holdout set**: Must beat a baseline (e.g., previous model or simple heuristic).\n",
    "- **Shadow deployment**: Run the new model in shadow mode for a period, logging its predictions for comparison without impacting users.\n",
    "- **Canary deployment**: Gradually roll out the new model while monitoring.\n",
    "\n",
    "Only after all gates pass should the model become the new champion.\n",
    "\n",
    "---\n",
    "\n",
    "## 46.10 Best Practices for Continuous Retraining\n",
    "\n",
    "1. **Automate the pipeline** \u2013 Manual retraining is error\u2011prone and doesn\u2019t scale.\n",
    "2. **Version everything** \u2013 Data, code, and models should be versioned together.\n",
    "3. **Monitor model performance in production** \u2013 Use the same metrics you used in validation.\n",
    "4. **Set up alerts for performance degradation** \u2013 Don\u2019t rely solely on scheduled retraining.\n",
    "5. **Use a holdout set that reflects production** \u2013 Time\u2011series cross\u2011validation is essential.\n",
    "6. **Consider the cost\u2011benefit** \u2013 Retraining too often wastes resources; retraining too rarely leads to stale models.\n",
    "7. **Test the retraining process itself** \u2013 Ensure your pipeline can handle data gaps, schema changes, etc.\n",
    "8. **Keep a human in the loop for critical decisions** \u2013 For high\u2011stakes applications like trading, fully automated deployment may require regulatory approval.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored the spectrum of continuous retraining strategies for time\u2011series prediction systems like the NEPSE stock predictor. We covered:\n",
    "\n",
    "- The three main triggers for retraining: time\u2011based, performance\u2011based, and drift\u2011based.\n",
    "- Batch retraining with scheduled jobs, including sliding windows to focus on recent data.\n",
    "- Incremental and online learning techniques that update models continuously without full retraining.\n",
    "- Active learning to selectively label the most informative examples.\n",
    "- Building continuous training pipelines with orchestration tools like Apache Airflow.\n",
    "- A/B testing and shadow deployments to safely validate new model versions.\n",
    "- Automated retraining loops that respond to drift and performance degradation.\n",
    "\n",
    "By implementing a robust retraining strategy, you ensure that your NEPSE prediction system remains accurate and reliable even as market dynamics evolve. In the next chapter, we will discuss **A/B Testing and Model Comparison** in greater depth, focusing on statistical rigor and experiment design.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 46**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='45. model_drift_detection.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='47. ab_testing_and_model_comparison.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}