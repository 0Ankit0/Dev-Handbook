{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 48: Scalability and Performance Optimization\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Identify performance bottlenecks in a time‑series prediction system\n",
    "- Profile Python code to locate CPU and memory hot spots\n",
    "- Apply algorithmic optimisations to reduce computational complexity\n",
    "- Implement parallel processing techniques (multiprocessing, multithreading, async I/O) to speed up data processing and inference\n",
    "- Leverage distributed computing frameworks (Dask, Ray, Spark) for large‑scale feature engineering and model training\n",
    "- Design effective caching strategies to avoid redundant computations\n",
    "- Optimise memory usage when working with large time‑series datasets\n",
    "- Utilise GPU acceleration for deep learning models and large matrix operations\n",
    "- Scale prediction services horizontally and vertically in cloud environments\n",
    "- Balance cost and performance through auto‑scaling and resource right‑sizing\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As your NEPSE prediction system grows—more stocks, higher frequency data, more complex models, and more users—the initial prototype that worked beautifully on your laptop will eventually buckle under the load. Prediction latency increases, batch processing jobs take too long to finish, and costs skyrocket. **Scalability and performance optimisation** are the disciplines that ensure your system can handle growth gracefully, whether it's a 10x increase in data volume or a 100x increase in prediction requests.\n",
    "\n",
    "Scalability is not an afterthought; it must be designed into the system from the beginning. However, even if you started with a simple script, there are many techniques to retrofit performance improvements. In this chapter, we will explore the entire stack: from optimising Python code and algorithms, to parallel and distributed computing, to caching and memory management, and finally to cloud‑scale architectures. Using the NEPSE system as our guide, we will identify typical bottlenecks and apply practical solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## 48.1 Identifying Performance Bottlenecks\n",
    "\n",
    "Before optimising anything, you must know where the time is spent. Guessing leads to wasted effort. Use **profiling** tools to measure exactly which functions and lines of code are the slowest.\n",
    "\n",
    "### 48.1.1 CPU Profiling with cProfile\n",
    "\n",
    "Python's built‑in `cProfile` module records how many times each function is called and how long it takes.\n",
    "\n",
    "```python\n",
    "import cProfile\n",
    "import pstats\n",
    "from nepse_pipeline import run_feature_engineering\n",
    "\n",
    "# Profile the feature engineering function\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "run_feature_engineering('nepse_data.csv')\n",
    "profiler.disable()\n",
    "\n",
    "# Save stats to a file\n",
    "with open('profile_results.txt', 'w') as f:\n",
    "    stats = pstats.Stats(profiler, stream=f)\n",
    "    stats.sort_stats('cumulative')  # Sort by cumulative time\n",
    "    stats.print_stats(20)            # Print top 20\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`cProfile` runs the target function and records every function call. After sorting by cumulative time, you see which functions consume the most time. For example, you might discover that a pandas `rolling` operation is the bottleneck.\n",
    "\n",
    "### 48.1.2 Line‑by‑Line Profiling with line_profiler\n",
    "\n",
    "For more granular insight, `line_profiler` shows time per line of code.\n",
    "\n",
    "```python\n",
    "# First, decorate the function you want to profile\n",
    "@profile\n",
    "def compute_rsi(df, period=14):\n",
    "    delta = df['Close'].diff()\n",
    "    gain = delta.where(delta > 0, 0.0)\n",
    "    loss = -delta.where(delta < 0, 0.0)\n",
    "    avg_gain = gain.rolling(window=period).mean()\n",
    "    avg_loss = loss.rolling(window=period).mean()\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    return rsi\n",
    "\n",
    "# Then run: kernprof -l -v my_script.py\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The `@profile` decorator is used by `kernprof` to record time per line. The output shows, for each line, how many times it was executed and the total time spent. This can reveal that a seemingly innocent line is actually slow.\n",
    "\n",
    "### 48.1.3 Memory Profiling\n",
    "\n",
    "Memory leaks or excessive memory usage can also slow down a system. Use `memory_profiler` to track memory consumption.\n",
    "\n",
    "```python\n",
    "from memory_profiler import profile\n",
    "\n",
    "@profile\n",
    "def load_and_process():\n",
    "    df = pd.read_csv('nepse_all.csv')\n",
    "    df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "    return df\n",
    "\n",
    "load_and_process()\n",
    "```\n",
    "\n",
    "The output shows memory usage per line, helping you identify where data copies or large intermediate objects are created.\n",
    "\n",
    "---\n",
    "\n",
    "## 48.2 Code Optimisation\n",
    "\n",
    "Once you know where the bottlenecks are, you can optimise the code itself. Often, simple changes yield significant speedups.\n",
    "\n",
    "### 48.2.1 Use Vectorised Operations\n",
    "\n",
    "Pandas and NumPy are built on vectorised operations that run in C, which is much faster than Python loops.\n",
    "\n",
    "**Inefficient (Python loop):**\n",
    "\n",
    "```python\n",
    "def compute_sma_loop(df, window):\n",
    "    sma = []\n",
    "    for i in range(len(df)):\n",
    "        if i < window:\n",
    "            sma.append(np.nan)\n",
    "        else:\n",
    "            sma.append(df['Close'].iloc[i-window:i].mean())\n",
    "    return sma\n",
    "```\n",
    "\n",
    "**Efficient (vectorised):**\n",
    "\n",
    "```python\n",
    "def compute_sma_vectorised(df, window):\n",
    "    return df['Close'].rolling(window).mean()\n",
    "```\n",
    "\n",
    "The vectorised version is often 100x faster.\n",
    "\n",
    "### 48.2.2 Avoid Common Pandas Pitfalls\n",
    "\n",
    "- **Use `loc` and `iloc` appropriately**: Chained indexing like `df[df['a'] > 0]['b']` can create copies. Use `df.loc[df['a'] > 0, 'b']`.\n",
    "- **Minise `apply` with custom functions**: `apply` is flexible but slow. If possible, use built‑in vectorised functions. If you must use `apply`, try to use it on a NumPy array rather than a DataFrame.\n",
    "- **Set data types**: Loading a CSV with default types may use more memory than necessary. Specify `dtype` to use smaller types (e.g., `float32` instead of `float64`).\n",
    "\n",
    "```python\n",
    "dtypes = {\n",
    "    'Open': 'float32',\n",
    "    'High': 'float32',\n",
    "    'Low': 'float32',\n",
    "    'Close': 'float32',\n",
    "    'Volume': 'int32'\n",
    "}\n",
    "df = pd.read_csv('nepse.csv', dtype=dtypes)\n",
    "```\n",
    "\n",
    "### 48.2.3 Use Efficient Data Structures\n",
    "\n",
    "For some operations, converting a DataFrame to a NumPy array can be faster because NumPy has less overhead.\n",
    "\n",
    "```python\n",
    "# Instead of df['Close'].values * 2\n",
    "arr = df['Close'].to_numpy()\n",
    "result = arr * 2\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 48.3 Algorithmic Optimisation\n",
    "\n",
    "Sometimes the algorithm itself is the problem. A quadratic algorithm on a large dataset will never be fast enough.\n",
    "\n",
    "### 48.3.1 Reduce Complexity\n",
    "\n",
    "For example, computing rolling statistics from scratch for each window is O(n·k) if done naively. Pandas' rolling implementation is optimised, but if you implement your own, use an incremental update.\n",
    "\n",
    "**Naive rolling sum:**\n",
    "\n",
    "```python\n",
    "def rolling_sum_naive(arr, window):\n",
    "    result = []\n",
    "    for i in range(len(arr)):\n",
    "        if i < window:\n",
    "            result.append(np.nan)\n",
    "        else:\n",
    "            result.append(np.sum(arr[i-window:i]))\n",
    "    return result\n",
    "```\n",
    "\n",
    "**Incremental rolling sum:**\n",
    "\n",
    "```python\n",
    "def rolling_sum_fast(arr, window):\n",
    "    result = [np.nan] * window\n",
    "    cumsum = np.cumsum(arr)\n",
    "    for i in range(window, len(arr)):\n",
    "        result.append(cumsum[i] - cumsum[i-window])\n",
    "    return result\n",
    "```\n",
    "\n",
    "The incremental version is O(n) instead of O(n·k).\n",
    "\n",
    "### 48.3.2 Use Appropriate Libraries\n",
    "\n",
    "For specialised tasks like technical indicators, use libraries that are already optimised.\n",
    "\n",
    "- **TA‑Lib** (Technical Analysis Library) provides C‑optimised functions for RSI, MACD, etc.\n",
    "- **NumPy** and **SciPy** for linear algebra and statistical operations.\n",
    "- **Numba** for just‑in‑time compilation of Python loops.\n",
    "\n",
    "**Example with Numba:**\n",
    "\n",
    "```python\n",
    "from numba import jit\n",
    "import numpy as np\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_rsi(prices, period=14):\n",
    "    deltas = np.diff(prices)\n",
    "    gain = np.where(deltas > 0, deltas, 0.0)\n",
    "    loss = np.where(deltas < 0, -deltas, 0.0)\n",
    "    avg_gain = np.zeros_like(prices)\n",
    "    avg_loss = np.zeros_like(prices)\n",
    "    avg_gain[period] = np.mean(gain[:period])\n",
    "    avg_loss[period] = np.mean(loss[:period])\n",
    "    for i in range(period+1, len(prices)):\n",
    "        avg_gain[i] = (avg_gain[i-1] * (period-1) + gain[i-1]) / period\n",
    "        avg_loss[i] = (avg_loss[i-1] * (period-1) + loss[i-1]) / period\n",
    "    rs = avg_gain / avg_loss\n",
    "    rsi = 100 - 100 / (1 + rs)\n",
    "    return rsi\n",
    "```\n",
    "\n",
    "Numba compiles this loop to machine code, often achieving speeds comparable to C.\n",
    "\n",
    "---\n",
    "\n",
    "## 48.4 Parallel Processing\n",
    "\n",
    "Modern CPUs have multiple cores. Parallel processing allows you to use them all.\n",
    "\n",
    "### 48.4.1 Multiprocessing\n",
    "\n",
    "The `multiprocessing` module is ideal for CPU‑bound tasks that can run independently, such as computing features for many stocks in parallel.\n",
    "\n",
    "**Example: Process multiple symbols in parallel**\n",
    "\n",
    "```python\n",
    "import multiprocessing as mp\n",
    "import pandas as pd\n",
    "\n",
    "def process_symbol(symbol):\n",
    "    \"\"\"Load data for a single symbol and compute features.\"\"\"\n",
    "    df = pd.read_csv(f'data/{symbol}.csv')\n",
    "    df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "    df['RSI'] = compute_rsi(df['Close'])\n",
    "    return df\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    symbols = ['NABIL', 'NTC', 'SBI', 'HRL', 'NICA']\n",
    "    with mp.Pool(processes=4) as pool:\n",
    "        results = pool.map(process_symbol, symbols)\n",
    "    # results is a list of DataFrames, one per symbol\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`Pool.map` distributes the list of symbols across worker processes. Each worker runs `process_symbol` independently. This can speed up batch feature engineering significantly.\n",
    "\n",
    "### 48.4.2 Multithreading\n",
    "\n",
    "Python's Global Interpreter Lock (GIL) prevents multiple threads from executing Python bytecode simultaneously. Therefore, multithreading is only beneficial for I/O‑bound tasks (e.g., waiting for network responses, reading files).\n",
    "\n",
    "**Example: Fetching data from an API concurrently**\n",
    "\n",
    "```python\n",
    "import concurrent.futures\n",
    "import requests\n",
    "\n",
    "def fetch_symbol_data(symbol):\n",
    "    url = f\"https://api.nepse.com/stock/{symbol}\"\n",
    "    response = requests.get(url)\n",
    "    return response.json()\n",
    "\n",
    "symbols = ['NABIL', 'NTC', 'SBI']\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=5) as executor:\n",
    "    results = list(executor.map(fetch_symbol_data, symbols))\n",
    "```\n",
    "\n",
    "While one thread waits for the network, others can run.\n",
    "\n",
    "### 48.4.3 Asynchronous I/O\n",
    "\n",
    "For even higher concurrency, use `asyncio` with an HTTP client like `aiohttp`.\n",
    "\n",
    "```python\n",
    "import aiohttp\n",
    "import asyncio\n",
    "\n",
    "async def fetch_symbol(session, symbol):\n",
    "    url = f\"https://api.nepse.com/stock/{symbol}\"\n",
    "    async with session.get(url) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def fetch_all(symbols):\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_symbol(session, sym) for sym in symbols]\n",
    "        return await asyncio.gather(*tasks)\n",
    "\n",
    "symbols = ['NABIL', 'NTC', 'SBI']\n",
    "loop = asyncio.get_event_loop()\n",
    "results = loop.run_until_complete(fetch_all(symbols))\n",
    "```\n",
    "\n",
    "Asyncio can handle thousands of concurrent connections efficiently.\n",
    "\n",
    "---\n",
    "\n",
    "## 48.5 Distributed Computing\n",
    "\n",
    "When a single machine is not enough—either because the data is too large or the computation too heavy—you need distributed computing.\n",
    "\n",
    "### 48.5.1 Dask\n",
    "\n",
    "Dask provides parallel and distributed computing with a familiar pandas/NumPy interface. It can scale from a single machine to a cluster.\n",
    "\n",
    "**Example: Parallel rolling window computation with Dask**\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "\n",
    "# Read multiple CSV files into a Dask DataFrame\n",
    "df = dd.read_csv('data/nepse_*.csv', parse_dates=['Date'])\n",
    "\n",
    "# Compute rolling mean (Dask handles partitioning)\n",
    "df['SMA_20'] = df.groupby('Symbol')['Close'].rolling(20).mean().reset_index(drop=True)\n",
    "\n",
    "# Trigger computation\n",
    "result = df.compute()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Dask splits the data into partitions and processes them in parallel. Operations are lazy; you build a computation graph, then call `.compute()` to execute. Dask can also scale to a cluster of machines.\n",
    "\n",
    "### 48.5.2 Ray\n",
    "\n",
    "Ray is a general‑purpose distributed execution framework. It's great for parallelising Python functions across a cluster.\n",
    "\n",
    "**Example: Distributed feature engineering with Ray**\n",
    "\n",
    "```python\n",
    "import ray\n",
    "import pandas as pd\n",
    "\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def process_symbol(symbol):\n",
    "    df = pd.read_csv(f'data/{symbol}.csv')\n",
    "    df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "    df['RSI'] = compute_rsi(df['Close'])\n",
    "    return df\n",
    "\n",
    "symbols = ['NABIL', 'NTC', 'SBI', 'HRL', 'NICA']\n",
    "futures = [process_symbol.remote(sym) for sym in symbols]\n",
    "results = ray.get(futures)\n",
    "```\n",
    "\n",
    "Ray handles task scheduling, data passing, and fault tolerance.\n",
    "\n",
    "### 48.5.3 Apache Spark\n",
    "\n",
    "Spark is the industry standard for large‑scale data processing. It's particularly suited for batch pipelines.\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, avg, window\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NEPSE\").getOrCreate()\n",
    "\n",
    "df = spark.read.option(\"header\", \"true\").csv(\"data/nepse_all.csv\")\n",
    "\n",
    "# Compute daily average price per symbol\n",
    "result = df.groupBy(\"Symbol\", window(\"Date\", \"1 day\")).agg(avg(\"Close\").alias(\"avg_close\"))\n",
    "result.show()\n",
    "```\n",
    "\n",
    "Spark's Catalyst optimiser and Tungsten execution engine make it very efficient for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## 48.6 Caching Strategies\n",
    "\n",
    "Caching avoids recomputing expensive results. In a prediction system, many intermediate values can be cached.\n",
    "\n",
    "### 48.6.1 In‑Memory Caching with Redis\n",
    "\n",
    "Redis is an in‑memory key‑value store that can cache feature vectors, model predictions, or even pre‑computed technical indicators.\n",
    "\n",
    "**Example: Cache rolling averages per symbol**\n",
    "\n",
    "```python\n",
    "import redis\n",
    "import pickle\n",
    "import pandas as pd\n",
    "\n",
    "r = redis.Redis(host='localhost', port=6379, db=0)\n",
    "\n",
    "def get_sma(symbol, window, force_recompute=False):\n",
    "    cache_key = f\"sma:{symbol}:{window}\"\n",
    "    if not force_recompute:\n",
    "        cached = r.get(cache_key)\n",
    "        if cached:\n",
    "            return pickle.loads(cached)\n",
    "    # Compute if not cached\n",
    "    df = pd.read_csv(f'data/{symbol}.csv')\n",
    "    sma = df['Close'].rolling(window).mean().tolist()\n",
    "    r.setex(cache_key, 3600, pickle.dumps(sma))  # Cache for 1 hour\n",
    "    return sma\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Before computing the SMA, we check Redis. If present, we return the cached result. If not, we compute and store it with an expiration time. This is especially useful for features that are expensive to compute and change infrequently.\n",
    "\n",
    "### 48.6.2 Application‑Level Caching with `functools.lru_cache`\n",
    "\n",
    "For pure functions, Python's built‑in `lru_cache` can memoise results.\n",
    "\n",
    "```python\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def compute_rsi(series_tuple, period=14):\n",
    "    # series_tuple is a tuple of prices (since lists are not hashable)\n",
    "    prices = np.array(series_tuple)\n",
    "    # ... RSI computation ...\n",
    "    return rsi\n",
    "\n",
    "# Usage\n",
    "prices_tuple = tuple(df['Close'].values)\n",
    "rsi = compute_rsi(prices_tuple)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`lru_cache` stores the results of function calls. If the same arguments are passed again, the cached result is returned. This is useful for functions called repeatedly with the same inputs.\n",
    "\n",
    "### 48.6.3 Database Caching with Materialised Views\n",
    "\n",
    "If you use a database like PostgreSQL, you can create materialised views that store pre‑computed aggregates and refresh them periodically.\n",
    "\n",
    "```sql\n",
    "CREATE MATERIALIZED VIEW sma_20 AS\n",
    "SELECT symbol, date, AVG(close) OVER (PARTITION BY symbol ORDER BY date ROWS 19 PRECEDING) AS sma_20\n",
    "FROM prices;\n",
    "\n",
    "REFRESH MATERIALIZED VIEW sma_20;  -- Run daily\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 48.7 Memory Optimisation\n",
    "\n",
    "Large time‑series datasets can easily exceed available RAM. Optimising memory usage allows you to work with more data on the same hardware.\n",
    "\n",
    "### 48.7.1 Downcast Numeric Types\n",
    "\n",
    "Use the smallest possible data type for each column.\n",
    "\n",
    "```python\n",
    "def optimise_floats(df):\n",
    "    floats = df.select_dtypes(include=['float64']).columns\n",
    "    for col in floats:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='float')\n",
    "    return df\n",
    "\n",
    "def optimise_ints(df):\n",
    "    ints = df.select_dtypes(include=['int64']).columns\n",
    "    for col in ints:\n",
    "        df[col] = pd.to_numeric(df[col], downcast='integer')\n",
    "    return df\n",
    "```\n",
    "\n",
    "This can reduce memory usage by 50% or more.\n",
    "\n",
    "### 48.7.2 Use Categoricals for Low‑Cardinality Columns\n",
    "\n",
    "Columns like `Symbol` have many repeated values. Convert them to categorical.\n",
    "\n",
    "```python\n",
    "df['Symbol'] = df['Symbol'].astype('category')\n",
    "```\n",
    "\n",
    "This stores the unique symbols once and uses integer indices, saving memory and speeding up groupby operations.\n",
    "\n",
    "### 48.7.3 Chunking\n",
    "\n",
    "When you cannot fit the entire dataset in memory, process it in chunks.\n",
    "\n",
    "```python\n",
    "chunk_size = 10000\n",
    "reader = pd.read_csv('nepse_all.csv', chunksize=chunk_size)\n",
    "for chunk in reader:\n",
    "    process_chunk(chunk)\n",
    "```\n",
    "\n",
    "Each chunk is processed independently, and results can be aggregated.\n",
    "\n",
    "### 48.7.4 Out‑of‑Core Computation with Dask or Vaex\n",
    "\n",
    "Libraries like Dask and Vaex are designed for datasets larger than memory. They operate lazily and only load data as needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 48.8 GPU Acceleration\n",
    "\n",
    "For deep learning models and large matrix operations, GPUs can provide massive speedups. Frameworks like TensorFlow, PyTorch, and RAPIDS (cuDF, cuML) leverage GPUs.\n",
    "\n",
    "### 48.8.1 Using cuDF for GPU‑Accelerated DataFrames\n",
    "\n",
    "RAPIDS cuDF provides a pandas‑like interface that runs on NVIDIA GPUs.\n",
    "\n",
    "```python\n",
    "import cudf\n",
    "\n",
    "# Read CSV directly into GPU memory\n",
    "df = cudf.read_csv('nepse_all.csv')\n",
    "\n",
    "# Rolling operations run on GPU\n",
    "df['SMA_20'] = df['Close'].rolling(20).mean()\n",
    "\n",
    "# Convert back to pandas if needed\n",
    "pandas_df = df.to_pandas()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "cuDF uses the GPU's massive parallelism to speed up operations. For large datasets, this can be 10‑100x faster than pandas.\n",
    "\n",
    "### 48.8.2 GPU‑Accelerated Machine Learning\n",
    "\n",
    "Use cuML for GPU‑accelerated scikit‑learn style models.\n",
    "\n",
    "```python\n",
    "from cuml import RandomForestClassifier\n",
    "\n",
    "model = RandomForestClassifier(n_estimators=100)\n",
    "model.fit(X_train, y_train)  # X_train and y_train can be cuDF DataFrames\n",
    "```\n",
    "\n",
    "### 48.8.3 Deep Learning with PyTorch/TensorFlow\n",
    "\n",
    "For neural networks, GPUs are essential. Ensure your data loading pipeline does not become a bottleneck.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "# Move data to GPU\n",
    "X_tensor = torch.tensor(X.values, device='cuda')\n",
    "y_tensor = torch.tensor(y.values, device='cuda')\n",
    "dataset = TensorDataset(X_tensor, y_tensor)\n",
    "dataloader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 48.9 Scaling Prediction Services\n",
    "\n",
    "The prediction API itself must scale to handle increasing request rates.\n",
    "\n",
    "### 48.9.1 Horizontal Scaling\n",
    "\n",
    "Run multiple copies of your prediction service behind a load balancer.\n",
    "\n",
    "**With Kubernetes:**\n",
    "\n",
    "```yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nepse-predictor\n",
    "spec:\n",
    "  replicas: 5  # Run 5 pods\n",
    "  template:\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: predictor\n",
    "        image: nepse-predictor:latest\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "---\n",
    "apiVersion: v1\n",
    "kind: Service\n",
    "metadata:\n",
    "  name: nepse-predictor\n",
    "spec:\n",
    "  selector:\n",
    "    app: nepse-predictor\n",
    "  ports:\n",
    "  - port: 80\n",
    "    targetPort: 8000\n",
    "  type: LoadBalancer\n",
    "```\n",
    "\n",
    "The load balancer distributes requests among the pods.\n",
    "\n",
    "### 48.9.2 Vertical Scaling\n",
    "\n",
    "Increase the resources (CPU, memory) of the machine running the service. This is simpler but has limits.\n",
    "\n",
    "### 48.9.3 Auto‑scaling\n",
    "\n",
    "In Kubernetes, you can use the Horizontal Pod Autoscaler to automatically adjust the number of replicas based on CPU utilisation or custom metrics.\n",
    "\n",
    "```yaml\n",
    "apiVersion: autoscaling/v2\n",
    "kind: HorizontalPodAutoscaler\n",
    "metadata:\n",
    "  name: nepse-predictor-hpa\n",
    "spec:\n",
    "  scaleTargetRef:\n",
    "    apiVersion: apps/v1\n",
    "    kind: Deployment\n",
    "    name: nepse-predictor\n",
    "  minReplicas: 2\n",
    "  maxReplicas: 10\n",
    "  metrics:\n",
    "  - type: Resource\n",
    "    resource:\n",
    "      name: cpu\n",
    "      target:\n",
    "        type: Utilization\n",
    "        averageUtilization: 70\n",
    "```\n",
    "\n",
    "When CPU usage exceeds 70%, Kubernetes will spin up more pods, up to a maximum of 10.\n",
    "\n",
    "### 48.9.4 Optimising Model Serving\n",
    "\n",
    "- **Batch inference**: If your model supports it, process multiple requests together to amortise overhead.\n",
    "- **Model quantisation**: Use reduced precision (e.g., float16) to speed up inference and reduce memory.\n",
    "- **Hardware acceleration**: Use specialised hardware like GPUs or TPUs for deep learning, or Intel's oneDNN for CPU optimisation.\n",
    "\n",
    "---\n",
    "\n",
    "## 48.10 Cloud Scaling\n",
    "\n",
    "Cloud providers offer virtually unlimited scalability, but it comes at a cost. You must balance performance with expenditure.\n",
    "\n",
    "### 48.10.1 Choosing Instance Types\n",
    "\n",
    "- **Compute‑optimised** (e.g., AWS C5 family) for CPU‑bound prediction services.\n",
    "- **Memory‑optimised** (e.g., AWS R5 family) for large feature stores.\n",
    "- **GPU instances** (e.g., AWS P3, P4) for deep learning training.\n",
    "\n",
    "### 48.10.2 Spot/Preemptible Instances\n",
    "\n",
    "For non‑critical batch jobs, use spot instances (AWS) or preemptible VMs (GCP) at a fraction of the cost. They can be terminated at any time, so design your pipeline to be resilient.\n",
    "\n",
    "### 48.10.3 Serverless Options\n",
    "\n",
    "For sporadic workloads, serverless (AWS Lambda, Google Cloud Functions) can be cost‑effective. However, cold starts and time limits may be problematic.\n",
    "\n",
    "### 48.10.4 Managed Services\n",
    "\n",
    "Consider managed services for parts of your stack:\n",
    "\n",
    "- **AWS SageMaker** for training and deployment.\n",
    "- **Google Vertex AI** for end‑to‑end ML.\n",
    "- **Azure Machine Learning** for similar capabilities.\n",
    "\n",
    "These services handle scaling, monitoring, and maintenance for you, but they can be more expensive than self‑managed solutions.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored the vast landscape of scalability and performance optimisation for time‑series prediction systems, using the NEPSE stock predictor as a concrete example. We covered:\n",
    "\n",
    "- Profiling to identify bottlenecks with `cProfile`, `line_profiler`, and memory profilers.\n",
    "- Code optimisation techniques such as vectorisation, efficient pandas usage, and data type tuning.\n",
    "- Algorithmic improvements to reduce complexity and leverage specialised libraries like Numba and TA‑Lib.\n",
    "- Parallel processing with `multiprocessing` for CPU‑bound tasks and multithreading/async for I/O‑bound tasks.\n",
    "- Distributed computing frameworks (Dask, Ray, Spark) for scaling beyond a single machine.\n",
    "- Caching strategies at multiple levels (Redis, `lru_cache`, materialised views) to avoid redundant work.\n",
    "- Memory optimisation through downcasting, categoricals, and chunking.\n",
    "- GPU acceleration with RAPIDS, cuML, and deep learning frameworks.\n",
    "- Scaling prediction services horizontally, vertically, and with auto‑scaling in Kubernetes.\n",
    "- Cloud considerations, including instance selection, spot instances, and managed services.\n",
    "\n",
    "By applying these techniques, you can ensure that your NEPSE prediction system remains responsive and cost‑effective as it grows. In the next chapter, we will discuss **Security and Compliance**, ensuring that your system protects sensitive financial data and meets regulatory requirements.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 48**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
