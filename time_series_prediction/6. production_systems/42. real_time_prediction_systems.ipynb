{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 42: Real-Time Prediction Systems\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the architecture of a real\u2011time prediction system and its components\n",
    "- Identify the differences between batch and stream processing for time\u2011series forecasting\n",
    "- Choose and implement a streaming platform (Apache Kafka) to ingest live market data\n",
    "- Build stream processing pipelines that compute features and serve predictions with low latency\n",
    "- Manage state in streaming applications to maintain rolling windows and feature aggregations\n",
    "- Handle backpressure and guarantee exactly\u2011once processing semantics\n",
    "- Monitor and scale real\u2011time prediction systems for production workloads\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous chapters, we built batch prediction systems that process historical NEPSE data once per day after the market closes. But in financial markets, opportunities vanish in seconds. A **real\u2011time prediction system** allows us to react to price movements as they happen\u2014detecting breakout patterns, sudden volatility changes, or circuit\u2011breaker triggers within milliseconds of a new trade.\n",
    "\n",
    "For the NEPSE (Nepal Stock Exchange) example, a real\u2011time system would continuously consume live tick data (or minute\u2011level aggregates) from a market data feed, compute technical indicators on the fly, and emit predictions (e.g., next\u2011minute price direction) before the next tick arrives. This chapter covers the design, implementation, and operational considerations of such systems, using NEPSE as a running example.\n",
    "\n",
    "---\n",
    "\n",
    "## 42.1 Real\u2011Time Architecture\n",
    "\n",
    "A real\u2011time prediction system is a continuous data pipeline that ingests events as they occur, processes them with minimal delay, and produces predictions or alerts. Unlike batch systems that run on a schedule (e.g., every 24 hours), stream processors operate on **unbounded data** and must handle out\u2011of\u2011order events, late arrivals, and stateful computations.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "1. **Data Source** \u2013 The origin of live events (e.g., stock ticks from a WebSocket, messages from a message queue, or changes in a database).\n",
    "2. **Ingestion Layer** \u2013 A distributed, fault\u2011tolerant message broker that buffers and distributes the event stream (e.g., Apache Kafka, Apache Pulsar).\n",
    "3. **Stream Processor** \u2013 A continuous computation engine that reads from the ingestion layer, applies transformations (feature engineering, model inference), and writes results (e.g., Apache Flink, Apache Spark Streaming, or custom microservices).\n",
    "4. **Model Serving** \u2013 The component that hosts the trained machine learning model and exposes it for low\u2011latency scoring. This can be embedded in the stream processor or run as a separate service.\n",
    "5. **Output Sink** \u2013 The destination of the predictions: a database, a dashboard, an alerting system, or another message queue for downstream applications.\n",
    "\n",
    "![Real\u2011Time Prediction Architecture](images/real_time_arch.png)\n",
    "\n",
    "For the NEPSE system, a realistic architecture might look like this:\n",
    "\n",
    "- **Data Source**: A WebSocket connection to a market data provider that pushes real\u2011time trade and quote data.\n",
    "- **Ingestion**: Apache Kafka, which accepts millions of events per second and retains them for replay.\n",
    "- **Stream Processor**: A Python application using the Faust library, or a Flink job written in Java, that:\n",
    "  - Parses each tick\n",
    "  - Updates rolling windows (e.g., 5\u2011minute moving average)\n",
    "  - Computes features (RSI, MACD, volume anomalies)\n",
    "  - Calls a pre\u2011trained XGBoost model to predict next\u2011minute direction\n",
    "- **Output Sink**: A PostgreSQL database for historical logging and a Redis cache for real\u2011time dashboards.\n",
    "\n",
    "The following code snippet simulates a simple real\u2011time ingestion pipeline using Python and `confluent_kafka` to consume NEPSE ticks from a Kafka topic.\n",
    "\n",
    "```python\n",
    "# consumer.py\n",
    "from confluent_kafka import Consumer, KafkaError\n",
    "import json\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "# Load pre\u2011trained model and feature scaler (trained in batch)\n",
    "model = joblib.load('nepse_xgboost.pkl')\n",
    "scaler = joblib.load('feature_scaler.pkl')\n",
    "\n",
    "# Configure Kafka consumer\n",
    "conf = {\n",
    "    'bootstrap.servers': 'localhost:9092',\n",
    "    'group.id': 'nepse-predictor',\n",
    "    'auto.offset.reset': 'latest'\n",
    "}\n",
    "consumer = Consumer(conf)\n",
    "consumer.subscribe(['nepse-ticks'])\n",
    "\n",
    "def extract_features(tick):\n",
    "    \"\"\"\n",
    "    Convert a raw tick dict into a feature vector.\n",
    "    In a real system, we would maintain state (previous ticks)\n",
    "    to compute lag features.\n",
    "    \"\"\"\n",
    "    # For demonstration, we use only the latest price and volume\n",
    "    return np.array([[tick['price'], tick['volume']]])\n",
    "\n",
    "while True:\n",
    "    msg = consumer.poll(1.0)\n",
    "    if msg is None:\n",
    "        continue\n",
    "    if msg.error():\n",
    "        if msg.error().code() == KafkaError._PARTITION_EOF:\n",
    "            continue\n",
    "        else:\n",
    "            print(msg.error())\n",
    "            break\n",
    "\n",
    "    # Decode the JSON message\n",
    "    tick = json.loads(msg.value().decode('utf-8'))\n",
    "    print(f\"Received tick: {tick}\")\n",
    "\n",
    "    # Feature extraction (simplified)\n",
    "    features = extract_features(tick)\n",
    "    features_scaled = scaler.transform(features)\n",
    "\n",
    "    # Predict (e.g., probability of price increase in next minute)\n",
    "    prob = model.predict_proba(features_scaled)[0, 1]\n",
    "    print(f\"Predicted up probability: {prob:.3f}\")\n",
    "\n",
    "    # Here we would write the prediction to a sink (e.g., Redis, InfluxDB)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This consumer demonstrates the core loop of a real\u2011time predictor. It subscribes to a Kafka topic `nepse-ticks` where each message is a JSON object representing a stock tick (price, volume, timestamp). After polling for new messages, it decodes the JSON, extracts features (in a real system we would maintain a state store with previous ticks), scales them using a pre\u2011fitted `StandardScaler`, and feeds them into an XGBoost classifier. The resulting probability is printed and could be sent to a dashboard. Note the use of `auto.offset.reset=latest` to only consume new messages from the moment the consumer starts \u2013 this is typical for real\u2011time applications that don't need historical replay.\n",
    "\n",
    "---\n",
    "\n",
    "## 42.2 Streaming Platforms\n",
    "\n",
    "A streaming platform is the backbone of any real\u2011time system. It decouples data producers from consumers, provides durability, and enables multiple applications to read the same stream independently. For the NEPSE system, we need a platform that can handle high throughput (thousands of ticks per second) and provide at\u2011least\u2011once or exactly\u2011once delivery guarantees.\n",
    "\n",
    "### 42.2.1 Apache Kafka\n",
    "\n",
    "Apache Kafka is the de facto standard for event streaming. It is a distributed, partitioned, replicated commit log service. Messages are organized into **topics**, and each topic can be split into **partitions** for parallelism. Producers write to topics, consumers read from them, and Kafka retains messages for a configurable period (even after consumption) allowing replay.\n",
    "\n",
    "**Key Concepts:**\n",
    "- **Producer**: Publishes messages to a topic.\n",
    "- **Consumer**: Subscribes to topics and processes messages.\n",
    "- **Consumer Group**: A set of consumers that cooperate to consume a topic; each partition is assigned to one consumer in the group.\n",
    "- **Offset**: A unique identifier for each message within a partition, used to track consumption progress.\n",
    "\n",
    "For the NEPSE system, we might have a topic `nepse-ticks` partitioned by stock symbol. This allows parallel consumption: one consumer per symbol, or multiple consumers sharing the load.\n",
    "\n",
    "**Example Producer** (simulating tick data from a CSV file):\n",
    "\n",
    "```python\n",
    "# producer.py\n",
    "import csv\n",
    "import json\n",
    "import time\n",
    "from confluent_kafka import Producer\n",
    "\n",
    "conf = {'bootstrap.servers': 'localhost:9092'}\n",
    "producer = Producer(conf)\n",
    "\n",
    "def delivery_report(err, msg):\n",
    "    if err is not None:\n",
    "        print(f\"Message delivery failed: {err}\")\n",
    "\n",
    "# Simulate real\u2011time by reading a NEPSE CSV file row by row\n",
    "with open('nepse_daily.csv', 'r') as f:\n",
    "    reader = csv.DictReader(f)\n",
    "    for row in reader:\n",
    "        # Convert to JSON and send\n",
    "        tick = {\n",
    "            'symbol': row['Symbol'],\n",
    "            'price': float(row['Close']),\n",
    "            'volume': int(row['Vol']),\n",
    "            'timestamp': row.get('Date', time.strftime('%Y-%m-%d %H:%M:%S'))\n",
    "        }\n",
    "        producer.produce(\n",
    "            'nepse-ticks',\n",
    "            key=tick['symbol'].encode('utf-8'),\n",
    "            value=json.dumps(tick).encode('utf-8'),\n",
    "            callback=delivery_report\n",
    "        )\n",
    "        producer.poll(0)  # Trigger delivery reports\n",
    "        time.sleep(1)     # Simulate one tick per second\n",
    "    producer.flush()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This producer reads a batch CSV file line by line, constructs a JSON tick message, and publishes it to the `nepse-ticks` topic. By using the stock symbol as the message key, we ensure that all ticks for the same symbol go to the same partition, preserving order per symbol. The `time.sleep(1)` simulates a real\u2011time stream; in production, ticks arrive as they happen, not on a fixed schedule.\n",
    "\n",
    "### 42.2.2 Apache Pulsar\n",
    "\n",
    "Apache Pulsar is another cloud\u2011native streaming platform that offers native support for multi\u2011tenancy, geo\u2011replication, and a simpler architecture than Kafka. It separates serving and storing layers, making it easier to scale independently. Pulsar also provides a richer set of subscription types (exclusive, shared, failover) and supports both queue and stream semantics. While less commonly used than Kafka, it is gaining traction for large\u2011scale deployments.\n",
    "\n",
    "### 42.2.3 Cloud Streaming Services\n",
    "\n",
    "Major cloud providers offer managed streaming services that abstract away cluster management:\n",
    "\n",
    "- **AWS Kinesis**: Fully managed service for real\u2011time data streaming. Data is stored in shards, and consumers read from shards using the Kinesis Client Library (KCL).\n",
    "- **Google Cloud Pub/Sub**: Simple, reliable messaging with at\u2011least\u2011once delivery and configurable retention.\n",
    "- **Azure Event Hubs**: Scalable event ingestion service compatible with Kafka protocol.\n",
    "\n",
    "For a NEPSE prototype, a managed service can be easier to set up, but for learning purposes, running Kafka locally (or in Docker) is sufficient.\n",
    "\n",
    "---\n",
    "\n",
    "## 42.3 Stream Processing\n",
    "\n",
    "Once data is in a streaming platform, we need to process it continuously. Stream processing engines apply transformations\u2014filtering, aggregation, joining, windowing\u2014to the unbounded data stream. They also maintain state and handle late\u2011arriving data.\n",
    "\n",
    "### 42.3.1 Apache Flink\n",
    "\n",
    "Apache Flink is a powerful stream processing framework that provides exactly\u2011once semantics, event\u2011time processing, and sophisticated windowing. It can be used with Java/Scala, but also offers a Python API (PyFlink). For the NEPSE system, we might use Flink to compute rolling technical indicators every minute.\n",
    "\n",
    "**Example Flink job (simplified, in Java):**\n",
    "\n",
    "```java\n",
    "// Pseudo\u2011code for Flink job that computes 5\u2011minute SMA\n",
    "DataStream<Tick> ticks = env.addSource(new FlinkKafkaConsumer<>(\"nepse-ticks\", ...));\n",
    "\n",
    "ticks\n",
    "    .keyBy(tick -> tick.symbol)\n",
    "    .window(TumblingProcessingTimeWindows.of(Time.minutes(5)))\n",
    "    .aggregate(new AveragePrice())\n",
    "    .addSink(new RedisSink<>());\n",
    "```\n",
    "\n",
    "Flink handles out\u2011of\u2011order events via watermarks and supports event\u2011time processing, which is crucial for accurate financial analytics where timestamps are embedded in the data, not when the message is processed.\n",
    "\n",
    "### 42.3.2 Apache Spark Streaming\n",
    "\n",
    "Apache Spark Streaming (now unified under Structured Streaming) treats streams as continuous tables. It offers a high\u2011level DataFrame API and integrates seamlessly with Spark MLlib. Micro\u2011batch processing (as opposed to true streaming) introduces small latencies (sub\u2011second to a few seconds), which may be acceptable for many financial applications.\n",
    "\n",
    "**Example Structured Streaming with Python:**\n",
    "\n",
    "```python\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import window, avg\n",
    "\n",
    "spark = SparkSession.builder.appName(\"NEPSEStreaming\").getOrCreate()\n",
    "\n",
    "# Read from Kafka\n",
    "df = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"nepse-ticks\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON and compute 5\u2011minute average price per symbol\n",
    "ticks = df.selectExpr(\"CAST(value AS STRING) as json\") \\\n",
    "    .selectExpr(\"json_tuple(json, 'symbol', 'price', 'timestamp') as (symbol, price, ts)\") \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"double\")) \\\n",
    "    .withColumn(\"timestamp\", col(\"ts\").cast(\"timestamp\"))\n",
    "\n",
    "windowedAvg = ticks \\\n",
    "    .withWatermark(\"timestamp\", \"1 minute\") \\\n",
    "    .groupBy(window(\"timestamp\", \"5 minutes\"), \"symbol\") \\\n",
    "    .agg(avg(\"price\").alias(\"avg_price\"))\n",
    "\n",
    "# Write to console (or sink)\n",
    "query = windowedAvg \\\n",
    "    .writeStream \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .format(\"console\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This Spark Structured Streaming application reads from Kafka, parses the JSON, casts fields to appropriate types, and then computes a 5\u2011minute tumbling window average of the price for each symbol. Watermarks handle late data (up to 1 minute). The result is printed to the console, but could be written to a database or Kafka.\n",
    "\n",
    "### 42.3.3 Custom Solutions with Python\n",
    "\n",
    "For lightweight applications or when integrating with custom ML models, a pure Python stream processor can be built using libraries like **Faust** (a stream processing library that mimics Kafka Streams) or **Bytewax** (a Python\u2011native stream engine). Faust allows you to define stateful operators using async/await syntax and integrates tightly with Kafka.\n",
    "\n",
    "**Example Faust application for NEPSE:**\n",
    "\n",
    "```python\n",
    "import faust\n",
    "import joblib\n",
    "import numpy as np\n",
    "\n",
    "app = faust.App('nepse-processor', broker='kafka://localhost:9092')\n",
    "\n",
    "# Define a Faust record (schema) for ticks\n",
    "class Tick(faust.Record):\n",
    "    symbol: str\n",
    "    price: float\n",
    "    volume: int\n",
    "    timestamp: str\n",
    "\n",
    "topic = app.topic('nepse-ticks', value_type=Tick)\n",
    "\n",
    "# Load model once at startup (outside agent)\n",
    "model = joblib.load('nepse_xgboost.pkl')\n",
    "scaler = joblib.load('feature_scaler.pkl')\n",
    "\n",
    "@app.agent(topic)\n",
    "async def process(ticks):\n",
    "    # Maintain state for last 10 prices per symbol (e.g., using a Table)\n",
    "    # Here we simply predict on each tick (not realistic, just for demo)\n",
    "    async for tick in ticks:\n",
    "        features = np.array([[tick.price, tick.volume]])\n",
    "        features_scaled = scaler.transform(features)\n",
    "        prob = model.predict_proba(features_scaled)[0, 1]\n",
    "        print(f\"{tick.symbol}: up prob {prob:.3f}\")\n",
    "        # Optionally send prediction to another topic\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Faust applications define agents (asynchronous generators) that consume from topics. The agent runs continuously, processing each message as it arrives. Faust also provides tables for stateful operations (e.g., keeping a rolling window). This is a good choice for Python\u2011heavy teams.\n",
    "\n",
    "---\n",
    "\n",
    "## 42.4 Low\u2011Latency Inference\n",
    "\n",
    "In real\u2011time prediction, the time from receiving a tick to emitting a prediction must be as short as possible\u2014ideally under a few milliseconds. Achieving low latency requires careful optimization at multiple levels:\n",
    "\n",
    "- **Model Choice**: Simple models (linear regression, small trees) are faster than complex neural networks. For NEPSE, an XGBoost model with limited depth (e.g., max_depth=6) can score in microseconds.\n",
    "- **Pre\u2011computation**: Features that depend only on current tick (price, volume) can be computed on\u2011the\u2011fly; features that require historical windows need state management.\n",
    "- **Model Serving**: Instead of loading the model in the stream processor, it can be exposed as a REST/gRPC service for better isolation and scaling. However, network overhead adds latency. For ultra\u2011low latency, embed the model directly.\n",
    "- **Hardware**: Use of GPUs or specialised inference chips (e.g., NVIDIA Triton) for deep learning; for tree\u2011based models, CPUs are sufficient.\n",
    "- **Batching**: Grouping multiple inference requests can improve throughput but adds latency; trade\u2011off depends on requirements.\n",
    "\n",
    "**Example of embedding an XGBoost model in a Faust agent:**\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "import numpy as np\n",
    "\n",
    "# Load model globally once\n",
    "model = xgb.Booster(model_file='nepse.model')\n",
    "# Load feature scaler\n",
    "scaler = joblib.load('scaler.pkl')\n",
    "\n",
    "# Inside the agent:\n",
    "features = np.array([[tick.price, tick.volume, tick.sma_5, tick.rsi]])\n",
    "dmatrix = xgb.DMatrix(features)\n",
    "pred = model.predict(dmatrix)  # returns probability\n",
    "```\n",
    "\n",
    "**Optimization tip:** Pre\u2011allocate arrays and reuse them to avoid allocation overhead.\n",
    "\n",
    "---\n",
    "\n",
    "## 42.5 State Management\n",
    "\n",
    "Many time\u2011series features require **state**\u2014for example, a 20\u2011period moving average needs the last 20 prices. In stream processing, state must be maintained across events and recovered after failures. Stream processors provide built\u2011in state stores (rocksDB, in\u2011memory) with exactly\u2011once semantics.\n",
    "\n",
    "### State in Faust\n",
    "\n",
    "Faust provides `Table` objects that are partitioned across instances and backed by a changelog topic in Kafka. They can store aggregations per key.\n",
    "\n",
    "```python\n",
    "# Table for storing last 5 prices per symbol\n",
    "price_buffer = app.Table('price_buffer', default=list, partitions=8)\n",
    "\n",
    "@app.agent(topic)\n",
    "async def process(ticks):\n",
    "    async for tick in ticks:\n",
    "        # Append price to buffer for this symbol\n",
    "        buf = price_buffer[tick.symbol]\n",
    "        buf.append(tick.price)\n",
    "        # Keep only last 5\n",
    "        if len(buf) > 5:\n",
    "            buf.pop(0)\n",
    "        price_buffer[tick.symbol] = buf\n",
    "\n",
    "        # Compute SMA_5 if buffer has enough values\n",
    "        if len(buf) == 5:\n",
    "            sma_5 = sum(buf) / 5\n",
    "            # Use sma_5 as a feature for prediction\n",
    "            # ... predict ...\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The `price_buffer` table keeps a list of recent prices per symbol. When a new tick arrives, we append the price and trim to the last 5. This state is fault\u2011tolerant because Faust persists every change to a Kafka changelog topic. If the instance fails, another can replay the changelog and reconstruct the buffer.\n",
    "\n",
    "### State in Flink\n",
    "\n",
    "Flink offers keyed state (ValueState, ListState, MapState) that is automatically checkpointed. For a rolling window, you can use a `ListState` or implement a custom `AggregateFunction`.\n",
    "\n",
    "---\n",
    "\n",
    "## 42.6 Backpressure Handling\n",
    "\n",
    "Backpressure occurs when the stream processor cannot keep up with the incoming data rate. Without handling, the system may crash or lose data. Modern stream processors provide mechanisms to deal with backpressure:\n",
    "\n",
    "- **Kafka Consumer Lag**: If the consumer falls behind, Kafka keeps messages; the lag (difference between latest offset and committed offset) grows. Monitoring lag is essential.\n",
    "- **Flow Control**: In Faust, the underlying `asyncio` queue can be bounded; if the agent cannot process fast enough, the producer will eventually block (depending on configuration).\n",
    "- **Dynamic Scaling**: Adding more consumers (partitions) can increase parallelism.\n",
    "- **Drop or Sample**: For non\u2011critical applications, you may sample events (e.g., keep only 10% of ticks) or drop late events.\n",
    "\n",
    "In the NEPSE scenario, tick rates are moderate (a few per second per symbol), so backpressure is unlikely. However, if we scale to many symbols or use complex models, we must plan.\n",
    "\n",
    "**Example of monitoring consumer lag with `kafka-consumer-groups` CLI:**\n",
    "\n",
    "```bash\n",
    "kafka-consumer-groups --bootstrap-server localhost:9092 \\\n",
    "  --group nepse-predictor --describe\n",
    "```\n",
    "\n",
    "This shows per\u2011partition lag.\n",
    "\n",
    "---\n",
    "\n",
    "## 42.7 Exactly\u2011Once Processing\n",
    "\n",
    "Exactly\u2011once semantics guarantee that each message is processed exactly one time, even in the face of failures. This is critical for financial applications where duplicate predictions or missed ticks could lead to incorrect trading decisions.\n",
    "\n",
    "Kafka introduced exactly\u2011once semantics via **transactions** and idempotent producers. A stream processor can participate in a transaction, committing offsets and output writes atomically.\n",
    "\n",
    "### Kafka Exactly\u2011Once in Python\n",
    "\n",
    "The `confluent_kafka` library supports idempotent producers and transactions, but using them correctly is complex. For simplicity, many applications settle for **at\u2011least\u2011once** and deduplicate downstream (idempotent writes to the sink).\n",
    "\n",
    "Flink and Spark Structured Streaming provide exactly\u2011once end\u2011to\u2011end when using appropriate sinks (e.g., Kafka, HDFS) and enabling checkpointing.\n",
    "\n",
    "**Example of enabling exactly\u2011once in Flink Kafka sink:**\n",
    "\n",
    "```java\n",
    "kafkaProducer.setWriteSemantic(WriteSemantic.EXACTLY_ONCE);\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 42.8 Monitoring Real\u2011Time Systems\n",
    "\n",
    "A real\u2011time prediction system must be continuously monitored to detect anomalies, performance degradation, or model drift. Key metrics include:\n",
    "\n",
    "- **Throughput**: Messages per second processed.\n",
    "- **Latency**: Time from ingestion to prediction output (p99).\n",
    "- **Error Rate**: Percentage of failed messages or predictions.\n",
    "- **Consumer Lag**: How far behind the consumer is from the latest message.\n",
    "- **Model Performance**: Drift in prediction distribution compared to training.\n",
    "\n",
    "### Tools\n",
    "\n",
    "- **Prometheus + Grafana**: Collect metrics from the application (using a client library like `prometheus_client`) and visualize dashboards.\n",
    "- **Kafka Monitoring**: Burrow, Kafka Lag Exporter.\n",
    "- **Logging**: Structured logs (JSON) to Elasticsearch, viewed in Kibana.\n",
    "\n",
    "**Example of exposing Prometheus metrics in a Faust app:**\n",
    "\n",
    "```python\n",
    "from prometheus_client import Counter, Histogram, start_http_server\n",
    "\n",
    "# Start Prometheus HTTP server on port 8000\n",
    "start_http_server(8000)\n",
    "\n",
    "PREDICTIONS = Counter('predictions_total', 'Total predictions', ['symbol'])\n",
    "LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency')\n",
    "\n",
    "@app.agent(topic)\n",
    "async def process(ticks):\n",
    "    async for tick in ticks:\n",
    "        with LATENCY.time():\n",
    "            # ... predict ...\n",
    "            PREDICTIONS.labels(symbol=tick.symbol).inc()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 42.9 Scaling Strategies\n",
    "\n",
    "As the number of symbols or the tick rate grows, the system must scale horizontally. Scaling a streaming application involves:\n",
    "\n",
    "- **Increasing Partitions**: More partitions in Kafka allow more consumers in a group. However, rebalancing can be heavy; plan for it.\n",
    "- **Parallelising the Stream Processor**: Faust and Flink automatically distribute work by key. Each instance handles a subset of keys.\n",
    "- **Separating Model Serving**: Offload inference to a dedicated service (e.g., using TensorFlow Serving or a custom gRPC server) that can scale independently.\n",
    "- **Auto\u2011Scaling**: Use Kubernetes Horizontal Pod Autoscaler based on CPU/memory or custom metrics (e.g., consumer lag).\n",
    "\n",
    "**Example of scaling a Faust application with multiple workers:**\n",
    "\n",
    "```bash\n",
    "faust -A myapp worker --web-port=6066 -l info &\n",
    "faust -A myapp worker --web-port=6067 -l info &\n",
    "```\n",
    "\n",
    "Faust uses Kafka\u2019s consumer groups to assign partitions among workers automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored the architecture and implementation of real\u2011time prediction systems using the NEPSE stock market as a motivating example. We covered:\n",
    "\n",
    "- The high\u2011level components of a real\u2011time pipeline: data source, ingestion, stream processing, model serving, and output sink.\n",
    "- Apache Kafka as a distributed streaming platform, with practical code for producing and consuming NEPSE tick data.\n",
    "- Stream processing with Apache Flink, Spark Structured Streaming, and custom Python frameworks like Faust.\n",
    "- Techniques for low\u2011latency inference, including embedding models and optimising feature computation.\n",
    "- State management to maintain rolling windows and other temporal aggregates.\n",
    "- Backpressure handling, exactly\u2011once processing, monitoring, and scaling strategies.\n",
    "\n",
    "Real\u2011time prediction systems are complex but essential for applications that require immediate responses. By combining the right tools and architectural patterns, we can build robust systems that deliver timely predictions for financial markets, IoT, or any domain where data flows continuously.\n",
    "\n",
    "In the next chapter, we will dive into **Batch Prediction Systems**, contrasting them with real\u2011time approaches and showing how to schedule large\u2011scale offline predictions for historical analysis and model retraining.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 42**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='41. batch_prediction_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='43. model_deployment_strategies.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}