{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 45: Model Drift Detection\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Distinguish between data drift, concept drift, and performance drift in the context of a time\u2011series prediction system\n",
    "- Choose appropriate statistical tests and machine learning methods to detect drift in your NEPSE prediction pipeline\n",
    "- Implement drift detection using Python libraries such as `evidently`, `scipy`, and `alibi-detect`\n",
    "- Monitor feature distributions and model residuals over time\n",
    "- Set up automated alerts when drift exceeds predefined thresholds\n",
    "- Quantify the magnitude and impact of drift\n",
    "- Design retraining triggers and mitigation strategies to keep your model accurate in changing market conditions\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "A machine learning model is a snapshot of the patterns present in the training data. When those patterns change after deployment\u2014because of evolving market dynamics, new regulations, or shifts in investor behaviour\u2014the model\u2019s predictions can become unreliable. This phenomenon is known as **model drift**.\n",
    "\n",
    "For the NEPSE stock prediction system, drift can manifest in many ways: the average trading volume might increase as more retail investors enter the market, the relationship between technical indicators and future returns might weaken, or a sudden political event could render historical patterns irrelevant. Detecting drift early allows you to retrain or adjust your model before it starts losing money for your users.\n",
    "\n",
    "In this chapter, we will explore the different types of drift, methods to detect them, and how to integrate drift detection into your production monitoring stack. We will use the NEPSE system as a running example, demonstrating how to monitor the feature distributions and prediction errors of your stock price model over time.\n",
    "\n",
    "---\n",
    "\n",
    "## 45.1 Types of Model Drift\n",
    "\n",
    "Model drift is usually categorised into three interrelated types:\n",
    "\n",
    "1. **Data Drift (Covariate Shift)**  \n",
    "   The statistical properties of the input features change over time. For example, the distribution of the `Volume` feature in NEPSE data might shift if the exchange introduces a new trading platform that increases liquidity.\n",
    "\n",
    "2. **Concept Drift**  \n",
    "   The relationship between the input features and the target variable changes. For instance, the same level of RSI (Relative Strength Index) might have indicated a buying opportunity during a bull market but becomes less reliable in a sideways market.\n",
    "\n",
    "3. **Performance Drift**  \n",
    "   The model\u2019s predictive performance (e.g., accuracy, RMSE) degrades over time. Performance drift is often a consequence of data or concept drift, but it can also be caused by changes in the data quality or the target definition (e.g., a change in the way \u201cclose price\u201d is calculated).\n",
    "\n",
    "In practice, these drifts are interrelated. A shift in feature distribution (data drift) can lead to concept drift if the model\u2019s decision boundary was learned on a different region of the feature space. Performance drift is the ultimate signal that something is wrong, but by the time you observe it, your users may already be affected. Therefore, we aim to detect data and concept drift early as leading indicators.\n",
    "\n",
    "---\n",
    "\n",
    "## 45.2 Detecting Data Drift\n",
    "\n",
    "Data drift detection compares the distribution of each feature (or a multivariate combination) between a **reference period** (usually the training data) and a **current window** (e.g., the last week of production data). If the difference is statistically significant, we flag a drift.\n",
    "\n",
    "### 45.2.1 Statistical Tests for Univariate Drift\n",
    "\n",
    "For numerical features like `Close`, `Volume`, or `RSI`, common statistical tests are:\n",
    "\n",
    "- **Kolmogorov\u2011Smirnov (KS) test**: Compares the empirical cumulative distribution functions of two samples. It is sensitive to differences in both location and shape.\n",
    "- **Population Stability Index (PSI)**: Measures how much a variable has shifted by binning the reference distribution and comparing the proportions in each bin.\n",
    "- **Wasserstein distance (Earth Mover\u2019s Distance)**: Measures the minimum amount of \u201cwork\u201d needed to transform one distribution into another.\n",
    "\n",
    "For categorical features (e.g., `Sector`), you can use the **chi\u2011square test**.\n",
    "\n",
    "**Example using SciPy to perform KS test on the `Volume` feature:**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "\n",
    "# Assume we have a reference dataset (training data)\n",
    "reference_volume = pd.read_csv('nepse_training.csv')['Volume']\n",
    "\n",
    "# Current production data (e.g., last 30 days)\n",
    "current_volume = pd.read_csv('nepse_production_last_30d.csv')['Volume']\n",
    "\n",
    "# Perform Kolmogorov-Smirnov test\n",
    "ks_stat, p_value = stats.ks_2samp(reference_volume, current_volume)\n",
    "\n",
    "print(f\"KS statistic: {ks_stat:.4f}, p-value: {p_value:.4f}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"\u26a0\ufe0f  Significant drift detected in Volume feature!\")\n",
    "else:\n",
    "    print(\"\u2705 Volume distribution stable.\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The KS test returns a statistic and a p\u2011value. The null hypothesis is that the two samples come from the same distribution. If the p\u2011value is below a threshold (commonly 0.05), we reject the null and conclude that drift has occurred. This test is sensitive to any difference, but it may be too sensitive for large sample sizes, where even trivial differences become significant. In practice, you may want to look at the magnitude of the KS statistic itself, not just the p\u2011value.\n",
    "\n",
    "### 45.2.2 Population Stability Index (PSI)\n",
    "\n",
    "PSI is widely used in credit scoring and finance. It bins the reference distribution into (typically 10) bins and compares the proportion of observations in each bin between reference and current.\n",
    "\n",
    "```python\n",
    "def calculate_psi(reference, current, bins=10):\n",
    "    \"\"\"\n",
    "    Calculate Population Stability Index.\n",
    "    \"\"\"\n",
    "    # Create bins based on reference percentiles\n",
    "    percentiles = np.percentile(reference, np.linspace(0, 100, bins+1))\n",
    "    # Clip current to avoid out-of-range values\n",
    "    current_clipped = np.clip(current, percentiles[0], percentiles[-1])\n",
    "    \n",
    "    # Count frequencies in each bin\n",
    "    ref_counts, _ = np.histogram(reference, bins=percentiles)\n",
    "    curr_counts, _ = np.histogram(current_clipped, bins=percentiles)\n",
    "    \n",
    "    # Convert to percentages\n",
    "    ref_pct = ref_counts / len(reference)\n",
    "    curr_pct = curr_counts / len(current)\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    ref_pct = np.where(ref_pct == 0, 0.0001, ref_pct)\n",
    "    curr_pct = np.where(curr_pct == 0, 0.0001, curr_pct)\n",
    "    \n",
    "    # Calculate PSI\n",
    "    psi = np.sum((curr_pct - ref_pct) * np.log(curr_pct / ref_pct))\n",
    "    return psi\n",
    "\n",
    "psi_value = calculate_psi(reference_volume, current_volume)\n",
    "print(f\"PSI: {psi_value:.4f}\")\n",
    "\n",
    "if psi_value > 0.25:\n",
    "    print(\"\u26a0\ufe0f  High drift (PSI > 0.25)\")\n",
    "elif psi_value > 0.1:\n",
    "    print(\"\u26a0\ufe0f  Moderate drift (PSI between 0.1 and 0.25)\")\n",
    "else:\n",
    "    print(\"\u2705 Low drift (PSI < 0.1)\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "PSI values below 0.1 indicate no significant change, 0.1\u20130.25 indicates moderate shift, and above 0.25 indicates a major shift. PSI is popular because it is bounded and interpretable, but it depends on the binning strategy.\n",
    "\n",
    "### 45.2.3 Multivariate Drift Detection\n",
    "\n",
    "Univariate tests may miss interactions: perhaps each feature individually looks stable, but their joint distribution has changed. For multivariate drift, you can use:\n",
    "\n",
    "- **Maximum Mean Discrepancy (MMD)**: A kernel\u2011based test that compares the distance between distributions in a reproducing kernel Hilbert space.\n",
    "- **Principal Component Analysis (PCA) on reference data, then compare the distribution of the reconstruction error**.\n",
    "- **Domain classifiers**: Train a classifier to distinguish between reference and current data; if it performs well, drift is present.\n",
    "\n",
    "**Example using `alibi-detect` for MMD:**\n",
    "\n",
    "```python\n",
    "from alibi_detect.cd import MMDDrift\n",
    "\n",
    "# Prepare reference data (training features)\n",
    "X_ref = np.load('nepse_training_features.npy')\n",
    "\n",
    "# Initialize detector\n",
    "cd = MMDDrift(X_ref, backend='pytorch', p_val=0.05)\n",
    "\n",
    "# On a batch of new data\n",
    "X_new = np.load('nepse_new_features.npy')\n",
    "preds = cd.predict(X_new)\n",
    "\n",
    "print(f\"Drift detected: {preds['data']['is_drift']}\")\n",
    "print(f\"p-value: {preds['data']['p_val']:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`alibi-detect` provides easy\u2011to\u2011use drift detectors. The MMDDrift uses a kernel method to test if the new sample comes from the same distribution as the reference. It returns a boolean and a p\u2011value.\n",
    "\n",
    "---\n",
    "\n",
    "## 45.3 Concept Drift Detection\n",
    "\n",
    "Concept drift detection requires monitoring the relationship between features and the target. This is more challenging because it often requires ground truth labels, which may be delayed (e.g., you only know the actual price change after the next day). Methods include:\n",
    "\n",
    "- **Monitoring prediction error** over time (if labels become available).\n",
    "- **Using a sliding window of recent data to retrain a simple model** and comparing its performance to the original.\n",
    "- **Statistical tests on model residuals**.\n",
    "\n",
    "### 45.3.1 Monitoring Residuals\n",
    "\n",
    "If you have a validation set from the training period, you can establish a baseline distribution of residuals (errors). In production, as labels arrive, you can compute the current residuals and test whether their distribution has shifted.\n",
    "\n",
    "```python\n",
    "# Baseline residuals from training validation set\n",
    "baseline_residuals = np.load('validation_residuals.npy')\n",
    "\n",
    "# Recent production residuals (e.g., last 30 predictions with known actuals)\n",
    "recent_residuals = get_production_residuals(30)\n",
    "\n",
    "# KS test on residuals\n",
    "ks_stat, p_value = stats.ks_2samp(baseline_residuals, recent_residuals)\n",
    "if p_value < 0.05:\n",
    "    print(\"\u26a0\ufe0f  Concept drift detected in residuals!\")\n",
    "```\n",
    "\n",
    "### 45.3.2 ADWIN (Adaptive Windowing)\n",
    "\n",
    "ADWIN is an algorithm that maintains a sliding window of data and grows or shrinks it based on detecting changes in the mean. It can be applied to any stream of values, such as prediction errors. The `river` library provides an implementation.\n",
    "\n",
    "```python\n",
    "from river import drift\n",
    "\n",
    "adwin = drift.ADWIN()\n",
    "\n",
    "for error in production_error_stream():\n",
    "    adwin.update(error)\n",
    "    if adwin.change_detected:\n",
    "        print(\"\u26a0\ufe0f  Drift detected at time\", adwin.n_detections)\n",
    "        break\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "ADWIN keeps a window and splits it into two sub\u2011windows whenever a change is suspected. If the means of the two sub\u2011windows differ significantly, it declares drift and shrinks the window to the recent sub\u2011window. This is useful for online detection.\n",
    "\n",
    "### 45.3.3 Using a Shadow Model\n",
    "\n",
    "A common approach in production is to deploy a \u201cshadow\u201d model that is periodically retrained on recent data. By comparing the shadow model\u2019s predictions with the production model\u2019s predictions (or with actuals when they arrive), you can detect when the production model is becoming stale.\n",
    "\n",
    "**Example workflow:**\n",
    "- Every week, retrain a model on the last 3 months of data.\n",
    "- Compare its performance on the last week\u2019s data with the production model.\n",
    "- If the shadow model outperforms production by a significant margin, trigger an alert.\n",
    "\n",
    "---\n",
    "\n",
    "## 45.4 Monitoring Drift in Production\n",
    "\n",
    "To operationalise drift detection, you need to:\n",
    "\n",
    "1. **Store feature distributions and predictions** over time (e.g., in a time\u2011series database like InfluxDB, or as logs).\n",
    "2. **Run periodic drift checks** (e.g., every hour, daily) on a sliding window.\n",
    "3. **Emit metrics** from these checks (e.g., drift p\u2011value per feature) to your monitoring system.\n",
    "\n",
    "### 45.4.1 Using Evidently for Automated Reports\n",
    "\n",
    "Evidently is an open\u2011source library specifically designed for monitoring ML models in production. It can generate HTML reports or produce JSON that can be ingested by monitoring tools.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from evidently import ColumnMapping\n",
    "from evidently.report import Report\n",
    "from evidently.metrics import ColumnDriftMetric, DatasetDriftMetric\n",
    "\n",
    "# Reference data (training)\n",
    "ref = pd.read_csv('nepse_training.csv')\n",
    "# Current data (recent production)\n",
    "cur = pd.read_csv('nepse_recent_prod.csv')\n",
    "\n",
    "column_mapping = ColumnMapping()\n",
    "column_mapping.target = 'Close'          # target column (optional)\n",
    "column_mapping.prediction = 'prediction' # prediction column (optional)\n",
    "column_mapping.numerical_features = ['Open', 'High', 'Low', 'Volume', 'RSI']\n",
    "\n",
    "# Create a drift report\n",
    "report = Report(metrics=[\n",
    "    ColumnDriftMetric(column_name='Volume'),\n",
    "    ColumnDriftMetric(column_name='RSI'),\n",
    "    DatasetDriftMetric()\n",
    "])\n",
    "report.run(reference_data=ref, current_data=cur, column_mapping=column_mapping)\n",
    "\n",
    "# Save as HTML or JSON\n",
    "report.save_html('drift_report.html')\n",
    "report.json()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Evidently computes drift for each specified column using statistical tests (configurable) and also provides an overall dataset drift score. The HTML report visualises the distributions side\u2011by\u2011side. The JSON output can be used to feed into a dashboard or to trigger alerts.\n",
    "\n",
    "### 45.4.2 Integrating with Prometheus\n",
    "\n",
    "You can run Evidently as a scheduled job and export the drift metrics to Prometheus using a custom exporter, or directly emit the drift p\u2011values as Prometheus gauges.\n",
    "\n",
    "```python\n",
    "from prometheus_client import Gauge, start_http_server\n",
    "import time\n",
    "\n",
    "drift_gauge = Gauge('feature_drift_p_value', 'Drift p-value per feature', ['feature'])\n",
    "\n",
    "while True:\n",
    "    # Compute drift for each feature\n",
    "    for feature in numerical_features:\n",
    "        p_value = compute_drift_p_value(ref[feature], cur[feature])\n",
    "        drift_gauge.labels(feature=feature).set(p_value)\n",
    "    time.sleep(3600)  # update every hour\n",
    "```\n",
    "\n",
    "Then you can create alerts when the p\u2011value drops below 0.05 for a certain duration.\n",
    "\n",
    "---\n",
    "\n",
    "## 45.5 Drift Quantification\n",
    "\n",
    "Detecting drift is only half the battle; you also need to understand its **impact**. Not all drift is harmful\u2014a feature may shift but the model\u2019s predictions remain accurate. You should quantify:\n",
    "\n",
    "- **Magnitude of shift**: PSI, KS statistic, or distance metric.\n",
    "- **Impact on predictions**: How much does the drift affect the model output? You can simulate by passing both reference and current data through the model and comparing the output distributions.\n",
    "\n",
    "**Example: Measuring shift in prediction distribution:**\n",
    "\n",
    "```python\n",
    "# Get predictions on reference data\n",
    "ref_preds = model.predict(ref_features)\n",
    "# Get predictions on current data\n",
    "cur_preds = model.predict(cur_features)\n",
    "\n",
    "# Compare prediction distributions\n",
    "psi_preds = calculate_psi(ref_preds, cur_preds)\n",
    "print(f\"PSI of predictions: {psi_preds:.4f}\")\n",
    "\n",
    "# If prediction distribution shifts, users will see different outputs.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 45.6 Adaptive Thresholds\n",
    "\n",
    "Setting static thresholds for drift detection (e.g., p\u2011value < 0.05) can lead to many false alarms in large\u2011scale systems. Consider using **adaptive thresholds** based on historical variability. For instance, you could compute the rolling mean and standard deviation of the drift metric and flag when it exceeds `mean + 3*std`.\n",
    "\n",
    "Another approach is to use **control charts** (e.g., CUSUM) commonly used in statistical process control.\n",
    "\n",
    "---\n",
    "\n",
    "## 45.7 Automated Alerts and Actions\n",
    "\n",
    "When drift is detected, you may want to trigger automated responses:\n",
    "\n",
    "- **Log an incident** in your monitoring system.\n",
    "- **Send a notification** (Slack, email, PagerDuty) to the data science team.\n",
    "- **Automatically retrain the model** if drift exceeds a critical threshold.\n",
    "- **Roll back to a previous model** if the drift is severe and a retrained model is not yet available.\n",
    "\n",
    "**Example of a Slack alert using a webhook:**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def send_slack_alert(message):\n",
    "    webhook_url = \"https://hooks.slack.com/services/...\"\n",
    "    payload = {\"text\": message}\n",
    "    requests.post(webhook_url, json=payload)\n",
    "\n",
    "if psi_value > 0.25:\n",
    "    send_slack_alert(f\"\u26a0\ufe0f High drift detected in Volume feature: PSI={psi_value:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 45.8 Drift Mitigation and Retraining Triggers\n",
    "\n",
    "Drift detection is only useful if it leads to action. The most common mitigation is **retraining** the model on recent data. However, retraining too often can be costly and may introduce instability. Define clear criteria:\n",
    "\n",
    "- **Time\u2011based retraining**: e.g., retrain every week regardless of drift (good for predictable seasonal patterns).\n",
    "- **Performance\u2011based trigger**: Retrain when prediction error exceeds a threshold.\n",
    "- **Drift\u2011based trigger**: Retrain when significant drift is detected in key features or residuals.\n",
    "\n",
    "For NEPSE, you might retrain your model every month, but also trigger an out\u2011of\u2011cycle retraining if a feature like `Volume` drifts beyond PSI > 0.2 for three consecutive days.\n",
    "\n",
    "**Example of a drift\u2011triggered retraining pipeline using Airflow:**\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "def check_drift_and_retrain():\n",
    "    # 1. Compute drift for all features\n",
    "    # 2. If any exceeds threshold, trigger retraining\n",
    "    if should_retrain():\n",
    "        # Trigger retraining DAG (e.g., via API or by setting a variable)\n",
    "        trigger_retraining()\n",
    "\n",
    "with DAG('drift_monitor', schedule_interval='@daily') as dag:\n",
    "    drift_task = PythonOperator(\n",
    "        task_id='check_drift',\n",
    "        python_callable=check_drift_and_retrain\n",
    "    )\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 45.9 Case Study: Monitoring NEPSE Drift\n",
    "\n",
    "Let\u2019s walk through a concrete example for the NEPSE prediction system.\n",
    "\n",
    "**Features:**  \n",
    "- `Close_Lag_1`, `Volume_Lag_1`, `SMA_20`, `RSI`, `Volume_Anomaly`\n",
    "\n",
    "**Target:**  \n",
    "- Binary: 1 if next\u2011day closing price is higher than today, else 0\n",
    "\n",
    "**Reference:** Training data from 2022\u20132023.  \n",
    "**Current:** Last 30 days of production data.\n",
    "\n",
    "We set up a daily job that:\n",
    "\n",
    "1. Loads the last 30 days of features from the feature store.\n",
    "2. For each feature, computes PSI against the reference distribution.\n",
    "3. For the target, if actuals are available, computes the prediction error rate and compares it to validation error rate using a binomial test.\n",
    "4. If any feature PSI > 0.25, or error rate increase > 5% with p < 0.05, sends a Slack alert and logs a metric in Prometheus.\n",
    "5. If the error rate increase persists for 3 days, automatically triggers a retraining pipeline.\n",
    "\n",
    "**Python snippet (simplified):**\n",
    "\n",
    "```python\n",
    "def monitor_drift():\n",
    "    # Load reference stats (precomputed from training)\n",
    "    ref_stats = load_reference_stats()\n",
    "    # Load current data\n",
    "    current = load_production_data(days=30)\n",
    "    \n",
    "    alerts = []\n",
    "    for feature in numerical_features:\n",
    "        psi = calculate_psi(ref_stats[feature]['values'], current[feature])\n",
    "        prom_metric.labels(feature=feature).set(psi)\n",
    "        if psi > 0.25:\n",
    "            alerts.append(f\"{feature} PSI={psi:.2f}\")\n",
    "    \n",
    "    # Check error rate if labels available\n",
    "    if 'actual' in current.columns:\n",
    "        error_rate = (current['prediction'] != current['actual']).mean()\n",
    "        # Compare to validation error rate (say 0.12)\n",
    "        if error_rate > 0.12 + 0.05:  # threshold\n",
    "            alerts.append(f\"Error rate increased to {error_rate:.2%}\")\n",
    "    \n",
    "    if alerts:\n",
    "        send_slack_alert(\"Drift detected:\\n\" + \"\\n\".join(alerts))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored the critical topic of model drift detection for time\u2011series prediction systems like the NEPSE stock predictor. We covered:\n",
    "\n",
    "- The three types of drift: data drift, concept drift, and performance drift.\n",
    "- Statistical methods for detecting univariate drift (KS test, PSI) and multivariate drift (MMD, domain classifiers).\n",
    "- Concept drift detection using residual monitoring, ADWIN, and shadow models.\n",
    "- Integrating drift detection into production with tools like Evidently and custom Prometheus exporters.\n",
    "- Quantifying drift magnitude and setting adaptive thresholds.\n",
    "- Automating alerts and retraining triggers to keep models fresh and accurate.\n",
    "\n",
    "By implementing drift detection, you ensure that your NEPSE prediction system remains reliable even as market conditions evolve. In the next chapter, we will discuss **Continuous Retraining Strategies**, diving deeper into how to automatically update your models in response to drift or on a schedule.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 45**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='44. monitoring_and_observability.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='46. continuous_retraining_strategies.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}