{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 92: Troubleshooting and Debugging\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand a systematic methodology for debugging issues in time\u2011series prediction systems.\n",
    "- Identify common failure modes in data pipelines, feature engineering, model training, and prediction services.\n",
    "- Use logging effectively to capture diagnostic information without overwhelming storage.\n",
    "- Leverage debugging tools (pdb, logging, tracebacks) to pinpoint errors in code.\n",
    "- Perform root cause analysis for data\u2011related issues, including missing values, outliers, and concept drift.\n",
    "- Debug distributed systems using distributed tracing and log aggregation.\n",
    "- Implement strategies for reproducing and fixing bugs in production.\n",
    "- Document troubleshooting steps and build a knowledge base for recurring issues.\n",
    "\n",
    "---\n",
    "\n",
    "## **92.1 Introduction to Troubleshooting and Debugging**\n",
    "\n",
    "No matter how well\u2011designed a system is, things will go wrong. The data feed may fail, a feature calculation may contain a bug, a model may produce nonsensical predictions, or an API may become unresponsive. The ability to systematically diagnose and resolve these issues is a critical skill for any engineer or data scientist working on a time\u2011series prediction system like the NEPSE stock predictor.\n",
    "\n",
    "Troubleshooting is the process of identifying the root cause of a problem. Debugging is the act of fixing it. This chapter will provide you with a structured approach to both, covering common pitfalls, tools, and techniques.\n",
    "\n",
    "We'll follow the lifecycle of a typical issue: from detection (often via monitoring alerts, as in Chapter 73), through investigation, to resolution and documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## **92.2 A Systematic Debugging Methodology**\n",
    "\n",
    "When faced with a problem, it's tempting to jump to conclusions and try random fixes. A systematic methodology saves time and ensures you actually fix the root cause.\n",
    "\n",
    "### **92.2.1 Step 1: Reproduce the Problem**\n",
    "Before you can fix a bug, you need to be able to reproduce it consistently. If the issue is intermittent, note the conditions under which it occurs (time, data, load, etc.). Create a minimal test case that isolates the problem.\n",
    "\n",
    "### **92.2.2 Step 2: Gather Information**\n",
    "Collect logs, metrics, and traces. Check monitoring dashboards. Talk to users or team members who observed the issue.\n",
    "\n",
    "### **92.2.3 Step 3: Formulate Hypotheses**\n",
    "Based on the information, hypothesise what might be wrong. For example: \u201cThe data ingestion service failed because the source CSV was in a different format.\u201d\n",
    "\n",
    "### **92.2.4 Step 4: Test Hypotheses**\n",
    "Design experiments to test each hypothesis. This could be as simple as checking the source file, or as complex as running a modified version of the code in a staging environment.\n",
    "\n",
    "### **92.2.5 Step 5: Identify Root Cause**\n",
    "Once a hypothesis is confirmed, you have found the root cause. Understand why it happened.\n",
    "\n",
    "### **92.2.6 Step 6: Implement a Fix**\n",
    "Apply a fix, ensuring it doesn't introduce new issues. Write tests to prevent regression.\n",
    "\n",
    "### **92.2.7 Step 7: Document**\n",
    "Record the issue, its root cause, and the solution. This helps others and prevents future occurrences.\n",
    "\n",
    "---\n",
    "\n",
    "## **92.3 Common Issues in Time\u2011Series Prediction Systems**\n",
    "\n",
    "Let's review common failure modes, using the NEPSE system as examples.\n",
    "\n",
    "### **92.3.1 Data Pipeline Issues**\n",
    "- **Missing data**: The daily CSV file is not available. Symptoms: stale predictions, alerts from monitoring.\n",
    "- **Schema changes**: The CSV format changes (e.g., new columns, different order). Symptoms: ingestion fails with column errors.\n",
    "- **Data quality**: Unexpected values (e.g., negative prices, zero volume). Symptoms: feature engineering produces NaNs, models output nonsense.\n",
    "\n",
    "### **92.3.2 Feature Engineering Issues**\n",
    "- **Look\u2011ahead bias**: Accidentally using future data in features (e.g., using today's high to predict today's close). Symptoms: model performs unrealistically well in backtesting but poorly in production.\n",
    "- **Incorrect calculations**: Bugs in technical indicator formulas (e.g., RSI, MACD). Symptoms: predictions deviate from expected patterns.\n",
    "- **Memory/performance**: Rolling window calculations become too slow or consume too much memory as data grows.\n",
    "\n",
    "### **92.3.3 Model Training Issues**\n",
    "- **Data leakage**: Training data includes information from the test period (e.g., scaling on full dataset). Symptoms: high validation accuracy but low real\u2011world performance.\n",
    "- **Overfitting**: Model performs well on training data but poorly on unseen data.\n",
    "- **Concept drift**: Model performance degrades over time due to changing market conditions.\n",
    "\n",
    "### **92.3.4 Prediction Service Issues**\n",
    "- **Latency spikes**: API responses become slow. Possible causes: model loading overhead, feature computation, database contention.\n",
    "- **Errors**: 500 Internal Server Error. Possible causes: unhandled exceptions, dependency failures.\n",
    "- **Incorrect predictions**: Model returns a value that is obviously wrong (e.g., negative price). Could be due to input feature issues or model corruption.\n",
    "\n",
    "### **92.3.5 Infrastructure Issues**\n",
    "- **Resource exhaustion**: CPU, memory, or disk full.\n",
    "- **Network problems**: Service cannot reach database or other services.\n",
    "- **Deployment failures**: Wrong model version deployed, configuration error.\n",
    "\n",
    "---\n",
    "\n",
    "## **92.4 Logging for Debugging**\n",
    "\n",
    "Logs are your primary window into the system's behaviour. Effective logging is essential for troubleshooting.\n",
    "\n",
    "### **92.4.1 What to Log**\n",
    "- **Errors and exceptions**: Always log the full stack trace.\n",
    "- **Warnings**: Unexpected but non\u2011fatal conditions (e.g., missing data, falling back to default).\n",
    "- **Key events**: Start/end of major processes (ingestion, training, prediction).\n",
    "- **Performance data**: Time taken for critical operations (can be sampled).\n",
    "- **Input/output summaries**: For debugging, log the first few rows of data or feature values (but beware of logging sensitive information).\n",
    "\n",
    "### **92.4.2 Log Levels**\n",
    "Use standard log levels: DEBUG, INFO, WARNING, ERROR, CRITICAL. In production, usually set to INFO or WARNING to avoid noise.\n",
    "\n",
    "### **92.4.3 Structured Logging**\n",
    "Log in JSON format to make it easy to parse and query. Use libraries like `python-json-logger`.\n",
    "\n",
    "```python\n",
    "import logging\n",
    "from pythonjsonlogger import jsonlogger\n",
    "\n",
    "logger = logging.getLogger()\n",
    "logHandler = logging.StreamHandler()\n",
    "formatter = jsonlogger.JsonFormatter('%(asctime)s %(levelname)s %(name)s %(message)s')\n",
    "logHandler.setFormatter(formatter)\n",
    "logger.addHandler(logHandler)\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"Data ingestion started\", extra={'symbol': 'NABIL', 'file': 'nepse_20230601.csv'})\n",
    "```\n",
    "\n",
    "This produces logs like:\n",
    "```json\n",
    "{\"asctime\": \"2024-06-01 10:00:00\", \"levelname\": \"INFO\", \"name\": \"__main__\", \"message\": \"Data ingestion started\", \"symbol\": \"NABIL\", \"file\": \"nepse_20230601.csv\"}\n",
    "```\n",
    "\n",
    "### **92.4.4 Centralised Logging**\n",
    "In a distributed system, logs from different services must be aggregated. Use tools like **ELK Stack** (Elasticsearch, Logstash, Kibana), **Loki**, or cloud services (AWS CloudWatch, GCP Logging). This allows you to search across all logs.\n",
    "\n",
    "### **92.4.5 Log Rotation**\n",
    "To prevent disks from filling up, configure log rotation (e.g., using `logrotate` or the logging module's `RotatingFileHandler`).\n",
    "\n",
    "---\n",
    "\n",
    "## **92.5 Debugging Tools**\n",
    "\n",
    "### **92.5.1 Python Debugger (pdb)**\n",
    "`pdb` allows you to step through code interactively. Insert `breakpoint()` (Python 3.7+) at the point you want to inspect.\n",
    "\n",
    "```python\n",
    "def compute_rsi(prices):\n",
    "    breakpoint()  # start debugger here\n",
    "    # ...\n",
    "```\n",
    "\n",
    "When the breakpoint is hit, you can inspect variables, step through lines, and evaluate expressions.\n",
    "\n",
    "Common pdb commands:\n",
    "- `n` (next): execute next line\n",
    "- `s` (step): step into function call\n",
    "- `c` (continue): continue until next breakpoint\n",
    "- `p variable` (print): print variable value\n",
    "- `q` (quit): exit debugger\n",
    "\n",
    "### **92.5.2 Logging as a Debugging Tool**\n",
    "Sometimes you can't attach a debugger (e.g., in production). Use logging strategically to output variable values at key points.\n",
    "\n",
    "```python\n",
    "logger.debug(f\"Processing symbol {symbol}, close price: {close_price}\")\n",
    "```\n",
    "\n",
    "### **92.5.3 Assertions**\n",
    "Use `assert` to catch invalid states early. Assertions can be disabled in production with the `-O` flag, but they are valuable during development.\n",
    "\n",
    "```python\n",
    "assert len(prices) > 0, \"prices must not be empty\"\n",
    "assert close_price > 0, f\"Close price {close_price} must be positive\"\n",
    "```\n",
    "\n",
    "### **92.5.4 Post\u2011mortem Debugging**\n",
    "If a program crashes, you can launch a debugger after the fact using `pdb.pm()`.\n",
    "\n",
    "```python\n",
    "import pdb\n",
    "\n",
    "try:\n",
    "    run()\n",
    "except Exception:\n",
    "    pdb.pm()  # start debugger at point of exception\n",
    "```\n",
    "\n",
    "### **92.5.5 Interactive Debugging with IDEs**\n",
    "Most IDEs (VS Code, PyCharm) have excellent built\u2011in debuggers with graphical interfaces. Learn to use breakpoints, watches, and variable inspection.\n",
    "\n",
    "---\n",
    "\n",
    "## **92.6 Debugging Data Issues**\n",
    "\n",
    "Data problems are common in time\u2011series systems. Here are techniques to diagnose them.\n",
    "\n",
    "### **92.6.1 Visual Inspection**\n",
    "Plot the data. A simple line chart can reveal missing periods, outliers, or unexpected patterns.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "df['close'].plot()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **92.6.2 Summary Statistics**\n",
    "Compute basic statistics and compare with expectations.\n",
    "\n",
    "```python\n",
    "print(df['close'].describe())\n",
    "```\n",
    "\n",
    "If the minimum is negative or the maximum is implausible, you've found an issue.\n",
    "\n",
    "### **92.6.3 Check for Missing Values**\n",
    "```python\n",
    "print(df.isnull().sum())\n",
    "```\n",
    "\n",
    "If critical columns have missing values, investigate why.\n",
    "\n",
    "### **92.6.4 Validate Against a Known Source**\n",
    "If you suspect the data is incorrect, compare a few samples with a trusted source (e.g., another data provider).\n",
    "\n",
    "### **92.6.5 Data Drift Detection**\n",
    "Use statistical tests (Kolmogorov\u2011Smirnov, chi\u2011square) to compare current data distribution with a reference period. This can reveal changes in data generation.\n",
    "\n",
    "```python\n",
    "from scipy.stats import ks_2samp\n",
    "\n",
    "ks_stat, p_value = ks_2samp(reference_data['close'], current_data['close'])\n",
    "if p_value < 0.05:\n",
    "    print(\"Significant drift detected\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **92.7 Debugging Model Issues**\n",
    "\n",
    "### **92.7.1 Performance Degradation**\n",
    "If model performance drops, first check if the data has drifted (as above). Then look at:\n",
    "\n",
    "- **Feature importance**: Has the importance of key features changed? Use SHAP or permutation importance on recent data.\n",
    "- **Residual analysis**: Plot residuals over time. Are errors increasing? Is there a pattern (e.g., bias on high\u2011volatility days)?\n",
    "\n",
    "### **92.7.2 Overfitting**\n",
    "- Compare training and validation errors. If training error is much lower, overfitting is likely.\n",
    "- Check feature importance: are there features that should not be predictive (e.g., row number)?\n",
    "\n",
    "### **92.7.3 Data Leakage**\n",
    "- Review feature engineering code: ensure all transformations use only past information (`shift()` correctly).\n",
    "- Check that train/test split respects time order.\n",
    "\n",
    "### **92.7.4 Model Serving Issues**\n",
    "If the model returns bad predictions in production but worked in testing:\n",
    "\n",
    "- Verify that the input features are being computed identically. Compare a sample input from production with a sample from training.\n",
    "- Check that the correct model version is loaded.\n",
    "- Look for feature scaling mismatches (e.g., scaler fitted on training data but not used in production).\n",
    "\n",
    "---\n",
    "\n",
    "## **92.8 Debugging Distributed Systems**\n",
    "\n",
    "When issues span multiple services, traditional debugging becomes harder.\n",
    "\n",
    "### **92.8.1 Distributed Tracing**\n",
    "Tools like **Jaeger** or **Zipkin** trace a request as it travels through services. They show the path, timing, and any errors. In FastAPI, you can instrument with OpenTelemetry as shown in Chapter 81.\n",
    "\n",
    "### **92.8.2 Centralised Log Aggregation**\n",
    "Use the ELK stack or Loki to search logs from all services in one place. For example, to find all logs related to a specific prediction request, include a request ID in every log entry.\n",
    "\n",
    "```python\n",
    "import uuid\n",
    "\n",
    "request_id = str(uuid.uuid4())\n",
    "logger.info(\"Prediction request\", extra={'request_id': request_id, 'symbol': symbol})\n",
    "```\n",
    "\n",
    "Then you can search for that request_id across all services.\n",
    "\n",
    "### **92.8.3 Health Checks and Readiness Probes**\n",
    "In a containerised environment, implement health check endpoints. Kubernetes uses these to restart unhealthy containers.\n",
    "\n",
    "```python\n",
    "@app.get(\"/health\")\n",
    "def health():\n",
    "    return {\"status\": \"healthy\"}\n",
    "```\n",
    "\n",
    "### **92.8.4 Chaos Engineering**\n",
    "Intentionally inject failures (e.g., kill a service, add latency) to see how the system behaves. This builds confidence in your debugging tools and resilience.\n",
    "\n",
    "---\n",
    "\n",
    "## **92.9 Reproducing and Fixing Bugs**\n",
    "\n",
    "Once you've identified the root cause, you need to fix it.\n",
    "\n",
    "### **92.9.1 Write a Regression Test**\n",
    "Before fixing, write a test that reproduces the bug. This ensures you understand it and prevents it from recurring.\n",
    "\n",
    "```python\n",
    "def test_rsi_calculation_edge_case():\n",
    "    prices = pd.Series([100, 100, 100])  # constant prices\n",
    "    expected = pd.Series([np.nan, np.nan, 50.0])\n",
    "    result = calculate_rsi(prices, period=2)\n",
    "    pd.testing.assert_series_equal(result, expected)\n",
    "```\n",
    "\n",
    "### **92.9.2 Apply the Fix**\n",
    "Make the minimal change necessary to pass the test.\n",
    "\n",
    "### **92.9.3 Run All Tests**\n",
    "Ensure your fix doesn't break anything else.\n",
    "\n",
    "### **92.9.4 Deploy Carefully**\n",
    "Deploy to staging first, verify, then deploy to production with a rollout strategy (e.g., canary).\n",
    "\n",
    "---\n",
    "\n",
    "## **92.10 Building a Troubleshooting Knowledge Base**\n",
    "\n",
    "Over time, you will encounter recurring issues. Document them in a knowledge base (e.g., a wiki). For each issue, note:\n",
    "\n",
    "- **Symptoms**: What alerts or user reports indicate this issue?\n",
    "- **Possible causes**: List common reasons.\n",
    "- **Diagnostic steps**: Commands to run, logs to check.\n",
    "- **Resolution**: How to fix it.\n",
    "- **Prevention**: How to avoid it in the future.\n",
    "\n",
    "This becomes an invaluable resource for on\u2011call engineers and new team members.\n",
    "\n",
    "**Example entry**:\n",
    "\n",
    "```\n",
    "# Issue: Data ingestion fails with \"Column mismatch\"\n",
    "\n",
    "## Symptoms\n",
    "- Alert: \"Data ingestion failed\"\n",
    "- Log shows: \"Expected columns ['Symbol','Date','Open',...] but got ...\"\n",
    "\n",
    "## Possible Causes\n",
    "1. Source CSV format changed (new columns, different order).\n",
    "2. Delimiter changed (e.g., from comma to semicolon).\n",
    "3. File is corrupted or empty.\n",
    "\n",
    "## Diagnostic Steps\n",
    "1. Check the source file: `aws s3 cp s3://nepse-raw/latest.csv - | head`\n",
    "2. Compare columns with expected schema in `config/schema.yaml`.\n",
    "3. Check file size: `aws s3 ls s3://nepse-raw/latest.csv`\n",
    "\n",
    "## Resolution\n",
    "- If format changed, update the ingestion code or schema.\n",
    "- If file corrupted, contact data provider for a replacement.\n",
    "\n",
    "## Prevention\n",
    "- Implement schema validation at ingestion time.\n",
    "- Alert on schema changes (e.g., using Great Expectations).\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored the art and science of troubleshooting and debugging in a time\u2011series prediction system like NEPSE. We covered:\n",
    "\n",
    "- A systematic methodology for diagnosing issues.\n",
    "- Common failure modes in data pipelines, feature engineering, models, and infrastructure.\n",
    "- Effective logging practices, including structured logging and centralisation.\n",
    "- Debugging tools: pdb, logging, assertions, and IDE debuggers.\n",
    "- Techniques for debugging data\u2011related issues (visualisation, statistics, drift detection).\n",
    "- Approaches for model performance degradation and data leakage.\n",
    "- Debugging distributed systems with tracing and log aggregation.\n",
    "- The importance of regression tests and careful deployment.\n",
    "- Building a knowledge base to capture recurring issues.\n",
    "\n",
    "Troubleshooting is a skill that improves with experience. By applying these techniques and documenting your findings, you'll become more effective at keeping your prediction system reliable and trustworthy.\n",
    "\n",
    "In the next chapter, we will explore **Emerging Technologies and Future Trends** in time\u2011series prediction, including foundation models and large language models for time series.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 92**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='91. performance_optimization.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../13. emerging_technologies_and_future_trends/93. foundation_models_for_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}