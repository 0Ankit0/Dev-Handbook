{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 90: Quality Assurance\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Define a comprehensive quality assurance (QA) strategy for a time‑series prediction system.\n",
    "- Understand the role of different types of testing: unit, integration, system, and acceptance.\n",
    "- Implement automated tests for data pipelines, feature engineering, model training, and prediction services.\n",
    "- Conduct manual testing to catch issues that automated tests might miss.\n",
    "- Measure and optimise system performance (latency, throughput, resource usage).\n",
    "- Identify and mitigate security vulnerabilities in ML systems.\n",
    "- Validate model behaviour using techniques like cross‑validation, backtesting, and shadow deployment.\n",
    "- Ensure data quality through schema validation, anomaly detection, and drift monitoring.\n",
    "- Integrate QA into the CI/CD pipeline to catch issues early.\n",
    "- Apply best practices for maintaining high quality in production ML systems.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.1 Introduction to Quality Assurance in ML Systems**\n",
    "\n",
    "Quality assurance (QA) in machine learning systems extends beyond traditional software testing. While we still need to verify that code behaves as expected, we must also validate data, models, and their interactions. A bug in a traditional software system might cause a crash or incorrect output; in an ML system, it might silently degrade predictions, leading to poor business decisions.\n",
    "\n",
    "For the NEPSE stock prediction system, quality issues could manifest as:\n",
    "\n",
    "- Data pipeline failures (missing or incorrect data) causing stale predictions.\n",
    "- Feature engineering bugs (e.g., wrong lag calculation) leading to systematically biased predictions.\n",
    "- Model training issues (data leakage, overfitting) resulting in poor generalisation.\n",
    "- Deployment problems (wrong model version, resource exhaustion) causing service outages.\n",
    "- Security vulnerabilities (unauthorised access, model theft) compromising the system.\n",
    "\n",
    "A robust QA strategy addresses all these aspects. This chapter will guide you through the key components, from automated testing to manual validation, performance testing, and security audits.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.2 QA Strategy**\n",
    "\n",
    "A QA strategy defines the overall approach to ensuring quality. It should be tailored to the system's risk profile, complexity, and regulatory requirements. For the NEPSE system, a typical strategy might include:\n",
    "\n",
    "- **Pre‑commit checks**: Linting, type checking, and unit tests run locally or in CI before code is merged.\n",
    "- **Continuous integration**: On every push, run a suite of automated tests: unit, integration, and data validation.\n",
    "- **Staging environment**: Deploy to a staging environment that mirrors production for manual testing and integration tests.\n",
    "- **Pre‑production validation**: Run model performance tests on hold‑out data, shadow deployments, and canary releases.\n",
    "- **Production monitoring**: Continuously monitor data drift, model performance, and system health (as in Chapter 73).\n",
    "- **Regular audits**: Periodically review code, data, and models for quality and security.\n",
    "\n",
    "This multi‑layered approach catches issues at the earliest possible stage and provides confidence when deploying changes.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.3 Testing Frameworks**\n",
    "\n",
    "Choosing the right tools is essential. For Python‑based systems like NEPSE, common frameworks include:\n",
    "\n",
    "- **pytest**: The most popular testing framework. Supports fixtures, parameterised tests, and plugins.\n",
    "- **unittest**: Built‑in, but less flexible than pytest.\n",
    "- **nose2**: Another alternative, but pytest is recommended.\n",
    "\n",
    "For data validation, we can use:\n",
    "\n",
    "- **Great Expectations**: Declarative validation of data quality.\n",
    "- **Pandera**: Schema validation for pandas DataFrames.\n",
    "- **Deequ** (if using Spark): Unit tests for data.\n",
    "\n",
    "For model testing, we might use:\n",
    "\n",
    "- **MLflow** for tracking experiments and comparing model performance.\n",
    "- **Custom scripts** to compute metrics on hold‑out data.\n",
    "- **SHAP** or **LIME** for interpretability checks.\n",
    "\n",
    "For performance testing:\n",
    "\n",
    "- **locust** or **k6** for load testing APIs.\n",
    "- **pytest‑benchmark** for micro‑benchmarks.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.4 Automated Testing**\n",
    "\n",
    "Automated tests are the foundation of QA. They should run frequently and provide fast feedback.\n",
    "\n",
    "### **90.4.1 Unit Tests**\n",
    "\n",
    "Unit tests verify individual functions or classes in isolation. For the NEPSE system, examples include:\n",
    "\n",
    "- Test that `compute_daily_return` correctly calculates percentage change.\n",
    "- Test that `calculate_rsi` returns expected values for a known input.\n",
    "- Test that data validation functions catch missing columns.\n",
    "\n",
    "**Example using pytest:**\n",
    "\n",
    "```python\n",
    "# test_features.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from feature_engineering import compute_daily_return, calculate_rsi\n",
    "\n",
    "def test_compute_daily_return():\n",
    "    df = pd.DataFrame({'Close': [100, 105, 103]})\n",
    "    result = compute_daily_return(df)\n",
    "    expected = [np.nan, 5.0, -1.9047619]\n",
    "    pd.testing.assert_series_equal(\n",
    "        result['daily_return'],\n",
    "        pd.Series(expected, name='daily_return'),\n",
    "        check_less_precise=True\n",
    "    )\n",
    "\n",
    "def test_rsi_edge_cases():\n",
    "    # Test with constant prices\n",
    "    prices = pd.Series([100]*20)\n",
    "    rsi = calculate_rsi(prices)\n",
    "    assert rsi.iloc[-1] == 50.0  # RSI should be 50 for constant prices\n",
    "    \n",
    "    # Test with insufficient data\n",
    "    with pytest.raises(ValueError):\n",
    "        calculate_rsi(pd.Series([100, 101]), period=14)\n",
    "```\n",
    "\n",
    "### **90.4.2 Integration Tests**\n",
    "\n",
    "Integration tests verify that components work together correctly. For example, test that the feature engineering pipeline produces the expected output when given a sample raw DataFrame, or that the prediction service returns a valid response when called with a known input.\n",
    "\n",
    "**Example:**\n",
    "\n",
    "```python\n",
    "# test_integration.py\n",
    "import pytest\n",
    "from fastapi.testclient import TestClient\n",
    "from prediction_service import app\n",
    "from feature_store import FeatureStore\n",
    "\n",
    "client = TestClient(app)\n",
    "\n",
    "def test_prediction_endpoint(monkeypatch):\n",
    "    # Mock the feature store to return known features\n",
    "    def mock_get_features(symbol, date):\n",
    "        return pd.DataFrame([{'feature1': 1.0, 'feature2': 2.0}])\n",
    "    \n",
    "    monkeypatch.setattr(FeatureStore, 'get_features', mock_get_features)\n",
    "    \n",
    "    response = client.post(\"/predict\", json={\"symbol\": \"NABIL\", \"date\": \"2023-06-01\"})\n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert \"predicted_close\" in data\n",
    "    assert isinstance(data[\"predicted_close\"], float)\n",
    "```\n",
    "\n",
    "### **90.4.3 Data Validation Tests**\n",
    "\n",
    "Automated data validation ensures that incoming data meets expectations. This can be part of the ingestion pipeline.\n",
    "\n",
    "**Example with Great Expectations:**\n",
    "\n",
    "```python\n",
    "import great_expectations as ge\n",
    "\n",
    "def test_raw_data_quality(df):\n",
    "    ge_df = ge.from_pandas(df)\n",
    "    # Expect no nulls in critical columns\n",
    "    ge_df.expect_column_values_to_not_be_null('Close')\n",
    "    ge_df.expect_column_values_to_not_be_null('Volume')\n",
    "    # Expect Close prices to be positive\n",
    "    ge_df.expect_column_values_to_be_between('Close', 0, None)\n",
    "    # Expect Volume to be non‑negative\n",
    "    ge_df.expect_column_values_to_be_between('Volume', 0, None)\n",
    "    # Expect no duplicate symbol‑date pairs\n",
    "    ge_df.expect_compound_columns_to_be_unique(['Symbol', 'Date'])\n",
    "    \n",
    "    results = ge_df.validate()\n",
    "    assert results['success'], results\n",
    "```\n",
    "\n",
    "### **90.4.4 Model Validation Tests**\n",
    "\n",
    "Model validation tests ensure that a newly trained model meets minimum performance criteria before deployment.\n",
    "\n",
    "```python\n",
    "def test_new_model_performance():\n",
    "    # Load hold‑out test data\n",
    "    X_test, y_test = load_test_data()\n",
    "    \n",
    "    # Load candidate model\n",
    "    model = load_model('candidate.pkl')\n",
    "    \n",
    "    # Compute metrics\n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    # Compare to baseline (e.g., previous model)\n",
    "    baseline_mae = 12.5\n",
    "    assert mae <= baseline_mae * 1.05, f\"MAE {mae} exceeds baseline by >5%\"\n",
    "    \n",
    "    # Also check performance on key segments (e.g., high volatility)\n",
    "    high_vol_idx = X_test['volatility'] > X_test['volatility'].quantile(0.9)\n",
    "    if high_vol_idx.any():\n",
    "        mae_high = mean_absolute_error(y_test[high_vol_idx], y_pred[high_vol_idx])\n",
    "        assert mae_high <= baseline_mae * 1.2, \"Performance on high vol degraded too much\"\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **90.5 Manual Testing**\n",
    "\n",
    "Automated tests cannot catch everything. Manual testing by a human (e.g., a data scientist or domain expert) is still valuable.\n",
    "\n",
    "### **90.5.1 Exploratory Testing**\n",
    "A tester interacts with the system without a script, trying to find unexpected behaviour. For the NEPSE system, they might:\n",
    "\n",
    "- Request predictions for unusual symbols or dates.\n",
    "- Examine feature distributions for anomalies.\n",
    "- Plot predictions against actuals to spot systematic biases.\n",
    "\n",
    "### **90.5.2 Usability Testing**\n",
    "If the system has a user interface (e.g., a dashboard), test that it is intuitive and displays information correctly.\n",
    "\n",
    "### **90.5.3 Acceptance Testing**\n",
    "Before a release, stakeholders (e.g., a trader) may test the system against acceptance criteria defined in the user stories.\n",
    "\n",
    "Manual testing should be guided by test plans and documented. However, it's not scalable for frequent releases; that's why we automate as much as possible.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.6 Performance Testing**\n",
    "\n",
    "Performance testing ensures the system meets its latency, throughput, and resource utilisation requirements.\n",
    "\n",
    "### **90.6.1 Load Testing**\n",
    "Simulate many concurrent users to see how the system behaves. For the prediction API, we can use **locust**.\n",
    "\n",
    "```python\n",
    "# locustfile.py\n",
    "from locust import HttpUser, task, between\n",
    "\n",
    "class PredictionUser(HttpUser):\n",
    "    wait_time = between(1, 3)\n",
    "    \n",
    "    @task\n",
    "    def predict(self):\n",
    "        self.client.post(\"/predict\", json={\n",
    "            \"symbol\": \"NABIL\",\n",
    "            \"date\": \"2023-06-01\"\n",
    "        })\n",
    "```\n",
    "\n",
    "Run with `locust -f locustfile.py` and observe response times and error rates.\n",
    "\n",
    "### **90.6.2 Endurance Testing**\n",
    "Run a moderate load over a long period (e.g., hours) to detect memory leaks or gradual performance degradation.\n",
    "\n",
    "### **90.6.3 Stress Testing**\n",
    "Push the system beyond expected limits to see where it breaks. This helps identify scaling bottlenecks.\n",
    "\n",
    "### **90.6.4 Benchmarking**\n",
    "Measure the performance of critical functions (e.g., feature computation) to ensure they are efficient. Use `pytest-benchmark`.\n",
    "\n",
    "```python\n",
    "def test_feature_computation_speed(benchmark):\n",
    "    df = load_large_dataframe()\n",
    "    result = benchmark(compute_all_features, df)\n",
    "    assert result is not None\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **90.7 Security Testing**\n",
    "\n",
    "Security is often overlooked in ML systems, but it's critical, especially in finance.\n",
    "\n",
    "### **90.7.1 Authentication and Authorization**\n",
    "Test that API endpoints require valid authentication and that users cannot access data they shouldn't. Use tools like **OWASP ZAP** or **Burp Suite** to scan for common vulnerabilities.\n",
    "\n",
    "### **90.7.2 Data Privacy**\n",
    "Ensure that sensitive data (e.g., API keys, database credentials) is not exposed in logs or error messages. Test that the system respects data minimisation and anonymisation.\n",
    "\n",
    "### **90.7.3 Model Theft Prevention**\n",
    "If the model is a valuable asset, protect it. Test that the model cannot be easily extracted via repeated API calls (e.g., by limiting rate, adding noise). Use techniques like model watermarking.\n",
    "\n",
    "### **90.7.4 Adversarial Attacks**\n",
    "Test the model's robustness against adversarial inputs. For example, small perturbations to features should not cause large changes in predictions (if that is a requirement). Libraries like **ART** (Adversarial Robustness Toolbox) can help.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.8 Model Testing**\n",
    "\n",
    "Model testing goes beyond performance metrics to ensure the model behaves as expected in various scenarios.\n",
    "\n",
    "### **90.8.1 Backtesting**\n",
    "As we did in Chapter 74, simulate how the model would have performed historically. This is a form of system testing for the model.\n",
    "\n",
    "### **90.8.2 Sensitivity Analysis**\n",
    "Test how predictions change when input features are slightly varied. This can reveal if the model relies too heavily on a single feature.\n",
    "\n",
    "```python\n",
    "def test_sensitivity(model, X_sample):\n",
    "    base_pred = model.predict(X_sample)\n",
    "    for col in X_sample.columns:\n",
    "        X_perturbed = X_sample.copy()\n",
    "        X_perturbed[col] *= 1.01  # 1% increase\n",
    "        new_pred = model.predict(X_perturbed)\n",
    "        change = abs(new_pred - base_pred) / base_pred\n",
    "        assert change < 0.1, f\"Too sensitive to {col}\"\n",
    "```\n",
    "\n",
    "### **90.8.3 Fairness Testing**\n",
    "If the model could impact people (e.g., loan approval), test for bias across demographic groups. For NEPSE, this is less relevant, but still good practice.\n",
    "\n",
    "### **90.8.4 Shadow Deployment**\n",
    "Before replacing a production model, run the new model in parallel (shadow mode) and compare its predictions to the current model. This provides real‑world validation without risk.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.9 Data Testing**\n",
    "\n",
    "Data quality is the foundation of ML. Tests should be applied both to static datasets and streaming data.\n",
    "\n",
    "### **90.9.1 Schema Validation**\n",
    "Ensure incoming data matches the expected schema (column names, types). Use Pandera or Great Expectations.\n",
    "\n",
    "```python\n",
    "import pandera as pa\n",
    "\n",
    "schema = pa.DataFrameSchema({\n",
    "    \"Symbol\": pa.Column(str),\n",
    "    \"Date\": pa.Column(pa.DateTime),\n",
    "    \"Open\": pa.Column(float, pa.Check.greater_than(0)),\n",
    "    \"High\": pa.Column(float, pa.Check.greater_than(0)),\n",
    "    \"Low\": pa.Column(float, pa.Check.greater_than(0)),\n",
    "    \"Close\": pa.Column(float, pa.Check.greater_than(0)),\n",
    "    \"Volume\": pa.Column(int, pa.Check.greater_than_or_equal_to(0)),\n",
    "})\n",
    "\n",
    "def test_data_schema(df):\n",
    "    validated_df = schema.validate(df)\n",
    "    assert validated_df is not None\n",
    "```\n",
    "\n",
    "### **90.9.2 Statistical Tests**\n",
    "Check that distributions of key features remain stable over time (drift detection). Use statistical tests (e.g., Kolmogorov‑Smirnov) as in Chapter 77.\n",
    "\n",
    "### **90.9.3 Anomaly Detection**\n",
    "Monitor for unexpected values (e.g., price = 0, volume negative). This can be integrated into the ingestion pipeline.\n",
    "\n",
    "### **90.9.4 Freshness**\n",
    "Ensure data is arriving on time. In the NEPSE system, if the daily CSV is not available by a certain time, an alert should fire.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.10 Integration Testing**\n",
    "\n",
    "Integration tests verify that all components work together in a realistic environment. This is especially important in a microservices architecture (Chapter 81).\n",
    "\n",
    "### **90.10.1 End‑to‑End Tests**\n",
    "Simulate a complete user scenario: from data ingestion to prediction. For example:\n",
    "\n",
    "1. Trigger ingestion of a sample CSV.\n",
    "2. Wait for feature computation.\n",
    "3. Call prediction API and verify the result.\n",
    "4. Check that logs and metrics are generated.\n",
    "\n",
    "These tests can be run in a staging environment that mirrors production.\n",
    "\n",
    "### **90.10.2 Contract Testing**\n",
    "Ensure that services communicate according to their API contracts. Tools like **Pact** can be used to test consumer‑driven contracts between services.\n",
    "\n",
    "### **90.10.3 Chaos Engineering**\n",
    "Intentionally introduce failures (e.g., kill a service, delay network) to see if the system degrades gracefully. This builds confidence in resilience.\n",
    "\n",
    "---\n",
    "\n",
    "## **90.11 Best Practices**\n",
    "\n",
    "1. **Shift left**: Test as early as possible in the development cycle.\n",
    "2. **Automate ruthlessly**: Any test that can be automated should be.\n",
    "3. **Maintain a healthy test suite**: Keep tests fast, reliable, and independent.\n",
    "4. **Use realistic data in tests**: But anonymise it if needed.\n",
    "5. **Monitor test coverage**: Aim for high coverage, but focus on critical paths.\n",
    "6. **Treat data as code**: Version data, test its quality, and document it.\n",
    "7. **Involve domain experts**: They can spot issues that automated tests miss.\n",
    "8. **Document test plans and results**: Especially for manual and acceptance testing.\n",
    "9. **Continuously improve**: Review test failures and update tests to catch similar issues in the future.\n",
    "10. **Balance effort and risk**: Not everything needs 100% coverage; focus on areas where failures would have the highest impact.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored the multifaceted world of quality assurance for a time‑series prediction system like NEPSE. We covered:\n",
    "\n",
    "- A layered QA strategy from pre‑commit checks to production monitoring.\n",
    "- Automated testing with unit, integration, data validation, and model tests.\n",
    "- Manual testing for exploratory and acceptance purposes.\n",
    "- Performance testing to ensure the system meets SLAs.\n",
    "- Security testing to protect data and models.\n",
    "- Model‑specific testing including backtesting and sensitivity analysis.\n",
    "- Data testing for schema, drift, and freshness.\n",
    "- Integration testing in a microservices environment.\n",
    "- Best practices to embed quality into the development process.\n",
    "\n",
    "Quality assurance is not a one‑time activity but a continuous discipline. By integrating these practices into your CI/CD pipeline and daily work, you can deliver a reliable, trustworthy prediction system that meets user needs and withstands the test of time.\n",
    "\n",
    "In the next chapter, we will explore **Performance Optimization**, focusing on how to make your system faster and more efficient.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 90**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
