{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 91: Performance Optimization\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Identify performance bottlenecks in time‑series prediction systems using profiling tools.\n",
    "- Apply code‑level optimizations (vectorization, data structures, algorithmic improvements) to speed up computation.\n",
    "- Leverage parallel processing (multiprocessing, multithreading, async I/O) for CPU‑bound and I/O‑bound tasks.\n",
    "- Use caching strategies to reduce redundant computations.\n",
    "- Optimize memory usage, especially when working with large time‑series datasets.\n",
    "- Scale computations using distributed frameworks like Dask and Ray.\n",
    "- Accelerate model training and inference with GPUs.\n",
    "- Design for low‑latency predictions in real‑time systems.\n",
    "- Implement performance monitoring and continuous optimization.\n",
    "\n",
    "---\n",
    "\n",
    "## **91.1 Introduction to Performance Optimization**\n",
    "\n",
    "Performance optimization is the process of making a system faster, more efficient, and more scalable. In the context of the NEPSE stock prediction system, performance considerations arise at multiple levels:\n",
    "\n",
    "- **Data ingestion**: Loading and validating large CSV files daily.\n",
    "- **Feature engineering**: Computing rolling statistics and technical indicators for thousands of stocks.\n",
    "- **Model training**: Training complex models (e.g., XGBoost, LSTM) on years of historical data.\n",
    "- **Prediction serving**: Responding to API requests with low latency.\n",
    "- **Batch predictions**: Generating predictions for many symbols efficiently.\n",
    "\n",
    "Optimization is a trade‑off: it often increases code complexity or reduces readability. Therefore, it should be guided by measurement: **profile first, optimize second**. This chapter will equip you with the tools and techniques to identify and address performance bottlenecks in your time‑series prediction system.\n",
    "\n",
    "---\n",
    "\n",
    "## **91.2 Identifying Performance Bottlenecks**\n",
    "\n",
    "Before optimising, you must know where the time is spent. Profiling tools help measure execution time and resource usage.\n",
    "\n",
    "### **91.2.1 Simple Timing with `time`**\n",
    "\n",
    "For quick checks, you can wrap code blocks with `time.time()` or `time.perf_counter()`.\n",
    "\n",
    "```python\n",
    "import time\n",
    "\n",
    "start = time.perf_counter()\n",
    "result = expensive_function()\n",
    "elapsed = time.perf_counter() - start\n",
    "print(f\"Function took {elapsed:.2f} seconds\")\n",
    "```\n",
    "\n",
    "### **91.2.2 Profiling with `cProfile`**\n",
    "\n",
    "Python's built‑in `cProfile` provides a detailed breakdown of function calls.\n",
    "\n",
    "```python\n",
    "import cProfile\n",
    "import pstats\n",
    "\n",
    "profiler = cProfile.Profile()\n",
    "profiler.enable()\n",
    "\n",
    "# Run your code\n",
    "run_feature_engineering()\n",
    "\n",
    "profiler.disable()\n",
    "stats = pstats.Stats(profiler).sort_stats('cumulative')\n",
    "stats.print_stats(20)  # print top 20 by cumulative time\n",
    "```\n",
    "\n",
    "For a more visual output, use `snakeviz`:\n",
    "\n",
    "```bash\n",
    "python -m cProfile -o output.prof my_script.py\n",
    "snakeviz output.prof\n",
    "```\n",
    "\n",
    "### **91.2.3 Line Profiler**\n",
    "\n",
    "`line_profiler` gives per‑line timing, useful for understanding bottlenecks within a function.\n",
    "\n",
    "```python\n",
    "# pip install line_profiler\n",
    "@profile\n",
    "def slow_function():\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Run with `kernprof -l -v my_script.py`.\n",
    "\n",
    "### **91.2.4 Memory Profiling**\n",
    "\n",
    "Use `memory_profiler` to track memory usage.\n",
    "\n",
    "```python\n",
    "from memory_profiler import profile\n",
    "\n",
    "@profile\n",
    "def memory_intensive_function():\n",
    "    # ...\n",
    "```\n",
    "\n",
    "Or monitor a running process with `mprof`.\n",
    "\n",
    "### **91.2.5 Application Performance Monitoring (APM)**\n",
    "\n",
    "For production systems, use APM tools like **New Relic**, **Datadog**, or **Prometheus** with **Grafana** to monitor latency, throughput, and resource usage over time.\n",
    "\n",
    "---\n",
    "\n",
    "## **91.3 Code‑Level Optimizations**\n",
    "\n",
    "Once you've identified bottlenecks, apply these common optimizations.\n",
    "\n",
    "### **91.3.1 Use Built‑in Functions and Libraries**\n",
    "\n",
    "Python's built‑in functions are implemented in C and are very fast. Use them instead of manual loops.\n",
    "\n",
    "```python\n",
    "# Slow\n",
    "total = 0\n",
    "for x in data:\n",
    "    total += x\n",
    "\n",
    "# Fast\n",
    "total = sum(data)\n",
    "```\n",
    "\n",
    "For numerical computations, use **NumPy** and **pandas**, which are vectorized.\n",
    "\n",
    "```python\n",
    "# Slow (Python loop)\n",
    "returns = [ (prices[i] - prices[i-1]) / prices[i-1] for i in range(1, len(prices)) ]\n",
    "\n",
    "# Fast (vectorized)\n",
    "returns = (prices[1:] - prices[:-1]) / prices[:-1]\n",
    "```\n",
    "\n",
    "### **91.3.2 Avoid Loops in pandas**\n",
    "\n",
    "Iterating over rows in pandas is slow. Use vectorized operations, `apply` with care, or `agg`.\n",
    "\n",
    "```python\n",
    "# Slow\n",
    "df['new_col'] = 0\n",
    "for idx, row in df.iterrows():\n",
    "    df.loc[idx, 'new_col'] = row['A'] + row['B']\n",
    "\n",
    "# Fast\n",
    "df['new_col'] = df['A'] + df['B']\n",
    "```\n",
    "\n",
    "If you must loop, consider `itertuples()` which is faster than `iterrows()`.\n",
    "\n",
    "### **91.3.3 Use Appropriate Data Structures**\n",
    "\n",
    "- Use `set` for membership tests (`O(1)` vs `O(n)` for lists).\n",
    "- Use `deque` for fast appends/pops from both ends.\n",
    "- Use `heapq` for priority queues.\n",
    "\n",
    "For time‑series, consider storing data in **numpy arrays** or **pandas Series** rather than lists of dictionaries.\n",
    "\n",
    "### **91.3.4 Lazy Evaluation**\n",
    "\n",
    "Avoid computing things until needed. For example, use generator expressions instead of lists when you don't need all values at once.\n",
    "\n",
    "```python\n",
    "# Eager (creates full list)\n",
    "results = [expensive(x) for x in huge_data]\n",
    "\n",
    "# Lazy (computes on demand)\n",
    "results = (expensive(x) for x in huge_data)\n",
    "```\n",
    "\n",
    "### **91.3.5 Caching Expensive Computations**\n",
    "\n",
    "If a function is called repeatedly with the same arguments, cache its results. Use `functools.lru_cache`.\n",
    "\n",
    "```python\n",
    "from functools import lru_cache\n",
    "\n",
    "@lru_cache(maxsize=128)\n",
    "def compute_rolling_features(symbol, date):\n",
    "    # expensive computation\n",
    "    return result\n",
    "```\n",
    "\n",
    "For larger datasets, consider a persistent cache like **Redis** (Chapter 81).\n",
    "\n",
    "---\n",
    "\n",
    "## **91.4 Parallel Processing**\n",
    "\n",
    "Python's Global Interpreter Lock (GIL) limits true parallelism for CPU‑bound tasks. Use `multiprocessing` to bypass the GIL.\n",
    "\n",
    "### **91.4.1 Multiprocessing**\n",
    "\n",
    "```python\n",
    "import multiprocessing as mp\n",
    "\n",
    "def process_symbol(symbol):\n",
    "    # process one symbol's data\n",
    "    return result\n",
    "\n",
    "symbols = ['NABIL', 'NEPSE', 'HRL', ...]\n",
    "with mp.Pool(processes=4) as pool:\n",
    "    results = pool.map(process_symbol, symbols)\n",
    "```\n",
    "\n",
    "This is ideal for embarrassingly parallel tasks like per‑symbol feature engineering.\n",
    "\n",
    "### **91.4.2 Multithreading for I/O‑Bound Tasks**\n",
    "\n",
    "For I/O‑bound tasks (e.g., network requests, file reads), threading can improve performance despite the GIL.\n",
    "\n",
    "```python\n",
    "import concurrent.futures\n",
    "\n",
    "def fetch_data(url):\n",
    "    return requests.get(url).json()\n",
    "\n",
    "urls = [...]\n",
    "with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:\n",
    "    results = list(executor.map(fetch_data, urls))\n",
    "```\n",
    "\n",
    "### **91.4.3 Asynchronous I/O**\n",
    "\n",
    "For high‑concurrency I/O (e.g., a prediction API), use `asyncio` with an async web framework like FastAPI.\n",
    "\n",
    "```python\n",
    "import asyncio\n",
    "import aiohttp\n",
    "\n",
    "async def fetch_data(session, url):\n",
    "    async with session.get(url) as response:\n",
    "        return await response.json()\n",
    "\n",
    "async def main():\n",
    "    async with aiohttp.ClientSession() as session:\n",
    "        tasks = [fetch_data(session, url) for url in urls]\n",
    "        results = await asyncio.gather(*tasks)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **91.5 Optimizing Feature Engineering**\n",
    "\n",
    "Feature engineering is often the most time‑consuming part of the pipeline.\n",
    "\n",
    "### **91.5.1 Efficient Rolling Windows**\n",
    "\n",
    "pandas rolling operations are optimized, but you can speed them up by using `numba` for custom functions.\n",
    "\n",
    "```python\n",
    "import numba\n",
    "\n",
    "@numba.jit(nopython=True)\n",
    "def rolling_mean_numba(data, window):\n",
    "    result = np.empty(len(data))\n",
    "    for i in range(len(data)):\n",
    "        start = max(0, i - window + 1)\n",
    "        result[i] = np.mean(data[start:i+1])\n",
    "    return result\n",
    "```\n",
    "\n",
    "### **91.5.2 Pre‑compute and Store Features**\n",
    "\n",
    "Instead of recomputing features every time, compute them once and store in a feature store (Chapter 74). This is especially important for online predictions.\n",
    "\n",
    "### **91.5.3 Use Efficient Data Types**\n",
    "\n",
    "Downcast numeric columns to save memory and speed up computations.\n",
    "\n",
    "```python\n",
    "df['Close'] = pd.to_numeric(df['Close'], downcast='float')\n",
    "df['Volume'] = pd.to_numeric(df['Volume'], downcast='integer')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **91.6 Optimizing Model Training**\n",
    "\n",
    "### **91.6.1 Hardware Acceleration (GPUs)**\n",
    "\n",
    "For deep learning models (LSTM, Transformers), use GPUs. Libraries like TensorFlow and PyTorch automatically leverage GPUs if available.\n",
    "\n",
    "```python\n",
    "import torch\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "```\n",
    "\n",
    "For XGBoost, you can use the GPU‑accelerated version:\n",
    "\n",
    "```python\n",
    "model = xgb.XGBRegressor(tree_method='gpu_hist', gpu_id=0)\n",
    "```\n",
    "\n",
    "### **91.6.2 Distributed Training**\n",
    "\n",
    "For very large datasets or models, use distributed training with frameworks like Ray or Horovod (Chapter 85).\n",
    "\n",
    "### **91.6.3 Hyperparameter Tuning Efficiency**\n",
    "\n",
    "Use **early stopping** to avoid overfitting and reduce training time. Use libraries like **Optuna** or **Hyperopt** that can prune unpromising trials.\n",
    "\n",
    "```python\n",
    "import optuna\n",
    "\n",
    "def objective(trial):\n",
    "    params = {\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3),\n",
    "    }\n",
    "    model = xgb.XGBRegressor(**params, n_estimators=1000, early_stopping_rounds=10)\n",
    "    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)\n",
    "    return mean_absolute_error(y_val, model.predict(X_val))\n",
    "\n",
    "study = optuna.create_study(direction='minimize')\n",
    "study.optimize(objective, n_trials=50)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **91.7 Optimizing Prediction Serving**\n",
    "\n",
    "Low‑latency predictions are critical for real‑time systems.\n",
    "\n",
    "### **91.7.1 Model Serialization and Loading**\n",
    "\n",
    "Use fast serialization formats. For XGBoost, the native `.json` format is faster than pickle.\n",
    "\n",
    "```python\n",
    "model.save_model('model.json')\n",
    "model = xgb.XGBRegressor()\n",
    "model.load_model('model.json')\n",
    "```\n",
    "\n",
    "### **91.7.2 Model Caching**\n",
    "\n",
    "If you have multiple model versions, load them once and cache in memory. In a web service, load the model at startup.\n",
    "\n",
    "### **91.7.3 Batching Predictions**\n",
    "\n",
    "If you receive many prediction requests, batch them to amortize overhead.\n",
    "\n",
    "```python\n",
    "@app.post(\"/predict_batch\")\n",
    "def predict_batch(requests: List[PredictionRequest]):\n",
    "    features = [get_features(req) for req in requests]\n",
    "    X = pd.DataFrame(features)\n",
    "    predictions = model.predict(X)\n",
    "    return [{\"prediction\": p} for p in predictions]\n",
    "```\n",
    "\n",
    "### **91.7.4 Lightweight Alternatives**\n",
    "\n",
    "If latency is critical, consider:\n",
    "\n",
    "- Using a simpler model (e.g., linear regression instead of XGBoost).\n",
    "- Quantizing neural networks (TensorFlow Lite, ONNX).\n",
    "- Moving inference to the edge (Chapter 78).\n",
    "\n",
    "### **91.7.5 Asynchronous Processing**\n",
    "\n",
    "For non‑critical predictions, use a message queue (Kafka) and process asynchronously.\n",
    "\n",
    "---\n",
    "\n",
    "## **91.8 Memory Optimization**\n",
    "\n",
    "Large time‑series datasets can consume significant memory.\n",
    "\n",
    "### **91.8.1 Chunking**\n",
    "\n",
    "Process data in chunks instead of loading everything at once.\n",
    "\n",
    "```python\n",
    "chunk_size = 10000\n",
    "for chunk in pd.read_csv('large_file.csv', chunksize=chunk_size):\n",
    "    process_chunk(chunk)\n",
    "```\n",
    "\n",
    "### **91.8.2 Use Efficient File Formats**\n",
    "\n",
    "Parquet is much more efficient than CSV in both storage and memory.\n",
    "\n",
    "```python\n",
    "df = pd.read_parquet('data.parquet')\n",
    "```\n",
    "\n",
    "### **91.8.3 Downcasting**\n",
    "\n",
    "As mentioned, downcast numeric columns.\n",
    "\n",
    "### **91.8.4 Garbage Collection**\n",
    "\n",
    "For long‑running processes, manually trigger garbage collection if needed, but let Python manage it normally.\n",
    "\n",
    "---\n",
    "\n",
    "## **91.9 Scaling with Dask and Ray**\n",
    "\n",
    "When a single machine is insufficient, use distributed frameworks.\n",
    "\n",
    "### **91.9.1 Dask for DataFrame Operations**\n",
    "\n",
    "Dask can handle larger‑than‑memory datasets by partitioning.\n",
    "\n",
    "```python\n",
    "import dask.dataframe as dd\n",
    "\n",
    "df = dd.read_parquet('data/*.parquet')\n",
    "df['daily_return'] = df.groupby('symbol')['close'].pct_change()\n",
    "df = df.dropna()\n",
    "result = df.compute()  # triggers computation\n",
    "```\n",
    "\n",
    "### **91.9.2 Ray for Distributed Execution**\n",
    "\n",
    "Ray is great for parallelizing custom functions across a cluster.\n",
    "\n",
    "```python\n",
    "import ray\n",
    "\n",
    "ray.init()\n",
    "\n",
    "@ray.remote\n",
    "def process_symbol(symbol):\n",
    "    # ... processing\n",
    "    return result\n",
    "\n",
    "futures = [process_symbol.remote(sym) for sym in symbols]\n",
    "results = ray.get(futures)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **91.10 Monitoring Performance in Production**\n",
    "\n",
    "Once optimizations are in place, continuously monitor to ensure they remain effective.\n",
    "\n",
    "- Track **request latency** (p50, p95, p99) over time.\n",
    "- Monitor **throughput** (requests per second).\n",
    "- Watch **CPU, memory, and GPU utilisation**.\n",
    "- Set up alerts for performance degradation (Chapter 73).\n",
    "\n",
    "Use tools like Prometheus, Grafana, and custom dashboards.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored performance optimization for time‑series prediction systems, using the NEPSE example as a guide. We covered:\n",
    "\n",
    "- Identifying bottlenecks with profiling tools.\n",
    "- Code‑level optimizations (vectorization, data structures, caching).\n",
    "- Parallel processing with multiprocessing, threading, and async.\n",
    "- Optimizing feature engineering and model training.\n",
    "- Reducing prediction latency through caching, batching, and efficient serialization.\n",
    "- Managing memory for large datasets.\n",
    "- Scaling with Dask and Ray.\n",
    "- Monitoring performance in production.\n",
    "\n",
    "Remember: measure first, optimise second. Premature optimisation can lead to complex, hard‑to‑maintain code. Focus on the bottlenecks that actually matter for your system's performance goals.\n",
    "\n",
    "In the next chapter, we will discuss **Security and Compliance**, ensuring that our prediction system is protected against threats and meets regulatory requirements.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 91**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
