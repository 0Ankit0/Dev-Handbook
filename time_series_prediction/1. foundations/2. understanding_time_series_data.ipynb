{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Chapter 2: Understanding Time-Series Data**\n",
    "\n",
    "---\n",
    "\n",
    "### **2.1 What Defines Time-Series Data?**\n",
    "\n",
    "Time-series data is a sequence of data points collected or recorded at successive points in time, typically at uniform intervals. What distinguishes time-series data from other data types is the fundamental relationship between observations: **the order matters**.\n",
    "\n",
    "#### **Key Characteristics of Time-Series Data**\n",
    "\n",
    "**1. Temporal Ordering**\n",
    "\n",
    "The sequence of observations carries critical information. In our NEPSE dataset, the order of trading days tells us about price evolution, momentum, and market dynamics.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load or create NEPSE data\n",
    "def load_nepse_data(filepath=None):\n",
    "    \"\"\"\n",
    "    Load NEPSE data from CSV or create sample data for demonstration.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    filepath : str, optional\n",
    "        Path to NEPSE CSV file\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : NEPSE data\n",
    "    \"\"\"\n",
    "    if filepath:\n",
    "        return pd.read_csv(filepath)\n",
    "    else:\n",
    "        # Create sample data for demonstration\n",
    "        np.random.seed(42)\n",
    "        n_days = 250  # About one trading year\n",
    "        \n",
    "        dates = pd.date_range(start='2024-01-01', periods=n_days, freq='B')  # Business days\n",
    "        \n",
    "        # Simulate ABL stock prices\n",
    "        base_price = 500\n",
    "        returns = np.random.normal(0.0003, 0.015, n_days)  # Slight upward drift\n",
    "        prices = base_price * np.cumprod(1 + returns)\n",
    "        \n",
    "        data = pd.DataFrame({\n",
    "            'Date': dates,\n",
    "            'S.No': range(1, n_days + 1),\n",
    "            'Symbol': 'ABL',\n",
    "            'Open': prices * (1 + np.random.uniform(-0.005, 0.005, n_days)),\n",
    "            'High': prices * (1 + np.abs(np.random.normal(0.01, 0.005, n_days))),\n",
    "            'Low': prices * (1 - np.abs(np.random.normal(0.01, 0.005, n_days))),\n",
    "            'Close': prices,\n",
    "            'Vol': np.random.randint(10000, 100000, n_days),\n",
    "            'VWAP': prices * (1 + np.random.uniform(-0.003, 0.003, n_days))\n",
    "        })\n",
    "        \n",
    "        return data\n",
    "\n",
    "\n",
    "def demonstrate_temporal_ordering(data):\n",
    "    \"\"\"\n",
    "    Demonstrate why temporal ordering matters in time-series data.\n",
    "    \n",
    "    This function shows that shuffling time-series data destroys\n",
    "    critical patterns needed for prediction.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Time-series data with chronological order\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Comparison metrics between ordered and shuffled data\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"DEMONSTRATION: Why Temporal Order Matters\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Extract closing prices\n",
    "    close_prices = data['Close'].values.copy()\n",
    "    \n",
    "    # ========================================\n",
    "    # Analysis 1: Autocorrelation in Ordered Data\n",
    "    # ========================================\n",
    "    print(\"\\n\ud83d\udcca Analysis 1: Autocorrelation (Relationship with Past Values)\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Autocorrelation measures how correlated a time series is with\n",
    "    # a lagged version of itself\n",
    "    #\n",
    "    # Formula: \u03c1(k) = Cov(X_t, X_{t-k}) / Var(X_t)\n",
    "    # where k is the lag (number of time periods)\n",
    "    \n",
    "    def calculate_autocorrelation(series, lag):\n",
    "        \"\"\"\n",
    "        Calculate autocorrelation at a specific lag.\n",
    "        \n",
    "        Autocorrelation tells us how much the current value\n",
    "        depends on the value k periods ago.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        series : np.array\n",
    "            Time series values\n",
    "        lag : int\n",
    "            Number of periods to look back\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        float : Autocorrelation coefficient (-1 to 1)\n",
    "        \"\"\"\n",
    "        n = len(series)\n",
    "        if lag >= n:\n",
    "            return np.nan\n",
    "        \n",
    "        # Get the overlapping portion\n",
    "        series_t = series[lag:]      # Current values\n",
    "        series_lag = series[:-lag]   # Lagged values\n",
    "        \n",
    "        # Calculate means\n",
    "        mean_t = np.mean(series_t)\n",
    "        mean_lag = np.mean(series_lag)\n",
    "        \n",
    "        # Calculate covariance and variances\n",
    "        covariance = np.sum((series_t - mean_t) * (series_lag - mean_lag)) / (n - lag)\n",
    "        variance_t = np.var(series_t)\n",
    "        variance_lag = np.var(series_lag)\n",
    "        \n",
    "        # Autocorrelation\n",
    "        if variance_t * variance_lag > 0:\n",
    "            autocorr = covariance / np.sqrt(variance_t * variance_lag)\n",
    "        else:\n",
    "            autocorr = 0\n",
    "        \n",
    "        return autocorr\n",
    "    \n",
    "    # Calculate autocorrelation for multiple lags\n",
    "    lags = [1, 2, 3, 5, 10, 20]\n",
    "    \n",
    "    print(\"\\nOrdered Data - Autocorrelation at Different Lags:\")\n",
    "    print(\"(How much does today's price depend on past prices?)\")\n",
    "    print()\n",
    "    \n",
    "    for lag in lags:\n",
    "        autocorr = calculate_autocorrelation(close_prices, lag)\n",
    "        bar = \"\u2588\" * int(abs(autocorr) * 50)\n",
    "        print(f\"  Lag {lag:2d} days: {autocorr:+.4f} {bar}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Analysis 2: Shuffled Data Comparison\n",
    "    # ========================================\n",
    "    print(\"\\n\ud83d\udcca Analysis 2: Shuffled Data Comparison\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # Shuffle the prices to destroy temporal structure\n",
    "    shuffled_prices = close_prices.copy()\n",
    "    np.random.shuffle(shuffled_prices)\n",
    "    \n",
    "    print(\"\\nShuffled Data - Autocorrelation at Different Lags:\")\n",
    "    print(\"(Temporal structure destroyed - should be near zero)\")\n",
    "    print()\n",
    "    \n",
    "    for lag in lags:\n",
    "        autocorr = calculate_autocorrelation(shuffled_prices, lag)\n",
    "        bar = \"\u2588\" * int(abs(autocorr) * 50)\n",
    "        print(f\"  Lag {lag:2d} days: {autocorr:+.4f} {bar}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Analysis 3: Predictability Comparison\n",
    "    # ========================================\n",
    "    print(\"\\n\ud83d\udcca Analysis 3: Predictability Analysis\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    # For ordered data: predict using previous value\n",
    "    ordered_predictions = close_prices[:-1]  # Use yesterday's price\n",
    "    ordered_actual = close_prices[1:]        # Today's actual price\n",
    "    ordered_mae = np.mean(np.abs(ordered_actual - ordered_predictions))\n",
    "    \n",
    "    # For shuffled data: predict using previous value\n",
    "    shuffled_predictions = shuffled_prices[:-1]\n",
    "    shuffled_actual = shuffled_prices[1:]\n",
    "    shuffled_mae = np.mean(np.abs(shuffled_actual - shuffled_predictions))\n",
    "    \n",
    "    print(f\"\\nNaive Prediction (predict today = yesterday):\")\n",
    "    print(f\"  Ordered Data MAE:   {ordered_mae:.4f}\")\n",
    "    print(f\"  Shuffled Data MAE:  {shuffled_mae:.4f}\")\n",
    "    print(f\"\\n  Ratio (Shuffled/Ordered): {shuffled_mae/ordered_mae:.2f}x\")\n",
    "    \n",
    "    print(\"\\n\ud83d\udca1 Key Insight:\")\n",
    "    print(\"   The shuffled data is MUCH harder to predict because\")\n",
    "    print(\"   the temporal dependencies have been destroyed.\")\n",
    "    \n",
    "    return {\n",
    "        'ordered_mae': ordered_mae,\n",
    "        'shuffled_mae': shuffled_mae,\n",
    "        'ratio': shuffled_mae / ordered_mae\n",
    "    }\n",
    "\n",
    "\n",
    "# Run the demonstration\n",
    "nepse_data = load_nepse_data()\n",
    "result = demonstrate_temporal_ordering(nepse_data)\n",
    "```\n",
    "\n",
    "**Explanation of Temporal Ordering**:\n",
    "\n",
    "The code above demonstrates the fundamental importance of temporal ordering in time-series data. Let me break down the key concepts:\n",
    "\n",
    "1. **Autocorrelation**: This measures how much a value at time t depends on values at time t-k. In financial data like NEPSE:\n",
    "   - High autocorrelation at lag 1 (0.99+) means today's price is very similar to yesterday's\n",
    "   - This gradually decreases as we look further back\n",
    "   - This is WHY time-series prediction works\u2014we can learn from past values\n",
    "\n",
    "2. **Shuffling Destroys Information**: When we randomly shuffle the data:\n",
    "   - Autocorrelation drops to nearly zero\n",
    "   - Prediction becomes much harder (higher MAE)\n",
    "   - The patterns that made prediction possible are gone\n",
    "\n",
    "**2. Time Intervals**\n",
    "\n",
    "Time-series data is collected at specific intervals. Understanding these intervals is crucial for proper analysis.\n",
    "\n",
    "```python\n",
    "def analyze_time_intervals(data):\n",
    "    \"\"\"\n",
    "    Analyze and explain time intervals in time-series data.\n",
    "    \n",
    "    Time interval (frequency) determines:\n",
    "    - How often observations are recorded\n",
    "    - What patterns can be detected\n",
    "    - What models are appropriate\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        Time-series data with Date or S.No column\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"ANALYSIS: Time Intervals in Time-Series Data\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # ========================================\n",
    "    # Different Time Intervals in Practice\n",
    "    # ========================================\n",
    "    print(\"\\n\ud83d\udcc5 Common Time Intervals in Different Domains:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    intervals = {\n",
    "        'Tick Data': {\n",
    "            'frequency': 'Milliseconds to seconds',\n",
    "            'example': 'High-frequency trading',\n",
    "            'patterns': 'Microstructure effects, order flow',\n",
    "            'nepse_relevance': 'Not commonly available for NEPSE'\n",
    "        },\n",
    "        'Intraday': {\n",
    "            'frequency': 'Minutes to hours',\n",
    "            'example': 'Day trading analysis',\n",
    "            'patterns': 'Intraday volatility, session effects',\n",
    "            'nepse_relevance': 'Available during trading hours'\n",
    "        },\n",
    "        'Daily': {\n",
    "            'frequency': 'One observation per trading day',\n",
    "            'example': 'NEPSE closing prices',\n",
    "            'patterns': 'Daily trends, day-of-week effects',\n",
    "            'nepse_relevance': 'Standard NEPSE data format'\n",
    "        },\n",
    "        'Weekly': {\n",
    "            'frequency': 'Weekly aggregation',\n",
    "            'example': 'Weekly market reports',\n",
    "            'patterns': 'Weekly seasonality',\n",
    "            'nepse_relevance': 'Can aggregate daily data'\n",
    "        },\n",
    "        'Monthly': {\n",
    "            'frequency': 'Monthly observations',\n",
    "            'example': 'Economic indicators',\n",
    "            'patterns': 'Monthly cycles, earnings seasons',\n",
    "            'nepse_relevance': 'Long-term trend analysis'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    for interval_type, info in intervals.items():\n",
    "        print(f\"\\n{interval_type}:\")\n",
    "        print(f\"  Frequency:      {info['frequency']}\")\n",
    "        print(f\"  Example:        {info['example']}\")\n",
    "        print(f\"  Patterns:       {info['patterns']}\")\n",
    "        print(f\"  NEPSE Use:      {info['nepse_relevance']}\")\n",
    "    \n",
    "    # ========================================\n",
    "    # NEPSE-Specific Interval Analysis\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"NEPSE TRADING SCHEDULE\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    Nepal Stock Exchange Trading Hours:\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502  Session          \u2502  Time (NPT)    \u2502  Activity              \u2502\n",
    "    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "    \u2502  Pre-Open         \u2502  10:30 - 11:00 \u2502  Order collection      \u2502\n",
    "    \u2502  Opening Auction  \u2502  11:00         \u2502  Opening price set     \u2502\n",
    "    \u2502  Continuous       \u2502  11:00 - 15:00 \u2502  Regular trading       \u2502\n",
    "    \u2502  Closing Auction  \u2502  15:00         \u2502  Closing price set     \u2502\n",
    "    \u2502  Post-Close       \u2502  15:00 - 15:30 \u2502  Settlement            \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "    \n",
    "    Trading Days: Sunday to Friday (Saturday closed)\n",
    "    Holidays: Public holidays as per Nepal government calendar\n",
    "    \n",
    "    Implications for Time-Series Analysis:\n",
    "    \u2022 Data frequency: Daily (one record per stock per day)\n",
    "    \u2022 Weekly patterns: 6 trading days, Saturday gap\n",
    "    \u2022 Holiday effects: Must account for market closures\n",
    "    \"\"\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Creating Time Features\n",
    "    # ========================================\n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"CREATING TIME-BASED FEATURES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    # Convert to datetime if available\n",
    "    if 'Date' in data.columns:\n",
    "        data['Date'] = pd.to_datetime(data['Date'])\n",
    "        \n",
    "        # Extract time components\n",
    "        data['Year'] = data['Date'].dt.year\n",
    "        data['Month'] = data['Date'].dt.month\n",
    "        data['Day'] = data['Date'].dt.day\n",
    "        data['DayOfWeek'] = data['Date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "        data['WeekOfYear'] = data['Date'].dt.isocalendar().week\n",
    "        data['Quarter'] = data['Date'].dt.quarter\n",
    "        \n",
    "        print(\"\\nTime Features Created:\")\n",
    "        print(data[['Date', 'Year', 'Month', 'DayOfWeek', 'Quarter']].head(10))\n",
    "        \n",
    "        # Analyze day-of-week patterns\n",
    "        print(\"\\n\ud83d\udcca Average Returns by Day of Week:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        data['Return'] = data['Close'].pct_change()\n",
    "        day_names = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        \n",
    "        # NEPSE trades Sunday-Friday (Saturday closed)\n",
    "        # dayofweek: 0=Monday, 5=Saturday, 6=Sunday\n",
    "        # So NEPSE trading days are: 0-4 (Mon-Fri) and 6 (Sunday)\n",
    "        \n",
    "        for day_num in range(7):\n",
    "            day_data = data[data['DayOfWeek'] == day_num]\n",
    "            if len(day_data) > 0:\n",
    "                avg_return = day_data['Return'].mean() * 100\n",
    "                print(f\"  {day_names[day_num]:12s}: {avg_return:+.4f}%\")\n",
    "    else:\n",
    "        # Using S.No as proxy for time\n",
    "        print(\"\\nNote: No Date column found. Using S.No as time index.\")\n",
    "        print(\"In real NEPSE data, always ensure you have proper dates!\")\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Analyze time intervals\n",
    "nepse_with_time = analyze_time_intervals(nepse_data)\n",
    "```\n",
    "\n",
    "**3. Irregular vs Regular Time-Series**\n",
    "\n",
    "```python\n",
    "def compare_regular_irregular():\n",
    "    \"\"\"\n",
    "    Explain the difference between regular and irregular time-series.\n",
    "    \n",
    "    This is important because:\n",
    "    - Regular time-series have constant intervals (easier to model)\n",
    "    - Irregular time-series have varying intervals (requires special handling)\n",
    "    \"\"\"\n",
    "    print(\"=\" * 70)\n",
    "    print(\"REGULAR VS IRREGULAR TIME-SERIES\")\n",
    "    print(\"=\" * 70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502                     REGULAR TIME-SERIES                             \u2502\n",
    "    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "    \u2502 \u2022 Constant time intervals between observations                      \u2502\n",
    "    \u2502 \u2022 Example: Daily closing prices (every trading day)                 \u2502\n",
    "    \u2502 \u2022 Example: Hourly temperature readings                               \u2502\n",
    "    \u2502                                                                     \u2502\n",
    "    \u2502     Day 1 \u2500\u2500 Day 2 \u2500\u2500 Day 3 \u2500\u2500 Day 4 \u2500\u2500 Day 5                       \u2502\n",
    "    \u2502     (24h)    (24h)    (24h)    (24h)                                \u2502\n",
    "    \u2502                                                                     \u2502\n",
    "    \u2502 \u2022 Advantages: Simpler to model, standard techniques apply           \u2502\n",
    "    \u2502 \u2022 NEPSE Context: Daily data with weekend/holiday gaps               \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "    \n",
    "    \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "    \u2502                    IRREGULAR TIME-SERIES                            \u2502\n",
    "    \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "    \u2502 \u2022 Variable time intervals between observations                      \u2502\n",
    "    \u2502 \u2022 Example: Stock trades (random timing)                             \u2502\n",
    "    \u2502 \u2022 Example: Patient hospital visits                                  \u2502\n",
    "    \u2502                                                                     \u2502\n",
    "    \u2502     Event 1 \u2500\u2500\u2500\u2500 Event 2 \u2500 Event 3 \u2500\u2500\u2500\u2500\u2500\u2500 Event 4                   \u2502\n",
    "    \u2502     (4 days)   (1 day)  (5 days)                                    \u2502\n",
    "    \u2502                                                                     \u2502\n",
    "    \u2502 \u2022 Challenges: Need to model time between events                     \u2502\n",
    "    \u2502 \u2022 Solutions: Interpolation, time-aware models                       \u2502\n",
    "    \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "    \"\"\")\n",
    "    \n",
    "    # ========================================\n",
    "    # Handling Weekend Gaps in NEPSE\n",
    "    # ========================================\n",
    "    print(\"\\n\ud83d\udcca Handling Non-Trading Days in NEPSE:\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    print(\"\"\"\n",
    "    NEPSE has regular daily data BUT with gaps:\n",
    "    \u2022 Saturdays: Market closed\n",
    "    \u2022 Public holidays: Market closed\n",
    "    \n",
    "    Approaches to handle these gaps:\n",
    "    \n",
    "    1. IGNORE GAPS (Simplest)\n",
    "       - Treat data as consecutive observations\n",
    "       - S.No becomes the time index\n",
    "       - Works well for short-term prediction\n",
    "       \n",
    "    2. FILL GAPS\n",
    "       - Use forward fill or interpolation\n",
    "       - Preserves calendar time\n",
    "       - Important for seasonality analysis\n",
    "    \n",
    "    3. TIME-AWARE MODELING\n",
    "       - Include day-of-week features\n",
    "       - Account for time between observations\n",
    "       - Most sophisticated approach\n",
    "    \"\"\")\n",
    "    \n",
    "    # Demonstration code for handling gaps\n",
    "    print(\"\\n\ud83d\udcbb Code Example: Handling Weekend Gaps\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    code_example = '''\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Option 1: Ignore gaps (use trading days as index)\n",
    "    data['Trading_Day'] = range(len(data))\n",
    "    \n",
    "    # Option 2: Fill gaps (add calendar days)\n",
    "    full_date_range = pd.date_range(\n",
    "        start=data['Date'].min(), \n",
    "        end=data['Date'].max(), \n",
    "        freq='D'  # Daily frequency\n",
    "    )\n",
    "    data = data.set_index('Date').reindex(full_date_range)\n",
    "    data = data.ffill()  # Forward fill prices\n",
    "    \n",
    "    # Option 3: Time-aware features\n",
    "    data['Is_Monday'] = (data['Date'].dt.dayofweek == 0).astype(int)\n",
    "    data['Is_Saturday'] = (data['Date'].dt.dayofweek == 5).astype(int)\n",
    "    data['Days_Since_Last_Trade'] = data['Date'].diff().dt.days\n",
    "    '''\n",
    "    \n",
    "    print(code_example)\n",
    "\n",
    "\n",
    "# Run the comparison\n",
    "compare_regular_irregular()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2.2 Components of Time-Series**\n",
    "\n",
    "Every time-series can be decomposed into fundamental components. Understanding these components is essential for building effective prediction models.\n",
    "\n",
    "#### **2.2.1 Trend**\n",
    "\n",
    "**Definition**: The long-term movement or direction in the data, representing the underlying tendency of the series to increase, decrease, or remain stable over time.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "class TrendAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive trend analysis for time-series data.\n",
    "    \n",
    "    Trend represents the long-term direction of a time series,\n",
    "    filtering out short-term fluctuations and seasonal patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        \"\"\"\n",
    "        Initialize the trend analyzer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Time-series data\n",
    "        price_column : str\n",
    "            Column containing the price/values to analyze\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "        self.trend = None\n",
    "        self.detrended = None\n",
    "    \n",
    "    def identify_trend_methods(self):\n",
    "        \"\"\"\n",
    "        Explain different methods for identifying trends.\n",
    "        \n",
    "        Each method has its strengths and is appropriate for\n",
    "        different types of data and analysis goals.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"TREND IDENTIFICATION METHODS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        methods = {\n",
    "            'Moving Average': {\n",
    "                'description': 'Smooth data by averaging over a window',\n",
    "                'pros': ['Simple to understand', 'Reduces noise', 'Flexible window size'],\n",
    "                'cons': ['Lags behind actual trend', 'Edge effects', 'Window size selection'],\n",
    "                'use_case': 'Identifying general direction in noisy data'\n",
    "            },\n",
    "            'Linear Regression': {\n",
    "                'description': 'Fit a straight line to the data',\n",
    "                'pros': ['Quantifies trend direction', 'Provides slope', 'Easy to interpret'],\n",
    "                'cons': ['Assumes linear trend', 'Sensitive to outliers', 'May miss curvature'],\n",
    "                'use_case': 'Determining if overall trend is up/down'\n",
    "            },\n",
    "            'Polynomial Regression': {\n",
    "                'description': 'Fit a curved line to the data',\n",
    "                'pros': ['Captures non-linear trends', 'More flexible'],\n",
    "                'cons': ['Risk of overfitting', 'Choosing degree', 'Less interpretable'],\n",
    "                'use_case': 'Data with accelerating or decelerating trends'\n",
    "            },\n",
    "            'Decomposition': {\n",
    "                'description': 'Separate trend, seasonal, and residual components',\n",
    "                'pros': ['Comprehensive view', 'Isolates trend cleanly'],\n",
    "                'cons': ['Assumes additive/multiplicative model', 'Requires seasonality'],\n",
    "                'use_case': 'Complex time-series with multiple patterns'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for method, info in methods.items():\n",
    "            print(f\"\\n\ud83d\udcca {method}\")\n",
    "            print(f\"   Description: {info['description']}\")\n",
    "            print(f\"   Pros: {', '.join(info['pros'])}\")\n",
    "            print(f\"   Cons: {', '.join(info['cons'])}\")\n",
    "            print(f\"   Best for: {info['use_case']}\")\n",
    "    \n",
    "    def moving_average_trend(self, window=20):\n",
    "        \"\"\"\n",
    "        Calculate trend using moving average.\n",
    "        \n",
    "        The moving average smooths out short-term fluctuations\n",
    "        and highlights the longer-term trend.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        window : int\n",
    "            Number of periods to average over\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.array : Smoothed trend values\n",
    "        \"\"\"\n",
    "        prices = self.data[self.price_column].values\n",
    "        \n",
    "        # Simple Moving Average (SMA)\n",
    "        # SMA_t = (P_t + P_{t-1} + ... + P_{t-n+1}) / n\n",
    "        \n",
    "        self.trend_sma = np.convolve(\n",
    "            prices, \n",
    "            np.ones(window) / window, \n",
    "            mode='valid'\n",
    "        )\n",
    "        \n",
    "        # Centered Moving Average\n",
    "        # This aligns the average with the center of the window\n",
    "        # Better for trend identification (not for prediction!)\n",
    "        \n",
    "        self.trend_centered = pd.Series(prices).rolling(\n",
    "            window=window, \n",
    "            center=True\n",
    "        ).mean().values\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Moving Average Trend (Window = {window} days)\")\n",
    "        print(f\"   Original data points: {len(prices)}\")\n",
    "        print(f\"   SMA trend points: {len(self.trend_sma)}\")\n",
    "        print(f\"   Centered MA points: {len(self.trend_centered) - window + 1}\")\n",
    "        \n",
    "        return self.trend_sma\n",
    "    \n",
    "    def linear_regression_trend(self):\n",
    "        \"\"\"\n",
    "        Calculate trend using linear regression.\n",
    "        \n",
    "        Linear regression finds the best-fit straight line through\n",
    "        the data points. The slope tells us the trend direction\n",
    "        and magnitude.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Trend statistics including slope, r_squared, direction\n",
    "        \"\"\"\n",
    "        prices = self.data[self.price_column].values\n",
    "        x = np.arange(len(prices))  # Time index\n",
    "        \n",
    "        # Perform linear regression\n",
    "        # y = mx + b, where m is slope (trend) and b is intercept\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x, prices)\n",
    "        \n",
    "        # Calculate trend line values\n",
    "        self.trend_linear = slope * x + intercept\n",
    "        \n",
    "        # Store statistics\n",
    "        self.linear_trend_stats = {\n",
    "            'slope': slope,                    # Trend direction and magnitude\n",
    "            'intercept': intercept,            # Starting point\n",
    "            'r_squared': r_value ** 2,         # How well line fits data\n",
    "            'p_value': p_value,                # Statistical significance\n",
    "            'std_error': std_err,              # Uncertainty in slope\n",
    "            'direction': 'Upward' if slope > 0 else 'Downward',\n",
    "            'daily_change': slope              # Average daily price change\n",
    "        }\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Linear Regression Trend Analysis\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Slope:          {slope:.4f} NPR/day\")\n",
    "        print(f\"   Intercept:      {intercept:.2f} NPR\")\n",
    "        print(f\"   R-squared:      {r_value**2:.4f}\")\n",
    "        print(f\"   P-value:        {p_value:.6f}\")\n",
    "        print(f\"   Trend:          {self.linear_trend_stats['direction']}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        if p_value < 0.05:\n",
    "            print(f\"\\n   \u2713 The trend is statistically significant (p < 0.05)\")\n",
    "        else:\n",
    "            print(f\"\\n   \u26a0 The trend is NOT statistically significant\")\n",
    "        \n",
    "        # Annualized trend\n",
    "        trading_days = 250  # Approximate trading days per year\n",
    "        annual_trend = slope * trading_days\n",
    "        annual_pct = (annual_trend / prices[0]) * 100\n",
    "        \n",
    "        print(f\"\\n   Projected annual change: {annual_trend:.2f} NPR ({annual_pct:+.2f}%)\")\n",
    "        \n",
    "        return self.linear_trend_stats\n",
    "    \n",
    "    def polynomial_trend(self, degree=2):\n",
    "        \"\"\"\n",
    "        Calculate trend using polynomial regression.\n",
    "        \n",
    "        Polynomial regression can capture non-linear trends\n",
    "        (accelerating or decelerating patterns).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        degree : int\n",
    "            Degree of polynomial (2=quadratic, 3=cubic, etc.)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.array : Polynomial trend values\n",
    "        \"\"\"\n",
    "        prices = self.data[self.price_column].values\n",
    "        x = np.arange(len(prices))\n",
    "        \n",
    "        # Fit polynomial\n",
    "        # degree=2: y = ax\u00b2 + bx + c\n",
    "        # degree=3: y = ax\u00b3 + bx\u00b2 + cx + d\n",
    "        coefficients = np.polyfit(x, prices, degree)\n",
    "        self.trend_poly = np.polyval(coefficients, x)\n",
    "        \n",
    "        # Store coefficients\n",
    "        self.poly_coefficients = coefficients\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Polynomial Trend (Degree = {degree})\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Polynomial equation: \", end=\"\")\n",
    "        \n",
    "        for i, coef in enumerate(coefficients):\n",
    "            power = degree - i\n",
    "            if power > 1:\n",
    "                print(f\"{coef:.4f}x^{power} + \", end=\"\")\n",
    "            elif power == 1:\n",
    "                print(f\"{coef:.4f}x + \", end=\"\")\n",
    "            else:\n",
    "                print(f\"{coef:.4f}\")\n",
    "        \n",
    "        return self.trend_poly\n",
    "    \n",
    "    def visualize_trends(self, window=20):\n",
    "        \"\"\"\n",
    "        Visualize different trend estimation methods.\n",
    "        \n",
    "        This creates a comprehensive visualization comparing\n",
    "        all trend estimation methods applied to the data.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        prices = self.data[self.price_column].values\n",
    "        x = np.arange(len(prices))\n",
    "        \n",
    "        # ========================================\n",
    "        # Plot 1: Original Data with SMA\n",
    "        # ========================================\n",
    "        ax = axes[0, 0]\n",
    "        ax.plot(x, prices, label='Original Prices', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        if hasattr(self, 'trend_sma'):\n",
    "            ax.plot(\n",
    "                x[window-1:], \n",
    "                self.trend_sma, \n",
    "                label=f'{window}-day SMA', \n",
    "                linewidth=2, \n",
    "                color='red'\n",
    "            )\n",
    "        \n",
    "        ax.set_title('Moving Average Trend', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Trading Day')\n",
    "        ax.set_ylabel('Price (NPR)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ========================================\n",
    "        # Plot 2: Linear Regression Trend\n",
    "        # ========================================\n",
    "        ax = axes[0, 1]\n",
    "        ax.plot(x, prices, label='Original Prices', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        if hasattr(self, 'trend_linear'):\n",
    "            ax.plot(\n",
    "                x, \n",
    "                self.trend_linear, \n",
    "                label='Linear Trend', \n",
    "                linewidth=2, \n",
    "                color='green'\n",
    "            )\n",
    "            \n",
    "            # Add trend annotation\n",
    "            stats = self.linear_trend_stats\n",
    "            annotation = f\"Slope: {stats['slope']:.3f} NPR/day\\nR\u00b2: {stats['r_squared']:.3f}\"\n",
    "            ax.annotate(\n",
    "                annotation, \n",
    "                xy=(0.05, 0.95), \n",
    "                xycoords='axes fraction',\n",
    "                verticalalignment='top',\n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5)\n",
    "            )\n",
    "        \n",
    "        ax.set_title('Linear Regression Trend', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Trading Day')\n",
    "        ax.set_ylabel('Price (NPR)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ========================================\n",
    "        # Plot 3: Polynomial Trend\n",
    "        # ========================================\n",
    "        ax = axes[1, 0]\n",
    "        ax.plot(x, prices, label='Original Prices', alpha=0.7, linewidth=1)\n",
    "        \n",
    "        if hasattr(self, 'trend_poly'):\n",
    "            ax.plot(\n",
    "                x, \n",
    "                self.trend_poly, \n",
    "                label='Polynomial Trend (deg=2)', \n",
    "                linewidth=2, \n",
    "                color='purple'\n",
    "            )\n",
    "        \n",
    "        ax.set_title('Polynomial Trend', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Trading Day')\n",
    "        ax.set_ylabel('Price (NPR)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        # ========================================\n",
    "        # Plot 4: Comparison\n",
    "        # ========================================\n",
    "        ax = axes[1, 1]\n",
    "        \n",
    "        if hasattr(self, 'trend_sma'):\n",
    "            ax.plot(x[window-1:], self.trend_sma, label='SMA', linewidth=2)\n",
    "        if hasattr(self, 'trend_linear'):\n",
    "            ax.plot(x, self.trend_linear, label='Linear', linewidth=2)\n",
    "        if hasattr(self, 'trend_poly'):\n",
    "            ax.plot(x, self.trend_poly, label='Polynomial', linewidth=2)\n",
    "        \n",
    "        ax.set_title('Trend Comparison', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Trading Day')\n",
    "        ax.set_ylabel('Price (NPR)')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print interpretation guide\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TREND ANALYSIS INTERPRETATION GUIDE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\"\"\n",
    "        \ud83d\udcc8 Understanding Trend Analysis for NEPSE Stocks:\n",
    "        \n",
    "        1. MOVING AVERAGE TREND\n",
    "           - Smooth line following the general price direction\n",
    "           - Good for identifying current trend direction\n",
    "           - Use for: Short to medium-term trend identification\n",
    "        \n",
    "        2. LINEAR REGRESSION TREND\n",
    "           - Single straight line best fit through all data\n",
    "           - Slope shows average daily change\n",
    "           - Use for: Determining long-term trend direction\n",
    "        \n",
    "        3. POLYNOMIAL TREND\n",
    "           - Curved line that can show acceleration/deceleration\n",
    "           - More flexible than linear\n",
    "           - Use for: Stocks with changing growth rates\n",
    "        \n",
    "        \ud83d\udcca For Trading Decisions:\n",
    "           - Positive trend slope: Bullish (consider long positions)\n",
    "           - Negative trend slope: Bearish (consider short or avoid)\n",
    "           - Steep slope: Strong trend (may continue or reverse)\n",
    "           - Flat slope: Sideways market (range trading)\n",
    "        \"\"\")\n",
    "    \n",
    "    def detrend_data(self, method='linear'):\n",
    "        \"\"\"\n",
    "        Remove trend from the data.\n",
    "        \n",
    "        Detrending is important for:\n",
    "        - Analyzing cyclical components\n",
    "        - Studying seasonality\n",
    "        - Making the series stationary\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Method to use ('linear', 'polynomial', 'difference')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.array : Detrended values\n",
    "        \"\"\"\n",
    "        prices = self.data[self.price_column].values\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Detrending using {method} method\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if method == 'linear':\n",
    "            # Subtract linear trend\n",
    "            self.detrended = prices - self.trend_linear\n",
    "            print(\"   Method: Subtract linear trend line\")\n",
    "            \n",
    "        elif method == 'polynomial':\n",
    "            # Subtract polynomial trend\n",
    "            self.detrended = prices - self.trend_poly\n",
    "            print(\"   Method: Subtract polynomial trend line\")\n",
    "            \n",
    "        elif method == 'difference':\n",
    "            # First differencing\n",
    "            # This is the most common detrending method\n",
    "            # New series: y_t' = y_t - y_{t-1}\n",
    "            self.detrended = np.diff(prices)\n",
    "            print(\"   Method: First differencing (y_t - y_{t-1})\")\n",
    "            \n",
    "        else:\n",
    "            raise ValueError(f\"Unknown method: {method}\")\n",
    "        \n",
    "        # Verify detrending effect\n",
    "        print(f\"\\n   Original mean: {prices.mean():.2f}\")\n",
    "        print(f\"   Detrended mean: {self.detrended.mean():.4f} (should be ~0)\")\n",
    "        \n",
    "        return self.detrended\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE WITH NEPSE DATA\n",
    "# ============================================================\n",
    "\n",
    "# Create sample NEPSE-like data with trend\n",
    "def create_trending_nepse_data(n_days=300, trend_type='upward'):\n",
    "    \"\"\"\n",
    "    Create sample NEPSE data with specific trend characteristics.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_days : int\n",
    "        Number of trading days\n",
    "    trend_type : str\n",
    "        Type of trend ('upward', 'downward', 'sideways', 'nonlinear')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame : Sample stock data\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Base price\n",
    "    base = 500\n",
    "    \n",
    "    # Create trend component\n",
    "    if trend_type == 'upward':\n",
    "        trend = np.linspace(0, 100, n_days)  # Linear upward\n",
    "    elif trend_type == 'downward':\n",
    "        trend = np.linspace(0, -80, n_days)  # Linear downward\n",
    "    elif trend_type == 'sideways':\n",
    "        trend = np.zeros(n_days)  # No trend\n",
    "    elif trend_type == 'nonlinear':\n",
    "        trend = 50 * np.sin(np.linspace(0, 3*np.pi, n_days))  # Cyclical\n",
    "    else:\n",
    "        trend = np.zeros(n_days)\n",
    "    \n",
    "    # Add noise\n",
    "    noise = np.random.normal(0, 5, n_days)\n",
    "    \n",
    "    # Price series\n",
    "    close_prices = base + trend + noise\n",
    "    \n",
    "    # Create DataFrame\n",
    "    data = pd.DataFrame({\n",
    "        'S.No': range(1, n_days + 1),\n",
    "        'Symbol': 'ABL',\n",
    "        'Close': close_prices,\n",
    "        'Open': close_prices * (1 + np.random.uniform(-0.01, 0.01, n_days)),\n",
    "        'High': close_prices * (1 + np.abs(np.random.normal(0.01, 0.005, n_days))),\n",
    "        'Low': close_prices * (1 - np.abs(np.random.normal(0.01, 0.005, n_days))),\n",
    "        'Vol': np.random.randint(10000, 100000, n_days)\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Analyze trends in NEPSE data\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TREND ANALYSIS FOR NEPSE STOCK DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create sample data with upward trend\n",
    "nepse_upward = create_trending_nepse_data(n_days=300, trend_type='upward')\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = TrendAnalyzer(nepse_upward, price_column='Close')\n",
    "\n",
    "# Show available methods\n",
    "analyzer.identify_trend_methods()\n",
    "\n",
    "# Apply different methods\n",
    "analyzer.moving_average_trend(window=30)\n",
    "linear_stats = analyzer.linear_regression_trend()\n",
    "analyzer.polynomial_trend(degree=2)\n",
    "\n",
    "# Visualize\n",
    "analyzer.visualize_trends(window=30)\n",
    "\n",
    "# Detrend\n",
    "detrended = analyzer.detrend_data(method='linear')\n",
    "```\n",
    "\n",
    "**Detailed Explanation of Trend Analysis**:\n",
    "\n",
    "The code above implements a comprehensive trend analysis system. Let me explain each component:\n",
    "\n",
    "**1. Moving Average Trend**:\n",
    "- **What it does**: Smooths out short-term fluctuations by averaging prices over a window\n",
    "- **Formula**: SMA_t = (P_t + P_{t-1} + ... + P_{t-n+1}) / n\n",
    "- **Why it works**: By averaging, random noise cancels out, revealing the underlying trend\n",
    "- **NEPSE Application**: A 20-day SMA shows the monthly trend direction\n",
    "\n",
    "**2. Linear Regression Trend**:\n",
    "- **What it does**: Fits a straight line through all data points\n",
    "- **Formula**: y = mx + b, where m is slope\n",
    "- **Interpretation**:\n",
    "  - Positive slope = Upward trend (bullish)\n",
    "  - Negative slope = Downward trend (bearish)\n",
    "  - Slope magnitude = Trend strength\n",
    "- **R-squared**: How much of price movement is explained by trend alone\n",
    "\n",
    "**3. Polynomial Trend**:\n",
    "- **What it does**: Fits a curved line to capture non-linear patterns\n",
    "- **When to use**: When prices are accelerating or decelerating\n",
    "- **Caution**: Higher degrees can overfit (follow noise instead of trend)\n",
    "\n",
    "**4. Detrending**:\n",
    "- **Purpose**: Remove trend to study other components (seasonality, cycles)\n",
    "- **Methods**:\n",
    "  - Subtraction: y_detrended = y - trend\n",
    "  - Differencing: y_detrended = y_t - y_{t-1}\n",
    "- **Application**: Required for certain models that assume stationarity\n",
    "\n",
    "#### **2.2.2 Seasonality**\n",
    "\n",
    "**Definition**: Regular, predictable patterns that repeat over fixed periods (daily, weekly, monthly, quarterly, annually).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "\n",
    "class SeasonalityAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive seasonality analysis for time-series data.\n",
    "    \n",
    "    Seasonality refers to regular, predictable patterns that repeat\n",
    "    over fixed time periods. Understanding seasonality is crucial\n",
    "    for accurate forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        \"\"\"\n",
    "        Initialize the seasonality analyzer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Time-series data\n",
    "        price_column : str\n",
    "            Column containing values to analyze\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "        self.seasonal_component = None\n",
    "        self.deseasonalized = None\n",
    "    \n",
    "    def explain_seasonality_types(self):\n",
    "        \"\"\"\n",
    "        Explain different types of seasonality patterns.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"TYPES OF SEASONALITY IN TIME-SERIES DATA\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        seasonality_types = {\n",
    "            'Daily Seasonality': {\n",
    "                'description': 'Patterns that repeat within a day',\n",
    "                'examples': ['Hourly web traffic', 'Hourly electricity demand'],\n",
    "                'nepse_context': 'Intraday trading patterns (not in daily data)',\n",
    "                'period': '24 hours'\n",
    "            },\n",
    "            'Weekly Seasonality': {\n",
    "                'description': 'Patterns that repeat every week',\n",
    "                'examples': ['Weekend retail sales', 'Monday blues in stocks'],\n",
    "                'nepse_context': 'Day-of-week effects in trading',\n",
    "                'period': '7 days'\n",
    "            },\n",
    "            'Monthly Seasonality': {\n",
    "                'description': 'Patterns within a month',\n",
    "                'examples': ['Month-end salary spending', 'Bill payment cycles'],\n",
    "                'nepse_context': 'Monthly investment flows',\n",
    "                'period': '~30 days'\n",
    "            },\n",
    "            'Quarterly Seasonality': {\n",
    "                'description': 'Patterns every quarter (3 months)',\n",
    "                'examples': ['Quarterly earnings reports', 'Tax payments'],\n",
    "                'nepse_context': 'Quarterly results announcement effects',\n",
    "                'period': '~90 days'\n",
    "            },\n",
    "            'Annual Seasonality': {\n",
    "                'description': 'Patterns that repeat every year',\n",
    "                'examples': ['Holiday shopping', 'Agricultural cycles'],\n",
    "                'nepse_context': 'Fiscal year effects, festival trading',\n",
    "                'period': '365 days'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for name, info in seasonality_types.items():\n",
    "            print(f\"\\n\ud83d\udcc5 {name}\")\n",
    "            print(f\"   Description: {info['description']}\")\n",
    "            print(f\"   Examples: {', '.join(info['examples'])}\")\n",
    "            print(f\"   NEPSE Context: {info['nepse_context']}\")\n",
    "            print(f\"   Period: {info['period']}\")\n",
    "    \n",
    "    def analyze_weekly_seasonality(self):\n",
    "        \"\"\"\n",
    "        Analyze day-of-week patterns in NEPSE data.\n",
    "        \n",
    "        This checks if certain days of the week consistently have\n",
    "        higher or lower returns, which is a form of weekly seasonality.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Day-of-week statistics\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"WEEKLY SEASONALITY ANALYSIS (Day-of-Week Effects)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Calculate returns\n",
    "        self.data['Return'] = self.data[self.price_column].pct_change()\n",
    "        \n",
    "        # If Date column exists, use it; otherwise create synthetic dates\n",
    "        if 'Date' not in self.data.columns:\n",
    "            # Create synthetic dates (assuming business days)\n",
    "            # Nepal trading days: Sunday to Friday (Saturday closed)\n",
    "            dates = pd.date_range(\n",
    "                start='2024-01-01', \n",
    "                periods=len(self.data), \n",
    "                freq='B'  # Business days\n",
    "            )\n",
    "            self.data['Date'] = dates\n",
    "        \n",
    "        self.data['Date'] = pd.to_datetime(self.data['Date'])\n",
    "        self.data['DayOfWeek'] = self.data['Date'].dt.dayofweek\n",
    "        self.data['DayName'] = self.data['Date'].dt.day_name()\n",
    "        \n",
    "        # Calculate statistics by day\n",
    "        day_stats = self.data.groupby('DayName').agg({\n",
    "            'Return': ['mean', 'std', 'count'],\n",
    "            'Vol': ['mean', 'sum']\n",
    "        }).round(6)\n",
    "        \n",
    "        # Flatten column names\n",
    "        day_stats.columns = ['Avg_Return', 'Return_Std', 'Count', 'Avg_Volume', 'Total_Volume']\n",
    "        \n",
    "        # Calculate return in percentage\n",
    "        day_stats['Avg_Return_Pct'] = day_stats['Avg_Return'] * 100\n",
    "        \n",
    "        # Sort by day order\n",
    "        day_order = ['Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday', 'Sunday']\n",
    "        day_stats = day_stats.reindex([d for d in day_order if d in day_stats.index])\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Day-of-Week Statistics:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for day in day_stats.index:\n",
    "            row = day_stats.loc[day]\n",
    "            print(f\"\\n   {day}:\")\n",
    "            print(f\"      Average Return:    {row['Avg_Return_Pct']:+.4f}%\")\n",
    "            print(f\"      Return Std Dev:    {row['Return_Std']*100:.4f}%\")\n",
    "            print(f\"      Trading Days:      {int(row['Count'])}\")\n",
    "            print(f\"      Average Volume:    {int(row['Avg_Volume']):,}\")\n",
    "        \n",
    "        # Test for significance\n",
    "        print(\"\\n\ud83d\udcc8 Interpretation:\")\n",
    "        \n",
    "        best_day = day_stats['Avg_Return_Pct'].idxmax()\n",
    "        worst_day = day_stats['Avg_Return_Pct'].idxmin()\n",
    "        \n",
    "        print(f\"   Best performing day:  {best_day} ({day_stats.loc[best_day, 'Avg_Return_Pct']:+.4f}%)\")\n",
    "        print(f\"   Worst performing day: {worst_day} ({day_stats.loc[worst_day, 'Avg_Return_Pct']:+.4f}%)\")\n",
    "        \n",
    "        # Day-of-week effect significance\n",
    "        # Using ANOVA-like comparison\n",
    "        from scipy import stats\n",
    "        \n",
    "        day_returns = {}\n",
    "        for day in day_stats.index:\n",
    "            day_returns[day] = self.data[self.data['DayName'] == day]['Return'].dropna().values\n",
    "        \n",
    "        # Perform one-way ANOVA\n",
    "        f_stat, p_value = stats.f_oneway(*[day_returns[d] for d in day_returns.keys()])\n",
    "        \n",
    "        print(f\"\\n   ANOVA F-statistic: {f_stat:.4f}\")\n",
    "        print(f\"   ANOVA p-value:     {p_value:.4f}\")\n",
    "        \n",
    "        if p_value < 0.05:\n",
    "            print(\"   \u2713 Day-of-week effect is statistically significant\")\n",
    "        else:\n",
    "            print(\"   \u26a0 Day-of-week effect is NOT statistically significant\")\n",
    "        \n",
    "        self.weekly_stats = day_stats\n",
    "        return day_stats\n",
    "    \n",
    "    def analyze_monthly_seasonality(self):\n",
    "        \"\"\"\n",
    "        Analyze monthly patterns in the data.\n",
    "        \n",
    "        This checks if certain months consistently perform better\n",
    "        or worse, which could indicate annual seasonality patterns.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"MONTHLY SEASONALITY ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if 'Date' not in self.data.columns:\n",
    "            print(\"\u26a0 Date column required for monthly analysis\")\n",
    "            return None\n",
    "        \n",
    "        self.data['Month'] = self.data['Date'].dt.month\n",
    "        self.data['MonthName'] = self.data['Date'].dt.month_name()\n",
    "        \n",
    "        # Monthly statistics\n",
    "        month_stats = self.data.groupby('Month').agg({\n",
    "            'Return': ['mean', 'std', 'count']\n",
    "        }).round(6)\n",
    "        \n",
    "        month_stats.columns = ['Avg_Return', 'Return_Std', 'Count']\n",
    "        month_stats['Avg_Return_Pct'] = month_stats['Avg_Return'] * 100\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Monthly Statistics:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        months = ['January', 'February', 'March', 'April', 'May', 'June',\n",
    "                  'July', 'August', 'September', 'October', 'November', 'December']\n",
    "        \n",
    "        for i, month in enumerate(months, 1):\n",
    "            if i in month_stats.index:\n",
    "                row = month_stats.loc[i]\n",
    "                bar = \"\u2588\" * int(row['Avg_Return_Pct'] * 50) if row['Avg_Return_Pct'] > 0 else \"\"\n",
    "                bar += \"\u2591\" * int(abs(row['Avg_Return_Pct']) * 50) if row['Avg_Return_Pct'] < 0 else \"\"\n",
    "                print(f\"   {month:12s}: {row['Avg_Return_Pct']:+.4f}% {bar}\")\n",
    "        \n",
    "        # Nepal-specific monthly patterns\n",
    "        print(\"\\n\ud83d\udcc5 Nepal-Specific Seasonal Patterns:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(\"\"\"\n",
    "        Key periods affecting NEPSE:\n",
    "        \n",
    "        \u2022 Mid-July to Mid-August (Shrawan/Bhadra):\n",
    "          - Many companies have AGMs\n",
    "          - Dividend announcements\n",
    "          - Often bullish period\n",
    "        \n",
    "        \u2022 October-November (Dashain/Tihar):\n",
    "          - Major festivals\n",
    "          - Reduced trading activity\n",
    "          - Often volatile\n",
    "        \n",
    "        \u2022 April-May (Year-end approaching):\n",
    "          - Book closing for dividends\n",
    "          - Rights issue announcements\n",
    "          - Increased activity\n",
    "        \n",
    "        \u2022 July (Fiscal Year End):\n",
    "          - Fiscal year ends mid-July in Nepal\n",
    "          - Tax-related selling\n",
    "          - Portfolio rebalancing\n",
    "        \"\"\")\n",
    "        \n",
    "        return month_stats\n",
    "    \n",
    "    def decompose_time_series(self, period=None, model='additive'):\n",
    "        \"\"\"\n",
    "        Decompose time-series into trend, seasonal, and residual components.\n",
    "        \n",
    "        This is a fundamental technique that separates a time series into:\n",
    "        - Trend: Long-term direction\n",
    "        - Seasonal: Repeating patterns\n",
    "        - Residual: Random fluctuations\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        period : int, optional\n",
    "            Period of seasonality (auto-detected if None)\n",
    "        model : str\n",
    "            'additive' or 'multiplicative'\n",
    "            - Additive: y = trend + seasonal + residual\n",
    "            - Multiplicative: y = trend \u00d7 seasonal \u00d7 residual\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        DecomposeResult : Object containing components\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"TIME-SERIES DECOMPOSITION ({model.upper()} MODEL)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Get price series\n",
    "        prices = self.data[self.price_column].dropna()\n",
    "        \n",
    "        # Auto-detect period if not provided\n",
    "        if period is None:\n",
    "            # Default to weekly seasonality (5 trading days)\n",
    "            period = 5\n",
    "        \n",
    "        print(f\"\\n   Decomposition Parameters:\")\n",
    "        print(f\"      Model:  {model}\")\n",
    "        print(f\"      Period: {period} observations\")\n",
    "        print(f\"      Data points: {len(prices)}\")\n",
    "        \n",
    "        # Check if we have enough data\n",
    "        if len(prices) < 2 * period:\n",
    "            print(f\"\\n   \u26a0 Warning: Need at least {2 * period} observations\")\n",
    "            print(f\"   Current: {len(prices)}\")\n",
    "            return None\n",
    "        \n",
    "        # Perform decomposition\n",
    "        # Additive: y = T + S + R\n",
    "        # Multiplicative: y = T \u00d7 S \u00d7 R\n",
    "        \n",
    "        decomposition = seasonal_decompose(\n",
    "            prices, \n",
    "            model=model, \n",
    "            period=period,\n",
    "            extrapolate_trend='freq'\n",
    "        )\n",
    "        \n",
    "        # Store components\n",
    "        self.trend_component = decomposition.trend\n",
    "        self.seasonal_component = decomposition.seasonal\n",
    "        self.residual_component = decomposition.resid\n",
    "        \n",
    "        # Print component statistics\n",
    "        print(\"\\n   Component Statistics:\")\n",
    "        print(f\"      Trend range:     {self.trend_component.min():.2f} to {self.trend_component.max():.2f}\")\n",
    "        print(f\"      Seasonal range:  {self.seasonal_component.min():.2f} to {self.seasonal_component.max():.2f}\")\n",
    "        print(f\"      Residual range:  {self.residual_component.min():.2f} to {self.residual_component.max():.2f}\")\n",
    "        \n",
    "        # Variance explained\n",
    "        total_var = prices.var()\n",
    "        trend_var = self.trend_component.dropna().var()\n",
    "        seasonal_var = self.seasonal_component.var()\n",
    "        resid_var = self.residual_component.dropna().var()\n",
    "        \n",
    "        print(\"\\n   Variance Explained:\")\n",
    "        print(f\"      Trend:     {(trend_var/total_var)*100:.1f}%\")\n",
    "        print(f\"      Seasonal:  {(seasonal_var/total_var)*100:.1f}%\")\n",
    "        print(f\"      Residual:  {(resid_var/total_var)*100:.1f}%\")\n",
    "        \n",
    "        return decomposition\n",
    "    \n",
    "    def visualize_decomposition(self, decomposition):\n",
    "        \"\"\"\n",
    "        Visualize the decomposed components.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        decomposition : DecomposeResult\n",
    "            Result from seasonal_decompose\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(4, 1, figsize=(14, 12))\n",
    "        \n",
    "        prices = self.data[self.price_column].dropna()\n",
    "        \n",
    "        # Original\n",
    "        axes[0].plot(prices.values, linewidth=1)\n",
    "        axes[0].set_title('Original Time Series', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_ylabel('Price (NPR)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Trend\n",
    "        axes[1].plot(decomposition.trend, linewidth=2, color='blue')\n",
    "        axes[1].set_title('Trend Component', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_ylabel('Price (NPR)')\n",
    "        axes[1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Seasonal\n",
    "        axes[2].plot(decomposition.seasonal, linewidth=1, color='green')\n",
    "        axes[2].set_title('Seasonal Component', fontsize=12, fontweight='bold')\n",
    "        axes[2].set_ylabel('Effect (NPR)')\n",
    "        axes[2].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[2].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Residual\n",
    "        axes[3].plot(decomposition.resid, linewidth=1, color='red', alpha=0.7)\n",
    "        axes[3].set_title('Residual Component (Noise)', fontsize=12, fontweight='bold')\n",
    "        axes[3].set_ylabel('Residual (NPR)')\n",
    "        axes[3].set_xlabel('Time Index')\n",
    "        axes[3].axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        axes[3].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Interpretation guide\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"DECOMPOSITION INTERPRETATION GUIDE\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Understanding the Components:\n",
    "        \n",
    "        1. ORIGINAL SERIES\n",
    "           - Raw price data as recorded\n",
    "           - Contains all patterns mixed together\n",
    "        \n",
    "        2. TREND COMPONENT\n",
    "           - Long-term direction of the series\n",
    "           - Smoothed to remove short-term fluctuations\n",
    "           - Use for: Identifying overall market direction\n",
    "        \n",
    "        3. SEASONAL COMPONENT\n",
    "           - Repeating patterns at fixed intervals\n",
    "           - Value shows deviation from trend\n",
    "           - Positive: Above trend that period\n",
    "           - Negative: Below trend that period\n",
    "           - Use for: Timing entries/exits based on patterns\n",
    "        \n",
    "        4. RESIDUAL COMPONENT\n",
    "           - What remains after removing trend and seasonality\n",
    "           - Should appear random (white noise)\n",
    "           - Large residuals may indicate anomalies\n",
    "           - Use for: Anomaly detection, risk assessment\n",
    "        \n",
    "        \ud83d\udca1 For NEPSE Trading:\n",
    "           - Strong trend: Follow the trend direction\n",
    "           - Strong seasonality: Time trades with seasonal patterns\n",
    "           - High residual variance: Higher uncertainty/risk\n",
    "        \"\"\")\n",
    "    \n",
    "    def create_seasonal_features(self):\n",
    "        \"\"\"\n",
    "        Create features based on seasonality for machine learning.\n",
    "        \n",
    "        Seasonal features can help models learn cyclical patterns.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Data with seasonal features added\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"CREATING SEASONAL FEATURES FOR ML MODELS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if 'Date' not in self.data.columns:\n",
    "            print(\"\u26a0 Date column required\")\n",
    "            return None\n",
    "        \n",
    "        # ========================================\n",
    "        # Cyclic Encoding for Periodic Features\n",
    "        # ========================================\n",
    "        # Instead of using raw values (1-12 for months), we use\n",
    "        # sine and cosine to capture the cyclic nature\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Cyclic Encoding Explanation:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(\"\"\"\n",
    "        Problem: Month 12 (December) and month 1 (January) are adjacent,\n",
    "        but raw values 12 and 1 are far apart numerically.\n",
    "        \n",
    "        Solution: Use sine and cosine encoding:\n",
    "        \n",
    "        sin(2\u03c0 \u00d7 month / 12)  and  cos(2\u03c0 \u00d7 month / 12)\n",
    "        \n",
    "        This creates a circular representation where December and January\n",
    "        are close together, as they should be.\n",
    "        \"\"\")\n",
    "        \n",
    "        # Day of week encoding\n",
    "        self.data['DayOfWeek_sin'] = np.sin(2 * np.pi * self.data['DayOfWeek'] / 7)\n",
    "        self.data['DayOfWeek_cos'] = np.cos(2 * np.pi * self.data['DayOfWeek'] / 7)\n",
    "        \n",
    "        # Month encoding\n",
    "        self.data['Month_sin'] = np.sin(2 * np.pi * self.data['Month'] / 12)\n",
    "        self.data['Month_cos'] = np.cos(2 * np.pi * self.data['Month'] / 12)\n",
    "        \n",
    "        # Quarter encoding\n",
    "        self.data['Quarter'] = self.data['Date'].dt.quarter\n",
    "        self.data['Quarter_sin'] = np.sin(2 * np.pi * self.data['Quarter'] / 4)\n",
    "        self.data['Quarter_cos'] = np.cos(2 * np.pi * self.data['Quarter'] / 4)\n",
    "        \n",
    "        # Day of month (for monthly patterns)\n",
    "        self.data['DayOfMonth'] = self.data['Date'].dt.day\n",
    "        self.data['DayOfMonth_sin'] = np.sin(2 * np.pi * self.data['DayOfMonth'] / 31)\n",
    "        self.data['DayOfMonth_cos'] = np.cos(2 * np.pi * self.data['DayOfMonth'] / 31)\n",
    "        \n",
    "        # Week of year\n",
    "        self.data['WeekOfYear'] = self.data['Date'].dt.isocalendar().week\n",
    "        self.data['Week_sin'] = np.sin(2 * np.pi * self.data['WeekOfYear'] / 52)\n",
    "        self.data['Week_cos'] = np.cos(2 * np.pi * self.data['WeekOfYear'] / 52)\n",
    "        \n",
    "        print(\"\\n\u2713 Seasonal features created:\")\n",
    "        print(\"   - DayOfWeek (sin, cos)\")\n",
    "        print(\"   - Month (sin, cos)\")\n",
    "        print(\"   - Quarter (sin, cos)\")\n",
    "        print(\"   - DayOfMonth (sin, cos)\")\n",
    "        print(\"   - WeekOfYear (sin, cos)\")\n",
    "        \n",
    "        # Show sample\n",
    "        print(\"\\n\ud83d\udccb Sample of seasonal features:\")\n",
    "        seasonal_cols = [col for col in self.data.columns if 'sin' in col or 'cos' in col]\n",
    "        print(self.data[['Date'] + seasonal_cols[:6]].head())\n",
    "        \n",
    "        return self.data\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "# Create sample data with seasonality\n",
    "def create_seasonal_nepse_data(n_days=500):\n",
    "    \"\"\"\n",
    "    Create NEPSE-like data with realistic seasonal patterns.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_days, freq='B')\n",
    "    \n",
    "    # Base price with trend\n",
    "    trend = np.linspace(500, 600, n_days)\n",
    "    \n",
    "    # Weekly seasonality (e.g., Monday effect)\n",
    "    day_of_week = dates.dayofweek\n",
    "    weekly_pattern = np.where(day_of_week == 0, -2,  # Monday dip\n",
    "                       np.where(day_of_week == 4, 1.5,  # Friday up\n",
    "                       np.where(day_of_week == 5, 0,  # Saturday closed\n",
    "                       np.zeros(n_days))))  # Other days\n",
    "    \n",
    "    # Monthly seasonality\n",
    "    month = dates.month\n",
    "    monthly_pattern = 3 * np.sin(2 * np.pi * month / 12)  # Annual cycle\n",
    "    \n",
    "    # Random noise\n",
    "    noise = np.random.normal(0, 5, n_days)\n",
    "    \n",
    "    # Combine\n",
    "    close_prices = trend + weekly_pattern + monthly_pattern + noise\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'S.No': range(1, n_days + 1),\n",
    "        'Symbol': 'ABL',\n",
    "        'Close': close_prices,\n",
    "        'Open': close_prices * (1 + np.random.uniform(-0.01, 0.01, n_days)),\n",
    "        'High': close_prices * (1 + np.abs(np.random.normal(0.01, 0.005, n_days))),\n",
    "        'Low': close_prices * (1 - np.abs(np.random.normal(0.01, 0.005, n_days))),\n",
    "        'Vol': np.random.randint(10000, 100000, n_days)\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "# Run seasonality analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"SEASONALITY ANALYSIS FOR NEPSE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "seasonal_data = create_seasonal_nepse_data(n_days=500)\n",
    "seasonal_analyzer = SeasonalityAnalyzer(seasonal_data, price_column='Close')\n",
    "\n",
    "# Explain seasonality types\n",
    "seasonal_analyzer.explain_seasonality_types()\n",
    "\n",
    "# Analyze weekly seasonality\n",
    "weekly_stats = seasonal_analyzer.analyze_weekly_seasonality()\n",
    "\n",
    "# Analyze monthly seasonality\n",
    "monthly_stats = seasonal_analyzer.analyze_monthly_seasonality()\n",
    "\n",
    "# Decompose time series\n",
    "decomposition = seasonal_analyzer.decompose_time_series(period=5, model='additive')\n",
    "\n",
    "# Visualize\n",
    "if decomposition:\n",
    "    seasonal_analyzer.visualize_decomposition(decomposition)\n",
    "\n",
    "# Create seasonal features for ML\n",
    "seasonal_features = seasonal_analyzer.create_seasonal_features()\n",
    "```\n",
    "\n",
    "**Detailed Explanation of Seasonality Analysis**:\n",
    "\n",
    "The seasonality analyzer above provides comprehensive tools for understanding repeating patterns. Key concepts:\n",
    "\n",
    "**1. Types of Seasonality**:\n",
    "- **Weekly**: Day-of-week effects (e.g., \"Monday effect\" in stocks)\n",
    "- **Monthly**: Patterns within months (salary cycles, bill payments)\n",
    "- **Quarterly**: Earnings announcements, fiscal quarters\n",
    "- **Annual**: Yearly cycles (festivals, fiscal year end)\n",
    "\n",
    "**2. Decomposition Methods**:\n",
    "- **Additive**: y = Trend + Seasonal + Residual\n",
    "  - Use when seasonal variation is constant over time\n",
    "- **Multiplicative**: y = Trend \u00d7 Seasonal \u00d7 Residual\n",
    "  - Use when seasonal variation grows with trend\n",
    "\n",
    "**3. Cyclic Encoding**:\n",
    "- Raw numeric values (1-12 for months) don't capture that December and January are adjacent\n",
    "- Sine/cosine encoding preserves cyclic nature\n",
    "- Formula: sin(2\u03c0 \u00d7 value / period) and cos(2\u03c0 \u00d7 value / period)\n",
    "\n",
    "**NEPSE-Specific Seasonality**:\n",
    "- **Dashain/Tihar**: Major festivals in Oct-Nov affecting trading\n",
    "- **Fiscal Year End (mid-July)**: Tax-related adjustments\n",
    "- **AGM Season**: July-August when many companies hold annual meetings\n",
    "- **Book Closure**: April-May for dividend declarations\n",
    "\n",
    "#### **2.2.3 Cyclicality**\n",
    "\n",
    "**Definition**: Longer-term fluctuations that don't have a fixed period, often tied to economic or business cycles.\n",
    "\n",
    "```python\n",
    "class CyclicalityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze cyclical patterns in time-series data.\n",
    "    \n",
    "    Unlike seasonality (fixed period), cyclicality refers to\n",
    "    longer-term fluctuations without a fixed frequency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "    \n",
    "    def explain_cyclicality_vs_seasonality(self):\n",
    "        \"\"\"\n",
    "        Explain the difference between cyclicality and seasonality.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"CYCLICALITY VS SEASONALITY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                        SEASONALITY                               \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502 \u2022 Fixed, known period (weekly, monthly, annually)                \u2502\n",
    "        \u2502 \u2022 Predictable timing                                             \u2502\n",
    "        \u2502 \u2022 Example: Holiday shopping spike every December                 \u2502\n",
    "        \u2502 \u2022 Example: Quarterly earnings announcements                      \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Pattern: \u25b2 \u25bc \u25b2 \u25bc \u25b2 \u25bc \u25b2 \u25bc \u25b2 \u25bc (Fixed frequency)                \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502 NEPSE Example: Dashain festival effects every year               \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                        CYCLICALITY                               \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502 \u2022 Variable, unknown period                                       \u2502\n",
    "        \u2502 \u2022 Tied to economic/business cycles                               \u2502\n",
    "        \u2502 \u2022 Example: Business cycles (3-7 years)                           \u2502\n",
    "        \u2502 \u2022 Example: Bull/bear market cycles                               \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Pattern: \u25b2\u25b2\u25b2\u2500\u2500\u25bc\u25bc\u25bc\u2500\u2500\u25b2\u25b2\u25b2\u2500\u2500\u25bc\u25bc (Variable frequency)               \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502 NEPSE Example: Market bull/bear cycles lasting months to years   \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \"\"\")\n",
    "    \n",
    "    def detect_cycles_using_spectral_analysis(self):\n",
    "        \"\"\"\n",
    "        Detect cyclical patterns using spectral analysis (FFT).\n",
    "        \n",
    "        Fast Fourier Transform identifies dominant frequencies\n",
    "        in the time series, helping detect cycles.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dominant cycle periods and their strengths\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"CYCLE DETECTION USING SPECTRAL ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        from scipy import signal\n",
    "        from scipy.fft import fft, fftfreq\n",
    "        \n",
    "        prices = self.data[self.price_column].dropna().values\n",
    "        \n",
    "        # Remove trend first (cycles are easier to detect in detrended data)\n",
    "        # Using differencing for detrending\n",
    "        detrended = np.diff(prices)\n",
    "        \n",
    "        # Apply FFT\n",
    "        n = len(detrended)\n",
    "        \n",
    "        # Compute FFT\n",
    "        fft_values = fft(detrended)\n",
    "        \n",
    "        # Compute frequencies\n",
    "        frequencies = fftfreq(n)\n",
    "        \n",
    "        # Get power spectrum (magnitude squared)\n",
    "        power = np.abs(fft_values) ** 2\n",
    "        \n",
    "        # Only look at positive frequencies\n",
    "        positive_freq_mask = frequencies > 0\n",
    "        frequencies = frequencies[positive_freq_mask]\n",
    "        power = power[positive_freq_mask]\n",
    "        \n",
    "        # Convert frequency to period (in days)\n",
    "        periods = 1 / frequencies\n",
    "        \n",
    "        # Find top dominant periods\n",
    "        top_indices = np.argsort(power)[-10:][::-1]\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Top 10 Dominant Cycles:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Period (Days)':<15} {'Period (Weeks)':<15} {'Power':<15}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        dominant_cycles = []\n",
    "        for idx in top_indices:\n",
    "            period_days = periods[idx]\n",
    "            period_weeks = period_days / 5  # Trading days\n",
    "            cycle_power = power[idx]\n",
    "            \n",
    "            # Filter out very long or very short cycles\n",
    "            if 5 < period_days < n / 2:\n",
    "                print(f\"{period_days:<15.1f} {period_weeks:<15.1f} {cycle_power:<15.1f}\")\n",
    "                dominant_cycles.append({\n",
    "                    'period_days': period_days,\n",
    "                    'period_weeks': period_weeks,\n",
    "                    'power': cycle_power\n",
    "                })\n",
    "        \n",
    "        self.dominant_cycles = dominant_cycles\n",
    "        \n",
    "        # Interpretation\n",
    "        print(\"\\n\ud83d\udcc8 Cycle Interpretation for NEPSE:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if dominant_cycles:\n",
    "            # Check for common cycle patterns\n",
    "            for cycle in dominant_cycles[:3]:\n",
    "                period = cycle['period_days']\n",
    "                \n",
    "                if 4 <= period <= 6:\n",
    "                    print(f\"   \u2022 {period:.0f}-day cycle: Weekly pattern detected\")\n",
    "                elif 20 <= period <= 25:\n",
    "                    print(f\"   \u2022 {period:.0f}-day cycle: Monthly pattern detected\")\n",
    "                elif 60 <= period <= 70:\n",
    "                    print(f\"   \u2022 {period:.0f}-day cycle: Quarterly pattern detected\")\n",
    "                elif 240 <= period <= 260:\n",
    "                    print(f\"   \u2022 {period:.0f}-day cycle: Annual pattern detected\")\n",
    "                else:\n",
    "                    print(f\"   \u2022 {period:.0f}-day cycle: Custom cycle detected\")\n",
    "        \n",
    "        return dominant_cycles\n",
    "    \n",
    "    def identify_market_regimes(self, window=50):\n",
    "        \"\"\"\n",
    "        Identify market regimes (bull/bear markets) as cyclical behavior.\n",
    "        \n",
    "        Market regimes represent longer-term cyclical patterns\n",
    "        in stock markets.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        window : int\n",
    "            Window for regime identification\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Data with regime labels\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"MARKET REGIME IDENTIFICATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        prices = self.data[self.price_column].values\n",
    "        \n",
    "        # Calculate rolling returns\n",
    "        returns = pd.Series(prices).pct_change(window)\n",
    "        \n",
    "        # Define regimes based on rolling returns\n",
    "        # Bull market: Strong positive returns\n",
    "        # Bear market: Strong negative returns\n",
    "        # Sideways: Near-zero returns\n",
    "        \n",
    "        def classify_regime(ret):\n",
    "            if pd.isna(ret):\n",
    "                return 'Unknown'\n",
    "            elif ret > 0.05:  # >5% return over window\n",
    "                return 'Bull'\n",
    "            elif ret < -0.05:  # <-5% return over window\n",
    "                return 'Bear'\n",
    "            else:\n",
    "                return 'Sideways'\n",
    "        \n",
    "        self.data['Regime'] = returns.apply(classify_regime)\n",
    "        \n",
    "        # Count regimes\n",
    "        regime_counts = self.data['Regime'].value_counts()\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Market Regimes (based on {window}-day returns):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for regime, count in regime_counts.items():\n",
    "            pct = count / len(self.data) * 100\n",
    "            bar = \"\u2588\" * int(pct / 2)\n",
    "            print(f\"   {regime:12s}: {count:4d} days ({pct:5.1f}%) {bar}\")\n",
    "        \n",
    "        # Calculate regime statistics\n",
    "        print(\"\\n\ud83d\udcc8 Regime Statistics:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for regime in ['Bull', 'Bear', 'Sideways']:\n",
    "            regime_data = self.data[self.data['Regime'] == regime]\n",
    "            if len(regime_data) > 0:\n",
    "                avg_return = regime_data[self.price_column].pct_change().mean() * 100\n",
    "                volatility = regime_data[self.price_column].pct_change().std() * 100\n",
    "                \n",
    "                print(f\"\\n   {regime}:\")\n",
    "                print(f\"      Average daily return: {avg_return:+.4f}%\")\n",
    "                print(f\"      Daily volatility:     {volatility:.4f}%\")\n",
    "        \n",
    "        return self.data\n",
    "    \n",
    "    def visualize_regimes(self):\n",
    "        \"\"\"\n",
    "        Visualize market regimes over time.\n",
    "        \"\"\"\n",
    "        if 'Regime' not in self.data.columns:\n",
    "            print(\"\u26a0 Run identify_market_regimes() first\")\n",
    "            return\n",
    "        \n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        prices = self.data[self.price_column].values\n",
    "        \n",
    "        # Plot prices\n",
    "        ax.plot(prices, linewidth=1, color='black', alpha=0.7)\n",
    "        \n",
    "        # Color background by regime\n",
    "        colors = {'Bull': 'lightgreen', 'Bear': 'lightcoral', 'Sideways': 'lightgray', 'Unknown': 'white'}\n",
    "        \n",
    "        current_regime = None\n",
    "        start_idx = 0\n",
    "        \n",
    "        for i, regime in enumerate(self.data['Regime']):\n",
    "            if regime != current_regime:\n",
    "                if current_regime is not None:\n",
    "                    ax.axvspan(start_idx, i-1, alpha=0.3, color=colors.get(current_regime, 'white'))\n",
    "                current_regime = regime\n",
    "                start_idx = i\n",
    "        \n",
    "        # Last segment\n",
    "        if current_regime is not None:\n",
    "            ax.axvspan(start_idx, len(self.data)-1, alpha=0.3, color=colors.get(current_regime, 'white'))\n",
    "        \n",
    "        ax.set_title('Market Regimes Over Time', fontsize=12, fontweight='bold')\n",
    "        ax.set_xlabel('Trading Day')\n",
    "        ax.set_ylabel('Price (NPR)')\n",
    "        \n",
    "        # Legend\n",
    "        from matplotlib.patches import Patch\n",
    "        legend_elements = [Patch(facecolor='lightgreen', label='Bull', alpha=0.3),\n",
    "                          Patch(facecolor='lightcoral', label='Bear', alpha=0.3),\n",
    "                          Patch(facecolor='lightgray', label='Sideways', alpha=0.3)]\n",
    "        ax.legend(handles=legend_elements, loc='upper left')\n",
    "        \n",
    "        ax.grid(True, alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# Run cyclicality analysis\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CYCLICALITY ANALYSIS FOR NEPSE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "cyclicality_analyzer = CyclicalityAnalyzer(seasonal_data, price_column='Close')\n",
    "cyclicality_analyzer.explain_cyclicality_vs_seasonality()\n",
    "\n",
    "# Detect cycles\n",
    "cycles = cyclicality_analyzer.detect_cycles_using_spectral_analysis()\n",
    "\n",
    "# Identify market regimes\n",
    "regime_data = cyclicality_analyzer.identify_market_regimes(window=50)\n",
    "\n",
    "# Visualize regimes\n",
    "cyclicality_analyzer.visualize_regimes()\n",
    "```\n",
    "\n",
    "#### **2.2.4 Irregularity (Noise/Residual)**\n",
    "\n",
    "**Definition**: Random, unpredictable fluctuations that remain after removing trend, seasonality, and cyclical components.\n",
    "\n",
    "```python\n",
    "class IrregularityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze irregular/random components in time-series data.\n",
    "    \n",
    "    Irregularity represents the unpredictable noise that remains\n",
    "    after removing systematic patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "    \n",
    "    def explain_noise_types(self):\n",
    "        \"\"\"\n",
    "        Explain different types of noise in time-series.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"TYPES OF IRREGULARITY IN TIME-SERIES DATA\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                     WHITE NOISE                                  \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502 \u2022 Completely random, no pattern                                  \u2502\n",
    "        \u2502 \u2022 Zero autocorrelation at all lags                               \u2502\n",
    "        \u2502 \u2022 Constant mean and variance                                     \u2502\n",
    "        \u2502 \u2022 Desirable residual after modeling                              \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Pattern: \u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502 (Random, no structure)            \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                     RED NOISE                                    \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502 \u2022 Some autocorrelation at short lags                             \u2502\n",
    "        \u2502 \u2022 Common in financial returns                                    \u2502\n",
    "        \u2502 \u2022 Indicates model hasn't captured all patterns                   \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Pattern: \u2581\u2582\u2583\u2584\u2585\u2586\u2587\u2588\u2587\u2586\u2585\u2584\u2583\u2582\u2581 (Some persistence)                  \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                HETEROSCEDASTIC NOISE                             \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502 \u2022 Variance changes over time                                     \u2502\n",
    "        \u2502 \u2022 Common in financial data (volatility clustering)               \u2502\n",
    "        \u2502 \u2022 Requires special handling (GARCH models)                       \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Pattern: \u2502\u2502\u2502\u2502\u2502\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502\u2502 (Variable spread)    \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \"\"\")\n",
    "    \n",
    "    def analyze_residuals(self, residuals):\n",
    "        \"\"\"\n",
    "        Analyze the residual component for randomness.\n",
    "        \n",
    "        A good model should have residuals that are random (white noise).\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        residuals : np.array\n",
    "            Residual values from decomposition or model\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Analysis results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"RESIDUAL ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        residuals = residuals.dropna().values\n",
    "        \n",
    "        # ========================================\n",
    "        # 1. Basic Statistics\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca Basic Statistics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Mean:      {residuals.mean():.6f} (should be ~0)\")\n",
    "        print(f\"   Std Dev:   {residuals.std():.4f}\")\n",
    "        print(f\"   Min:       {residuals.min():.4f}\")\n",
    "        print(f\"   Max:       {residuals.max():.4f}\")\n",
    "        print(f\"   Skewness:  {pd.Series(residuals).skew():.4f} (should be ~0)\")\n",
    "        print(f\"   Kurtosis:  {pd.Series(residuals).kurtosis():.4f} (should be ~0)\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 2. Autocorrelation Test (Ljung-Box)\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca Autocorrelation Test (Ljung-Box):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "        \n",
    "        # Test for autocorrelation at multiple lags\n",
    "        lags = [5, 10, 20]\n",
    "        lb_results = acorr_ljungbox(residuals, lags=lags)\n",
    "        \n",
    "        for lag in lags:\n",
    "            p_value = lb_results.loc[lag, 'lb_pvalue']\n",
    "            status = \"\u2713 No autocorrelation\" if p_value > 0.05 else \"\u26a0 Autocorrelation detected\"\n",
    "            print(f\"   Lag {lag}: p-value = {p_value:.4f} {status}\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 3. Normality Test\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca Normality Test (Jarque-Bera):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        from scipy.stats import jarque_bera\n",
    "        \n",
    "        jb_stat, jb_pvalue = jarque_bera(residuals)\n",
    "        \n",
    "        print(f\"   Jarque-Bera statistic: {jb_stat:.4f}\")\n",
    "        print(f\"   p-value:               {jb_pvalue:.4f}\")\n",
    "        \n",
    "        if jb_pvalue > 0.05:\n",
    "            print(\"   \u2713 Residuals appear normally distributed\")\n",
    "        else:\n",
    "            print(\"   \u26a0 Residuals may not be normally distributed\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 4. Heteroscedasticity Test\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca Heteroscedasticity Test:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Split residuals into two halves and compare variances\n",
    "        mid = len(residuals) // 2\n",
    "        first_half_var = residuals[:mid].var()\n",
    "        second_half_var = residuals[mid:].var()\n",
    "        \n",
    "        variance_ratio = second_half_var / first_half_var\n",
    "        \n",
    "        print(f\"   First half variance:  {first_half_var:.4f}\")\n",
    "        print(f\"   Second half variance: {second_half_var:.4f}\")\n",
    "        print(f\"   Variance ratio:       {variance_ratio:.4f}\")\n",
    "        \n",
    "        if 0.5 < variance_ratio < 2.0:\n",
    "            print(\"   \u2713 Variance appears relatively constant (homoscedastic)\")\n",
    "        else:\n",
    "            print(\"   \u26a0 Variance appears to change over time (heteroscedastic)\")\n",
    "        \n",
    "        return {\n",
    "            'mean': residuals.mean(),\n",
    "            'std': residuals.std(),\n",
    "            'is_random': all(lb_results['lb_pvalue'] > 0.05),\n",
    "            'is_normal': jb_pvalue > 0.05,\n",
    "            'is_homoscedastic': 0.5 < variance_ratio < 2.0\n",
    "        }\n",
    "    \n",
    "    def check_model_quality(self, residuals):\n",
    "        \"\"\"\n",
    "        Check if residuals indicate a good model fit.\n",
    "        \n",
    "        Good residuals should be:\n",
    "        1. Random (no autocorrelation)\n",
    "        2. Zero mean\n",
    "        3. Constant variance\n",
    "        4. Ideally normally distributed\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"MODEL QUALITY ASSESSMENT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        results = self.analyze_residuals(pd.Series(residuals))\n",
    "        \n",
    "        print(\"\\n\ud83d\udccb Model Quality Checklist:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        checks = [\n",
    "            (\"Random residuals (no pattern)\", results['is_random']),\n",
    "            (\"Zero mean residuals\", abs(results['mean']) < 0.01),\n",
    "            (\"Constant variance\", results['is_homoscedastic']),\n",
    "            (\"Normal distribution\", results['is_normal'])\n",
    "        ]\n",
    "        \n",
    "        all_passed = True\n",
    "        for check_name, passed in checks:\n",
    "            status = \"\u2713 PASS\" if passed else \"\u2717 FAIL\"\n",
    "            print(f\"   {check_name:<35} {status}\")\n",
    "            all_passed = all_passed and passed\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 50)\n",
    "        if all_passed:\n",
    "            print(\"   \ud83c\udf89 EXCELLENT! Model residuals meet all quality criteria.\")\n",
    "        else:\n",
    "            print(\"   \u26a0 Model may need improvement based on residual analysis.\")\n",
    "        \n",
    "        return all_passed\n",
    "\n",
    "\n",
    "# Analyze irregularity\n",
    "irregularity_analyzer = IrregularityAnalyzer(seasonal_data, price_column='Close')\n",
    "irregularity_analyzer.explain_noise_types()\n",
    "\n",
    "# Use residuals from decomposition\n",
    "if hasattr(seasonal_analyzer, 'residual_component'):\n",
    "    residual_results = irregularity_analyzer.analyze_residuals(seasonal_analyzer.residual_component)\n",
    "    quality = irregularity_analyzer.check_model_quality(seasonal_analyzer.residual_component)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2.3 Time-Series Properties**\n",
    "\n",
    "#### **2.3.1 Stationarity**\n",
    "\n",
    "**Definition**: A time series is stationary if its statistical properties (mean, variance, autocorrelation) remain constant over time.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller, kpss\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "class StationarityAnalyzer:\n",
    "    \"\"\"\n",
    "    Comprehensive stationarity analysis for time-series data.\n",
    "    \n",
    "    Stationarity is a critical property because many time-series\n",
    "    models assume the data is stationary.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "    \n",
    "    def explain_stationarity(self):\n",
    "        \"\"\"\n",
    "        Explain stationarity concepts and importance.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"UNDERSTANDING STATIONARITY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                   STATIONARY TIME SERIES                         \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502 \u2022 Constant mean over time                                        \u2502\n",
    "        \u2502 \u2022 Constant variance over time                                    \u2502\n",
    "        \u2502 \u2022 Constant autocorrelation structure                             \u2502\n",
    "        \u2502 \u2022 No trend or seasonality                                        \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Visual: \u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 (Flat, constant)       \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Example: Random walk with no drift, detrended data             \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                 NON-STATIONARY TIME SERIES                       \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502 \u2022 Mean changes over time (trend)                                 \u2502\n",
    "        \u2502 \u2022 Variance changes over time (heteroscedasticity)                \u2502\n",
    "        \u2502 \u2022 Seasonal patterns present                                      \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Visual: \u2500\u2500\u2500\u2500/\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500/\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500/\u2500\u2500\u2500\u2500\u2500\u2500 (Trending)               \u2502\n",
    "        \u2502            or /\\//\\//\\//\\//\\//\\//\\\\ (Seasonal)                   \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502   Example: Stock prices (typically trending), sales data         \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u26a0 WHY STATIONARITY MATTERS:\n",
    "        \n",
    "        Many statistical models assume stationarity:\n",
    "        \u2022 ARIMA models require stationary data\n",
    "        \u2022 Linear regression requires stationary residuals\n",
    "        \u2022 Granger causality tests need stationary variables\n",
    "        \n",
    "        Non-stationary data can lead to:\n",
    "        \u2022 Spurious regression (false relationships)\n",
    "        \u2022 Unreliable forecasts\n",
    "        \u2022 Invalid statistical inference\n",
    "        \"\"\")\n",
    "    \n",
    "    def adf_test(self, series=None, significance=0.05):\n",
    "        \"\"\"\n",
    "        Perform Augmented Dickey-Fuller test for stationarity.\n",
    "        \n",
    "        Null Hypothesis (H0): Series has a unit root (non-stationary)\n",
    "        Alternative Hypothesis (H1): Series is stationary\n",
    "        \n",
    "        If p-value < significance, reject H0 (series is stationary)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        series : np.array, optional\n",
    "            Series to test (uses price column if None)\n",
    "        significance : float\n",
    "            Significance level (default 0.05)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Test results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"AUGMENTED DICKEY-FULLER (ADF) TEST\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if series is None:\n",
    "            series = self.data[self.price_column].dropna().values\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Test Explanation:\n",
    "        \n",
    "        The ADF test checks if a unit root is present in the time series.\n",
    "        A unit root indicates non-stationarity.\n",
    "        \n",
    "        Hypotheses:\n",
    "        \u2022 H0 (Null):     Series has unit root (non-stationary)\n",
    "        \u2022 H1 (Alt):      Series has no unit root (stationary)\n",
    "        \n",
    "        Decision Rule:\n",
    "        \u2022 If p-value < 0.05: Reject H0 \u2192 Series is stationary\n",
    "        \u2022 If p-value >= 0.05: Cannot reject H0 \u2192 Series is non-stationary\n",
    "        \"\"\")\n",
    "        \n",
    "        # Perform test\n",
    "        result = adfuller(series, autolag='AIC')\n",
    "        \n",
    "        adf_statistic = result[0]\n",
    "        p_value = result[1]\n",
    "        critical_values = result[4]\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Test Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   ADF Statistic:      {adf_statistic:.4f}\")\n",
    "        print(f\"   p-value:            {p_value:.6f}\")\n",
    "        print(f\"\\n   Critical Values:\")\n",
    "        for key, value in critical_values.items():\n",
    "            print(f\"      {key}: {value:.4f}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        print(\"\\n\ud83d\udcc8 Interpretation:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if p_value < significance:\n",
    "            print(f\"   \u2713 p-value ({p_value:.6f}) < {significance}\")\n",
    "            print(\"   \u2192 Reject null hypothesis\")\n",
    "            print(\"   \u2192 Series is STATIONARY\")\n",
    "        else:\n",
    "            print(f\"   \u26a0 p-value ({p_value:.6f}) >= {significance}\")\n",
    "            print(\"   \u2192 Cannot reject null hypothesis\")\n",
    "            print(\"   \u2192 Series is NON-STATIONARY\")\n",
    "        \n",
    "        # Compare ADF statistic with critical values\n",
    "        if adf_statistic < critical_values['5%']:\n",
    "            print(f\"\\n   \u2713 ADF statistic ({adf_statistic:.4f}) < 5% critical value ({critical_values['5%']:.4f})\")\n",
    "            print(\"   \u2192 Strong evidence of stationarity\")\n",
    "        \n",
    "        return {\n",
    "            'adf_statistic': adf_statistic,\n",
    "            'p_value': p_value,\n",
    "            'critical_values': critical_values,\n",
    "            'is_stationary': p_value < significance\n",
    "        }\n",
    "    \n",
    "    def kpss_test(self, series=None, significance=0.05):\n",
    "        \"\"\"\n",
    "        Perform KPSS test for stationarity.\n",
    "        \n",
    "        Opposite of ADF test:\n",
    "        Null Hypothesis (H0): Series is stationary\n",
    "        Alternative Hypothesis (H1): Series is non-stationary\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        series : np.array, optional\n",
    "            Series to test\n",
    "        significance : float\n",
    "            Significance level\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Test results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"KPSS (KWIATKOWSKI-PHILLIPS-SCHMIDT-SHIN) TEST\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if series is None:\n",
    "            series = self.data[self.price_column].dropna().values\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Test Explanation:\n",
    "        \n",
    "        The KPSS test has opposite hypotheses from ADF:\n",
    "        \n",
    "        Hypotheses:\n",
    "        \u2022 H0 (Null):     Series is stationary\n",
    "        \u2022 H1 (Alt):      Series is non-stationary\n",
    "        \n",
    "        Decision Rule:\n",
    "        \u2022 If p-value < 0.05: Reject H0 \u2192 Series is non-stationary\n",
    "        \u2022 If p-value >= 0.05: Cannot reject H0 \u2192 Series is stationary\n",
    "        \"\"\")\n",
    "        \n",
    "        # Perform test\n",
    "        from statsmodels.tsa.stattools import kpss\n",
    "        \n",
    "        statistic, p_value, n_lags, critical_values = kpss(series, regression='c')\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Test Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   KPSS Statistic:     {statistic:.4f}\")\n",
    "        print(f\"   p-value:            {p_value:.6f}\")\n",
    "        print(f\"   Lags used:          {n_lags}\")\n",
    "        print(f\"\\n   Critical Values:\")\n",
    "        for key, value in critical_values.items():\n",
    "            print(f\"      {key}: {value:.4f}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        print(\"\\n\ud83d\udcc8 Interpretation:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if p_value < significance:\n",
    "            print(f\"   \u26a0 p-value ({p_value:.6f}) < {significance}\")\n",
    "            print(\"   \u2192 Reject null hypothesis\")\n",
    "            print(\"   \u2192 Series is NON-STATIONARY\")\n",
    "        else:\n",
    "            print(f\"   \u2713 p-value ({p_value:.6f}) >= {significance}\")\n",
    "            print(\"   \u2192 Cannot reject null hypothesis\")\n",
    "            print(\"   \u2192 Series is STATIONARY\")\n",
    "        \n",
    "        return {\n",
    "            'kpss_statistic': statistic,\n",
    "            'p_value': p_value,\n",
    "            'critical_values': critical_values,\n",
    "            'is_stationary': p_value >= significance\n",
    "        }\n",
    "    \n",
    "    def make_stationary(self, method='difference', order=1):\n",
    "        \"\"\"\n",
    "        Transform non-stationary series to stationary.\n",
    "        \n",
    "        Common methods:\n",
    "        1. Differencing: y_t' = y_t - y_{t-1}\n",
    "        2. Log transformation: y_t' = log(y_t)\n",
    "        3. Log difference: y_t' = log(y_t) - log(y_{t-1})\n",
    "        4. Detrending: Remove trend component\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        method : str\n",
    "            Transformation method\n",
    "        order : int\n",
    "            Order of differencing\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.array : Transformed stationary series\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(f\"MAKING SERIES STATIONARY (Method: {method.upper()})\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        series = self.data[self.price_column].dropna().values.copy()\n",
    "        original = series.copy()\n",
    "        \n",
    "        if method == 'difference':\n",
    "            # First differencing\n",
    "            for i in range(order):\n",
    "                series = np.diff(series)\n",
    "                ```python\n",
    "            print(f\"\\n   Applied differencing of order {order}\")\n",
    "            print(f\"   Original length: {len(original)}\")\n",
    "            print(f\"   Transformed length: {len(series)}\")\n",
    "            \n",
    "        elif method == 'log':\n",
    "            # Log transformation (for multiplicative patterns)\n",
    "            if (series <= 0).any():\n",
    "                print(\"   \u26a0 Warning: Series contains non-positive values\")\n",
    "                series = series + abs(series.min()) + 1\n",
    "            series = np.log(series)\n",
    "            print(\"   Applied log transformation\")\n",
    "            \n",
    "        elif method == 'log_difference':\n",
    "            # Log returns (common in finance)\n",
    "            if (series <= 0).any():\n",
    "                series = series + abs(series.min()) + 1\n",
    "            series = np.diff(np.log(series))\n",
    "            print(\"   Applied log differencing (log returns)\")\n",
    "            \n",
    "        elif method == 'detrend':\n",
    "            # Remove linear trend\n",
    "            x = np.arange(len(series))\n",
    "            slope, intercept = np.polyfit(x, series, 1)\n",
    "            trend = slope * x + intercept\n",
    "            series = series - trend\n",
    "            print(\"   Removed linear trend\")\n",
    "            \n",
    "        elif method == 'pct_change':\n",
    "            # Percentage change\n",
    "            series = np.diff(series) / series[:-1]\n",
    "            print(\"   Applied percentage change transformation\")\n",
    "        \n",
    "        # Test stationarity of transformed series\n",
    "        print(\"\\n\ud83d\udcca Testing Transformed Series for Stationarity:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # ADF test\n",
    "        adf_result = adfuller(series, autolag='AIC')\n",
    "        print(f\"   ADF p-value: {adf_result[1]:.6f}\")\n",
    "        \n",
    "        if adf_result[1] < 0.05:\n",
    "            print(\"   \u2713 Series is now STATIONARY\")\n",
    "        else:\n",
    "            print(\"   \u26a0 Series is still NON-STATIONARY\")\n",
    "            print(\"   Consider trying a different method or higher order\")\n",
    "        \n",
    "        # Store transformed series\n",
    "        self.stationary_series = series\n",
    "        \n",
    "        # Compare statistics\n",
    "        print(\"\\n\ud83d\udcca Comparison of Original vs Transformed:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Original Mean:     {original.mean():.4f}\")\n",
    "        print(f\"   Original Std:      {original.std():.4f}\")\n",
    "        print(f\"   Transformed Mean:  {series.mean():.6f}\")\n",
    "        print(f\"   Transformed Std:   {series.std():.4f}\")\n",
    "        \n",
    "        return series\n",
    "    \n",
    "    def visualize_stationarity(self, original, transformed):\n",
    "        \"\"\"\n",
    "        Visualize original vs transformed series.\n",
    "        \"\"\"\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 8))\n",
    "        \n",
    "        # Original series\n",
    "        axes[0, 0].plot(original, linewidth=1)\n",
    "        axes[0, 0].set_title('Original Series', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('Price (NPR)')\n",
    "        axes[0, 0].axhline(y=original.mean(), color='r', linestyle='--', label=f'Mean: {original.mean():.2f}')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Transformed series\n",
    "        axes[0, 1].plot(transformed, linewidth=1, color='green')\n",
    "        axes[0, 1].set_title('Transformed Series (Differenced)', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Time')\n",
    "        axes[0, 1].set_ylabel('Differenced Value')\n",
    "        axes[0, 1].axhline(y=transformed.mean(), color='r', linestyle='--', label=f'Mean: {transformed.mean():.4f}')\n",
    "        axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Original distribution\n",
    "        axes[1, 0].hist(original, bins=50, edgecolor='black', alpha=0.7)\n",
    "        axes[1, 0].set_title('Original Distribution', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].axvline(x=original.mean(), color='r', linestyle='--', label='Mean')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Transformed distribution\n",
    "        axes[1, 1].hist(transformed, bins=50, edgecolor='black', alpha=0.7, color='green')\n",
    "        axes[1, 1].set_title('Transformed Distribution', fontsize=11, fontweight='bold')\n",
    "        axes[1, 1].axvline(x=transformed.mean(), color='r', linestyle='--', label='Mean')\n",
    "        axes[1, 1].legend()\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcc8 Visual Assessment of Stationarity:\n",
    "        \n",
    "        Original Series (Non-Stationary):\n",
    "        \u2022 Clear trend visible\n",
    "        \u2022 Mean is not constant\n",
    "        \u2022 Variance may change over time\n",
    "        \n",
    "        Transformed Series (Stationary):\n",
    "        \u2022 Fluctuates around zero\n",
    "        \u2022 Mean is approximately constant\n",
    "        \u2022 Variance is more uniform\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "# Create trending NEPSE data\n",
    "nepse_trending = create_trending_nepse_data(n_days=500, trend_type='upward')\n",
    "\n",
    "# Initialize analyzer\n",
    "stationarity_analyzer = StationarityAnalyzer(nepse_trending, price_column='Close')\n",
    "\n",
    "# Explain stationarity\n",
    "stationarity_analyzer.explain_stationarity()\n",
    "\n",
    "# Perform ADF test\n",
    "adf_result = stationarity_analyzer.adf_test()\n",
    "\n",
    "# Perform KPSS test  \n",
    "kpss_result = stationarity_analyzer.kpss_test()\n",
    "\n",
    "# Make stationary\n",
    "transformed = stationarity_analyzer.make_stationary(method='difference', order=1)\n",
    "\n",
    "# Visualize\n",
    "stationarity_analyzer.visualize_stationarity(\n",
    "    nepse_trending['Close'].values, \n",
    "    transformed\n",
    ")\n",
    "```\n",
    "\n",
    "**Understanding the Code Output**:\n",
    "\n",
    "When you run the above code, here's what you should observe:\n",
    "\n",
    "1. **ADF Test on Original Data**:\n",
    "   - ADF Statistic will likely be positive (e.g., +0.5)\n",
    "   - p-value will be large (e.g., 0.8)\n",
    "   - Conclusion: Series is **NON-STATIONARY**\n",
    "\n",
    "2. **ADF Test on Differenced Data**:\n",
    "   - ADF Statistic will be negative and large (e.g., -15)\n",
    "   - p-value will be very small (e.g., 0.0001)\n",
    "   - Conclusion: Series is **STATIONARY**\n",
    "\n",
    "**Why Differencing Works**:\n",
    "```\n",
    "Original:    500, 502, 505, 501, 503, ...  (Trending upward)\n",
    "Differenced:     2,   3,  -4,   2, ...  (Fluctuates around 0)\n",
    "```\n",
    "\n",
    "#### **2.3.2 Autocorrelation**\n",
    "\n",
    "**Definition**: Autocorrelation measures the correlation between a time series and a lagged version of itself.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import acf, pacf\n",
    "\n",
    "class AutocorrelationAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze autocorrelation patterns in time-series data.\n",
    "    \n",
    "    Autocorrelation is fundamental to time-series analysis because:\n",
    "    1. It reveals how current values relate to past values\n",
    "    2. It helps identify model orders (AR, MA terms)\n",
    "    3. It detects seasonality and cyclic patterns\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        \"\"\"\n",
    "        Initialize the autocorrelation analyzer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Time-series data\n",
    "        price_column : str\n",
    "            Column to analyze\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "        self.acf_values = None\n",
    "        self.pacf_values = None\n",
    "    \n",
    "    def explain_autocorrelation(self):\n",
    "        \"\"\"\n",
    "        Explain autocorrelation concepts in detail.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"UNDERSTANDING AUTOCORRELATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca AUTOCORRELATION FUNCTION (ACF)\n",
    "        \n",
    "        Definition:\n",
    "        ACF measures the correlation between a time series and\n",
    "        its lagged values.\n",
    "        \n",
    "        Formula:\n",
    "        \u03c1(k) = Cov(Y_t, Y_{t-k}) / Var(Y_t)\n",
    "        \n",
    "        Where:\n",
    "        \u2022 k = lag (number of periods)\n",
    "        \u2022 \u03c1(k) = autocorrelation at lag k\n",
    "        \u2022 Values range from -1 to +1\n",
    "        \n",
    "        Interpretation:\n",
    "        \u2022 \u03c1(k) = +1: Perfect positive correlation\n",
    "        \u2022 \u03c1(k) = 0: No correlation\n",
    "        \u2022 \u03c1(k) = -1: Perfect negative correlation\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502                  ACF PATTERNS AND MEANINGS                       \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502  Pattern 1: Slow Decay                                          \u2502\n",
    "        \u2502  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591                         \u2502\n",
    "        \u2502  \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591                                 \u2502\n",
    "        \u2502  \u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591                                 \u2502\n",
    "        \u2502  \u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591                                 \u2502\n",
    "        \u2502  Lag: 1  5  10 15 20 25                                         \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502  \u2192 Non-stationary series (has trend)                            \u2502\n",
    "        \u2502  \u2192 Need differencing                                            \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502  Pattern 2: Sharp Cutoff                                        \u2502\n",
    "        \u2502  \u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591                         \u2502\n",
    "        \u2502  \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591                         \u2502\n",
    "        \u2502  \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591                         \u2502\n",
    "        \u2502  \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591                         \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502  \u2192 Stationary series, MA process                                \u2502\n",
    "        \u2502  \u2192 Suggests MA(q) model where q = cutoff lag                    \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502  Pattern 3: Seasonal Spikes                                     \u2502\n",
    "        \u2502  \u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2588\u2588             \u2502\n",
    "        \u2502  \u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2588\u2588\u2588\u2588\u2591\u2591             \u2502\n",
    "        \u2502  \u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591\u2591             \u2502\n",
    "        \u2502  Lag: 1    7    14   21                                         \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502  \u2192 Seasonal pattern with period = 7                             \u2502\n",
    "        \u2502  \u2192 Need seasonal differencing or seasonal terms                 \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \ud83d\udcca PARTIAL AUTOCORRELATION FUNCTION (PACF)\n",
    "        \n",
    "        Definition:\n",
    "        PACF measures the correlation between Y_t and Y_{t-k}\n",
    "        AFTER removing the effect of intermediate lags.\n",
    "        \n",
    "        Why PACF?\n",
    "        \u2022 ACF at lag k includes effects from all lags 1 to k-1\n",
    "        \u2022 PACF isolates the direct effect of lag k\n",
    "        \n",
    "        Interpretation for Model Selection:\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502  AR(p) Process:                                                 \u2502\n",
    "        \u2502  \u2022 ACF: Decays exponentially or as sine wave                    \u2502\n",
    "        \u2502  \u2022 PACF: Sharp cutoff after lag p                               \u2502\n",
    "        \u2502  \u2192 Look at PACF cutoff to determine p                           \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502  MA(q) Process:                                                 \u2502\n",
    "        \u2502  \u2022 ACF: Sharp cutoff after lag q                                \u2502\n",
    "        \u2502  \u2022 PACF: Decays exponentially or as sine wave                   \u2502\n",
    "        \u2502  \u2192 Look at ACF cutoff to determine q                            \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502  ARMA(p,q) Process:                                             \u2502\n",
    "        \u2502  \u2022 Both ACF and PACF decay gradually                            \u2502\n",
    "        \u2502  \u2192 Need more sophisticated model selection                      \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \"\"\")\n",
    "    \n",
    "    def calculate_acf(self, nlags=40):\n",
    "        \"\"\"\n",
    "        Calculate autocorrelation function values.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        nlags : int\n",
    "            Maximum number of lags to calculate\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.array : ACF values\n",
    "        \"\"\"\n",
    "        series = self.data[self.price_column].dropna().values\n",
    "        \n",
    "        # Calculate ACF\n",
    "        # acf function returns values for lags 0 to nlags\n",
    "        self.acf_values = acf(series, nlags=nlags, fft=True)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Autocorrelation Values (First {nlags} lags):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Display with visual bars\n",
    "        for i in range(min(15, nlags + 1)):\n",
    "            val = self.acf_values[i]\n",
    "            # Create visual bar\n",
    "            if val >= 0:\n",
    "                bar = \"\u2588\" * int(val * 40)\n",
    "            else:\n",
    "                bar = \"\u2591\" * int(abs(val) * 40)\n",
    "            \n",
    "            print(f\"   Lag {i:2d}: {val:+.4f}  {bar}\")\n",
    "        \n",
    "        # Confidence interval\n",
    "        # For 95% confidence: \u00b11.96 / sqrt(n)\n",
    "        n = len(series)\n",
    "        conf_interval = 1.96 / np.sqrt(n)\n",
    "        \n",
    "        print(f\"\\n   95% Confidence Interval: \u00b1{conf_interval:.4f}\")\n",
    "        print(\"   (Values outside this range are statistically significant)\")\n",
    "        \n",
    "        return self.acf_values\n",
    "    \n",
    "    def calculate_pacf(self, nlags=40):\n",
    "        \"\"\"\n",
    "        Calculate partial autocorrelation function values.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        nlags : int\n",
    "            Maximum number of lags to calculate\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        np.array : PACF values\n",
    "        \"\"\"\n",
    "        series = self.data[self.price_column].dropna().values\n",
    "        \n",
    "        # Calculate PACF\n",
    "        self.pacf_values = pacf(series, nlags=nlags, method='yw')\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Partial Autocorrelation Values (First {nlags} lags):\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Display with visual bars\n",
    "        for i in range(min(15, nlags + 1)):\n",
    "            val = self.pacf_values[i]\n",
    "            if val >= 0:\n",
    "                bar = \"\u2588\" * int(val * 40)\n",
    "            else:\n",
    "                bar = \"\u2591\" * int(abs(val) * 40)\n",
    "            \n",
    "            print(f\"   Lag {i:2d}: {val:+.4f}  {bar}\")\n",
    "        \n",
    "        n = len(series)\n",
    "        conf_interval = 1.96 / np.sqrt(n)\n",
    "        \n",
    "        print(f\"\\n   95% Confidence Interval: \u00b1{conf_interval:.4f}\")\n",
    "        \n",
    "        return self.pacf_values\n",
    "    \n",
    "    def plot_acf_pacf(self, nlags=40):\n",
    "        \"\"\"\n",
    "        Create ACF and PACF plots.\n",
    "        \n",
    "        These plots are essential tools for:\n",
    "        1. Identifying stationarity\n",
    "        2. Determining model orders\n",
    "        3. Detecting seasonality\n",
    "        \"\"\"\n",
    "        series = self.data[self.price_column].dropna()\n",
    "        \n",
    "        fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "        \n",
    "        # ACF Plot\n",
    "        plot_acf(series, lags=nlags, ax=axes[0], alpha=0.05)\n",
    "        axes[0].set_title('Autocorrelation Function (ACF)', fontsize=12, fontweight='bold')\n",
    "        axes[0].set_xlabel('Lag')\n",
    "        axes[0].set_ylabel('Correlation')\n",
    "        \n",
    "        # PACF Plot\n",
    "        plot_pacf(series, lags=nlags, ax=axes[1], alpha=0.05, method='yw')\n",
    "        axes[1].set_title('Partial Autocorrelation Function (PACF)', fontsize=12, fontweight='bold')\n",
    "        axes[1].set_xlabel('Lag')\n",
    "        axes[1].set_ylabel('Correlation')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Interpretation guide\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"HOW TO INTERPRET ACF/PACF PLOTS\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"\"\"\n",
    "        \ud83d\udcc8 Reading the Plots:\n",
    "        \n",
    "        1. BLUE SHADED AREA = 95% Confidence Interval\n",
    "           \u2022 Spikes outside this area are statistically significant\n",
    "           \u2022 Spikes inside may just be random noise\n",
    "        \n",
    "        2. LAG 0 ALWAYS = 1.0\n",
    "           \u2022 A series is perfectly correlated with itself\n",
    "           \u2022 This is not informative, ignore it\n",
    "        \n",
    "        3. PATTERNS TO LOOK FOR:\n",
    "        \n",
    "           Slow decay in ACF \u2192 Non-stationary (trending)\n",
    "           Quick cutoff in ACF \u2192 Suggests MA process\n",
    "           Quick cutoff in PACF \u2192 Suggests AR process\n",
    "           Spikes at regular intervals \u2192 Seasonality\n",
    "        \n",
    "        \ud83d\udcca Example Interpretations for NEPSE:\n",
    "        \n",
    "        Case 1: ACF decays slowly, PACF has spike at lag 1\n",
    "        \u2192 Stock prices are non-stationary\n",
    "        \u2192 Need differencing before modeling\n",
    "        \n",
    "        Case 2: After differencing, ACF cuts off at lag 2\n",
    "        \u2192 Consider MA(2) model\n",
    "        \n",
    "        Case 3: After differencing, PACF cuts off at lag 1\n",
    "        \u2192 Consider AR(1) model\n",
    "        \n",
    "        Case 4: Weekly pattern visible (spikes at lag 5)\n",
    "        \u2192 Include weekly seasonality in model\n",
    "        \"\"\")\n",
    "    \n",
    "    def lagrange_multiplier_test(self, maxlag=10):\n",
    "        \"\"\"\n",
    "        Perform Ljung-Box test for autocorrelation.\n",
    "        \n",
    "        This test checks if autocorrelations are significantly\n",
    "        different from zero as a group.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        maxlag : int\n",
    "            Maximum lag to test\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"LJUNG-BOX TEST FOR AUTOCORRELATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        series = self.data[self.price_column].dropna()\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Test Explanation:\n",
    "        \n",
    "        Null Hypothesis (H0): No autocorrelation exists\n",
    "        Alternative Hypothesis (H1): Autocorrelation exists\n",
    "        \n",
    "        Decision Rule:\n",
    "        \u2022 If p-value < 0.05: Reject H0 \u2192 Autocorrelation present\n",
    "        \u2022 If p-value >= 0.05: Cannot reject H0 \u2192 No autocorrelation\n",
    "        \"\"\")\n",
    "        \n",
    "        # Perform test\n",
    "        result = acorr_ljungbox(series, lags=range(1, maxlag + 1), return_df=True)\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Test Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"{'Lag':<8} {'Test Stat':<15} {'p-value':<15} {'Result':<20}\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        for lag in range(1, maxlag + 1):\n",
    "            stat = result.loc[lag, 'lb_stat']\n",
    "            pval = result.loc[lag, 'lb_pvalue']\n",
    "            \n",
    "            if pval < 0.05:\n",
    "                result_text = \"\u26a0 Autocorrelation\"\n",
    "            else:\n",
    "                result_text = \"\u2713 No autocorrelation\"\n",
    "            \n",
    "            print(f\"{lag:<8} {stat:<15.4f} {pval:<15.6f} {result_text}\")\n",
    "        \n",
    "        # Interpretation\n",
    "        print(\"\\n\ud83d\udcc8 Implications for NEPSE Analysis:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Check if autocorrelation is present\n",
    "        has_autocorr = any(result['lb_pvalue'] < 0.05)\n",
    "        \n",
    "        if has_autocorr:\n",
    "            print(\"\"\"\n",
    "        \u2713 Autocorrelation is present in the data\n",
    "        \n",
    "        This means:\n",
    "        \u2022 Past values can help predict future values\n",
    "        \u2022 Time-series models will be effective\n",
    "        \u2022 The data has exploitable patterns\n",
    "        \n",
    "        For modeling:\n",
    "        \u2022 Consider AR/MA components\n",
    "        \u2022 Include lagged features in ML models\n",
    "        \u2022 Use appropriate time-series models\n",
    "            \"\"\")\n",
    "        else:\n",
    "            print(\"\"\"\n",
    "        The data appears to be random (white noise)\n",
    "        \n",
    "        This means:\n",
    "        \u2022 Past values don't help predict future values\n",
    "        \u2022 Simple models may be as good as complex ones\n",
    "        \u2022 Focus on other factors (external variables)\n",
    "            \"\"\")\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def suggest_model_order(self):\n",
    "        \"\"\"\n",
    "        Suggest ARIMA model orders based on ACF/PACF patterns.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"MODEL ORDER SUGGESTIONS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if self.acf_values is None:\n",
    "            self.calculate_acf()\n",
    "        if self.pacf_values is None:\n",
    "            self.calculate_pacf()\n",
    "        \n",
    "        series = self.data[self.price_column].dropna().values\n",
    "        n = len(series)\n",
    "        conf_interval = 1.96 / np.sqrt(n)\n",
    "        \n",
    "        # Find significant lags in ACF\n",
    "        sig_acf_lags = []\n",
    "        for i in range(1, len(self.acf_values)):\n",
    "            if abs(self.acf_values[i]) > conf_interval:\n",
    "                sig_acf_lags.append(i)\n",
    "        \n",
    "        # Find significant lags in PACF\n",
    "        sig_pacf_lags = []\n",
    "        for i in range(1, len(self.pacf_values)):\n",
    "            if abs(self.pacf_values[i]) > conf_interval:\n",
    "                sig_pacf_lags.append(i)\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Significant Lags:\")\n",
    "        print(f\"   ACF:  {sig_acf_lags[:10]}{'...' if len(sig_acf_lags) > 10 else ''}\")\n",
    "        print(f\"   PACF: {sig_pacf_lags[:10]}{'...' if len(sig_pacf_lags) > 10 else ''}\")\n",
    "        \n",
    "        # Suggest models based on patterns\n",
    "        print(\"\\n\ud83d\udcc8 Model Suggestions:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Check for slow decay (non-stationarity)\n",
    "        if len(sig_acf_lags) > 10:\n",
    "            print(\"\"\"\n",
    "        \u26a0 Pattern detected: Slow ACF decay\n",
    "        \n",
    "        Suggestion:\n",
    "        \u2022 Data is likely non-stationary\n",
    "        \u2022 Apply differencing first\n",
    "        \u2022 Then re-examine ACF/PACF\n",
    "        \n",
    "        Recommended: ARIMA(p, 1, q) where:\n",
    "        \u2022 p = significant PACF lags after differencing\n",
    "        \u2022 q = significant ACF lags after differencing\n",
    "            \"\"\")\n",
    "        else:\n",
    "            # Check for AR pattern (PACF cutoff)\n",
    "            if len(sig_pacf_lags) <= 3 and len(sig_pacf_lags) > 0:\n",
    "                p_suggestion = max(sig_pacf_lags)\n",
    "                print(f\"\\n   \u2713 AR pattern detected\")\n",
    "                print(f\"     Suggested AR order (p): {p_suggestion}\")\n",
    "            \n",
    "            # Check for MA pattern (ACF cutoff)\n",
    "            if len(sig_acf_lags) <= 3 and len(sig_acf_lags) > 0:\n",
    "                q_suggestion = max(sig_acf_lags)\n",
    "                print(f\"\\n   \u2713 MA pattern detected\")\n",
    "                print(f\"     Suggested MA order (q): {q_suggestion}\")\n",
    "        \n",
    "        print(\"\\n\ud83d\udca1 Note: These are starting suggestions.\")\n",
    "        print(\"   Always validate models with proper testing.\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "# Use the trending NEPSE data\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"AUTOCORRELATION ANALYSIS FOR NEPSE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create sample data\n",
    "nepse_sample = create_trending_nepse_data(n_days=300, trend_type='upward')\n",
    "\n",
    "# Initialize analyzer\n",
    "autocorr_analyzer = AutocorrelationAnalyzer(nepse_sample, price_column='Close')\n",
    "\n",
    "# Explain concepts\n",
    "autocorr_analyzer.explain_autocorrelation()\n",
    "\n",
    "# Calculate and display ACF\n",
    "acf_values = autocorr_analyzer.calculate_acf(nlags=20)\n",
    "\n",
    "# Calculate and display PACF\n",
    "pacf_values = autocorr_analyzer.calculate_pacf(nlags=20)\n",
    "\n",
    "# Plot ACF and PACF\n",
    "autocorr_analyzer.plot_acf_pacf(nlags=30)\n",
    "\n",
    "# Perform Ljung-Box test\n",
    "lb_result = autocorr_analyzer.lagrange_multiplier_test(maxlag=10)\n",
    "\n",
    "# Suggest model order\n",
    "autocorr_analyzer.suggest_model_order()\n",
    "```\n",
    "\n",
    "**Detailed Explanation of Autocorrelation Analysis**:\n",
    "\n",
    "The code above provides a comprehensive toolkit for understanding autocorrelation:\n",
    "\n",
    "**1. ACF (Autocorrelation Function)**:\n",
    "- Measures correlation at each lag\n",
    "- **Slow decay** indicates non-stationarity\n",
    "- **Sharp cutoff** suggests MA process\n",
    "- **Seasonal spikes** indicate seasonality\n",
    "\n",
    "**2. PACF (Partial Autocorrelation Function)**:\n",
    "- Measures direct effect of each lag\n",
    "- Removes effects of intermediate lags\n",
    "- **Sharp cutoff** suggests AR process\n",
    "\n",
    "**3. Model Selection Using ACF/PACF**:\n",
    "| Pattern | ACF | PACF | Model Suggestion |\n",
    "|---------|-----|------|------------------|\n",
    "| AR(p) | Decays gradually | Cuts off after lag p | ARIMA(p, 0, 0) |\n",
    "| MA(q) | Cuts off after lag q | Decays gradually | ARIMA(0, 0, q) |\n",
    "| ARMA(p,q) | Decays gradually | Decays gradually | ARIMA(p, 0, q) |\n",
    "| ARIMA(p,d,q) | Slow decay | Slow decay | Difference first |\n",
    "\n",
    "#### **2.3.3 Heteroscedasticity**\n",
    "\n",
    "**Definition**: When the variance of a time series changes over time (non-constant variance).\n",
    "\n",
    "```python\n",
    "class HeteroscedasticityAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze heteroscedasticity in time-series data.\n",
    "    \n",
    "    Heteroscedasticity is common in financial data where volatility\n",
    "    clusters - periods of high volatility followed by low volatility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "    \n",
    "    def explain_heteroscedasticity(self):\n",
    "        \"\"\"\n",
    "        Explain heteroscedasticity concepts.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"UNDERSTANDING HETEROSCEDASTICITY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca DEFINITION\n",
    "        \n",
    "        Heteroscedasticity occurs when the variance of errors\n",
    "        (or the spread of data) changes over time.\n",
    "        \n",
    "        Opposite: Homoscedasticity (constant variance)\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502              HOMOSCEDASTIC (Constant Variance)                  \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502     \u2502  \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7                            \u2502\n",
    "        \u2502     \u2502  \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7                            \u2502\n",
    "        \u2502     \u2502  \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7                            \u2502\n",
    "        \u2502     \u2502  \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7 \u00b7                            \u2502\n",
    "        \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500                        \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502     Spread is constant over time                                \u2502\n",
    "        \u2502     Standard models work well                                   \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502            HETEROSCEDASTIC (Changing Variance)                  \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502     \u2502                    \u00b7  \u00b7  \u00b7  \u00b7                             \u2502\n",
    "        \u2502     \u2502              \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7                       \u2502\n",
    "        \u2502     \u2502        \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7                   \u2502\n",
    "        \u2502     \u2502  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7  \u00b7               \u2502\n",
    "        \u2502     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500 \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2502     Spread increases over time (or varies)                      \u2502\n",
    "        \u2502     Need special handling (GARCH models)                        \u2502\n",
    "        \u2502                                                                  \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \n",
    "        \ud83d\udcca WHY IT MATTERS FOR NEPSE\n",
    "        \n",
    "        Stock prices often exhibit:\n",
    "        1. Volatility clustering - high volatility periods cluster together\n",
    "        2. Leverage effect - negative returns increase volatility more\n",
    "        3. Mean reversion - volatility tends to return to average\n",
    "        \n",
    "        These patterns violate the constant variance assumption of\n",
    "        many models, leading to:\n",
    "        \u2022 Inefficient parameter estimates\n",
    "        \u2022 Invalid standard errors\n",
    "        \u2022 Poor prediction intervals\n",
    "        \"\"\")\n",
    "    \n",
    "    def visualize_volatility_clustering(self):\n",
    "        \"\"\"\n",
    "        Visualize volatility clustering in returns.\n",
    "        \n",
    "        Volatility clustering is a form of heteroscedasticity where\n",
    "        large changes tend to be followed by large changes, and\n",
    "        small changes by small changes.\n",
    "        \"\"\"\n",
    "        series = self.data[self.price_column].dropna().values\n",
    "        \n",
    "        # Calculate returns\n",
    "        returns = np.diff(np.log(series)) * 100  # Log returns in %\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Price series\n",
    "        axes[0, 0].plot(series, linewidth=1)\n",
    "        axes[0, 0].set_title('Price Series', fontsize=11, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Time')\n",
    "        axes[0, 0].set_ylabel('Price (NPR)')\n",
    "        axes[0, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Returns series\n",
    "        axes[0, 1].plot(returns, linewidth=1, color='green')\n",
    "        axes[0, 1].axhline(y=0, color='black', linestyle='-', alpha=0.3)\n",
    "        axes[0, 1].set_title('Log Returns (%)', fontsize=11, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Time')\n",
    "        axes[0, 1].set_ylabel('Return (%)')\n",
    "        axes[0, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Highlight volatility clustering\n",
    "        # Rolling standard deviation (volatility)\n",
    "        volatility = pd.Series(returns).rolling(window=20).std().values\n",
    "        \n",
    "        axes[1, 0].plot(volatility, linewidth=1, color='red')\n",
    "        axes[1, 0].set_title('Rolling Volatility (20-day Std Dev)', fontsize=11, fontweight='bold')\n",
    "        axes[1, 0].set_xlabel('Time')\n",
    "        axes[1, 0].set_ylabel('Volatility (%)')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Squared returns (absolute value shows volatility)\n",
    "        squared_returns = returns ** 2\n",
    "        \n",
    "        axes[1, 1].plot(squared_returns, linewidth=1, color='purple', alpha=0.7)\n",
    "        axes[1, 1].set_title('Squared Returns (Volatility Indicator)', fontsize=11, fontweight='bold')\n",
    "        axes[1, 1].set_xlabel('Time')\n",
    "        axes[1, 1].set_ylabel('Squared Return')\n",
    "        axes[1, 1].grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Interpretation\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"VOLATILITY CLUSTERING ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcc8 What to Look For:\n",
    "        \n",
    "        1. RETURNS PLOT (Top Right):\n",
    "           \u2022 Clusters of large swings indicate volatility clustering\n",
    "           \u2022 Look for periods where returns swing widely vs. quietly\n",
    "        \n",
    "        2. ROLLING VOLATILITY (Bottom Left):\n",
    "           \u2022 Peaks indicate high-volatility periods\n",
    "           \u2022 Troughs indicate calm periods\n",
    "           \u2022 Persistence suggests GARCH effects\n",
    "        \n",
    "        3. SQUARED RETURNS (Bottom Right):\n",
    "           \u2022 Used in ARCH/GARCH models\n",
    "           \u2022 Autocorrelation here indicates ARCH effects\n",
    "        \"\"\")\n",
    "        \n",
    "        # Calculate statistics\n",
    "        print(f\"\\n\ud83d\udcca Volatility Statistics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Mean return:          {returns.mean():.4f}%\")\n",
    "        print(f\"   Std dev of returns:   {returns.std():.4f}%\")\n",
    "        print(f\"   Min return:           {returns.min():.4f}%\")\n",
    "        print(f\"   Max return:           {returns.max():.4f}%\")\n",
    "        print(f\"   Mean volatility:      {np.nanmean(volatility):.4f}%\")\n",
    "        \n",
    "        return returns, volatility\n",
    "    \n",
    "    def arch_test(self, returns=None, lags=10):\n",
    "        \"\"\"\n",
    "        Perform Engle's ARCH test for heteroscedasticity.\n",
    "        \n",
    "        ARCH (Autoregressive Conditional Heteroscedasticity) test\n",
    "        checks if variance of errors depends on past squared errors.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        returns : np.array, optional\n",
    "            Return series (calculated if not provided)\n",
    "        lags : int\n",
    "            Number of lags to test\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Test results\n",
    "        \"\"\"\n",
    "        from statsmodels.stats.diagnostic import het_arch\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ENGLE'S ARCH TEST FOR HETEROSCEDASTICITY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if returns is None:\n",
    "            series = self.data[self.price_column].dropna().values\n",
    "            returns = np.diff(np.log(series)) * 100\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Test Explanation:\n",
    "        \n",
    "        Null Hypothesis (H0): No ARCH effects (homoscedastic)\n",
    "        Alternative Hypothesis (H1): ARCH effects present (heteroscedastic)\n",
    "        \n",
    "        ARCH effects mean that past squared residuals help predict\n",
    "        current variance - indicating volatility clustering.\n",
    "        \n",
    "        Decision Rule:\n",
    "        \u2022 If p-value < 0.05: Reject H0 \u2192 ARCH effects present\n",
    "        \u2022 If p-value >= 0.05: Cannot reject H0 \u2192 No ARCH effects\n",
    "        \"\"\")\n",
    "        \n",
    "        # Perform test\n",
    "        lm_stat, lm_pvalue, f_stat, f_pvalue = het_arch(returns, nlags=lags)\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Test Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   LM Statistic:     {lm_stat:.4f}\")\n",
    "        print(f\"   LM p-value:       {lm_pvalue:.6f}\")\n",
    "        print(f\"   F Statistic:      {f_stat:.4f}\")\n",
    "        print(f\"   F p-value:        {f_pvalue:.6f}\")\n",
    "        \n",
    "        print(\"\\n\ud83d\udcc8 Interpretation:\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        if lm_pvalue < 0.05:\n",
    "            print(f\"   \u26a0 p-value ({lm_pvalue:.6f}) < 0.05\")\n",
    "            print(\"   \u2192 Reject null hypothesis\")\n",
    "            print(\"   \u2192 ARCH EFFECTS ARE PRESENT\")\n",
    "            print(\"\\n   Implications:\")\n",
    "            print(\"   \u2022 Volatility clustering exists\")\n",
    "            print(\"   \u2022 Consider GARCH models for volatility\")\n",
    "            print(\"   \u2022 Standard models may underestimate risk\")\n",
    "        else:\n",
    "            print(f\"   \u2713 p-value ({lm_pvalue:.6f}) >= 0.05\")\n",
    "            print(\"   \u2192 Cannot reject null hypothesis\")\n",
    "            print(\"   \u2192 No significant ARCH effects\")\n",
    "            print(\"\\n   Standard models should be adequate.\")\n",
    "        \n",
    "        return {\n",
    "            'lm_statistic': lm_stat,\n",
    "            'lm_pvalue': lm_pvalue,\n",
    "            'f_statistic': f_stat,\n",
    "            'f_pvalue': f_pvalue,\n",
    "            'has_arch_effects': lm_pvalue < 0.05\n",
    "        }\n",
    "    \n",
    "    def rolling_volatility_analysis(self, window=20):\n",
    "        \"\"\"\n",
    "        Analyze rolling volatility characteristics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        window : int\n",
    "            Rolling window for volatility calculation\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"ROLLING VOLATILITY ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        series = self.data[self.price_column].dropna().values\n",
    "        returns = pd.Series(np.diff(np.log(series)) * 100)\n",
    "        \n",
    "        # Calculate rolling statistics\n",
    "        rolling_mean = returns.rolling(window=window).mean()\n",
    "        rolling_std = returns.rolling(window=window).std()\n",
    "        rolling_var = returns.rolling(window=window).var()\n",
    "        \n",
    "        # Find high and low volatility periods\n",
    "        vol_threshold_high = rolling_std.mean() + rolling_std.std()\n",
    "        vol_threshold_low = rolling_std.mean() - rolling_std.std()\n",
    "        \n",
    "        high_vol_periods = (rolling_std > vol_threshold_high).sum()\n",
    "        low_vol_periods = (rolling_std < vol_threshold_low).sum()\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Rolling Volatility Statistics (Window = {window} days):\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Average volatility:    {rolling_std.mean():.4f}%\")\n",
    "        print(f\"   Volatility std dev:    {rolling_std.std():.4f}%\")\n",
    "        print(f\"   Min volatility:        {rolling_std.min():.4f}%\")\n",
    "        print(f\"   Max volatility:        {rolling_std.max():.4f}%\")\n",
    "        print(f\"   High volatility days:  {high_vol_periods} ({high_vol_periods/len(returns)*100:.1f}%)\")\n",
    "        print(f\"   Low volatility days:   {low_vol_periods} ({low_vol_periods/len(returns)*100:.1f}%)\")\n",
    "        \n",
    "        # Volatility of volatility\n",
    "        vol_of_vol = rolling_std.std() / rolling_std.mean()\n",
    "        print(f\"\\n   Volatility of Volatility: {vol_of_vol:.4f}\")\n",
    "        \n",
    "        if vol_of_vol > 0.5:\n",
    "            print(\"   \u2192 High variation in volatility over time\")\n",
    "            print(\"   \u2192 Strong heteroscedasticity present\")\n",
    "        else:\n",
    "            print(\"   \u2192 Relatively stable volatility\")\n",
    "        \n",
    "        return {\n",
    "            'rolling_std': rolling_std,\n",
    "            'avg_volatility': rolling_std.mean(),\n",
    "            'high_vol_periods': high_vol_periods,\n",
    "            'low_vol_periods': low_vol_periods\n",
    "        }\n",
    "    \n",
    "    def suggest_volatility_model(self):\n",
    "        \"\"\"\n",
    "        Suggest appropriate volatility model based on analysis.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"VOLATILITY MODEL RECOMMENDATIONS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        series = self.data[self.price_column].dropna().values\n",
    "        returns = np.diff(np.log(series)) * 100\n",
    "        \n",
    "        # Run ARCH test\n",
    "        arch_result = self.arch_test(returns)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Volatility Modeling Options:\n",
    "        \n",
    "        \u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n",
    "        \u2502  MODEL          \u2502 USE CASE                         \u2502 COMPLEXITY \u2502\n",
    "        \u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n",
    "        \u2502  Constant Vol   \u2502 No ARCH effects                  \u2502 Low        \u2502\n",
    "        \u2502  ARCH(q)        \u2502 ARCH effects, simple             \u2502 Medium     \u2502\n",
    "        \u2502  GARCH(1,1)     \u2502 ARCH effects, persistent vol     \u2502 Medium     \u2502\n",
    "        \u2502  EGARCH         \u2502 Asymmetric effects               \u2502 High       \u2502\n",
    "        \u2502  GJR-GARCH      \u2502 Leverage effect                  \u2502 High       \u2502\n",
    "        \u2502  TGARCH         \u2502 Threshold effects                \u2502 High       \u2502\n",
    "        \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n",
    "        \"\"\")\n",
    "        \n",
    "        if arch_result['has_arch_effects']:\n",
    "            print(\"\"\"\n",
    "        \u2713 RECOMMENDATION: Use GARCH-type models\n",
    "        \n",
    "        For NEPSE data with ARCH effects:\n",
    "        \n",
    "        1. START WITH: GARCH(1,1)\n",
    "           - Captures volatility clustering\n",
    "           - Most commonly used\n",
    "           - Formula: \u03c3\u00b2_t = \u03c9 + \u03b1\u00b7\u03b5\u00b2_{t-1} + \u03b2\u00b7\u03c3\u00b2_{t-1}\n",
    "        \n",
    "        2. IF ASYMMETRY: Use EGARCH or GJR-GARCH\n",
    "           - Negative returns may increase volatility more\n",
    "           - Common in stock markets\n",
    "        \n",
    "        3. IMPLEMENTATION:\n",
    "        ```python\n",
    "        from arch import arch_model\n",
    "        \n",
    "        # Fit GARCH(1,1)\n",
    "        model = arch_model(returns, vol='Garch', p=1, q=1)\n",
    "        results = model.fit()\n",
    "        \n",
    "        # Get conditional volatility\n",
    "        conditional_vol = results.conditional_volatility\n",
    "        ```\n",
    "            \"\"\")\n",
    "        else:\n",
    "            print(\"\"\"\n",
    "        \u2713 RECOMMENDATION: Standard models adequate\n",
    "        \n",
    "        No significant ARCH effects detected:\n",
    "        \u2022 Use constant volatility assumption\n",
    "        \u2022 Standard prediction intervals should be valid\n",
    "        \u2022 No need for GARCH modeling\n",
    "            \"\"\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"HETEROSCEDASTICITY ANALYSIS FOR NEPSE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create sample data with varying volatility\n",
    "def create_heteroscedastic_data(n_days=500):\n",
    "    \"\"\"Create NEPSE-like data with volatility clustering.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    # Create base returns\n",
    "    returns = np.zeros(n_days)\n",
    "    \n",
    "    # Simulate volatility clustering\n",
    "    # High volatility periods alternate with low volatility\n",
    "    volatility = np.ones(n_days) * 1.0  # Base volatility\n",
    "    \n",
    "    # Create volatility regimes\n",
    "    regime = 0\n",
    "    for i in range(n_days):\n",
    "        if np.random.random() < 0.05:  # 5% chance to change regime\n",
    "            regime = 1 - regime\n",
    "        \n",
    "        if regime == 1:\n",
    "            volatility[i] = 3.0  # High volatility\n",
    "        else:\n",
    "            volatility[i] = 1.0  # Low volatility\n",
    "    \n",
    "    # Generate returns with varying volatility\n",
    "    returns = np.random.normal(0.05, volatility)  # Mean 0.05%, varying std\n",
    "    \n",
    "    # Convert returns to prices\n",
    "    prices = 500 * np.cumprod(1 + returns / 100)\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'S.No': range(1, n_days + 1),\n",
    "        'Symbol': 'ABL',\n",
    "        'Close': prices\n",
    "    })\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "hetero_data = create_heteroscedastic_data(n_days=500)\n",
    "\n",
    "# Initialize analyzer\n",
    "hetero_analyzer = HeteroscedasticityAnalyzer(hetero_data, price_column='Close')\n",
    "\n",
    "# Explain concepts\n",
    "hetero_analyzer.explain_heteroscedasticity()\n",
    "\n",
    "# Visualize volatility clustering\n",
    "returns, volatility = hetero_analyzer.visualize_volatility_clustering()\n",
    "\n",
    "# Perform ARCH test\n",
    "arch_result = hetero_analyzer.arch_test(returns)\n",
    "\n",
    "# Rolling volatility analysis\n",
    "rolling_results = hetero_analyzer.rolling_volatility_analysis(window=20)\n",
    "\n",
    "# Suggest model\n",
    "hetero_analyzer.suggest_volatility_model()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2.4 Common Data Challenges**\n",
    "\n",
    "Time-series data comes with various challenges that must be addressed for successful prediction.\n",
    "\n",
    "```python\n",
    "class DataChallengeAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze and address common data challenges in time-series.\n",
    "    \n",
    "    Understanding these challenges is crucial for building\n",
    "    robust prediction systems.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "    \n",
    "    def explain_challenges(self):\n",
    "        \"\"\"\n",
    "        Explain common time-series data challenges.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"COMMON DATA CHALLENGES IN TIME-SERIES\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        challenges = {\n",
    "            'Missing Values': {\n",
    "                'description': 'Gaps in the time series where data is absent',\n",
    "                'causes': ['System downtime', 'Trading holidays', 'Data entry errors', 'API failures'],\n",
    "                'impact': 'Breaks temporal continuity, affects lag calculations',\n",
    "                'solutions': ['Forward fill', 'Interpolation', 'Model-based imputation']\n",
    "            },\n",
    "            'Outliers': {\n",
    "                'description': 'Extreme values that deviate significantly from pattern',\n",
    "                'causes': ['Market crashes', 'Data errors', 'Corporate actions', 'News events'],\n",
    "                'impact': 'Distorts statistics, affects model training',\n",
    "                'solutions': ['Winsorization', 'Removal', 'Robust methods', 'Domain rules']\n",
    "            },\n",
    "            'Non-Stationarity': {\n",
    "                'description': 'Statistical properties change over time',\n",
    "                'causes': ['Trends', 'Seasonality', 'Structural breaks', 'Market regime changes'],\n",
    "                'impact': 'Many models assume stationarity',\n",
    "                'solutions': ['Differencing', 'Detrending', 'Transformation']\n",
    "            },\n",
    "            'Irregular Sampling': {\n",
    "                'description': 'Non-uniform time intervals between observations',\n",
    "                'causes': ['Missing data', 'Different data sources', 'Schedule changes'],\n",
    "                'impact': 'Complicates modeling, affects lag calculations',\n",
    "                'solutions': ['Resampling', 'Interpolation', 'Time-aware models']\n",
    "            },\n",
    "            'Noise': {\n",
    "                'description': 'Random fluctuations that obscure signal',\n",
    "                'causes': ['Measurement errors', 'Market microstructure', 'Random trading'],\n",
    "                'impact': 'Reduces predictability, affects accuracy',\n",
    "                'solutions': ['Smoothing', 'Filtering', 'Aggregation']\n",
    "            },\n",
    "            'Limited History': {\n",
    "                'description': 'Insufficient historical data for training',\n",
    "                'causes': ['New stocks', 'Recent IPOs', 'Data loss'],\n",
    "                'impact': 'Cannot train complex models, poor generalization',\n",
    "                'solutions': ['Transfer learning', 'Simple models', 'Similar stock data']\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for challenge, info in challenges.items():\n",
    "            print(f\"\\n\ud83d\udcca {challenge.upper()}\")\n",
    "            print(f\"   Description: {info['description']}\")\n",
    "            print(f\"   Causes: {', '.join(info['causes'])}\")\n",
    "            print(f\"   Impact: {info['impact']}\")\n",
    "            print(f\"   Solutions: {', '.join(info['solutions'])}\")\n",
    "    \n",
    "    def analyze_missing_patterns(self):\n",
    "        \"\"\"\n",
    "        Analyze patterns of missing data.\n",
    "        \n",
    "        Understanding the type of missing data helps choose\n",
    "        the right imputation strategy.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"MISSING DATA PATTERN ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca TYPES OF MISSING DATA:\n",
    "        \n",
    "        1. MCAR (Missing Completely At Random)\n",
    "           \u2022 Missing values are random, no pattern\n",
    "           \u2022 Safe to delete or impute\n",
    "           \u2022 Example: Random system glitches\n",
    "        \n",
    "        2. MAR (Missing At Random)\n",
    "           \u2022 Missing depends on observed variables\n",
    "           \u2022 Can be modeled using other features\n",
    "           \u2022 Example: Missing volume on low-activity days\n",
    "        \n",
    "        3. MNAR (Missing Not At Random)\n",
    "           \u2022 Missing depends on unobserved values\n",
    "           \u2022 Most problematic, requires domain knowledge\n",
    "           \u2022 Example: Missing prices during market crashes\n",
    "        \n",
    "        \ud83d\udcc8 NEPSE-SPECIFIC MISSING PATTERNS:\n",
    "        \n",
    "        1. Weekend/Holiday Gaps\n",
    "           - Expected and systematic\n",
    "           - Use forward fill or ignore\n",
    "        \n",
    "        2. Trading Halts\n",
    "           - During extreme volatility\n",
    "           - May need special handling\n",
    "        \n",
    "        3. Data Feed Issues\n",
    "           - Random, typically MCAR\n",
    "           - Can interpolate\n",
    "        \"\"\")\n",
    "        \n",
    "        # Simulate missing data analysis\n",
    "        series = self.data[self.price_column].copy()\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing_count = series.isnull().sum()\n",
    "        missing_pct = missing_count / len(series) * 100\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Missing Data Summary:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Total observations: {len(series)}\")\n",
    "        print(f\"   Missing values:     {missing_count}\")\n",
    "        print(f\"   Missing percentage: {missing_pct:.2f}%\")\n",
    "        \n",
    "        if missing_count > 0:\n",
    "            # Check for patterns\n",
    "            # Are missing values clustered?\n",
    "            missing_indices = series[series.isnull()].index\n",
    "            \n",
    "            if len(missing_indices) > 1:\n",
    "                gaps = np.diff(missing_indices)\n",
    "                avg_gap = gaps.mean()\n",
    "                print(f\"\\n   Missing value gaps:\")\n",
    "                print(f\"   Average gap: {avg_gap:.1f} observations\")\n",
    "                print(f\"   Max gap:     {gaps.max()} observations\")\n",
    "                \n",
    "                if avg_gap < 2:\n",
    "                    print(\"   \u2192 Missing values are clustered together\")\n",
    "                else:\n",
    "                    print(\"   \u2192 Missing values are scattered\")\n",
    "        \n",
    "        return {\n",
    "            'missing_count': missing_count,\n",
    "            'missing_pct': missing_pct\n",
    "        }\n",
    "    \n",
    "    def demonstrate_imputation(self):\n",
    "        \"\"\"\n",
    "        Demonstrate different imputation methods.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TIME-SERIES IMPUTATION METHODS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca COMMON IMPUTATION METHODS:\n",
    "        \n",
    "        1. FORWARD FILL (ffill)\n",
    "           \u2022 Use last known value\n",
    "           \u2022 Best for prices (maintains level)\n",
    "           \u2022 Code: data.fillna(method='ffill')\n",
    "        \n",
    "        2. BACKWARD FILL (bfill)\n",
    "           \u2022 Use next known value\n",
    "           \u2022 Good for post-processing\n",
    "           \u2022 Code: data.fillna(method='bfill')\n",
    "        \n",
    "        3. LINEAR INTERPOLATION\n",
    "           \u2022 Connect known values with line\n",
    "           \u2022 Good for smooth data\n",
    "           \u2022 Code: data.interpolate(method='linear')\n",
    "        \n",
    "        4. SPLINE INTERPOLATION\n",
    "           \u2022 Smooth curve fitting\n",
    "           \u2022 Better for non-linear patterns\n",
    "           \u2022 Code: data.interpolate(method='spline', order=3)\n",
    "        \n",
    "        5. MEAN/MEDIAN IMPUTATION\n",
    "           \u2022 Use historical average\n",
    "           \u2022 Simple but can distort trends\n",
    "           \u2022 Code: data.fillna(data.mean())\n",
    "        \n",
    "        \ud83d\udcc8 NEPSE EXAMPLE:\n",
    "        \"\"\")\n",
    "        \n",
    "        # Create example with missing values\n",
    "        np.random.seed(42)\n",
    "        n = 20\n",
    "        prices = 500 + np.cumsum(np.random.randn(n) * 5)\n",
    "        prices_with_missing = prices.copy()\n",
    "        missing_indices = [5, 6, 7, 12, 13]\n",
    "        prices_with_missing[missing_indices] = np.nan\n",
    "        \n",
    "        # Create DataFrame\n",
    "        example_df = pd.DataFrame({\n",
    "            'Original': prices,\n",
    "            'With_Missing': prices_with_missing,\n",
    "            'Forward_Fill': pd.Series(prices_with_missing).fillna(method='ffill'),\n",
    "            'Linear_Interp': pd.Series(prices_with_missing).interpolate(method='linear'),\n",
    "            'Mean_Fill': pd.Series(prices_with_missing).fillna(np.nanmean(prices_with_missing))\n",
    "        })\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(example_df.round(2))\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udca1 RECOMMENDATIONS FOR NEPSE:\n",
    "        \n",
    "        For Price Data:\n",
    "        \u2022 Use forward fill (most recent price is best estimate)\n",
    "        \u2022 Or ignore missing rows (for model training)\n",
    "        \n",
    "        For Volume Data:\n",
    "        \u2022 Use mean of recent days (volume patterns repeat)\n",
    "        \u2022 Or set to 0 (no trading occurred)\n",
    "        \n",
    "        For Derived Features:\n",
    "        \u2022 Recalculate after filling raw data\n",
    "        \u2022 Don't fill derived features directly\n",
    "        \"\"\")\n",
    "        \n",
    "        return example_df\n",
    "    \n",
    "    def analyze_outliers(self, threshold=3):\n",
    "        \"\"\"\n",
    "        Detect and analyze outliers in the time series.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        threshold : float\n",
    "            Z-score threshold for outlier detection\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"OUTLIER DETECTION AND ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        series = self.data[self.price_column].dropna()\n",
    "        returns = series.pct_change().dropna()\n",
    "        \n",
    "        # Method 1: Z-score based\n",
    "        z_scores = (returns - returns.mean()) / returns.std()\n",
    "        z_outliers = returns[np.abs(z_scores) > threshold]\n",
    "        \n",
    "        # Method 2: IQR based\n",
    "        Q1 = returns.quantile(0.25)\n",
    "        Q3 = returns.quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        iqr_outliers = returns[(returns < Q1 - 1.5*IQR) | (returns > Q3 + 1.5*IQR)]\n",
    "        \n",
    "        # Method 3: Rolling Z-score\n",
    "        rolling_mean = returns.rolling(window=20).mean()\n",
    "        rolling_std = returns.rolling(window=20).std()\n",
    "        rolling_z = (returns - rolling_mean) / rolling_std\n",
    "        rolling_outliers = returns[np.abs(rolling_z) > threshold]\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Outlier Detection Results:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Z-score method (threshold={threshold}):  {len(z_outliers)} outliers\")\n",
    "        print(f\"   IQR method:                              {len(iqr_outliers)} outliers\")\n",
    "        print(f\"   Rolling Z-score method:                  {len(rolling_outliers)} outliers\")\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Outlier Statistics:\")\n",
    "        print(\"-\" * 50)\n",
    "        print(f\"   Return mean:     {returns.mean()*100:.4f}%\")\n",
    "        print(f\"   Return std:      {returns.std()*100:.4f}%\")\n",
    "        print(f\"   Return min:      {returns.min()*100:.4f}%\")\n",
    "        print(f\"   Return max:      {returns.max()*100:.4f}%\")\n",
    "        \n",
    "        if len(z_outliers) > 0:\n",
    "            print(f\"\\n\ud83d\udcca Extreme Outliers (Z > {threshold}):\")\n",
    "            print(\"-\" * 50)\n",
    "            for idx in z_outliers.index[:5]:\n",
    "                ret = returns.loc[idx] * 100\n",
    "                print(f\"   Index {idx}: {ret:+.2f}%\")\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udca1 OUTLIER HANDLING STRATEGIES:\n",
    "        \n",
    "        1. INVESTIGATE FIRST\n",
    "           \u2022 Check news/events on outlier days\n",
    "           \u2022 Verify data accuracy\n",
    "           \u2022 Understand the cause\n",
    "        \n",
    "        2. FOR DATA ERRORS\n",
    "           \u2022 Correct if possible\n",
    "           \u2022 Remove if uncorrectable\n",
    "        \n",
    "        3. FOR GENUINE OUTLIERS\n",
    "           \u2022 Keep for risk modeling\n",
    "           \u2022 Winsorize (cap at threshold)\n",
    "           \u2022 Use robust statistics\n",
    "        \n",
    "        4. FOR NEPSE\n",
    "           \u2022 Check for stock splits, dividends\n",
    "           \u2022 Verify against official sources\n",
    "           \u2022 Consider regulatory announcements\n",
    "        \"\"\")\n",
    "        \n",
    "        return {\n",
    "            'z_outliers': z_outliers,\n",
    "            'iqr_outliers': iqr_outliers,\n",
    "            'rolling_outliers': rolling_outliers\n",
    "        }\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"DATA CHALLENGE ANALYSIS FOR NEPSE\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create sample data\n",
    "sample_data = create_trending_nepse_data(n_days=300, trend_type='upward')\n",
    "\n",
    "# Initialize analyzer\n",
    "challenge_analyzer = DataChallengeAnalyzer(sample_data, price_column='Close')\n",
    "\n",
    "# Explain challenges\n",
    "challenge_analyzer.explain_challenges()\n",
    "\n",
    "# Analyze missing patterns\n",
    "missing_results = challenge_analyzer.analyze_missing_patterns()\n",
    "\n",
    "# Demonstrate imputation\n",
    "imputation_example = challenge_analyzer.demonstrate_imputation()\n",
    "\n",
    "# Analyze outliers\n",
    "outlier_results = challenge_analyzer.analyze_outliers(threshold=2.5)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2.5 Exploring Your First Time-Series Dataset**\n",
    "\n",
    "Now let's create a comprehensive exploration function for NEPSE data:\n",
    "\n",
    "```python\n",
    "class NEPSEDataExplorer:\n",
    "    \"\"\"\n",
    "    Comprehensive data exploration for NEPSE time-series data.\n",
    "    \n",
    "    This class provides a systematic approach to understanding\n",
    "    time-series data before modeling.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=None, data=None):\n",
    "        \"\"\"\n",
    "        Initialize the explorer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : str, optional\n",
    "            Path to NEPSE CSV file\n",
    "        data : pd.DataFrame, optional\n",
    "            Pre-loaded data\n",
    "        \"\"\"\n",
    "        if data is not None:\n",
    "            self.data = data.copy()\n",
    "        elif data_path is not None:\n",
    "            self.data = pd.read_csv(data_path)\n",
    "        else:\n",
    "            raise ValueError(\"Provide either data_path or data\")\n",
    "        \n",
    "        self.numeric_columns = None\n",
    "        self.exploration_results = {}\n",
    "    \n",
    "    def initial_overview(self):\n",
    "        \"\"\"\n",
    "        Generate initial overview of the dataset.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"NEPSE TIME-SERIES DATA EXPLORATION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca DATASET OVERVIEW\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Basic info\n",
    "        print(f\"   Shape:           {self.data.shape}\")\n",
    "        print(f\"   Rows:            {self.data.shape[0]:,}\")\n",
    "        print(f\"   Columns:         {self.data.shape[1]}\")\n",
    "        print(f\"   Memory usage:    {self.data.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "        \n",
    "        # Column types\n",
    "        print(f\"\\n   Column Types:\")\n",
    "        print(f\"   - Numeric:       {self.data.select_dtypes(include=[np.number]).shape[1]}\")\n",
    "        print(f\"   - Categorical:   {self.data.select_dtypes(include=['object']).shape[1]}\")\n",
    "        print(f\"   - DateTime:      {self.data.select_dtypes(include=['datetime']).shape[1]}\")\n",
    "        \n",
    "        # Unique stocks\n",
    "        if 'Symbol' in self.data.columns:\n",
    "            print(f\"\\n   Stock Information:\")\n",
    "            print(f\"   - Unique stocks: {self.data['Symbol'].nunique()}\")\n",
    "            print(f\"   - Records/stock: {self.data.groupby('Symbol').size().describe()['mean']:.1f} avg\")\n",
    "        \n",
    "        # Sample data\n",
    "        print(\"\\n\ud83d\udccb SAMPLE DATA (First 5 Rows):\")\n",
    "        print(\"-\" * 70)\n",
    "        print(self.data.head())\n",
    "        \n",
    "        # Data types\n",
    "        print(\"\\n\ud83d\udccb DATA TYPES:\")\n",
    "        print(\"-\" * 70)\n",
    "        print(self.data.dtypes.to_string())\n",
    "        \n",
    "        return self.data.info()\n",
    "    \n",
    "    def identify_numeric_columns(self):\n",
    "        \"\"\"\n",
    "        Identify numeric columns for analysis.\n",
    "        \"\"\"\n",
    "        self.numeric_columns = self.data.select_dtypes(\n",
    "            include=[np.number]\n",
    "        ).columns.tolist()\n",
    "        \n",
    "        # Exclude index-like columns\n",
    "        exclude_cols = ['S.No']\n",
    "        self.numeric_columns = [c for c in self.numeric_columns \n",
    "                                if c not in exclude_cols]\n",
    "        \n",
    "        print(f\"\\n\ud83d\udcca Numeric Columns for Analysis:\")\n",
    "        print(f\"   {self.numeric_columns}\")\n",
    "        \n",
    "        return self.numeric_columns\n",
    "    \n",
    "    def statistical_summary(self):\n",
    "        \"\"\"\n",
    "        Generate comprehensive statistical summary.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"STATISTICAL SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if self.numeric_columns is None:\n",
    "            self.identify_numeric_columns()\n",
    "        \n",
    "        summary = self.data[self.numeric_columns].describe()\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 70)\n",
    "        print(summary.round(2))\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Additional statistics\n",
    "        print(\"\\n\ud83d\udcca Additional Statistics:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for col in self.numeric_columns[:5]:  # Top 5 columns\n",
    "            data = self.data[col].dropna()\n",
    "            print(f\"\\n   {col}:\")\n",
    "            print(f\"      Skewness: {data.skew():.4f}\")\n",
    "            print(f\"      Kurtosis: {data.kurtosis():.4f}\")\n",
    "            print(f\"      CV:       {data.std()/data.mean()*100:.2f}%\")\n",
    "        \n",
    "        self.exploration_results['summary'] = summary\n",
    "        return summary\n",
    "    \n",
    "    def time_range_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze the time range and data completeness.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"TIME RANGE ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if 'S.No' in self.data.columns:\n",
    "            print(f\"\\n   Serial Number Range:\")\n",
    "            print(f\"   - Min:     {self.data['S.No'].min()}\")\n",
    "            print(f\"   - Max:     {self.data['S.No'].max()}\")\n",
    "            print(f\"   - Records: {self.data['S.No'].nunique()}\")\n",
    "        \n",
    "        if 'Date' in self.data.columns:\n",
    "            self.data['Date'] = pd.to_datetime(self.data['Date'])\n",
    "            print(f\"\\n   Date Range:\")\n",
    "            print(f\"   - Start:   {self.data['Date'].min()}\")\n",
    "            print(f\"   - End:     {self.data['Date'].max()}\")\n",
    "            print(f\"   - Days:    {(self.data['Date'].max() - self.data['Date'].min()).days}\")\n",
    "        \n",
    "        # Records per stock\n",
    "        if 'Symbol' in self.data.columns:\n",
    "            print(f\"\\n   Records per Stock:\")\n",
    "            records_per_stock = self.data.groupby('Symbol').size()\n",
    "            print(f\"   - Min:     {records_per_stock.min()}\")\n",
    "            print(f\"   - Max:     {records_per_stock.max()}\")\n",
    "            print(f\"   - Mean:    {records_per_stock.mean():.1f}\")\n",
    "    \n",
    "    def correlation_analysis(self):\n",
    "        \"\"\"\n",
    "        Analyze correlations between numeric variables.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"CORRELATION ANALYSIS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        if self.numeric_columns is None:\n",
    "            self.identify_numeric_columns()\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = self.data[self.numeric_columns].corr()\n",
    "        \n",
    "        print(\"\\n\ud83d\udcca Correlation Matrix (Top 5 columns):\")\n",
    "        print(\"-\" * 70)\n",
    "        print(corr_matrix.iloc[:5, :5].round(3))\n",
    "        \n",
    "        # Find highly correlated pairs\n",
    "        print(\"\\n\ud83d\udcca Highly Correlated Pairs (|r| > 0.8):\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        for i in range(len(corr_matrix.columns)):\n",
    "            for j in range(i+1, len(corr_matrix.columns)):\n",
    "                col1 = corr_matrix.columns[i]\n",
    "                col2 = corr_matrix.columns[j]\n",
    "                corr_val = corr_matrix.iloc[i, j]\n",
    "                \n",
    "                if abs(corr_val) > 0.8:\n",
    "                    print(f\"   {col1} <-> {col2}: {corr_val:.3f}\")\n",
    "        \n",
    "        return corr_matrix\n",
    "    \n",
    "    def run_full_exploration(self):\n",
    "        \"\"\"\n",
    "        Run complete data exploration pipeline.\n",
    "        \"\"\"\n",
    "        self.initial_overview()\n",
    "        self.statistical_summary()\n",
    "        self.time_range_analysis()\n",
    "        self.correlation_analysis()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"EXPLORATION COMPLETE\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        return self.exploration_results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# COMPLETE EXAMPLE\n",
    "# ============================================================\n",
    "\n",
    "# Create comprehensive NEPSE sample data\n",
    "def create_comprehensive_nepse_data():\n",
    "    \"\"\"\n",
    "    Create comprehensive NEPSE-like dataset for exploration.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    symbols = ['ABL', 'ADBL', 'NABIL', 'NICA', 'SCB']\n",
    "    all_data = []\n",
    "    \n",
    "    for symbol in symbols:\n",
    "        n_days = np.random.randint(200, 400)\n",
    "        \n",
    "        # Generate realistic price data\n",
    "        base_price = np.random.uniform(300, 800)\n",
    "        trend = np.random.uniform(-0.0002, 0.0005)\n",
    "        volatility = np.random.uniform(0.015, 0.025)\n",
    "        \n",
    "        returns = np.random.normal(trend, volatility, n_days)\n",
    "        prices = base_price * np.cumprod(1 + returns)\n",
    "        \n",
    "        for i in range(n_days):\n",
    "            close = prices[i]\n",
    "            high = close * (1 + np.abs(np.random.normal(0, 0.01)))\n",
    "            low = close * (1 - np.abs(np.random.normal(0, 0.01)))\n",
    "            open_price = low + (high - low) * np.random.random()\n",
    "            \n",
    "            volume = int(np.random.lognormal(9 + np.random.uniform(-1, 1), 0.5))\n",
    "            vwap = (high + low + close) / 3 * (1 + np.random.normal(0, 0.002))\n",
    "            \n",
    "            record = {\n",
    "                'S.No': i + 1,\n",
    "                'Symbol': symbol,\n",
    "                'Conf.': 'Standard',\n",
    "                'Open': round(open_price, 2),\n",
    "                'High': round(high, 2),\n",
    "                'Low': round(low, 2),\n",
    "                'Close': round(close, 2),\n",
    "                'LTP': round(close, 2),\n",
    "                'Close - LTP': 0.0,\n",
    "                'Close - LTP %': 0.0,\n",
    "                'VWAP': round(vwap, 2),\n",
    "                ```python\n",
    "                'Vol': volume,\n",
    "                'Prev. Close': round(prices[i-1] if i > 0 else base_price, 2),\n",
    "                'Turnover': int(volume * close),\n",
    "                'Trans.': int(volume * 0.1),\n",
    "                'Diff': round(close - prices[i-1] if i > 0 else 0, 2),\n",
    "                'Range': round(high - low, 2),\n",
    "                'Diff %': round((close - prices[i-1]) / prices[i-1] * 100 if i > 0 else 0, 2),\n",
    "                'Range %': round((high - low) / close * 100, 2),\n",
    "                'VWAP %': round((close - vwap) / vwap * 100, 2),\n",
    "                '52 Weeks High': round(close * 1.3, 2),\n",
    "                '52 Weeks Low': round(close * 0.7, 2)\n",
    "            }\n",
    "            all_data.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Run complete exploration\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"RUNNING COMPLETE NEPSE DATA EXPLORATION\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "nepse_full_data = create_comprehensive_nepse_data()\n",
    "explorer = NEPSEDataExplorer(data=nepse_full_data)\n",
    "exploration_results = explorer.run_full_exploration()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2.6 Visual Inspection Techniques**\n",
    "\n",
    "Visual inspection is one of the most powerful tools for understanding time-series data. A good visualization can reveal patterns that statistics alone might miss.\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as mdates\n",
    "import seaborn as sns\n",
    "from pandas.plotting import register_matplotlib_converters\n",
    "\n",
    "register_matplotlib_converters()\n",
    "\n",
    "\n",
    "class TimeSeriesVisualizer:\n",
    "    \"\"\"\n",
    "    Comprehensive visualization toolkit for time-series data.\n",
    "    \n",
    "    Visualization is crucial because:\n",
    "    1. Reveals patterns invisible in summary statistics\n",
    "    2. Helps identify data quality issues\n",
    "    3. Guides modeling decisions\n",
    "    4. Communicates findings effectively\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close', date_column=None):\n",
    "        \"\"\"\n",
    "        Initialize the visualizer.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            Time-series data\n",
    "        price_column : str\n",
    "            Column containing values to visualize\n",
    "        date_column : str, optional\n",
    "            Date column (creates synthetic dates if None)\n",
    "        \"\"\"\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "        self.date_column = date_column\n",
    "        \n",
    "        # Ensure we have a time index\n",
    "        if date_column and date_column in data.columns:\n",
    "            self.data[date_column] = pd.to_datetime(self.data[date_column])\n",
    "            self.data = self.data.sort_values(date_column)\n",
    "        \n",
    "        # Set style\n",
    "        plt.style.use('seaborn-v0_8-whitegrid')\n",
    "    \n",
    "    def line_plot_overview(self, symbol=None):\n",
    "        \"\"\"\n",
    "        Create basic line plot of the time series.\n",
    "        \n",
    "        Line plots are fundamental for:\n",
    "        - Seeing overall trends\n",
    "        - Identifying abrupt changes\n",
    "        - Spotting outliers visually\n",
    "        \"\"\"\n",
    "        fig, ax = plt.subplots(figsize=(14, 6))\n",
    "        \n",
    "        if symbol and 'Symbol' in self.data.columns:\n",
    "            plot_data = self.data[self.data['Symbol'] == symbol]\n",
    "            title = f'Price Series for {symbol}'\n",
    "        else:\n",
    "            plot_data = self.data\n",
    "            title = 'Price Series Overview'\n",
    "        \n",
    "        ax.plot(plot_data[self.price_column].values, linewidth=1, alpha=0.8)\n",
    "        \n",
    "        ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "        ax.set_xlabel('Time Index', fontsize=12)\n",
    "        ax.set_ylabel('Price (NPR)', fontsize=12)\n",
    "        \n",
    "        # Add mean line\n",
    "        mean_price = plot_data[self.price_column].mean()\n",
    "        ax.axhline(y=mean_price, color='red', linestyle='--', alpha=0.5, \n",
    "                   label=f'Mean: {mean_price:.2f}')\n",
    "        ax.legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcc8 What to Look For:\n",
    "        \n",
    "        1. TREND: Is the line generally going up, down, or sideways?\n",
    "        2. VOLATILITY: Are the swings large or small?\n",
    "        3. OUTLIERS: Are there any extreme spikes or dips?\n",
    "        4. REGIME CHANGES: Are there periods with different behavior?\n",
    "        5. GAPS: Are there any missing periods?\n",
    "        \"\"\")\n",
    "    \n",
    "    def candlestick_style_plot(self, symbol=None, n_points=100):\n",
    "        \"\"\"\n",
    "        Create OHLC-style visualization.\n",
    "        \n",
    "        For NEPSE data, we can visualize the Open, High, Low, Close\n",
    "        to understand daily price action.\n",
    "        \"\"\"\n",
    "        if symbol and 'Symbol' in self.data.columns:\n",
    "            plot_data = self.data[self.data['Symbol'] == symbol].tail(n_points)\n",
    "        else:\n",
    "            plot_data = self.data.tail(n_points)\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 1, figsize=(14, 8), \n",
    "                                  gridspec_kw={'height_ratios': [3, 1]})\n",
    "        \n",
    "        # Price plot with High-Low range\n",
    "        x = range(len(plot_data))\n",
    "        \n",
    "        # Plot High-Low range as vertical lines\n",
    "        for i, (idx, row) in enumerate(plot_data.iterrows()):\n",
    "            color = 'green' if row['Close'] >= row['Open'] else 'red'\n",
    "            # High-Low line\n",
    "            axes[0].plot([i, i], [row['Low'], row['High']], color=color, linewidth=1)\n",
    "            # Open-Close rectangle (represented by thicker line)\n",
    "            axes[0].plot([i, i], [row['Open'], row['Close']], color=color, linewidth=3)\n",
    "        \n",
    "        axes[0].set_title(f'Price Action (OHLC) - Last {n_points} Periods', \n",
    "                          fontsize=14, fontweight='bold')\n",
    "        axes[0].set_ylabel('Price (NPR)')\n",
    "        axes[0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Volume plot\n",
    "        colors = ['green' if c >= o else 'red' \n",
    "                  for c, o in zip(plot_data['Close'], plot_data['Open'])]\n",
    "        axes[1].bar(x, plot_data['Vol'], color=colors, alpha=0.7)\n",
    "        axes[1].set_title('Volume', fontsize=12)\n",
    "        axes[1].set_ylabel('Volume')\n",
    "        axes[1].set_xlabel('Time Index')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Reading OHLC Charts:\n",
    "        \n",
    "        GREEN bars: Close > Open (Bullish day)\n",
    "        RED bars:   Close < Open (Bearish day)\n",
    "        \n",
    "        Vertical line: High to Low range\n",
    "        Thick part: Open to Close range\n",
    "        \n",
    "        Volume bars show trading activity\n",
    "        \"\"\")\n",
    "    \n",
    "    def distribution_analysis(self, symbol=None):\n",
    "        \"\"\"\n",
    "        Analyze the distribution of prices and returns.\n",
    "        \"\"\"\n",
    "        if symbol and 'Symbol' in self.data.columns:\n",
    "            plot_data = self.data[self.data['Symbol'] == symbol]\n",
    "        else:\n",
    "            plot_data = self.data\n",
    "        \n",
    "        prices = plot_data[self.price_column]\n",
    "        returns = prices.pct_change().dropna()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "        \n",
    "        # Price distribution\n",
    "        axes[0, 0].hist(prices, bins=50, edgecolor='black', alpha=0.7, density=True)\n",
    "        prices.plot(kind='kde', ax=axes[0, 0], color='red', linewidth=2)\n",
    "        axes[0, 0].set_title('Price Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[0, 0].set_xlabel('Price (NPR)')\n",
    "        axes[0, 0].axvline(prices.mean(), color='blue', linestyle='--', label='Mean')\n",
    "        axes[0, 0].legend()\n",
    "        \n",
    "        # Return distribution\n",
    "        axes[0, 1].hist(returns, bins=50, edgecolor='black', alpha=0.7, density=True)\n",
    "        returns.plot(kind='kde', ax=axes[0, 1], color='red', linewidth=2)\n",
    "        axes[0, 1].set_title('Return Distribution', fontsize=12, fontweight='bold')\n",
    "        axes[0, 1].set_xlabel('Daily Return')\n",
    "        axes[0, 1].axvline(0, color='blue', linestyle='--')\n",
    "        \n",
    "        # QQ plot for normality check\n",
    "        from scipy import stats\n",
    "        stats.probplot(returns, dist=\"norm\", plot=axes[1, 0])\n",
    "        axes[1, 0].set_title('Q-Q Plot (Normality Check)', fontsize=12, fontweight='bold')\n",
    "        axes[1, 0].grid(True, alpha=0.3)\n",
    "        \n",
    "        # Box plot by period (e.g., month)\n",
    "        if 'Date' in plot_data.columns:\n",
    "            plot_data['Month'] = pd.to_datetime(plot_data['Date']).dt.month\n",
    "            plot_data.boxplot(column=self.price_column, by='Month', ax=axes[1, 1])\n",
    "            axes[1, 1].set_title('Price by Month', fontsize=12, fontweight='bold')\n",
    "        else:\n",
    "            # Rolling statistics instead\n",
    "            rolling_mean = prices.rolling(window=20).mean()\n",
    "            rolling_std = prices.rolling(window=20).std()\n",
    "            axes[1, 1].plot(rolling_mean.values, label='Rolling Mean')\n",
    "            axes[1, 1].fill_between(range(len(rolling_mean)), \n",
    "                                     rolling_mean - 2*rolling_std,\n",
    "                                     rolling_mean + 2*rolling_std, \n",
    "                                     alpha=0.2, label='\u00b12 Std Dev')\n",
    "            axes[1, 1].set_title('Rolling Statistics (20-period)', fontsize=12, fontweight='bold')\n",
    "            axes[1, 1].legend()\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Distribution Analysis Insights:\n",
    "        \n",
    "        1. PRICE DISTRIBUTION:\n",
    "           - Bell-shaped: Normal distribution\n",
    "           - Skewed right: More low prices, few high prices\n",
    "           - Multiple peaks: Multiple regimes or stocks\n",
    "        \n",
    "        2. RETURN DISTRIBUTION:\n",
    "           - Should be centered around 0\n",
    "           - Fat tails: Extreme returns more common than normal\n",
    "           - Common in stock markets (leptokurtic)\n",
    "        \n",
    "        3. Q-Q PLOT:\n",
    "           - Points on line: Normal distribution\n",
    "           - S-curves: Heavy tails\n",
    "           - Deviations at ends: Outliers present\n",
    "        \"\"\")\n",
    "    \n",
    "    def seasonal_subseries_plot(self, period=5):\n",
    "        \"\"\"\n",
    "        Create subseries plot to visualize seasonal patterns.\n",
    "        \n",
    "        Each subseries shows the values for a specific \"season\"\n",
    "        (e.g., day of week) across time.\n",
    "        \"\"\"\n",
    "        prices = self.data[self.price_column].values\n",
    "        n = len(prices)\n",
    "        \n",
    "        # Group by period position\n",
    "        fig, axes = plt.subplots(1, period, figsize=(14, 4), sharey=True)\n",
    "        \n",
    "        for i in range(period):\n",
    "            # Get all values at position i in each period\n",
    "            indices = range(i, n, period)\n",
    "            values = prices[list(indices)]\n",
    "            \n",
    "            axes[i].plot(values, linewidth=1)\n",
    "            axes[i].axhline(y=values.mean(), color='red', linestyle='--', linewidth=2)\n",
    "            axes[i].set_title(f'Position {i+1}')\n",
    "            axes[i].set_xlabel('Period Number')\n",
    "        \n",
    "        axes[0].set_ylabel('Price (NPR)')\n",
    "        fig.suptitle('Subseries Plot (by Position in Period)', fontsize=14, fontweight='bold')\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcc8 Subseries Plot Interpretation:\n",
    "        \n",
    "        Each panel shows all values at a specific position within each period.\n",
    "        - For period=5 (trading week): Shows Mon, Tue, Wed, Thu, Fri patterns\n",
    "        - Horizontal red line: Mean for that position\n",
    "        - Consistent patterns across positions suggest seasonality\n",
    "        \"\"\")\n",
    "    \n",
    "    def lag_scatter_plot(self, lags=[1, 2, 5, 10]):\n",
    "        \"\"\"\n",
    "        Create scatter plots of y(t) vs y(t-k).\n",
    "        \n",
    "        This visualizes autocorrelation structure.\n",
    "        \"\"\"\n",
    "        prices = self.data[self.price_column].dropna()\n",
    "        \n",
    "        fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "        \n",
    "        for i, lag in enumerate(lags):\n",
    "            row, col = i // 2, i % 2\n",
    "            ax = axes[row, col]\n",
    "            \n",
    "            y = prices.values[lag:]\n",
    "            y_lag = prices.values[:-lag]\n",
    "            \n",
    "            ax.scatter(y_lag, y, alpha=0.3, s=10)\n",
    "            \n",
    "            # Add regression line\n",
    "            z = np.polyfit(y_lag, y, 1)\n",
    "            p = np.poly1d(z)\n",
    "            ax.plot(y_lag, p(y_lag), \"r--\", alpha=0.8, linewidth=2)\n",
    "            \n",
    "            # Calculate correlation\n",
    "            corr = np.corrcoef(y_lag, y)[0, 1]\n",
    "            \n",
    "            ax.set_title(f'Lag {lag}: Correlation = {corr:.3f}', fontsize=12)\n",
    "            ax.set_xlabel(f'y(t-{lag})')\n",
    "            ax.set_ylabel('y(t)')\n",
    "            ax.grid(True, alpha=0.3)\n",
    "        \n",
    "        fig.suptitle('Lag Scatter Plots', fontsize=14, fontweight='bold')\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(\"\"\"\n",
    "        \ud83d\udcca Lag Scatter Plot Interpretation:\n",
    "        \n",
    "        Strong diagonal pattern = High autocorrelation\n",
    "        Scattered points = Low autocorrelation\n",
    "        \n",
    "        For NEPSE:\n",
    "        - Lag 1 usually shows strong positive correlation\n",
    "        - Correlation decreases as lag increases\n",
    "        - Patterns may suggest model type (AR, MA, etc.)\n",
    "        \"\"\")\n",
    "    \n",
    "    def comprehensive_visualization(self, symbol=None):\n",
    "        \"\"\"\n",
    "        Create a comprehensive multi-panel visualization.\n",
    "        \"\"\"\n",
    "        if symbol and 'Symbol' in self.data.columns:\n",
    "            plot_data = self.data[self.data['Symbol'] == symbol].copy()\n",
    "            title_suffix = f' - {symbol}'\n",
    "        else:\n",
    "            plot_data = self.data.copy()\n",
    "            title_suffix = ''\n",
    "        \n",
    "        prices = plot_data[self.price_column]\n",
    "        returns = prices.pct_change().dropna()\n",
    "        \n",
    "        fig = plt.figure(figsize=(16, 12))\n",
    "        \n",
    "        # 1. Price time series\n",
    "        ax1 = plt.subplot(3, 2, 1)\n",
    "        ax1.plot(prices.values, linewidth=1)\n",
    "        ax1.set_title(f'Price Series{title_suffix}', fontweight='bold')\n",
    "        ax1.set_ylabel('Price (NPR)')\n",
    "        ax1.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 2. Returns\n",
    "        ax2 = plt.subplot(3, 2, 2)\n",
    "        ax2.plot(returns.values, linewidth=1, color='green')\n",
    "        ax2.axhline(0, color='black', linestyle='-', alpha=0.3)\n",
    "        ax2.set_title('Daily Returns', fontweight='bold')\n",
    "        ax2.set_ylabel('Return')\n",
    "        ax2.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 3. Return distribution\n",
    "        ax3 = plt.subplot(3, 2, 3)\n",
    "        ax3.hist(returns, bins=50, edgecolor='black', alpha=0.7, density=True)\n",
    "        ax3.set_title('Return Distribution', fontweight='bold')\n",
    "        ax3.set_xlabel('Return')\n",
    "        ax3.set_ylabel('Density')\n",
    "        ax3.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 4. ACF\n",
    "        ax4 = plt.subplot(3, 2, 4)\n",
    "        from statsmodels.graphics.tsaplots import plot_acf\n",
    "        plot_acf(returns.dropna(), lags=30, ax=ax4)\n",
    "        ax4.set_title('ACF of Returns', fontweight='bold')\n",
    "        \n",
    "        # 5. Rolling volatility\n",
    "        ax5 = plt.subplot(3, 2, 5)\n",
    "        volatility = returns.rolling(window=20).std()\n",
    "        ax5.plot(volatility.values, linewidth=1, color='red')\n",
    "        ax5.set_title('Rolling Volatility (20-day)', fontweight='bold')\n",
    "        ax5.set_ylabel('Std Dev')\n",
    "        ax5.set_xlabel('Time Index')\n",
    "        ax5.grid(True, alpha=0.3)\n",
    "        \n",
    "        # 6. Price vs Volume\n",
    "        ax6 = plt.subplot(3, 2, 6)\n",
    "        if 'Vol' in plot_data.columns:\n",
    "            ax6_twin = ax6.twinx()\n",
    "            ax6.plot(prices.values, linewidth=1, label='Price')\n",
    "            ax6_twin.bar(range(len(plot_data)), plot_data['Vol'].values, \n",
    "                         alpha=0.3, color='orange', label='Volume')\n",
    "            ax6.set_title('Price vs Volume', fontweight='bold')\n",
    "            ax6.set_ylabel('Price (NPR)')\n",
    "            ax6_twin.set_ylabel('Volume')\n",
    "            ax6.legend(loc='upper left')\n",
    "            ax6.grid(True, alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        print(f\"\"\"\n",
    "        \ud83d\udcca Comprehensive Visualization Summary{title_suffix}\n",
    "        \n",
    "        Panel 1 (Price Series):\n",
    "        - Shows overall trend and level\n",
    "        \n",
    "        Panel 2 (Returns):\n",
    "        - Centered around 0 for stationary returns\n",
    "        - Clusters indicate volatility clustering\n",
    "        \n",
    "        Panel 3 (Distribution):\n",
    "        - Should be roughly bell-shaped\n",
    "        - Fat tails common in stock returns\n",
    "        \n",
    "        Panel 4 (ACF):\n",
    "        - Shows autocorrelation structure\n",
    "        - Helps determine model order\n",
    "        \n",
    "        Panel 5 (Volatility):\n",
    "        - Shows changing volatility over time\n",
    "        - Peaks indicate turbulent periods\n",
    "        \n",
    "        Panel 6 (Price vs Volume):\n",
    "        - High volume often accompanies large moves\n",
    "        - Can reveal support/resistance levels\n",
    "        \"\"\")\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"TIME-SERIES VISUALIZATION FOR NEPSE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Create sample data\n",
    "viz_data = create_comprehensive_nepse_data()\n",
    "single_stock_data = viz_data[viz_data['Symbol'] == 'ABL'].copy()\n",
    "\n",
    "# Initialize visualizer\n",
    "visualizer = TimeSeriesVisualizer(single_stock_data, price_column='Close')\n",
    "\n",
    "# Create visualizations\n",
    "visualizer.line_plot_overview()\n",
    "visualizer.candlestick_style_plot(n_points=50)\n",
    "visualizer.distribution_analysis()\n",
    "visualizer.seasonal_subseries_plot(period=5)\n",
    "visualizer.lag_scatter_plot()\n",
    "visualizer.comprehensive_visualization()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **2.7 Statistical Summary and Diagnostics**\n",
    "\n",
    "The final section of this chapter covers systematic statistical diagnostics.\n",
    "\n",
    "```python\n",
    "class TimeSeriesDiagnostics:\n",
    "    \"\"\"\n",
    "    Comprehensive statistical diagnostics for time-series data.\n",
    "    \n",
    "    This class provides systematic methods to evaluate\n",
    "    time-series properties and generate diagnostic reports.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data, price_column='Close'):\n",
    "        self.data = data.copy()\n",
    "        self.price_column = price_column\n",
    "        self.diagnostics = {}\n",
    "    \n",
    "    def generate_full_report(self):\n",
    "        \"\"\"\n",
    "        Generate a comprehensive diagnostic report.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"TIME-SERIES DIAGNOSTIC REPORT\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        prices = self.data[self.price_column].dropna()\n",
    "        returns = prices.pct_change().dropna()\n",
    "        \n",
    "        # ========================================\n",
    "        # 1. Basic Statistics\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca SECTION 1: BASIC STATISTICS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        self.diagnostics['basic_stats'] = {\n",
    "            'n_observations': len(prices),\n",
    "            'mean': prices.mean(),\n",
    "            'std': prices.std(),\n",
    "            'min': prices.min(),\n",
    "            'max': prices.max(),\n",
    "            'median': prices.median(),\n",
    "            'skewness': prices.skew(),\n",
    "            'kurtosis': prices.kurtosis()\n",
    "        }\n",
    "        \n",
    "        for key, value in self.diagnostics['basic_stats'].items():\n",
    "            if isinstance(value, float):\n",
    "                print(f\"   {key:20s}: {value:.4f}\")\n",
    "            else:\n",
    "                print(f\"   {key:20s}: {value}\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 2. Stationarity Tests\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca SECTION 2: STATIONARITY TESTS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # ADF test on prices\n",
    "        adf_price = adfuller(prices, autolag='AIC')\n",
    "        print(f\"\\n   ADF Test on Prices:\")\n",
    "        print(f\"      Statistic: {adf_price[0]:.4f}\")\n",
    "        print(f\"      p-value:   {adf_price[1]:.6f}\")\n",
    "        print(f\"      Result:    {'Stationary' if adf_price[1] < 0.05 else 'Non-Stationary'}\")\n",
    "        \n",
    "        # ADF test on returns\n",
    "        adf_return = adfuller(returns.dropna(), autolag='AIC')\n",
    "        print(f\"\\n   ADF Test on Returns:\")\n",
    "        print(f\"      Statistic: {adf_return[0]:.4f}\")\n",
    "        print(f\"      p-value:   {adf_return[1]:.6f}\")\n",
    "        print(f\"      Result:    {'Stationary' if adf_return[1] < 0.05 else 'Non-Stationary'}\")\n",
    "        \n",
    "        self.diagnostics['stationarity'] = {\n",
    "            'price_adf_pvalue': adf_price[1],\n",
    "            'return_adf_pvalue': adf_return[1],\n",
    "            'price_stationary': adf_price[1] < 0.05,\n",
    "            'return_stationary': adf_return[1] < 0.05\n",
    "        }\n",
    "        \n",
    "        # ========================================\n",
    "        # 3. Autocorrelation Analysis\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca SECTION 3: AUTOCORRELATION ANALYSIS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # Ljung-Box test\n",
    "        lb_result = acorr_ljungbox(returns.dropna(), lags=[5, 10, 20], return_df=True)\n",
    "        \n",
    "        print(\"\\n   Ljung-Box Test on Returns:\")\n",
    "        for lag in [5, 10, 20]:\n",
    "            pval = lb_result.loc[lag, 'lb_pvalue']\n",
    "            result = 'Significant' if pval < 0.05 else 'Not Significant'\n",
    "            print(f\"      Lag {lag}: p-value = {pval:.6f} ({result})\")\n",
    "        \n",
    "        # ACF of squared returns (for ARCH effects)\n",
    "        sq_returns = returns.dropna() ** 2\n",
    "        acf_sq = acf(sq_returns, nlags=10, fft=True)\n",
    "        \n",
    "        print(f\"\\n   ACF of Squared Returns (ARCH indicator):\")\n",
    "        print(f\"      Lag 1: {acf_sq[1]:.4f}\")\n",
    "        print(f\"      Lag 5: {acf_sq[5]:.4f}\")\n",
    "        \n",
    "        if acf_sq[1] > 0.1:\n",
    "            print(\"      \u2192 ARCH effects may be present\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 4. Normality Tests\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca SECTION 4: NORMALITY TESTS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        from scipy.stats import jarque_bera, shapiro, normaltest\n",
    "        \n",
    "        # Jarque-Bera test\n",
    "        jb_stat, jb_pvalue = jarque_bera(returns.dropna())\n",
    "        print(f\"\\n   Jarque-Bera Test:\")\n",
    "        print(f\"      Statistic: {jb_stat:.4f}\")\n",
    "        print(f\"      p-value:   {jb_pvalue:.6f}\")\n",
    "        print(f\"      Result:    {'Not Normal' if jb_pvalue < 0.05 else 'Normal'}\")\n",
    "        \n",
    "        # Shapiro-Wilk test (for smaller samples)\n",
    "        if len(returns) < 5000:\n",
    "            sw_stat, sw_pvalue = shapiro(returns.dropna().values[:5000])\n",
    "            print(f\"\\n   Shapiro-Wilk Test:\")\n",
    "            print(f\"      Statistic: {sw_stat:.4f}\")\n",
    "            print(f\"      p-value:   {sw_pvalue:.6f}\")\n",
    "            print(f\"      Result:    {'Not Normal' if sw_pvalue < 0.05 else 'Normal'}\")\n",
    "        \n",
    "        print(f\"\\n   Skewness: {returns.skew():.4f}\")\n",
    "        print(f\"   Kurtosis: {returns.kurtosis():.4f}\")\n",
    "        print(\"   (Normal = skewness \u2248 0, kurtosis \u2248 0)\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 5. Volatility Analysis\n",
    "        # ========================================\n",
    "        print(\"\\n\ud83d\udcca SECTION 5: VOLATILITY ANALYSIS\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        # ARCH test\n",
    "        arch_result = het_arch(returns.dropna(), nlags=5)\n",
    "        \n",
    "        print(f\"\\n   Engle's ARCH Test:\")\n",
    "        print(f\"      LM Statistic: {arch_result[0]:.4f}\")\n",
    "        print(f\"      p-value:      {arch_result[1]:.6f}\")\n",
    "        print(f\"      Result:       {'ARCH Effects Present' if arch_result[1] < 0.05 else 'No ARCH Effects'}\")\n",
    "        \n",
    "        # Rolling volatility stats\n",
    "        rolling_vol = returns.rolling(window=20).std()\n",
    "        \n",
    "        print(f\"\\n   Rolling Volatility Statistics:\")\n",
    "        print(f\"      Mean:   {rolling_vol.mean():.6f}\")\n",
    "        print(f\"      Std:    {rolling_vol.std():.6f}\")\n",
    "        print(f\"      Min:    {rolling_vol.min():.6f}\")\n",
    "        print(f\"      Max:    {rolling_vol.max():.6f}\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 6. Summary and Recommendations\n",
    "        # ========================================\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"DIAGNOSTIC SUMMARY AND RECOMMENDATIONS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        recommendations = []\n",
    "        \n",
    "        # Check stationarity\n",
    "        if not self.diagnostics['stationarity']['price_stationary']:\n",
    "            recommendations.append({\n",
    "                'issue': 'Non-stationary prices',\n",
    "                'impact': 'Cannot use standard AR models directly',\n",
    "                'solution': 'Apply differencing or use ARIMA models'\n",
    "            })\n",
    "        \n",
    "        # Check for ARCH effects\n",
    "        if arch_result[1] < 0.05:\n",
    "            recommendations.append({\n",
    "                'issue': 'ARCH effects detected',\n",
    "                'impact': 'Volatility clustering present',\n",
    "                'solution': 'Consider GARCH models for volatility forecasting'\n",
    "            })\n",
    "        \n",
    "        # Check normality\n",
    "        if jb_pvalue < 0.05:\n",
    "            recommendations.append({\n",
    "                'issue': 'Non-normal returns',\n",
    "                'impact': 'Standard errors may be unreliable',\n",
    "                'solution': 'Use robust standard errors or bootstrap'\n",
    "            })\n",
    "        \n",
    "        # Check autocorrelation\n",
    "        if lb_result.loc[5, 'lb_pvalue'] < 0.05:\n",
    "            recommendations.append({\n",
    "                'issue': 'Significant autocorrelation in returns',\n",
    "                'impact': 'Predictable patterns exist',\n",
    "                'solution': 'Use AR/MA terms or lagged features'\n",
    "            })\n",
    "        \n",
    "        print(\"\\n\ud83d\udccb Issues Detected and Solutions:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        if recommendations:\n",
    "            for i, rec in enumerate(recommendations, 1):\n",
    "                print(f\"\\n   Issue {i}: {rec['issue']}\")\n",
    "                print(f\"      Impact:   {rec['impact']}\")\n",
    "                print(f\"      Solution: {rec['solution']}\")\n",
    "        else:\n",
    "            print(\"\\n   \u2713 No major issues detected!\")\n",
    "        \n",
    "        # Model suggestions\n",
    "        print(\"\\n\ud83d\udccb Suggested Modeling Approaches:\")\n",
    "        print(\"-\" * 70)\n",
    "        \n",
    "        if self.diagnostics['stationarity']['return_stationary']:\n",
    "            print(\"   \u2713 Returns are stationary \u2192 Can use ARMA models\")\n",
    "        \n",
    "        if arch_result[1] < 0.05:\n",
    "            print(\"   \u2713 ARCH effects present \u2192 Consider GARCH modeling\")\n",
    "        \n",
    "        print(\"\\n   Recommended models for NEPSE price prediction:\")\n",
    "        print(\"   1. ARIMA for point forecasts (after differencing)\")\n",
    "        print(\"   2. GARCH for volatility forecasting\")\n",
    "        print(\"   3. Machine Learning (Random Forest, XGBoost) with lag features\")\n",
    "        print(\"   4. LSTM/Transformer for sequence modeling\")\n",
    "        \n",
    "        return self.diagnostics\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# FINAL EXAMPLE - RUNNING COMPLETE DIAGNOSTICS\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"COMPLETE TIME-SERIES DIAGNOSTICS FOR NEPSE DATA\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "# Use previously created data\n",
    "diagnostic_data = create_comprehensive_nepse_data()\n",
    "single_stock = diagnostic_data[diagnostic_data['Symbol'] == 'ABL'].copy()\n",
    "\n",
    "# Run diagnostics\n",
    "diagnostics = TimeSeriesDiagnostics(single_stock, price_column='Close')\n",
    "report = diagnostics.generate_full_report()\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# CHAPTER SUMMARY\n",
    "# ============================================================\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"CHAPTER 2 SUMMARY\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\"\"\n",
    "\ud83d\udcda KEY CONCEPTS COVERED:\n",
    "\n",
    "1. TIME-SERIES CHARACTERISTICS\n",
    "   \u2022 Temporal ordering - sequence matters\n",
    "   \u2022 Time intervals - regular vs irregular\n",
    "   \u2022 Sequential dependence - past affects future\n",
    "\n",
    "2. TIME-SERIES COMPONENTS\n",
    "   \u2022 Trend - long-term direction\n",
    "   \u2022 Seasonality - repeating patterns (fixed period)\n",
    "   \u2022 Cyclicality - longer-term fluctuations (variable period)\n",
    "   \u2022 Irregularity - random noise/residual\n",
    "\n",
    "3. TIME-SERIES PROPERTIES\n",
    "   \u2022 Stationarity - constant statistical properties\n",
    "   \u2022 Autocorrelation - correlation with lagged self\n",
    "   \u2022 Heteroscedasticity - changing variance\n",
    "\n",
    "4. DATA CHALLENGES\n",
    "   \u2022 Missing values and imputation\n",
    "   \u2022 Outliers and their treatment\n",
    "   \u2022 Non-stationarity and transformation\n",
    "\n",
    "5. ANALYSIS TECHNIQUES\n",
    "   \u2022 Visual inspection methods\n",
    "   \u2022 Statistical tests (ADF, KPSS, ARCH, Ljung-Box)\n",
    "   \u2022 Diagnostic reporting\n",
    "\n",
    "\ud83d\udca1 FOR NEPSE PREDICTION:\n",
    "\n",
    "1. ALWAYS CHECK STATIONARITY FIRST\n",
    "   - Prices are typically non-stationary\n",
    "   - Returns are typically stationary\n",
    "   - Use differencing for modeling\n",
    "\n",
    "2. ANALYZE AUTOCORRELATION\n",
    "   - Helps determine model order\n",
    "   - Identifies exploitable patterns\n",
    "   - Guides feature engineering\n",
    "\n",
    "3. CONSIDER VOLATILITY CLUSTERING\n",
    "   - Common in stock markets\n",
    "   - May need GARCH models\n",
    "   - Important for risk management\n",
    "\n",
    "4. VISUALIZE YOUR DATA\n",
    "   - Patterns visible in plots\n",
    "   - Helps identify issues\n",
    "   - Guides modeling decisions\n",
    "\n",
    "\ud83d\udcd6 NEXT STEPS:\n",
    "\n",
    "In Chapter 3, we will set up the development environment\n",
    "for building time-series prediction systems, including:\n",
    "- Python installation and configuration\n",
    "- Essential libraries for time-series analysis\n",
    "- IDE setup and best practices\n",
    "- Project structure for prediction systems\n",
    "\"\"\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='1. introduction_to_time_series_prediction_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='3. setting_up_your_development_environment.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}