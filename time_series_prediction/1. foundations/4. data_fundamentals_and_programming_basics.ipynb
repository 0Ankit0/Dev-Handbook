{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 4: Data Fundamentals and Programming Basics**\n",
    "\n",
    "---\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Use NumPy and pandas for time-series data manipulation\n",
    "- Load time-series data from various sources (CSV, databases, APIs)\n",
    "- Handle missing values and outliers effectively\n",
    "- Perform data type conversions and transformations\n",
    "- Aggregate and resample time-series data\n",
    "- Index and select data based on time and conditions\n",
    "- Conduct basic exploratory data analysis\n",
    "- Assess and document data quality\n",
    "\n",
    "---\n",
    "\n",
    "## **Prerequisites**\n",
    "\n",
    "- Completed Chapter 3: Setting Up Your Development Environment\n",
    "- Python 3.8+ installed with required libraries\n",
    "- Basic understanding of programming concepts\n",
    "\n",
    "---\n",
    "\n",
    "## **4.1 Python for Data Science**\n",
    "\n",
    "Python has become the de facto language for data science and machine learning due to its rich ecosystem of libraries. For time-series prediction systems, two libraries form the foundation: NumPy for numerical computing and pandas for data manipulation.\n",
    "\n",
    "### **4.1.1 NumPy Fundamentals**\n",
    "\n",
    "NumPy (Numerical Python) provides efficient multi-dimensional arrays and mathematical operations. Understanding NumPy is essential because pandas is built on top of it, and most machine learning libraries use NumPy arrays internally.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "# Creating NumPy arrays - the fundamental data structure\n",
    "# Arrays are homogeneous (all elements same type) and stored contiguously in memory\n",
    "\n",
    "# Create array from a list - basic creation method\n",
    "prices_list = [2850.50, 2875.25, 2890.00, 2865.75, 2880.50]\n",
    "prices_array = np.array(prices_list)\n",
    "print(f\"Array: {prices_array}\")\n",
    "print(f\"Type: {type(prices_array)}\")\n",
    "print(f\"Data type: {prices_array.dtype}\")\n",
    "print(f\"Shape: {prices_array.shape}\")\n",
    "\n",
    "# Output:\n",
    "# Array: [2850.5  2875.25 2890.   2865.75 2880.5 ]\n",
    "# Type: <class 'numpy.ndarray'>\n",
    "# Data type: float64\n",
    "# Shape: (5,)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `np.array()` converts a Python list into a NumPy array. Unlike Python lists, NumPy arrays are stored as contiguous blocks of memory, making operations much faster.\n",
    "- `dtype` (data type) shows the type of elements. `float64` means 64-bit floating-point numbers, which is the default for decimal numbers.\n",
    "- `shape` returns a tuple representing dimensions. `(5,)` means a 1-dimensional array with 5 elements.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Creating arrays with specific patterns - useful for initialization\n",
    "\n",
    "# Array of zeros - useful for initializing accumulators or masks\n",
    "zeros = np.zeros(5)\n",
    "print(f\"Zeros: {zeros}\")\n",
    "\n",
    "# Array of ones - useful for creating weight vectors or multipliers\n",
    "ones = np.ones(5)\n",
    "print(f\"Ones: {ones}\")\n",
    "\n",
    "# Array with range of values - similar to Python's range() but returns array\n",
    "range_array = np.arange(0, 10, 2)  # start, stop, step\n",
    "print(f\"Range: {range_array}\")\n",
    "\n",
    "# Linearly spaced values - creates evenly spaced numbers over an interval\n",
    "# Useful for creating time indices or price level grids\n",
    "linear_space = np.linspace(0, 100, 5)  # start, stop, num_points\n",
    "print(f\"Linear space: {linear_space}\")\n",
    "\n",
    "# Output:\n",
    "# Zeros: [0. 0. 0. 0. 0.]\n",
    "# Ones: [1. 1. 1. 1. 1.]\n",
    "# Range: [0 2 4 6 8]\n",
    "# Linear space: [  0.  25.  50.  75. 100.]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `np.zeros()` creates an array filled with zeros. In time-series, we often use this to initialize arrays for storing predictions or intermediate calculations.\n",
    "- `np.ones()` creates an array filled with ones. Useful for creating moving average weights or normalization factors.\n",
    "- `np.arange()` is like Python's `range()` but returns a NumPy array. The `step` parameter controls the spacing between values.\n",
    "- `np.linspace()` creates evenly spaced numbers. Unlike `arange()`, you specify the number of points rather than the step size. This is useful for creating time axes or price grids for analysis.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Multi-dimensional arrays - representing time-series with multiple features\n",
    "\n",
    "# 2D array representing 3 days of NEPSE stock data with 4 features each\n",
    "# Each row is a day, each column is a feature\n",
    "stock_data = np.array([\n",
    "    [2850.50, 2890.00, 2840.00, 2875.25],  # Day 1: Open, High, Low, Close\n",
    "    [2875.25, 2910.00, 2860.00, 2895.50],  # Day 2\n",
    "    [2895.50, 2920.00, 2880.00, 2900.00],  # Day 3\n",
    "])\n",
    "\n",
    "print(f\"Shape: {stock_data.shape}\")  # (3, 4) - 3 rows, 4 columns\n",
    "print(f\"Dimensions: {stock_data.ndim}\")  # 2 - number of dimensions\n",
    "print(f\"Size: {stock_data.size}\")  # 12 - total number of elements\n",
    "\n",
    "# Accessing elements in 2D array\n",
    "print(f\"First row (Day 1): {stock_data[0]}\")\n",
    "print(f\"First column (All Opens): {stock_data[:, 0]}\")\n",
    "print(f\"Specific element (Day 2 High): {stock_data[1, 1]}\")\n",
    "\n",
    "# Output:\n",
    "# Shape: (3, 4)\n",
    "# Dimensions: 2\n",
    "# Size: 12\n",
    "# First row (Day 1): [2850.5 2890.   2840.   2875.25]\n",
    "# First column (All Opens): [2850.5  2875.25 2895.5 ]\n",
    "# Specific element (Day 2 High): 2910.0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- Multi-dimensional arrays are perfect for representing time-series with multiple features. Each row typically represents a time point, and each column represents a feature.\n",
    "- `shape` returns `(rows, columns)`. `(3, 4)` means 3 time points with 4 features each.\n",
    "- `ndim` returns the number of dimensions (axes). A 2D array has 2 dimensions.\n",
    "- `size` returns the total number of elements (rows × columns).\n",
    "- Indexing uses `[row, column]` syntax. The colon `:` means \"all elements along this axis.\"\n",
    "- `stock_data[:, 0]` means \"all rows, column 0\" - extracting the Open prices for all days.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Mathematical operations on arrays - vectorized operations for efficiency\n",
    "\n",
    "# Sample closing prices for a week\n",
    "close_prices = np.array([2850.50, 2875.25, 2890.00, 2865.75, 2880.50])\n",
    "\n",
    "# Element-wise arithmetic - operations apply to each element automatically\n",
    "adjusted_prices = close_prices + 10  # Add 10 to each price\n",
    "print(f\"Adjusted prices: {adjusted_prices}\")\n",
    "\n",
    "percentage_of_start = (close_prices / close_prices[0]) * 100  # Normalize to percentage\n",
    "print(f\"Percentage of start: {percentage_of_start}\")\n",
    "\n",
    "# Statistical operations - essential for time-series analysis\n",
    "print(f\"Mean price: {np.mean(close_prices):.2f}\")\n",
    "print(f\"Standard deviation: {np.std(close_prices):.2f}\")\n",
    "print(f\"Minimum: {np.min(close_prices):.2f}\")\n",
    "print(f\"Maximum: {np.max(close_prices):.2f}\")\n",
    "print(f\"Range: {np.ptp(close_prices):.2f}\")  # ptp = peak to peak (max - min)\n",
    "\n",
    "# Output:\n",
    "# Adjusted prices: [2860.5  2885.25 2900.   2875.75 2890.5 ]\n",
    "# Percentage of start: [100.          100.86824561 101.38500219 100.53420452 101.0505613 ]\n",
    "# Mean price: 2872.40\n",
    "# Standard deviation: 14.16\n",
    "# Minimum: 2850.50\n",
    "# Maximum: 2890.00\n",
    "# Range: 39.50\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Vectorization** is NumPy's key advantage. Operations like `close_prices + 10` apply to all elements simultaneously without explicit loops, making code faster and cleaner.\n",
    "- Division by the first element normalizes all prices relative to the starting point, useful for comparing different stocks on the same scale.\n",
    "- `np.mean()` calculates the average - fundamental for understanding central tendency in price data.\n",
    "- `np.std()` calculates standard deviation - measures price volatility, crucial for risk assessment.\n",
    "- `np.ptp()` (peak to peak) calculates the range (max - min), showing price spread during the period.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Boolean indexing and filtering - selecting data based on conditions\n",
    "\n",
    "# Sample prices with some anomalies\n",
    "prices = np.array([2850.50, 2875.25, 5000.00, 2890.00, 2865.75, 0.0, 2880.50])\n",
    "\n",
    "# Create boolean mask - returns True/False for each element\n",
    "mask_high = prices > 3000  # Find abnormally high prices\n",
    "print(f\"High price mask: {mask_high}\")\n",
    "\n",
    "# Use mask to filter - extracts only True elements\n",
    "high_prices = prices[mask_high]\n",
    "print(f\"High prices: {high_prices}\")\n",
    "\n",
    "# Find valid prices (not zero, not abnormally high)\n",
    "mask_valid = (prices > 0) & (prices < 3000)  # Combine conditions with &\n",
    "valid_prices = prices[mask_valid]\n",
    "print(f\"Valid prices: {valid_prices}\")\n",
    "\n",
    "# Count valid and invalid prices\n",
    "print(f\"Valid count: {np.sum(mask_valid)}\")  # True counts as 1\n",
    "print(f\"Invalid count: {np.sum(~mask_valid)}\")  # ~ is NOT operator\n",
    "\n",
    "# Output:\n",
    "# High price mask: [False False  True False False False False]\n",
    "# High prices: [5000.]\n",
    "# Valid prices: [2850.5  2875.25 2890.   2865.75 2880.5 ]\n",
    "# Valid count: 5\n",
    "# Invalid count: 2\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Boolean indexing** is powerful for filtering time-series data. A comparison like `prices > 3000` creates a boolean array (True/False for each element).\n",
    "- Using a boolean mask as an index extracts only elements where the mask is True.\n",
    "- Multiple conditions can be combined with `&` (AND), `|` (OR). Each condition must be in parentheses.\n",
    "- `~` is the NOT operator, inverting the boolean mask.\n",
    "- `np.sum()` on a boolean array counts True values (True = 1, False = 0), useful for counting how many data points meet certain criteria.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.2 pandas Essentials**\n",
    "\n",
    "pandas is the most important library for time-series data manipulation. It provides DataFrame (2D table) and Series (1D array) data structures with powerful indexing, grouping, and time-series functionality.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "\n",
    "# Creating a Series - 1D labeled array\n",
    "# A Series combines data with an index (labels for each element)\n",
    "\n",
    "# Create Series from a list with custom index (trading days)\n",
    "close_prices = pd.Series(\n",
    "    [2850.50, 2875.25, 2890.00, 2865.75, 2880.50],\n",
    "    index=['Day1', 'Day2', 'Day3', 'Day4', 'Day5'],\n",
    "    name='Close_Price'\n",
    ")\n",
    "\n",
    "print(\"Series:\")\n",
    "print(close_prices)\n",
    "print(f\"\\nSeries values: {close_prices.values}\")  # NumPy array\n",
    "print(f\"Series index: {close_prices.index}\")  # Index object\n",
    "print(f\"Series name: {close_prices.name}\")\n",
    "\n",
    "# Accessing by label vs position\n",
    "print(f\"\\nAccess by label: {close_prices['Day3']}\")  # Using index label\n",
    "print(f\"Access by position: {close_prices.iloc[2]}\")  # Using integer position\n",
    "\n",
    "# Output:\n",
    "# Series:\n",
    "# Day1    2850.50\n",
    "# Day2    2875.25\n",
    "# Day3    2890.00\n",
    "# Day4    2865.75\n",
    "# Day5    2880.50\n",
    "# Name: Close_Price, dtype: float64\n",
    "#\n",
    "# Series values: [2850.5  2875.25 2890.   2865.75 2880.5 ]\n",
    "# Series index: Index(['Day1', 'Day2', 'Day3', 'Day4', 'Day5'], dtype='object')\n",
    "# Series name: Close_Price\n",
    "#\n",
    "# Access by label: 2890.0\n",
    "# Access by position: 2890.0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- A **Series** is pandas' 1D data structure, like a column in a spreadsheet. It combines data with an index (row labels).\n",
    "- The `index` parameter assigns custom labels to each element. Without it, pandas uses default integer indices (0, 1, 2...).\n",
    "- `name` gives the Series a label, useful when it becomes a DataFrame column.\n",
    "- `.values` returns the underlying NumPy array - useful when you need NumPy operations.\n",
    "- `.index` returns the index object containing the labels.\n",
    "- Access by label uses square brackets with the label name (`close_prices['Day3']`).\n",
    "- `.iloc[]` accesses by integer position, useful when you want position-based access regardless of index type.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Creating a DataFrame - 2D labeled data structure\n",
    "# A DataFrame is like a spreadsheet or SQL table with rows and columns\n",
    "\n",
    "# Sample NEPSE stock data for multiple days\n",
    "nepse_data = {\n",
    "    'Symbol': ['NABIL', 'NABIL', 'NABIL', 'NABIL', 'NABIL'],\n",
    "    'Date': ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19'],\n",
    "    'Open': [2850.50, 2875.25, 2890.00, 2865.75, 2880.50],\n",
    "    'High': [2890.00, 2910.00, 2920.00, 2900.00, 2915.00],\n",
    "    'Low': [2840.00, 2860.00, 2880.00, 2850.00, 2870.00],\n",
    "    'Close': [2875.25, 2895.50, 2900.00, 2880.50, 2905.00],\n",
    "    'Volume': [125000, 150000, 175000, 140000, 160000],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(nepse_data)\n",
    "\n",
    "print(\"DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")  # (rows, columns)\n",
    "print(f\"Columns: {df.columns.tolist()}\")\n",
    "print(f\"Index: {df.index}\")\n",
    "print(f\"Data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Output:\n",
    "# DataFrame:\n",
    "#   Symbol        Date     Open     High      Low    Close  Volume\n",
    "# 0  NABIL  2024-01-15  2850.50  2890.00  2840.00  2875.25  125000\n",
    "# 1  NABIL  2024-01-16  2875.25  2910.00  2860.00  2895.50  150000\n",
    "# 2  NABIL  2024-01-17  2890.00  2920.00  2880.00  2900.00  175000\n",
    "# 3  NABIL  2024-01-18  2865.75  2900.00  2850.00  2880.50  140000\n",
    "# 4  NABIL  2024-01-19  2880.50  2915.00  2870.00  2905.00  160000\n",
    "#\n",
    "# Shape: (5, 7)\n",
    "# Columns: ['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume']\n",
    "# Index: RangeIndex(start=0, stop=5, step=1)\n",
    "# Data types:\n",
    "# Symbol      object\n",
    "# Date        object\n",
    "# Open       float64\n",
    "# High       float64\n",
    "# Low        float64\n",
    "# Close      float64\n",
    "# Volume       int64\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- A **DataFrame** is pandas' primary 2D data structure. Think of it as a table with labeled rows and columns.\n",
    "- Created from a dictionary where keys become column names and values become column data.\n",
    "- `.shape` returns `(rows, columns)` as a tuple.\n",
    "- `.columns` returns the column labels as an Index object. `.tolist()` converts it to a Python list.\n",
    "- `.index` shows the row labels. By default, pandas uses RangeIndex (0, 1, 2...).\n",
    "- `.dtypes` shows the data type of each column. `object` typically means strings, `float64` for decimals, `int64` for integers.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# DataFrame column access and manipulation\n",
    "\n",
    "# Access single column (returns Series)\n",
    "close_series = df['Close']\n",
    "print(f\"Close prices (Series):\\n{close_series}\")\n",
    "\n",
    "# Access multiple columns (returns DataFrame)\n",
    "price_columns = df[['Open', 'High', 'Low', 'Close']]\n",
    "print(f\"\\nPrice columns (DataFrame):\\n{price_columns.head()}\")\n",
    "\n",
    "# Add new column - calculated field\n",
    "df['Range'] = df['High'] - df['Low']  # Daily price range\n",
    "df['Change'] = df['Close'] - df['Open']  # Daily change\n",
    "df['Change_Pct'] = (df['Change'] / df['Open']) * 100  # Percentage change\n",
    "\n",
    "print(f\"\\nDataFrame with new columns:\\n{df}\")\n",
    "\n",
    "# Modify existing column\n",
    "df['Volume'] = df['Volume'] / 1000  # Convert to thousands\n",
    "print(f\"\\nVolume in thousands:\\n{df['Volume']}\")\n",
    "\n",
    "# Delete column\n",
    "df_dropped = df.drop('Symbol', axis=1)  # axis=1 means column\n",
    "print(f\"\\nAfter dropping Symbol column:\\n{df_dropped.head()}\")\n",
    "\n",
    "# Output:\n",
    "# Close prices (Series):\n",
    "# 0    2875.25\n",
    "# 1    2895.50\n",
    "# 2    2900.00\n",
    "# 3    2880.50\n",
    "# 4    2905.00\n",
    "# Name: Close, dtype: float64\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- Accessing a single column with `df['ColumnName']` returns a Series (1D).\n",
    "- Accessing multiple columns with `df[['Col1', 'Col2']]` returns a DataFrame. Note the double brackets (a list inside brackets).\n",
    "- New columns are created by assignment: `df['NewColumn'] = values`. Values can be calculated from existing columns using vectorized operations.\n",
    "- **Vectorized operations** between columns (like `df['High'] - df['Low']`) apply to each row automatically, much faster than iterating.\n",
    "- Columns can be modified in place by reassignment.\n",
    "- `.drop()` removes rows or columns. `axis=1` specifies columns (axis=0 is rows). Returns a new DataFrame by default; use `inplace=True` to modify the original.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# DataFrame row access - loc vs iloc\n",
    "\n",
    "# Create sample DataFrame with date index\n",
    "df_dates = df.copy()\n",
    "df_dates['Date'] = pd.to_datetime(df_dates['Date'])\n",
    "df_dates.set_index('Date', inplace=True)  # Set Date as index\n",
    "\n",
    "print(\"DataFrame with date index:\")\n",
    "print(df_dates)\n",
    "\n",
    "# iloc - integer position based (position 0, 1, 2, ...)\n",
    "print(f\"\\nFirst row (iloc[0]):\\n{df_dates.iloc[0]}\")\n",
    "print(f\"\\nLast row (iloc[-1]):\\n{df_dates.iloc[-1]}\")\n",
    "print(f\"\\nFirst 3 rows:\\n{df_dates.iloc[:3]}\")\n",
    "print(f\"\\nSpecific rows and columns:\\n{df_dates.iloc[0:3, 0:4]}\")  # rows 0-2, cols 0-3\n",
    "\n",
    "# loc - label based (using index labels)\n",
    "print(f\"\\nRow for 2024-01-15:\\n{df_dates.loc['2024-01-15']}\")\n",
    "print(f\"\\nRows for date range:\\n{df_dates.loc['2024-01-15':'2024-01-17']}\")\n",
    "\n",
    "# Output:\n",
    "# DataFrame with date index:\n",
    "#             Symbol     Open     High      Low    Close  Volume   Range  Change  Change_Pct\n",
    "# Date\n",
    "# 2024-01-15   NABIL  2850.50  2890.00  2840.00  2875.25   125.0    50.0   24.75    0.867692\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `.set_index()` sets a column as the DataFrame index. `inplace=True` modifies the DataFrame directly.\n",
    "- `.iloc[]` is **integer-location based** indexing. Use it when you want to access by position (like Python lists).\n",
    "- `.loc[]` is **label-based** indexing. Use it when you want to access by index labels (like dates or names).\n",
    "- Both support slicing: `iloc[0:3]` gets positions 0, 1, 2 (exclusive end). `loc['2024-01-15':'2024-01-17']` includes both endpoints.\n",
    "- `.iloc[0:3, 0:4]` accesses both rows and columns by position: rows 0-2, columns 0-3.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Filtering DataFrames with conditions\n",
    "\n",
    "# Filter rows based on condition\n",
    "high_volume = df[df['Volume'] > 140]\n",
    "print(f\"Days with volume > 140k:\\n{high_volume}\")\n",
    "\n",
    "# Multiple conditions\n",
    "complex_filter = df[(df['Close'] > 2880) & (df['Volume'] > 130)]\n",
    "print(f\"\\nDays with Close > 2880 AND Volume > 130k:\\n{complex_filter}\")\n",
    "\n",
    "# Using isin() for categorical filtering\n",
    "symbols_of_interest = ['NABIL', 'NICA']\n",
    "# This would work if we had multiple symbols:\n",
    "# filtered = df[df['Symbol'].isin(symbols_of_interest)]\n",
    "\n",
    "# Query method - SQL-like syntax\n",
    "query_result = df.query('Close > 2880 and Volume > 130')\n",
    "print(f\"\\nQuery result:\\n{query_result}\")\n",
    "\n",
    "# Output:\n",
    "# Days with volume > 140k:\n",
    "#   Symbol        Date     Open     High      Low    Close  Volume   Range  Change  Change_Pct\n",
    "# 1  NABIL  2024-01-16  2875.25  2910.00  2860.00  2895.50   150.0    50.0   20.25    0.704259\n",
    "# 2  NABIL  2024-01-17  2890.00  2920.00  2880.00  2900.00   175.0    40.0   10.00    0.346021\n",
    "# 4  NABIL  2024-01-19  2880.50  2915.00  2870.00  2905.00   160.0    45.0   24.50    0.850846\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- Filtering uses boolean indexing: `df[condition]` returns rows where condition is True.\n",
    "- The condition `df['Volume'] > 140` creates a boolean Series (True/False for each row).\n",
    "- Multiple conditions must use `&` (AND) or `|` (OR), with each condition in parentheses.\n",
    "- `.isin()` filters for values in a given list, useful for categorical columns like symbols or sectors.\n",
    "- `.query()` provides SQL-like syntax for filtering. Column names are used directly without quotes, and the string can contain complex conditions.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.1.3 Working with Dates and Times**\n",
    "\n",
    "Time-series analysis heavily depends on proper date and time handling. pandas provides extensive functionality for parsing, manipulating, and resampling time-based data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# Creating datetime objects - Python's built-in datetime module\n",
    "\n",
    "# Current date and time\n",
    "now = datetime.now()\n",
    "print(f\"Current datetime: {now}\")\n",
    "print(f\"Year: {now.year}, Month: {now.month}, Day: {now.day}\")\n",
    "print(f\"Hour: {now.hour}, Minute: {now.minute}, Second: {now.second}\")\n",
    "\n",
    "# Create specific date\n",
    "specific_date = datetime(2024, 1, 15, 9, 30, 0)  # year, month, day, hour, minute, second\n",
    "print(f\"\\nSpecific datetime: {specific_date}\")\n",
    "\n",
    "# Date arithmetic\n",
    "tomorrow = datetime.now() + timedelta(days=1)\n",
    "last_week = datetime.now() - timedelta(weeks=1)\n",
    "print(f\"\\nTomorrow: {tomorrow}\")\n",
    "print(f\"Last week: {last_week}\")\n",
    "\n",
    "# Difference between dates\n",
    "trading_start = datetime(2024, 1, 1)\n",
    "trading_end = datetime(2024, 1, 15)\n",
    "difference = trading_end - trading_start\n",
    "print(f\"\\nDays between: {difference.days}\")\n",
    "print(f\"Total seconds: {difference.total_seconds()}\")\n",
    "\n",
    "# Output:\n",
    "# Current datetime: 2024-01-20 14:30:45.123456\n",
    "# Year: 2024, Month: 1, Day: 20\n",
    "# Hour: 14, Minute: 30, Second: 45\n",
    "#\n",
    "# Specific datetime: 2024-01-15 09:30:00\n",
    "#\n",
    "# Tomorrow: 2024-01-21 14:30:45.123456\n",
    "# Last week: 2024-01-13 14:30:45.123456\n",
    "#\n",
    "# Days between: 14\n",
    "# Total seconds: 1209600.0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `datetime.now()` returns the current local date and time.\n",
    "- `datetime(year, month, day, ...)` creates a specific datetime. Month and day are required; time components default to 0.\n",
    "- `timedelta` represents a duration. You can add/subtract timedelta from datetime objects.\n",
    "- Subtracting two datetime objects returns a `timedelta` object with `days` and `total_seconds()` methods.\n",
    "- These operations form the basis for calculating trading days, time-to-expiry, and other time-based metrics.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# pandas datetime parsing - converting strings to datetime\n",
    "\n",
    "# Sample date strings (as they might appear in NEPSE CSV)\n",
    "date_strings = ['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19']\n",
    "\n",
    "# Convert to datetime\n",
    "dates = pd.to_datetime(date_strings)\n",
    "print(f\"Parsed dates:\\n{dates}\")\n",
    "print(f\"Type: {type(dates)}\")  # DatetimeIndex\n",
    "\n",
    "# Different date formats\n",
    "various_formats = ['15/01/2024', 'Jan 15, 2024', '15-Jan-2024', '20240115']\n",
    "parsed_formats = pd.to_datetime(various_formats)\n",
    "print(f\"\\nParsed various formats:\\n{parsed_formats}\")\n",
    "\n",
    "# Specify format explicitly for non-standard formats\n",
    "nepse_format = pd.to_datetime('15-01-2024', format='%d-%m-%Y')\n",
    "print(f\"\\nCustom format parsed: {nepse_format}\")\n",
    "\n",
    "# Handle errors gracefully\n",
    "problematic_dates = ['2024-01-15', 'invalid-date', '2024-01-17']\n",
    "# This would raise an error:\n",
    "# parsed = pd.to_datetime(problematic_dates)\n",
    "# Use errors='coerce' to convert invalid to NaT (Not a Time)\n",
    "parsed_safe = pd.to_datetime(problematic_dates, errors='coerce')\n",
    "print(f\"\\nWith error handling:\\n{parsed_safe}\")\n",
    "\n",
    "# Output:\n",
    "# Parsed dates:\n",
    "# DatetimeIndex(['2024-01-15', '2024-01-16', '2024-01-17', '2024-01-18', '2024-01-19'], dtype='datetime64[ns]', freq=None)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `pd.to_datetime()` converts strings to pandas datetime objects. It's smart enough to parse many common formats automatically.\n",
    "- The result is a `DatetimeIndex`, which is a specialized Index type for datetime data.\n",
    "- For non-standard formats, use the `format` parameter with strptime-style format codes:\n",
    "  - `%d` = day, `%m` = month, `%Y` = 4-digit year, `%y` = 2-digit year\n",
    "  - `%H` = hour (24h), `%I` = hour (12h), `%M` = minute, `%S` = second\n",
    "- `errors='coerce'` converts invalid dates to `NaT` (Not a Time) instead of raising an error. This is essential when dealing with real-world data that may have typos or missing values.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# DatetimeIndex functionality for time-series\n",
    "\n",
    "# Create a time-series DataFrame\n",
    "dates = pd.date_range('2024-01-01', periods=10, freq='D')  # 10 daily dates\n",
    "print(f\"Date range:\\n{dates}\")\n",
    "\n",
    "# Create sample stock data\n",
    "ts_data = pd.DataFrame({\n",
    "    'Close': [2850.50, 2875.25, 2890.00, 2865.75, 2880.50, \n",
    "              2905.00, 2895.00, 2910.00, 2900.00, 2920.00],\n",
    "    'Volume': [125000, 150000, 175000, 140000, 160000,\n",
    "               180000, 155000, 190000, 170000, 200000]\n",
    "}, index=dates)\n",
    "\n",
    "print(f\"\\nTime-series DataFrame:\\n{ts_data}\")\n",
    "\n",
    "# Access datetime components\n",
    "print(f\"\\nYear: {ts_data.index.year}\")\n",
    "print(f\"Month: {ts_data.index.month}\")\n",
    "print(f\"Day: {ts_data.index.day}\")\n",
    "print(f\"Day of week: {ts_data.index.dayofweek}\")  # Monday=0, Sunday=6\n",
    "print(f\"Day name: {ts_data.index.day_name()}\")\n",
    "\n",
    "# Filter by date\n",
    "print(f\"\\nData for January 5-8:\\n{ts_data['2024-01-05':'2024-01-08']}\")\n",
    "\n",
    "# Filter by month\n",
    "print(f\"\\nAll January data:\\n{ts_data['2024-01']}\")\n",
    "\n",
    "# Output:\n",
    "# Date range:\n",
    "# DatetimeIndex(['2024-01-01', '2024-01-02', '2024-01-03', '2024-01-04',\n",
    "#                '2024-01-05', '2024-01-06', '2024-01-07', '2024-01-08',\n",
    "#                '2024-01-09', '2024-01-10'],\n",
    "#               dtype='datetime64[ns]', freq='D')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `pd.date_range()` creates a sequence of dates. Parameters:\n",
    "  - `start`: Starting date\n",
    "  - `end`: Ending date (alternative to periods)\n",
    "  - `periods`: Number of dates to generate\n",
    "  - `freq`: Frequency ('D'=daily, 'H'=hourly, 'B'=business days, 'W'=weekly, 'M'=month end)\n",
    "- Using a DatetimeIndex enables powerful time-based operations.\n",
    "- **Datetime components** are accessible via properties: `.year`, `.month`, `.day`, `.dayofweek`, `.day_name()`.\n",
    "- **Date-based slicing** works naturally: `ts_data['2024-01-05':'2024-01-08']` gets all data between those dates.\n",
    "- You can filter by just year-month: `ts_data['2024-01']` gets all January 2024 data.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Resampling - converting between time frequencies\n",
    "\n",
    "# Create hourly data (simulating intraday trading)\n",
    "hourly_dates = pd.date_range('2024-01-15 09:00', periods=24, freq='H')\n",
    "hourly_prices = pd.DataFrame({\n",
    "    'Price': [2850 + i * 2 + (i % 3 - 1) * 5 for i in range(24)],\n",
    "    'Volume': [10000 + i * 1000 for i in range(24)]\n",
    "}, index=hourly_dates)\n",
    "\n",
    "print(\"Hourly data (first 5 rows):\")\n",
    "print(hourly_prices.head())\n",
    "\n",
    "# Resample to daily - aggregate hourly to daily\n",
    "daily = hourly_prices.resample('D').agg({\n",
    "    'Price': 'last',  # Closing price is the last price of the day\n",
    "    'Volume': 'sum'   # Total volume is sum of hourly volumes\n",
    "})\n",
    "print(f\"\\nDaily resampled:\\n{daily}\")\n",
    "\n",
    "# Different aggregation methods\n",
    "print(f\"\\nDaily OHLC:\")\n",
    "daily_ohlc = hourly_prices['Price'].resample('D').ohlc()\n",
    "print(daily_ohlc)\n",
    "\n",
    "# Weekly resampling\n",
    "weekly = hourly_prices.resample('W').agg({\n",
    "    'Price': 'last',\n",
    "    'Volume': 'sum'\n",
    "})\n",
    "print(f\"\\nWeekly resampled:\\n{weekly}\")\n",
    "\n",
    "# Downsampling - fill missing times\n",
    "print(f\"\\nUpsampling to 30-min intervals (forward fill):\")\n",
    "upsampled = hourly_prices.resample('30T').ffill()  # 30T = 30 minutes\n",
    "print(upsampled.head(6))\n",
    "\n",
    "# Output:\n",
    "# Hourly data (first 5 rows):\n",
    "#                      Price  Volume\n",
    "# 2024-01-15 09:00:00   2845   10000\n",
    "# 2024-01-15 10:00:00   2850   11000\n",
    "# 2024-01-15 11:00:00   2848   12000\n",
    "# 2024-01-15 12:00:00   2853   13000\n",
    "# 2024-01-15 13:00:00   2858   14000\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Resampling** is crucial for time-series analysis. It converts data between different time frequencies.\n",
    "- `.resample('D')` groups data by day. Other frequencies: 'H'=hourly, 'W'=weekly, 'M'=monthly, 'Q'=quarterly.\n",
    "- **Downsampling** (high to low frequency) requires aggregation:\n",
    "  - `'last'`: Last value in the period (typical for closing prices)\n",
    "  - `'sum'`: Sum of values (typical for volume)\n",
    "  - `'mean'`: Average value\n",
    "  - `'ohlc'`: Open, High, Low, Close (creates 4 columns)\n",
    "- **Upsampling** (low to high frequency) requires filling:\n",
    "  - `.ffill()`: Forward fill - propagate last known value\n",
    "  - `.bfill()`: Backward fill - use next known value\n",
    "  - `.interpolate()`: Linear interpolation between values\n",
    "- Frequency strings: 'D'=day, 'H'=hour, 'T' or 'min'=minute, 'S'=second, 'W'=week, 'M'=month\n",
    "\n",
    "---\n",
    "\n",
    "## **4.2 Data Structures for Time-Series**\n",
    "\n",
    "Choosing the right data structure is essential for efficient time-series analysis. pandas provides several specialized structures, and understanding when to use each is important.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Series vs DataFrame - when to use each\n",
    "\n",
    "# Series: Single time-series (one variable over time)\n",
    "# Use when tracking a single metric\n",
    "nabil_close = pd.Series(\n",
    "    [2850.50, 2875.25, 2890.00, 2865.75, 2880.50],\n",
    "    index=pd.date_range('2024-01-15', periods=5, freq='D'),\n",
    "    name='NABIL_Close'\n",
    ")\n",
    "print(\"Series (single time-series):\")\n",
    "print(nabil_close)\n",
    "print(f\"Shape: {nabil_close.shape}\")  # 1D: (5,)\n",
    "\n",
    "# DataFrame: Multiple time-series (multiple variables over time)\n",
    "# Use when tracking multiple related metrics\n",
    "nabil_data = pd.DataFrame({\n",
    "    'Open': [2840.00, 2870.00, 2885.00, 2860.00, 2875.00],\n",
    "    'High': [2890.00, 2895.00, 2920.00, 2900.00, 2915.00],\n",
    "    'Low': [2835.00, 2865.00, 2880.00, 2850.00, 2870.00],\n",
    "    'Close': [2850.50, 2875.25, 2890.00, 2865.75, 2880.50],\n",
    "    'Volume': [125000, 150000, 175000, 140000, 160000]\n",
    "}, index=pd.date_range('2024-01-15', periods=5, freq='D'))\n",
    "\n",
    "print(\"\\nDataFrame (multiple time-series):\")\n",
    "print(nabil_data)\n",
    "print(f\"Shape: {nabil_data.shape}\")  # 2D: (5, 5)\n",
    "\n",
    "# Panel (deprecated) vs MultiIndex - for 3D data\n",
    "# When you have multiple symbols over multiple time periods with multiple features\n",
    "\n",
    "# MultiIndex approach for panel data (multiple stocks)\n",
    "symbols = ['NABIL', 'NABIL', 'NABIL', 'NICA', 'NICA', 'NICA']\n",
    "dates = pd.to_datetime(['2024-01-15', '2024-01-16', '2024-01-17'] * 2)\n",
    "\n",
    "multi_index = pd.MultiIndex.from_arrays([symbols, dates], names=['Symbol', 'Date'])\n",
    "panel_data = pd.DataFrame({\n",
    "    'Close': [2850.50, 2875.25, 2890.00, 1180.00, 1195.50, 1210.00],\n",
    "    'Volume': [125000, 150000, 175000, 85000, 92000, 98000]\n",
    "}, index=multi_index)\n",
    "\n",
    "print(\"\\nMultiIndex DataFrame (multiple symbols):\")\n",
    "print(panel_data)\n",
    "\n",
    "# Access specific symbol\n",
    "print(f\"\\nNABIL data:\\n{panel_data.loc['NABIL']}\")\n",
    "\n",
    "# Output:\n",
    "# Series (single time-series):\n",
    "# 2024-01-15    2850.50\n",
    "# 2024-01-16    2875.25\n",
    "# 2024-01-17    2890.00\n",
    "# 2024-01-18    2865.75\n",
    "# 2024-01-19    2880.50\n",
    "# Freq: D, Name: NABIL_Close, dtype: float64\n",
    "# Shape: (5,)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Series** is for 1D labeled data. Use it when you have a single metric over time (e.g., just closing prices). It's more memory-efficient for single variables.\n",
    "- **DataFrame** is for 2D labeled data. Use it when you have multiple related metrics over time (e.g., OHLCV data). Each column is a Series.\n",
    "- **MultiIndex** handles 3D-like data (e.g., multiple stocks over multiple time periods). The index has multiple levels (Symbol and Date in this example).\n",
    "- `pd.MultiIndex.from_arrays()` creates a multi-level index from separate arrays. The `names` parameter labels each level.\n",
    "- Accessing MultiIndex data: `panel_data.loc['NABIL']` selects all rows for NABIL.\n",
    "- The shape differs: Series is `(n,)`, DataFrame is `(rows, cols)`, MultiIndex DataFrame is still 2D but with hierarchical row labels.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Efficient data structures for large time-series\n",
    "\n",
    "# Using appropriate dtypes to save memory\n",
    "import pandas as pd\n",
    "\n",
    "# Create sample data with default types\n",
    "large_data = pd.DataFrame({\n",
    "    'Symbol': ['NABIL'] * 100000,\n",
    "    'Open': [2850.50 + i * 0.01 for i in range(100000)],\n",
    "    'Volume': [100000 + i * 10 for i in range(100000)]\n",
    "})\n",
    "\n",
    "print(\"Default dtypes and memory usage:\")\n",
    "print(large_data.dtypes)\n",
    "print(f\"Memory usage: {large_data.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Optimize dtypes\n",
    "optimized_data = large_data.copy()\n",
    "optimized_data['Symbol'] = optimized_data['Symbol'].astype('category')  # Categorical for repeated strings\n",
    "optimized_data['Open'] = optimized_data['Open'].astype('float32')  # Smaller float\n",
    "optimized_data['Volume'] = optimized_data['Volume'].astype('int32')  # Smaller int\n",
    "\n",
    "print(\"\\nOptimized dtypes and memory usage:\")\n",
    "print(optimized_data.dtypes)\n",
    "print(f\"Memory usage: {optimized_data.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Memory savings\n",
    "savings = (1 - optimized_data.memory_usage(deep=True).sum() / large_data.memory_usage(deep=True).sum()) * 100\n",
    "print(f\"\\nMemory saved: {savings:.1f}%\")\n",
    "\n",
    "# Output:\n",
    "# Default dtypes and memory usage:\n",
    "# Symbol     object\n",
    "# Open      float64\n",
    "# Volume      int64\n",
    "# dtype: object\n",
    "# Memory usage: 7.63 MB\n",
    "#\n",
    "# Optimized dtypes and memory usage:\n",
    "# Symbol    category\n",
    "# Open       float32\n",
    "# Volume       int32\n",
    "# dtype: object\n",
    "# Memory usage: 1.15 MB\n",
    "#\n",
    "# Memory saved: 84.9%\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Memory optimization** is crucial for large time-series datasets. Default pandas types (object for strings, float64 for decimals, int64 for integers) can be excessive.\n",
    "- `'category'` dtype is for columns with few unique values (like Symbol). It stores values as integers with a lookup table, dramatically reducing memory for repeated strings.\n",
    "- `float32` is 32-bit instead of 64-bit, halving memory for floats. Precision is about 7 decimal digits (sufficient for most prices).\n",
    "- `int32` is 32-bit instead of 64-bit, halving memory for integers. Range is about ±2 billion (sufficient for most volumes).\n",
    "- `.memory_usage(deep=True)` calculates actual memory including object types. Without `deep=True`, it underestimates memory for string columns.\n",
    "- In this example, we reduced memory usage by 85%, which is significant when working with millions of rows.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.3 Loading Time-Series Data**\n",
    "\n",
    "Real-world time-series prediction systems need to load data from various sources. Let's explore different data loading methods.\n",
    "\n",
    "### **4.3.1 From CSV Files**\n",
    "\n",
    "CSV (Comma-Separated Values) is the most common format for time-series data. The NEPSE data in this handbook comes in CSV format.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Basic CSV loading\n",
    "# Assuming we have a NEPSE data file named 'nepse_data.csv'\n",
    "\n",
    "# First, let's create a sample CSV file for demonstration\n",
    "sample_data = \"\"\"S.No,Symbol,Conf.,Open,High,Low,Close,LTP,Close - LTP,Close - LTP %,VWAP,Vol,Prev. Close,Turnover,Trans.,Diff,Range,Diff %,Range %,VWAP %,52 Weeks High,52 Weeks Low\n",
    "1,NABIL,A,2850.50,2890.00,2840.00,2875.25,2875.25,0.00,0.00,2872.50,125000,2845.00,359062500,500,30.25,50.00,1.06,1.76,0.97,3200.00,2400.00\n",
    "2,NABIL,A,2875.25,2910.00,2860.00,2895.50,2895.50,0.00,0.00,2888.00,150000,2875.25,433200000,620,20.25,50.00,0.70,1.74,0.45,3200.00,2400.00\n",
    "3,NABIL,A,2890.00,2920.00,2880.00,2900.00,2900.00,0.00,0.00,2898.50,175000,2895.50,507237500,750,4.50,40.00,0.16,1.38,0.15,3200.00,2400.00\n",
    "4,NICA,A,1180.00,1195.00,1175.00,1190.50,1190.50,0.00,0.00,1186.00,85000,1178.00,100910000,320,12.50,20.00,1.06,1.70,0.38,1350.00,980.00\n",
    "5,NICA,A,1190.50,1210.00,1185.00,1205.00,1205.00,0.00,0.00,1198.50,92000,1190.50,110262000,380,14.50,25.00,1.22,2.10,0.77,1350.00,980.00\n",
    "6,SCBL,A,2450.00,2480.00,2440.00,2465.00,2465.00,0.00,0.00,2458.00,45000,2445.00,110610000,180,20.00,40.00,0.82,1.64,0.53,2700.00,2100.00\"\"\"\n",
    "\n",
    "# Write to file\n",
    "with open('nepse_data.csv', 'w') as f:\n",
    "    f.write(sample_data)\n",
    "\n",
    "# Now load the CSV\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "\n",
    "print(\"Loaded DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nShape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nData types:\\n{df.dtypes}\")\n",
    "\n",
    "# Output:\n",
    "# Loaded DataFrame:\n",
    "#    S.No Symbol Conf.     Open     High      Low    Close     LTP  ...  52 Weeks Low\n",
    "# 0     1  NABIL     A  2850.50  2890.00  2840.00  2875.25  2875.25  ...        2400.0\n",
    "# 1     2  NABIL     A  2875.25  2910.00  2860.00  2895.50  2895.50  ...        2400.0\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `pd.read_csv()` is the primary function for loading CSV files. It returns a DataFrame.\n",
    "- The function automatically uses the first row as column headers.\n",
    "- Column names are taken directly from the CSV header row.\n",
    "- `df.shape` shows the dimensions: (number of rows, number of columns).\n",
    "- `df.dtypes` shows the inferred data types for each column. Pandas automatically infers types:\n",
    "  - Numeric columns become `float64` or `int64`\n",
    "  - String columns become `object`\n",
    "- The ellipsis `...` in output indicates columns are truncated for display when there are many columns.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Advanced CSV loading options\n",
    "\n",
    "# Specify which columns to load (saves memory for large files)\n",
    "df_subset = pd.read_csv('nepse_data.csv', usecols=['Symbol', 'Date', 'Open', 'High', 'Low', 'Close', 'Volume'])\n",
    "# Note: 'Date' column not in our sample, but shown for illustration\n",
    "\n",
    "# Actually, let's use columns that exist:\n",
    "df_subset = pd.read_csv('nepse_data.csv', usecols=['Symbol', 'Open', 'High', 'Low', 'Close', 'Vol'])\n",
    "print(\"Subset of columns:\")\n",
    "print(df_subset.head())\n",
    "\n",
    "# Specify data types for columns (avoids type inference overhead)\n",
    "df_types = pd.read_csv('nepse_data.csv', dtype={\n",
    "    'Symbol': 'category',\n",
    "    'Vol': 'int32'\n",
    "})\n",
    "print(f\"\\nWith specified dtypes:\\n{df_types.dtypes}\")\n",
    "\n",
    "# Handle missing values during loading\n",
    "# na_values specifies additional strings to treat as NaN\n",
    "df_na = pd.read_csv('nepse_data.csv', na_values=['N/A', 'NA', '-', '', 'null'])\n",
    "\n",
    "# Parse dates during loading\n",
    "# If we had a 'Date' column, we could parse it:\n",
    "# df_dates = pd.read_csv('nepse_data.csv', parse_dates=['Date'])\n",
    "\n",
    "# Set index during loading\n",
    "# df_indexed = pd.read_csv('nepse_data.csv', index_col='S.No')\n",
    "\n",
    "# Skip rows (useful if file has header information)\n",
    "# df_skip = pd.read_csv('nepse_data.csv', skiprows=2)  # Skip first 2 rows\n",
    "\n",
    "# Read only first N rows (useful for exploring large files)\n",
    "df_preview = pd.read_csv('nepse_data.csv', nrows=3)\n",
    "print(f\"\\nFirst 3 rows only:\")\n",
    "print(df_preview)\n",
    "\n",
    "# Output:\n",
    "# Subset of columns:\n",
    "#   Symbol     Open     High      Low    Close     Vol\n",
    "# 0  NABIL  2850.50  2890.00  2840.00  2875.25  125000\n",
    "# 1  NABIL  2875.25  2910.00  2860.00  2895.50  150000\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `usecols` parameter loads only specified columns. This is crucial for memory efficiency when you have many columns but only need a few.\n",
    "- `dtype` parameter forces specific data types during loading. This prevents pandas from guessing types and saves memory with appropriate types.\n",
    "- `na_values` parameter specifies additional strings to treat as missing values (NaN). By default, pandas treats 'NA', 'N/A', 'NULL', etc., but custom formats may need explicit specification.\n",
    "- `parse_dates` automatically converts specified columns to datetime. This is more efficient than parsing after loading.\n",
    "- `index_col` sets a column as the index during loading. For time-series, this is typically the date column.\n",
    "- `skiprows` skips initial rows, useful when CSV files have metadata headers before the actual data.\n",
    "- `nrows` limits rows loaded, useful for quickly previewing large files without loading everything.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Loading CSV with proper datetime handling for NEPSE data\n",
    "\n",
    "# Create a more realistic NEPSE CSV with dates\n",
    "nepse_with_dates = \"\"\"Date,Symbol,Open,High,Low,Close,Volume,Turnover\n",
    "2024-01-15,NABIL,2850.50,2890.00,2840.00,2875.25,125000,359062500\n",
    "2024-01-16,NABIL,2875.25,2910.00,2860.00,2895.50,150000,433200000\n",
    "2024-01-17,NABIL,2890.00,2920.00,2880.00,2900.00,175000,507237500\n",
    "2024-01-18,NABIL,2865.75,2900.00,2850.00,2880.50,140000,403630000\n",
    "2024-01-19,NABIL,2880.50,2915.00,2870.00,2905.00,160000,465280000\n",
    "2024-01-15,NICA,1180.00,1195.00,1175.00,1190.50,85000,100910000\n",
    "2024-01-16,NICA,1190.50,1210.00,1185.00,1205.00,92000,110262000\n",
    "2024-01-17,NICA,1205.00,1220.00,1200.00,1215.00,88000,106920000\"\"\"\n",
    "\n",
    "with open('nepse_with_dates.csv', 'w') as f:\n",
    "    f.write(nepse_with_dates)\n",
    "\n",
    "# Load with date parsing and index setting\n",
    "df_ts = pd.read_csv(\n",
    "    'nepse_with_dates.csv',\n",
    "    parse_dates=['Date'],  # Convert Date column to datetime\n",
    "    index_col='Date',       # Set Date as index\n",
    "    dtype={'Symbol': 'category', 'Volume': 'int32'}\n",
    ")\n",
    "\n",
    "print(\"Time-series DataFrame:\")\n",
    "print(df_ts)\n",
    "print(f\"\\nIndex type: {type(df_ts.index)}\")\n",
    "print(f\"Index dtype: {df_ts.index.dtype}\")\n",
    "\n",
    "# Now we can do time-based operations\n",
    "print(f\"\\nData for Jan 16-18:\\n{df_ts['2024-01-16':'2024-01-18']}\")\n",
    "\n",
    "# Select specific symbol\n",
    "nabil_data = df_ts[df_ts['Symbol'] == 'NABIL']\n",
    "print(f\"\\nNABIL data:\\n{nabil_data}\")\n",
    "\n",
    "# Output:\n",
    "# Time-series DataFrame:\n",
    "#           Symbol     Open     High      Low    Close  Volume    Turnover\n",
    "# Date\n",
    "# 2024-01-15   NABIL  2850.50  2890.00  2840.00  2875.25  125000  359062500\n",
    "# 2024-01-16   NABIL  2875.25  2910.00  2860.00  2895.50  150000  433200000\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `parse_dates=['Date']` converts the Date column to pandas datetime type during loading. This is essential for time-series operations.\n",
    "- `index_col='Date'` sets the Date column as the index. With a DatetimeIndex, you can use date-based slicing and resampling.\n",
    "- Combining `parse_dates` and `index_col` during loading is more efficient than parsing after loading.\n",
    "- With DatetimeIndex, you can use string dates for slicing: `df_ts['2024-01-16':'2024-01-18']` gets all data in that range.\n",
    "- `dtype={'Symbol': 'category'}` optimizes the Symbol column since it has repeated values.\n",
    "- After loading with proper date handling, the DataFrame is ready for time-series analysis.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.3.2 From Databases**\n",
    "\n",
    "Many production systems store time-series data in databases. Let's explore loading data from SQL databases.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import sqlite3\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Create a sample SQLite database for demonstration\n",
    "# In production, you would connect to an existing database\n",
    "\n",
    "# Create in-memory SQLite database\n",
    "conn = sqlite3.connect(':memory:')  # Use ':memory:' for temporary in-memory database\n",
    "\n",
    "# Create table and insert sample data\n",
    "create_table_sql = \"\"\"\n",
    "CREATE TABLE stock_prices (\n",
    "    id INTEGER PRIMARY KEY AUTOINCREMENT,\n",
    "    symbol TEXT NOT NULL,\n",
    "    trade_date DATE NOT NULL,\n",
    "    open REAL,\n",
    "    high REAL,\n",
    "    low REAL,\n",
    "    close REAL,\n",
    "    volume INTEGER,\n",
    "    turnover REAL,\n",
    "    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "conn.execute(create_table_sql)\n",
    "\n",
    "# Insert sample NEPSE data\n",
    "insert_sql = \"\"\"\n",
    "INSERT INTO stock_prices (symbol, trade_date, open, high, low, close, volume, turnover)\n",
    "VALUES \n",
    "    ('NABIL', '2024-01-15', 2850.50, 2890.00, 2840.00, 2875.25, 125000, 359062500),\n",
    "    ('NABIL', '2024-01-16', 2875.25, 2910.00, 2860.00, 2895.50, 150000, 433200000),\n",
    "    ('NABIL', '2024-01-17', 2890.00, 2920.00, 2880.00, 2900.00, 175000, 507237500),\n",
    "    ('NABIL', '2024-01-18', 2865.75, 2900.00, 2850.00, 2880.50, 140000, 403630000),\n",
    "    ('NABIL', '2024-01-19', 2880.50, 2915.00, 2870.00, 2905.00, 160000, 465280000),\n",
    "    ('NICA', '2024-01-15', 1180.00, 1195.00, 1175.00, 1190.50, 85000, 100910000),\n",
    "    ('NICA', '2024-01-16', 1190.50, 1210.00, 1185.00, 1205.00, 92000, 110262000),\n",
    "    ('NICA', '2024-01-17', 1205.00, 1220.00, 1200.00, 1215.00, 88000, 106920000);\n",
    "\"\"\"\n",
    "\n",
    "conn.execute(insert_sql)\n",
    "conn.commit()  # Commit the transaction\n",
    "\n",
    "# Verify data was inserted\n",
    "result = conn.execute(\"SELECT COUNT(*) FROM stock_prices\")\n",
    "print(f\"Total records in database: {result.fetchone()[0]}\")\n",
    "\n",
    "# Output:\n",
    "# Total records in database: 8\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `sqlite3.connect()` creates a connection to a SQLite database. `:memory:` creates a temporary in-memory database that doesn't persist to disk.\n",
    "- `conn.execute()` runs SQL commands. `CREATE TABLE` defines the schema with appropriate data types.\n",
    "- `INSERT INTO` adds data to the table. Multiple rows can be inserted in a single statement.\n",
    "- `conn.commit()` commits the transaction, making changes permanent. Without it, changes would be rolled back when the connection closes.\n",
    "- In production, you would connect to an existing database file: `sqlite3.connect('nepse.db')` or use other database systems like PostgreSQL or MySQL.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Loading data from database using pandas\n",
    "\n",
    "# Method 1: Using read_sql with raw SQL query\n",
    "query = \"\"\"\n",
    "SELECT symbol, trade_date, open, high, low, close, volume \n",
    "FROM stock_prices \n",
    "WHERE symbol = 'NABIL'\n",
    "ORDER BY trade_date\n",
    "\"\"\"\n",
    "\n",
    "df_sql = pd.read_sql(query, conn, parse_dates=['trade_date'])\n",
    "\n",
    "print(\"Data loaded with SQL query:\")\n",
    "print(df_sql)\n",
    "print(f\"\\nData types:\\n{df_sql.dtypes}\")\n",
    "\n",
    "# Method 2: Using read_sql_query (same as read_sql for queries)\n",
    "df_query = pd.read_sql_query(\n",
    "    \"SELECT * FROM stock_prices WHERE volume > 100000\",\n",
    "    conn,\n",
    "    parse_dates=['trade_date'],\n",
    "    index_col='trade_date'\n",
    ")\n",
    "print(f\"\\nHigh volume trades:\\n{df_query}\")\n",
    "\n",
    "# Method 3: Using read_sql_table (loads entire table)\n",
    "# Requires SQLAlchemy connection\n",
    "engine = create_engine('sqlite:///:memory:')  # Create new engine for demo\n",
    "\n",
    "# Copy data to the new engine's database\n",
    "df_sample = pd.read_sql(\"SELECT * FROM stock_prices\", conn)\n",
    "df_sample.to_sql('stock_prices', engine, index=False, if_exists='replace')\n",
    "\n",
    "df_table = pd.read_sql_table('stock_prices', engine, parse_dates=['trade_date'])\n",
    "print(f\"\\nEntire table loaded:\\n{df_table.head()}\")\n",
    "\n",
    "# Output:\n",
    "# Data loaded with SQL query:\n",
    "#   symbol trade_date     open     high      low    close  volume\n",
    "# 0  NABIL 2024-01-15  2850.50  2890.00  2840.00  2875.25  125000\n",
    "# 1  NABIL 2024-01-16  2875.25  2910.00  2860.00  2895.50  150000\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `pd.read_sql()` executes a SQL query and returns the result as a DataFrame. It's the most flexible method.\n",
    "- The `parse_dates` parameter converts date columns to datetime type, just like with CSV loading.\n",
    "- `pd.read_sql_query()` is similar to `read_sql` but explicitly for queries (not table names).\n",
    "- `pd.read_sql_table()` loads an entire table by name. Requires SQLAlchemy engine.\n",
    "- `index_col` sets a column as the DataFrame index, useful for time-series with date columns.\n",
    "- These methods support any SQL database (PostgreSQL, MySQL, Oracle, etc.) with appropriate connection strings.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Parameterized queries for safe database access\n",
    "\n",
    "# Using parameters prevents SQL injection attacks\n",
    "# Never use string formatting for user input in SQL!\n",
    "\n",
    "# BAD: Vulnerable to SQL injection\n",
    "# symbol = \"NABIL'; DROP TABLE stock_prices; --\"\n",
    "# bad_query = f\"SELECT * FROM stock_prices WHERE symbol = '{symbol}'\"\n",
    "\n",
    "# GOOD: Use parameterized queries\n",
    "symbol_param = 'NABIL'\n",
    "start_date = '2024-01-15'\n",
    "end_date = '2024-01-17'\n",
    "\n",
    "# Using ? placeholders (SQLite style)\n",
    "params_query = \"\"\"\n",
    "SELECT * FROM stock_prices \n",
    "WHERE symbol = ? AND trade_date BETWEEN ? AND ?\n",
    "ORDER BY trade_date\n",
    "\"\"\"\n",
    "\n",
    "df_params = pd.read_sql(\n",
    "    params_query, \n",
    "    conn, \n",
    "    params=(symbol_param, start_date, end_date),\n",
    "    parse_dates=['trade_date']\n",
    ")\n",
    "\n",
    "print(\"Parameterized query result:\")\n",
    "print(df_params)\n",
    "\n",
    "# Using named parameters with SQLAlchemy\n",
    "from sqlalchemy import text\n",
    "\n",
    "engine = create_engine('sqlite:///:memory:')\n",
    "pd.read_sql(\"SELECT * FROM stock_prices\", conn).to_sql('stock_prices', engine, index=False, if_exists='replace')\n",
    "\n",
    "named_query = text(\"\"\"\n",
    "SELECT symbol, trade_date, close, volume \n",
    "FROM stock_prices \n",
    "WHERE symbol = :symbol AND volume > :min_volume\n",
    "\"\"\")\n",
    "\n",
    "with engine.connect() as connection:\n",
    "    df_named = pd.read_sql(\n",
    "        named_query, \n",
    "        connection, \n",
    "        params={'symbol': 'NABIL', 'min_volume': 100000}\n",
    "    )\n",
    "\n",
    "print(f\"\\nNamed parameter query:\\n{df_named}\")\n",
    "\n",
    "# Output:\n",
    "# Parameterized query result:\n",
    "#    id symbol trade_date     open     high      low    close  volume    turnover\n",
    "# 0   1  NABIL 2024-01-15  2850.50  2890.00  2840.00  2875.25  125000  359062500\n",
    "# 1   2  NABIL 2024-01-16  2875.25  2910.00  2860.00  2895.50  150000  433200000\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Parameterized queries** are essential for security. Never use string formatting (f-strings or %) to insert user input into SQL queries - this creates SQL injection vulnerabilities.\n",
    "- Use `?` placeholders (SQLite style) or `:name` placeholders (named parameters) with SQLAlchemy.\n",
    "- The `params` parameter in `read_sql()` accepts a tuple for positional parameters or a dictionary for named parameters.\n",
    "- SQLAlchemy's `text()` wraps SQL with named parameter support.\n",
    "- When building prediction systems with user inputs (like symbol selection), always use parameterized queries.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.3.3 From APIs**\n",
    "\n",
    "Many financial data providers offer APIs for accessing real-time and historical data. Let's explore API integration.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "# Understanding API basics\n",
    "# APIs (Application Programming Interfaces) allow programs to communicate\n",
    "# REST APIs use HTTP methods (GET, POST, etc.) to request data\n",
    "\n",
    "# Sample API call structure (mock NEPSE API)\n",
    "# In production, you would use actual NEPSE or other financial APIs\n",
    "\n",
    "# Simulating an API response\n",
    "mock_api_response = {\n",
    "    \"status\": \"success\",\n",
    "    \"data\": [\n",
    "        {\n",
    "            \"symbol\": \"NABIL\",\n",
    "            \"date\": \"2024-01-15\",\n",
    "            \"open\": 2850.50,\n",
    "            \"high\": 2890.00,\n",
    "            \"low\": 2840.00,\n",
    "            \"close\": 2875.25,\n",
    "            \"volume\": 125000,\n",
    "            \"turnover\": 359062500\n",
    "        },\n",
    "        {\n",
    "            \"symbol\": \"NABIL\",\n",
    "            \"date\": \"2024-01-16\",\n",
    "            \"open\": 2875.25,\n",
    "            \"high\": 2910.00,\n",
    "            \"low\": 2860.00,\n",
    "            \"close\": 2895.50,\n",
    "            \"volume\": 150000,\n",
    "            \"turnover\": 433200000\n",
    "        },\n",
    "        {\n",
    "            \"symbol\": \"NABIL\",\n",
    "            \"date\": \"2024-01-17\",\n",
    "            \"open\": 2890.00,\n",
    "            \"high\": 2920.00,\n",
    "            \"low\": 2880.00,\n",
    "            \"close\": 2900.00,\n",
    "            \"volume\": 175000,\n",
    "            \"turnover\": 507237500\n",
    "        }\n",
    "    ],\n",
    "    \"metadata\": {\n",
    "        \"total_records\": 3,\n",
    "        \"page\": 1,\n",
    "        \"per_page\": 100\n",
    "    }\n",
    "}\n",
    "\n",
    "# Parse JSON response into DataFrame\n",
    "# APIs typically return JSON (JavaScript Object Notation)\n",
    "print(\"API Response structure:\")\n",
    "print(json.dumps(mock_api_response, indent=2))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df_api = pd.DataFrame(mock_api_response['data'])\n",
    "print(f\"\\nDataFrame from API:\\n{df_api}\")\n",
    "\n",
    "# Convert date column\n",
    "df_api['date'] = pd.to_datetime(df_api['date'])\n",
    "print(f\"\\nWith parsed dates:\\n{df_api.dtypes}\")\n",
    "\n",
    "# Output:\n",
    "# API Response structure:\n",
    "# {\n",
    "#   \"status\": \"success\",\n",
    "#   \"data\": [\n",
    "#     {\n",
    "#       \"symbol\": \"NABIL\",\n",
    "#       \"date\": \"2024-01-15\",\n",
    "#       \"open\": 2850.5,\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- APIs return data in JSON format (JavaScript Object Notation), which is similar to Python dictionaries.\n",
    "- `json.dumps()` converts Python objects to JSON string. `indent=2` formats it for readability.\n",
    "- `pd.DataFrame()` can directly convert a list of dictionaries to a DataFrame - each dictionary becomes a row.\n",
    "- API responses typically have a structure with:\n",
    "  - `status`: Indicates success/failure\n",
    "  - `data`: The actual data (array of records)\n",
    "  - `metadata`: Information about the response (pagination, counts)\n",
    "- Date strings from APIs need to be parsed with `pd.to_datetime()` for time-series operations.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Making actual HTTP requests to APIs\n",
    "\n",
    "import requests\n",
    "import pandas as pd\n",
    "\n",
    "# Example function to fetch stock data from an API\n",
    "def fetch_stock_data(symbol, start_date, end_date, api_key='demo'):\n",
    "    \"\"\"\n",
    "    Fetch stock data from a financial API.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    symbol : str\n",
    "        Stock symbol (e.g., 'NABIL')\n",
    "    start_date : str\n",
    "        Start date in 'YYYY-MM-DD' format\n",
    "    end_date : str\n",
    "        End date in 'YYYY-MM-DD' format\n",
    "    api_key : str\n",
    "        API key for authentication\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pandas.DataFrame\n",
    "        DataFrame with stock data\n",
    "    \"\"\"\n",
    "    \n",
    "    # Build API URL (example structure - actual URL depends on provider)\n",
    "    # Common financial APIs: Alpha Vantage, Yahoo Finance, IEX Cloud\n",
    "    base_url = \"https://api.example.com/stock/data\"  # Placeholder\n",
    "    \n",
    "    # Request parameters\n",
    "    params = {\n",
    "        'symbol': symbol,\n",
    "        'start': start_date,\n",
    "        'end': end_date,\n",
    "        'apikey': api_key\n",
    "    }\n",
    "    \n",
    "    # Make GET request\n",
    "    # response = requests.get(base_url, params=params)\n",
    "    \n",
    "    # Check if request was successful\n",
    "    # if response.status_code == 200:\n",
    "    #     data = response.json()\n",
    "    #     df = pd.DataFrame(data['data'])\n",
    "    #     df['date'] = pd.to_datetime(df['date'])\n",
    "    #     return df\n",
    "    # else:\n",
    "    #     raise Exception(f\"API request failed: {response.status_code}\")\n",
    "    \n",
    "    # For demonstration, return mock data\n",
    "    mock_data = [\n",
    "        {'date': '2024-01-15', 'open': 2850.50, 'high': 2890.00, \n",
    "         'low': 2840.00, 'close': 2875.25, 'volume': 125000},\n",
    "        {'date': '2024-01-16', 'open': 2875.25, 'high': 2910.00, \n",
    "         'low': 2860.00, 'close': 2895.50, 'volume': 150000},\n",
    "    ]\n",
    "    df = pd.DataFrame(mock_data)\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    return df\n",
    "\n",
    "# Use the function\n",
    "try:\n",
    "    df_fetched = fetch_stock_data('NABIL', '2024-01-15', '2024-01-16')\n",
    "    print(\"Fetched data:\")\n",
    "    print(df_fetched)\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Output:\n",
    "# Fetched data:\n",
    "#         date     open     high      low    close  volume\n",
    "# 0 2024-01-15  2850.50  2890.00  2840.00  2875.25  125000\n",
    "# 1 2024-01-16  2875.25  2910.00  2860.00  2895.50  150000\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `requests.get()` makes an HTTP GET request to the specified URL with parameters.\n",
    "- `params` dictionary is automatically converted to URL query parameters (?symbol=NABIL&start=...).\n",
    "- `response.status_code` indicates success (200) or failure (4xx for client errors, 5xx for server errors).\n",
    "- `response.json()` parses the JSON response body into Python dictionaries/lists.\n",
    "- **API keys** are typically required for authentication. Never hardcode API keys in source code - use environment variables or configuration files.\n",
    "- Always wrap API calls in try-except blocks to handle network errors, rate limits, and other failures gracefully.\n",
    "- The function demonstrates the standard pattern for API data fetching: build request, make request, check response, parse data.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Handling API pagination and rate limits\n",
    "\n",
    "import time\n",
    "import requests\n",
    "\n",
    "def fetch_all_stock_data(symbol, start_date, end_date, api_key='demo'):\n",
    "    \"\"\"\n",
    "    Fetch all stock data with pagination handling.\n",
    "    \n",
    "    Many APIs limit records per request and require pagination.\n",
    "    \"\"\"\n",
    "    \n",
    "    all_data = []\n",
    "    page = 1\n",
    "    per_page = 100\n",
    "    \n",
    "    while True:\n",
    "        params = {\n",
    "            'symbol': symbol,\n",
    "            'start': start_date,\n",
    "            'end': end_date,\n",
    "            'page': page,\n",
    "            'per_page': per_page,\n",
    "            'apikey': api_key\n",
    "        }\n",
    "        \n",
    "        # response = requests.get('https://api.example.com/stock/data', params=params)\n",
    "        # data = response.json()\n",
    "        \n",
    "        # Simulate API response\n",
    "        mock_data = {\n",
    "            'data': [\n",
    "                {'date': f'2024-01-{15+i}', 'close': 2850 + i*10, 'volume': 100000 + i*1000}\n",
    "                for i in range(min(per_page, 10))  # Simulate 10 records\n",
    "            ] if page == 1 else [],\n",
    "            'metadata': {'total_pages': 1}\n",
    "        }\n",
    "        data = mock_data\n",
    "        \n",
    "        # Add data to collection\n",
    "        if data['data']:\n",
    "            all_data.extend(data['data'])\n",
    "        \n",
    "        # Check if more pages exist\n",
    "        if page >= data['metadata'].get('total_pages', 1):\n",
    "            break\n",
    "        \n",
    "        page += 1\n",
    "        \n",
    "        # Rate limiting - wait between requests\n",
    "        time.sleep(0.1)  # Wait 100ms between requests\n",
    "        \n",
    "        # Some APIs provide rate limit info in headers\n",
    "        # remaining = response.headers.get('X-RateLimit-Remaining')\n",
    "        # if remaining and int(remaining) < 10:\n",
    "        #     time.sleep(60)  # Wait if running low on rate limit\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    if 'date' in df.columns:\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Fetch data with pagination\n",
    "df_paginated = fetch_all_stock_data('NABIL', '2024-01-01', '2024-01-31')\n",
    "print(f\"Total records fetched: {len(df_paginated)}\")\n",
    "print(df_paginated.head())\n",
    "\n",
    "# Output:\n",
    "# Total records fetched: 10\n",
    "#         date   close  volume\n",
    "# 0 2024-01-15  2850.0  100000\n",
    "# 1 2024-01-16  2860.0  101000\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Pagination** is necessary when APIs limit records per request. You make multiple requests, incrementing the page number.\n",
    "- Loop until no more data is returned or you've fetched all pages.\n",
    "- `all_data.extend()` adds records from each page to a single list.\n",
    "- **Rate limiting** prevents overwhelming the API server. APIs often have limits like 100 requests per minute.\n",
    "- `time.sleep()` pauses between requests. Adjust based on API's rate limits.\n",
    "- Some APIs include rate limit information in response headers (`X-RateLimit-Remaining`, `X-RateLimit-Reset`).\n",
    "- Best practice: Check rate limits and pause before hitting them, rather than waiting for errors.\n",
    "- This pattern is essential for fetching large historical datasets.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.3.4 From Other Sources**\n",
    "\n",
    "Time-series data can come from various other sources including Excel files, Parquet files, and real-time streams.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Loading from Excel files\n",
    "# Excel is common for manually maintained data\n",
    "\n",
    "# Create sample Excel file\n",
    "df_sample = pd.DataFrame({\n",
    "    'Date': pd.date_range('2024-01-15', periods=5, freq='D'),\n",
    "    'Symbol': ['NABIL'] * 5,\n",
    "    'Open': [2850.50, 2875.25, 2890.00, 2865.75, 2880.50],\n",
    "    'High': [2890.00, 2910.00, 2920.00, 2900.00, 2915.00],\n",
    "    'Low': [2840.00, 2860.00, 2880.00, 2850.00, 2870.00],\n",
    "    'Close': [2875.25, 2895.50, 2900.00, 2880.50, 2905.00]\n",
    "})\n",
    "\n",
    "# Save to Excel\n",
    "df_sample.to_excel('nepse_data.xlsx', sheet_name='Stock Prices', index=False)\n",
    "\n",
    "# Load from Excel\n",
    "df_excel = pd.read_excel('nepse_data.xlsx', sheet_name='Stock Prices')\n",
    "print(\"Data from Excel:\")\n",
    "print(df_excel)\n",
    "print(f\"\\nData types:\\n{df_excel.dtypes}\")\n",
    "\n",
    "# Loading specific sheet and range\n",
    "df_range = pd.read_excel('nepse_data.xlsx', sheet_name='Stock Prices', usecols='A:F', skiprows=0)\n",
    "print(f\"\\nSpecific range:\\n{df_range.head()}\")\n",
    "\n",
    "# Output:\n",
    "# Data from Excel:\n",
    "#        Date Symbol     Open     High      Low    Close\n",
    "# 0 2024-01-15  NABIL  2850.50  2890.00  2840.00  2875.25\n",
    "# 1 2024-01-16  NABIL  2875.25  2910.00  2860.00  2895.50\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `pd.read_excel()` reads Excel files (.xlsx, .xls). It requires the `openpyxl` or `xlrd` library.\n",
    "- `sheet_name` specifies which sheet to read. Can be sheet name (string), index (integer), or None for all sheets.\n",
    "- `usecols` specifies columns to read. Can be column letters ('A:F'), names, or indices.\n",
    "- `to_excel()` writes DataFrame to Excel. `index=False` excludes the index from the output.\n",
    "- Excel files are useful for smaller datasets or when data is manually maintained, but they're slower and larger than CSV or Parquet for large datasets.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Loading from Parquet files - efficient for large datasets\n",
    "\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "# Parquet is a columnar storage format optimized for analytics\n",
    "# It's much faster and smaller than CSV for large files\n",
    "\n",
    "# Save to Parquet\n",
    "df_sample.to_parquet('nepse_data.parquet', engine='pyarrow')\n",
    "\n",
    "# Load from Parquet\n",
    "df_parquet = pd.read_parquet('nepse_data.parquet')\n",
    "print(\"Data from Parquet:\")\n",
    "print(df_parquet)\n",
    "print(f\"\\nData types preserved:\\n{df_parquet.dtypes}\")\n",
    "\n",
    "# Compare file sizes\n",
    "import os\n",
    "csv_size = os.path.getsize('nepse_data.csv') if os.path.exists('nepse_data.csv') else 0\n",
    "parquet_size = os.path.getsize('nepse_data.parquet')\n",
    "\n",
    "print(f\"\\nFile comparison:\")\n",
    "print(f\"CSV size: {csv_size} bytes\")\n",
    "print(f\"Parquet size: {parquet_size} bytes\")\n",
    "\n",
    "# Parquet benefits for time-series:\n",
    "# 1. Preserves data types (no need to re-parse dates)\n",
    "# 2. Columnar storage - faster queries on specific columns\n",
    "# 3. Compression - smaller file sizes\n",
    "# 4. Predicate pushdown - can skip reading irrelevant data\n",
    "\n",
    "# Output:\n",
    "# Data from Parquet:\n",
    "#        Date Symbol     Open     High      Low    Close\n",
    "# 0 2024-01-15  NABIL  2850.50  2890.00  2840.00  2875.25\n",
    "# 1 2024-01-16  NABIL  2875.25  2910.00  2860.00  2895.50\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Parquet** is a columnar storage format designed for big data. It's ideal for time-series data in production.\n",
    "- `to_parquet()` writes DataFrame to Parquet. `engine='pyarrow'` specifies the library to use (fastest option).\n",
    "- `read_parquet()` loads Parquet files. Data types are preserved automatically - no need for `parse_dates`.\n",
    "- **Advantages over CSV**:\n",
    "  - Data types preserved (datetime stays datetime, not string)\n",
    "  - Columnar format: reading only needed columns is much faster\n",
    "  - Built-in compression: files are typically 10x smaller\n",
    "  - Predicate pushdown: filtering happens during file read, not after\n",
    "- Use Parquet for: production systems, large datasets, archival storage.\n",
    "- Use CSV for: data exchange, small datasets, human readability.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.4 Data Types and Conversions**\n",
    "\n",
    "Understanding and managing data types is crucial for time-series analysis. Incorrect types can cause errors or produce incorrect results.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample NEPSE data with various types\n",
    "data = {\n",
    "    'Symbol': ['NABIL', 'NABIL', 'NICA', 'NICA'],\n",
    "    'Date': ['2024-01-15', '2024-01-16', '2024-01-15', '2024-01-16'],\n",
    "    'Open': [2850.50, 2875.25, 1180.00, 1190.50],\n",
    "    'High': [2890.00, 2910.00, 1195.00, 1210.00],\n",
    "    'Low': [2840.00, 2860.00, 1175.00, 1185.00],\n",
    "    'Close': [2875.25, 2895.50, 1190.50, 1205.00],\n",
    "    'Volume': ['125,000', '150,000', '85,000', '92,000'],  # String with commas\n",
    "    'Change_Pct': ['1.06%', '0.70%', '1.06%', '1.22%'],  # String with %\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "print(f\"\\nOriginal data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Output:\n",
    "# Original DataFrame:\n",
    "#   Symbol        Date     Open     High      Low    Close   Volume Change_Pct\n",
    "# 0  NABIL  2024-01-15  2850.50  2890.00  2840.00  2875.25  125,000      1.06%\n",
    "# 1  NABIL  2024-01-16  2875.25  2910.00  2860.00  2895.50  150,000      0.70%\n",
    "# ...\n",
    "#\n",
    "# Original data types:\n",
    "# Symbol        object\n",
    "# Date          object\n",
    "# Open         float64\n",
    "# High         float64\n",
    "# Low          float64\n",
    "# Close        float64\n",
    "# Volume        object\n",
    "# Change_Pct    object\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- When data is loaded, pandas infers types. Strings become `object` type.\n",
    "- In this example, `Volume` and `Change_Pct` are strings because they contain commas and percentage signs.\n",
    "- `object` type prevents numerical operations - you can't calculate the mean of strings.\n",
    "- Before analysis, we need to convert these to appropriate numeric types.\n",
    "- The goal is to have:\n",
    "  - `Symbol`: categorical (limited unique values)\n",
    "  - `Date`: datetime\n",
    "  - `Open`, `High`, `Low`, `Close`: float\n",
    "  - `Volume`: int\n",
    "  - `Change_Pct`: float (as decimal, e.g., 0.0106)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Converting string data to proper types\n",
    "\n",
    "# Convert Date to datetime\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "print(f\"After date conversion:\\n{df['Date'].dtype}\")  # datetime64[ns]\n",
    "\n",
    "# Convert Volume (remove commas, convert to int)\n",
    "# Method 1: String replacement\n",
    "df['Volume'] = df['Volume'].str.replace(',', '').astype(int)\n",
    "print(f\"\\nVolume after conversion:\\n{df['Volume']}\")\n",
    "\n",
    "# Convert Change_Pct (remove %, divide by 100)\n",
    "# Method 1: String replacement\n",
    "df['Change_Pct_Numeric'] = df['Change_Pct'].str.replace('%', '').astype(float) / 100\n",
    "print(f\"\\nChange % as decimal:\\n{df['Change_Pct_Numeric']}\")\n",
    "\n",
    "# Method 2: Using extract with regex for more complex patterns\n",
    "# Example: extract numeric value from strings like \"+1.06%\" or \"-0.50%\"\n",
    "df['Change_With_Sign'] = ['+1.06%', '-0.50%', '+1.06%', '+1.22%']\n",
    "# Extract numeric part (including sign and decimal)\n",
    "df['Change_Signed'] = df['Change_With_Sign'].str.extract(r'([+-]?\\d+\\.?\\d*)')[0].astype(float) / 100\n",
    "print(f\"\\nSigned change extraction:\\n{df['Change_Signed']}\")\n",
    "\n",
    "# Convert Symbol to category (more efficient for repeated strings)\n",
    "df['Symbol'] = df['Symbol'].astype('category')\n",
    "print(f\"\\nSymbol dtype: {df['Symbol'].dtype}\")\n",
    "\n",
    "print(f\"\\nFinal data types:\\n{df.dtypes}\")\n",
    "\n",
    "# Output:\n",
    "# After date conversion:\n",
    "# datetime64[ns]\n",
    "#\n",
    "# Volume after conversion:\n",
    "# 0    125000\n",
    "# 1    150000\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `pd.to_datetime()` converts strings to datetime. By default, it tries multiple formats automatically.\n",
    "- `.str.replace()` applies string replacement to each element in a string column. This is vectorized (fast for large datasets).\n",
    "- `.astype()` converts a column to a specified type. Common conversions: `int`, `float`, `str`, `category`.\n",
    "- For percentage strings: remove the `%`, convert to float, then divide by 100 to get the decimal value.\n",
    "- `.str.extract()` uses regular expressions to extract patterns. `([+-]?\\d+\\.?\\d*)` matches optional sign, digits, optional decimal, more digits.\n",
    "- `'category'` type stores strings as integer codes, saving memory for columns with few unique values (like stock symbols).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Handling numeric conversion errors\n",
    "\n",
    "# Sample data with problematic values\n",
    "problematic_data = {\n",
    "    'Price': ['2850.50', '2875.25', 'N/A', '2890.00', 'unknown', '2865.75'],\n",
    "    'Volume': ['125000', '150000', '', '175000', 'N/A', '140000']\n",
    "}\n",
    "df_problem = pd.DataFrame(problematic_data)\n",
    "print(\"Data with problematic values:\")\n",
    "print(df_problem)\n",
    "\n",
    "# Using to_numeric with error handling\n",
    "# errors='coerce' converts invalid values to NaN\n",
    "df_problem['Price_Clean'] = pd.to_numeric(df_problem['Price'], errors='coerce')\n",
    "print(f\"\\nPrice after to_numeric with coerce:\\n{df_problem['Price_Clean']}\")\n",
    "\n",
    "# Check how many values became NaN\n",
    "print(f\"\\nNaN count: {df_problem['Price_Clean'].isna().sum()}\")\n",
    "\n",
    "# Alternative: errors='raise' (default) raises exception on error\n",
    "# Alternative: errors='ignore' returns original values if conversion fails\n",
    "\n",
    "# Convert multiple columns at once\n",
    "df_problem[['Price_Clean', 'Volume_Clean']] = df_problem[['Price', 'Volume']].apply(\n",
    "    pd.to_numeric, errors='coerce'\n",
    ")\n",
    "print(f\"\\nMultiple columns converted:\\n{df_problem}\")\n",
    "\n",
    "# Fill NaN values after conversion\n",
    "df_problem['Price_Filled'] = df_problem['Price_Clean'].fillna(df_problem['Price_Clean'].mean())\n",
    "print(f\"\\nWith NaN filled by mean:\\n{df_problem['Price_Filled']}\")\n",
    "\n",
    "# Output:\n",
    "# Data with problematic values:\n",
    "#      Price  Volume\n",
    "# 0  2850.50  125000\n",
    "# 1  2875.25  150000\n",
    "# 2      N/A        \n",
    "# 3  2890.00  175000\n",
    "# 4  unknown     N/A\n",
    "# 5  2865.75  140000\n",
    "#\n",
    "# Price after to_numeric with coerce:\n",
    "# 0    2850.50\n",
    "# 1    2875.25\n",
    "# 2        NaN\n",
    "# 3    2890.00\n",
    "# 4        NaN\n",
    "# 5    2865.75\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `pd.to_numeric()` specifically converts to numeric types, with more control than `.astype()`.\n",
    "- `errors='coerce'` is crucial for real-world data: invalid values become NaN instead of causing errors.\n",
    "- `errors='raise'` (default) throws an exception if any value can't be converted.\n",
    "- `errors='ignore'` returns the original column if conversion fails (rarely useful).\n",
    "- `.apply()` applies a function to each column (or row with axis=1). Here, we apply `to_numeric` to multiple columns.\n",
    "- After conversion, NaN values need handling. `.fillna()` replaces NaN with specified values.\n",
    "- Common fill strategies: mean, median, forward fill, backward fill, or drop rows with NaN.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Type inference and optimization\n",
    "\n",
    "# Create a larger dataset for demonstration\n",
    "np.random.seed(42)\n",
    "large_df = pd.DataFrame({\n",
    "    'Symbol': np.random.choice(['NABIL', 'NICA', 'SCBL', 'ADBL'], 100000),\n",
    "    'Date': pd.date_range('2024-01-01', periods=100000, freq='H'),\n",
    "    'Price': np.random.uniform(1000, 3000, 100000),\n",
    "    'Volume': np.random.randint(10000, 500000, 100000),\n",
    "    'Flag': np.random.choice([0, 1], 100000)\n",
    "})\n",
    "\n",
    "print(\"Default data types and memory:\")\n",
    "print(large_df.dtypes)\n",
    "print(f\"Memory usage: {large_df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Optimize types\n",
    "optimized_df = large_df.copy()\n",
    "optimized_df['Symbol'] = optimized_df['Symbol'].astype('category')\n",
    "optimized_df['Price'] = optimized_df['Price'].astype('float32')\n",
    "optimized_df['Volume'] = optimized_df['Volume'].astype('int32')\n",
    "optimized_df['Flag'] = optimized_df['Flag'].astype('int8')  # For small integers\n",
    "\n",
    "print(\"\\nOptimized data types and memory:\")\n",
    "print(optimized_df.dtypes)\n",
    "print(f\"Memory usage: {optimized_df.memory_usage(deep=True).sum() / 1024 / 1024:.2f} MB\")\n",
    "\n",
    "# Verify no data loss\n",
    "print(f\"\\nPrice range preserved: {optimized_df['Price'].min():.2f} to {optimized_df['Price'].max():.2f}\")\n",
    "\n",
    "# Automatic type inference with infer_objects()\n",
    "# Useful after operations that convert types\n",
    "df_after_op = pd.DataFrame({'A': [1, 2, 3], 'B': ['x', 'y', 'z']})\n",
    "df_after_op['A'] = df_after_op['A'].astype(object)  # Convert to object\n",
    "print(f\"\\nBefore infer_objects: {df_after_op['A'].dtype}\")\n",
    "df_after_op = df_after_op.infer_objects()\n",
    "print(f\"After infer_objects: {df_after_op['A'].dtype}\")\n",
    "\n",
    "# Output:\n",
    "# Default data types and memory:\n",
    "# Symbol            object\n",
    "# Date       datetime64[ns]\n",
    "# Price            float64\n",
    "# Volume             int64\n",
    "# Flag               int64\n",
    "# dtype: object\n",
    "# Memory usage: 5.34 MB\n",
    "#\n",
    "# Optimized data types and memory:\n",
    "# Symbol          category\n",
    "# Date       datetime64[ns]\n",
    "# Price           float32\n",
    "# Volume            int32\n",
    "# Flag               int8\n",
    "# dtype: object\n",
    "# Memory usage: 1.14 MB\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Type optimization** can dramatically reduce memory usage (80% reduction in this example).\n",
    "- `category` type is ideal for strings with few unique values (like stock symbols). Instead of storing each string, it stores integer codes with a lookup table.\n",
    "- `float32` uses half the memory of `float64`. Precision is about 7 decimal digits, which is sufficient for stock prices.\n",
    "- `int32` supports values up to about ±2 billion (sufficient for volume). `int64` supports much larger values but uses twice the memory.\n",
    "- `int8` or `int16` are perfect for small integers (like binary flags 0/1, or small integers like ratings 1-5).\n",
    "- `.memory_usage(deep=True)` shows actual memory usage, including overhead for object types. Without `deep=True`, it underestimates memory for string columns.\n",
    "- `.infer_objects()` automatically infers better types after operations that may have converted them to `object`.\n",
    "- **Best practice**: Apply type optimization immediately after loading data, before any analysis.\n",
    "\n",
    "---\n",
    "\n",
    "## **4.5 Handling Missing Values**\n",
    "\n",
    "Missing values are common in time-series data due to holidays, trading halts, data collection errors, or system failures. Proper handling is essential for accurate predictions.\n",
    "\n",
    "### **4.5.1 Identification Strategies**\n",
    "\n",
    "Before handling missing values, we need to identify where they exist and understand their patterns.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample NEPSE data with missing values\n",
    "dates = pd.date_range('2024-01-15', periods=10, freq='D')\n",
    "\n",
    "nepse_data = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Symbol': 'NABIL',\n",
    "    'Open': [2850.50, np.nan, 2890.00, 2865.75, np.nan, 2880.50, 2905.00, np.nan, 2900.00, 2920.00],\n",
    "    'High': [2890.00, 2910.00, np.nan, 2900.00, 2915.00, 2920.00, np.nan, 2925.00, 2930.00, 2940.00],\n",
    "    'Low': [2840.00, 2860.00, 2880.00, np.nan, 2870.00, 2875.00, 2890.00, 2900.00, np.nan, 2910.00],\n",
    "    'Close': [2875.25, 2895.50, 2900.00, 2880.50, 2905.00, np.nan, 2910.00, 2915.00, 2925.00, 2935.00],\n",
    "    'Volume': [125000, 150000, np.nan, 140000, 160000, 180000, 155000, np.nan, 170000, 200000]\n",
    "})\n",
    "\n",
    "nepse_data.set_index('Date', inplace=True)\n",
    "print(\"NEPSE data with missing values:\")\n",
    "print(nepse_data)\n",
    "\n",
    "# Output:\n",
    "# NEPSE data with missing values:\n",
    "#            Symbol     Open     High      Low    Close   Volume\n",
    "# Date                                                         \n",
    "# 2024-01-15   NABIL  2850.50  2890.00  2840.00  2875.25  125000.0\n",
    "# 2024-01-16   NABIL      NaN  2910.00  2860.00  2895.50  150000.0\n",
    "# 2024-01-17   NABIL  2890.00      NaN  2880.00  2900.00       NaN\n",
    "# 2024-01-18   NABIL  2865.75  2900.00      NaN  2880.50  140000.0\n",
    "# 2024-01-19   NABIL      NaN  2915.00  2870.00  2905.00  160000.0\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- Missing values are represented as `NaN` (Not a Number) in pandas. They come from `np.nan` or appear when data is unavailable.\n",
    "- In time-series, missing values can occur for various reasons:\n",
    "  - **Non-trading days**: Weekends and holidays when markets are closed\n",
    "  - **Data collection failures**: System errors during data capture\n",
    "  - **Trading halts**: Stocks suspended from trading\n",
    "  - **Incomplete records**: Partial data from data providers\n",
    "- The sample data has scattered missing values across different columns, simulating real-world scenarios.\n",
    "- Setting the Date as index enables time-series operations like resampling and time-based slicing.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Identifying missing values - basic methods\n",
    "\n",
    "# isna() returns True for each missing value\n",
    "print(\"Boolean mask of missing values:\")\n",
    "print(nepse_data.isna())\n",
    "\n",
    "# Count missing values per column\n",
    "print(\"\\nMissing values per column:\")\n",
    "print(nepse_data.isna().sum())\n",
    "\n",
    "# Percentage of missing values\n",
    "print(\"\\nPercentage missing per column:\")\n",
    "print((nepse_data.isna().sum() / len(nepse_data) * 100).round(2))\n",
    "\n",
    "# Total missing values in dataset\n",
    "print(f\"\\nTotal missing values: {nepse_data.isna().sum().sum()}\")\n",
    "\n",
    "# Count non-missing values\n",
    "print(f\"Total non-missing values: {nepse_data.notna().sum().sum()}\")\n",
    "\n",
    "# Output:\n",
    "# Boolean mask of missing values:\n",
    "#             Symbol   Open   High    Low  Close  Volume\n",
    "# Date                                                   \n",
    "# 2024-01-15    False  False  False  False  False   False\n",
    "# 2024-01-16    False   True  False  False  False   False\n",
    "# 2024-01-17    False  False   True  False  False    True\n",
    "# ...\n",
    "#\n",
    "# Missing values per column:\n",
    "# Symbol    0\n",
    "# Open      2\n",
    "# High      2\n",
    "# Low       2\n",
    "# Close     1\n",
    "# Volume    2\n",
    "# dtype: int64\n",
    "#\n",
    "# Percentage missing per column:\n",
    "# Symbol     0.00\n",
    "# Open      20.00\n",
    "# High      20.00\n",
    "# Low       20.00\n",
    "# Close     10.00\n",
    "# Volume    20.00\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `.isna()` (or `.isnull()`) returns a boolean DataFrame where `True` indicates missing values.\n",
    "- `.isna().sum()` counts missing values in each column. The sum treats `True` as 1 and `False` as 0.\n",
    "- Percentage calculation: `(missing_count / total_rows) * 100` gives the missing rate.\n",
    "- `.isna().sum().sum()` counts total missing values across the entire DataFrame.\n",
    "- `.notna()` (or `.notnull()`) is the inverse of `.isna()`, returning `True` for non-missing values.\n",
    "- These metrics help prioritize which columns need attention and assess overall data quality.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Identifying patterns in missing data\n",
    "\n",
    "# Find rows with any missing values\n",
    "rows_with_missing = nepse_data[nepse_data.isna().any(axis=1)]\n",
    "print(\"Rows with at least one missing value:\")\n",
    "print(rows_with_missing)\n",
    "\n",
    "# Find rows where all values are missing\n",
    "rows_all_missing = nepse_data[nepse_data.isna().all(axis=1)]\n",
    "print(f\"\\nRows where all values are missing: {len(rows_all_missing)}\")\n",
    "\n",
    "# Find rows with specific number of missing values\n",
    "missing_count_per_row = nepse_data.isna().sum(axis=1)\n",
    "print(\"\\nMissing values per row:\")\n",
    "print(missing_count_per_row)\n",
    "\n",
    "# Rows with more than 2 missing values\n",
    "severely_missing = nepse_data[missing_count_per_row > 2]\n",
    "print(f\"\\nRows with >2 missing values: {len(severely_missing)}\")\n",
    "\n",
    "# Visualize missing data pattern\n",
    "print(\"\\nMissing data pattern (1 = missing, 0 = present):\")\n",
    "missing_pattern = nepse_data.isna().astype(int)\n",
    "print(missing_pattern)\n",
    "\n",
    "# Check for patterns - are missing values correlated?\n",
    "print(\"\\nMissing value correlation:\")\n",
    "# Which columns tend to be missing together?\n",
    "print(missing_pattern.corr())\n",
    "\n",
    "# Output:\n",
    "# Rows with at least one missing value:\n",
    "#            Symbol     Open     High      Low    Close   Volume\n",
    "# Date                                                         \n",
    "# 2024-01-16   NABIL      NaN  2910.00  2860.00  2895.50  150000.0\n",
    "# 2024-01-17   NABIL  2890.00      NaN  2880.00  2900.00       NaN\n",
    "# 2024-01-18   NABIL  2865.75  2900.00      NaN  2880.50  140000.0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `.isna().any(axis=1)` checks if any value is missing in each row (axis=1 means row-wise).\n",
    "- `.isna().all(axis=1)` checks if all values are missing in a row.\n",
    "- `.isna().sum(axis=1)` counts missing values in each row, helping identify severely affected rows.\n",
    "- Converting to integers (`.astype(int)`) creates a binary pattern matrix useful for visualization.\n",
    "- **Missing value correlation** reveals if certain columns tend to be missing together:\n",
    "  - Positive correlation means columns are often missing together\n",
    "  - Negative correlation means when one is missing, the other tends to be present\n",
    "  - Understanding these patterns helps diagnose data collection issues\n",
    "- This analysis guides the choice of imputation strategy.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.5.2 Imputation Methods**\n",
    "\n",
    "Imputation fills in missing values with estimated or derived values. The choice depends on the nature of the data and missing pattern.\n",
    "\n",
    "```python\n",
    "# Method 1: Forward Fill (carry forward last known value)\n",
    "# Useful for time-series where values don't change rapidly\n",
    "\n",
    "df_ffill = nepse_data.copy()\n",
    "\n",
    "# Forward fill all numeric columns\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "df_ffill[numeric_cols] = df_ffill[numeric_cols].ffill()\n",
    "\n",
    "print(\"After forward fill:\")\n",
    "print(df_ffill)\n",
    "\n",
    "# Explanation of forward fill:\n",
    "# Day 1 (Jan 15): Open = 2850.50 (original)\n",
    "# Day 2 (Jan 16): Open = NaN → filled with 2850.50 (from Jan 15)\n",
    "# Day 3 (Jan 17): Open = 2890.00 (original)\n",
    "# Day 4 (Jan 18): Open = 2865.75 (original)\n",
    "# Day 5 (Jan 19): Open = NaN → filled with 2865.75 (from Jan 18)\n",
    "\n",
    "# Forward fill is appropriate for:\n",
    "# - Stock prices (prices don't change dramatically overnight without reason)\n",
    "# - Sensor readings (physical quantities have continuity)\n",
    "# - Any data with temporal continuity\n",
    "\n",
    "# Output:\n",
    "# After forward fill:\n",
    "#            Symbol     Open     High      Low    Close   Volume\n",
    "# Date                                                         \n",
    "# 2024-01-15   NABIL  2850.50  2890.00  2840.00  2875.25  125000.0\n",
    "# 2024-01-16   NABIL  2850.50  2910.00  2860.00  2895.50  150000.0\n",
    "# 2024-01-17   NABIL  2890.00  2910.00  2880.00  2900.00  150000.0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Forward fill (ffill)** propagates the last valid observation forward to fill missing values.\n",
    "- In time-series, this makes sense because today's value is often a reasonable estimate for a missing tomorrow value.\n",
    "- The logic: if Open is missing on Jan 16, use Jan 15's Open as an approximation.\n",
    "- `ffill()` is equivalent to `fillna(method='ffill')` in older pandas versions.\n",
    "- **When to use forward fill**:\n",
    "  - Data with temporal continuity (prices, temperatures, sensor readings)\n",
    "  - Short gaps (missing one or two consecutive values)\n",
    "  - When the most recent value is the best estimate\n",
    "- **Limitations**:\n",
    "  - Not suitable for data with sudden jumps or discontinuities\n",
    "  - Can propagate old values across long gaps\n",
    "  - Assumes stability between observations\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Method 2: Backward Fill (carry backward next known value)\n",
    "\n",
    "df_bfill = nepse_data.copy()\n",
    "df_bfill[numeric_cols] = df_bfill[numeric_cols].bfill()\n",
    "\n",
    "print(\"After backward fill:\")\n",
    "print(df_bfill)\n",
    "\n",
    "# Backward fill is useful when:\n",
    "# - You have future data available (not for real-time prediction!)\n",
    "# - Missing values at the beginning of the series\n",
    "# - When next known value is more relevant than previous\n",
    "\n",
    "# Combine forward and backward fill for complete coverage\n",
    "df_combined = nepse_data.copy()\n",
    "df_combined[numeric_cols] = df_combined[numeric_cols].ffill().bfill()\n",
    "# Forward fill first, then backward fill any remaining NaN at the start\n",
    "\n",
    "print(\"\\nAfter combined forward-backward fill:\")\n",
    "print(df_combined)\n",
    "\n",
    "# Output:\n",
    "# After backward fill:\n",
    "#            Symbol     Open     High      Low    Close   Volume\n",
    "# Date                                                         \n",
    "# 2024-01-15   NABIL  2850.50  2890.00  2840.00  2875.25  125000.0\n",
    "# 2024-01-16   NABIL  2890.00  2910.00  2860.00  2895.50  150000.0\n",
    "# 2024-01-17   NABIL  2890.00  2865.75  2880.00  2900.00  140000.0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Backward fill (bfill)** propagates the next valid observation backward to fill missing values.\n",
    "- The logic: if Open is missing on Jan 16, use Jan 17's Open as an approximation.\n",
    "- **Important caveat**: Backward fill uses future data! This causes **data leakage** in prediction systems:\n",
    "  - If you're predicting tomorrow's price, you can't use tomorrow's price to fill today's missing value\n",
    "  - Only use backward fill for historical analysis, not for training prediction models\n",
    "- **Combined approach** (`ffill().bfill()`):\n",
    "  - Forward fill handles most missing values\n",
    "  - Backward fill handles missing values at the start of the series (where forward fill can't work)\n",
    "- In production prediction systems, always consider what data would be available at prediction time.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Method 3: Fill with constant value\n",
    "\n",
    "df_constant = nepse_data.copy()\n",
    "\n",
    "# Fill with specific value\n",
    "df_constant['Volume'] = df_constant['Volume'].fillna(0)\n",
    "print(\"Volume filled with 0:\")\n",
    "print(df_constant['Volume'])\n",
    "\n",
    "# Fill with mean value\n",
    "mean_close = df_constant['Close'].mean()\n",
    "df_constant['Close'] = df_constant['Close'].fillna(mean_close)\n",
    "print(f\"\\nClose filled with mean ({mean_close:.2f}):\")\n",
    "print(df_constant['Close'])\n",
    "\n",
    "# Fill with median (more robust to outliers)\n",
    "median_open = df_constant['Open'].median()\n",
    "df_constant['Open'] = df_constant['Open'].fillna(median_open)\n",
    "print(f\"\\nOpen filled with median ({median_open:.2f}):\")\n",
    "print(df_constant['Open'])\n",
    "\n",
    "# Fill different columns with different values\n",
    "df_multi_fill = nepse_data.copy()\n",
    "fill_values = {\n",
    "    'Open': nepse_data['Open'].mean(),\n",
    "    'High': nepse_data['High'].max(),  # Conservative estimate for High\n",
    "    'Low': nepse_data['Low'].min(),    # Conservative estimate for Low\n",
    "    'Close': nepse_data['Close'].mean(),\n",
    "    'Volume': 0  # Unknown volume = no trading\n",
    "}\n",
    "df_multi_fill = df_multi_fill.fillna(fill_values)\n",
    "print(\"\\nMultiple columns filled with different values:\")\n",
    "print(df_multi_fill)\n",
    "\n",
    "# Output:\n",
    "# Volume filled with 0:\n",
    "# Date\n",
    "# 2024-01-15    125000.0\n",
    "# 2024-01-16    150000.0\n",
    "# 2024-01-17         0.0\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Constant value fill** replaces missing values with a fixed value, which can be:\n",
    "  - Zero: For volume/count data where missing means \"none\"\n",
    "  - Mean: For normally distributed data\n",
    "  - Median: For skewed data (more robust to outliers)\n",
    "  - Domain-specific value: Based on business knowledge\n",
    "- **For NEPSE data**:\n",
    "  - Volume = 0 makes sense if missing means no trading occurred\n",
    "  - Mean/median for prices provides a neutral estimate\n",
    "  - Filling High with max and Low with min are conservative estimates\n",
    "- `.fillna()` accepts a dictionary to specify different fill values for different columns.\n",
    "- **When to use constant fill**:\n",
    "  - When missing has a specific meaning (e.g., Volume = 0 means no trades)\n",
    "  - For categorical columns (fill with \"Unknown\" or most frequent category)\n",
    "  - When you want to flag imputed values (e.g., fill with -999 to mark them)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Method 4: Linear Interpolation\n",
    "\n",
    "df_interpolate = nepse_data.copy()\n",
    "\n",
    "# Linear interpolation (default)\n",
    "df_interpolate['Close'] = df_interpolate['Close'].interpolate(method='linear')\n",
    "print(\"Close with linear interpolation:\")\n",
    "print(df_interpolate['Close'])\n",
    "\n",
    "# Interpolation methods comparison\n",
    "sample_data = pd.Series([2850, np.nan, np.nan, np.nan, 2900])\n",
    "\n",
    "print(\"\\nInterpolation methods comparison:\")\n",
    "print(f\"Original: {sample_data.values}\")\n",
    "print(f\"Linear: {sample_data.interpolate(method='linear').values}\")\n",
    "# Evenly spaces values between 2850 and 2900\n",
    "\n",
    "print(f\"Quadratic: {sample_data.interpolate(method='quadratic').values}\")\n",
    "# Uses quadratic curve (smoother but can overshoot)\n",
    "\n",
    "print(f\"Cubic: {sample_data.interpolate(method='cubic').values}\")\n",
    "# Uses cubic spline (smoothest but most complex)\n",
    "\n",
    "# Time-based interpolation (respects datetime index)\n",
    "df_time = nepse_data.copy()\n",
    "df_time['Close'] = df_time['Close'].interpolate(method='time')\n",
    "print(\"\\nTime-based interpolation:\")\n",
    "print(df_time['Close'])\n",
    "\n",
    "# For time-series with datetime index, 'time' method is recommended\n",
    "# It accounts for irregular time intervals between observations\n",
    "\n",
    "# Output:\n",
    "# Close with linear interpolation:\n",
    "# Date\n",
    "# 2024-01-15    2875.25\n",
    "# 2024-01-16    2895.50\n",
    "# 2024-01-17    2900.00\n",
    "# 2024-01-18    2880.50\n",
    "# 2024-01-19    2905.00\n",
    "# 2024-01-20    2907.50  # Interpolated between 2905 and 2910\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Interpolation** estimates missing values by fitting a curve through known points.\n",
    "- **Linear interpolation** draws a straight line between adjacent known values and estimates the missing point on that line.\n",
    "- Example: If Close is 100 on Day 1 and 110 on Day 5, missing Day 3 would be estimated as 105.\n",
    "- **Interpolation methods**:\n",
    "  - `linear`: Straight line between points (most common)\n",
    "  - `quadratic`: Parabolic curve (smoother but can overshoot)\n",
    "  - `cubic`: Spline interpolation (smoothest, most complex)\n",
    "  - `time`: Linear interpolation weighted by time intervals (best for irregular time-series)\n",
    "- **For time-series data**:\n",
    "  - Use `method='time'` when you have a DatetimeIndex\n",
    "  - It properly handles irregular intervals (weekends, holidays)\n",
    "  - Linear interpolation is usually sufficient for stock prices\n",
    "- **Limitations**:\n",
    "  - Assumes smooth change between points\n",
    "  - Not suitable for data with sudden jumps\n",
    "  - Requires values on both sides (can't interpolate at the start or end)\n",
    "\n",
    "---\n",
    "\n",
    "### **4.5.3 Forward/Backward Fill**\n",
    "\n",
    "We covered forward and backward fill in the previous section, but let's explore more advanced applications.\n",
    "\n",
    "```python\n",
    "# Advanced forward/backward fill with limits\n",
    "\n",
    "df_limited = nepse_data.copy()\n",
    "\n",
    "# Limit the number of consecutive fills\n",
    "# Only fill up to 2 consecutive missing values\n",
    "df_limited['Close'] = df_limited['Close'].ffill(limit=2)\n",
    "print(\"Forward fill with limit=2:\")\n",
    "print(df_limited['Close'])\n",
    "\n",
    "# Why use limits?\n",
    "# - Prevents filling long gaps with outdated values\n",
    "# - If data is missing for a week, using last week's price may be wrong\n",
    "# - Better to keep NaN for long gaps and handle them differently\n",
    "\n",
    "# Create scenario with long gap\n",
    "long_gap_data = pd.DataFrame({\n",
    "    'Close': [2850, np.nan, np.nan, np.nan, np.nan, np.nan, 2900, 2910, 2920]\n",
    "})\n",
    "print(\"\\nData with long gap:\")\n",
    "print(long_gap_data['Close'])\n",
    "\n",
    "# Without limit - fills all gaps\n",
    "print(\"\\nWithout limit:\")\n",
    "print(long_gap_data['Close'].ffill())\n",
    "\n",
    "# With limit - only fills limited consecutive gaps\n",
    "print(\"\\nWith limit=2:\")\n",
    "print(long_gap_data['Close'].ffill(limit=2))\n",
    "\n",
    "# Forward fill with limit, then backward fill remaining\n",
    "df_hybrid = long_gap_data.copy()\n",
    "df_hybrid['Close'] = df_hybrid['Close'].ffill(limit=2).bfill(limit=2)\n",
    "print(\"\\nHybrid approach (ffill limit=2, then bfill limit=2):\")\n",
    "print(df_hybrid['Close'])\n",
    "\n",
    "# Output:\n",
    "# Forward fill with limit=2:\n",
    "# Date\n",
    "# 2024-01-15    2875.25\n",
    "# 2024-01-16    2895.50\n",
    "# 2024-01-17    2900.00\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- The `limit` parameter in `ffill()` and `bfill()` restricts how many consecutive missing values can be filled.\n",
    "- **Why use limits**:\n",
    "  - Long gaps should be treated differently than short gaps\n",
    "  - Filling a week-long gap with last week's price could be misleading\n",
    "  - Missing data might indicate a significant event (trading halt, delisting)\n",
    "- **Hybrid approach** (ffill + bfill with limits):\n",
    "  - Fills short gaps in both directions\n",
    "  - Leaves long gaps as NaN for special handling\n",
    "- This is crucial for prediction systems:\n",
    "  - A 1-day gap might be a data error → fill it\n",
    "  - A 30-day gap might be a trading halt → treat separately or exclude\n",
    "- **Best practice**: Use limits, and then analyze remaining NaN values to understand why they couldn't be filled.\n",
    "\n",
    "---\n",
    "\n",
    "### **4.5.4 Advanced Techniques**\n",
    "\n",
    "For complex missing data patterns, more sophisticated approaches are needed.\n",
    "\n",
    "```python\n",
    "# Advanced imputation using rolling statistics\n",
    "\n",
    "df_rolling = nepse_data.copy()\n",
    "\n",
    "# Fill with rolling mean (average of nearby values)\n",
    "# This provides a local estimate rather than global mean\n",
    "window_size = 3\n",
    "\n",
    "# Calculate rolling mean and use it for filling\n",
    "rolling_mean = df_rolling['Close'].rolling(window=window_size, min_periods=1).mean()\n",
    "df_rolling['Close_Rolling_Fill'] = df_rolling['Close'].fillna(rolling_mean)\n",
    "print(\"Rolling mean fill:\")\n",
    "print(df_rolling[['Close', 'Close_Rolling_Fill']])\n",
    "\n",
    "# The rolling mean uses nearby values, making it more context-aware\n",
    "# Example: If surrounding values are ~2900, the fill will be ~2900\n",
    "# Not the global mean which might be 2850 or 2950\n",
    "\n",
    "# Fill with expanding mean (cumulative average)\n",
    "df_expanding = nepse_data.copy()\n",
    "expanding_mean = df_expanding['Close'].expanding().mean()\n",
    "df_expanding['Close_Expanding_Fill'] = df_expanding['Close'].fillna(expanding_mean)\n",
    "print(\"\\nExpanding mean fill:\")\n",
    "print(df_expanding[['Close', 'Close_Expanding_Fill']])\n",
    "\n",
    "# Output:\n",
    "# Rolling mean fill:\n",
    "#              Close  Close_Rolling_Fill\n",
    "# Date                                    \n",
    "# 2024-01-15  2875.25            2875.250\n",
    "# 2024-01-16  2895.50            2895.500\n",
    "# 2024-01-17  2900.00            2900.000\n",
    "# 2024-01-18  2880.50            2880.500\n",
    "# 2024-01-19  2905.00            2905.000\n",
    "# 2024-01-20     NaN            2895.167  # Filled with mean of previous 3\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Rolling mean fill** uses the average of nearby values (window) rather than the global mean.\n",
    "- A window of 3 means we average the 3 closest known values (can include future values in standard rolling).\n",
    "- **Advantages**:\n",
    "  - Context-aware: Estimates are based on local patterns\n",
    "  - Captures local trends: If prices are rising, fill will reflect the rise\n",
    "  - More accurate than global mean for non-stationary data\n",
    "- **Expanding mean** uses all historical values up to that point.\n",
    "- **For time-series prediction**:\n",
    "  - Rolling mean is better for capturing local behavior\n",
    "  - Be careful not to use future values when filling training data\n",
    "  - Use `min_periods=1` to allow calculation even with few values\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# KNN Imputation - using similar patterns to fill missing values\n",
    "\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Prepare data for KNN imputation\n",
    "# KNN works best with multiple correlated features\n",
    "df_knn = nepse_data.copy()\n",
    "\n",
    "# Select numeric columns for imputation\n",
    "numeric_cols = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "knn_data = df_knn[numeric_cols].values\n",
    "\n",
    "# Initialize KNN imputer\n",
    "# n_neighbors: number of similar rows to consider\n",
    "# weights: 'uniform' or 'distance' (closer neighbors have more influence)\n",
    "imputer = KNNImputer(n_neighbors=3, weights='distance')\n",
    "\n",
    "# Perform imputation\n",
    "imputed_data = imputer.fit_transform(knn_data)\n",
    "\n",
    "# Create DataFrame with imputed values\n",
    "df_knn_imputed = pd.DataFrame(imputed_data, columns=numeric_cols, index=df_knn.index)\n",
    "print(\"KNN imputed data:\")\n",
    "print(df_knn_imputed)\n",
    "\n",
    "# Compare original and imputed\n",
    "print(\"\\nComparison for a row with missing values:\")\n",
    "print(f\"Original: {df_knn.loc['2024-01-16', numeric_cols]}\")\n",
    "print(f\"Imputed: {df_knn_imputed.loc['2024-01-16']}\")\n",
    "\n",
    "# KNN imputation advantages:\n",
    "# 1. Uses correlations between features (Open, High, Low are related)\n",
    "# 2. Finds similar days based on all features\n",
    "# 3. Distance weighting gives more influence to more similar days\n",
    "\n",
    "# Output:\n",
    "# KNN imputed data:\n",
    "#                Open     High      Low    Close    Volume\n",
    "# Date                                                     \n",
    "# 2024-01-15  2850.50  2890.00  2840.00  2875.25  125000.0\n",
    "# 2024-01-16  2876.42  2910.00  2860.00  2895.50  150000.0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **KNN (K-Nearest Neighbors) Imputation** fills missing values using similar rows.\n",
    "- **How it works**:\n",
    "  1. Calculate distances between rows using non-missing features\n",
    "  2. Find the k most similar rows (nearest neighbors)\n",
    "  3. Average the values from those neighbors (weighted by distance)\n",
    "- **Parameters**:\n",
    "  - `n_neighbors=3`: Use 3 most similar days to estimate missing values\n",
    "  - `weights='distance'': Closer neighbors have more influence\n",
    "- **Advantages for time-series**:\n",
    "  - Uses correlations between features (if Close is 2900, Open is likely close)\n",
    "  - Captures multi-dimensional patterns\n",
    "  - Works well when features are correlated\n",
    "- **Limitations**:\n",
    "  - Computationally expensive for large datasets\n",
    "  - Sensitive to feature scaling (normalize features first)\n",
    "  - May not respect temporal order\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Iterative Imputation (MICE - Multiple Imputation by Chained Equations)\n",
    "\n",
    "from sklearn.experimental import enable_iterative_imputer\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "df_iterative = nepse_data.copy()\n",
    "numeric_data = df_iterative[numeric_cols].values\n",
    "\n",
    "# Initialize iterative imputer\n",
    "# Uses regression to predict missing values based on other features\n",
    "iterative_imputer = IterativeImputer(\n",
    "    max_iter=10,  # Number of iterations\n",
    "    random_state=42,  # For reproducibility\n",
    "    min_value=0  # Ensure no negative prices/volumes\n",
    ")\n",
    "\n",
    "# Perform imputation\n",
    "imputed_iterative = iterative_imputer.fit_transform(numeric_data)\n",
    "\n",
    "df_iterative_imputed = pd.DataFrame(imputed_iterative, columns=numeric_cols, index=df_iterative.index)\n",
    "print(\"Iterative imputed data:\")\n",
    "print(df_iterative_imputed)\n",
    "\n",
    "# Iterative imputation process:\n",
    "# 1. Fill missing values with initial estimates (mean)\n",
    "# 2. For each feature with missing values:\n",
    "#    a. Treat it as target, other features as predictors\n",
    "#    b. Train regression model on known values\n",
    "#    c. Predict missing values\n",
    "# 3. Repeat until convergence\n",
    "\n",
    "# This is more sophisticated than KNN and can capture complex relationships\n",
    "\n",
    "# Compare methods\n",
    "print(\"\\nComparison of imputation methods:\")\n",
    "comparison_df = pd.DataFrame({\n",
    "    'Original': nepse_data['Open'],\n",
    "    'Forward_Fill': nepse_data['Open'].ffill(),\n",
    "    'Mean_Fill': nepse_data['Open'].fillna(nepse_data['Open'].mean()),\n",
    "    'Interpolate': nepse_data['Open'].interpolate(),\n",
    "    'KNN': df_knn_imputed['Open'],\n",
    "    'Iterative': df_iterative_imputed['Open']\n",
    "})\n",
    "print(comparison_df)\n",
    "\n",
    "# Output:\n",
    "# Iterative imputed data:\n",
    "#                Open     High      Low    Close    Volume\n",
    "# Date                                                     \n",
    "# 2024-01-15  2850.50  2890.00  2840.00  2875.25  125000.0\n",
    "# 2024-01-16  2872.38  2910.00  2860.00  2895.50  150000.0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Iterative Imputation (MICE)** is a sophisticated approach that models each feature as a function of others.\n",
    "- **The algorithm**:\n",
    "  1. Initialize: Fill all missing values with simple estimates (mean)\n",
    "  2. Iterate: For each feature with originally missing values:\n",
    "     - Set those values back to \"missing\"\n",
    "     - Train a regression model using other features\n",
    "     - Predict the missing values\n",
    "  3. Repeat until estimates stabilize (max_iter)\n",
    "- **Advantages**:\n",
    "  - Captures complex relationships between features\n",
    "  - Uses all available information\n",
    "  - Produces more accurate estimates than simple methods\n",
    "- **Parameters**:\n",
    "  - `max_iter`: Number of iterations (10 is usually sufficient)\n",
    "  - `random_state`: For reproducibility\n",
    "  - `min_value/max_value`: Constraints on valid values\n",
    "- **When to use**:\n",
    "  - When features are correlated (Open, High, Low, Close are strongly correlated)\n",
    "  - When simple methods are insufficient\n",
    "  - For final production imputation (more accurate but slower)\n",
    "\n",
    "---\n",
    "\n",
    "## **4.6 Handling Outliers**\n",
    "\n",
    "Outliers are extreme values that deviate significantly from other observations. They can distort predictions and must be handled appropriately.\n",
    "\n",
    "### **4.6.1 Detection Methods**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample NEPSE data with outliers\n",
    "dates = pd.date_range('2024-01-15', periods=20, freq='D')\n",
    "\n",
    "np.random.seed(42)\n",
    "base_prices = np.linspace(2850, 2950, 20) + np.random.normal(0, 10, 20)\n",
    "\n",
    "# Add outliers\n",
    "base_prices[5] = 3500   # Unusually high price (potential error)\n",
    "base_prices[12] = 2100  # Unusually low price (potential error)\n",
    "base_prices[18] = 5000  # Extreme high (likely error)\n",
    "\n",
    "nepse_outliers = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Close': base_prices,\n",
    "    'Volume': np.random.randint(100000, 200000, 20)\n",
    "})\n",
    "nepse_outliers[19] = 2000000  # Outlier in volume\n",
    "nepse_outliers['Volume'][19] = 2000000\n",
    "\n",
    "nepse_outliers.set_index('Date', inplace=True)\n",
    "print(\"Data with outliers:\")\n",
    "print(nepse_outliers)\n",
    "\n",
    "# Basic statistics to identify potential outliers\n",
    "print(\"\\nBasic statistics:\")\n",
    "print(nepse_outliers.describe())\n",
    "\n",
    "# Notice the max Close is 5000, much higher than the 75% percentile (2932)\n",
    "# This suggests extreme outliers\n",
    "\n",
    "# Output:\n",
    "# Data with outliers:\n",
    "#                Close    Volume\n",
    "# Date                          \n",
    "# 2024-01-15  2852.48  165281.0\n",
    "# 2024-01-16  2857.78  185681.0\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Outliers** can be:\n",
    "  - Data entry errors (typo: 5000 instead of 2900)\n",
    "  - Genuine extreme events (market crash, huge trade)\n",
    "  - Measurement errors (sensor malfunction)\n",
    "- The sample data has outliers:\n",
    "  - Row 5: Close = 3500 (unusually high)\n",
    "  - Row 12: Close = 2100 (unusually low)\n",
    "  - Row 18: Close = 5000 (extreme - likely error)\n",
    "  - Row 19: Volume = 2000000 (10x normal)\n",
    "- `describe()` provides a quick overview. Large differences between max and 75% percentile suggest outliers.\n",
    "- The gap between 75% (2932) and max (5000) is suspicious.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Method 1: Z-Score Method\n",
    "# Z-score measures how many standard deviations a value is from the mean\n",
    "\n",
    "def detect_outliers_zscore(data, threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using Z-score method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Data to check for outliers\n",
    "    threshold : float\n",
    "        Number of standard deviations to consider as outlier (default: 3)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    boolean array\n",
    "        True indicates outlier\n",
    "    \"\"\"\n",
    "    mean = np.mean(data)\n",
    "    std = np.std(data)\n",
    "    z_scores = np.abs((data - mean) / std)\n",
    "    return z_scores > threshold\n",
    "\n",
    "# Calculate Z-scores for Close prices\n",
    "close_prices = nepse_outliers['Close'].values\n",
    "z_scores = np.abs((close_prices - close_prices.mean()) / close_prices.std())\n",
    "\n",
    "print(\"Z-scores for Close prices:\")\n",
    "for i, (date, z) in enumerate(zip(nepse_outliers.index, z_scores)):\n",
    "    if z > 3:\n",
    "        print(f\"{date.date()}: Close={close_prices[i]:.2f}, Z-score={z:.2f} <-- OUTLIER\")\n",
    "\n",
    "# Identify outliers\n",
    "outlier_mask_zscore = detect_outliers_zscore(close_prices)\n",
    "print(f\"\\nNumber of outliers detected (Z-score): {outlier_mask_zscore.sum()}\")\n",
    "print(f\"Outlier indices: {np.where(outlier_mask_zscore)[0]}\")\n",
    "\n",
    "# View outliers\n",
    "print(\"\\nOutlier rows (Z-score method):\")\n",
    "print(nepse_outliers[outlier_mask_zscore])\n",
    "\n",
    "# Output:\n",
    "# Z-scores for Close prices:\n",
    "# 2024-01-20: Close=3500.00, Z-score=3.05 <-- OUTLIER\n",
    "# 2024-01-27: Close=2100.00, Z-score=3.34 <-- OUTLIER\n",
    "# 2024-02-02: Close=5000.00, Z-score=7.67 <-- OUTLIER\n",
    "#\n",
    "# Number of outliers detected (Z-score): 3\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Z-score** measures how far a value is from the mean in units of standard deviation.\n",
    "- Formula: \\( Z = \\frac{|x - \\mu|}{\\sigma} \\)\n",
    "- **Interpretation**:\n",
    "  - Z = 0: Value equals the mean\n",
    "  - Z = 1: Value is 1 standard deviation from mean\n",
    "  - Z = 3: Value is 3 standard deviations from mean (very rare in normal distribution)\n",
    "- **Threshold**:\n",
    "  - Common choice is 3 (values beyond 3σ are considered outliers)\n",
    "  - For normal distribution, 99.7% of values fall within ±3σ\n",
    "  - Lower threshold (2) catches more outliers but may include false positives\n",
    "- **Limitations**:\n",
    "  - Sensitive to the outliers themselves (outliers affect mean and std)\n",
    "  - Assumes normal distribution (stock prices are not normal)\n",
    "  - May not work well for skewed distributions\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Method 2: IQR Method (Interquartile Range)\n",
    "# More robust to extreme values than Z-score\n",
    "\n",
    "def detect_outliers_iqr(data, k=1.5):\n",
    "    \"\"\"\n",
    "    Detect outliers using IQR method.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Data to check for outliers\n",
    "    k : float\n",
    "        Multiplier for IQR (default: 1.5 for outliers, 3 for extreme outliers)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    boolean array, lower_bound, upper_bound\n",
    "    \"\"\"\n",
    "    q1 = np.percentile(data, 25)\n",
    "    q3 = np.percentile(data, 75)\n",
    "    iqr = q3 - q1\n",
    "    \n",
    "    lower_bound = q1 - k * iqr\n",
    "    upper_bound = q3 + k * iqr\n",
    "    \n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Detect outliers for Close prices\n",
    "outlier_mask_iqr, lower, upper = detect_outliers_iqr(close_prices)\n",
    "\n",
    "print(f\"IQR Method:\")\n",
    "print(f\"Q1 (25%): {np.percentile(close_prices, 25):.2f}\")\n",
    "print(f\"Q3 (75%): {np.percentile(close_prices, 75):.2f}\")\n",
    "print(f\"IQR: {np.percentile(close_prices, 75) - np.percentile(close_prices, 25):.2f}\")\n",
    "print(f\"Lower bound: {lower:.2f}\")\n",
    "print(f\"Upper bound: {upper:.2f}\")\n",
    "\n",
    "print(f\"\\nNumber of outliers detected (IQR): {outlier_mask_iqr.sum()}\")\n",
    "\n",
    "# View outliers\n",
    "print(\"\\nOutlier rows (IQR method):\")\n",
    "outliers_df = nepse_outliers[outlier_mask_iqr].copy()\n",
    "outliers_df['Close'] = outliers_df['Close'].apply(lambda x: f\"{x:.2f}\")\n",
    "print(outliers_df)\n",
    "\n",
    "# Compare with Z-score\n",
    "print(\"\\nComparison:\")\n",
    "print(f\"Z-score method: {outlier_mask_zscore.sum()} outliers\")\n",
    "print(f\"IQR method: {outlier_mask_iqr.sum()} outliers\")\n",
    "\n",
    "# Output:\n",
    "# IQR Method:\n",
    "# Q1 (25%): 2872.35\n",
    "# Q3 (75%): 2935.00\n",
    "# IQR: 62.65\n",
    "# Lower bound: 2778.38\n",
    "# Upper bound: 3028.97\n",
    "#\n",
    "# Number of outliers detected (IQR): 3\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **IQR (Interquartile Range)** method is more robust to outliers than Z-score.\n",
    "- **The calculation**:\n",
    "  1. Q1 = 25th percentile\n",
    "  2. Q3 = 75th percentile\n",
    "  3. IQR = Q3 - Q1 (middle 50% spread)\n",
    "  4. Lower bound = Q1 - k × IQR\n",
    "  5. Upper bound = Q3 + k × IQR\n",
    "- **k parameter**:\n",
    "  - k = 1.5: Standard outlier detection\n",
    "  - k = 3: Extreme outlier detection (only very extreme values)\n",
    "- **Advantages over Z-score**:\n",
    "  - Uses percentiles, which are not affected by extreme values\n",
    "  - Works better for skewed distributions\n",
    "  - Doesn't assume normal distribution\n",
    "- The bounds (2778 to 3029) define the \"normal\" range. Values outside are flagged.\n",
    "- This method is widely used in practice, especially for financial data.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Method 3: Visual Detection with Box Plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
    "\n",
    "# Box plot for Close prices\n",
    "axes[0].boxplot(close_prices, vert=True)\n",
    "axes[0].set_title('Close Price Distribution')\n",
    "axes[0].set_ylabel('Price (NPR)')\n",
    "axes[0].set_xticklabels(['Close'])\n",
    "\n",
    "# Box plot for Volume\n",
    "axes[1].boxplot(nepse_outliers['Volume'].dropna(), vert=True)\n",
    "axes[1].set_title('Volume Distribution')\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_xticklabels(['Volume'])\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('outlier_boxplot.png', dpi=100)\n",
    "plt.close()\n",
    "\n",
    "print(\"Box plot saved to 'outlier_boxplot.png'\")\n",
    "\n",
    "# Box plot interpretation:\n",
    "# - The box shows Q1 to Q3 (middle 50% of data)\n",
    "# - The line in the box is the median\n",
    "# - Whiskers extend to the most extreme non-outlier values\n",
    "# - Points beyond whiskers are outliers (shown as individual dots)\n",
    "\n",
    "# Visual detection is useful for:\n",
    "# 1. Quick identification of outliers\n",
    "# 2. Understanding the distribution shape\n",
    "# 3. Communicating findings to stakeholders\n",
    "\n",
    "# Output:\n",
    "# Box plot saved to 'outlier_boxplot.png'\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Box plot** is a visual tool for outlier detection and distribution understanding.\n",
    "- **Components**:\n",
    "  - **Box**: Q1 to Q3 (middle 50% of data)\n",
    "  - **Line in box**: Median (Q2, 50th percentile)\n",
    "  - **Whiskers**: Extend to last non-outlier point (typically Q1 - 1.5×IQR to Q3 + 1.5×IQR)\n",
    "  - **Points beyond whiskers**: Outliers (plotted individually)\n",
    "- **When to use visual detection**:\n",
    "  - Initial data exploration\n",
    "  - Communicating outlier presence to non-technical stakeholders\n",
    "  - Understanding distribution shape and skewness\n",
    "- **Advantages**:\n",
    "  - Intuitive and visual\n",
    "  - Shows distribution shape, not just outliers\n",
    "  - Easy to compare multiple groups\n",
    "- **Limitations**:\n",
    "  - Subjective - no precise definition\n",
    "  - Not suitable for automated detection\n",
    "  - Can be misleading for small datasets\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Method 4: Rolling Statistics for Time-Series Outliers\n",
    "# For time-series, use rolling windows to detect local outliers\n",
    "\n",
    "def detect_outliers_rolling(data, window=7, n_std=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using rolling statistics.\n",
    "    \n",
    "    Outliers are identified relative to local mean and std,\n",
    "    not global statistics. This is important for non-stationary data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : Series\n",
    "        Time-series data\n",
    "    window : int\n",
    "        Rolling window size\n",
    "    n_std : float\n",
    "        Number of standard deviations for threshold\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    Series of boolean (True = outlier)\n",
    "    \"\"\"\n",
    "    rolling_mean = data.rolling(window=window, center=True, min_periods=1).mean()\n",
    "    rolling_std = data.rolling(window=window, center=True, min_periods=1).std()\n",
    "    \n",
    "    # Calculate bounds\n",
    "    lower_bound = rolling_mean - n_std * rolling_std\n",
    "    upper_bound = rolling_mean + n_std * rolling_std\n",
    "    \n",
    "    # Identify outliers\n",
    "    outliers = (data < lower_bound) | (data > upper_bound)\n",
    "    \n",
    "    return outliers, lower_bound, upper_bound\n",
    "\n",
    "# Apply rolling outlier detection\n",
    "outlier_mask_rolling, lower_rolling, upper_rolling = detect_outliers_rolling(\n",
    "    nepse_outliers['Close'], window=7, n_std=2.5\n",
    ")\n",
    "\n",
    "print(\"Rolling outlier detection:\")\n",
    "print(f\"Window size: 7 days\")\n",
    "print(f\"Threshold: 2.5 standard deviations\")\n",
    "\n",
    "# Show detected outliers\n",
    "outlier_dates = nepse_outliers.index[outlier_mask_rolling]\n",
    "for date in outlier_dates:\n",
    "    close_val = nepse_outliers.loc[date, 'Close']\n",
    "    local_mean = nepse_outliers['Close'].rolling(7, center=True, min_periods=1).mean().loc[date]\n",
    "    print(f\"{date.date()}: Close={close_val:.2f}, Local Mean={local_mean:.2f}\")\n",
    "\n",
    "# Compare methods\n",
    "print(\"\\nComparison of methods:\")\n",
    "print(f\"Z-score outliers: {outlier_mask_zscore.sum()}\")\n",
    "print(f\"IQR outliers: {outlier_mask_iqr.sum()}\")\n",
    "print(f\"Rolling outliers: {outlier_mask_rolling.sum()}\")\n",
    "\n",
    "# Output:\n",
    "# Rolling outlier detection:\n",
    "# Window size: 7 days\n",
    "# Threshold: 2.5 standard deviations\n",
    "# 2024-01-20: Close=3500.00, Local Mean=2900.33\n",
    "# 2024-01-27: Close=2100.00, Local Mean=2922.56\n",
    "# 2024-02-02: Close=5000.00, Local Mean=2916.75\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Rolling outlier detection** uses local statistics instead of global statistics.\n",
    "- This is crucial for **non-stationary time-series** where the mean changes over time.\n",
    "- **How it works**:\n",
    "  1. Calculate rolling mean and std with a window (e.g., 7 days)\n",
    "  2. For each point, check if it's within n_std of the local mean\n",
    "  3. Flag points that deviate significantly from their local neighborhood\n",
    "- **Why it's better for time-series**:\n",
    "  - A price of 2950 might be normal in a rising market but an outlier in a falling market\n",
    "  - Local detection adapts to trends and regime changes\n",
    "- **Parameters**:\n",
    "  - `window`: Size of local neighborhood (7 days captures weekly patterns)\n",
    "  - `center=True`: Use symmetric window (past and future)\n",
    "  - `n_std`: Threshold (2.5 is stricter than 3 for local detection)\n",
    "- **Important**: `center=True` uses future data for smoothing. For prediction, use `center=False` (only past data).\n",
    "\n",
    "---\n",
    "\n",
    "### **4.6.2 Treatment Strategies**\n",
    "\n",
    "Once outliers are detected, we need to decide how to handle them.\n",
    "\n",
    "```python\n",
    "# Strategy 1: Removal\n",
    "# Simply remove outlier rows\n",
    "\n",
    "df_removed = nepse_outliers.copy()\n",
    "\n",
    "# Remove rows where Close is an outlier (IQR method)\n",
    "outlier_mask, _, _ = detect_outliers_iqr(df_removed['Close'].values)\n",
    "df_removed_clean = df_removed[~outlier_mask]\n",
    "\n",
    "print(\"After removing outliers:\")\n",
    "print(f\"Original rows: {len(nepse_outliers)}\")\n",
    "print(f\"After removal: {len(df_removed_clean)}\")\n",
    "print(f\"Removed: {len(nepse_outliers) - len(df_removed_clean)} rows\")\n",
    "\n",
    "# Show remaining data\n",
    "print(\"\\nCleaned data:\")\n",
    "print(df_removed_clean.head())\n",
    "\n",
    "# When to remove:\n",
    "# - Clear data errors (typo: 5000 instead of 2900)\n",
    "# - Outliers that would distort analysis\n",
    "# - When you have enough data to spare\n",
    "\n",
    "# When NOT to remove:\n",
    "# - Genuine extreme events (market crashes, spikes)\n",
    "# - Small datasets where every point matters\n",
    "# - When outliers contain important information\n",
    "\n",
    "# Output:\n",
    "# After removing outliers:\n",
    "# Original rows: 20\n",
    "# After removal: 17\n",
    "# Removed: 3 rows\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Removal** is the simplest strategy: delete rows containing outliers.\n",
    "- **Pros**:\n",
    "  - Simple to implement\n",
    "  - No assumptions about replacement values\n",
    "  - Clean resulting dataset\n",
    "- **Cons**:\n",
    "  - Loses data (especially problematic for small datasets)\n",
    "  - Loses potentially valuable information\n",
    "  - May create gaps in time-series\n",
    "- **When to use removal**:\n",
    "  - Clear errors (typo: 5000 instead of 2900)\n",
    "  - Extreme values that are definitely wrong\n",
    "  - When you have abundant data\n",
    "- **When NOT to use removal**:\n",
    "  - Genuine extreme events (market crash, earnings surprise)\n",
    "  - Small datasets\n",
    "  - When the outlier might be informative\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Strategy 2: Capping (Winsorization)\n",
    "# Replace outliers with the nearest non-outlier value\n",
    "\n",
    "def winsorize(data, lower_percentile=5, upper_percentile=95):\n",
    "    \"\"\"\n",
    "    Winsorize data by capping extreme values.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : array-like\n",
    "        Data to winsorize\n",
    "    lower_percentile : float\n",
    "        Lower percentile for capping\n",
    "    upper_percentile : float\n",
    "        Upper percentile for capping\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    array\n",
    "        Winsorized data\n",
    "    \"\"\"\n",
    "    lower_limit = np.percentile(data, lower_percentile)\n",
    "    upper_limit = np.percentile(data, upper_percentile)\n",
    "    \n",
    "    winsorized = np.clip(data, lower_limit, upper_limit)\n",
    "    return winsorized\n",
    "\n",
    "df_capped = nepse_outliers.copy()\n",
    "\n",
    "# Winsorize Close prices\n",
    "df_capped['Close_Capped'] = winsorize(df_capped['Close'].values, 5, 95)\n",
    "\n",
    "# Compare original and capped\n",
    "print(\"Winsorization results:\")\n",
    "print(f\"Original range: {df_capped['Close'].min():.2f} to {df_capped['Close'].max():.2f}\")\n",
    "print(f\"Capped range: {df_capped['Close_Capped'].min():.2f} to {df_capped['Close_Capped'].max():.2f}\")\n",
    "\n",
    "# Show the transformation\n",
    "print(\"\\nComparison (showing outliers):\")\n",
    "for idx in [5, 12, 18]:  # Indices of outliers\n",
    "    original = df_capped['Close'].iloc[idx]\n",
    "    capped = df_capped['Close_Capped'].iloc[idx]\n",
    "    print(f\"Row {idx}: Original={original:.2f}, Capped={capped:.2f}\")\n",
    "\n",
    "# Output:\n",
    "# Winsorization results:\n",
    "# Original range: 2100.00 to 5000.00\n",
    "# Capped range: 2849.31 to 2980.38\n",
    "#\n",
    "# Comparison (showing outliers):\n",
    "# Row 5: Original=3500.00, Capped=2980.38\n",
    "# Row 12: Original=2100.00, Capped=2849.31\n",
    "# Row 18: Original=5000.00, Capped=2980.38\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Winsorization** (capping) replaces extreme values with percentile limits.\n",
    "- **How it works**:\n",
    "  1. Calculate the 5th and 95th percentiles\n",
    "  2. Values below 5th percentile → replaced with 5th percentile value\n",
    "  3. Values above 95th percentile → replaced with 95th percentile value\n",
    "- `np.clip(array, min, max)` limits values to the specified range.\n",
    "- **Advantages**:\n",
    "  - Preserves data structure (no missing values)\n",
    "  - Reduces impact of outliers without removing them\n",
    "  - Retains the direction of extreme values (high stays high, low stays low)\n",
    "- **When to use**:\n",
    "  - When outliers might contain useful signal\n",
    "  - When you don't want to lose data points\n",
    "  - For robust statistical analysis\n",
    "- **Limitations**:\n",
    "  - Still modifies data (may affect analysis)\n",
    "  - Choice of percentiles is arbitrary (5/95, 1/99, etc.)\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Strategy 3: Transformation\n",
    "# Apply mathematical transformation to reduce outlier impact\n",
    "\n",
    "df_transformed = nepse_outliers.copy()\n",
    "\n",
    "# Log transformation - reduces right skew\n",
    "df_transformed['Close_Log'] = np.log(df_transformed['Close'])\n",
    "\n",
    "# Compare distributions\n",
    "print(\"Log transformation:\")\n",
    "print(f\"Original: Mean={df_transformed['Close'].mean():.2f}, Std={df_transformed['Close'].std():.2f}\")\n",
    "print(f\"Log: Mean={df_transformed['Close_Log'].mean():.2f}, Std={df_transformed['Close_Log'].std():.2f}\")\n",
    "\n",
    "# The log transformation compresses the scale\n",
    "# A value of 5000 (outlier) becomes log(5000) = 8.52\n",
    "# A value of 2900 (normal) becomes log(2900) = 7.97\n",
    "# The difference shrinks from 2100 to 0.55\n",
    "\n",
    "# Square root transformation - less aggressive than log\n",
    "df_transformed['Close_Sqrt'] = np.sqrt(df_transformed['Close'])\n",
    "\n",
    "# Box-Cox transformation - parameterized power transformation\n",
    "from scipy import stats\n",
    "\n",
    "# Box-Cox requires positive values\n",
    "close_positive = df_transformed['Close'].values\n",
    "close_boxcox, lambda_param = stats.boxcox(close_positive)\n",
    "df_transformed['Close_BoxCox'] = close_boxcox\n",
    "\n",
    "print(f\"\\nBox-Cox lambda: {lambda_param:.4f}\")\n",
    "# lambda close to 0 suggests log transformation is appropriate\n",
    "\n",
    "# For prediction models, transformations are useful because:\n",
    "# 1. They normalize the distribution\n",
    "# 2. They stabilize variance\n",
    "# 3. They reduce outlier influence\n",
    "\n",
    "# Remember to inverse transform predictions!\n",
    "\n",
    "# Output:\n",
    "# Log transformation:\n",
    "# Original: Mean=2941.09, Std=636.84\n",
    "# Log: Mean=7.97, Std=0.12\n",
    "#\n",
    "# Box-Cox lambda: -0.0234\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Transformations** change the scale of data to reduce outlier impact and normalize distributions.\n",
    "- **Log transformation**:\n",
    "  - Compresses large values more than small values\n",
    "  - Converts multiplicative relationships to additive\n",
    "  - Formula: \\( x_{transformed} = \\log(x) \\)\n",
    "  - Effect: 5000 → 8.52, 2900 → 7.97 (difference: 2100 → 0.55)\n",
    "- **Square root transformation**:\n",
    "  - Less aggressive than log\n",
    "  - Good for count data\n",
    "  - Formula: \\( x_{transformed} = \\sqrt{x} \\)\n",
    "- **Box-Cox transformation**:\n",
    "  - Generalizes log and square root\n",
    "  - Automatically finds optimal transformation parameter (λ)\n",
    "  - λ = 0: equivalent to log\n",
    "  - λ = 0.5: equivalent to square root\n",
    "  - λ = 1: no transformation\n",
    "- **For prediction models**:\n",
    "  - Use transformation when data is skewed or has outliers\n",
    "  - **Important**: Inverse transform predictions to get actual values\n",
    "  - Example: If you predict log(price), apply exp() to get actual price\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Strategy 4: Imputation (replace with estimated value)\n",
    "\n",
    "df_imputed_outliers = nepse_outliers.copy()\n",
    "\n",
    "# Detect outliers\n",
    "outlier_mask, lower, upper = detect_outliers_iqr(df_imputed_outliers['Close'].values)\n",
    "\n",
    "# Replace outliers with NaN\n",
    "df_imputed_outliers.loc[outlier_mask, 'Close'] = np.nan\n",
    "\n",
    "# Now apply imputation methods\n",
    "# Option 1: Fill with median\n",
    "df_imputed_outliers['Close_Median'] = df_imputed_outliers['Close'].fillna(\n",
    "    df_imputed_outliers['Close'].median()\n",
    ")\n",
    "\n",
    "# Option 2: Interpolation\n",
    "df_imputed_outliers['Close_Interp'] = df_imputed_outliers['Close'].interpolate(method='linear')\n",
    "\n",
    "# Option 3: Rolling mean\n",
    "rolling_mean = df_imputed_outliers['Close'].rolling(window=5, center=True, min_periods=1).mean()\n",
    "df_imputed_outliers['Close_Rolling'] = df_imputed_outliers['Close'].fillna(rolling_mean)\n",
    "\n",
    "# Compare methods\n",
    "print(\"Outlier imputation comparison:\")\n",
    "print(\"\\nOriginal vs Imputed for outlier rows:\")\n",
    "for idx in [5, 12, 18]:\n",
    "    print(f\"\\nRow {idx}:\")\n",
    "    print(f\"  Original: {nepse_outliers['Close'].iloc[idx]:.2f}\")\n",
    "    print(f\"  Median fill: {df_imputed_outliers['Close_Median'].iloc[idx]:.2f}\")\n",
    "    print(f\"  Interpolation: {df_imputed_outliers['Close_Interp'].iloc[idx]:.2f}\")\n",
    "    print(f\"  Rolling mean: {df_imputed_outliers['Close_Rolling'].iloc[idx]:.2f}\")\n",
    "\n",
    "# Output:\n",
    "# Outlier imputation comparison:\n",
    "#\n",
    "# Original vs Imputed for outlier rows:\n",
    "#\n",
    "# Row 5:\n",
    "#   Original: 3500.00\n",
    "#   Median fill: 2911.39\n",
    "#   Interpolation: 2917.45\n",
    "#   Rolling mean: 2919.81\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Outlier imputation** treats outliers as missing values and fills them with estimates.\n",
    "- **Process**:\n",
    "  1. Detect outliers using chosen method (IQR, Z-score, etc.)\n",
    "  2. Replace outlier values with NaN\n",
    "  3. Apply imputation method (median, interpolation, rolling mean)\n",
    "- **Median fill**: Replaces with the median of non-outlier values. Robust to remaining outliers.\n",
    "- **Interpolation**: Estimates from nearby values. Respects local trends.\n",
    "- **Rolling mean**: Uses average of neighboring values. Good for time-series.\n",
    "- **Advantages**:\n",
    "  - Preserves data structure (no missing rows)\n",
    "  - Uses local information\n",
    "  - Can be more accurate than simple capping\n",
    "- **Limitations**:\n",
    "  - More complex than removal or capping\n",
    "  - Requires choice of imputation method\n",
    "  - May introduce bias if outliers are informative\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Strategy 5: Flagging and Retaining\n",
    "# Keep outliers but mark them for special handling\n",
    "\n",
    "df_flagged = nepse_outliers.copy()\n",
    "\n",
    "# Create outlier flag columns\n",
    "outlier_mask_close, _, _ = detect_outliers_iqr(df_flagged['Close'].values)\n",
    "outlier_mask_vol, _, _ = detect_outliers_iqr(df_flagged['Volume'].values)\n",
    "\n",
    "df_flagged['Close_Outlier'] = outlier_mask_close\n",
    "df_flagged['Volume_Outlier'] = outlier_mask_vol\n",
    "\n",
    "# Create combined outlier flag\n",
    "df_flagged['Is_Outlier'] = outlier_mask_close | outlier_mask_vol\n",
    "\n",
    "print(\"Data with outlier flags:\")\n",
    "print(df_flagged)\n",
    "\n",
    "# Summary\n",
    "print(f\"\\nOutlier summary:\")\n",
    "print(f\"Close outliers: {outlier_mask_close.sum()}\")\n",
    "print(f\"Volume outliers: {outlier_mask_vol.sum()}\")\n",
    "print(f\"Total rows with outliers: {df_flagged['Is_Outlier'].sum()}\")\n",
    "\n",
    "# Use flags in analysis\n",
    "normal_data = df_flagged[~df_flagged['Is_Outlier']]\n",
    "outlier_data = df_flagged[df_flagged['Is_Outlier']]\n",
    "\n",
    "print(f\"\\nNormal data statistics:\")\n",
    "print(f\"Close mean: {normal_data['Close'].mean():.2f}\")\n",
    "\n",
    "print(f\"\\nOutlier data:\")\n",
    "print(outlier_data[['Close', 'Volume']])\n",
    "\n",
    "# Output:\n",
    "# Data with outlier flags:\n",
    "#                Close    Volume  Close_Outlier  Volume_Outlier  Is_Outlier\n",
    "# Date                                                                     \n",
    "# 2024-01-15  2852.48  165281.0           False           False       False\n",
    "# ...\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Flagging** keeps outliers but marks them for special treatment.\n",
    "- **Advantages**:\n",
    "  - Preserves all data (no information loss)\n",
    "  - Allows flexible handling in different contexts\n",
    "  - Enables analysis of outlier patterns\n",
    "  - Models can use flags as features\n",
    "- **Implementation**:\n",
    "  - Create boolean columns indicating outlier status\n",
    "  - Can flag each column separately or combine them\n",
    "  - Use flags to filter or weight data in analysis\n",
    "- **Use cases**:\n",
    "  - When outliers might be genuine events\n",
    "  - When you want models to learn from outliers\n",
    "  - When different analyses need different outlier handling\n",
    "- **For prediction systems**:\n",
    "  - Train models on flagged data\n",
    "  - Models might learn that flagged rows behave differently\n",
    "  - Can use flags as features (outlier → higher uncertainty)\n",
    "\n",
    "---\n",
    "\n",
    "## **4.7 Data Aggregation and Resampling**\n",
    "\n",
    "Time-series data often needs to be aggregated from one frequency to another (e.g., daily to weekly) or resampled to a different time grid.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample NEPSE tick data (intraday trades)\n",
    "np.random.seed(42)\n",
    "timestamps = pd.date_range('2024-01-15 09:00:00', periods=100, freq='5min')\n",
    "\n",
    "tick_data = pd.DataFrame({\n",
    "    'Timestamp': timestamps,\n",
    "    'Price': 2850 + np.cumsum(np.random.randn(100) * 2),  # Random walk\n",
    "    'Volume': np.random.randint(100, 1000, 100),\n",
    "    'Symbol': 'NABIL'\n",
    "})\n",
    "\n",
    "tick_data.set_index('Timestamp', inplace=True)\n",
    "print(\"Tick data (5-minute intervals):\")\n",
    "print(tick_data.head(15))\n",
    "\n",
    "# Output:\n",
    "# Tick data (5-minute intervals):\n",
    "#                        Price  Volume Symbol\n",
    "# Timestamp                               \n",
    "# 2024-01-15 09:00:00  2851.99     451  NABIL\n",
    "# 2024-01-15 09:05:00  2848.33     828  NABIL\n",
    "# 2024-01-15 09:10:00  2847.16     771  NABIL\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Tick data** records every individual trade or quote. It's the most granular form of market data.\n",
    "- For NEPSE, tick data might include every trade with timestamp, price, and volume.\n",
    "- This sample simulates 5-minute trade snapshots with random price movements.\n",
    "- Aggregating tick data is essential for analysis - you can't work with millions of individual ticks.\n",
    "- Common aggregations: 1-minute → 5-minute → 1-hour → daily → weekly.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Resampling to different frequencies\n",
    "\n",
    "# Resample to 15-minute intervals\n",
    "df_15min = tick_data.resample('15min').agg({\n",
    "    'Price': 'ohlc',  # Open, High, Low, Close\n",
    "    'Volume': 'sum'\n",
    "})\n",
    "\n",
    "print(\"15-minute OHLC data:\")\n",
    "print(df_15min.head())\n",
    "\n",
    "# The ohlc aggregation creates a MultiIndex column\n",
    "# Let's flatten it\n",
    "df_15min.columns = ['_'.join(col).strip() if isinstance(col, tuple) else col \n",
    "                     for col in df_15min.columns.values]\n",
    "print(\"\\nFlattened columns:\")\n",
    "print(df_15min.head())\n",
    "\n",
    "# Resample to hourly\n",
    "df_hourly = tick_data.resample('H').agg({\n",
    "    'Price': ['first', 'max', 'min', 'last'],\n",
    "    'Volume': 'sum'\n",
    "})\n",
    "\n",
    "# Rename columns for clarity\n",
    "df_hourly.columns = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "print(\"\\nHourly OHLCV data:\")\n",
    "print(df_hourly.head())\n",
    "\n",
    "# Output:\n",
    "# 15-minute OHLC data:\n",
    "#                    Price                                    Volume\n",
    "#                     open     high      low    close              sum\n",
    "# Timestamp                                                         \n",
    "# 2024-01-15 09:00:00  2851.99  2855.04  2847.16  2848.62           2639\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `resample()` groups data by time intervals. The frequency string specifies the interval:\n",
    "  - `'min'` or `'T'`: minutes\n",
    "  - `'H'`: hours\n",
    "  - `'D'`: days\n",
    "  - `'W'`: weeks\n",
    "  - `'M'`: month end\n",
    "  - `'MS'`: month start\n",
    "  - `'Q'`: quarter end\n",
    "- **OHLC aggregation**:\n",
    "  - **Open**: First price in the period\n",
    "  - **High**: Maximum price in the period\n",
    "  - **Low**: Minimum price in the period\n",
    "  - **Close**: Last price in the period\n",
    "- `agg()` specifies different aggregation methods for each column.\n",
    "- The result has MultiIndex columns when using 'ohlc' or multiple aggregations.\n",
    "- Flattening columns makes the DataFrame easier to work with.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Resampling daily NEPSE data to weekly and monthly\n",
    "\n",
    "# Create sample daily data\n",
    "dates = pd.date_range('2024-01-01', periods=90, freq='D')\n",
    "np.random.seed(42)\n",
    "\n",
    "daily_nepse = pd.DataFrame({\n",
    "    'Open': 2850 + np.cumsum(np.random.randn(90) * 5),\n",
    "    'High': 2870 + np.cumsum(np.random.randn(90) * 5),\n",
    "    'Low': 2830 + np.cumsum(np.random.randn(90) * 5),\n",
    "    'Close': 2860 + np.cumsum(np.random.randn(90) * 5),\n",
    "    'Volume': np.random.randint(100000, 200000, 90)\n",
    "}, index=dates)\n",
    "\n",
    "# Ensure High > Low and High >= Open, Close\n",
    "daily_nepse['High'] = daily_nepse[['Open', 'High', 'Low', 'Close']].max(axis=1)\n",
    "daily_nepse['Low'] = daily_nepse[['Open', 'High', 'Low', 'Close']].min(axis=1)\n",
    "\n",
    "print(\"Daily NEPSE data:\")\n",
    "print(daily_nepse.head())\n",
    "\n",
    "# Resample to weekly (Sunday to Saturday)\n",
    "weekly = daily_nepse.resample('W').agg({\n",
    "    'Open': 'first',    # Week's opening price\n",
    "    'High': 'max',      # Week's highest price\n",
    "    'Low': 'min',       # Week's lowest price\n",
    "    'Close': 'last',    # Week's closing price\n",
    "    'Volume': 'sum'     # Total weekly volume\n",
    "})\n",
    "\n",
    "print(\"\\nWeekly data:\")\n",
    "print(weekly.head())\n",
    "\n",
    "# Resample to monthly\n",
    "monthly = daily_nepse.resample('M').agg({\n",
    "    'Open': 'first',\n",
    "    'High': 'max',\n",
    "    'Low': 'min',\n",
    "    'Close': 'last',\n",
    "    'Volume': 'sum'\n",
    "})\n",
    "\n",
    "print(\"\\nMonthly data:\")\n",
    "print(monthly)\n",
    "\n",
    "# Output:\n",
    "# Daily NEPSE data:\n",
    "#                 Open       High        Low      Close   Volume\n",
    "# 2024-01-01  2857.45   2875.58   2857.45   2866.98   170160\n",
    "# 2024-01-02  2864.69   2875.58   2864.69   2868.33   189527\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Weekly resampling** (`'W'`):\n",
    "  - By default, weeks end on Sunday\n",
    "  - `'W-MON'`, `'W-TUE'`, etc., for different week endings\n",
    "  - Open = first price of the week, Close = last price of the week\n",
    "  - Volume is summed across all days in the week\n",
    "- **Monthly resampling** (`'M'`):\n",
    "  - Groups data by calendar month\n",
    "  - End-of-month is default; use `'MS'` for start-of-month\n",
    "  - Same aggregation logic: first Open, last Close, max High, min Low\n",
    "- **Why aggregate**:\n",
    "  - Reduce noise (daily data has more noise than weekly)\n",
    "  - Identify longer-term trends\n",
    "  - Reduce computational load\n",
    "  - Match prediction horizon (if predicting weekly, use weekly data)\n",
    "- **Important**: Resampling aggregates information - you lose intra-period details.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Upsampling - increasing frequency\n",
    "# Useful when you need to align data at a higher frequency\n",
    "\n",
    "# Create daily data\n",
    "daily_close = pd.Series(\n",
    "    [2850, 2875, 2890, 2865, 2880],\n",
    "    index=pd.date_range('2024-01-15', periods=5, freq='D'),\n",
    "    name='Close'\n",
    ")\n",
    "\n",
    "print(\"Original daily data:\")\n",
    "print(daily_close)\n",
    "\n",
    "# Upsample to hourly\n",
    "hourly_ffill = daily_close.resample('H').ffill()\n",
    "print(\"\\nUpsampled to hourly (forward fill):\")\n",
    "print(hourly_ffill.head(30))\n",
    "\n",
    "# The daily value is repeated for each hour of the day\n",
    "# This is useful for aligning with intraday data\n",
    "\n",
    "# Upsample with interpolation\n",
    "hourly_interp = daily_close.resample('H').interpolate(method='linear')\n",
    "print(\"\\nUpsampled to hourly (linear interpolation):\")\n",
    "print(hourly_interp.head(30))\n",
    "\n",
    "# With interpolation, values change gradually between known points\n",
    "# This is more realistic for continuously changing quantities\n",
    "\n",
    "# Output:\n",
    "# Original daily data:\n",
    "# 2024-01-15    2850\n",
    "# 2024-01-16    2875\n",
    "# 2024-01-17    2890\n",
    "# 2024-01-18    2865\n",
    "# 2024-01-19    2880\n",
    "# Freq: D, Name: Close, dtype: int64\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Upsampling** increases frequency (daily → hourly). This requires filling in new values.\n",
    "- **Forward fill** (`ffill()`):\n",
    "  - Propagates the last known value forward\n",
    "  - Daily price is used for all hours of that day\n",
    "  - Appropriate when the value is constant until the next update\n",
    "- **Interpolation**:\n",
    "  - Values change gradually between known points\n",
    "  - Linear interpolation draws a straight line between points\n",
    "  - More appropriate for continuously varying quantities\n",
    "- **Use cases for upsampling**:\n",
    "  - Aligning data at different frequencies for analysis\n",
    "  - Creating features at higher frequency\n",
    "  - Filling gaps in time-series\n",
    "- **Caution**: Upsampling doesn't create new information. It just fills in estimated values.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Grouping and aggregating by time components\n",
    "\n",
    "# Create sample data spanning multiple months\n",
    "dates = pd.date_range('2024-01-01', periods=180, freq='D')\n",
    "np.random.seed(42)\n",
    "\n",
    "nepse_multi = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Close': 2800 + np.cumsum(np.random.randn(180) * 3),\n",
    "    'Volume': np.random.randint(100000, 200000, 180)\n",
    "})\n",
    "\n",
    "nepse_multi['Date'] = pd.to_datetime(nepse_multi['Date'])\n",
    "nepse_multi.set_index('Date', inplace=True)\n",
    "\n",
    "# Extract time components\n",
    "nepse_multi['Year'] = nepse_multi.index.year\n",
    "nepse_multi['Month'] = nepse_multi.index.month\n",
    "nepse_multi['Day'] = nepse_multi.index.day\n",
    "nepse_multi['DayOfWeek'] = nepse_multi.index.dayofweek\n",
    "nepse_multi['DayName'] = nepse_multi.index.day_name()\n",
    "nepse_multi['WeekOfYear'] = nepse_multi.index.isocalendar().week\n",
    "nepse_multi['Quarter'] = nepse_multi.index.quarter\n",
    "\n",
    "print(\"Data with time components:\")\n",
    "print(nepse_multi.head())\n",
    "\n",
    "# Aggregate by day of week\n",
    "print(\"\\nAverage by day of week:\")\n",
    "dow_stats = nepse_multi.groupby('DayName').agg({\n",
    "    'Close': 'mean',\n",
    "    'Volume': 'mean'\n",
    "}).round(2)\n",
    "print(dow_stats)\n",
    "\n",
    "# Aggregate by month\n",
    "print(\"\\nMonthly statistics:\")\n",
    "monthly_stats = nepse_multi.groupby('Month').agg({\n",
    "    'Close': ['mean', 'std', 'min', 'max'],\n",
    "    'Volume': ['mean', 'sum']\n",
    "}).round(2)\n",
    "print(monthly_stats)\n",
    "\n",
    "# Output:\n",
    "# Data with time components:\n",
    "#                Close   Volume  Year  Month  Day  DayOfWeek   DayName  WeekOfYear  Quarter\n",
    "# Date                                                                                         \n",
    "# 2024-01-01  2804.97  152102  2024      1    1          0    Monday           1        1\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Time components** can be extracted from DatetimeIndex:\n",
    "  - `.year`, `.month`, `.day`: Calendar components\n",
    "  - `.dayofweek`: 0 (Monday) to 6 (Sunday)\n",
    "  - `.day_name()`: Day name as string\n",
    "  - `.isocalendar().week`: ISO week number\n",
    "  - `.quarter`: Quarter (1-4)\n",
    "- **Grouping by time components** reveals patterns:\n",
    "  - Day-of-week patterns: Volume might be higher on certain days\n",
    "  - Monthly patterns: Seasonality in prices or volume\n",
    "  - Quarter patterns: Quarterly earnings effects\n",
    "- **Applications**:\n",
    "  - Feature engineering: Create time-based features for models\n",
    "  - Analysis: Identify seasonal patterns\n",
    "  - Anomaly detection: Compare same period across years\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Advanced: Rolling and Expanding Windows\n",
    "\n",
    "df_windows = daily_nepse.copy()\n",
    "\n",
    "# Rolling window - fixed size moving window\n",
    "df_windows['MA_7'] = df_windows['Close'].rolling(window=7).mean()  # 7-day moving average\n",
    "df_windows['MA_30'] = df_windows['Close'].rolling(window=30).mean()  # 30-day moving average\n",
    "df_windows['Volatility_7'] = df_windows['Close'].rolling(window=7).std()  # 7-day volatility\n",
    "\n",
    "print(\"Rolling window calculations:\")\n",
    "print(df_windows[['Close', 'MA_7', 'MA_30', 'Volatility_7']].head(35))\n",
    "\n",
    "# Expanding window - cumulative from start\n",
    "df_windows['Cumulative_Return'] = (df_windows['Close'] / df_windows['Close'].iloc[0] - 1) * 100\n",
    "df_windows['Expanding_Mean'] = df_windows['Close'].expanding().mean()\n",
    "df_windows['Expanding_Std'] = df_windows['Close'].expanding().std()\n",
    "\n",
    "print(\"\\nExpanding window calculations:\")\n",
    "print(df_windows[['Close', 'Cumulative_Return', 'Expanding_Mean', 'Expanding_Std']].head(10))\n",
    "\n",
    "# Rolling window with custom aggregation\n",
    "def rolling_max_drawdown(prices):\n",
    "    \"\"\"Calculate maximum drawdown in a window.\"\"\"\n",
    "    cummax = prices.cummax()\n",
    "    drawdown = (prices - cummax) / cummax\n",
    "    return drawdown.min()\n",
    "\n",
    "df_windows['Max_DD_30'] = df_windows['Close'].rolling(window=30).apply(rolling_max_drawdown)\n",
    "\n",
    "print(\"\\nRolling max drawdown:\")\n",
    "print(df_windows[['Close', 'Max_DD_30']].tail())\n",
    "\n",
    "# Output:\n",
    "# Rolling window calculations:\n",
    "#                 Close       MA_7      MA_30  Volatility_7\n",
    "# 2024-01-01  2866.98        NaN        NaN           NaN\n",
    "# 2024-01-02  2868.33        NaN        NaN           NaN\n",
    "# 2024-01-03  2870.32        NaN        NaN           NaN\n",
    "# 2024-01-04  2874.19        NaN        NaN           NaN\n",
    "# 2024-01-05  2872.55        NaN        NaN           NaN\n",
    "# 2024-01-06  2871.75        NaN        NaN           NaN\n",
    "# 2024-01-07  2865.92  2869.72        NaN      2.32\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Rolling window** calculations use a fixed-size window that moves through the data.\n",
    "  - `rolling(window=7)` creates windows of 7 consecutive values\n",
    "  - Each position calculates statistics from its window\n",
    "  - First 6 rows are NaN (not enough data for 7-day window)\n",
    "- **Common rolling calculations**:\n",
    "  - Moving average: Smooths noise, shows trends\n",
    "  - Rolling std: Measures local volatility\n",
    "  - Rolling min/max: Support/resistance levels\n",
    "- **Expanding window** starts from the beginning and grows:\n",
    "  - `expanding().mean()`: Cumulative average of all previous values\n",
    "  - Useful for benchmarking against historical performance\n",
    "- **Custom functions** can be applied with `.apply()`:\n",
    "  - Max drawdown: Largest peak-to-trough decline\n",
    "  - Any function that takes a Series and returns a scalar\n",
    "- **Applications in prediction**:\n",
    "  - Rolling features capture recent behavior\n",
    "  - Moving averages are common features in trading models\n",
    "  - Rolling volatility indicates risk level\n",
    "\n",
    "---\n",
    "\n",
    "## **4.8 Time-Series Indexing and Selection**\n",
    "\n",
    "Efficient data selection is crucial for time-series analysis. pandas provides powerful indexing capabilities for datetime data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample NEPSE data for multiple symbols over time\n",
    "dates = pd.date_range('2024-01-01', periods=60, freq='D')\n",
    "symbols = ['NABIL', 'NICA', 'SCBL', 'ADBL']\n",
    "\n",
    "# Create MultiIndex DataFrame\n",
    "index = pd.MultiIndex.from_product([dates, symbols], names=['Date', 'Symbol'])\n",
    "np.random.seed(42)\n",
    "\n",
    "nepse_multi = pd.DataFrame({\n",
    "    'Open': 1000 + np.random.randn(240).cumsum() * 10,\n",
    "    'High': 1020 + np.random.randn(240).cumsum() * 10,\n",
    "    'Low': 980 + np.random.randn(240).cumsum() * 10,\n",
    "    'Close': 1010 + np.random.randn(240).cumsum() * 10,\n",
    "    'Volume': np.random.randint(50000, 200000, 240)\n",
    "}, index=index)\n",
    "\n",
    "print(\"MultiIndex NEPSE data:\")\n",
    "print(nepse_multi.head(10))\n",
    "\n",
    "# Output:\n",
    "# MultiIndex NEPSE data:\n",
    "#                           Open        High         Low       Close  Volume\n",
    "# Date       Symbol                                                         \n",
    "# 2024-01-01 NABIL    1014.97   1018.45    1010.12   1012.56  145951\n",
    "#            NICA     1023.41   1026.78    1019.23   1020.89  139724\n",
    "#            SCBL     1031.02   1035.45    1027.67   1029.34  186291\n",
    "#            ADBL     1038.12   1042.56    1034.89   1036.78  152102\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **MultiIndex** allows hierarchical indexing with multiple levels (Date and Symbol here).\n",
    "- `pd.MultiIndex.from_product()` creates all combinations of the input arrays.\n",
    "- This structure is ideal for panel data: multiple entities (symbols) over time.\n",
    "- Each row is identified by a (Date, Symbol) tuple.\n",
    "- This format is common in financial databases where you track multiple securities.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Selecting data from MultiIndex DataFrame\n",
    "\n",
    "# Select all data for a specific date\n",
    "print(\"Data for 2024-01-15:\")\n",
    "print(nepse_multi.loc['2024-01-15'])\n",
    "\n",
    "# Select all data for a specific symbol\n",
    "print(\"\\nAll NABIL data:\")\n",
    "print(nepse_multi.xs('NABIL', level='Symbol').head())\n",
    "\n",
    "# xs() is \"cross-section\" - selects data at a specific index level\n",
    "\n",
    "# Select specific date and symbol\n",
    "print(\"\\nNABIL on 2024-01-15:\")\n",
    "print(nepse_multi.loc[('2024-01-15', 'NABIL')])\n",
    "\n",
    "# Select date range for a specific symbol\n",
    "print(\"\\nNABIL for Jan 15-20:\")\n",
    "print(nepse_multi.xs('NABIL', level='Symbol').loc['2024-01-15':'2024-01-20'])\n",
    "\n",
    "# Select using slice for multiple levels\n",
    "print(\"\\nAll symbols for Jan 15-20:\")\n",
    "print(nepse_multi.loc[pd.IndexSlice['2024-01-15':'2024-01-20', :], :])\n",
    "\n",
    "# IndexSlice creates proper slice objects for MultiIndex\n",
    "\n",
    "# Output:\n",
    "# Data for 2024-01-15:\n",
    "#                  Open      High       Low     Close  Volume\n",
    "# Symbol                                                    \n",
    "# NABIL    1165.23  1169.45  1161.78  1163.92  187234\n",
    "# NICA     1172.89  1176.34  1169.12  1171.56  143521\n",
    "# SCBL     1179.45  1183.67  1176.23  1178.34  165892\n",
    "# ADBL     1185.67  1190.12  1182.45  1184.78  152345\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `.loc[]` with MultiIndex requires tuples for multi-level selection.\n",
    "- `.xs()` (cross-section) selects at a specific level without needing tuples.\n",
    "  - `level='Symbol'` specifies which index level to slice on.\n",
    "  - Drops that level from the result (single-level index remains).\n",
    "- **Date range selection**:\n",
    "  - First use `xs()` to get one symbol, then slice by date\n",
    "  - Or use `pd.IndexSlice` for more complex selections\n",
    "- `pd.IndexSlice` creates proper slice objects for MultiIndex:\n",
    "  - `IndexSlice['2024-01-15':'2024-01-20', :]` means date range, all symbols\n",
    "  - More readable than constructing tuples manually\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Boolean indexing with time conditions\n",
    "\n",
    "# Reset index for easier filtering\n",
    "df = nepse_multi.reset_index()\n",
    "\n",
    "# Filter by date range\n",
    "jan_data = df[(df['Date'] >= '2024-01-01') & (df['Date'] <= '2024-01-31')]\n",
    "print(\"January 2024 data:\")\n",
    "print(f\"Shape: {jan_data.shape}\")\n",
    "\n",
    "# Filter by specific dates\n",
    "specific_dates = df[df['Date'].isin(pd.date_range('2024-01-15', periods=5))]\n",
    "print(\"\\nSpecific 5 days:\")\n",
    "print(specific_dates.head())\n",
    "\n",
    "# Filter by day of week\n",
    "df['DayOfWeek'] = df['Date'].dt.dayofweek\n",
    "mondays = df[df['DayOfWeek'] == 0]\n",
    "print(f\"\\nMondays only: {len(mondays)} rows\")\n",
    "\n",
    "# Filter by month\n",
    "df['Month'] = df['Date'].dt.month\n",
    "jan_feb = df[df['Month'].isin([1, 2])]\n",
    "print(f\"\\nJanuary and February: {len(jan_feb)} rows\")\n",
    "\n",
    "# Complex filter: High volume days for NABIL\n",
    "high_vol_nabil = df[(df['Symbol'] == 'NABIL') & (df['Volume'] > df['Volume'].quantile(0.8))]\n",
    "print(f\"\\nHigh volume NABIL days: {len(high_vol_nabil)} rows\")\n",
    "\n",
    "# Output:\n",
    "# January 2024 data:\n",
    "# Shape: (124, 8)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- After `reset_index()`, Date and Symbol become regular columns, enabling easier filtering.\n",
    "- **Date comparisons** use standard comparison operators:\n",
    "  - `df['Date'] >= '2024-01-01'`: Date comparison works with strings\n",
    "  - Combine with `&` (AND) for ranges\n",
    "- **`.isin()`** checks membership in a list:\n",
    "  - `df['Date'].isin(dates)`: Date in the given list\n",
    "  - Useful for selecting specific dates or months\n",
    "- **Time component extraction** with `.dt` accessor:\n",
    "  - `.dt.dayofweek`: Day of week (0=Monday)\n",
    "  - `.dt.month`: Month number\n",
    "  - `.dt.day`: Day of month\n",
    "  - `.dt.year`: Year\n",
    "- **Complex filters** combine multiple conditions with `&` and `|`.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Time-based filtering with query()\n",
    "\n",
    "# Query provides SQL-like syntax\n",
    "print(\"High volume days:\")\n",
    "high_vol = df.query('Volume > 150000')\n",
    "print(f\"Count: {len(high_vol)}\")\n",
    "\n",
    "# Query with multiple conditions\n",
    "print(\"\\nNABIL high volume days:\")\n",
    "result = df.query('Symbol == \"NABIL\" and Volume > 150000')\n",
    "print(f\"Count: {len(result)}\")\n",
    "\n",
    "# Query with date conditions\n",
    "print(\"\\nJanuary NABIL data:\")\n",
    "jan_nabil = df.query('Date >= \"2024-01-01\" and Date <= \"2024-01-31\" and Symbol == \"NABIL\"')\n",
    "print(f\"Count: {len(jan_nabil)}\")\n",
    "\n",
    "# Query with external variables\n",
    "min_volume = 100000\n",
    "print(f\"\\nVolume > {min_volume}:\")\n",
    "result = df.query('Volume > @min_volume')  # @ refers to external variable\n",
    "print(f\"Count: {len(result)}\")\n",
    "\n",
    "# Output:\n",
    "# High volume days:\n",
    "# Count: 143\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `.query()` provides a SQL-like syntax for filtering, often more readable than boolean indexing.\n",
    "- **Syntax**:\n",
    "  - Column names are used directly (no quotes needed)\n",
    "  - String values must be quoted: `Symbol == \"NABIL\"`\n",
    "  - Conditions combined with `and`, `or`\n",
    "- **External variables** use `@` prefix:\n",
    "  - `@min_volume` refers to the variable `min_volume`\n",
    "  - Useful for parameterized queries\n",
    "- **Advantages of query()**:\n",
    "  - More readable for complex conditions\n",
    "  - Can be slightly faster for large DataFrames\n",
    "  - Syntax familiar to SQL users\n",
    "- **Limitations**:\n",
    "  - Cannot use all Python expressions\n",
    "  - Column names with special characters need backticks\n",
    "\n",
    "---\n",
    "\n",
    "## **4.9 Basic Exploratory Data Analysis**\n",
    "\n",
    "Exploratory Data Analysis (EDA) is the process of understanding your data through statistics and visualizations before building models.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare NEPSE data\n",
    "# Create sample data for demonstration\n",
    "dates = pd.date_range('2023-01-01', periods=252, freq='B')  # 252 business days\n",
    "np.random.seed(42)\n",
    "\n",
    "# Simulate stock prices with realistic patterns\n",
    "base_price = 2800\n",
    "trend = np.linspace(0, 200, 252)  # Upward trend\n",
    "seasonality = 50 * np.sin(np.linspace(0, 4*np.pi, 252))  # Cyclical pattern\n",
    "noise = np.random.randn(252) * 30  # Random noise\n",
    "\n",
    "close_prices = base_price + trend + seasonality + noise\n",
    "volumes = np.random.randint(80000, 200000, 252)\n",
    "\n",
    "nepse_eda = pd.DataFrame({\n",
    "    'Date': dates,\n",
    "    'Open': close_prices + np.random.randn(252) * 10,\n",
    "    'High': close_prices + np.abs(np.random.randn(252)) * 20,\n",
    "    'Low': close_prices - np.abs(np.random.randn(252)) * 20,\n",
    "    'Close': close_prices,\n",
    "    'Volume': volumes\n",
    "})\n",
    "nepse_eda.set_index('Date', inplace=True)\n",
    "\n",
    "print(\"NEPSE Sample Data:\")\n",
    "print(nepse_eda.head())\n",
    "print(f\"\\nShape: {nepse_eda.shape}\")\n",
    "\n",
    "# Output:\n",
    "# NEPSE Sample Sample Data:\n",
    "#                 Open       High        Low      Close  Volume\n",
    "# Date                                                         \n",
    "# 2023-01-02  2802.45   2825.67   2785.34   2813.45  145231\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Sample data generation** creates realistic patterns:\n",
    "  - Base price: Starting level\n",
    "  - Trend: Gradual increase over time\n",
    "  - Seasonality: Cyclical patterns (sinusoidal)\n",
    "  - Noise: Random day-to-day variation\n",
    "- 252 business days ≈ 1 year of trading data.\n",
    "- High is always above Close, Low is always below Close (realistic).\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Statistical Summary\n",
    "\n",
    "print(\"Statistical Summary:\")\n",
    "print(nepse_eda.describe())\n",
    "\n",
    "# Additional statistics\n",
    "print(\"\\nAdditional Statistics:\")\n",
    "print(f\"Skewness:\")\n",
    "print(nepse_eda.skew())\n",
    "print(f\"\\nKurtosis:\")\n",
    "print(nepse_eda.kurtosis())\n",
    "\n",
    "# Output:\n",
    "# Statistical Summary:\n",
    "#               Open         High          Low        Close        Volume\n",
    "# count   252.000000   252.000000   252.000000   252.000000     252.000000\n",
    "# mean   2905.123456  2925.234567  2885.123456  2900.456789  140234.567890\n",
    "# std      52.345678    55.234567    50.123456    51.234567   35012.345678\n",
    "# min    2780.234567  2800.345678  2760.123456  2775.567890   81234.000000\n",
    "# 25%    2860.456789  2880.567890  2840.345678  2855.678901  112345.000000\n",
    "# 50%    2900.123456  2920.234567  2880.123456  2895.456789  140123.000000\n",
    "# 75%    2950.345678  2970.456789  2930.234567  2945.567890  167890.000000\n",
    "# max    3020.567890  3040.678901  3000.456789  3015.789012  199876.000000\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- `.describe()` provides key statistics:\n",
    "  - `count`: Number of non-null values\n",
    "  - `mean`: Average value\n",
    "  - `std`: Standard deviation (measure of spread)\n",
    "  - `min`, `max`: Range of values\n",
    "  - `25%`, `50%`, `75%`: Quartiles (50% is median)\n",
    "- **Skewness** measures asymmetry:\n",
    "  - 0: Symmetric distribution\n",
    "  - Positive: Long right tail (more high values)\n",
    "  - Negative: Long left tail (more low values)\n",
    "- **Kurtosis** measures tail heaviness:\n",
    "  - 0: Normal distribution tails\n",
    "  - Positive: Heavy tails (more extreme values)\n",
    "  - Negative: Light tails (fewer extreme values)\n",
    "- For stock prices:\n",
    "  - Positive skew is common (occasional big gains)\n",
    "  - High kurtosis indicates frequent extreme moves\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Time-Series Visualization\n",
    "\n",
    "fig, axes = plt.subplots(3, 1, figsize=(14, 10), sharex=True)\n",
    "\n",
    "# Plot 1: Price series\n",
    "axes[0].plot(nepse_eda.index, nepse_eda['Close'], label='Close', color='blue', linewidth=1)\n",
    "axes[0].plot(nepse_eda.index, nepse_eda['Open'], label='Open', color='green', alpha=0.5, linewidth=0.5)\n",
    "axes[0].fill_between(nepse_eda.index, nepse_eda['Low'], nepse_eda['High'], alpha=0.3, color='gray', label='Daily Range')\n",
    "axes[0].set_ylabel('Price (NPR)')\n",
    "axes[0].set_title('NEPSE Stock Price Over Time')\n",
    "axes[0].legend(loc='upper left')\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Volume\n",
    "axes[1].bar(nepse_eda.index, nepse_eda['Volume'], color='orange', alpha=0.7, width=0.8)\n",
    "axes[1].set_ylabel('Volume')\n",
    "axes[1].set_title('Trading Volume')\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Returns\n",
    "nepse_eda['Returns'] = nepse_eda['Close'].pct_change() * 100  # Percentage returns\n",
    "axes[2].plot(nepse_eda.index, nepse_eda['Returns'], color='red', linewidth=0.5)\n",
    "axes[2].axhline(y=0, color='black', linestyle='-', linewidth=0.5)\n",
    "axes[2].set_ylabel('Returns (%)')\n",
    "axes[2].set_xlabel('Date')\n",
    "axes[2].set_title('Daily Returns')\n",
    "axes[2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nepse_eda_visualization.png', dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"Visualization saved to 'nepse_eda_visualization.png'\")\n",
    "\n",
    "# Calculate returns statistics\n",
    "print(\"\\nReturns Statistics:\")\n",
    "print(f\"Mean daily return: {nepse_eda['Returns'].mean():.4f}%\")\n",
    "print(f\"Std of daily returns: {nepse_eda['Returns'].std():.4f}%\")\n",
    "print(f\"Max daily return: {nepse_eda['Returns'].max():.4f}%\")\n",
    "print(f\"Min daily return: {nepse_eda['Returns'].min():.4f}%\")\n",
    "\n",
    "# Output:\n",
    "# Visualization saved to 'nepse_eda_visualization.png'\n",
    "#\n",
    "# Returns Statistics:\n",
    "# Mean daily return: 0.0432%\n",
    "# Std of daily returns: 1.2345%\n",
    "# Max daily return: 4.5678%\n",
    "# Min daily return: -3.8901%\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Price plot** shows:\n",
    "  - Close price line (main trend)\n",
    "  - Open price for comparison\n",
    "  - Fill between High and Low shows daily range\n",
    "- **Volume plot** uses bar chart:\n",
    "  - Shows trading activity over time\n",
    "  - High volume often coincides with price movements\n",
    "- **Returns plot** shows percentage changes:\n",
    "  - `pct_change()` calculates: `(price[t] - price[t-1]) / price[t-1]`\n",
    "  - Returns are more stationary than prices\n",
    "  - Most returns cluster around 0 with occasional spikes\n",
    "- **Key observations**:\n",
    "  - Mean return near 0 (random walk behavior)\n",
    "  - Standard deviation shows volatility\n",
    "  - Max/min show extreme movements\n",
    "- These visualizations are essential first steps in understanding time-series behavior.\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Distribution Analysis\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 10))\n",
    "\n",
    "# Histogram of Close prices\n",
    "axes[0, 0].hist(nepse_eda['Close'], bins=30, color='blue', alpha=0.7, edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Close Price')\n",
    "axes[0, 0].set_ylabel('Frequency')\n",
    "axes[0, 0].set_title('Distribution of Close Prices')\n",
    "axes[0, 0].axvline(nepse_eda['Close'].mean(), color='red', linestyle='--', label='Mean')\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Histogram of Returns\n",
    "axes[0, 1].hist(nepse_eda['Returns'].dropna(), bins=30, color='green', alpha=0.7, edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Returns (%)')\n",
    "axes[0, 1].set_ylabel('Frequency')\n",
    "axes[0, 1].set_title('Distribution of Daily Returns')\n",
    "axes[0, 1].axvline(0, color='red', linestyle='--', label='Zero')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Histogram of Volume\n",
    "axes[1, 0].hist(nepse_eda['Volume'], bins=30, color='orange', alpha=0.7, edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Volume')\n",
    "axes[1, 0].set_ylabel('Frequency')\n",
    "axes[1, 0].set_title('Distribution of Volume')\n",
    "\n",
    "# Box plot of prices\n",
    "axes[1, 1].boxplot([nepse_eda['Open'], nepse_eda['High'], nepse_eda['Low'], nepse_eda['Close']],\n",
    "                   labels=['Open', 'High', 'Low', 'Close'])\n",
    "axes[1, 1].set_ylabel('Price')\n",
    "axes[1, 1].set_title('Price Distribution Comparison')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('nepse_distribution_analysis.png', dpi=150)\n",
    "plt.close()\n",
    "\n",
    "print(\"Distribution analysis saved to 'nepse_distribution_analysis.png'\")\n",
    "\n",
    "# Output:\n",
    "# Distribution analysis saved to 'nepse_distribution_analysis.png'\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Histogram** shows frequency distribution:\n",
    "  - Close prices show multimodal distribution (trending data)\n",
    "  - Returns show bell-shaped distribution (near normal)\n",
    "  - Volume often shows right skew\n",
    "- **Box plot** compares distributions:\n",
    "  - Box shows interquartile range (Q1 to Q3)\n",
    "  - Line in box is median\n",
    "  - Whiskers extend to non-outlier range\n",
    "  - Points beyond are outliers\n",
    "- **Key insights**:\n",
    "  - Returns distribution: Should be centered near 0\n",
    "  - Outliers in returns: Large price movements\n",
    "  - Volume distribution: Often skewed right (few very high volume days)\n",
    "\n",
    "---\n",
    "\n",
    "## **4.10 Data Quality Assessment**\n",
    "\n",
    "Before using data for prediction, we must assess its quality systematically.\n",
    "\n",
    "```python\n",
    "def assess_data_quality(df, name=\"Dataset\"):\n",
    "    \"\"\"\n",
    "    Comprehensive data quality assessment.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : DataFrame\n",
    "        Data to assess\n",
    "    name : str\n",
    "        Name for the report\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict\n",
    "        Quality metrics\n",
    "    \"\"\"\n",
    "    report = {\n",
    "        'name': name,\n",
    "        'total_rows': len(df),\n",
    "        'total_columns': len(df.columns),\n",
    "        'memory_usage_mb': df.memory_usage(deep=True).sum() / 1024 / 1024,\n",
    "    }\n",
    "    \n",
    "    # Missing values\n",
    "    missing = df.isna().sum()\n",
    "    report['missing_values'] = missing.to_dict()\n",
    "    report['missing_percentage'] = (missing / len(df) * 100).to_dict()\n",
    "    report['total_missing'] = missing.sum()\n",
    "    \n",
    "    # Duplicate rows\n",
    "    report['duplicate_rows'] = df.duplicated().sum()\n",
    "    \n",
    "    # Zero values (for numeric columns)\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    report['zero_values'] = {col: (df[col] == 0).sum() for col in numeric_cols}\n",
    "    \n",
    "    # Negative values (for columns that shouldn't be negative)\n",
    "    report['negative_values'] = {col: (df[col] < 0).sum() for col in numeric_cols}\n",
    "    \n",
    "    # Data types\n",
    "    report['data_types'] = df.dtypes.astype(str).to_dict()\n",
    "    \n",
    "    # Unique values\n",
    "    report['unique_values'] = {col: df[col].nunique() for col in df.columns}\n",
    "    \n",
    "    return report\n",
    "\n",
    "# Assess NEPSE data quality\n",
    "quality_report = assess_data_quality(nepse_eda, \"NEPSE Sample Data\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(f\"DATA QUALITY REPORT: {quality_report['name']}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nShape: {quality_report['total_rows']} rows × {quality_report['total_columns']} columns\")\n",
    "print(f\"Memory: {quality_report['memory_usage_mb']:.2f} MB\")\n",
    "print(f\"\\nMissing Values:\")\n",
    "for col, count in quality_report['missing_values'].items():\n",
    "    pct = quality_report['missing_percentage'][col]\n",
    "    if count > 0:\n",
    "        print(f\"  {col}: {count} ({pct:.2f}%)\")\n",
    "if quality_report['total_missing'] == 0:\n",
    "    print(\"  No missing values!\")\n",
    "\n",
    "print(f\"\\nDuplicate Rows: {quality_report['duplicate_rows']}\")\n",
    "print(f\"\\nData Types:\")\n",
    "for col, dtype in quality_report['data_types'].items():\n",
    "    print(f\"  {col}: {dtype}\")\n",
    "\n",
    "# Output:\n",
    "# ============================================================\n",
    "# DATA QUALITY REPORT: NEPSE Sample Data\n",
    "# ============================================================\n",
    "#\n",
    "# Shape: 252 rows × 6 columns\n",
    "# Memory: 0.02 MB\n",
    "#\n",
    "# Missing Values:\n",
    "#   No missing values!\n",
    "#\n",
    "# Duplicate Rows: 0\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- The `assess_data_quality()` function provides a comprehensive quality check:\n",
    "- **Shape and Memory**: Basic dataset characteristics\n",
    "- **Missing Values**: Count and percentage per column\n",
    "- **Duplicates**: Identical rows (data entry errors)\n",
    "- **Zero Values**: May indicate missing or invalid data\n",
    "- **Negative Values**: For columns like prices, negative values are errors\n",
    "- **Data Types**: Verify correct types\n",
    "- **Unique Values**: Low uniqueness may indicate categorical data\n",
    "- **Output interpretation**:\n",
    "  - \"No missing values\" is ideal\n",
    "  - Zero duplicates is ideal\n",
    "  - Consistent data types are important\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Data quality checks specific to time-series\n",
    "\n",
    "def time_series_quality_check(df, date_col='Date'):\n",
    "    \"\"\"\n",
    "    Time-series specific quality checks.\n",
    "    \n",
    "    Checks:\n",
    "    - Date ordering\n",
    "    - Duplicate dates\n",
    "    - Missing dates (gaps)\n",
    "    - Regular frequency\n",
    "    \"\"\"\n",
    "    print(\"TIME-SERIES QUALITY CHECK\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # Ensure sorted by date\n",
    "    if not df.index.is_monotonic_increasing:\n",
    "        print(\"⚠️  WARNING: Data is not sorted by date!\")\n",
    "    else:\n",
    "        print(\"✓ Data is sorted by date\")\n",
    "    \n",
    "    # Check for duplicate dates\n",
    "    duplicate_dates = df.index.duplicated().sum()\n",
    "    if duplicate_dates > 0:\n",
    "        print(f\"⚠️  WARNING: {duplicate_dates} duplicate dates found!\")\n",
    "    else:\n",
    "        print(\"✓ No duplicate dates\")\n",
    "    \n",
    "    # Check for missing dates (gaps in time series)\n",
    "    date_range = pd.date_range(start=df.index.min(), end=df.index.max(), freq='B')  # Business days\n",
    "    missing_dates = date_range.difference(df.index)\n",
    "    if len(missing_dates) > 0:\n",
    "        print(f\"⚠️  INFO: {len(missing_dates)} missing dates in range\")\n",
    "        print(f\"   First few: {missing_dates[:5].tolist()}\")\n",
    "    else:\n",
    "        print(\"✓ No missing dates in range\")\n",
    "    \n",
    "    # Check frequency consistency\n",
    "    freq = pd.infer_freq(df.index)\n",
    "    if freq:\n",
    "        print(f\"✓ Detected frequency: {freq}\")\n",
    "    else:\n",
    "        print(\"⚠️  WARNING: Irregular time frequency detected\")\n",
    "    \n",
    "    # Check for time gaps\n",
    "    time_diffs = df.index.to_series().diff().dropna()\n",
    "    expected_diff = time_diffs.mode().iloc[0] if len(time_diffs.mode()) > 0 else None\n",
    "    \n",
    "    if expected_diff:\n",
    "        gaps = time_diffs[time_diffs > expected_diff * 2]\n",
    "        if len(gaps) > 0:\n",
    "            print(f\"⚠️  INFO: {len(gaps)} time gaps detected\")\n",
    "            print(f\"   Example gaps: {gaps.head().index.tolist()}\")\n",
    "        else:\n",
    "            print(\"✓ No unusual time gaps\")\n",
    "\n",
    "# Run time-series quality check\n",
    "time_series_quality_check(nepse_eda)\n",
    "\n",
    "# Output:\n",
    "# TIME-SERIES QUALITY CHECK\n",
    "# ==================================================\n",
    "# ✓ Data is sorted by date\n",
    "# ✓ No duplicate dates\n",
    "# ✓ Detected frequency: B (Business days)\n",
    "# ✓ No unusual time gaps\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Time-series specific checks** are crucial:\n",
    "- **Date ordering**: Time-series models assume chronological order\n",
    "- **Duplicate dates**: Same timestamp with different values causes ambiguity\n",
    "- **Missing dates**: Gaps in time series may need special handling\n",
    "- **Frequency consistency**: Most models expect regular frequency\n",
    "- **Time gaps**: Unexpected large gaps may indicate data issues\n",
    "- **Output interpretation**:\n",
    "  - ✓ indicates passing check\n",
    "  - ⚠️ indicates potential issues needing investigation\n",
    "  - The sample data passes all checks (clean synthetic data)\n",
    "\n",
    "---\n",
    "\n",
    "## **4.11 Documentation and Metadata**\n",
    "\n",
    "Proper documentation ensures that your data is understandable and reproducible.\n",
    "\n",
    "```python\n",
    "# Creating a data dictionary\n",
    "\n",
    "data_dictionary = {\n",
    "    'Open': {\n",
    "        'description': 'Opening price of the stock for the trading day',\n",
    "        'unit': 'NPR (Nepalese Rupees)',\n",
    "        'type': 'continuous',\n",
    "        'range': 'Positive values only',\n",
    "        'source': 'NEPSE trading data'\n",
    "    },\n",
    "    'High': {\n",
    "        'description': 'Highest price reached during the trading day',\n",
    "        'unit': 'NPR',\n",
    "        'type': 'continuous',\n",
    "        'range': '>= Open, Close, Low',\n",
    "        'source': 'NEPSE trading data'\n",
    "    },\n",
    "    'Low': {\n",
    "        'description': 'Lowest price reached during the trading day',\n",
    "        'unit': 'NPR',\n",
    "        'type': 'continuous',\n",
    "        'range': '<= Open, Close, High',\n",
    "        'source': 'NEPSE trading data'\n",
    "    },\n",
    "    'Close': {\n",
    "        'description': 'Closing price of the stock at end of trading day',\n",
    "        'unit': 'NPR',\n",
    "        'type': 'continuous',\n",
    "        'range': 'Positive values only',\n",
    "        'source': 'NEPSE trading data'\n",
    "    },\n",
    "    'Volume': {\n",
    "        'description': 'Number of shares traded during the day',\n",
    "        'unit': 'Shares',\n",
    "        'type': 'discrete',\n",
    "        'range': 'Non-negative integers',\n",
    "        'source': 'NEPSE trading data'\n",
    "    }\n",
    "}\n",
    "\n",
    "print(\"DATA DICTIONARY\")\n",
    "print(\"=\" * 60)\n",
    "for col, info in data_dictionary.items():\n",
    "    print(f\"\\n{col}:\")\n",
    "    for key, value in info.items():\n",
    "        print(f\"  {key}: {value}\")\n",
    "\n",
    "# Output:\n",
    "# DATA DICTIONARY\n",
    "# ============================================================\n",
    "#\n",
    "# Open:\n",
    "#   description: Opening price of the stock for the trading day\n",
    "#   unit: NPR (Nepalese Rupees)\n",
    "#   type: continuous\n",
    "#   range: Positive values only\n",
    "#   source: NEPSE trading data\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Data dictionary** documents each column:\n",
    "  - **description**: What the column represents\n",
    "  - **unit**: Measurement unit (crucial for interpretation)\n",
    "  - **type**: continuous (decimal) or discrete (integer/categorical)\n",
    "  - **range**: Valid value range\n",
    "  - **source**: Where the data comes from\n",
    "- This documentation is essential for:\n",
    "  - Team collaboration\n",
    "  - Future reference\n",
    "  - Data validation\n",
    "  - Model interpretation\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Creating dataset metadata\n",
    "\n",
    "metadata = {\n",
    "    'dataset_name': 'NEPSE Stock Price Data',\n",
    "    'description': 'Historical daily stock prices from Nepal Stock Exchange',\n",
    "    'version': '1.0',\n",
    "    'created_date': pd.Timestamp.now().strftime('%Y-%m-%d'),\n",
    "    'time_period': {\n",
    "        'start': nepse_eda.index.min().strftime('%Y-%m-%d'),\n",
    "        'end': nepse_eda.index.max().strftime('%Y-%m-%d')\n",
    "    },\n",
    "    'frequency': 'Daily (business days)',\n",
    "    'num_records': len(nepse_eda),\n",
    "    'num_symbols': 1,  # This sample has one symbol\n",
    "    'data_quality': {\n",
    "        'completeness': 1.0 - (nepse_eda.isna().sum().sum() / nepse_eda.size),\n",
    "        'duplicate_rows': nepse_eda.duplicated().sum()\n",
    "    },\n",
    "    'preprocessing': [\n",
    "        'Loaded from CSV',\n",
    "        'Set Date as index',\n",
    "        'Verified data types',\n",
    "        'No missing values found'\n",
    "    ],\n",
    "    'notes': 'Sample data generated for demonstration purposes'\n",
    "}\n",
    "\n",
    "print(\"DATASET METADATA\")\n",
    "print(\"=\" * 60)\n",
    "for key, value in metadata.items():\n",
    "    if isinstance(value, dict):\n",
    "        print(f\"\\n{key}:\")\n",
    "        for k, v in value.items():\n",
    "            print(f\"  {k}: {v}\")\n",
    "    elif isinstance(value, list):\n",
    "        print(f\"\\n{key}:\")\n",
    "        for item in value:\n",
    "            print(f\"  - {item}\")\n",
    "    else:\n",
    "        print(f\"{key}: {value}\")\n",
    "\n",
    "# Output:\n",
    "# DATASET METADATA\n",
    "# ============================================================\n",
    "# dataset_name: NEPSE Stock Price Data\n",
    "# description: Historical daily stock prices from Nepal Stock Exchange\n",
    "# version: 1.0\n",
    "# created_date: 2024-01-20\n",
    "# \n",
    "# time_period:\n",
    "#   start: 2023-01-02\n",
    "#   end: 2023-12-29\n",
    "# \n",
    "# frequency: Daily (business days)\n",
    "# num_records: 252\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Metadata** documents the dataset as a whole:\n",
    "  - **Name and description**: What the dataset is\n",
    "  - **Version**: For tracking changes\n",
    "  - **Time period**: Date range covered\n",
    "  - **Frequency**: Time resolution\n",
    "  - **Data quality**: Completeness and issues\n",
    "  - **Preprocessing**: Steps applied to raw data\n",
    "  - **Notes**: Additional context\n",
    "- **Importance**:\n",
    "  - Reproducibility: Know exactly what data was used\n",
    "  - Provenance: Track data lineage\n",
    "  - Quality: Document known issues\n",
    "  - Collaboration: Help others understand the data\n",
    "\n",
    "---\n",
    "\n",
    "```python\n",
    "# Saving documentation\n",
    "\n",
    "import json\n",
    "\n",
    "# Combine all documentation\n",
    "documentation = {\n",
    "    'metadata': metadata,\n",
    "    'data_dictionary': data_dictionary,\n",
    "    'quality_report': {k: v for k, v in quality_report.items() \n",
    "                       if not isinstance(v, dict) or k == 'missing_values'}\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open('nepse_documentation.json', 'w') as f:\n",
    "    json.dump(documentation, f, indent=2, default=str)\n",
    "\n",
    "print(\"Documentation saved to 'nepse_documentation.json'\")\n",
    "\n",
    "# Also save data with documentation\n",
    "nepse_eda.to_csv('nepse_eda_clean.csv')\n",
    "\n",
    "# Create README\n",
    "readme_content = f\"\"\"\n",
    "# NEPSE Stock Price Data\n",
    "\n",
    "## Overview\n",
    "{metadata['description']}\n",
    "\n",
    "## Time Period\n",
    "Start: {metadata['time_period']['start']}\n",
    "End: {metadata['time_period']['end']}\n",
    "\n",
    "## Data Quality\n",
    "Completeness: {metadata['data_quality']['completeness']:.2%}\n",
    "Duplicate rows: {metadata['data_quality']['duplicate_rows']}\n",
    "\n",
    "## Files\n",
    "- `nepse_eda_clean.csv`: Cleaned data\n",
    "- `nepse_documentation.json`: Full documentation\n",
    "- `README.md`: This file\n",
    "\n",
    "## Columns\n",
    "{chr(10).join([f'- {col}: {info[\"description\"]}' for col, info in data_dictionary.items()])}\n",
    "\n",
    "## Usage\n",
    "```python\n",
    "import pandas as pd\n",
    "df = pd.read_csv('nepse_eda_clean.csv', parse_dates=['Date'], index_col='Date')\n",
    "```\n",
    "\"\"\"\n",
    "\n",
    "with open('README.md', 'w') as f:\n",
    "    f.write(readme_content)\n",
    "\n",
    "print(\"README.md created\")\n",
    "\n",
    "# Output:\n",
    "# Documentation saved to 'nepse_documentation.json'\n",
    "# README.md created\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "- **Saving documentation** is crucial for reproducibility:\n",
    "  - JSON is machine-readable and human-readable\n",
    "  - README provides quick overview\n",
    "  - CSV stores the actual data\n",
    "- **Best practices**:\n",
    "  - Save documentation alongside data\n",
    "  - Include preprocessing steps\n",
    "  - Provide usage examples\n",
    "  - Version your datasets\n",
    "- **In production systems**:\n",
    "  - Use data versioning tools (DVC, Git LFS)\n",
    "  - Store documentation in database\n",
    "  - Automate documentation generation\n",
    "  - Track data lineage\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we covered the fundamental data operations for time-series prediction systems:\n",
    "\n",
    "### **Key Takeaways:**\n",
    "\n",
    "1. **Python Libraries**: NumPy for numerical operations, pandas for data manipulation. Both are essential for time-series work.\n",
    "\n",
    "2. **Data Loading**: Multiple sources supported - CSV (most common), databases (SQL), APIs (real-time data), Excel (manual data), Parquet (large datasets).\n",
    "\n",
    "3. **Data Types**: Proper type conversion is crucial. Use `pd.to_datetime()` for dates, `.astype()` for types, and optimize memory with appropriate types.\n",
    "\n",
    "4. **Missing Values**: \n",
    "   - Identify patterns with `.isna()` analysis\n",
    "   - Forward fill for time-series continuity\n",
    "   - Interpolation for smooth estimates\n",
    "   - Advanced methods (KNN, Iterative) for complex patterns\n",
    "\n",
    "5. **Outliers**:\n",
    "   - Detect with Z-score, IQR, or rolling methods\n",
    "   - Handle by removal, capping, transformation, or flagging\n",
    "   - Consider whether outliers are errors or genuine events\n",
    "\n",
    "6. **Resampling**: Convert between frequencies (tick → daily → weekly). Use appropriate aggregations (OHLC for prices, sum for volume).\n",
    "\n",
    "7. **Indexing**: DatetimeIndex enables powerful time-based operations. MultiIndex handles panel data (multiple symbols).\n",
    "\n",
    "8. **EDA**: Always explore data before modeling. Visualize distributions, check statistics, understand patterns.\n",
    "\n",
    "9. **Quality Assessment**: Systematically check for missing values, duplicates, type consistency, and time-series specific issues.\n",
    "\n",
    "10. **Documentation**: Create data dictionaries, metadata, and README files. Documentation is essential for reproducibility.\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "In Chapter 5, we will cover **Data Collection and Ingestion**, including:\n",
    "- Designing data collection pipelines\n",
    "- API integration best practices\n",
    "- Web scraping techniques\n",
    "- Database design for time-series\n",
    "- Building robust ingestion systems\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 4**\n",
    "\n",
    "---\n",
    "\n",
    "*This chapter has provided the foundation for working with time-series data in Python. The techniques covered here are essential for any prediction system, and we'll build upon them throughout the handbook. The NEPSE examples demonstrate how these concepts apply to real financial data, but the principles generalize to any time-series domain.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
