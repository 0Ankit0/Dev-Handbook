{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Chapter 1: Introduction to Time-Series Prediction Systems**\n",
    "\n",
    "---\n",
    "\n",
    "### **1.1 What is a Time-Series Prediction System?**\n",
    "\n",
    "A time-series prediction system is a specialized computational framework designed to analyze historical data collected over time and generate forecasts about future values. Unlike traditional machine learning problems where data points are assumed to be independent, time-series data has an inherent temporal ordering where each observation is related to its predecessors and successors.\n",
    "\n",
    "#### **Understanding the Core Concept**\n",
    "\n",
    "At its heart, a time-series prediction system answers a fundamental question: *\"Given what has happened in the past, what is likely to happen in the future?\"* This question manifests differently across domains:\n",
    "\n",
    "- **Finance**: \"Given the historical stock prices, what will the price be tomorrow?\"\n",
    "- **Retail**: \"Given past sales data, how much inventory should we stock next month?\"\n",
    "- **Weather**: \"Given temperature readings over the past week, what will the temperature be this weekend?\"\n",
    "- **Healthcare**: \"Given a patient's vital signs over time, will their condition deteriorate?\"\n",
    "\n",
    "#### **The NEPSE Example Context**\n",
    "\n",
    "Let's ground this concept with our running example throughout this handbook: the **Nepal Stock Exchange (NEPSE)** prediction system. NEPSE is the primary stock exchange of Nepal, and predicting its movements involves analyzing historical trading data to forecast future price movements.\n",
    "\n",
    "Consider a sample from our NEPSE dataset:\n",
    "\n",
    "```csv\n",
    "S.No,Symbol,Conf.,Open,High,Low,Close,LTP,Close - LTP,Close - LTP %,VWAP,Vol,Prev. Close,Turnover,Trans.,Diff,Range,Diff %,Range %,VWAP %,52 Weeks High,52 Weeks Low\n",
    "1,ABL,Standard,580.00,590.00,575.00,588.00,588.00,0.00,0.00,584.50,15230,580.00,8900000,285,8.00,15.00,1.38,2.59,0.77,650.00,420.00\n",
    "2,ADBL,Standard,620.00,635.00,615.00,630.00,630.00,0.00,0.00,626.80,8500,618.00,5330000,152,12.00,20.00,1.94,3.24,1.42,720.00,450.00\n",
    "```\n",
    "\n",
    "Each row represents a single trading day for a specific stock symbol. The temporal aspect comes from the sequence of these daily recordsâ€”today's closing price is influenced by yesterday's closing price, the trading volume, market sentiment, and numerous other factors.\n",
    "\n",
    "#### **Key Characteristics of Time-Series Prediction Systems**\n",
    "\n",
    "**1. Temporal Dependence**\n",
    "\n",
    "The most defining characteristic is that observations are not independent. In our NEPSE example:\n",
    "\n",
    "```python\n",
    "# This illustrates temporal dependence\n",
    "# Today's close price depends on previous days' prices\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Load NEPSE data\n",
    "nepse_data = pd.read_csv('nepse_data.csv')\n",
    "\n",
    "# For a single stock, let's examine the relationship\n",
    "stock_data = nepse_data[nepse_data['Symbol'] == 'ABL'].copy()\n",
    "\n",
    "# Calculate the correlation between consecutive close prices\n",
    "# This demonstrates temporal dependence\n",
    "stock_data['Prev_Close'] = stock_data['Close'].shift(1)\n",
    "\n",
    "# Calculate correlation between today's close and yesterday's close\n",
    "correlation = stock_data['Close'].corr(stock_data['Prev_Close'])\n",
    "print(f\"Correlation between consecutive close prices: {correlation:.4f}\")\n",
    "```\n",
    "\n",
    "**Output Explanation**:\n",
    "```\n",
    "Correlation between consecutive close prices: 0.9987\n",
    "```\n",
    "\n",
    "The extremely high correlation (close to 1.0) confirms that consecutive daily closing prices are highly dependent on each other. This is the essence of temporal dependenceâ€”if yesterday's price is 588, today's price is very likely to be close to 588, not suddenly jump to 300 or 900 without significant market events.\n",
    "\n",
    "**2. Sequential Nature**\n",
    "\n",
    "Time-series data must maintain its order. Shuffling the data destroys critical information:\n",
    "\n",
    "```python\n",
    "# Demonstrating why order matters in time-series\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Create a simple visualization showing the difference\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Original ordered data\n",
    "axes[0].plot(stock_data['Close'].values[:50], marker='o', markersize=3)\n",
    "axes[0].set_title('Original Ordered Time-Series\\n(Shows Trends and Patterns)')\n",
    "axes[0].set_xlabel('Time Index (Days)')\n",
    "axes[0].set_ylabel('Close Price (NPR)')\n",
    "\n",
    "# Shuffled data - temporal information destroyed\n",
    "shuffled = stock_data['Close'].values[:50].copy()\n",
    "np.random.shuffle(shuffled)\n",
    "axes[1].plot(shuffled, marker='o', markersize=3, color='orange')\n",
    "axes[1].set_title('Shuffled Time-Series\\n(Trends and Patterns Destroyed)')\n",
    "axes[1].set_xlabel('Random Index (Meaningless)')\n",
    "axes[1].set_ylabel('Close Price (NPR)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Visual Explanation**:\n",
    "The left plot shows the natural progression of stock prices over 50 daysâ€”you can observe trends, potential reversals, and the natural ebb and flow of the market. The right plot shows the same data shuffled randomlyâ€”the temporal patterns are completely destroyed, making any meaningful prediction impossible.\n",
    "\n",
    "**3. Time-Based Patterns**\n",
    "\n",
    "Time-series data often exhibits patterns that repeat or evolve over time:\n",
    "\n",
    "- **Trends**: Long-term upward or downward movement\n",
    "- **Seasonality**: Regular, predictable patterns (e.g., higher trading volumes during earnings season)\n",
    "- **Cyclicality**: Longer-term fluctuations without fixed periods\n",
    "- **Irregularity**: Random, unpredictable variations\n",
    "\n",
    "```python\n",
    "# Identifying basic patterns in NEPSE data\n",
    "\n",
    "def analyze_time_series_patterns(data, symbol, column='Close'):\n",
    "    \"\"\"\n",
    "    Analyze and identify basic time-series patterns in stock data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        The complete NEPSE dataset\n",
    "    symbol : str\n",
    "        Stock symbol to analyze (e.g., 'ABL')\n",
    "    column : str\n",
    "        Column to analyze (default: 'Close')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary containing pattern analysis results\n",
    "    \"\"\"\n",
    "    \n",
    "    # Filter data for the specific stock\n",
    "    stock = data[data['Symbol'] == symbol].copy()\n",
    "    stock = stock.sort_values('S.No').reset_index(drop=True)\n",
    "    \n",
    "    # Basic statistics\n",
    "    mean_price = stock[column].mean()\n",
    "    std_price = stock[column].std()\n",
    "    \n",
    "    # Trend analysis: Compare first half mean to second half mean\n",
    "    mid_point = len(stock) // 2\n",
    "    first_half_mean = stock[column][:mid_point].mean()\n",
    "    second_half_mean = stock[column][mid_point:].mean()\n",
    "    \n",
    "    trend_direction = \"Upward\" if second_half_mean > first_half_mean else \"Downward\"\n",
    "    trend_magnitude = abs(second_half_mean - first_half_mean) / first_half_mean * 100\n",
    "    \n",
    "    # Volatility analysis using rolling standard deviation\n",
    "    stock['Rolling_Std_20'] = stock[column].rolling(window=20).std()\n",
    "    avg_volatility = stock['Rolling_Std_20'].mean()\n",
    "    \n",
    "    return {\n",
    "        'symbol': symbol,\n",
    "        'mean_price': mean_price,\n",
    "        'std_price': std_price,\n",
    "        'trend_direction': trend_direction,\n",
    "        'trend_magnitude': trend_magnitude,\n",
    "        'avg_volatility': avg_volatility,\n",
    "        'data_points': len(stock)\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "# analysis = analyze_time_series_patterns(nepse_data, 'ABL')\n",
    "# print(f\"Trend: {analysis['trend_direction']} by {analysis['trend_magnitude']:.2f}%\")\n",
    "```\n",
    "\n",
    "**Code Explanation**:\n",
    "\n",
    "1. **Function Purpose**: This function takes raw stock data and performs basic pattern analysis, which is a fundamental first step in any time-series prediction system.\n",
    "\n",
    "2. **Parameter Breakdown**:\n",
    "   - `data`: The full dataset containing all stocks\n",
    "   - `symbol`: The specific stock we want to analyze (like 'ABL' for Agriculture Development Bank)\n",
    "   - `column`: Which price metric to focus on (Open, High, Low, Close, etc.)\n",
    "\n",
    "3. **Trend Analysis Logic**:\n",
    "   - We split the data into two halves\n",
    "   - Compare the average price in the first half vs. second half\n",
    "   - If the second half average is higher, the trend is upward\n",
    "   - The magnitude tells us how significant the trend is\n",
    "\n",
    "4. **Rolling Standard Deviation**:\n",
    "   - This measures how much prices typically deviate from the mean over a 20-day window\n",
    "   - Higher values indicate more volatile periods\n",
    "   - This is crucial for risk assessment in prediction systems\n",
    "\n",
    "#### **Components of a Time-Series Prediction System**\n",
    "\n",
    "A complete prediction system consists of several interconnected components:\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                    TIME-SERIES PREDICTION SYSTEM                 â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                  â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "â”‚  â”‚ Data         â”‚â”€â”€â”€â–¶â”‚ Feature      â”‚â”€â”€â”€â–¶â”‚ Model        â”‚       â”‚\n",
    "â”‚  â”‚ Collection   â”‚    â”‚ Engineering  â”‚    â”‚ Development  â”‚       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚         â”‚                   â”‚                   â”‚                â”‚\n",
    "â”‚         â–¼                   â–¼                   â–¼                â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”       â”‚\n",
    "â”‚  â”‚ Data         â”‚    â”‚ Feature      â”‚    â”‚ Model        â”‚       â”‚\n",
    "â”‚  â”‚ Storage      â”‚    â”‚ Selection    â”‚    â”‚ Training     â”‚       â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜       â”‚\n",
    "â”‚         â”‚                   â”‚                   â”‚                â”‚\n",
    "â”‚         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                â”‚\n",
    "â”‚                             â–¼                                    â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚\n",
    "â”‚                    â”‚ Model        â”‚                              â”‚\n",
    "â”‚                    â”‚ Evaluation   â”‚                              â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚\n",
    "â”‚                             â”‚                                    â”‚\n",
    "â”‚                             â–¼                                    â”‚\n",
    "â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                              â”‚\n",
    "â”‚                    â”‚ Deployment   â”‚                              â”‚\n",
    "â”‚                    â”‚ & Monitoring â”‚                              â”‚\n",
    "â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                              â”‚\n",
    "â”‚                                                                  â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **1.2 Real-World Applications and Use Cases**\n",
    "\n",
    "Time-series prediction systems are ubiquitous in modern technology and business. Understanding the breadth of applications helps us appreciate the versatility of the techniques we'll learn.\n",
    "\n",
    "#### **Financial Markets (Our Focus: NEPSE)**\n",
    "\n",
    "Financial time-series prediction is perhaps the most well-known application. In the context of NEPSE:\n",
    "\n",
    "```python\n",
    "# Financial application example: Price prediction for NEPSE stocks\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "class NEPSEPricePredictor:\n",
    "    \"\"\"\n",
    "    A simplified demonstration of a price prediction framework for NEPSE stocks.\n",
    "    This is a conceptual framework - actual implementation would use sophisticated\n",
    "    machine learning models which we'll develop throughout this handbook.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        \"\"\"\n",
    "        Initialize the predictor with NEPSE data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : str\n",
    "            Path to the CSV file containing NEPSE data\n",
    "        \"\"\"\n",
    "        self.data = pd.read_csv(data_path)\n",
    "        self.prepare_data()\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        \"\"\"\n",
    "        Prepare and clean the NEPSE data for analysis.\n",
    "        \n",
    "        This method handles:\n",
    "        - Date conversion (if date column exists)\n",
    "        - Missing value identification\n",
    "        - Data type standardization\n",
    "        - Basic validation\n",
    "        \"\"\"\n",
    "        # Display basic information about the dataset\n",
    "        print(\"=\" * 60)\n",
    "        print(\"NEPSE Dataset Overview\")\n",
    "        print(\"=\" * 60)\n",
    "        print(f\"Total records: {len(self.data)}\")\n",
    "        print(f\"Unique stocks: {self.data['Symbol'].nunique()}\")\n",
    "        print(f\"\\nColumns available: {list(self.data.columns)}\")\n",
    "        \n",
    "        # Check for missing values\n",
    "        missing = self.data.isnull().sum()\n",
    "        if missing.any():\n",
    "            print(\"\\nâš ï¸ Missing values detected:\")\n",
    "            print(missing[missing > 0])\n",
    "        else:\n",
    "            print(\"\\nâœ“ No missing values detected\")\n",
    "        \n",
    "        # Display sample data\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Sample Data (First 5 Records)\")\n",
    "        print(\"=\" * 60)\n",
    "        print(self.data.head())\n",
    "        \n",
    "        # Basic statistics for numeric columns\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Statistical Summary\")\n",
    "        print(\"=\" * 60)\n",
    "        numeric_cols = ['Open', 'High', 'Low', 'Close', 'VWAP', 'Vol', 'Turnover']\n",
    "        print(self.data[numeric_cols].describe())\n",
    "    \n",
    "    def get_stock_history(self, symbol):\n",
    "        \"\"\"\n",
    "        Retrieve historical data for a specific stock.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str\n",
    "            Stock symbol (e.g., 'ABL', 'ADBL', 'NABIL')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Historical data for the specified stock\n",
    "        \"\"\"\n",
    "        stock_data = self.data[self.data['Symbol'] == symbol].copy()\n",
    "        stock_data = stock_data.sort_values('S.No').reset_index(drop=True)\n",
    "        \n",
    "        if len(stock_data) == 0:\n",
    "            print(f\"âš ï¸ No data found for symbol: {symbol}\")\n",
    "            return None\n",
    "        \n",
    "        print(f\"\\n{'=' * 60}\")\n",
    "        print(f\"Stock History for {symbol}\")\n",
    "        print(f\"{'=' * 60}\")\n",
    "        print(f\"Records available: {len(stock_data)}\")\n",
    "        print(f\"Date range: Record {stock_data['S.No'].min()} to {stock_data['S.No'].max()}\")\n",
    "        print(f\"Price range: NPR {stock_data['Low'].min():.2f} - {stock_data['High'].max():.2f}\")\n",
    "        \n",
    "        return stock_data\n",
    "\n",
    "# Example usage (when data is available):\n",
    "# predictor = NEPSEPricePredictor('nepse_data.csv')\n",
    "# abl_history = predictor.get_stock_history('ABL')\n",
    "```\n",
    "\n",
    "**Detailed Code Explanation**:\n",
    "\n",
    "1. **Class Structure**: We define a class `NEPSEPricePredictor` that encapsulates all prediction-related functionality. This follows object-oriented design principles and makes the code modular and reusable.\n",
    "\n",
    "2. **`__init__` Method**:\n",
    "   - This is the constructor that runs when we create a new instance of the predictor\n",
    "   - It loads the CSV data using pandas\n",
    "   - Calls `prepare_data()` to ensure data is ready for analysis\n",
    "   - The `self` keyword refers to the current instance, allowing us to store data that persists across method calls\n",
    "\n",
    "3. **`prepare_data` Method**:\n",
    "   - **Purpose**: Ensures data quality before any analysis begins\n",
    "   - **Information Display**: Shows total records, unique stocks, and available columns\n",
    "   - **Missing Value Check**: Uses `isnull().sum()` to count null values per column\n",
    "   - **Sample Data**: Displays first 5 records to give a quick visual verification\n",
    "   - **Statistical Summary**: Provides count, mean, std, min, quartiles, and max for numeric columns\n",
    "\n",
    "4. **`get_stock_history` Method**:\n",
    "   - Filters the entire dataset to get records for a single stock symbol\n",
    "   - Sorts by `S.No` (serial number) to maintain temporal order\n",
    "   - Provides summary statistics for the specific stock\n",
    "   - Returns the filtered DataFrame for further analysis\n",
    "\n",
    "**Understanding the NEPSE Data Columns**:\n",
    "\n",
    "| Column | Description | Example Value |\n",
    "|--------|-------------|---------------|\n",
    "| `S.No` | Serial number (acts as time index) | 1, 2, 3... |\n",
    "| `Symbol` | Stock ticker symbol | ABL, ADBL, NABIL |\n",
    "| `Conf.` | Confirmation status | Standard |\n",
    "| `Open` | Opening price for the day (NPR) | 580.00 |\n",
    "| `High` | Highest price during the day (NPR) | 590.00 |\n",
    "| `Low` | Lowest price during the day (NPR) | 575.00 |\n",
    "| `Close` | Closing price for the day (NPR) | 588.00 |\n",
    "| `LTP` | Last Traded Price | 588.00 |\n",
    "| `VWAP` | Volume Weighted Average Price | 584.50 |\n",
    "| `Vol` | Trading volume (number of shares) | 15230 |\n",
    "| `Turnover` | Total value traded (NPR) | 8900000 |\n",
    "| `52 Weeks High/Low` | 52-week high and low prices | 650.00/420.00 |\n",
    "\n",
    "#### **Other Industry Applications**\n",
    "\n",
    "While our focus is financial prediction, understanding other domains helps appreciate the universality of time-series concepts:\n",
    "\n",
    "**1. Retail and E-Commerce**\n",
    "\n",
    "```python\n",
    "# Example: Sales forecasting structure (adaptable to NEPSE-like format)\n",
    "\n",
    "def retail_forecasting_example():\n",
    "    \"\"\"\n",
    "    Demonstrates how time-series concepts apply to retail forecasting.\n",
    "    The structure is similar to our NEPSE data but with retail-specific columns.\n",
    "    \"\"\"\n",
    "    # Hypothetical retail data structure\n",
    "    retail_data_structure = {\n",
    "        'date': 'Trading day equivalent',\n",
    "        'store_id': 'Stock symbol equivalent',\n",
    "        'product_category': 'Sector equivalent',\n",
    "        'units_sold': 'Volume equivalent',\n",
    "        'revenue': 'Turnover equivalent',\n",
    "        'avg_price': 'VWAP equivalent',\n",
    "        'promotion_flag': 'External factor (like market events)'\n",
    "    }\n",
    "    \n",
    "    print(\"Retail forecasting uses similar patterns:\")\n",
    "    print(\"- Daily/weekly seasonality (like trading day patterns)\")\n",
    "    print(\"- Holiday effects (like market holidays)\")\n",
    "    print(\"- Promotional impacts (like dividend announcements)\")\n",
    "    print(\"- Trend analysis (like market trends)\")\n",
    "    \n",
    "    return retail_data_structure\n",
    "```\n",
    "\n",
    "**2. Energy Demand Forecasting**\n",
    "\n",
    "```python\n",
    "def energy_forecasting_structure():\n",
    "    \"\"\"\n",
    "    Energy demand prediction structure comparison with NEPSE.\n",
    "    \"\"\"\n",
    "    comparison = {\n",
    "        'Aspect': ['Time Granularity', 'Prediction Target', 'Key Drivers', 'Seasonality'],\n",
    "        'NEPSE': ['Daily', 'Stock Price', 'Market sentiment, Earnings', 'Quarterly patterns'],\n",
    "        'Energy': ['Hourly', 'Electricity Demand', 'Temperature, Industry activity', 'Daily + Seasonal']\n",
    "    }\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"Comparison: NEPSE vs Energy Forecasting\")\n",
    "    print(\"=\" * 70)\n",
    "    for i, aspect in enumerate(comparison['Aspect']):\n",
    "        print(f\"\\n{aspect}:\")\n",
    "        print(f\"  NEPSE:   {comparison['NEPSE'][i]}\")\n",
    "        print(f\"  Energy:  {comparison['Energy'][i]}\")\n",
    "    \n",
    "    return comparison\n",
    "```\n",
    "\n",
    "**3. Healthcare Monitoring**\n",
    "\n",
    "```python\n",
    "def healthcare_monitoring_example():\n",
    "    \"\"\"\n",
    "    Patient vital signs monitoring as time-series (similar concepts to NEPSE).\n",
    "    \"\"\"\n",
    "    # Structure for patient monitoring data\n",
    "    patient_data_example = \"\"\"\n",
    "    Patient Monitoring Data Structure:\n",
    "    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "    â”‚ Timestamp   â”‚ Heart Rate  â”‚ Blood Press.â”‚ Temperature â”‚\n",
    "    â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "    â”‚ 10:00:00    â”‚ 72          â”‚ 120/80      â”‚ 98.6        â”‚\n",
    "    â”‚ 10:05:00    â”‚ 75          â”‚ 122/82      â”‚ 98.7        â”‚\n",
    "    â”‚ 10:10:00    â”‚ 73          â”‚ 119/79      â”‚ 98.6        â”‚\n",
    "    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "    \n",
    "    Similar to NEPSE where:\n",
    "    - Timestamp â‰ˆ S.No (time index)\n",
    "    - Heart Rate â‰ˆ Close (primary metric)\n",
    "    - Blood Pressure â‰ˆ High/Low range\n",
    "    - Temperature â‰ˆ VWAP (derived metric)\n",
    "    \n",
    "    Prediction Goals:\n",
    "    - NEPSE: Will price go up or down?\n",
    "    - Healthcare: Will patient's condition deteriorate?\n",
    "    \"\"\"\n",
    "    print(patient_data_example)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **1.3 Types of Prediction Problems**\n",
    "\n",
    "Time-series prediction problems can be categorized into several types, each requiring different approaches and evaluation methods. Understanding these categories helps us choose the right techniques for our NEPSE prediction system.\n",
    "\n",
    "#### **1.3.1 Regression Problems**\n",
    "\n",
    "**Definition**: Predicting a continuous numerical value.\n",
    "\n",
    "**NEPSE Example**: Predicting tomorrow's closing price.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score\n",
    "\n",
    "class NEPSERegressionPredictor:\n",
    "    \"\"\"\n",
    "    Regression-based price prediction for NEPSE stocks.\n",
    "    This demonstrates predicting continuous values (prices) from historical data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        \"\"\"\n",
    "        Initialize with NEPSE data.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data : pd.DataFrame\n",
    "            NEPSE dataset containing historical stock information\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.model = None\n",
    "        self.feature_columns = None\n",
    "    \n",
    "    def create_features(self, symbol, lookback_days=5):\n",
    "        \"\"\"\n",
    "        Create features for regression from historical data.\n",
    "        \n",
    "        This method transforms raw time-series data into a format suitable\n",
    "        for machine learning by creating lag features and derived metrics.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str\n",
    "            Stock symbol to create features for\n",
    "        lookback_days : int\n",
    "            Number of previous days to use as features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Feature matrix with target variable\n",
    "        \"\"\"\n",
    "        # Filter for specific stock and sort chronologically\n",
    "        stock = self.data[self.data['Symbol'] == symbol].copy()\n",
    "        stock = stock.sort_values('S.No').reset_index(drop=True)\n",
    "        \n",
    "        if len(stock) < lookback_days + 1:\n",
    "            print(f\"âš ï¸ Insufficient data for {symbol}. Need at least {lookback_days + 1} records.\")\n",
    "            return None\n",
    "        \n",
    "        # Create lag features: previous days' closing prices\n",
    "        # These are the inputs (X) to our prediction model\n",
    "        for i in range(1, lookback_days + 1):\n",
    "            stock[f'Close_Lag_{i}'] = stock['Close'].shift(i)\n",
    "        \n",
    "        # Create derived features\n",
    "        # These help the model understand patterns beyond raw prices\n",
    "        \n",
    "        # Price momentum: rate of change\n",
    "        stock['Price_Momentum_1'] = stock['Close'] - stock['Close'].shift(1)\n",
    "        stock['Price_Momentum_5'] = stock['Close'] - stock['Close'].shift(5)\n",
    "        \n",
    "        # Volatility: standard deviation over recent days\n",
    "        stock['Volatility_5'] = stock['Close'].rolling(window=5).std()\n",
    "        \n",
    "        # Volume features\n",
    "        stock['Volume_Lag_1'] = stock['Vol'].shift(1)\n",
    "        stock['Volume_Change'] = stock['Vol'] / stock['Vol'].shift(1) - 1\n",
    "        \n",
    "        # Target: Next day's closing price\n",
    "        # This is what we want to predict (y)\n",
    "        stock['Target'] = stock['Close'].shift(-1)\n",
    "        \n",
    "        # Remove rows with NaN values (created by lag operations)\n",
    "        stock = stock.dropna()\n",
    "        \n",
    "        # Define feature columns\n",
    "        self.feature_columns = [col for col in stock.columns \n",
    "                                if 'Lag' in col or 'Momentum' in col \n",
    "                                or 'Volatility' in col or 'Change' in col]\n",
    "        \n",
    "        return stock\n",
    "    \n",
    "    def train(self, symbol, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train a linear regression model for price prediction.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str\n",
    "            Stock symbol to train model for\n",
    "        test_size : float\n",
    "            Proportion of data to use for testing (0.0 to 1.0)\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Training results including metrics\n",
    "        \"\"\"\n",
    "        # Prepare features\n",
    "        feature_data = self.create_features(symbol)\n",
    "        \n",
    "        if feature_data is None or len(feature_data) < 10:\n",
    "            return {'error': 'Insufficient data for training'}\n",
    "        \n",
    "        # Split features (X) and target (y)\n",
    "        X = feature_data[self.feature_columns]\n",
    "        y = feature_data['Target']\n",
    "        \n",
    "        # Time-series split: use earlier data for training, later for testing\n",
    "        # IMPORTANT: We don't use random splitting for time-series!\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Initialize and train model\n",
    "        self.model = LinearRegression()\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate evaluation metrics\n",
    "        metrics = {\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'R2': r2_score(y_test, y_pred),\n",
    "            'train_size': len(X_train),\n",
    "            'test_size': len(X_test)\n",
    "        }\n",
    "        \n",
    "        # Store predictions for analysis\n",
    "        self.predictions = pd.DataFrame({\n",
    "            'Actual': y_test,\n",
    "            'Predicted': y_pred,\n",
    "            'Error': y_test - y_pred,\n",
    "            'Abs_Error': np.abs(y_test - y_pred)\n",
    "        })\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    def analyze_predictions(self):\n",
    "        \"\"\"\n",
    "        Analyze the prediction errors to understand model performance.\n",
    "        \"\"\"\n",
    "        if self.predictions is None:\n",
    "            print(\"âš ï¸ No predictions available. Run train() first.\")\n",
    "            return\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"Prediction Analysis\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Basic error statistics\n",
    "        print(\"\\nğŸ“Š Error Statistics:\")\n",
    "        print(f\"   Mean Error:      {self.predictions['Error'].mean():.2f} NPR\")\n",
    "        print(f\"   Std Dev Error:   {self.predictions['Error'].std():.2f} NPR\")\n",
    "        print(f\"   Mean Abs Error:  {self.predictions['Abs_Error'].mean():.2f} NPR\")\n",
    "        \n",
    "        # Percentage of predictions within different error thresholds\n",
    "        actual = self.predictions['Actual']\n",
    "        pct_error = (self.predictions['Abs_Error'] / actual) * 100\n",
    "        \n",
    "        print(\"\\nğŸ“ˆ Prediction Accuracy:\")\n",
    "        print(f\"   Within 1% error:  {(pct_error < 1).sum() / len(pct_error) * 100:.1f}%\")\n",
    "        print(f\"   Within 2% error:  {(pct_error < 2).sum() / len(pct_error) * 100:.1f}%\")\n",
    "        print(f\"   Within 5% error:  {(pct_error < 5).sum() / len(pct_error) * 100:.1f}%\")\n",
    "\n",
    "# Example usage:\n",
    "# predictor = NEPSERegressionPredictor(nepse_data)\n",
    "# metrics = predictor.train('ABL')\n",
    "# print(f\"RÂ² Score: {metrics['R2']:.4f}\")\n",
    "# predictor.analyze_predictions()\n",
    "```\n",
    "\n",
    "**Detailed Code Walkthrough**:\n",
    "\n",
    "1. **Feature Creation (`create_features` method)**:\n",
    "   - **Lag Features**: We create `Close_Lag_1`, `Close_Lag_2`, etc., which represent the closing prices from 1, 2, 3... days ago. These are crucial because stock prices exhibit momentumâ€”yesterday's price heavily influences today's.\n",
    "   - **Momentum Features**: The difference between current price and prices from 1 or 5 days ago captures the direction and strength of price movement.\n",
    "   - **Volatility Feature**: Rolling standard deviation over 5 days measures how unstable the price has been recentlyâ€”high volatility often precedes significant moves.\n",
    "   - **Volume Features**: Trading volume is a leading indicator; unusual volume often signals impending price moves.\n",
    "\n",
    "2. **Training Process (`train` method)**:\n",
    "   - **Time-Series Split**: Unlike typical ML problems where we shuffle data randomly, time-series requires chronological splitting. We train on past data to predict future data.\n",
    "   - **Linear Regression**: The simplest modelâ€”we'll explore more sophisticated models in later chapters.\n",
    "   - **Metrics Calculation**: MAE (Mean Absolute Error) gives average error in NPR; RMSE penalizes large errors more; RÂ² measures how well the model explains variance.\n",
    "\n",
    "3. **Error Analysis (`analyze_predictions` method)**:\n",
    "   - Shows how close predictions are to actual values\n",
    "   - Percentage-based thresholds are intuitive for financial data\n",
    "\n",
    "#### **1.3.2 Classification Problems**\n",
    "\n",
    "**Definition**: Predicting a categorical outcome.\n",
    "\n",
    "**NEPSE Example**: Predicting whether the price will go UP or DOWN tomorrow.\n",
    "\n",
    "```python\n",
    "class NEPSEClassificationPredictor:\n",
    "    \"\"\"\n",
    "    Classification-based direction prediction for NEPSE stocks.\n",
    "    Instead of predicting exact price, we predict if price will rise or fall.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.model = None\n",
    "        self.feature_columns = None\n",
    "    \n",
    "    def create_classification_features(self, symbol, lookback_days=5):\n",
    "        \"\"\"\n",
    "        Create features and binary target for classification.\n",
    "        \n",
    "        The target is:\n",
    "        - 1 if tomorrow's close > today's close (price goes UP)\n",
    "        - 0 if tomorrow's close <= today's close (price goes DOWN)\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str\n",
    "            Stock symbol to analyze\n",
    "        lookback_days : int\n",
    "            Number of previous days to use as features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Feature matrix with binary target\n",
    "        \"\"\"\n",
    "        stock = self.data[self.data['Symbol'] == symbol].copy()\n",
    "        stock = stock.sort_values('S.No').reset_index(drop=True)\n",
    "        \n",
    "        # Create lag features (same as regression)\n",
    "        for i in range(1, lookback_days + 1):\n",
    "            stock[f'Close_Lag_{i}'] = stock['Close'].shift(i)\n",
    "        \n",
    "        # Price-based features\n",
    "        stock['Price_Momentum_1'] = stock['Close'].pct_change()\n",
    "        stock['Price_Range'] = (stock['High'] - stock['Low']) / stock['Close']\n",
    "        \n",
    "        # VWAP deviation: how far is close from VWAP?\n",
    "        stock['VWAP_Deviation'] = (stock['Close'] - stock['VWAP']) / stock['VWAP']\n",
    "        \n",
    "        # Volume features\n",
    "        stock['Volume_SMA_5'] = stock['Vol'].rolling(5).mean()\n",
    "        stock['Volume_Ratio'] = stock['Vol'] / stock['Volume_SMA_5']\n",
    "        \n",
    "        # Relative position in 52-week range\n",
    "        stock['Position_52W'] = (stock['Close'] - stock['52 Weeks Low']) / \\\n",
    "                                (stock['52 Weeks High'] - stock['52 Weeks Low'])\n",
    "        \n",
    "        # Create binary target: 1 if price goes UP, 0 if DOWN\n",
    "        stock['Target'] = (stock['Close'].shift(-1) > stock['Close']).astype(int)\n",
    "        \n",
    "        # Clean data\n",
    "        stock = stock.dropna()\n",
    "        \n",
    "        self.feature_columns = [col for col in stock.columns \n",
    "                                if 'Lag' in col or 'Momentum' in col \n",
    "                                or 'Range' in col or 'Deviation' in col\n",
    "                                or 'Ratio' in col or 'Position' in col]\n",
    "        \n",
    "        return stock\n",
    "    \n",
    "    def train_classifier(self, symbol, test_size=0.2):\n",
    "        \"\"\"\n",
    "        Train a classifier to predict price direction.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Training results including classification metrics\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LogisticRegression\n",
    "        from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "        from sklearn.metrics import confusion_matrix, classification_report\n",
    "        \n",
    "        feature_data = self.create_classification_features(symbol)\n",
    "        \n",
    "        if feature_data is None or len(feature_data) < 10:\n",
    "            return {'error': 'Insufficient data'}\n",
    "        \n",
    "        X = feature_data[self.feature_columns]\n",
    "        y = feature_data['Target']\n",
    "        \n",
    "        # Chronological split\n",
    "        split_idx = int(len(X) * (1 - test_size))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Train logistic regression classifier\n",
    "        self.model = LogisticRegression(random_state=42, max_iter=1000)\n",
    "        self.model.fit(X_train, y_train)\n",
    "        \n",
    "        # Predict\n",
    "        y_pred = self.model.predict(X_test)\n",
    "        y_prob = self.model.predict_proba(X_test)[:, 1]  # Probability of UP\n",
    "        \n",
    "        # Metrics\n",
    "        metrics = {\n",
    "            'accuracy': accuracy_score(y_test, y_pred),\n",
    "            'precision': precision_score(y_test, y_pred),\n",
    "            'recall': recall_score(y_test, y_pred),\n",
    "            'f1_score': f1_score(y_test, y_pred)\n",
    "        }\n",
    "        \n",
    "        # Confusion matrix interpretation\n",
    "        cm = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(f\"Classification Results for {symbol}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        print(\"\\nğŸ“Š Confusion Matrix:\")\n",
    "        print(\"                 Predicted\")\n",
    "        print(\"                 DOWN    UP\")\n",
    "        print(f\"Actual DOWN    {cm[0][0]:5d}  {cm[0][1]:5d}\")\n",
    "        print(f\"       UP      {cm[1][0]:5d}  {cm[1][1]:5d}\")\n",
    "        \n",
    "        print(\"\\nğŸ“ˆ Classification Metrics:\")\n",
    "        print(f\"   Accuracy:  {metrics['accuracy']:.4f}\")\n",
    "        print(f\"   Precision: {metrics['precision']:.4f}\")\n",
    "        print(f\"   Recall:    {metrics['recall']:.4f}\")\n",
    "        print(f\"   F1 Score:  {metrics['f1_score']:.4f}\")\n",
    "        \n",
    "        # Feature importance\n",
    "        feature_importance = pd.DataFrame({\n",
    "            'Feature': self.feature_columns,\n",
    "            'Coefficient': self.model.coef_[0]\n",
    "        }).sort_values('Coefficient', key=abs, ascending=False)\n",
    "        \n",
    "        print(\"\\nğŸ” Top 5 Important Features:\")\n",
    "        print(feature_importance.head())\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "# Example usage:\n",
    "# classifier = NEPSEClassificationPredictor(nepse_data)\n",
    "# metrics = classifier.train_classifier('ABL')\n",
    "```\n",
    "\n",
    "**Understanding Classification vs. Regression**:\n",
    "\n",
    "| Aspect | Regression | Classification |\n",
    "|--------|------------|----------------|\n",
    "| **Output** | Continuous value (e.g., 588.50) | Discrete category (UP/DOWN) |\n",
    "| **NEPSE Goal** | Predict exact price | Predict direction |\n",
    "| **Metric** | MAE, RMSE, RÂ² | Accuracy, Precision, Recall |\n",
    "| **Use Case** | Valuation, target setting | Trading signals, decisions |\n",
    "| **Difficulty** | Harder (exact values) | Easier (direction only) |\n",
    "\n",
    "**Interpreting Classification Metrics**:\n",
    "\n",
    "- **Accuracy**: Overall percentage of correct predictions\n",
    "  - Formula: `(TP + TN) / (TP + TN + FP + FN)`\n",
    "  - In NEPSE context: How often do we correctly predict UP or DOWN?\n",
    "\n",
    "- **Precision**: Of all UP predictions, how many were correct?\n",
    "  - Formula: `TP / (TP + FP)`\n",
    "  - In NEPSE context: When we say price will go UP, how often are we right?\n",
    "\n",
    "- **Recall**: Of all actual UP movements, how many did we catch?\n",
    "  - Formula: `TP / (TP + FN)`\n",
    "  - In NEPSE context: Of all days the price actually went UP, how many did we predict correctly?\n",
    "\n",
    "- **F1 Score**: Harmonic mean of Precision and Recall\n",
    "  - Formula: `2 Ã— (Precision Ã— Recall) / (Precision + Recall)`\n",
    "  - Useful when you need a balance between precision and recall\n",
    "\n",
    "#### **1.3.3 Multi-Step Forecasting**\n",
    "\n",
    "**Definition**: Predicting multiple future time points.\n",
    "\n",
    "**NEPSE Example**: Predicting closing prices for the next 5 trading days.\n",
    "\n",
    "```python\n",
    "class NEPSEMultiStepForecaster:\n",
    "    \"\"\"\n",
    "    Multi-step forecasting for NEPSE stocks.\n",
    "    Predicts multiple future values instead of just one.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.models = {}  # Store separate model for each horizon\n",
    "    \n",
    "    def create_multi_step_features(self, symbol, forecast_horizon=5):\n",
    "        \"\"\"\n",
    "        Create features and multiple targets for multi-step forecasting.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str\n",
    "            Stock symbol\n",
    "        forecast_horizon : int\n",
    "            Number of future days to predict\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Features with multiple target columns\n",
    "        \"\"\"\n",
    "        stock = self.data[self.data['Symbol'] == symbol].copy()\n",
    "        stock = stock.sort_values('S.No').reset_index(drop=True)\n",
    "        \n",
    "        # Create lag features\n",
    "        for i in range(1, 11):  # Use 10 days of history\n",
    "            stock[f'Close_Lag_{i}'] = stock['Close'].shift(i)\n",
    "        \n",
    "        # Create targets for each future step\n",
    "        for h in range(1, forecast_horizon + 1):\n",
    "            stock[f'Target_Day_{h}'] = stock['Close'].shift(-h)\n",
    "        \n",
    "        stock = stock.dropna()\n",
    "        \n",
    "        return stock\n",
    "    \n",
    "    def train_recursive(self, symbol, forecast_horizon=5):\n",
    "        \"\"\"\n",
    "        Train recursive multi-step forecasting model.\n",
    "        \n",
    "        Recursive approach:\n",
    "        1. Predict tomorrow's price\n",
    "        2. Use that prediction to predict day after tomorrow\n",
    "        3. Continue for all forecast horizons\n",
    "        \n",
    "        This is simpler but errors can accumulate.\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "        feature_data = self.create_multi_step_features(symbol, forecast_horizon)\n",
    "        \n",
    "        if len(feature_data) < 20:\n",
    "            return {'error': 'Insufficient data'}\n",
    "        \n",
    "        # Features are the lag columns\n",
    "        feature_cols = [col for col in feature_data.columns if 'Lag' in col]\n",
    "        \n",
    "        # Train a single model to predict next day\n",
    "        # Then use recursively for further days\n",
    "        X = feature_data[feature_cols]\n",
    "        y = feature_data['Target_Day_1']\n",
    "        \n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        model = LinearRegression()\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Recursive forecasting\n",
    "        predictions = []\n",
    "        current_features = X_test.iloc[0:1].copy()\n",
    "        \n",
    "        for h in range(forecast_horizon):\n",
    "            pred = model.predict(current_features)[0]\n",
    "            predictions.append(pred)\n",
    "            \n",
    "            # Update features for next prediction\n",
    "            # Shift all lag features and insert new prediction\n",
    "            for i in range(len(feature_cols) - 1, 0, -1):\n",
    "                current_features[feature_cols[i]] = current_features[feature_cols[i-1]].values\n",
    "            current_features[feature_cols[0]] = pred\n",
    "        \n",
    "        return predictions\n",
    "    \n",
    "    def train_direct(self, symbol, forecast_horizon=5):\n",
    "        \"\"\"\n",
    "        Train direct multi-step forecasting model.\n",
    "        \n",
    "        Direct approach:\n",
    "        - Train a separate model for each forecast horizon\n",
    "        - More accurate but requires more models\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str\n",
    "            Stock symbol\n",
    "        forecast_horizon : int\n",
    "            Number of future days to predict\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Predictions and models for each horizon\n",
    "        \"\"\"\n",
    "        from sklearn.linear_model import LinearRegression\n",
    "        \n",
    "        feature_data = self.create_multi_step_features(symbol, forecast_horizon)\n",
    "        \n",
    "        if len(feature_data) < 20:\n",
    "            return {'error': 'Insufficient data'}\n",
    "        \n",
    "        feature_cols = [col for col in feature_data.columns if 'Lag' in col]\n",
    "        X = feature_data[feature_cols]\n",
    "        \n",
    "        split_idx = int(len(X) * 0.8)\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        \n",
    "        results = {'predictions': [], 'models': {}}\n",
    "        \n",
    "        for h in range(1, forecast_horizon + 1):\n",
    "            target_col = f'Target_Day_{h}'\n",
    "            y = feature_data[target_col]\n",
    "            y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "            \n",
    "            # Train separate model for this horizon\n",
    "            model = LinearRegression()\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            # Store model\n",
    "            self.models[h] = model\n",
    "            results['models'][h] = model\n",
    "            \n",
    "            # Get prediction\n",
    "            pred = model.predict(X_test.iloc[0:1])[0]\n",
    "            results['predictions'].append(pred)\n",
    "        \n",
    "        return results\n",
    "\n",
    "# Example usage:\n",
    "# forecaster = NEPSEMultiStepForecaster(nepse_data)\n",
    "# predictions = forecaster.train_direct('ABL', forecast_horizon=5)\n",
    "# print(\"5-Day Forecast:\", predictions['predictions'])\n",
    "```\n",
    "\n",
    "**Comparison of Multi-Step Approaches**:\n",
    "\n",
    "| Approach | Description | Pros | Cons |\n",
    "|----------|-------------|------|------|\n",
    "| **Recursive** | Predict one step, use as input for next | Simple, single model | Errors accumulate |\n",
    "| **Direct** | Separate model for each horizon | More accurate | More complex, more models |\n",
    "| **Direct-Recursive** | Hybrid of both | Balanced accuracy | Implementation complexity |\n",
    "| **Multiple Output** | Model outputs all predictions at once | Captures correlations | Limited model support |\n",
    "\n",
    "---\n",
    "\n",
    "### **1.4 The Machine Learning Pipeline Overview**\n",
    "\n",
    "A machine learning pipeline for time-series prediction is a systematic sequence of steps that transform raw data into actionable predictions. Understanding this pipeline is crucial for building robust systems.\n",
    "\n",
    "#### **The Complete Pipeline Architecture**\n",
    "\n",
    "```\n",
    "â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”\n",
    "â”‚                     MACHINE LEARNING PIPELINE                           â”‚\n",
    "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚   DATA      â”‚   â”‚   DATA      â”‚   â”‚   FEATURE   â”‚   â”‚   MODEL     â”‚ â”‚\n",
    "â”‚  â”‚ COLLECTION  â”‚â”€â”€â–¶â”‚   CLEANING  â”‚â”€â”€â–¶â”‚ ENGINEERING â”‚â”€â”€â–¶â”‚  SELECTION  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚         â”‚                 â”‚                 â”‚                 â”‚         â”‚\n",
    "â”‚         â–¼                 â–¼                 â–¼                 â–¼         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚ - APIs      â”‚   â”‚ - Missing   â”‚   â”‚ - Lags      â”‚   â”‚ - Linear    â”‚ â”‚\n",
    "â”‚  â”‚ - Files     â”‚   â”‚   values    â”‚   â”‚ - Rolling   â”‚   â”‚ - Tree      â”‚ â”‚\n",
    "â”‚  â”‚ - Database  â”‚   â”‚ - Outliers  â”‚   â”‚ - Domain    â”‚   â”‚ - Neural    â”‚ â”‚\n",
    "â”‚  â”‚ - Scraping  â”‚   â”‚ - Formats   â”‚   â”‚ - Time      â”‚   â”‚ - Ensemble  â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚   TRAINING  â”‚   â”‚  EVALUATION â”‚   â”‚  DEPLOYMENT â”‚   â”‚ MONITORING  â”‚ â”‚\n",
    "â”‚  â”‚             â”‚â—€â”€â”€â”‚             â”‚â—€â”€â”€â”‚             â”‚â—€â”€â”€â”‚             â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚         â”‚                 â”‚                 â”‚                 â”‚         â”‚\n",
    "â”‚         â–¼                 â–¼                 â–¼                 â–¼         â”‚\n",
    "â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚\n",
    "â”‚  â”‚ - Split     â”‚   â”‚ - Metrics   â”‚   â”‚ - API       â”‚   â”‚ - Drift     â”‚ â”‚\n",
    "â”‚  â”‚ - Validate  â”‚   â”‚ - Backtest  â”‚   â”‚ - Container â”‚   â”‚ - Retrain   â”‚ â”‚\n",
    "â”‚  â”‚ - Tune      â”‚   â”‚ - Compare   â”‚   â”‚ - Scale     â”‚   â”‚ - Alert     â”‚ â”‚\n",
    "â”‚  â”‚ - Save      â”‚   â”‚ - Report    â”‚   â”‚ - Version   â”‚   â”‚ - Log       â”‚ â”‚\n",
    "â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚\n",
    "â”‚                                                                         â”‚\n",
    "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
    "```\n",
    "\n",
    "#### **Pipeline Implementation for NEPSE**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NEPSEPipeline:\n",
    "    \"\"\"\n",
    "    Complete ML pipeline implementation for NEPSE price prediction.\n",
    "    This class demonstrates the end-to-end workflow from data loading\n",
    "    to model evaluation.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path=None, data=None):\n",
    "        \"\"\"\n",
    "        Initialize the pipeline.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        data_path : str, optional\n",
    "            Path to CSV file containing NEPSE data\n",
    "        data : pd.DataFrame, optional\n",
    "            Pre-loaded NEPSE data\n",
    "        \"\"\"\n",
    "        if data is not None:\n",
    "            self.raw_data = data.copy()\n",
    "        elif data_path is not None:\n",
    "            self.raw_data = pd.read_csv(data_path)\n",
    "        else:\n",
    "            raise ValueError(\"Either data_path or data must be provided\")\n",
    "        \n",
    "        # Pipeline state\n",
    "        self.cleaned_data = None\n",
    "        self.feature_data = None\n",
    "        self.model = None\n",
    "        self.scaler = None\n",
    "        self.pipeline_results = {}\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 1: DATA COLLECTION (Already done - data is loaded)\n",
    "    # ============================================================\n",
    "    \n",
    "    def validate_raw_data(self):\n",
    "        \"\"\"\n",
    "        Validate that the raw data meets minimum requirements.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Validation results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 1: DATA VALIDATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        required_columns = ['S.No', 'Symbol', 'Open', 'High', 'Low', 'Close']\n",
    "        \n",
    "        validation = {\n",
    "            'total_records': len(self.raw_data),\n",
    "            'unique_symbols': self.raw_data['Symbol'].nunique(),\n",
    "            'columns_present': True,\n",
    "            'missing_columns': [],\n",
    "            'data_types_ok': True\n",
    "        }\n",
    "        \n",
    "        # Check required columns\n",
    "        for col in required_columns:\n",
    "            if col not in self.raw_data.columns:\n",
    "                validation['columns_present'] = False\n",
    "                validation['missing_columns'].append(col)\n",
    "        \n",
    "        # Report\n",
    "        print(f\"âœ“ Total records: {validation['total_records']:,}\")\n",
    "        print(f\"âœ“ Unique stocks: {validation['unique_symbols']}\")\n",
    "        \n",
    "        if validation['columns_present']:\n",
    "            print(\"âœ“ All required columns present\")\n",
    "        else:\n",
    "            print(f\"âš  Missing columns: {validation['missing_columns']}\")\n",
    "        \n",
    "        return validation\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 2: DATA CLEANING\n",
    "    # ============================================================\n",
    "    \n",
    "    def clean_data(self, symbol=None):\n",
    "        \"\"\"\n",
    "        Clean the NEPSE data.\n",
    "        \n",
    "        This step handles:\n",
    "        - Missing value detection and treatment\n",
    "        - Duplicate removal\n",
    "        - Data type conversion\n",
    "        - Basic outlier detection\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str, optional\n",
    "            Specific stock to clean. If None, cleans all data.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Cleaned data\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 2: DATA CLEANING\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Start with raw data\n",
    "        data = self.raw_data.copy()\n",
    "        \n",
    "        # Filter for specific symbol if provided\n",
    "        if symbol:\n",
    "            data = data[data['Symbol'] == symbol].copy()\n",
    "            print(f\"Processing symbol: {symbol}\")\n",
    "        \n",
    "        # Initial data shape\n",
    "        initial_shape = data.shape\n",
    "        print(f\"\\nInitial data shape: {initial_shape}\")\n",
    "        \n",
    "        # 2.1: Check for duplicates\n",
    "        duplicates = data.duplicated().sum()\n",
    "        if duplicates > 0:\n",
    "            data = data.drop_duplicates()\n",
    "            print(f\"âœ“ Removed {duplicates} duplicate rows\")\n",
    "        else:\n",
    "            print(\"âœ“ No duplicates found\")\n",
    "        \n",
    "        # 2.2: Handle missing values\n",
    "        missing = data.isnull().sum()\n",
    "        total_missing = missing.sum()\n",
    "        \n",
    "        if total_missing > 0:\n",
    "            print(f\"\\nâš  Total missing values: {total_missing}\")\n",
    "            print(\"Missing by column:\")\n",
    "            for col, count in missing[missing > 0].items():\n",
    "                print(f\"   {col}: {count}\")\n",
    "            \n",
    "            # Strategy: Forward fill for price columns, drop others\n",
    "            price_cols = ['Open', 'High', 'Low', 'Close', 'VWAP']\n",
    "            for col in price_cols:\n",
    "                if col in data.columns and data[col].isnull().any():\n",
    "                    data[col] = data[col].fillna(method='ffill')\n",
    "                    print(f\"   Filled {col} with forward fill\")\n",
    "        else:\n",
    "            print(\"âœ“ No missing values found\")\n",
    "        \n",
    "        # 2.3: Sort by serial number (time order)\n",
    "        data = data.sort_values('S.No').reset_index(drop=True)\n",
    "        print(\"âœ“ Data sorted chronologically\")\n",
    "        \n",
    "        # 2.4: Validate price consistency\n",
    "        # High should be >= Low, High >= Open, High >= Close, etc.\n",
    "        invalid_prices = (\n",
    "            (data['High'] < data['Low']) |\n",
    "            (data['High'] < data['Open']) |\n",
    "            (data['High'] < data['Close']) |\n",
    "            (data['Low'] > data['Open']) |\n",
    "            (data['Low'] > data['Close'])\n",
    "        ).sum()\n",
    "        \n",
    "        if invalid_prices > 0:\n",
    "            print(f\"âš  Found {invalid_prices} records with inconsistent prices\")\n",
    "        else:\n",
    "            print(\"âœ“ All price records are consistent\")\n",
    "        \n",
    "        # Final shape\n",
    "        final_shape = data.shape\n",
    "        print(f\"\\nFinal data shape: {final_shape}\")\n",
    "        print(f\"Records removed: {initial_shape[0] - final_shape[0]}\")\n",
    "        \n",
    "        self.cleaned_data = data\n",
    "        return data\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 3: FEATURE ENGINEERING\n",
    "    # ============================================================\n",
    "    \n",
    "    def engineer_features(self, lookback=10):\n",
    "        \"\"\"\n",
    "        Create features for the prediction model.\n",
    "        \n",
    "        Feature engineering transforms raw data into meaningful inputs\n",
    "        that capture patterns relevant for prediction.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        lookback : int\n",
    "            Number of historical days to create features from\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        pd.DataFrame : Data with engineered features\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 3: FEATURE ENGINEERING\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if self.cleaned_data is None:\n",
    "            print(\"âš  No cleaned data. Run clean_data() first.\")\n",
    "            return None\n",
    "        \n",
    "        data = self.cleaned_data.copy()\n",
    "        \n",
    "        # ========================================\n",
    "        # 3.1: Lag Features\n",
    "        # ========================================\n",
    "        # These capture the autoregressive nature of time series\n",
    "        print(\"\\nCreating lag features...\")\n",
    "        \n",
    "        for i in range(1, lookback + 1):\n",
    "            data[f'Close_Lag_{i}'] = data['Close'].shift(i)\n",
    "            # Also lag volume\n",
    "            data[f'Vol_Lag_{i}'] = data['Vol'].shift(i)\n",
    "        \n",
    "        print(f\"  âœ“ Created {lookback * 2} lag features\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 3.2: Rolling Window Features\n",
    "        # ========================================\n",
    "        # These capture trends and volatility over windows\n",
    "        print(\"\\nCreating rolling window features...\")\n",
    "        \n",
    "        windows = [5, 10, 20]\n",
    "        for window in windows:\n",
    "            # Rolling mean (trend)\n",
    "            data[f'Close_MA_{window}'] = data['Close'].rolling(window).mean()\n",
    "            \n",
    "            # Rolling std (volatility)\n",
    "            data[f'Close_STD_{window}'] = data['Close'].rolling(window).std()\n",
    "            \n",
    "            # Rolling volume mean\n",
    "            data[f'Vol_MA_{window}'] = data['Vol'].rolling(window).mean()\n",
    "        \n",
    "        print(f\"  âœ“ Created {len(windows) * 3} rolling features\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 3.3: Price-Based Features\n",
    "        # ========================================\n",
    "        print(\"\\nCreating price-based features...\")\n",
    "        \n",
    "        # Daily return\n",
    "        data['Return'] = data['Close'].pct_change()\n",
    "        \n",
    "        # Price range (volatility indicator)\n",
    "        data['Price_Range'] = data['High'] - data['Low']\n",
    "        data['Price_Range_Pct'] = data['Price_Range'] / data['Close']\n",
    "        \n",
    "        # Gap (open vs previous close)\n",
    "        data['Gap'] = data['Open'] - data['Close'].shift(1)\n",
    "        data['Gap_Pct'] = data['Gap'] / data['Close'].shift(1)\n",
    "        \n",
    "        # VWAP deviation\n",
    "        data['VWAP_Deviation'] = (data['Close'] - data['VWAP']) / data['VWAP']\n",
    "        \n",
    "        print(\"  âœ“ Created 6 price-based features\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 3.4: Momentum Features\n",
    "        # ========================================\n",
    "        print(\"\\nCreating momentum features...\")\n",
    "        \n",
    "        # Rate of change at different periods\n",
    "        for period in [1, 5, 10]:\n",
    "            data[f'ROC_{period}'] = (\n",
    "                (data['Close'] - data['Close'].shift(period)) / \n",
    "                data['Close'].shift(period) * 100\n",
    "            )\n",
    "        \n",
    "        # Price position in 52-week range\n",
    "        data['Position_52W'] = (\n",
    "            (data['Close'] - data['52 Weeks Low']) /\n",
    "            (data['52 Weeks High'] - data['52 Weeks Low'])\n",
    "        )\n",
    "        \n",
    "        print(\"  âœ“ Created 4 momentum features\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 3.5: Target Variable\n",
    "        # ========================================\n",
    "        print(\"\\nCreating target variable...\")\n",
    "        \n",
    "        # Next day's close (for regression)\n",
    "        data['Target_Close'] = data['Close'].shift(-1)\n",
    "        \n",
    "        # Direction (for classification)\n",
    "        data['Target_Direction'] = (data['Close'].shift(-1) > data['Close']).astype(int)\n",
    "        \n",
    "        print(\"  âœ“ Created 2 target variables\")\n",
    "        \n",
    "        # ========================================\n",
    "        # 3.6: Clean and Report\n",
    "        # ========================================\n",
    "        initial_rows = len(data)\n",
    "        data = data.dropna()\n",
    "        final_rows = len(data)\n",
    "        \n",
    "        feature_columns = [col for col in data.columns \n",
    "                          if col not in ['S.No', 'Symbol', 'Conf.', 'LTP', \n",
    "                                         'Close - LTP', 'Close - LTP %',\n",
    "                                         'Prev. Close', 'Trans.', 'Diff',\n",
    "                                         'Diff %', 'Range %', 'VWAP %',\n",
    "                                         'Target_Close', 'Target_Direction']]\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        print(\"FEATURE ENGINEERING SUMMARY\")\n",
    "        print(\"-\" * 40)\n",
    "        print(f\"Total features created: {len(feature_columns)}\")\n",
    "        print(f\"Rows removed (NaN): {initial_rows - final_rows}\")\n",
    "        print(f\"Final dataset shape: {data.shape}\")\n",
    "        \n",
    "        self.feature_data = data\n",
    "        self.feature_columns = feature_columns\n",
    "        \n",
    "        return data\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 4: DATA SPLITTING\n",
    "    # ============================================================\n",
    "    \n",
    "    def split_data(self, test_size=0.2, val_size=0.1):\n",
    "        \"\"\"\n",
    "        Split data into train, validation, and test sets.\n",
    "        \n",
    "        CRITICAL: For time-series, we must preserve chronological order!\n",
    "        We cannot randomly shuffle data like typical ML problems.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        test_size : float\n",
    "            Proportion of data for testing\n",
    "        val_size : float\n",
    "            Proportion of training data for validation\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Dictionary containing X_train, X_val, X_test, y_train, y_val, y_test\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 4: DATA SPLITTING\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if self.feature_data is None:\n",
    "            print(\"âš  No feature data. Run engineer_features() first.\")\n",
    "            return None\n",
    "        \n",
    "        data = self.feature_data.copy()\n",
    "        \n",
    "        # Features and target\n",
    "        X = data[self.feature_columns]\n",
    "        y = data['Target_Close']\n",
    "        \n",
    "        # Calculate split indices\n",
    "        n = len(X)\n",
    "        test_idx = int(n * (1 - test_size))\n",
    "        train_idx = int(test_idx * (1 - val_size))\n",
    "        \n",
    "        # Split into train/val/test\n",
    "        X_train = X[:train_idx]\n",
    "        X_val = X[train_idx:test_idx]\n",
    "        X_test = X[test_idx:]\n",
    "        \n",
    "        y_train = y[:train_idx]\n",
    "        y_val = y[train_idx:test_idx]\n",
    "        y_test = y[test_idx:]\n",
    "        \n",
    "        print(f\"\\nData splits:\")\n",
    "        print(f\"  Training:   {len(X_train)} samples ({len(X_train)/n*100:.1f}%)\")\n",
    "        print(f\"  Validation: {len(X_val)} samples ({len(X_val)/n*100:.1f}%)\")\n",
    "        print(f\"  Test:       {len(X_test)} samples ({len(X_test)/n*100:.1f}%)\")\n",
    "        \n",
    "        print(f\"\\nâš  IMPORTANT: Data is NOT shuffled (time-series requirement)\")\n",
    "        print(f\"   Training uses earliest {train_idx} records\")\n",
    "        print(f\"   Validation uses records {train_idx} to {test_idx}\")\n",
    "        print(f\"   Test uses records {test_idx} onwards\")\n",
    "        \n",
    "        # Store splits\n",
    "        self.splits = {\n",
    "            'X_train': X_train, 'X_val': X_val, 'X_test': X_test,\n",
    "            'y_train': y_train, 'y_val': y_val, 'y_test': y_test\n",
    "        }\n",
    "        \n",
    "        return self.splits\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 5: FEATURE SCALING\n",
    "    # ============================================================\n",
    "    \n",
    "    def scale_features(self):\n",
    "        \"\"\"\n",
    "        Scale features to standardize their ranges.\n",
    "        \n",
    "        Feature scaling is important because:\n",
    "        - Features have different scales (Close ~500, Vol ~10000)\n",
    "        - Many ML algorithms work better with standardized features\n",
    "        - Gradient descent converges faster with scaled features\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Scaled train, validation, and test sets\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 5: FEATURE SCALING\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if self.splits is None:\n",
    "            print(\"âš  No data splits. Run split_data() first.\")\n",
    "            return None\n",
    "        \n",
    "        # Initialize scaler\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "        # Fit scaler on training data ONLY\n",
    "        # This prevents data leakage from validation/test sets\n",
    "        X_train_scaled = self.scaler.fit_transform(self.splits['X_train'])\n",
    "        \n",
    "        # Transform validation and test using training statistics\n",
    "        X_val_scaled = self.scaler.transform(self.splits['X_val'])\n",
    "        X_test_scaled = self.scaler.transform(self.splits['X_test'])\n",
    "        \n",
    "        print(\"âœ“ Features scaled using StandardScaler\")\n",
    "        print(\"  (fit on training data, applied to all sets)\")\n",
    "        \n",
    "        # Show example of scaling effect\n",
    "        example_feature = self.feature_columns[0]\n",
    "        example_idx = list(self.feature_columns).index(example_feature)\n",
    "        \n",
    "        original_mean = self.splits['X_train'][example_feature].mean()\n",
    "        original_std = self.splits['X_train'][example_feature].std()\n",
    "        \n",
    "        print(f\"\\nExample: {example_feature}\")\n",
    "        print(f\"  Original mean: {original_mean:.2f}\")\n",
    "        print(f\"  Original std:  {original_std:.2f}\")\n",
    "        print(f\"  Scaled mean:   {X_train_scaled[:, example_idx].mean():.4f}\")\n",
    "        print(f\"  Scaled std:    {X_train_scaled[:, example_idx].std():.4f}\")\n",
    "        \n",
    "        # Update splits\n",
    "        self.splits['X_train_scaled'] = X_train_scaled\n",
    "        self.splits['X_val_scaled'] = X_val_scaled\n",
    "        self.splits['X_test_scaled'] = X_test_scaled\n",
    "        \n",
    "        return self.splits\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 6: MODEL TRAINING\n",
    "    # ============================================================\n",
    "    \n",
    "    def train_model(self, model_type='linear'):\n",
    "        \"\"\"\n",
    "        Train a prediction model.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        model_type : str\n",
    "            Type of model to train ('linear', 'rf', 'xgboost')\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        model : Trained model\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 6: MODEL TRAINING\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if 'X_train_scaled' not in self.splits:\n",
    "            print(\"âš  No scaled data. Run scale_features() first.\")\n",
    "            return None\n",
    "        \n",
    "        from sklearn.linear_model import Ridge\n",
    "        from sklearn.ensemble import RandomForestRegressor\n",
    "        \n",
    "        # Select model\n",
    "        if model_type == 'linear':\n",
    "            self.model = Ridge(alpha=1.0)\n",
    "            print(\"\\nUsing Ridge Regression (L2 regularization)\")\n",
    "        elif model_type == 'rf':\n",
    "            self.model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "            print(\"\\nUsing Random Forest Regressor\")\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown model type: {model_type}\")\n",
    "        \n",
    "        # Train on training data\n",
    "        X_train = self.splits['X_train_scaled']\n",
    "        y_train = self.splits['y_train']\n",
    "        \n",
    "        self.model.fit(X_train, y_train)\n",
    "        print(f\"âœ“ Model trained on {len(X_train)} samples\")\n",
    "        \n",
    "        # Validate on validation set\n",
    "        X_val = self.splits['X_val_scaled']\n",
    "        y_val = self.splits['y_val']\n",
    "        \n",
    "        val_predictions = self.model.predict(X_val)\n",
    "        val_mae = mean_absolute_error(y_val, val_predictions)\n",
    "        \n",
    "        print(f\"\\nValidation MAE: {val_mae:.4f}\")\n",
    "        \n",
    "        return self.model\n",
    "    \n",
    "    # ============================================================\n",
    "    # STEP 7: MODEL EVALUATION\n",
    "    # ============================================================\n",
    "    \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Evaluate the trained model on test data.\n",
    "        \n",
    "        This step assesses how well the model generalizes to unseen data.\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Evaluation metrics\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STEP 7: MODEL EVALUATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        if self.model is None:\n",
    "            print(\"âš  No trained model. Run train_model() first.\")\n",
    "            return None\n",
    "        \n",
    "        from sklearn.metrics import (mean_squared_error, r2_score, \n",
    "                                     mean_absolute_percentage_error)\n",
    "        \n",
    "        # Test predictions\n",
    "        X_test = self.splits['X_test_scaled']\n",
    "        y_test = self.splits['y_test']\n",
    "        \n",
    "        predictions = self.model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        metrics = {\n",
    "            'MAE': mean_absolute_error(y_test, predictions),\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, predictions)),\n",
    "            'R2': r2_score(y_test, predictions),\n",
    "            'MAPE': mean_absolute_percentage_error(y_test, predictions) * 100\n",
    "        }\n",
    "        \n",
    "        print(\"\\nğŸ“Š Test Set Performance:\")\n",
    "        print(f\"   MAE:  {metrics['MAE']:.4f} NPR\")\n",
    "        print(f\"   RMSE: {metrics['RMSE']:.4f} NPR\")\n",
    "        print(f\"   RÂ²:   {metrics['R2']:.4f}\")\n",
    "        print(f\"   MAPE: {metrics['MAPE']:.2f}%\")\n",
    "        \n",
    "        # Direction accuracy\n",
    "        actual_direction = (y_test.values[1:] > y_test.values[:-1])\n",
    "        pred_direction = (predictions[1:] > predictions[:-1])\n",
    "        direction_accuracy = (actual_direction == pred_direction).mean()\n",
    "        \n",
    "        print(f\"\\nğŸ“ˆ Direction Accuracy: {direction_accuracy*100:.1f}%\")\n",
    "        print(f\"   (How often the model correctly predicts UP/DOWN)\")\n",
    "        \n",
    "        # Store results\n",
    "        self.pipeline_results['metrics'] = metrics\n",
    "        self.pipeline_results['predictions'] = predictions\n",
    "        self.pipeline_results['actual'] = y_test.values\n",
    "        \n",
    "        return metrics\n",
    "    \n",
    "    # ============================================================\n",
    "    # RUN COMPLETE PIPELINE\n",
    "    # ============================================================\n",
    "    \n",
    "    def run_full_pipeline(self, symbol, model_type='linear'):\n",
    "        \"\"\"\n",
    "        Execute the complete ML pipeline.\n",
    "        \n",
    "        This method runs all steps in sequence, demonstrating\n",
    "        the end-to-end workflow.\n",
    "        \n",
    "        Parameters:\n",
    "        -----------\n",
    "        symbol : str\n",
    "            Stock symbol to analyze\n",
    "        model_type : str\n",
    "            Type of model to train\n",
    "        \n",
    "        Returns:\n",
    "        --------\n",
    "        dict : Complete pipeline results\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"RUNNING COMPLETE ML PIPELINE\")\n",
    "        print(f\"Symbol: {symbol}\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Step 1: Validate\n",
    "        self.validate_raw_data()\n",
    "        \n",
    "        # Step 2: Clean\n",
    "        self.clean_data(symbol)\n",
    "        \n",
    "        # Step 3: Engineer features\n",
    "        self.engineer_features(lookback=10)\n",
    "        \n",
    "        # Step 4: Split\n",
    "        self.split_data()\n",
    "        \n",
    "        # Step 5: Scale\n",
    "        self.scale_features()\n",
    "        \n",
    "        # Step 6: Train\n",
    "        self.train_model(model_type)\n",
    "        \n",
    "        # Step 7: Evaluate\n",
    "        metrics = self.evaluate_model()\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PIPELINE COMPLETE\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        return self.pipeline_results\n",
    "\n",
    "\n",
    "# ============================================================\n",
    "# EXAMPLE USAGE\n",
    "# ============================================================\n",
    "\n",
    "# Create sample NEPSE data for demonstration\n",
    "def create_sample_nepse_data(n_records=500, n_symbols=5):\n",
    "    \"\"\"\n",
    "    Create synthetic NEPSE-like data for demonstration.\n",
    "    \n",
    "    This generates realistic-looking stock data that follows\n",
    "    typical market patterns.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    \n",
    "    symbols = ['ABL', 'ADBL', 'NABIL', 'NICA', 'SCB']\n",
    "    \n",
    "    all_data = []\n",
    "    \n",
    "    for symbol in symbols[:n_symbols]:\n",
    "        # Generate price series with random walk + trend\n",
    "        n = n_records\n",
    "        \n",
    "        # Starting price varies by symbol\n",
    "        base_prices = {'ABL': 500, 'ADBL': 600, 'NABIL': 800, 'NICA': 400, 'SCB': 700}\n",
    "        start_price = base_prices.get(symbol, 500)\n",
    "        \n",
    "        # Random walk with slight upward drift\n",
    "        returns = np.random.normal(0.0005, 0.02, n)\n",
    "        prices = start_price * np.cumprod(1 + returns)\n",
    "        \n",
    "        for i in range(n):\n",
    "            close = prices[i]\n",
    "            # High/Low around close\n",
    "            high = close * (1 + abs(np.random.normal(0, 0.01)))\n",
    "            low = close * (1 - abs(np.random.normal(0, 0.01)))\n",
    "            open_price = close * (1 + np.random.normal(0, 0.005))\n",
    "            \n",
    "            volume = int(np.random.lognormal(9, 0.5))\n",
    "            vwap = (high + low + close) / 3 * (1 + np.random.normal(0, 0.002))\n",
    "            \n",
    "            record = {\n",
    "                'S.No': i + 1,\n",
    "                'Symbol': symbol,\n",
    "                'Conf.': 'Standard',\n",
    "                'Open': round(open_price, 2),\n",
    "                'High': round(high, 2),\n",
    "                'Low': round(low, 2),\n",
    "                'Close': round(close, 2),\n",
    "                'LTP': round(close, 2),\n",
    "                'Close - LTP': 0.0,\n",
    "                'Close - LTP %': 0.0,\n",
    "                'VWAP': round(vwap, 2),\n",
    "                'Vol': volume,\n",
    "                'Prev. Close': round(prices[i-1] if i > 0 else start_price, 2),\n",
    "                'Turnover': int(volume * close),\n",
    "                'Trans.': int(volume * 0.1),\n",
    "                'Diff': round(close - prices[i-1] if i > 0 else 0, 2),\n",
    "                'Range': round(high - low, 2),\n",
    "                'Diff %': round((close - prices[i-1]) / prices[i-1] * 100 if i > 0 else 0, 2),\n",
    "                'Range %': round((high - low) / close * 100, 2),\n",
    "                'VWAP %': round((close - vwap) / vwap * 100, 2),\n",
    "                '52 Weeks High': round(close * 1.3, 2),\n",
    "                '52 Weeks Low': round(close * 0.7, 2)\n",
    "            }\n",
    "            all_data.append(record)\n",
    "    \n",
    "    df = pd.DataFrame(all_data)\n",
    "    return df\n",
    "\n",
    "\n",
    "# Generate sample data and demonstrate pipeline\n",
    "sample_data = create_sample_nepse_data(n_records=500, n_symbols=3)\n",
    "print(\"Sample NEPSE Data Generated:\")\n",
    "print(sample_data.head(10))\n",
    "\n",
    "# Run pipeline\n",
    "pipeline = NEPSEPipeline(data=sample_data)\n",
    "results = pipeline.run_full_pipeline(symbol='ABL', model_type='linear')\n",
    "```\n",
    "\n",
    "**Complete Pipeline Explanation**:\n",
    "\n",
    "This comprehensive pipeline demonstrates every critical step in building a time-series prediction system. Let me explain each component in detail:\n",
    "\n",
    "**Step 1 - Data Validation**:\n",
    "- Verifies that all required columns exist\n",
    "- Checks for basic data integrity\n",
    "- Reports on data volume and uniqueness\n",
    "\n",
    "**Step 2 - Data Cleaning**:\n",
    "- **Duplicate Removal**: Ensures each record is unique\n",
    "- **Missing Value Handling**: Uses forward-fill for prices (appropriate for time-series)\n",
    "- **Price Validation**: Ensures High â‰¥ Low, High â‰¥ Open, etc.\n",
    "- **Chronological Sorting**: Critical for time-series analysis\n",
    "\n",
    "**Step 3 - Feature Engineering**:\n",
    "- **Lag Features**: Capture autocorrelation (relationship with past values)\n",
    "- **Rolling Windows**: Capture trends and volatility over time\n",
    "- **Price-Based Features**: Capture intra-day patterns\n",
    "- **Momentum Features**: Capture rate of change and relative position\n",
    "\n",
    "**Step 4 - Data Splitting**:\n",
    "- Uses chronological split (not random!)\n",
    "- Creates separate train/validation/test sets\n",
    "- Explains why shuffling is wrong for time-series\n",
    "\n",
    "**Step 5 - Feature Scaling**:\n",
    "- Standardizes features to mean=0, std=1\n",
    "- Fits scaler on training data only (prevents leakage)\n",
    "- Applies same transformation to all sets\n",
    "\n",
    "**Step 6 - Model Training**:\n",
    "- Selects appropriate model\n",
    "- Trains on training data\n",
    "- Validates on validation data\n",
    "\n",
    "**Step 7 - Model Evaluation**:\n",
    "- Tests on held-out test data\n",
    "- Calculates multiple metrics\n",
    "- Reports direction accuracy (important for trading)\n",
    "\n",
    "---\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
