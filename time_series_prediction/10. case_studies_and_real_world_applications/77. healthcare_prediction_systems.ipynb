{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 77: Healthcare Prediction Systems**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the unique characteristics of healthcare data, including longitudinal patient records, high dimensionality, and missingness.\n",
    "- Identify and engineer features from structured electronic health records (EHR) and time\u2011series vital signs.\n",
    "- Address privacy concerns through anonymization, differential privacy, and secure computation.\n",
    "- Evaluate and mitigate algorithmic bias to ensure fair predictions across demographic groups.\n",
    "- Navigate regulatory frameworks such as HIPAA, GDPR, and FDA requirements for clinical decision support.\n",
    "- Design interpretable models to gain trust from clinicians and patients.\n",
    "- Implement a complete healthcare prediction pipeline that respects ethical and legal constraints.\n",
    "- Validate models using appropriate time\u2011based and patient\u2011based splits.\n",
    "- Deploy models in a clinical setting with continuous monitoring and feedback.\n",
    "\n",
    "---\n",
    "\n",
    "## **77.1 Introduction to Healthcare Prediction Systems**\n",
    "\n",
    "Healthcare prediction systems aim to forecast clinical outcomes\u2014such as disease progression, readmission risk, mortality, or treatment response\u2014using patient data. These systems can improve patient care, reduce costs, and assist clinicians in decision\u2011making. However, they also introduce profound ethical, legal, and technical challenges.\n",
    "\n",
    "Unlike stock prices or retail sales, healthcare data:\n",
    "\n",
    "- **Is highly sensitive**: Patient privacy is protected by law.\n",
    "- **Has complex structure**: Longitudinal records, irregular time intervals, multiple modalities (text, images, vitals).\n",
    "- **Exhibits strong class imbalance**: Rare events (e.g., mortality) are often the most important.\n",
    "- **Requires interpretability**: Clinicians must understand why a prediction is made to trust it.\n",
    "- **Must be fair**: Models should not discriminate based on race, gender, or socioeconomic status.\n",
    "\n",
    "In this chapter, we will build a simplified healthcare prediction system using synthetic electronic health record (EHR) data. The task will be to predict the risk of hospital readmission within 30 days after discharge. This is a common problem with clear clinical and financial implications.\n",
    "\n",
    "We will draw parallels with the NEPSE system where appropriate: both involve time\u2011series data, feature engineering, and model deployment. However, healthcare adds layers of privacy, fairness, and regulatory compliance that we must address.\n",
    "\n",
    "---\n",
    "\n",
    "## **77.2 Patient Data Features**\n",
    "\n",
    "Healthcare data comes from multiple sources:\n",
    "\n",
    "- **Demographics**: age, gender, race, ethnicity.\n",
    "- **Vital signs**: heart rate, blood pressure, temperature, respiratory rate (often irregularly sampled time series).\n",
    "- **Lab results**: blood tests, urine tests (sparse and irregular).\n",
    "- **Medications**: prescriptions, dosages, adherence.\n",
    "- **Diagnoses**: ICD\u201110 codes (categorical, multiple per visit).\n",
    "- **Procedures**: CPT codes.\n",
    "- **Notes**: unstructured clinical text (not covered here).\n",
    "\n",
    "For our example, we will generate synthetic EHR data for a cohort of patients. Each patient has:\n",
    "\n",
    "- A unique ID.\n",
    "- Demographic attributes.\n",
    "- A sequence of hospital visits (each visit has a date, diagnosis codes, procedures, and lab values).\n",
    "- Vital signs recorded during each visit (irregularly).\n",
    "\n",
    "We will then engineer features to predict 30\u2011day readmission.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "def generate_ehr_data(num_patients=1000, max_visits=10, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic EHR data for a cohort of patients.\n",
    "    \n",
    "    Returns two DataFrames:\n",
    "        - patients: static demographic info\n",
    "        - visits: each hospital visit with date, diagnoses, labs, vitals\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    \n",
    "    # Patient demographics\n",
    "    patients = []\n",
    "    for i in range(1, num_patients+1):\n",
    "        age = np.random.randint(18, 90)\n",
    "        gender = np.random.choice(['M', 'F'])\n",
    "        race = np.random.choice(['White', 'Black', 'Asian', 'Hispanic', 'Other'], p=[0.6, 0.15, 0.1, 0.1, 0.05])\n",
    "        socioeconomic_status = np.random.choice(['Low', 'Medium', 'High'], p=[0.3, 0.5, 0.2])\n",
    "        patients.append({\n",
    "            'patient_id': i,\n",
    "            'age': age,\n",
    "            'gender': gender,\n",
    "            'race': race,\n",
    "            'socioeconomic_status': socioeconomic_status\n",
    "        })\n",
    "    \n",
    "    patients_df = pd.DataFrame(patients)\n",
    "    \n",
    "    # Visit data\n",
    "    visits = []\n",
    "    start_date = datetime(2015, 1, 1)\n",
    "    for patient_id in range(1, num_patients+1):\n",
    "        num_visits = np.random.poisson(lam=3) + 1  # at least 1 visit\n",
    "        # Randomly decide if this patient will have a readmission event (for labeling)\n",
    "        # We'll generate a base readmission probability that depends on age and SES\n",
    "        p_readmit = 0.1 + 0.01 * (patients_df.loc[patient_id-1, 'age'] - 50) / 10\n",
    "        if patients_df.loc[patient_id-1, 'socioeconomic_status'] == 'Low':\n",
    "            p_readmit += 0.1\n",
    "        elif patients_df.loc[patient_id-1, 'socioeconomic_status'] == 'High':\n",
    "            p_readmit -= 0.05\n",
    "        p_readmit = np.clip(p_readmit, 0.05, 0.5)\n",
    "        \n",
    "        # Generate visits with increasing dates\n",
    "        visit_dates = []\n",
    "        current_date = start_date + timedelta(days=np.random.randint(0, 365))\n",
    "        for v in range(num_visits):\n",
    "            visit_dates.append(current_date)\n",
    "            # Next visit after random interval (days)\n",
    "            interval = np.random.exponential(scale=90)  # average 3 months\n",
    "            current_date = current_date + timedelta(days=int(interval))\n",
    "        \n",
    "        # Determine if any visit leads to readmission within 30 days\n",
    "        readmission_flag = 0\n",
    "        for i in range(len(visit_dates)-1):\n",
    "            if (visit_dates[i+1] - visit_dates[i]).days <= 30:\n",
    "                readmission_flag = 1\n",
    "                break\n",
    "        \n",
    "        # For each visit, generate data\n",
    "        for i, vdate in enumerate(visit_dates):\n",
    "            # Diagnoses (ICD-10 codes, simplified as categories)\n",
    "            num_dx = np.random.randint(1, 4)\n",
    "            dx_codes = [f\"DX{np.random.randint(100,999)}\" for _ in range(num_dx)]\n",
    "            dx_str = ','.join(dx_codes)\n",
    "            \n",
    "            # Procedures\n",
    "            num_proc = np.random.randint(0, 3)\n",
    "            proc_codes = [f\"CPT{np.random.randint(1000,9999)}\" for _ in range(num_proc)]\n",
    "            proc_str = ','.join(proc_codes)\n",
    "            \n",
    "            # Lab values (simulated)\n",
    "            labs = {\n",
    "                'glucose': np.random.normal(100, 20),\n",
    "                'creatinine': np.random.normal(1.0, 0.3),\n",
    "                'wbc': np.random.normal(8, 3),\n",
    "                'hb': np.random.normal(14, 2)\n",
    "            }\n",
    "            \n",
    "            # Vital signs\n",
    "            vitals = {\n",
    "                'heart_rate': np.random.normal(75, 15),\n",
    "                'sbp': np.random.normal(120, 15),\n",
    "                'dbp': np.random.normal(80, 10),\n",
    "                'temperature': np.random.normal(36.8, 0.5)\n",
    "            }\n",
    "            \n",
    "            # Outcome label: 1 if this visit is followed by a readmission within 30 days\n",
    "            # For the last visit, we don't know; we'll set to 0 for now\n",
    "            if i < len(visit_dates)-1 and (visit_dates[i+1] - vdate).days <= 30:\n",
    "                outcome = 1\n",
    "            else:\n",
    "                outcome = 0\n",
    "            \n",
    "            visit_record = {\n",
    "                'patient_id': patient_id,\n",
    "                'visit_date': vdate,\n",
    "                'diagnoses': dx_str,\n",
    "                'procedures': proc_str,\n",
    "                **labs,\n",
    "                **vitals,\n",
    "                'readmission_30d': outcome\n",
    "            }\n",
    "            visits.append(visit_record)\n",
    "    \n",
    "    visits_df = pd.DataFrame(visits)\n",
    "    return patients_df, visits_df\n",
    "\n",
    "# Generate data\n",
    "patients, visits = generate_ehr_data(num_patients=500, max_visits=8)\n",
    "print(\"Patients:\", patients.shape)\n",
    "print(\"Visits:\", visits.shape)\n",
    "print(visits.head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We generate two related tables: `patients` (static) and `visits` (longitudinal).\n",
    "- The readmission outcome is determined by whether the next visit occurs within 30 days.\n",
    "- Lab values and vitals are drawn from normal distributions (simplified).\n",
    "- In real data, you would have many more variables, irregular timing, and missing values.\n",
    "\n",
    "---\n",
    "\n",
    "## **77.3 Feature Engineering for EHR**\n",
    "\n",
    "Feature engineering for healthcare must handle:\n",
    "\n",
    "- **Longitudinal sequences**: past visits, time gaps, trends in vitals/labs.\n",
    "- **Categorical codes**: diagnoses and procedures (high cardinality, hierarchical).\n",
    "- **Irregular sampling**: lab tests are not done at every visit.\n",
    "- **Missing data**: common and often informative (e.g., a missing lab may indicate it wasn't ordered).\n",
    "\n",
    "We will engineer features for each visit, aggregating information from previous visits.\n",
    "\n",
    "```python\n",
    "class EHRFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering for EHR readmission prediction.\n",
    "    Features are created per visit.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_columns = []\n",
    "    \n",
    "    def add_demographics(self, df, patients_df):\n",
    "        \"\"\"Merge static patient demographics.\"\"\"\n",
    "        df = df.merge(patients_df, on='patient_id', how='left')\n",
    "        return df\n",
    "    \n",
    "    def add_time_since_last_visit(self, df):\n",
    "        \"\"\"Compute days since previous visit for each patient.\"\"\"\n",
    "        df = df.sort_values(['patient_id', 'visit_date'])\n",
    "        df['days_since_last_visit'] = df.groupby('patient_id')['visit_date'].diff().dt.days\n",
    "        return df\n",
    "    \n",
    "    def add_lag_features(self, df, variables, lags=[1]):\n",
    "        \"\"\"Add lagged values of variables from previous visit(s).\"\"\"\n",
    "        df = df.sort_values(['patient_id', 'visit_date'])\n",
    "        for var in variables:\n",
    "            for lag in lags:\n",
    "                df[f'{var}_lag_{lag}'] = df.groupby('patient_id')[var].shift(lag)\n",
    "        return df\n",
    "    \n",
    "    def add_rolling_features(self, df, variables, windows=[2, 3]):\n",
    "        \"\"\"Rolling statistics over past visits.\"\"\"\n",
    "        df = df.sort_values(['patient_id', 'visit_date'])\n",
    "        for var in variables:\n",
    "            for window in windows:\n",
    "                df[f'{var}_rolling_mean_{window}'] = df.groupby('patient_id')[var].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).mean().shift(1)  # exclude current\n",
    "                )\n",
    "                df[f'{var}_rolling_std_{window}'] = df.groupby('patient_id')[var].transform(\n",
    "                    lambda x: x.rolling(window, min_periods=1).std().shift(1)\n",
    "                )\n",
    "        return df\n",
    "    \n",
    "    def add_visit_count_features(self, df):\n",
    "        \"\"\"Cumulative count of visits and time since first visit.\"\"\"\n",
    "        df = df.sort_values(['patient_id', 'visit_date'])\n",
    "        df['visit_number'] = df.groupby('patient_id').cumcount() + 1\n",
    "        df['days_since_first_visit'] = df.groupby('patient_id')['visit_date'].transform(\n",
    "            lambda x: (x - x.iloc[0]).dt.days\n",
    "        )\n",
    "        return df\n",
    "    \n",
    "    def encode_diagnoses(self, df, top_k=20):\n",
    "        \"\"\"\n",
    "        Simplified diagnosis encoding: create dummy variables for the most frequent diagnosis codes.\n",
    "        In practice, you might use embeddings or hierarchical aggregation.\n",
    "        \"\"\"\n",
    "        # Split diagnosis strings into lists\n",
    "        df = df.copy()\n",
    "        df['dx_list'] = df['diagnoses'].str.split(',')\n",
    "        # Explode to get one row per diagnosis per visit\n",
    "        exploded = df[['patient_id', 'visit_date', 'dx_list']].explode('dx_list')\n",
    "        # Get top K diagnoses overall\n",
    "        top_dx = exploded['dx_list'].value_counts().head(top_k).index.tolist()\n",
    "        # Create dummies\n",
    "        for dx in top_dx:\n",
    "            df[f'dx_{dx}'] = df['diagnoses'].str.contains(dx, regex=False).astype(int)\n",
    "        return df\n",
    "    \n",
    "    def add_missing_indicators(self, df, variables):\n",
    "        \"\"\"Add binary flags for missing values (informative).\"\"\"\n",
    "        for var in variables:\n",
    "            df[f'{var}_missing'] = df[var].isna().astype(int)\n",
    "        return df\n",
    "    \n",
    "    def compute_features(self, visits_df, patients_df, target='readmission_30d'):\n",
    "        \"\"\"\n",
    "        Main entry point.\n",
    "        \"\"\"\n",
    "        df = visits_df.copy()\n",
    "        \n",
    "        # Merge demographics\n",
    "        df = self.add_demographics(df, patients_df)\n",
    "        \n",
    "        # Time features\n",
    "        df = self.add_time_since_last_visit(df)\n",
    "        df = self.add_visit_count_features(df)\n",
    "        \n",
    "        # Extract date features\n",
    "        df['visit_month'] = pd.to_datetime(df['visit_date']).dt.month\n",
    "        df['visit_dayofweek'] = pd.to_datetime(df['visit_date']).dt.dayofweek\n",
    "        \n",
    "        # Lab and vital features (numeric)\n",
    "        lab_vars = ['glucose', 'creatinine', 'wbc', 'hb']\n",
    "        vital_vars = ['heart_rate', 'sbp', 'dbp', 'temperature']\n",
    "        all_numeric = lab_vars + vital_vars\n",
    "        \n",
    "        # Add missing indicators\n",
    "        df = self.add_missing_indicators(df, all_numeric)\n",
    "        \n",
    "        # Lag features (using previous visit's values)\n",
    "        df = self.add_lag_features(df, all_numeric, lags=[1])\n",
    "        \n",
    "        # Rolling features over past 2 and 3 visits\n",
    "        df = self.add_rolling_features(df, all_numeric, windows=[2, 3])\n",
    "        \n",
    "        # Diagnosis encoding\n",
    "        df = self.encode_diagnoses(df, top_k=20)\n",
    "        \n",
    "        # Drop rows with NaN created by lags (first visit for each patient)\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Define target\n",
    "        df['target'] = df[target]\n",
    "        \n",
    "        # Store feature columns (excluding identifiers and target)\n",
    "        exclude = ['patient_id', 'visit_date', 'diagnoses', 'procedures', 'dx_list', target, 'target']\n",
    "        self.feature_columns = [c for c in df.columns if c not in exclude]\n",
    "        \n",
    "        return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Demographics are merged from the static table.\n",
    "- Time features capture the irregular spacing between visits.\n",
    "- Lag and rolling features use only past information (shift and rolling with `shift(1)` to exclude current visit).\n",
    "- Diagnosis encoding is simplified: we create dummy variables for the most common codes. In reality, you might use medical ontologies (e.g., CCS categories) or embeddings.\n",
    "- Missing indicators are added because missingness can be informative (e.g., a lab not ordered may indicate lower severity).\n",
    "- The target is readmission within 30 days.\n",
    "\n",
    "---\n",
    "\n",
    "## **77.4 Privacy Considerations**\n",
    "\n",
    "Healthcare data is protected by laws like HIPAA (US) and GDPR (Europe). Any prediction system must ensure:\n",
    "\n",
    "- **De\u2011identification**: Removal of direct identifiers (name, SSN, medical record number).\n",
    "- **Anonymization**: Ensuring that data cannot be re\u2011identified by combining with other sources.\n",
    "- **Access controls**: Only authorized personnel can access data.\n",
    "- **Audit trails**: All access is logged.\n",
    "- **Data minimization**: Only necessary data is collected and used.\n",
    "\n",
    "For machine learning, we also need to consider **differential privacy** \u2013 adding noise to training to prevent leakage of individual patient information.\n",
    "\n",
    "We'll demonstrate a simple implementation of **k\u2011anonymity** and **differential privacy** for aggregated statistics. For model training, we can use libraries like `diffprivlib` or `tensorflow/privacy`.\n",
    "\n",
    "```python\n",
    "# Example: k-anonymity check on patient demographics\n",
    "def check_k_anonymity(df, quasi_identifiers, k=5):\n",
    "    \"\"\"\n",
    "    Check if a dataset satisfies k-anonymity on given quasi-identifiers.\n",
    "    Quasi-identifiers: attributes that could be combined with external data to re-identify.\n",
    "    \"\"\"\n",
    "    group_sizes = df.groupby(quasi_identifiers).size()\n",
    "    if (group_sizes < k).any():\n",
    "        print(f\"Warning: k-anonymity violated. Minimum group size: {group_sizes.min()}\")\n",
    "        return False\n",
    "    else:\n",
    "        print(f\"k-anonymity satisfied (k={k})\")\n",
    "        return True\n",
    "\n",
    "# Example quasi-identifiers in our data\n",
    "quasi = ['age', 'gender', 'race']\n",
    "check_k_anonymity(patients, quasi, k=5)\n",
    "```\n",
    "\n",
    "For differential privacy, we might add Laplace noise to model gradients during training. Here's a conceptual snippet using TensorFlow Privacy:\n",
    "\n",
    "```python\n",
    "# Conceptual (requires tensorflow-privacy)\n",
    "# optimizer = DPKerasAdamOptimizer(\n",
    "#     l2_norm_clip=1.0,\n",
    "#     noise_multiplier=0.1,\n",
    "#     num_microbatches=256\n",
    "# )\n",
    "# model.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- k\u2011anonymity ensures that each combination of quasi\u2011identifiers appears at least k times, making it harder to single out an individual.\n",
    "- Differential privacy adds calibrated noise during training, providing a mathematical guarantee that the model does not memorize individual records.\n",
    "- In practice, you would work with your institution's privacy officer and use approved tools.\n",
    "\n",
    "---\n",
    "\n",
    "## **77.5 Model Fairness**\n",
    "\n",
    "Machine learning models can perpetuate or amplify biases present in the data. In healthcare, this is critical: a model that underpredicts risk for certain racial groups could lead to unequal care.\n",
    "\n",
    "Fairness metrics include:\n",
    "\n",
    "- **Demographic parity**: prediction rates are equal across groups.\n",
    "- **Equal opportunity**: true positive rates are equal.\n",
    "- **Predictive parity**: positive predictive values are equal.\n",
    "- **Individual fairness**: similar individuals receive similar predictions.\n",
    "\n",
    "We'll evaluate our model using these metrics and discuss mitigation strategies (reweighting, adversarial debiasing, etc.).\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def fairness_metrics(y_true, y_pred, sensitive_attr):\n",
    "    \"\"\"\n",
    "    Compute fairness metrics for binary classification.\n",
    "    sensitive_attr: array of group labels (e.g., race).\n",
    "    \"\"\"\n",
    "    groups = np.unique(sensitive_attr)\n",
    "    results = {}\n",
    "    for group in groups:\n",
    "        mask = sensitive_attr == group\n",
    "        yt = y_true[mask]\n",
    "        yp = y_pred[mask]\n",
    "        tn, fp, fn, tp = confusion_matrix(yt, yp).ravel()\n",
    "        \n",
    "        tpr = tp / (tp + fn) if (tp+fn)>0 else 0\n",
    "        tnr = tn / (tn + fp) if (tn+fp)>0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp+fp)>0 else 0\n",
    "        npv = tn / (tn + fn) if (tn+fn)>0 else 0\n",
    "        \n",
    "        results[group] = {\n",
    "            'size': len(yt),\n",
    "            'positive_rate': (yp==1).mean(),\n",
    "            'tpr': tpr,\n",
    "            'tnr': tnr,\n",
    "            'ppv': ppv,\n",
    "            'npv': npv\n",
    "        }\n",
    "    return results\n",
    "\n",
    "# After training, compute fairness across race groups\n",
    "# fairness = fairness_metrics(y_test, y_pred, test_df['race'])\n",
    "# print(fairness)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We compute key rates per group. If rates differ significantly, the model may be biased.\n",
    "- Mitigation can occur at data level (reweighting), algorithm level (constraints), or post\u2011processing (adjusting thresholds).\n",
    "\n",
    "---\n",
    "\n",
    "## **77.6 Regulatory Compliance**\n",
    "\n",
    "In the US, healthcare AI systems may be regulated by the FDA if they are intended for diagnosis or treatment. The FDA has issued guidance on **Clinical Decision Support (CDS)** software. Key considerations:\n",
    "\n",
    "- **Intended use**: Is the model providing a specific recommendation, or just information?\n",
    "- **Validation**: Prospective studies may be required.\n",
    "- **Transparency**: Users must understand the basis of recommendations.\n",
    "\n",
    "In Europe, the **Medical Device Regulation (MDR)** applies to many AI systems. Additionally, GDPR imposes strict rules on processing health data.\n",
    "\n",
    "For our system, we must document:\n",
    "\n",
    "- Data provenance and consent.\n",
    "- Model development process.\n",
    "- Validation results (including subgroup analyses).\n",
    "- Deployment and monitoring plan.\n",
    "\n",
    "We'll create a simple model card (as introduced by Mitchell et al.) to summarize these aspects.\n",
    "\n",
    "```python\n",
    "def generate_model_card(model_name, description, performance, fairness, limitations):\n",
    "    \"\"\"\n",
    "    Generate a markdown model card.\n",
    "    \"\"\"\n",
    "    card = f\"\"\"\n",
    "# Model Card: {model_name}\n",
    "\n",
    "## Model Description\n",
    "{description}\n",
    "\n",
    "## Performance\n",
    "- Overall AUC: {performance.get('auc', 'N/A')}\n",
    "- Accuracy: {performance.get('accuracy', 'N/A')}\n",
    "- Sensitivity: {performance.get('sensitivity', 'N/A')}\n",
    "- Specificity: {performance.get('specificity', 'N/A')}\n",
    "\n",
    "## Fairness Evaluation\n",
    "Fairness metrics by group:\n",
    "\"\"\"\n",
    "    for group, metrics in fairness.items():\n",
    "        card += f\"- {group}: TPR={metrics['tpr']:.2f}, PPV={metrics['ppv']:.2f}, rate={metrics['positive_rate']:.2f}\\n\"\n",
    "    \n",
    "    card += f\"\"\"\n",
    "## Limitations\n",
    "{limitations}\n",
    "\n",
    "## Intended Use\n",
    "This model is intended to assist clinicians in identifying patients at risk of 30\u2011day readmission. It does not replace clinical judgment.\n",
    "\"\"\"\n",
    "    return card\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **77.7 Clinical Deployment**\n",
    "\n",
    "Deploying a model in a clinical setting is more complex than a typical software deployment. It must integrate with electronic health record systems (EHRs), often via **HL7/FHIR** APIs. Predictions should be presented to clinicians at the point of care (e.g., within the EHR interface).\n",
    "\n",
    "Key steps:\n",
    "\n",
    "1. **Integration**: Use FHIR to fetch patient data in real time.\n",
    "2. **Inference**: Run the model on the fetched data (may be batch or on\u2011demand).\n",
    "3. **Presentation**: Display risk scores with explanations.\n",
    "4. **Feedback loop**: Collect clinician feedback and eventual outcomes to monitor and retrain.\n",
    "\n",
    "We'll sketch a simplified deployment as a REST API that accepts patient data and returns a risk score.\n",
    "\n",
    "```python\n",
    "from fastapi import FastAPI, HTTPException\n",
    "from pydantic import BaseModel\n",
    "import joblib\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "app = FastAPI(title=\"Readmission Risk API\")\n",
    "\n",
    "# Load model and feature engineer (assume saved)\n",
    "model = joblib.load(\"readmission_model.pkl\")\n",
    "feature_engineer = joblib.load(\"feature_engineer.pkl\")\n",
    "\n",
    "class PatientData(BaseModel):\n",
    "    patient_id: int\n",
    "    age: int\n",
    "    gender: str\n",
    "    race: str\n",
    "    socioeconomic_status: str\n",
    "    visits: list  # list of dicts with visit data\n",
    "\n",
    "class RiskScore(BaseModel):\n",
    "    patient_id: int\n",
    "    risk_score: float\n",
    "    risk_category: str  # e.g., Low, Medium, High\n",
    "\n",
    "@app.post(\"/predict\", response_model=RiskScore)\n",
    "def predict_risk(patient: PatientData):\n",
    "    # Convert input to DataFrame (simulate the visits table)\n",
    "    visits_df = pd.DataFrame(patient.visits)\n",
    "    # Add patient_id\n",
    "    visits_df['patient_id'] = patient.patient_id\n",
    "    # Add static demographics to each row (simplified)\n",
    "    patients_df = pd.DataFrame([{\n",
    "        'patient_id': patient.patient_id,\n",
    "        'age': patient.age,\n",
    "        'gender': patient.gender,\n",
    "        'race': patient.race,\n",
    "        'socioeconomic_status': patient.socioeconomic_status\n",
    "    }])\n",
    "    # Engineer features for the most recent visit (or all visits)\n",
    "    # For simplicity, assume we pass all visits and the engineer handles lags\n",
    "    featured = feature_engineer.compute_features(visits_df, patients_df)\n",
    "    # Take the latest visit for prediction\n",
    "    latest = featured.sort_values('visit_date').iloc[-1:]\n",
    "    X = latest[feature_engineer.feature_columns]\n",
    "    prob = model.predict_proba(X)[0, 1]\n",
    "    \n",
    "    # Categorize risk\n",
    "    if prob < 0.2:\n",
    "        cat = \"Low\"\n",
    "    elif prob < 0.5:\n",
    "        cat = \"Medium\"\n",
    "    else:\n",
    "        cat = \"High\"\n",
    "    \n",
    "    return RiskScore(patient_id=patient.patient_id, risk_score=prob, risk_category=cat)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The API accepts a patient's full history and returns a risk score.\n",
    "- In practice, the EHR would push data via FHIR, and we would store patient records in a database.\n",
    "- The model and feature engineer are loaded from disk (they must be saved after training).\n",
    "\n",
    "---\n",
    "\n",
    "## **77.8 Interpretability Requirements**\n",
    "\n",
    "Clinicians need to understand why a model made a prediction. Techniques include:\n",
    "\n",
    "- **Feature importance**: global (e.g., permutation importance) or local (SHAP, LIME).\n",
    "- **Counterfactual explanations**: what would need to change to alter the prediction?\n",
    "- **Rule\u2011based models**: decision trees or rule lists.\n",
    "\n",
    "We'll demonstrate SHAP for local explanations.\n",
    "\n",
    "```python\n",
    "import shap\n",
    "\n",
    "def explain_prediction(model, X_sample, feature_names):\n",
    "    \"\"\"\n",
    "    Generate SHAP explanation for a single prediction.\n",
    "    \"\"\"\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X_sample)\n",
    "    shap.force_plot(explainer.expected_value, shap_values[0,:], X_sample.iloc[0,:], feature_names=feature_names, matplotlib=True)\n",
    "    # Or summary plot\n",
    "    # shap.summary_plot(shap_values, X_sample, feature_names=feature_names)\n",
    "\n",
    "# After training, select a test sample\n",
    "# X_sample = X_test.iloc[[0]]\n",
    "# explain_prediction(model, X_sample, feature_engineer.feature_columns)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- SHAP values show the contribution of each feature to the prediction.\n",
    "- For a clinician, seeing that \"high creatinine\" increased the risk score can be clinically meaningful.\n",
    "- In deployment, you could return the top contributing factors along with the risk score.\n",
    "\n",
    "---\n",
    "\n",
    "## **77.9 Validation**\n",
    "\n",
    "Validation of healthcare models must go beyond simple train/test splits. Considerations:\n",
    "\n",
    "- **Temporal validation**: train on older data, test on newer data (to simulate real deployment).\n",
    "- **External validation**: test on data from a different institution.\n",
    "- **Subgroup validation**: ensure performance holds across age, gender, race groups.\n",
    "- **Calibration**: predicted probabilities should match observed frequencies.\n",
    "\n",
    "We'll implement a temporal split and calibration plot.\n",
    "\n",
    "```python\n",
    "from sklearn.calibration import calibration_curve\n",
    "\n",
    "def temporal_split(visits_df, patients_df, split_date):\n",
    "    \"\"\"\n",
    "    Split data by visit date: train on visits before split_date, test on after.\n",
    "    \"\"\"\n",
    "    train_visits = visits_df[visits_df['visit_date'] < split_date]\n",
    "    test_visits = visits_df[visits_df['visit_date'] >= split_date]\n",
    "    \n",
    "    # Patients may appear in both; we keep all patients but ensure no leakage from future visits\n",
    "    # In healthcare, it's common to split by patient or by date. Here we split by date.\n",
    "    # We need to ensure that for test visits, we don't use any future information in feature engineering.\n",
    "    # Our feature engineer already uses only past visits per patient, so it's safe.\n",
    "    return train_visits, test_visits\n",
    "\n",
    "# Example usage\n",
    "split = datetime(2022, 1, 1)\n",
    "train_visits, test_visits = temporal_split(visits, patients, split)\n",
    "\n",
    "# Engineer features separately (ensuring no cross-contamination)\n",
    "train_feat = feature_engineer.compute_features(train_visits, patients)\n",
    "test_feat = feature_engineer.compute_features(test_visits, patients)\n",
    "\n",
    "# Train on train_feat, evaluate on test_feat\n",
    "# ...\n",
    "\n",
    "# Calibration\n",
    "prob_true, prob_pred = calibration_curve(y_test, y_pred_proba, n_bins=10)\n",
    "plt.plot(prob_pred, prob_true, marker='o')\n",
    "plt.plot([0,1], [0,1], '--')\n",
    "plt.xlabel('Mean Predicted Probability')\n",
    "plt.ylabel('Observed Fraction')\n",
    "plt.title('Calibration Plot')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Temporal split is crucial because healthcare practices change over time; a model trained on old data may not generalize.\n",
    "- Calibration ensures that when the model says 30% risk, roughly 30% of such patients are readmitted.\n",
    "\n",
    "---\n",
    "\n",
    "## **77.10 Best Practices**\n",
    "\n",
    "Drawing from the above, best practices for healthcare prediction systems include:\n",
    "\n",
    "1. **Involve clinicians** throughout development to ensure clinical relevance.\n",
    "2. **Use appropriate data splits** (temporal, by patient) to avoid leakage.\n",
    "3. **Handle missing data** explicitly, not just imputation.\n",
    "4. **Evaluate fairness** across relevant subgroups and mitigate bias.\n",
    "5. **Ensure interpretability** \u2013 clinicians will not trust black boxes.\n",
    "6. **Maintain privacy** \u2013 de\u2011identify, consider differential privacy.\n",
    "7. **Comply with regulations** \u2013 document everything, involve legal/ethics board.\n",
    "8. **Monitor after deployment** \u2013 track performance drift and data shifts.\n",
    "9. **Plan for updates** \u2013 model retraining when new data arrives.\n",
    "10. **Communicate limitations** \u2013 be clear about what the model cannot do.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we built a healthcare prediction system for 30\u2011day readmission risk using synthetic EHR data. We engineered features from longitudinal visits, addressed privacy through anonymization and differential privacy concepts, evaluated fairness across demographic groups, and discussed regulatory compliance. We also implemented a deployment API and demonstrated interpretability with SHAP. Finally, we outlined validation strategies and best practices.\n",
    "\n",
    "The healthcare domain adds significant complexity beyond the NEPSE, retail, and weather systems we previously built, but the core pipeline remains similar: data ingestion, feature engineering, model training, validation, deployment, and monitoring. The key differentiators are the ethical and legal constraints that must be woven into every step.\n",
    "\n",
    "In the next chapter, we will explore **IoT and Sensor Analytics**, where data arrives in high\u2011velocity streams from distributed devices, requiring real\u2011time processing and edge deployment.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 77**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='76. weather_and_climate_prediction.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='78. iot_and_sensor_analytics.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}