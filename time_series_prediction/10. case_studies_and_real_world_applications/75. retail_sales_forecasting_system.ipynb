{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 75: Retail Sales Forecasting System**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the unique challenges of retail sales forecasting (multi‑location, promotions, seasonality, product hierarchies).\n",
    "- Adapt the time‑series pipeline developed for financial data (NEPSE) to retail datasets.\n",
    "- Engineer features specific to retail, such as promotional calendars, holiday effects, and store‑level attributes.\n",
    "- Handle multiple time series (stores, products) efficiently.\n",
    "- Implement a forecasting model that accounts for external factors (price changes, events).\n",
    "- Evaluate forecasts using metrics relevant to retail (bias, forecast value added).\n",
    "- Deploy and monitor a retail forecasting system in production.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.1 Introduction to Retail Sales Forecasting**\n",
    "\n",
    "Retail sales forecasting is a critical business function that drives inventory management, staffing, supply chain logistics, and financial planning. Unlike financial time series (e.g., stock prices), retail data exhibits several distinctive characteristics:\n",
    "\n",
    "- **Hierarchical structure**: Sales can be aggregated by product category, store, region, or channel.\n",
    "- **Strong seasonality**: Weekly patterns (weekend vs weekday), yearly patterns (holidays, seasons), and special events (Black Friday, Christmas).\n",
    "- **Promotions and markdowns**: Temporary price reductions or marketing campaigns cause spikes that are difficult to model without external data.\n",
    "- **External factors**: Weather, local events, competitor actions.\n",
    "- **Multiple interrelated time series**: Sales of one product may be affected by stockouts of another (cannibalisation or complementarity).\n",
    "\n",
    "The NEPSE prediction system we built is a single‑series (or multi‑symbol but each stock independent) model. For retail, we often need to model hundreds or thousands of series simultaneously, sharing information across them.\n",
    "\n",
    "In this chapter, we will build a retail sales forecasting system for a hypothetical chain of stores. We will reuse many components from the NEPSE system: data ingestion, feature engineering, model training, backtesting, and deployment. The key differences lie in the feature set and the modeling approach.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.2 Understanding Retail Data**\n",
    "\n",
    "A typical retail dataset might include:\n",
    "\n",
    "- **Sales data** (daily or weekly): store ID, product ID, date, units sold, revenue.\n",
    "- **Product attributes**: category, price, cost, promotion flag.\n",
    "- **Store attributes**: location, size, type (e.g., mall, standalone).\n",
    "- **Calendar**: holidays, promotional periods, events.\n",
    "- **External data**: weather (temperature, precipitation), economic indicators.\n",
    "\n",
    "For our example, we will use a synthetic dataset that mimics a real retail chain. The data will be in CSV format with the following columns:\n",
    "\n",
    "- `store_id`: unique store identifier\n",
    "- `product_id`: unique product identifier\n",
    "- `date`: date of sale\n",
    "- `sales`: number of units sold\n",
    "- `price`: selling price (may vary due to promotions)\n",
    "- `promotion`: binary flag (1 if product on promotion that day)\n",
    "- `holiday`: binary flag for public holiday\n",
    "- `temperature`: daily average temperature at store location\n",
    "- `weekday`: day of week (0=Monday, ..., 6=Sunday)\n",
    "\n",
    "We will generate this data for multiple stores and products.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "import random\n",
    "\n",
    "def generate_retail_data(num_stores=5, num_products=10, days=365*2):\n",
    "    \"\"\"\n",
    "    Generate synthetic retail sales data.\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    start_date = datetime(2022, 1, 1)\n",
    "    dates = [start_date + timedelta(days=i) for i in range(days)]\n",
    "    \n",
    "    records = []\n",
    "    for store in range(1, num_stores+1):\n",
    "        store_base = np.random.uniform(50, 200)  # base sales level\n",
    "        store_trend = np.random.uniform(-0.01, 0.01)  # daily trend\n",
    "        for product in range(1, num_products+1):\n",
    "            product_popularity = np.random.uniform(0.5, 2.0)\n",
    "            # Weekly seasonality\n",
    "            weekly_pattern = np.array([0.8, 0.7, 0.9, 1.0, 1.2, 1.5, 1.1])  # Monday low, weekend high\n",
    "            \n",
    "            for date in dates:\n",
    "                day_of_week = date.weekday()\n",
    "                # Base sales with trend and seasonality\n",
    "                base = store_base * (1 + store_trend * (date - start_date).days) * weekly_pattern[day_of_week] * product_popularity\n",
    "                \n",
    "                # Promotion effect: increase sales by 20-50%\n",
    "                promotion = 1 if np.random.random() < 0.1 else 0\n",
    "                promo_mult = 1.3 if promotion else 1.0\n",
    "                \n",
    "                # Holiday effect: maybe higher sales\n",
    "                holiday = 1 if date.weekday() in [5,6] and np.random.random() < 0.2 else 0  # simplified\n",
    "                holiday_mult = 1.2 if holiday else 1.0\n",
    "                \n",
    "                # Temperature effect (e.g., cold weather increases sales of warm products)\n",
    "                temp = np.random.normal(15, 10)\n",
    "                temp_effect = 1 + 0.01 * (temp - 15)  # slight effect\n",
    "                \n",
    "                # Random noise\n",
    "                noise = np.random.normal(1, 0.1)\n",
    "                \n",
    "                sales = base * promo_mult * holiday_mult * temp_effect * noise\n",
    "                sales = max(0, int(sales))\n",
    "                \n",
    "                records.append({\n",
    "                    'store_id': store,\n",
    "                    'product_id': product,\n",
    "                    'date': date,\n",
    "                    'sales': sales,\n",
    "                    'price': np.random.uniform(10, 100),\n",
    "                    'promotion': promotion,\n",
    "                    'holiday': holiday,\n",
    "                    'temperature': temp,\n",
    "                    'weekday': day_of_week\n",
    "                })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# Generate data\n",
    "retail_df = generate_retail_data(num_stores=3, num_products=5, days=365)\n",
    "print(retail_df.head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This function generates a realistic retail dataset with multiple stores and products.\n",
    "- It incorporates several factors: base store level, product popularity, weekly seasonality, promotions, holidays, temperature, and random noise.\n",
    "- The data can be saved as CSV and used for the rest of the pipeline.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.3 Adapting the Data Ingestion Pipeline**\n",
    "\n",
    "We can reuse the `NEPSEIngestion` class from Chapter 74, but we need to handle multiple identifiers (store, product) and possibly different time frequencies. For retail, we might have daily or weekly data. We'll create a generic `RetailIngestion` class.\n",
    "\n",
    "```python\n",
    "from pathlib import Path\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class RetailIngestion:\n",
    "    \"\"\"\n",
    "    Ingests retail sales data from CSV.\n",
    "    \"\"\"\n",
    "    def __init__(self, data_dir: str = \"./data/retail\"):\n",
    "        self.data_dir = Path(data_dir)\n",
    "        self.data_dir.mkdir(parents=True, exist_ok=True)\n",
    "    \n",
    "    def load_csv(self, file_path: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load retail CSV and perform basic validation.\n",
    "        Expected columns: store_id, product_id, date, sales, price, promotion, holiday, temperature, weekday\n",
    "        \"\"\"\n",
    "        logger.info(f\"Loading retail data from {file_path}\")\n",
    "        df = pd.read_csv(file_path, parse_dates=['date'])\n",
    "        \n",
    "        required = ['store_id', 'product_id', 'date', 'sales']\n",
    "        missing = [c for c in required if c not in df.columns]\n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing columns: {missing}\")\n",
    "        \n",
    "        # Ensure date is datetime and sort\n",
    "        df['date'] = pd.to_datetime(df['date'])\n",
    "        df = df.sort_values(['store_id', 'product_id', 'date'])\n",
    "        \n",
    "        logger.info(f\"Loaded {len(df)} records from {df['date'].min()} to {df['date'].max()}\")\n",
    "        return df\n",
    "    \n",
    "    def save_raw(self, df: pd.DataFrame, filename: str = None):\n",
    "        if filename is None:\n",
    "            filename = f\"retail_raw_{datetime.now().strftime('%Y%m%d')}.parquet\"\n",
    "        path = self.data_dir / filename\n",
    "        df.to_parquet(path, index=False)\n",
    "        logger.info(f\"Saved raw data to {path}\")\n",
    "        return path\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The ingestion is similar to NEPSE's, but we now expect `store_id` and `product_id` as key columns.\n",
    "- Data is sorted by store, product, and date to ensure correct time ordering for each time series.\n",
    "- We save as Parquet for efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.4 Feature Engineering for Retail**\n",
    "\n",
    "Retail feature engineering must capture patterns across multiple dimensions. We will create:\n",
    "\n",
    "- **Temporal features**: day of week, month, quarter, holiday flags, days to nearest holiday.\n",
    "- **Lagged features**: past sales for the same store/product.\n",
    "- **Rolling statistics**: moving averages, standard deviations over various windows (7, 14, 30 days).\n",
    "- **Promotion features**: promotion flag, days since last promotion, promotion intensity (price discount).\n",
    "- **Store/product attributes**: average price, category popularity.\n",
    "- **External features**: temperature (lagged and rolling), weather conditions.\n",
    "\n",
    "We'll also create features that combine information across products (e.g., total store sales) to capture substitution effects.\n",
    "\n",
    "```python\n",
    "class RetailFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering for retail sales data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_columns = []\n",
    "    \n",
    "    def compute_features(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Main feature engineering function.\n",
    "        Expects columns: store_id, product_id, date, sales, price, promotion, holiday, temperature, weekday\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        \n",
    "        # Ensure sorted\n",
    "        df = df.sort_values(['store_id', 'product_id', 'date'])\n",
    "        \n",
    "        # --- Temporal features ---\n",
    "        df['year'] = df['date'].dt.year\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['day'] = df['date'].dt.day\n",
    "        df['day_of_week'] = df['date'].dt.dayofweek\n",
    "        df['week_of_year'] = df['date'].dt.isocalendar().week\n",
    "        df['quarter'] = df['date'].dt.quarter\n",
    "        df['is_weekend'] = (df['day_of_week'] >= 5).astype(int)\n",
    "        \n",
    "        # Holiday flags (assuming holiday column already exists)\n",
    "        # Also create days before/after holiday\n",
    "        df['days_to_holiday'] = df.groupby('store_id')['holiday'].transform(\n",
    "            lambda x: (x.rolling(window=30, min_periods=1).apply(lambda s: (s.index[s].max() - s.index[-1]).days if s.any() else np.nan))\n",
    "        )  # Simplified; better to compute using actual holiday calendar\n",
    "        \n",
    "        # --- Lag features (per store and product) ---\n",
    "        for lag in [1, 2, 3, 7, 14, 28]:\n",
    "            df[f'sales_lag_{lag}'] = df.groupby(['store_id', 'product_id'])['sales'].shift(lag)\n",
    "            df[f'price_lag_{lag}'] = df.groupby(['store_id', 'product_id'])['price'].shift(lag)\n",
    "            df[f'promotion_lag_{lag}'] = df.groupby(['store_id', 'product_id'])['promotion'].shift(lag)\n",
    "        \n",
    "        # --- Rolling statistics (per store and product) ---\n",
    "        windows = [7, 14, 28, 56]\n",
    "        for window in windows:\n",
    "            # Sales rolling stats\n",
    "            df[f'sales_rolling_mean_{window}'] = df.groupby(['store_id', 'product_id'])['sales'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).mean()\n",
    "            )\n",
    "            df[f'sales_rolling_std_{window}'] = df.groupby(['store_id', 'product_id'])['sales'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).std()\n",
    "            )\n",
    "            df[f'sales_rolling_min_{window}'] = df.groupby(['store_id', 'product_id'])['sales'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).min()\n",
    "            )\n",
    "            df[f'sales_rolling_max_{window}'] = df.groupby(['store_id', 'product_id'])['sales'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).max()\n",
    "            )\n",
    "            \n",
    "            # Promotion rolling (percentage of days on promotion)\n",
    "            df[f'promotion_rate_{window}'] = df.groupby(['store_id', 'product_id'])['promotion'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).mean()\n",
    "            )\n",
    "            \n",
    "            # Price rolling\n",
    "            df[f'price_rolling_mean_{window}'] = df.groupby(['store_id', 'product_id'])['price'].transform(\n",
    "                lambda x: x.rolling(window, min_periods=1).mean()\n",
    "            )\n",
    "        \n",
    "        # --- Store-level features (aggregated across products) ---\n",
    "        # Total store sales\n",
    "        df['store_total_sales'] = df.groupby(['store_id', 'date'])['sales'].transform('sum')\n",
    "        # Store average price\n",
    "        df['store_avg_price'] = df.groupby(['store_id', 'date'])['price'].transform('mean')\n",
    "        # Number of products sold (as proxy for assortment)\n",
    "        df['store_products_sold'] = df.groupby(['store_id', 'date'])['product_id'].transform('nunique')\n",
    "        \n",
    "        # --- Product-level features (across stores) ---\n",
    "        # Product total sales across all stores\n",
    "        df['product_total_sales'] = df.groupby(['product_id', 'date'])['sales'].transform('sum')\n",
    "        # Product average price\n",
    "        df['product_avg_price'] = df.groupby(['product_id', 'date'])['price'].transform('mean')\n",
    "        \n",
    "        # --- Interaction features ---\n",
    "        # Sales * promotion (maybe use as separate feature)\n",
    "        df['sales_promo_interaction'] = df['sales'] * df['promotion']\n",
    "        # Price * promotion\n",
    "        df['price_promo'] = df['price'] * df['promotion']\n",
    "        \n",
    "        # --- External features (weather) ---\n",
    "        # Could include rolling weather stats\n",
    "        df['temp_rolling_mean_7'] = df.groupby(['store_id'])['temperature'].transform(\n",
    "            lambda x: x.rolling(7, min_periods=1).mean()\n",
    "        )\n",
    "        \n",
    "        # --- Target: next day sales (for supervised learning) ---\n",
    "        df['target_sales'] = df.groupby(['store_id', 'product_id'])['sales'].shift(-1)\n",
    "        \n",
    "        # Drop rows with NaN created by shifts\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Store feature columns (exclude identifiers and target)\n",
    "        exclude = ['store_id', 'product_id', 'date', 'sales', 'target_sales']\n",
    "        self.feature_columns = [c for c in df.columns if c not in exclude]\n",
    "        \n",
    "        logger.info(f\"Computed {len(self.feature_columns)} features\")\n",
    "        return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The feature engineering is per store‑product pair, using `groupby` to ensure correct lags and rolling statistics within each time series.\n",
    "- We create both product‑specific features (lagged sales, rolling statistics) and aggregate features (store total sales, product total across stores) to capture cross‑series dependencies.\n",
    "- The target is next‑day sales (`shift(-1)`).\n",
    "- This approach is computationally intensive for many series; in production, you would use distributed computing (e.g., Spark, Dask) or a feature store that pre‑computes these.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.5 Model Selection and Training**\n",
    "\n",
    "For retail, we often need to forecast thousands of time series. A common approach is to use a global model that trains on all series simultaneously, learning shared patterns. We can use tree‑based models like XGBoost or LightGBM, or deep learning models like DeepAR (Amazon) or N‑BEATS. Here we'll use LightGBM, which is fast and handles categorical features well.\n",
    "\n",
    "We'll also incorporate categorical features like store_id, product_id, month, day_of_week, etc. LightGBM can handle these natively.\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "import numpy as np\n",
    "\n",
    "class RetailForecaster:\n",
    "    \"\"\"\n",
    "    Global forecasting model using LightGBM.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_columns, categorical_features=None):\n",
    "        self.feature_columns = feature_columns\n",
    "        self.categorical_features = categorical_features if categorical_features else []\n",
    "        self.model = None\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        X = df[self.feature_columns]\n",
    "        y = df['target_sales']\n",
    "        # Convert categorical columns to category dtype\n",
    "        for col in self.categorical_features:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].astype('category')\n",
    "        return X, y\n",
    "    \n",
    "    def train_with_cv(self, X, y, n_splits=5):\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        cv_scores = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=self.categorical_features)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=self.categorical_features, reference=train_data)\n",
    "            \n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'mae',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.05,\n",
    "                'feature_fraction': 0.9,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': -1\n",
    "            }\n",
    "            \n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                valid_sets=[val_data],\n",
    "                num_boost_round=1000,\n",
    "                callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            rmse = np.sqrt(mean_squared_error(y_val, y_pred))\n",
    "            cv_scores.append({'fold': fold, 'mae': mae, 'rmse': rmse, 'best_iteration': model.best_iteration})\n",
    "            print(f\"Fold {fold}: MAE={mae:.2f}, RMSE={rmse:.2f}, best_iter={model.best_iteration}\")\n",
    "        \n",
    "        # Select best model (lowest validation MAE)\n",
    "        best_fold = min(cv_scores, key=lambda x: x['mae'])\n",
    "        print(f\"Best fold: {best_fold['fold']} with MAE={best_fold['mae']:.2f}\")\n",
    "        \n",
    "        # Retrain on full data with best iteration\n",
    "        self.model = lgb.LGBMRegressor(\n",
    "            objective='regression',\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.05,\n",
    "            feature_fraction=0.9,\n",
    "            bagging_fraction=0.8,\n",
    "            bagging_freq=5,\n",
    "            n_estimators=best_fold['best_iteration'],\n",
    "            verbose=-1\n",
    "        )\n",
    "        self.model.fit(X, y, categorical_feature=self.categorical_features)\n",
    "        \n",
    "        return cv_scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The forecaster uses LightGBM with early stopping on time‑series cross‑validation folds.\n",
    "- `categorical_features` can include `store_id`, `product_id`, `month`, `day_of_week`, etc., which LightGBM handles internally.\n",
    "- After CV, we retrain on the full dataset using the best number of iterations found in the best fold.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.6 Backtesting for Retail**\n",
    "\n",
    "Backtesting in retail is similar to financial backtesting: we simulate how the forecast would have performed historically. However, instead of trading, we evaluate the forecast error over time and possibly simulate inventory decisions.\n",
    "\n",
    "We can reuse the `NEPSEBacktester` with modifications for retail metrics. Important retail metrics include:\n",
    "\n",
    "- **Forecast bias**: average error (should be near zero).\n",
    "- **Forecast accuracy**: MAE, RMSE, MAPE.\n",
    "- **Forecast value added**: comparison with naive benchmarks (e.g., seasonal naive).\n",
    "- **Pinball loss** for quantile forecasts (if we produce prediction intervals).\n",
    "\n",
    "We'll create a simple backtester that rolls through time, training on expanding window and predicting the next day.\n",
    "\n",
    "```python\n",
    "class RetailBacktester:\n",
    "    \"\"\"\n",
    "    Walk‑forward backtesting for retail forecasts.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, forecaster, feature_columns, categorical_features, \n",
    "                 initial_train_days=365, step_days=1):\n",
    "        self.forecaster = forecaster\n",
    "        self.feature_columns = feature_columns\n",
    "        self.categorical_features = categorical_features\n",
    "        self.initial_train_days = initial_train_days\n",
    "        self.step_days = step_days\n",
    "    \n",
    "    def run(self, df):\n",
    "        \"\"\"\n",
    "        df must be sorted by date.\n",
    "        \"\"\"\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        unique_dates = df['date'].unique()\n",
    "        \n",
    "        results = []\n",
    "        \n",
    "        for i in range(self.initial_train_days, len(unique_dates), self.step_days):\n",
    "            train_end_date = unique_dates[i-1]\n",
    "            test_date = unique_dates[i]\n",
    "            \n",
    "            # Training data: up to train_end_date\n",
    "            train_df = df[df['date'] <= train_end_date]\n",
    "            # Test data: exactly test_date\n",
    "            test_df = df[df['date'] == test_date]\n",
    "            \n",
    "            if test_df.empty:\n",
    "                continue\n",
    "            \n",
    "            # Prepare training data\n",
    "            X_train, y_train = self.forecaster.prepare_data(train_df)\n",
    "            # Train a new model on this expanding window\n",
    "            model = lgb.LGBMRegressor(\n",
    "                objective='regression',\n",
    "                num_leaves=31,\n",
    "                learning_rate=0.05,\n",
    "                feature_fraction=0.9,\n",
    "                bagging_fraction=0.8,\n",
    "                bagging_freq=5,\n",
    "                n_estimators=100,  # we'll use early stopping on a validation set\n",
    "                verbose=-1\n",
    "            )\n",
    "            # Use last 30 days as validation for early stopping\n",
    "            val_df = train_df[train_df['date'] > train_df['date'].max() - pd.Timedelta(days=30)]\n",
    "            X_val, y_val = self.forecaster.prepare_data(val_df)\n",
    "            model.fit(\n",
    "                X_train, y_train,\n",
    "                eval_set=[(X_val, y_val)],\n",
    "                callbacks=[lgb.early_stopping(10)],\n",
    "                categorical_feature=self.categorical_features\n",
    "            )\n",
    "            \n",
    "            # Predict on test\n",
    "            X_test, _ = self.forecaster.prepare_data(test_df)\n",
    "            y_pred = model.predict(X_test)\n",
    "            \n",
    "            # Store results\n",
    "            test_df = test_df.copy()\n",
    "            test_df['predicted_sales'] = y_pred\n",
    "            test_df['error'] = test_df['predicted_sales'] - test_df['target_sales']\n",
    "            test_df['abs_error'] = test_df['error'].abs()\n",
    "            test_df['train_end_date'] = train_end_date\n",
    "            results.append(test_df[['store_id', 'product_id', 'date', 'target_sales', 'predicted_sales', 'error', 'abs_error']])\n",
    "        \n",
    "        results_df = pd.concat(results, ignore_index=True)\n",
    "        return results_df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This backtester iterates through time, expanding the training window and predicting one day ahead.\n",
    "- For each step, it trains a new LightGBM model with early stopping on a recent validation set.\n",
    "- The results include actuals and predictions, which can be used to compute overall metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.7 Deployment and Monitoring**\n",
    "\n",
    "Deployment for retail is similar to the NEPSE system: we can have a batch job that runs daily to produce forecasts for all stores/products, and possibly a REST API for ad‑hoc queries.\n",
    "\n",
    "### **75.7.1 Batch Prediction**\n",
    "\n",
    "We can create a batch predictor that loads the latest features and generates forecasts for the next day.\n",
    "\n",
    "```python\n",
    "class RetailBatchPredictor:\n",
    "    def __init__(self, model, feature_columns, categorical_features, feature_store):\n",
    "        self.model = model\n",
    "        self.feature_columns = feature_columns\n",
    "        self.categorical_features = categorical_features\n",
    "        self.feature_store = feature_store\n",
    "    \n",
    "    def predict_next_day(self, current_date):\n",
    "        \"\"\"\n",
    "        Predict sales for date = current_date + 1 day.\n",
    "        We need features up to current_date.\n",
    "        \"\"\"\n",
    "        # Load features up to current_date\n",
    "        # In practice, we would have a feature store that can retrieve the latest feature vectors\n",
    "        # Here we assume we have a DataFrame with the latest features for each store-product\n",
    "        # This is a placeholder.\n",
    "        pass\n",
    "```\n",
    "\n",
    "### **75.7.2 Monitoring**\n",
    "\n",
    "Monitoring for retail should track forecast error over time, detect data drift (e.g., changes in promotion effectiveness), and alert when accuracy drops below thresholds. We can reuse the `ModelMonitor` class from Chapter 74, adapting it to store‑product level metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.8 Case Study: Implementation with Synthetic Data**\n",
    "\n",
    "Let's put it all together with our synthetic retail data.\n",
    "\n",
    "```python\n",
    "# Generate data\n",
    "retail_df = generate_retail_data(num_stores=3, num_products=5, days=365*2)\n",
    "\n",
    "# Ingest (save raw)\n",
    "ingestor = RetailIngestion()\n",
    "ingestor.save_raw(retail_df, \"retail_sample.parquet\")\n",
    "\n",
    "# Feature engineering\n",
    "engineer = RetailFeatureEngineer()\n",
    "featured_df = engineer.compute_features(retail_df)\n",
    "print(featured_df.head())\n",
    "\n",
    "# Define categorical features\n",
    "cat_features = ['store_id', 'product_id', 'month', 'day_of_week', 'is_weekend', 'promotion', 'holiday']\n",
    "\n",
    "# Prepare forecaster\n",
    "forecaster = RetailForecaster(engineer.feature_columns, categorical_features=cat_features)\n",
    "X, y = forecaster.prepare_data(featured_df)\n",
    "\n",
    "# Train with CV\n",
    "cv_scores = forecaster.train_with_cv(X, y)\n",
    "\n",
    "# Backtest\n",
    "backtester = RetailBacktester(forecaster, engineer.feature_columns, cat_features, initial_train_days=365)\n",
    "backtest_results = backtester.run(featured_df)\n",
    "\n",
    "# Evaluate overall MAE\n",
    "overall_mae = backtest_results['abs_error'].mean()\n",
    "print(f\"Overall MAE: {overall_mae:.2f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **75.9 Lessons Learned from Retail Forecasting**\n",
    "\n",
    "1. **Data quality is even more critical**: Missing sales data (due to stockouts) must be handled carefully; otherwise, the model learns to predict zero when product is unavailable.\n",
    "2. **Promotion effects are powerful but tricky**: Promotions can cause spikes that are hard to predict if not properly feature‑engineered (e.g., include promotion duration, discount depth).\n",
    "3. **Hierarchical reconciliation**: Forecasts at different levels (store, product, total) may not add up. Using reconciliation methods (top‑down, bottom‑up, optimal combination) improves consistency.\n",
    "4. **Global models outperform local models** when many series are available, as they share strength across sparse series.\n",
    "5. **External data (weather, events) adds value** but must be available for the forecast horizon.\n",
    "\n",
    "---\n",
    "\n",
    "## **75.10 Future Improvements**\n",
    "\n",
    "- **Quantile forecasting** to provide prediction intervals for safety stock.\n",
    "- **Deep learning models** like DeepAR or Temporal Fusion Transformer.\n",
    "- **Incorporating image/text data** from advertisements.\n",
    "- **Automated promotion feature extraction** from calendar data.\n",
    "- **Real‑time inventory feedback**: adjust forecasts based on current stock levels.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we adapted our time‑series prediction system to the retail domain. We built a synthetic retail dataset, engineered features specific to retail (promotions, holidays, store/product aggregates), trained a global LightGBM model, and backtested it with a walk‑forward approach. We highlighted the differences from financial forecasting and discussed deployment and monitoring considerations.\n",
    "\n",
    "The principles remain the same as in the NEPSE system: clean data, robust feature engineering, time‑aware validation, and production deployment. The retail example demonstrates the flexibility of our pipeline.\n",
    "\n",
    "In the next chapter, we will explore **Weather and Climate Prediction**, another domain with unique characteristics.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 75**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
