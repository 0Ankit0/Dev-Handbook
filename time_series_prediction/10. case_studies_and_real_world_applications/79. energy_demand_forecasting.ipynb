{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 79: Energy Demand Forecasting**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the unique challenges of energy demand forecasting: multiple seasonalities, weather dependence, and regulatory influences.\n",
    "- Identify and acquire relevant data sources, including historical load, weather, and calendar information.\n",
    "- Engineer features that capture daily, weekly, and annual cycles, as well as holiday effects and temperature sensitivities.\n",
    "- Apply both classical time‑series models (SARIMA, exponential smoothing) and machine learning models (tree‑based, neural networks) to forecast demand.\n",
    "- Implement probabilistic forecasts to quantify uncertainty, which is essential for grid operations.\n",
    "- Evaluate forecasts using energy‑specific metrics (e.g., pinball loss for quantiles).\n",
    "- Deploy a forecasting system in a production environment, integrating with real‑time data feeds and alerting mechanisms.\n",
    "\n",
    "---\n",
    "\n",
    "## **79.1 Introduction to Energy Demand Forecasting**\n",
    "\n",
    "Energy demand forecasting is critical for the reliable and economical operation of power grids. Utilities use forecasts to schedule generation, manage reserves, plan maintenance, and set prices. Forecast errors can lead to blackouts (if demand exceeds supply) or wasted fuel (if too much generation is scheduled).\n",
    "\n",
    "Unlike the financial time series we studied with NEPSE, energy demand exhibits:\n",
    "\n",
    "- **Strong multiple seasonalities**: daily (peak vs. off‑peak), weekly (weekday vs. weekend), and annual (seasonal heating/cooling).\n",
    "- **Weather sensitivity**: temperature, humidity, cloud cover directly affect heating and cooling loads.\n",
    "- **Holiday effects**: demand patterns change dramatically on public holidays.\n",
    "- **Special events**: major sports events, festivals can cause spikes.\n",
    "- **Trends**: population growth, energy efficiency improvements, adoption of electric vehicles.\n",
    "\n",
    "In this chapter, we will build an energy demand forecasting system for a hypothetical utility. We'll use publicly available data (e.g., from PJM in the US) but simplified for demonstration. The pipeline will include data ingestion, feature engineering, model training (both point and probabilistic forecasts), evaluation, and deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## **79.2 Energy Demand Data Characteristics**\n",
    "\n",
    "A typical energy demand dataset includes:\n",
    "\n",
    "- **Timestamp**: usually hourly or half‑hourly.\n",
    "- **Load**: actual demand (MW).\n",
    "- **Temperature**: dry bulb, dew point.\n",
    "- **Other weather variables**: humidity, wind speed, cloud cover, precipitation.\n",
    "- **Calendar indicators**: hour of day, day of week, month, holiday flags.\n",
    "\n",
    "We'll generate synthetic data that mimics real hourly load with these features.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_load_data(years=3, start_date='2020-01-01', seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic hourly load data with weather.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    hours = years * 365 * 24\n",
    "    dates = pd.date_range(start=start_date, periods=hours, freq='H')\n",
    "    \n",
    "    # Base load: trend + annual seasonality\n",
    "    t = np.arange(hours)\n",
    "    trend = 0.0001 * t  # slight upward trend\n",
    "    annual = 200 * np.sin(2 * np.pi * t / (365.25 * 24) + 0.5)  # annual cycle\n",
    "    \n",
    "    # Weekly seasonality\n",
    "    weekly = 150 * np.sin(2 * np.pi * t / (7*24))\n",
    "    \n",
    "    # Daily seasonality\n",
    "    daily = 300 * np.sin(2 * np.pi * t / 24 - np.pi/2)  # peak in afternoon\n",
    "    \n",
    "    # Temperature effect (heating and cooling)\n",
    "    # Simulate temperature with annual cycle and noise\n",
    "    temp = 15 + 15 * np.sin(2 * np.pi * t / (365.25 * 24) - np.pi/2) + np.random.normal(0, 2, hours)\n",
    "    # Heating load (cold temperatures increase demand)\n",
    "    heating_effect = 50 * np.maximum(0, 10 - temp)\n",
    "    # Cooling load (hot temperatures increase demand)\n",
    "    cooling_effect = 80 * np.maximum(0, temp - 20)\n",
    "    \n",
    "    # Holiday effect (simplified: Christmas, New Year)\n",
    "    holiday_effect = np.zeros(hours)\n",
    "    for date in pd.date_range(start=start_date, periods=years*365, freq='D'):\n",
    "        if date.month == 12 and date.day == 25:  # Christmas\n",
    "            holiday_effect[(date - pd.Timestamp(start_date)).days * 24 : (date - pd.Timestamp(start_date)).days * 24 + 24] = -150\n",
    "        if date.month == 1 and date.day == 1:    # New Year\n",
    "            holiday_effect[(date - pd.Timestamp(start_date)).days * 24 : (date - pd.Timestamp(start_date)).days * 24 + 24] = -200\n",
    "    \n",
    "    # Random noise\n",
    "    noise = np.random.normal(0, 20, hours)\n",
    "    \n",
    "    load = 1000 + trend + annual + weekly + daily + heating_effect + cooling_effect + holiday_effect + noise\n",
    "    load = np.maximum(load, 200)  # minimum load\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': dates,\n",
    "        'load': load,\n",
    "        'temperature': temp,\n",
    "        'hour': dates.hour,\n",
    "        'dayofweek': dates.dayofweek,\n",
    "        'month': dates.month,\n",
    "        'day': dates.day,\n",
    "        'is_weekend': (dates.dayofweek >= 5).astype(int)\n",
    "    })\n",
    "    \n",
    "    # Add holiday flags\n",
    "    df['is_holiday'] = 0\n",
    "    for date in pd.date_range(start=start_date, periods=years*365, freq='D'):\n",
    "        if (date.month == 12 and date.day == 25) or (date.month == 1 and date.day == 1):\n",
    "            df.loc[df['timestamp'].dt.date == date.date(), 'is_holiday'] = 1\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate 2 years of hourly data\n",
    "df_load = generate_load_data(years=2)\n",
    "print(df_load.head())\n",
    "\n",
    "# Plot one week of load and temperature\n",
    "week = df_load.iloc[:7*24]\n",
    "fig, ax1 = plt.subplots(figsize=(12,5))\n",
    "ax1.plot(week['timestamp'], week['load'], color='tab:blue', label='Load (MW)')\n",
    "ax1.set_xlabel('Time')\n",
    "ax1.set_ylabel('Load', color='tab:blue')\n",
    "ax2 = ax1.twinx()\n",
    "ax2.plot(week['timestamp'], week['temperature'], color='tab:red', label='Temperature')\n",
    "ax2.set_ylabel('Temperature', color='tab:red')\n",
    "plt.title('Load and Temperature Over One Week')\n",
    "fig.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This function generates hourly load data with realistic patterns: trend, annual, weekly, and daily seasonality.\n",
    "- Temperature is simulated with an annual cycle; heating and cooling effects are modelled as piecewise linear functions of temperature.\n",
    "- Holiday effects are added as drops on Christmas and New Year (demand typically lower on holidays).\n",
    "- The resulting DataFrame is the basis for our forecasting experiments.\n",
    "\n",
    "---\n",
    "\n",
    "## **79.3 Feature Engineering for Energy Demand**\n",
    "\n",
    "Feature engineering for load forecasting must capture:\n",
    "\n",
    "- **Time features**: hour, day of week, month, holiday flags, interactions (e.g., hour × weekday).\n",
    "- **Weather features**: current temperature, lagged temperature, heating/cooling degree days.\n",
    "- **Lagged load**: autoregressive terms (load at same hour yesterday, last week).\n",
    "- **Rolling statistics**: moving averages of load and temperature over recent hours/days.\n",
    "- **Calendar events**: special days (e.g., Super Bowl) may need manual flags.\n",
    "\n",
    "We'll build a feature engineering class similar to previous chapters.\n",
    "\n",
    "```python\n",
    "class EnergyFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering for hourly load forecasting.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_columns = []\n",
    "    \n",
    "    def add_time_features(self, df):\n",
    "        \"\"\"Add cyclical time features.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['hour_sin'] = np.sin(2 * np.pi * df['hour'] / 24)\n",
    "        df['hour_cos'] = np.cos(2 * np.pi * df['hour'] / 24)\n",
    "        df['dayofweek_sin'] = np.sin(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['dayofweek_cos'] = np.cos(2 * np.pi * df['dayofweek'] / 7)\n",
    "        df['month_sin'] = np.sin(2 * np.pi * df['month'] / 12)\n",
    "        df['month_cos'] = np.cos(2 * np.pi * df['month'] / 12)\n",
    "        return df\n",
    "    \n",
    "    def add_lag_features(self, df, target='load', lags=[1, 2, 24, 48, 168]):\n",
    "        \"\"\"Lag features for load.\"\"\"\n",
    "        df = df.copy()\n",
    "        for lag in lags:\n",
    "            df[f'{target}_lag_{lag}'] = df[target].shift(lag)\n",
    "        return df\n",
    "    \n",
    "    def add_rolling_features(self, df, target='load', windows=[24, 168]):\n",
    "        \"\"\"Rolling statistics over past hours.\"\"\"\n",
    "        df = df.copy()\n",
    "        for window in windows:\n",
    "            df[f'{target}_rolling_mean_{window}'] = df[target].rolling(window, min_periods=1).mean().shift(1)\n",
    "            df[f'{target}_rolling_std_{window}'] = df[target].rolling(window, min_periods=1).std().shift(1)\n",
    "            df[f'{target}_rolling_min_{window}'] = df[target].rolling(window, min_periods=1).min().shift(1)\n",
    "            df[f'{target}_rolling_max_{window}'] = df[target].rolling(window, min_periods=1).max().shift(1)\n",
    "        return df\n",
    "    \n",
    "    def add_weather_features(self, df, temp_col='temperature'):\n",
    "        \"\"\"Add temperature-related features.\"\"\"\n",
    "        df = df.copy()\n",
    "        # Heating and cooling degree days (base 65°F = 18.3°C, approximate)\n",
    "        base_temp = 18.3\n",
    "        df['hdd'] = np.maximum(0, base_temp - df[temp_col])\n",
    "        df['cdd'] = np.maximum(0, df[temp_col] - base_temp)\n",
    "        # Lagged temperature\n",
    "        for lag in [1, 2, 24]:\n",
    "            df[f'{temp_col}_lag_{lag}'] = df[temp_col].shift(lag)\n",
    "        # Rolling temperature\n",
    "        df[f'{temp_col}_rolling_mean_24'] = df[temp_col].rolling(24, min_periods=1).mean().shift(1)\n",
    "        return df\n",
    "    \n",
    "    def add_interaction_features(self, df):\n",
    "        \"\"\"Interactions between hour and weekend/holiday.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['hour_weekend'] = df['hour'] * df['is_weekend']\n",
    "        df['hour_holiday'] = df['hour'] * df['is_holiday']\n",
    "        return df\n",
    "    \n",
    "    def compute_features(self, df, target='load', forecast_horizon=24):\n",
    "        \"\"\"\n",
    "        Main entry point.\n",
    "        Creates features and target (load shifted by horizon).\n",
    "        \"\"\"\n",
    "        df = df.copy().sort_values('timestamp')\n",
    "        \n",
    "        # Add all feature groups\n",
    "        df = self.add_time_features(df)\n",
    "        df = self.add_lag_features(df, target, lags=[1, 2, 24, 48, 168])\n",
    "        df = self.add_rolling_features(df, target, windows=[24, 168])\n",
    "        df = self.add_weather_features(df)\n",
    "        df = self.add_interaction_features(df)\n",
    "        \n",
    "        # Create target: load at horizon hours ahead\n",
    "        df[f'target_{target}_{forecast_horizon}'] = df[target].shift(-forecast_horizon)\n",
    "        \n",
    "        # Drop rows with NaN created by shifts\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Store feature columns (exclude identifiers and target)\n",
    "        exclude = ['timestamp', target, f'target_{target}_{forecast_horizon}']\n",
    "        self.feature_columns = [c for c in df.columns if c not in exclude]\n",
    "        \n",
    "        return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Time features are cyclically encoded to preserve circular continuity.\n",
    "- Lag features include load from 1 hour ago, 2 hours ago, same hour yesterday (24), same hour two days ago (48), and same hour last week (168).\n",
    "- Rolling statistics over 24 hours (daily) and 168 hours (weekly) provide recent trend and variability.\n",
    "- Weather features include heating/cooling degree days (common in energy modelling) and lagged/rolling temperature.\n",
    "- Interactions like hour×weekend help capture different daily patterns on weekends.\n",
    "- The target is load shifted forward by the forecast horizon (e.g., 24 hours ahead).\n",
    "\n",
    "---\n",
    "\n",
    "## **79.4 Modeling Approaches for Load Forecasting**\n",
    "\n",
    "Load forecasting can be tackled with:\n",
    "\n",
    "- **Classical time‑series models**: SARIMA, exponential smoothing (Holt‑Winters). Good for capturing seasonality but may struggle with exogenous variables like weather.\n",
    "- **Machine learning models**: Gradient boosting (XGBoost, LightGBM) often perform very well because they can handle many features and interactions.\n",
    "- **Deep learning**: LSTM, Sequence‑to‑sequence, or Transformer models can capture long‑range dependencies.\n",
    "- **Hybrid**: Combining a statistical baseline with ML corrections.\n",
    "\n",
    "We'll demonstrate both a classical SARIMA model (for comparison) and a LightGBM model, which typically excels in load forecasting competitions.\n",
    "\n",
    "### **79.4.1 SARIMA Baseline**\n",
    "\n",
    "SARIMA (Seasonal ARIMA) models the time series with seasonal components. For hourly data, we might have seasonality of 24 (daily) and 168 (weekly). We'll use `statsmodels`.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "def fit_sarima(train_series, order=(1,1,1), seasonal_order=(1,1,1,24)):\n",
    "    \"\"\"\n",
    "    Fit a SARIMA model.\n",
    "    \"\"\"\n",
    "    model = SARIMAX(train_series, order=order, seasonal_order=seasonal_order,\n",
    "                    enforce_stationarity=False, enforce_invertibility=False)\n",
    "    results = model.fit(disp=False)\n",
    "    return results\n",
    "\n",
    "# Example: forecast next 24 hours\n",
    "# train = df_load['load'].iloc[:-24]\n",
    "# test = df_load['load'].iloc[-24:]\n",
    "# sarima_model = fit_sarima(train)\n",
    "# forecast = sarima_model.forecast(steps=24)\n",
    "# mae = np.mean(np.abs(forecast - test))\n",
    "# print(f\"SARIMA 24h MAE: {mae:.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- SARIMA requires careful parameter selection (order, seasonal order). Here we use a simple configuration; in practice, you would use AIC or grid search.\n",
    "- The model can only use past load; it cannot incorporate temperature forecasts. This limits its accuracy.\n",
    "\n",
    "### **79.4.2 LightGBM Model**\n",
    "\n",
    "LightGBM can incorporate all our features, including weather forecasts (if available). We'll train a model to predict load 24 hours ahead.\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "class LoadForecaster:\n",
    "    def __init__(self, feature_columns, categorical_features=None):\n",
    "        self.feature_columns = feature_columns\n",
    "        self.categorical_features = categorical_features if categorical_features else []\n",
    "        self.model = None\n",
    "    \n",
    "    def prepare_data(self, df, target_col):\n",
    "        X = df[self.feature_columns]\n",
    "        y = df[target_col]\n",
    "        for col in self.categorical_features:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].astype('category')\n",
    "        return X, y\n",
    "    \n",
    "    def train_with_cv(self, X, y, n_splits=5, params=None):\n",
    "        if params is None:\n",
    "            params = {\n",
    "                'objective': 'regression',\n",
    "                'metric': 'mae',\n",
    "                'boosting_type': 'gbdt',\n",
    "                'num_leaves': 31,\n",
    "                'learning_rate': 0.05,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 5,\n",
    "                'verbose': -1\n",
    "            }\n",
    "        \n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        cv_scores = []\n",
    "        models = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            \n",
    "            train_data = lgb.Dataset(X_train, label=y_train, categorical_feature=self.categorical_features)\n",
    "            val_data = lgb.Dataset(X_val, label=y_val, categorical_feature=self.categorical_features, reference=train_data)\n",
    "            \n",
    "            model = lgb.train(\n",
    "                params,\n",
    "                train_data,\n",
    "                valid_sets=[val_data],\n",
    "                num_boost_round=1000,\n",
    "                callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)]\n",
    "            )\n",
    "            \n",
    "            y_pred = model.predict(X_val, num_iteration=model.best_iteration)\n",
    "            mae = mean_absolute_error(y_val, y_pred)\n",
    "            cv_scores.append(mae)\n",
    "            models.append(model)\n",
    "            print(f\"Fold {fold+1} MAE: {mae:.2f}\")\n",
    "        \n",
    "        best_idx = np.argmin(cv_scores)\n",
    "        self.model = models[best_idx]\n",
    "        print(f\"Best CV MAE: {cv_scores[best_idx]:.2f}\")\n",
    "        return cv_scores\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The forecaster uses time‑series cross‑validation to avoid look‑ahead.\n",
    "- LightGBM can handle categorical features (e.g., hour, dayofweek) natively.\n",
    "- The model is trained to predict load at horizon (e.g., 24 hours ahead) using features known at forecast time (including lagged load and weather forecasts).\n",
    "\n",
    "---\n",
    "\n",
    "## **79.5 Probabilistic Forecasting**\n",
    "\n",
    "In energy, point forecasts are insufficient; operators need to know the uncertainty. Probabilistic forecasts provide prediction intervals or full quantiles. Methods include:\n",
    "\n",
    "- **Quantile regression**: train models to predict specific quantiles (e.g., 10th, 50th, 90th).\n",
    "- **Conformal prediction**: add intervals to any point forecast.\n",
    "- **Bayesian methods**: e.g., Bayesian neural networks.\n",
    "\n",
    "LightGBM can be used for quantile regression by specifying the `objective` as `'quantile'` and `alpha` for the quantile.\n",
    "\n",
    "```python\n",
    "def train_quantile_model(X, y, alpha=0.5):\n",
    "    params = {\n",
    "        'objective': 'quantile',\n",
    "        'alpha': alpha,\n",
    "        'metric': 'quantile',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'num_leaves': 31,\n",
    "        'learning_rate': 0.05,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'verbose': -1\n",
    "    }\n",
    "    train_data = lgb.Dataset(X, label=y)\n",
    "    model = lgb.train(params, train_data, num_boost_round=100)\n",
    "    return model\n",
    "\n",
    "# Train three models for 10th, 50th, 90th percentiles\n",
    "# model_p10 = train_quantile_model(X_train, y_train, alpha=0.1)\n",
    "# model_p50 = train_quantile_model(X_train, y_train, alpha=0.5)\n",
    "# model_p90 = train_quantile_model(X_train, y_train, alpha=0.9)\n",
    "# pred_p10 = model_p10.predict(X_test)\n",
    "# pred_p50 = model_p50.predict(X_test)\n",
    "# pred_p90 = model_p90.predict(X_test)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Three models give the lower, median, and upper quantiles.\n",
    "- The interval [p10, p90] contains 80% of the probability mass.\n",
    "- This provides operators with a range of possible outcomes.\n",
    "\n",
    "---\n",
    "\n",
    "## **79.6 Evaluation Metrics for Load Forecasts**\n",
    "\n",
    "Standard metrics:\n",
    "\n",
    "- **MAE**, **RMSE** for point forecasts.\n",
    "- **Pinball loss** for quantile forecasts:  \n",
    "  `L(y, q, α) = (y - q) * α  if y ≥ q else (q - y) * (1-α)`\n",
    "- **Continuous Ranked Probability Score (CRPS)** for full distributions.\n",
    "\n",
    "We'll implement pinball loss.\n",
    "\n",
    "```python\n",
    "def pinball_loss(y_true, y_pred, alpha):\n",
    "    \"\"\"Compute pinball loss for a single quantile.\"\"\"\n",
    "    error = y_true - y_pred\n",
    "    loss = np.where(error >= 0, alpha * error, (alpha - 1) * error)\n",
    "    return np.mean(loss)\n",
    "\n",
    "# For the three quantile models, average pinball loss over alphas\n",
    "# loss_p10 = pinball_loss(y_test, pred_p10, 0.1)\n",
    "# loss_p50 = pinball_loss(y_test, pred_p50, 0.5)\n",
    "# loss_p90 = pinball_loss(y_test, pred_p90, 0.9)\n",
    "# avg_loss = (loss_p10 + loss_p50 + loss_p90) / 3\n",
    "# print(f\"Average pinball loss: {avg_loss:.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Pinball loss is the standard metric for quantile forecasts; lower is better.\n",
    "- Averaging over quantiles gives an overall measure of probabilistic forecast quality.\n",
    "\n",
    "---\n",
    "\n",
    "## **79.7 Case Study: 24‑Hour Ahead Load Forecasting**\n",
    "\n",
    "We'll now run a complete example using our synthetic data.\n",
    "\n",
    "```python\n",
    "# Generate data\n",
    "df = generate_load_data(years=3)\n",
    "\n",
    "# Feature engineering\n",
    "engineer = EnergyFeatureEngineer()\n",
    "featured_df = engineer.compute_features(df, target='load', forecast_horizon=24)\n",
    "print(f\"Features shape: {featured_df.shape}\")\n",
    "\n",
    "# Split by time (last 30 days for test)\n",
    "split_date = df['timestamp'].max() - pd.Timedelta(days=30)\n",
    "train_df = featured_df[featured_df['timestamp'] < split_date]\n",
    "test_df = featured_df[featured_df['timestamp'] >= split_date]\n",
    "\n",
    "# Define categorical features\n",
    "cat_features = ['hour', 'dayofweek', 'month', 'is_weekend', 'is_holiday']\n",
    "\n",
    "# Prepare data\n",
    "forecaster = LoadForecaster(engineer.feature_columns, categorical_features=cat_features)\n",
    "X_train, y_train = forecaster.prepare_data(train_df, 'target_load_24')\n",
    "X_test, y_test = forecaster.prepare_data(test_df, 'target_load_24')\n",
    "\n",
    "# Train with cross‑validation\n",
    "cv_scores = forecaster.train_with_cv(X_train, y_train, n_splits=3)\n",
    "\n",
    "# Predict on test\n",
    "y_pred = forecaster.predict(X_test)\n",
    "test_mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"Test MAE (24h ahead): {test_mae:.2f}\")\n",
    "\n",
    "# Plot actual vs predicted for a sample week\n",
    "plot_idx = test_df['timestamp'].iloc[:7*24]\n",
    "plt.figure(figsize=(12,5))\n",
    "plt.plot(plot_idx, y_test[:7*24], label='Actual')\n",
    "plt.plot(plot_idx, y_pred[:7*24], label='Predicted')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Load (MW)')\n",
    "plt.title('Actual vs Predicted Load (24h ahead)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We train on 3 years of data, test on the last 30 days.\n",
    "- The model predicts load 24 hours ahead using features available at forecast time (e.g., temperature forecasts would need to be provided; here we use actual temperature, which would not be known in reality – in practice you would use forecasted temperature).\n",
    "- The plot shows that the model captures the daily and weekly patterns well.\n",
    "\n",
    "---\n",
    "\n",
    "## **79.8 Deployment and Integration**\n",
    "\n",
    "In production, an energy forecasting system typically runs daily, producing forecasts for the next day (or week) at hourly resolution. The pipeline might:\n",
    "\n",
    "1. **Ingest latest load and weather observations** (e.g., from SCADA and weather services).\n",
    "2. **Compute features** up to the current hour.\n",
    "3. **Load the latest trained model** from a model registry (e.g., MLflow).\n",
    "4. **Generate forecasts** for the next 24–168 hours.\n",
    "5. **Save forecasts** to a database and expose via API or push to control room dashboards.\n",
    "6. **Monitor forecast errors** and trigger retraining when performance degrades.\n",
    "\n",
    "We can reuse the batch prediction pattern from Chapter 74. For real‑time updates (e.g., every hour), we would have a streaming version.\n",
    "\n",
    "```python\n",
    "class EnergyBatchPredictor:\n",
    "    def __init__(self, model, feature_engineer, feature_columns, categorical_features):\n",
    "        self.model = model\n",
    "        self.feature_engineer = feature_engineer\n",
    "        self.feature_columns = feature_columns\n",
    "        self.categorical_features = categorical_features\n",
    "    \n",
    "    def predict_next_day(self, historical_df, weather_forecast_df, forecast_horizon=24):\n",
    "        \"\"\"\n",
    "        historical_df: DataFrame with recent load and observed weather up to now.\n",
    "        weather_forecast_df: DataFrame with forecasted temperature for next hours.\n",
    "        Returns predictions for next `forecast_horizon` hours.\n",
    "        \"\"\"\n",
    "        # Combine historical and forecast weather\n",
    "        # This is simplified; you would align timestamps and fill future weather.\n",
    "        # For demonstration, we assume weather_forecast_df has the same structure and is already merged.\n",
    "        full_df = pd.concat([historical_df, weather_forecast_df], ignore_index=True)\n",
    "        # Engineer features (this will compute lags, etc., using historical data)\n",
    "        featured = self.feature_engineer.compute_features(full_df, target='load', forecast_horizon=0)\n",
    "        # The last `forecast_horizon` rows correspond to the future times\n",
    "        future_features = featured.iloc[-forecast_horizon:][self.feature_columns]\n",
    "        for col in self.categorical_features:\n",
    "            if col in future_features.columns:\n",
    "                future_features[col] = future_features[col].astype('category')\n",
    "        preds = self.model.predict(future_features)\n",
    "        return preds\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The batch predictor uses the latest historical data and future weather forecasts to generate predictions.\n",
    "- In practice, you would have a separate process to fetch weather forecasts from an API.\n",
    "\n",
    "---\n",
    "\n",
    "## **79.9 Lessons Learned from Energy Forecasting**\n",
    "\n",
    "1. **Weather is the most important exogenous variable**: Accurate temperature forecasts are crucial.\n",
    "2. **Holidays are difficult**: They occur infrequently, so models may not learn them well. Consider adding holiday dummies or using separate models for holidays.\n",
    "3. **Multiple seasonalities require careful feature design**: Our use of lag features at 24 and 168 hours captures daily and weekly cycles.\n",
    "4. **Probabilistic forecasts add value**: Operators need to know the range of possible outcomes for risk management.\n",
    "5. **Model retraining is necessary**: Load patterns change over time (e.g., due to new appliances, energy efficiency). Monitor and retrain periodically.\n",
    "6. **Regulatory and market implications**: In deregulated markets, forecasts directly affect bidding strategies; accuracy translates to profit.\n",
    "\n",
    "---\n",
    "\n",
    "## **79.10 Future Directions**\n",
    "\n",
    "- **Incorporating renewables**: Solar and wind generation forecasts are becoming essential as they add variability.\n",
    "- **Ensemble methods**: Combine multiple models (e.g., SARIMA, LightGBM, LSTM) for improved accuracy.\n",
    "- **Spatio‑temporal models**: Forecast load across multiple zones simultaneously, capturing regional correlations.\n",
    "- **Deep learning with attention**: Transformers can capture long‑range dependencies and multiple seasonalities.\n",
    "- **Transfer learning**: Use models pretrained on one region to improve forecasts in another with limited data.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we built an energy demand forecasting system. We generated synthetic hourly load data with realistic patterns, engineered features to capture seasonalities and weather effects, trained a LightGBM model for 24‑hour ahead point forecasts, extended to probabilistic forecasts using quantile regression, and discussed deployment considerations. The principles of feature engineering, time‑based validation, and model deployment mirror those from earlier chapters, but with domain‑specific adaptations for energy.\n",
    "\n",
    "This chapter concludes our exploration of domain‑specific forecasting systems. The next chapter, **Supply Chain Optimization**, will apply similar techniques to demand forecasting, inventory optimization, and lead time prediction.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 79**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
