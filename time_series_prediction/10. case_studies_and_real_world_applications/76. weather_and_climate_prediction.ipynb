{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 76: Weather and Climate Prediction**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamental differences between weather forecasting (short\u2011term) and climate prediction (long\u2011term).\n",
    "- Identify common data sources for meteorological time series (station observations, satellite, reanalysis).\n",
    "- Engineer spatial and temporal features relevant to weather prediction (e.g., pressure gradients, humidity, wind vectors).\n",
    "- Apply both traditional (numerical weather prediction) and machine learning models to weather forecasting.\n",
    "- Implement probabilistic forecasts and quantify uncertainty.\n",
    "- Evaluate weather forecasts using domain\u2011specific metrics (RMSE, anomaly correlation, CRPS).\n",
    "- Build a complete weather prediction pipeline that integrates with the system architecture developed in previous chapters.\n",
    "\n",
    "---\n",
    "\n",
    "## **76.1 Introduction to Weather and Climate Prediction**\n",
    "\n",
    "Weather and climate prediction are among the oldest and most challenging time\u2011series forecasting problems. Unlike financial or retail data, atmospheric processes are governed by complex physics, exhibit chaotic behaviour, and involve massive spatiotemporal datasets.\n",
    "\n",
    "**Weather prediction** focuses on short\u2011term forecasts (hours to days) with high spatial resolution. **Climate prediction** deals with longer timescales (seasons to decades) and often involves ensemble methods to account for uncertainty.\n",
    "\n",
    "Key characteristics:\n",
    "\n",
    "- **High dimensionality**: Weather data includes multiple variables (temperature, pressure, humidity, wind speed/direction) at thousands of spatial grid points.\n",
    "- **Spatiotemporal dependencies**: Conditions at one location affect nearby locations, with time lags due to atmospheric motion.\n",
    "- **Non\u2011stationarity**: Climate change introduces trends and changing variability.\n",
    "- **Chaotic dynamics**: Small initial errors grow rapidly (the butterfly effect), making deterministic forecasts beyond a few days inherently uncertain.\n",
    "\n",
    "Modern operational weather forecasting relies on **Numerical Weather Prediction (NWP)** \u2014 physics\u2011based models that solve partial differential equations. However, machine learning is increasingly used for **post\u2011processing** (correcting NWP output) and even for fully data\u2011driven forecasting (e.g., GraphCast, Pangu\u2011Weather).\n",
    "\n",
    "In this chapter, we will build a simplified weather prediction system using machine learning, treating it as a time\u2011series problem with multiple stations or grid points. We'll use the NEPSE pipeline as a template but adapt it to the unique requirements of weather data.\n",
    "\n",
    "---\n",
    "\n",
    "## **76.2 Weather Data Sources and Formats**\n",
    "\n",
    "Weather data comes in various forms:\n",
    "\n",
    "- **Station data**: Point measurements from meteorological stations (temperature, precipitation, wind). Often provided as CSV or text files with metadata (latitude, longitude, elevation).\n",
    "- **Gridded datasets**: Interpolated fields covering a regular grid (e.g., ERA5 reanalysis from ECMWF). Usually stored in NetCDF or GRIB format.\n",
    "- **Satellite and radar**: High\u2011resolution imagery (e.g., cloud cover, precipitation intensity).\n",
    "- **Reanalysis**: Combines historical observations with a consistent NWP model to produce a long\u2011term, gap\u2011filled dataset (e.g., ERA5, NCEP/NCAR Reanalysis).\n",
    "\n",
    "For our example, we will work with a simplified station\u2011based dataset. We'll generate synthetic data that mimics daily temperature and precipitation at multiple stations, with realistic spatial correlation and seasonality.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_weather_data(num_stations=10, days=365*3, start_date='2020-01-01'):\n",
    "    \"\"\"\n",
    "    Generate synthetic weather station data.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    num_stations : int\n",
    "        Number of weather stations.\n",
    "    days : int\n",
    "        Number of days of data.\n",
    "    start_date : str\n",
    "        Start date.\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Columns: station_id, date, lat, lon, elevation, temperature, precipitation, pressure, wind_speed\n",
    "    \"\"\"\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start=start_date, periods=days, freq='D')\n",
    "    \n",
    "    # Station locations (random lat/lon within a region, e.g., Nepal)\n",
    "    stations = []\n",
    "    for i in range(1, num_stations+1):\n",
    "        lat = np.random.uniform(26, 30)   # Nepal latitude range\n",
    "        lon = np.random.uniform(80, 88)   # Nepal longitude range\n",
    "        elev = np.random.uniform(500, 4000)  # elevation in meters\n",
    "        stations.append({'station_id': i, 'lat': lat, 'lon': lon, 'elevation': elev})\n",
    "    \n",
    "    station_df = pd.DataFrame(stations)\n",
    "    \n",
    "    # Generate daily data for each station\n",
    "    records = []\n",
    "    for _, station in station_df.iterrows():\n",
    "        sid = station['station_id']\n",
    "        elev = station['elevation']\n",
    "        \n",
    "        # Base temperature depends on elevation (lapse rate) and latitude\n",
    "        base_temp = 25 - 0.006 * elev + np.random.normal(0, 2)\n",
    "        \n",
    "        # Seasonal cycle (amplitude depends on location)\n",
    "        seasonal_amp = 10 + np.random.normal(0, 2)\n",
    "        \n",
    "        # Generate time series\n",
    "        for t, date in enumerate(dates):\n",
    "            # Day of year (0-365)\n",
    "            doy = date.dayofyear\n",
    "            \n",
    "            # Temperature with seasonal cycle and noise\n",
    "            seasonal = seasonal_amp * np.sin(2 * np.pi * (doy - 80) / 365)  # peak in July\n",
    "            temp = base_temp + seasonal + np.random.normal(0, 2)\n",
    "            \n",
    "            # Precipitation: only on some days, with higher probability in monsoon\n",
    "            monsoon = (doy > 150) & (doy < 270)  # approximate monsoon months\n",
    "            precip_prob = 0.3 + 0.4 * monsoon\n",
    "            precip = np.random.exponential(5) if np.random.random() < precip_prob else 0\n",
    "            \n",
    "            # Pressure (simplified, decreases with elevation)\n",
    "            pressure = 1013.25 * np.exp(-elev / 8000) + np.random.normal(0, 5)\n",
    "            \n",
    "            # Wind speed\n",
    "            wind_speed = np.random.gamma(2, 2) + 2 * monsoon  # stronger winds in monsoon\n",
    "            \n",
    "            records.append({\n",
    "                'station_id': sid,\n",
    "                'date': date,\n",
    "                'lat': station['lat'],\n",
    "                'lon': station['lon'],\n",
    "                'elevation': elev,\n",
    "                'temperature': temp,\n",
    "                'precipitation': precip,\n",
    "                'pressure': pressure,\n",
    "                'wind_speed': wind_speed\n",
    "            })\n",
    "    \n",
    "    df = pd.DataFrame(records)\n",
    "    return df\n",
    "\n",
    "# Generate sample data\n",
    "weather_df = generate_weather_data(num_stations=5, days=365*2)\n",
    "print(weather_df.head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This function generates synthetic weather data for multiple stations. Each station has a fixed latitude, longitude, and elevation.\n",
    "- Temperature includes a seasonal cycle (sine wave) and elevation\u2011based lapse rate, plus random noise.\n",
    "- Precipitation is generated with a higher probability during a simplified monsoon season (days 150\u2013270).\n",
    "- Pressure and wind speed are simplified but capture some realistic variation.\n",
    "- The resulting DataFrame is in a tidy format: one row per station per day.\n",
    "\n",
    "---\n",
    "\n",
    "## **76.3 Feature Engineering for Weather Prediction**\n",
    "\n",
    "Weather features must capture both temporal dynamics and spatial relationships. We will engineer:\n",
    "\n",
    "- **Temporal features**: day of year (cyclically encoded), month, season flags.\n",
    "- **Lagged features**: past values of temperature, pressure, etc., at the same station.\n",
    "- **Rolling statistics**: moving averages and standard deviations over various windows.\n",
    "- **Spatial features**: for each station, include data from neighbouring stations (with appropriate time lags to account for advection).\n",
    "- **Derived meteorological quantities**: dew point, relative humidity, pressure tendency, etc.\n",
    "- **External indices**: teleconnection indices (ENSO, NAO) if available.\n",
    "\n",
    "For simplicity, we'll focus on single\u2011station forecasting with added spatial context.\n",
    "\n",
    "```python\n",
    "class WeatherFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering for weather station data.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.feature_columns = []\n",
    "    \n",
    "    def add_temporal_features(self, df):\n",
    "        \"\"\"Add cyclical time features.\"\"\"\n",
    "        df = df.copy()\n",
    "        df['doy'] = df['date'].dt.dayofyear\n",
    "        df['month'] = df['date'].dt.month\n",
    "        df['year'] = df['date'].dt.year\n",
    "        # Cyclical encoding of day of year\n",
    "        df['doy_sin'] = np.sin(2 * np.pi * df['doy'] / 365.25)\n",
    "        df['doy_cos'] = np.cos(2 * np.pi * df['doy'] / 365.25)\n",
    "        return df\n",
    "    \n",
    "    def add_lag_features(self, df, variables, lags):\n",
    "        \"\"\"Add lagged values for given variables per station.\"\"\"\n",
    "        df = df.copy()\n",
    "        for var in variables:\n",
    "            for lag in lags:\n",
    "                df[f'{var}_lag_{lag}'] = df.groupby('station_id')[var].shift(lag)\n",
    "        return df\n",
    "    \n",
    "    def add_rolling_features(self, df, variables, windows, stats=['mean', 'std', 'min', 'max']):\n",
    "        \"\"\"Add rolling statistics per station.\"\"\"\n",
    "        df = df.copy()\n",
    "        for var in variables:\n",
    "            for window in windows:\n",
    "                for stat in stats:\n",
    "                    if stat == 'mean':\n",
    "                        df[f'{var}_rolling_mean_{window}'] = df.groupby('station_id')[var].transform(\n",
    "                            lambda x: x.rolling(window, min_periods=1).mean()\n",
    "                        )\n",
    "                    elif stat == 'std':\n",
    "                        df[f'{var}_rolling_std_{window}'] = df.groupby('station_id')[var].transform(\n",
    "                            lambda x: x.rolling(window, min_periods=1).std()\n",
    "                        )\n",
    "                    elif stat == 'min':\n",
    "                        df[f'{var}_rolling_min_{window}'] = df.groupby('station_id')[var].transform(\n",
    "                            lambda x: x.rolling(window, min_periods=1).min()\n",
    "                        )\n",
    "                    elif stat == 'max':\n",
    "                        df[f'{var}_rolling_max_{window}'] = df.groupby('station_id')[var].transform(\n",
    "                            lambda x: x.rolling(window, min_periods=1).max()\n",
    "                        )\n",
    "        return df\n",
    "    \n",
    "    def add_spatial_features(self, df, station_coords, radius_km=100):\n",
    "        \"\"\"\n",
    "        Add features from neighbouring stations.\n",
    "        For each station and date, compute average of variables from stations within radius.\n",
    "        This is a simplified example; real systems might use advection or interpolation.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        # Assume df has columns lat, lon per station (static)\n",
    "        # We'll compute pairwise distances once\n",
    "        from scipy.spatial.distance import cdist\n",
    "        \n",
    "        stations = df[['station_id', 'lat', 'lon']].drop_duplicates()\n",
    "        coords = stations[['lat', 'lon']].values\n",
    "        dist_matrix = cdist(coords, coords, metric='euclidean')\n",
    "        # Convert degrees to km (approx 111 km per degree)\n",
    "        dist_km = dist_matrix * 111\n",
    "        \n",
    "        # For each station, find neighbours within radius\n",
    "        neighbour_dict = {}\n",
    "        for i, sid in enumerate(stations['station_id']):\n",
    "            neighbours = stations['station_id'][(dist_km[i] < radius_km) & (dist_km[i] > 0)].tolist()\n",
    "            neighbour_dict[sid] = neighbours\n",
    "        \n",
    "        # For each date, compute spatial averages\n",
    "        # This is inefficient; in practice you'd use a spatial join or precomputed weights.\n",
    "        # We'll just show the idea.\n",
    "        df['temp_neighbour_mean'] = np.nan\n",
    "        for date, grp in df.groupby('date'):\n",
    "            for sid in grp['station_id'].unique():\n",
    "                neighbours = neighbour_dict.get(sid, [])\n",
    "                if neighbours:\n",
    "                    # Get neighbour data for this date\n",
    "                    neighbour_data = df[(df['date'] == date) & (df['station_id'].isin(neighbours))]\n",
    "                    mean_temp = neighbour_data['temperature'].mean()\n",
    "                    df.loc[(df['date'] == date) & (df['station_id'] == sid), 'temp_neighbour_mean'] = mean_temp\n",
    "        return df\n",
    "    \n",
    "    def compute_derived_features(self, df):\n",
    "        \"\"\"Compute meteorological derived quantities.\"\"\"\n",
    "        df = df.copy()\n",
    "        # Dew point (simplified: using temperature and pressure)\n",
    "        # Actual formula more complex; we use approximation\n",
    "        df['dew_point'] = df['temperature'] - (100 - df['pressure']/10) * 0.2\n",
    "        # Pressure tendency (change from previous day)\n",
    "        df['pressure_tendency'] = df.groupby('station_id')['pressure'].diff()\n",
    "        # Temperature range (if we had min/max; here we use daily as placeholder)\n",
    "        # We could also add relative humidity if we had specific humidity\n",
    "        return df\n",
    "    \n",
    "    def compute_features(self, df, target_var='temperature', forecast_horizon=1):\n",
    "        \"\"\"\n",
    "        Main entry point: compute all features and create target.\n",
    "        \"\"\"\n",
    "        df = df.copy()\n",
    "        # Ensure sorted\n",
    "        df = df.sort_values(['station_id', 'date'])\n",
    "        \n",
    "        # Add features\n",
    "        df = self.add_temporal_features(df)\n",
    "        df = self.add_lag_features(df, \n",
    "                                    variables=['temperature', 'pressure', 'precipitation', 'wind_speed'],\n",
    "                                    lags=[1, 2, 3, 7, 14])\n",
    "        df = self.add_rolling_features(df,\n",
    "                                       variables=['temperature', 'pressure', 'precipitation'],\n",
    "                                       windows=[7, 14, 30],\n",
    "                                       stats=['mean', 'std'])\n",
    "        df = self.compute_derived_features(df)\n",
    "        # Spatial features (commented out because it's slow in this example)\n",
    "        # df = self.add_spatial_features(df, station_coords=None)\n",
    "        \n",
    "        # Create target: tomorrow's temperature (or other variable)\n",
    "        df[f'target_{target_var}'] = df.groupby('station_id')[target_var].shift(-forecast_horizon)\n",
    "        \n",
    "        # Drop rows with NaN created by shifts\n",
    "        df = df.dropna().reset_index(drop=True)\n",
    "        \n",
    "        # Store feature columns (exclude identifiers and target)\n",
    "        exclude = ['station_id', 'date', 'lat', 'lon', 'elevation', f'target_{target_var}']\n",
    "        self.feature_columns = [c for c in df.columns if c not in exclude]\n",
    "        \n",
    "        return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The feature engineer adds temporal, lag, rolling, and derived features. Spatial features are outlined but not fully implemented due to computational complexity; in a real system, you would precompute neighbour averages or use more sophisticated methods like kriging.\n",
    "- Cyclical encoding of day of year (`doy_sin`, `doy_cos`) captures seasonality without discontinuities.\n",
    "- Lag features use past observations up to 14 days.\n",
    "- Rolling statistics capture recent trends and variability.\n",
    "- Derived features like dew point and pressure tendency are common in meteorology.\n",
    "- The target is shifted by the forecast horizon (default 1 day ahead).\n",
    "\n",
    "---\n",
    "\n",
    "## **76.4 Modeling Approaches**\n",
    "\n",
    "Weather forecasting can be approached with:\n",
    "\n",
    "- **Persistence and climatology**: simple baselines.\n",
    "- **Statistical models**: ARIMA, VAR, etc.\n",
    "- **Machine learning**: random forests, gradient boosting, neural networks.\n",
    "- **Hybrid**: post\u2011processing of NWP output.\n",
    "\n",
    "Here we'll use a gradient boosting model (LightGBM) as a global model across all stations, similar to retail. This allows the model to learn shared patterns (e.g., seasonality, elevation effects) while still having station\u2011specific characteristics via the station ID as a categorical feature.\n",
    "\n",
    "We'll also demonstrate a simple **ensemble** approach by training multiple models with different random seeds and averaging predictions, which can improve robustness.\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "\n",
    "class WeatherForecaster:\n",
    "    def __init__(self, feature_columns, categorical_features=None, n_estimators=100):\n",
    "        self.feature_columns = feature_columns\n",
    "        self.categorical_features = categorical_features if categorical_features else []\n",
    "        self.n_estimators = n_estimators\n",
    "        self.models = []  # for ensemble\n",
    "    \n",
    "    def prepare_data(self, df, target_var='temperature'):\n",
    "        X = df[self.feature_columns]\n",
    "        y = df[f'target_{target_var}']\n",
    "        for col in self.categorical_features:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].astype('category')\n",
    "        return X, y\n",
    "    \n",
    "    def train_single(self, X_train, y_train, X_val, y_val):\n",
    "        \"\"\"Train a single LightGBM model with early stopping.\"\"\"\n",
    "        model = lgb.LGBMRegressor(\n",
    "            objective='regression',\n",
    "            num_leaves=31,\n",
    "            learning_rate=0.05,\n",
    "            feature_fraction=0.8,\n",
    "            bagging_fraction=0.8,\n",
    "            bagging_freq=5,\n",
    "            n_estimators=self.n_estimators,\n",
    "            verbose=-1\n",
    "        )\n",
    "        model.fit(\n",
    "            X_train, y_train,\n",
    "            eval_set=[(X_val, y_val)],\n",
    "            callbacks=[lgb.early_stopping(10)],\n",
    "            categorical_feature=self.categorical_features\n",
    "        )\n",
    "        return model\n",
    "    \n",
    "    def train_ensemble(self, X, y, n_splits=5, n_models=5):\n",
    "        \"\"\"\n",
    "        Train an ensemble of models using time\u2011series cross\u2011validation.\n",
    "        Each model is trained on a different training portion.\n",
    "        \"\"\"\n",
    "        tscv = TimeSeriesSplit(n_splits=n_splits)\n",
    "        self.models = []\n",
    "        \n",
    "        for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "            if fold >= n_models:\n",
    "                break\n",
    "            X_train, X_val = X.iloc[train_idx], X.iloc[val_idx]\n",
    "            y_train, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "            model = self.train_single(X_train, y_train, X_val, y_val)\n",
    "            self.models.append(model)\n",
    "            print(f\"Trained model {fold+1}/{min(n_models, n_splits)}\")\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Ensemble prediction: average of all models.\"\"\"\n",
    "        if not self.models:\n",
    "            raise ValueError(\"No models trained.\")\n",
    "        preds = np.zeros((len(X), len(self.models)))\n",
    "        for i, model in enumerate(self.models):\n",
    "            preds[:, i] = model.predict(X)\n",
    "        return preds.mean(axis=1)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The forecaster can train an ensemble of LightGBM models on different time periods (using time\u2011series splits). This is a simple form of ensemble that captures uncertainty and often improves accuracy.\n",
    "- Categorical features might include `station_id`, `month`, `season`, etc.\n",
    "- In prediction, we average the outputs of all models.\n",
    "\n",
    "---\n",
    "\n",
    "## **76.5 Evaluation Metrics for Weather Forecasts**\n",
    "\n",
    "Standard regression metrics (MAE, RMSE) are used, but meteorology also has specialised metrics:\n",
    "\n",
    "- **Anomaly Correlation Coefficient (ACC)**: Measures how well the forecast captures anomalies from climatology.\n",
    "- **Continuous Ranked Probability Score (CRPS)**: For probabilistic forecasts, measures the difference between predicted and observed cumulative distributions.\n",
    "- **Brier Score**: For binary events (e.g., rain yes/no).\n",
    "- **Equitable Threat Score (ETS)**: For categorical forecasts.\n",
    "\n",
    "We'll implement a few of these for evaluation.\n",
    "\n",
    "```python\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "def anomaly_correlation(forecast, observed, climatology):\n",
    "    \"\"\"\n",
    "    Compute anomaly correlation coefficient.\n",
    "    forecast, observed, climatology are 1D arrays.\n",
    "    \"\"\"\n",
    "    f_anom = forecast - climatology\n",
    "    o_anom = observed - climatology\n",
    "    corr, _ = pearsonr(f_anom, o_anom)\n",
    "    return corr\n",
    "\n",
    "def crps(forecast_dist, observed):\n",
    "    \"\"\"\n",
    "    Simplified CRPS for ensemble forecasts.\n",
    "    forecast_dist: array of ensemble members (n_members, n_points)\n",
    "    observed: array of observations (n_points)\n",
    "    Returns mean CRPS.\n",
    "    \"\"\"\n",
    "    n_members = forecast_dist.shape[0]\n",
    "    # CRPS = (1/n_members) * sum(|x_i - y|) - (1/(2*n_members^2)) * sum_{i,j} |x_i - x_j|\n",
    "    # This is a simplified version; in practice use proper libraries.\n",
    "    abs_error = np.abs(forecast_dist - observed).mean(axis=0)\n",
    "    # Pairwise differences within ensemble\n",
    "    pairwise_diff = 0\n",
    "    for i in range(n_members):\n",
    "        for j in range(n_members):\n",
    "            pairwise_diff += np.abs(forecast_dist[i] - forecast_dist[j])\n",
    "    pairwise_diff /= (2 * n_members**2)\n",
    "    crps = abs_error - pairwise_diff\n",
    "    return crps.mean()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- ACC requires a climatology baseline (e.g., long\u2011term average for each day of year). It's widely used in operational weather forecasting.\n",
    "- CRPS is a proper scoring rule for probabilistic forecasts; our implementation is a simplified version for ensemble forecasts.\n",
    "\n",
    "---\n",
    "\n",
    "## **76.6 Backtesting and Validation**\n",
    "\n",
    "We'll use walk\u2011forward validation (expanding window) to simulate realistic forecast conditions.\n",
    "\n",
    "```python\n",
    "class WeatherBacktester:\n",
    "    def __init__(self, forecaster, feature_columns, categorical_features,\n",
    "                 target_var='temperature', initial_train_days=365, step_days=1):\n",
    "        self.forecaster = forecaster\n",
    "        self.feature_columns = feature_columns\n",
    "        self.categorical_features = categorical_features\n",
    "        self.target_var = target_var\n",
    "        self.initial_train_days = initial_train_days\n",
    "        self.step_days = step_days\n",
    "    \n",
    "    def run(self, df):\n",
    "        df = df.sort_values('date').reset_index(drop=True)\n",
    "        unique_dates = df['date'].unique()\n",
    "        results = []\n",
    "        \n",
    "        for i in range(self.initial_train_days, len(unique_dates), self.step_days):\n",
    "            train_end_date = unique_dates[i-1]\n",
    "            test_date = unique_dates[i]\n",
    "            \n",
    "            train_df = df[df['date'] <= train_end_date]\n",
    "            test_df = df[df['date'] == test_date]\n",
    "            \n",
    "            if test_df.empty:\n",
    "                continue\n",
    "            \n",
    "            # Prepare data\n",
    "            X_train, y_train = self.forecaster.prepare_data(train_df, self.target_var)\n",
    "            X_test, y_test = self.forecaster.prepare_data(test_df, self.target_var)\n",
    "            \n",
    "            # Train ensemble on expanding window\n",
    "            # For speed, we might not retrain every step; here we do.\n",
    "            forecaster = WeatherForecaster(self.feature_columns, self.categorical_features)\n",
    "            forecaster.train_ensemble(X_train, y_train, n_splits=3, n_models=3)\n",
    "            \n",
    "            # Predict\n",
    "            y_pred = forecaster.predict(X_test)\n",
    "            \n",
    "            # Store\n",
    "            test_df = test_df.copy()\n",
    "            test_df['predicted'] = y_pred\n",
    "            test_df['error'] = y_pred - y_test.values\n",
    "            test_df['abs_error'] = np.abs(test_df['error'])\n",
    "            test_df['train_end_date'] = train_end_date\n",
    "            results.append(test_df[['station_id', 'date', f'target_{self.target_var}', 'predicted', 'error', 'abs_error']])\n",
    "        \n",
    "        results_df = pd.concat(results, ignore_index=True)\n",
    "        return results_df\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **76.7 Integration with the Prediction System**\n",
    "\n",
    "We can now integrate the weather components into our existing pipeline. The flow is:\n",
    "\n",
    "1. **Ingestion**: Load station data (CSV, NetCDF) using a custom `WeatherIngestion` class (similar to `RetailIngestion`).\n",
    "2. **Feature engineering**: Use `WeatherFeatureEngineer`.\n",
    "3. **Model training**: Use `WeatherForecaster` with time\u2011series CV.\n",
    "4. **Backtesting**: Use `WeatherBacktester`.\n",
    "5. **Deployment**: Batch prediction for all stations, possibly a REST API for a single station.\n",
    "6. **Monitoring**: Track forecast errors, detect drift (e.g., if model performance degrades during extreme events).\n",
    "\n",
    "We'll sketch a deployment script for daily batch prediction:\n",
    "\n",
    "```python\n",
    "class WeatherBatchPredictor:\n",
    "    def __init__(self, model, feature_engineer, feature_columns, categorical_features):\n",
    "        self.model = model\n",
    "        self.feature_engineer = feature_engineer\n",
    "        self.feature_columns = feature_columns\n",
    "        self.categorical_features = categorical_features\n",
    "    \n",
    "    def predict_next_day(self, df_today):\n",
    "        \"\"\"\n",
    "        df_today: DataFrame with today's data for all stations (including lagged features).\n",
    "        Returns predictions for tomorrow.\n",
    "        \"\"\"\n",
    "        # Compute features up to today (including lags)\n",
    "        df_features = self.feature_engineer.compute_features(df_today)\n",
    "        # Keep only today's rows (the ones we want to predict from)\n",
    "        # Actually, after compute_features, the last row for each station will have target tomorrow.\n",
    "        # For prediction, we need the feature vectors for today (which include lags up to today)\n",
    "        # The target is tomorrow, so we need to take the rows where date == today\n",
    "        today_str = df_today['date'].max()\n",
    "        df_today_features = df_features[df_features['date'] == today_str]\n",
    "        if df_today_features.empty:\n",
    "            raise ValueError(\"No data for today after feature engineering\")\n",
    "        X = df_today_features[self.feature_columns]\n",
    "        for col in self.categorical_features:\n",
    "            if col in X.columns:\n",
    "                X[col] = X[col].astype('category')\n",
    "        preds = self.model.predict(X)\n",
    "        # Return DataFrame with station_id and prediction\n",
    "        result = df_today_features[['station_id']].copy()\n",
    "        result['predicted_tomorrow'] = preds\n",
    "        return result\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **76.8 Case Study: Forecasting Temperature at Multiple Stations**\n",
    "\n",
    "Let's run a complete example with our synthetic data.\n",
    "\n",
    "```python\n",
    "# Generate data\n",
    "weather_df = generate_weather_data(num_stations=5, days=365*3)\n",
    "\n",
    "# Feature engineering\n",
    "engineer = WeatherFeatureEngineer()\n",
    "featured_df = engineer.compute_features(weather_df, target_var='temperature', forecast_horizon=1)\n",
    "print(featured_df.head())\n",
    "\n",
    "# Define categorical features\n",
    "cat_features = ['station_id', 'month']\n",
    "\n",
    "# Prepare forecaster\n",
    "forecaster = WeatherForecaster(engineer.feature_columns, categorical_features=cat_features, n_estimators=100)\n",
    "\n",
    "# Split into train/val/test (time-based)\n",
    "train_df = featured_df[featured_df['date'] < '2022-01-01']\n",
    "val_df = featured_df[(featured_df['date'] >= '2022-01-01') & (featured_df['date'] < '2022-07-01')]\n",
    "test_df = featured_df[featured_df['date'] >= '2022-07-01']\n",
    "\n",
    "X_train, y_train = forecaster.prepare_data(train_df)\n",
    "X_val, y_val = forecaster.prepare_data(val_df)\n",
    "X_test, y_test = forecaster.prepare_data(test_df)\n",
    "\n",
    "# Train ensemble\n",
    "forecaster.train_ensemble(pd.concat([X_train, X_val]), pd.concat([y_train, y_val]), n_splits=3, n_models=3)\n",
    "\n",
    "# Predict on test\n",
    "y_pred = forecaster.predict(X_test)\n",
    "\n",
    "# Evaluate\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test MAE: {mae:.2f}, RMSE: {rmse:.2f}\")\n",
    "\n",
    "# Optionally, compute anomaly correlation using climatology\n",
    "# For simplicity, we'll use day-of-year climatology from training\n",
    "clim_df = train_df.groupby('doy')['target_temperature'].mean().reset_index()\n",
    "test_with_clim = test_df.merge(clim_df, on='doy', suffixes=('', '_clim'))\n",
    "acc = anomaly_correlation(y_pred, y_test, test_with_clim['target_temperature_clim'].values)\n",
    "print(f\"Anomaly Correlation: {acc:.3f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **76.9 Lessons Learned from Weather Prediction**\n",
    "\n",
    "1. **Spatial context matters**: Including neighbour station data improved forecasts in our tests (not fully shown, but the idea is important).\n",
    "2. **Uncertainty quantification is essential**: Deterministic forecasts are rarely sufficient; ensembles or probabilistic methods provide valuable confidence intervals.\n",
    "3. **Seasonal cycles must be handled carefully**: Simple sin/cos encoding works well, but climate change may introduce non\u2011stationarity.\n",
    "4. **Model interpretability**: Meteorologists often need to understand why a forecast was made; SHAP values can help.\n",
    "5. **Data quality is paramount**: Missing data, instrument errors, and changes in station location must be tracked.\n",
    "\n",
    "---\n",
    "\n",
    "## **76.10 Future Directions**\n",
    "\n",
    "- **Deep learning**: Models like ConvLSTM, Graph Neural Networks (for spatial grids), and Transformers are state\u2011of\u2011the\u2011art.\n",
    "- **Integration with NWP**: Use NWP output as features (e.g., from GFS) and apply ML for post\u2011processing (MOS \u2013 Model Output Statistics).\n",
    "- **High\u2011resolution forecasting**: Downscaling coarse NWP to local stations.\n",
    "- **Climate projections**: Extending to seasonal or decadal timescales.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we adapted our time\u2011series prediction system to the domain of weather forecasting. We generated synthetic station data, engineered temporal, lag, rolling, and derived features, trained an ensemble of gradient boosting models, and evaluated using weather\u2011specific metrics. We discussed the importance of spatial context and uncertainty quantification. The pipeline remains flexible and can be extended to more sophisticated models and real datasets.\n",
    "\n",
    "This concludes our exploration of three diverse domains: finance (NEPSE), retail, and weather. The principles of robust data ingestion, feature engineering, model validation, and deployment apply universally.\n",
    "\n",
    "In the next chapter, we will dive into **Healthcare Prediction Systems**, addressing unique challenges such as privacy, regulatory compliance, and model interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 76**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='75. retail_sales_forecasting_system.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='77. healthcare_prediction_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}