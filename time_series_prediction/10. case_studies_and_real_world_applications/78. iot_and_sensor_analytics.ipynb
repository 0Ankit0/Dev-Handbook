{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 78: IoT and Sensor Analytics**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the unique characteristics of IoT sensor data: high velocity, volume, variety, and veracity.\n",
    "- Design data ingestion pipelines for streaming sensor data using message brokers (MQTT, Kafka).\n",
    "- Engineer features from raw sensor readings, including time\u2011domain, frequency\u2011domain, and statistical features.\n",
    "- Implement real\u2011time anomaly detection and predictive maintenance models.\n",
    "- Deploy models at the edge for low\u2011latency inference.\n",
    "- Integrate IoT analytics with the monitoring and alerting system developed in Chapter 73.\n",
    "- Evaluate models using domain\u2011specific metrics such as remaining useful life (RUL) accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.1 Introduction to IoT and Sensor Analytics**\n",
    "\n",
    "The Internet of Things (IoT) generates vast amounts of time\u2011series data from sensors embedded in machines, environments, and wearables. Predicting equipment failures, detecting anomalies, and optimising operations based on this data can deliver immense value.\n",
    "\n",
    "Sensor data differs from the time series we have seen so far (financial, retail, weather) in several ways:\n",
    "\n",
    "- **High frequency**: Sensors may sample at rates from 1 Hz to many kHz.\n",
    "- **Multivariate**: Many correlated channels (e.g., vibration, temperature, pressure).\n",
    "- **Noisy**: Electrical interference, calibration drift.\n",
    "- **Missing data**: Communication dropouts, sensor failures.\n",
    "- **Spatial distribution**: Sensors are often deployed across a physical asset or environment.\n",
    "\n",
    "A common application is **predictive maintenance**: using sensor data to predict when a machine will fail, so that maintenance can be scheduled just in time. This avoids unplanned downtime and reduces costs.\n",
    "\n",
    "In this chapter, we will build an IoT analytics system for a simulated industrial machine (e.g., a pump or a motor) equipped with multiple sensors. We will generate synthetic data that mimics normal operation and developing faults. Then we will:\n",
    "\n",
    "1. Ingest streaming data.\n",
    "2. Engineer features.\n",
    "3. Train a model to predict remaining useful life (RUL) or detect anomalies.\n",
    "4. Deploy the model for real\u2011time inference.\n",
    "5. Trigger alerts when anomalies are detected (integrating with Chapter 73).\n",
    "\n",
    "---\n",
    "\n",
    "## **78.2 Sensor Data Characteristics and Challenges**\n",
    "\n",
    "Before building the pipeline, let's understand the data characteristics.\n",
    "\n",
    "### **78.2.1 Velocity**\n",
    "Sensor data often arrives at high rates. A single machine might produce thousands of readings per second. Aggregating over time windows (e.g., 1\u2011second averages) is common to reduce the load while preserving signal.\n",
    "\n",
    "### **78.2.2 Volume**\n",
    "With many sensors and high frequencies, data volumes can be enormous. Efficient storage (e.g., time\u2011series databases like InfluxDB, or compressed columnar formats) and stream processing are essential.\n",
    "\n",
    "### **78.2.3 Variety**\n",
    "Different sensors measure different physical quantities: vibration (accelerometers), temperature (thermocouples), pressure, current, etc. They may have different units, ranges, and sampling rates.\n",
    "\n",
    "### **78.2.4 Veracity**\n",
    "Sensor data is noisy. Electrical noise, quantization error, and occasional dropouts must be handled. Outliers may indicate faults or just spurious readings.\n",
    "\n",
    "### **78.2.5 Temporal Dependencies**\n",
    "Faults develop over time. Features must capture trends, cycles, and changes in the signal characteristics (e.g., increasing vibration amplitude).\n",
    "\n",
    "---\n",
    "\n",
    "## **78.3 Generating Synthetic Sensor Data**\n",
    "\n",
    "To illustrate the concepts, we'll generate synthetic data for a rotating machine (e.g., a pump). We'll simulate three sensors:\n",
    "\n",
    "- **Vibration** (accelerometer): amplitude and frequency content change as bearings wear.\n",
    "- **Temperature**: rises slowly as friction increases.\n",
    "- **Current**: motor current draw increases with load or friction.\n",
    "\n",
    "We'll generate data for a machine that runs continuously and eventually fails after a certain amount of time. The data will include:\n",
    "\n",
    "- A baseline healthy period.\n",
    "- A degradation period where a fault develops.\n",
    "- A failure point.\n",
    "\n",
    "We'll use a simple model: remaining useful life (RUL) decreases linearly, and sensor signals evolve accordingly.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def generate_machine_data(machine_id=1, duration_hours=1000, sampling_rate_hz=1, seed=42):\n",
    "    \"\"\"\n",
    "    Generate synthetic sensor data for a machine.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    machine_id : int\n",
    "        Identifier for the machine.\n",
    "    duration_hours : float\n",
    "        Total run time until failure (hours).\n",
    "    sampling_rate_hz : int\n",
    "        Samples per second (simplified, here we use per minute for demo).\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    df : pd.DataFrame\n",
    "        Columns: timestamp, machine_id, vibration, temperature, current, fault_progress, rul\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Total number of samples (using minutes for simplicity)\n",
    "    total_minutes = int(duration_hours * 60)\n",
    "    timestamps = [datetime.now() + timedelta(minutes=i) for i in range(total_minutes)]\n",
    "    \n",
    "    # Fault progress: from 0 (healthy) to 1 (failure)\n",
    "    fault_progress = np.linspace(0, 1, total_minutes)\n",
    "    \n",
    "    # Remaining useful life (minutes)\n",
    "    rul = np.linspace(total_minutes, 0, total_minutes)\n",
    "    \n",
    "    # Sensor signals\n",
    "    vibration = np.zeros(total_minutes)\n",
    "    temperature = np.zeros(total_minutes)\n",
    "    current = np.zeros(total_minutes)\n",
    "    \n",
    "    for i, progress in enumerate(fault_progress):\n",
    "        # Vibration: amplitude increases, frequency content shifts (simplified)\n",
    "        # Add sinusoidal component with increasing amplitude and frequency modulation\n",
    "        t = i / 60.0  # time in hours\n",
    "        # Base vibration (healthy)\n",
    "        base_vib = 0.1 * np.sin(2 * np.pi * 10 * t) + 0.05 * np.random.randn()\n",
    "        # Fault contribution: amplitude grows exponentially\n",
    "        fault_vib = 0.5 * progress**2 * np.sin(2 * np.pi * (10 + 20*progress) * t)\n",
    "        vibration[i] = base_vib + fault_vib + 0.02 * np.random.randn()\n",
    "        \n",
    "        # Temperature: slowly rises with fault\n",
    "        temperature[i] = 25 + 10 * progress + 2 * np.random.randn()\n",
    "        \n",
    "        # Current: increases with load/friction\n",
    "        current[i] = 10 + 5 * progress + 1 * np.random.randn()\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'timestamp': timestamps,\n",
    "        'machine_id': machine_id,\n",
    "        'vibration': vibration,\n",
    "        'temperature': temperature,\n",
    "        'current': current,\n",
    "        'fault_progress': fault_progress,\n",
    "        'rul': rul\n",
    "    })\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Generate data for one machine\n",
    "df_machine = generate_machine_data(machine_id=1, duration_hours=500, sampling_rate_hz=1)  # per minute\n",
    "print(df_machine.head())\n",
    "\n",
    "# Plot the signals\n",
    "fig, axes = plt.subplots(3, 1, figsize=(12, 8), sharex=True)\n",
    "axes[0].plot(df_machine['timestamp'], df_machine['vibration'])\n",
    "axes[0].set_ylabel('Vibration')\n",
    "axes[1].plot(df_machine['timestamp'], df_machine['temperature'])\n",
    "axes[1].set_ylabel('Temperature')\n",
    "axes[2].plot(df_machine['timestamp'], df_machine['current'])\n",
    "axes[2].set_ylabel('Current')\n",
    "axes[2].set_xlabel('Time')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We simulate a machine running for `duration_hours`. The fault progresses linearly from 0 (healthy) to 1 (failure).\n",
    "- Vibration is modelled as a sinusoidal signal whose amplitude and frequency increase with fault progress, plus noise.\n",
    "- Temperature and current also increase with fault progress.\n",
    "- The target variable for prediction could be `rul` (remaining useful life) or a binary `fault` indicator.\n",
    "\n",
    "In reality, you would collect such data from actual machines, possibly with run\u2011to\u2011failure experiments or historical maintenance logs.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.4 Data Ingestion for Streaming Sensor Data**\n",
    "\n",
    "In an IoT system, data arrives continuously. We need a scalable ingestion layer. Common choices:\n",
    "\n",
    "- **MQTT**: Lightweight pub\u2011sub protocol, ideal for sensors.\n",
    "- **Kafka**: Distributed streaming platform, handles high throughput.\n",
    "- **Amazon Kinesis / Google Pub/Sub**: Cloud\u2011managed alternatives.\n",
    "\n",
    "For our example, we'll simulate a Kafka producer that sends sensor readings in real time. We'll then consume them with a streaming processing engine (e.g., Apache Flink, Spark Streaming) or simply with a Python script using the `kafka-python` library.\n",
    "\n",
    "We'll create a producer that reads from our generated DataFrame and publishes messages to a Kafka topic.\n",
    "\n",
    "```python\n",
    "# Simulated Kafka producer (requires kafka-python)\n",
    "# pip install kafka-python\n",
    "\n",
    "from kafka import KafkaProducer\n",
    "import json\n",
    "import time\n",
    "\n",
    "def kafka_producer_simulation(df, topic='sensor-data', bootstrap_servers='localhost:9092'):\n",
    "    \"\"\"\n",
    "    Simulate streaming data by publishing each row to Kafka with a delay.\n",
    "    \"\"\"\n",
    "    producer = KafkaProducer(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        value_serializer=lambda v: json.dumps(v).encode('utf-8')\n",
    "    )\n",
    "    \n",
    "    for _, row in df.iterrows():\n",
    "        message = {\n",
    "            'timestamp': row['timestamp'].isoformat(),\n",
    "            'machine_id': int(row['machine_id']),\n",
    "            'vibration': float(row['vibration']),\n",
    "            'temperature': float(row['temperature']),\n",
    "            'current': float(row['current'])\n",
    "        }\n",
    "        producer.send(topic, value=message)\n",
    "        print(f\"Sent: {message}\")\n",
    "        time.sleep(0.01)  # simulate real-time (10 ms between readings = 100 Hz)\n",
    "    \n",
    "    producer.flush()\n",
    "    producer.close()\n",
    "\n",
    "# In practice, you would run this in a separate process or container.\n",
    "# For demonstration, we'll skip actual Kafka and process directly from the DataFrame.\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The producer serializes each row as JSON and sends it to a Kafka topic.\n",
    "- The delay controls the simulated data rate; in a real system, sensors would push data at their natural frequency.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.5 Feature Engineering for Sensor Data**\n",
    "\n",
    "Raw sensor readings are rarely used directly. We need to extract features that capture the machine's health state. Common feature families:\n",
    "\n",
    "- **Time\u2011domain statistical features**: mean, variance, skewness, kurtosis, peak\u2011to\u2011peak, RMS over windows.\n",
    "- **Frequency\u2011domain features**: FFT magnitudes at key frequencies, spectral power in bands.\n",
    "- **Time\u2011frequency features**: wavelet coefficients, spectrograms.\n",
    "- **Domain\u2011specific features**: e.g., for vibration, bearing fault frequencies.\n",
    "\n",
    "We'll implement a streaming feature extractor that computes rolling window statistics and spectral features.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "from scipy.fft import fft, fftfreq\n",
    "from scipy.stats import skew, kurtosis\n",
    "\n",
    "class StreamingFeatureExtractor:\n",
    "    \"\"\"\n",
    "    Extracts features from streaming sensor data using sliding windows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, window_size=60, step_size=1, sampling_rate=1.0):\n",
    "        \"\"\"\n",
    "        window_size: number of samples in each window.\n",
    "        step_size: number of samples to slide each step.\n",
    "        sampling_rate: Hz, for frequency calculations.\n",
    "        \"\"\"\n",
    "        self.window_size = window_size\n",
    "        self.step_size = step_size\n",
    "        self.sampling_rate = sampling_rate\n",
    "        self.buffer = {i: [] for i in range(1, 6)}  # machine_id -> list of samples\n",
    "        self.feature_names = []\n",
    "    \n",
    "    def _time_features(self, data):\n",
    "        \"\"\"Compute time-domain statistical features.\"\"\"\n",
    "        features = {\n",
    "            'mean': np.mean(data),\n",
    "            'std': np.std(data),\n",
    "            'min': np.min(data),\n",
    "            'max': np.max(data),\n",
    "            'range': np.max(data) - np.min(data),\n",
    "            'rms': np.sqrt(np.mean(data**2)),\n",
    "            'skew': skew(data) if len(data)>2 else 0,\n",
    "            'kurtosis': kurtosis(data) if len(data)>2 else 0\n",
    "        }\n",
    "        return features\n",
    "    \n",
    "    def _freq_features(self, data):\n",
    "        \"\"\"Compute frequency-domain features.\"\"\"\n",
    "        n = len(data)\n",
    "        if n < 2:\n",
    "            return {}\n",
    "        fft_vals = fft(data)\n",
    "        fft_abs = np.abs(fft_vals[:n//2])\n",
    "        freqs = fftfreq(n, d=1/self.sampling_rate)[:n//2]\n",
    "        \n",
    "        # Dominant frequency\n",
    "        dominant_freq = freqs[np.argmax(fft_abs[1:])+1] if len(fft_abs)>1 else 0\n",
    "        # Spectral centroid\n",
    "        if np.sum(fft_abs) > 0:\n",
    "            centroid = np.sum(freqs * fft_abs) / np.sum(fft_abs)\n",
    "        else:\n",
    "            centroid = 0\n",
    "        # Power in bands (e.g., 0-10 Hz, 10-20 Hz, ...)\n",
    "        bands = [(0,10), (10,20), (20,50), (50,100)]\n",
    "        band_powers = {}\n",
    "        for (low, high) in bands:\n",
    "            mask = (freqs >= low) & (freqs < high)\n",
    "            band_powers[f'power_{low}_{high}'] = np.sum(fft_abs[mask]) if np.any(mask) else 0\n",
    "        \n",
    "        return {\n",
    "            'dominant_freq': dominant_freq,\n",
    "            'spectral_centroid': centroid,\n",
    "            **band_powers\n",
    "        }\n",
    "    \n",
    "    def process_sample(self, machine_id, timestamp, vibration, temperature, current):\n",
    "        \"\"\"\n",
    "        Add a new sample to the buffer. If a window is completed, compute and return features.\n",
    "        \"\"\"\n",
    "        if machine_id not in self.buffer:\n",
    "            self.buffer[machine_id] = []\n",
    "        \n",
    "        self.buffer[machine_id].append({\n",
    "            'timestamp': timestamp,\n",
    "            'vibration': vibration,\n",
    "            'temperature': temperature,\n",
    "            'current': current\n",
    "        })\n",
    "        \n",
    "        # If we have enough samples, process the oldest window\n",
    "        if len(self.buffer[machine_id]) >= self.window_size:\n",
    "            # Take the first window_size samples\n",
    "            window = self.buffer[machine_id][:self.window_size]\n",
    "            # Remove those samples from buffer (sliding window)\n",
    "            self.buffer[machine_id] = self.buffer[machine_id][self.step_size:]\n",
    "            \n",
    "            # Extract features\n",
    "            vib_data = [w['vibration'] for w in window]\n",
    "            temp_data = [w['temperature'] for w in window]\n",
    "            curr_data = [w['current'] for w in window]\n",
    "            \n",
    "            features = {\n",
    "                'timestamp': window[-1]['timestamp'],  # timestamp of last sample in window\n",
    "                'machine_id': machine_id\n",
    "            }\n",
    "            \n",
    "            for sensor, data in [('vib', vib_data), ('temp', temp_data), ('curr', curr_data)]:\n",
    "                time_feat = self._time_features(data)\n",
    "                for k, v in time_feat.items():\n",
    "                    features[f'{sensor}_{k}'] = v\n",
    "                # Frequency features only for vibration (can also do for others if relevant)\n",
    "                if sensor == 'vib':\n",
    "                    freq_feat = self._freq_features(data)\n",
    "                    for k, v in freq_feat.items():\n",
    "                        features[f'{sensor}_{k}'] = v\n",
    "            \n",
    "            return features\n",
    "        else:\n",
    "            return None\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The `StreamingFeatureExtractor` maintains a buffer per machine. When enough samples are accumulated, it computes a feature vector for the oldest window and then slides.\n",
    "- Time\u2011domain features include basic statistics and higher\u2011order moments.\n",
    "- Frequency\u2011domain features use FFT to extract dominant frequency, spectral centroid, and band powers.\n",
    "- The window size and step size control the trade\u2011off between latency and feature richness. For predictive maintenance, a window of a few minutes to hours is common.\n",
    "- This extractor can be used in a streaming pipeline: each new sample is passed to `process_sample`, and if a feature vector is returned, it is sent to the model for inference.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.6 Modeling Approaches for Predictive Maintenance**\n",
    "\n",
    "We can formulate predictive maintenance as:\n",
    "\n",
    "- **Regression**: predict remaining useful life (RUL) \u2013 a continuous value.\n",
    "- **Classification**: predict whether a fault will occur within a certain time window (e.g., next 24 hours).\n",
    "- **Anomaly detection**: flag unusual behaviour that may indicate a developing fault.\n",
    "\n",
    "We'll demonstrate both regression (RUL prediction) and anomaly detection.\n",
    "\n",
    "### **78.6.1 Regression for Remaining Useful Life**\n",
    "\n",
    "Using the engineered features, we can train a model to predict RUL. Because RUL decreases monotonically, we must ensure no data leakage (i.e., we only use information available at the time of prediction).\n",
    "\n",
    "We'll use a random forest regressor.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "# Assume we have a DataFrame `features_df` with columns: timestamp, machine_id, features..., rul\n",
    "# We need to split by time to avoid leakage\n",
    "def prepare_rul_data(features_df, test_size=0.2):\n",
    "    features_df = features_df.sort_values('timestamp')\n",
    "    split_idx = int(len(features_df) * (1 - test_size))\n",
    "    train = features_df.iloc[:split_idx]\n",
    "    test = features_df.iloc[split_idx:]\n",
    "    \n",
    "    feature_cols = [c for c in features_df.columns if c not in ['timestamp', 'machine_id', 'rul']]\n",
    "    X_train = train[feature_cols]\n",
    "    y_train = train['rul']\n",
    "    X_test = test[feature_cols]\n",
    "    y_test = test['rul']\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, feature_cols\n",
    "\n",
    "# Example usage (assuming we already have features_df)\n",
    "# X_train, y_train, X_test, y_test, feat_cols = prepare_rul_data(features_df)\n",
    "# model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "# model.fit(X_train, y_train)\n",
    "# y_pred = model.predict(X_test)\n",
    "# mae = mean_absolute_error(y_test, y_pred)\n",
    "# print(f\"RUL MAE: {mae:.2f} minutes\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We split the data chronologically to avoid using future information.\n",
    "- The target `rul` is the remaining life in minutes at the time of the feature window.\n",
    "- The model can then be used in real time: for each new feature window, predict RUL.\n",
    "\n",
    "### **78.6.2 Anomaly Detection**\n",
    "\n",
    "Alternatively, we can detect when the machine starts to deviate from normal behaviour. This can be done with:\n",
    "\n",
    "- **Statistical methods**: control charts, moving thresholds.\n",
    "- **Machine learning**: one\u2011class SVM, isolation forest, autoencoders.\n",
    "\n",
    "We'll implement a simple autoencoder for anomaly detection. An autoencoder is trained on normal (healthy) data only; when reconstruction error exceeds a threshold, an anomaly is flagged.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "def build_autoencoder(input_dim, encoding_dim=16):\n",
    "    model = keras.Sequential([\n",
    "        layers.Dense(64, activation='relu', input_shape=(input_dim,)),\n",
    "        layers.Dense(encoding_dim, activation='relu'),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(input_dim, activation='linear')\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse')\n",
    "    return model\n",
    "\n",
    "# Train on healthy data only\n",
    "# healthy_data = features_df[features_df['rul'] > some_threshold]  # e.g., RUL > 100\n",
    "# X_healthy = healthy_data[feat_cols]\n",
    "# autoencoder = build_autoencoder(len(feat_cols))\n",
    "# autoencoder.fit(X_healthy, X_healthy, epochs=50, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# For each new sample, compute reconstruction error\n",
    "# reconstructions = autoencoder.predict(X_test)\n",
    "# mse = np.mean(np.square(X_test - reconstructions), axis=1)\n",
    "# If mse > threshold, anomaly\n",
    "\n",
    "# Determine threshold from training (e.g., 95th percentile of training errors)\n",
    "# threshold = np.percentile(train_errors, 95)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Autoencoders learn to compress and reconstruct normal patterns.\n",
    "- When a fault develops, the reconstruction error increases because the model hasn't seen such patterns.\n",
    "- The threshold can be set based on the distribution of errors on healthy validation data.\n",
    "- Anomaly detection is useful when you don't have labelled failure data (common in real life).\n",
    "\n",
    "---\n",
    "\n",
    "## **78.7 Real\u2011Time Processing with Stream Processing Frameworks**\n",
    "\n",
    "For production IoT systems, you need a stream processing engine that can handle high throughput, stateful operations (like our sliding windows), and integration with machine learning models. Popular choices:\n",
    "\n",
    "- **Apache Flink**: Provides exactly\u2011once semantics, event time processing, and a machine learning library (FlinkML).\n",
    "- **Apache Spark Streaming**: Micro\u2011batch processing, integrates with MLlib.\n",
    "- **Kafka Streams**: Lightweight library that runs inside your application.\n",
    "\n",
    "We'll illustrate a conceptual pipeline using Kafka and a Python consumer that uses our `StreamingFeatureExtractor` and then calls the model.\n",
    "\n",
    "```python\n",
    "# Conceptual streaming consumer (using kafka-python)\n",
    "from kafka import KafkaConsumer\n",
    "import json\n",
    "import joblib\n",
    "\n",
    "# Load pre\u2011trained model and feature extractor\n",
    "model = joblib.load(\"rul_model.pkl\")\n",
    "feature_extractor = StreamingFeatureExtractor(window_size=60, step_size=1, sampling_rate=1.0)\n",
    "\n",
    "consumer = KafkaConsumer(\n",
    "    'sensor-data',\n",
    "    bootstrap_servers='localhost:9092',\n",
    "    value_deserializer=lambda m: json.loads(m.decode('utf-8'))\n",
    ")\n",
    "\n",
    "for message in consumer:\n",
    "    data = message.value\n",
    "    machine_id = data['machine_id']\n",
    "    timestamp = data['timestamp']\n",
    "    vib = data['vibration']\n",
    "    temp = data['temperature']\n",
    "    curr = data['current']\n",
    "    \n",
    "    # Extract features (may return None if window not complete)\n",
    "    features = feature_extractor.process_sample(machine_id, timestamp, vib, temp, curr)\n",
    "    \n",
    "    if features is not None:\n",
    "        # Prepare feature vector for model\n",
    "        # Assume feature extractor returns a dict with all features\n",
    "        X = np.array([[features[f] for f in feature_cols]])  # feature_cols must be defined\n",
    "        rul_pred = model.predict(X)[0]\n",
    "        print(f\"Machine {machine_id} at {timestamp}: predicted RUL = {rul_pred:.1f} minutes\")\n",
    "        \n",
    "        # Optionally trigger alert if RUL < threshold\n",
    "        if rul_pred < 60:  # less than 1 hour\n",
    "            # Send alert to alert manager (Chapter 73)\n",
    "            pass\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The consumer listens to the sensor topic, processes each sample through the feature extractor, and when a window is complete, it runs inference.\n",
    "- The predicted RUL can be used to trigger maintenance alerts.\n",
    "- This architecture scales by partitioning the Kafka topic by `machine_id` and running multiple consumer instances.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.8 Edge Deployment**\n",
    "\n",
    "Many IoT applications require low\u2011latency inference at the edge (on the device itself) to avoid sending all data to the cloud. Edge deployment involves:\n",
    "\n",
    "- **Model compression**: quantization, pruning, knowledge distillation.\n",
    "- **Lightweight runtimes**: TensorFlow Lite, ONNX Runtime, or specialised hardware (e.g., ARM CMSIS\u2011NN).\n",
    "- **Local storage and buffering**: in case of network outages.\n",
    "\n",
    "We'll demonstrate how to convert a trained model to TensorFlow Lite and run it on a simulated edge device.\n",
    "\n",
    "```python\n",
    "# Convert Keras model to TFLite\n",
    "def convert_to_tflite(model, representative_dataset=None):\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(model)\n",
    "    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    if representative_dataset:\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_types = [tf.float16]\n",
    "    tflite_model = converter.convert()\n",
    "    return tflite_model\n",
    "\n",
    "# Save the model\n",
    "# with open('model.tflite', 'wb') as f:\n",
    "#     f.write(tflite_model)\n",
    "\n",
    "# Inference on edge (using Python, but could be C++ on device)\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# Load TFLite model\n",
    "interpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\n",
    "interpreter.allocate_tensors()\n",
    "\n",
    "input_details = interpreter.get_input_details()\n",
    "output_details = interpreter.get_output_details()\n",
    "\n",
    "# Prepare input data (e.g., a feature vector)\n",
    "input_data = np.array([feature_vector], dtype=np.float32)\n",
    "interpreter.set_tensor(input_details[0]['index'], input_data)\n",
    "interpreter.invoke()\n",
    "output_data = interpreter.get_tensor(output_details[0]['index'])\n",
    "print(\"Prediction:\", output_data)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The model is converted to TensorFlow Lite, which is optimized for mobile and edge devices.\n",
    "- Quantization reduces model size and speeds up inference, with minimal accuracy loss.\n",
    "- On the edge, you would embed the TFLite runtime in your device firmware or application.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.9 Integrating with Monitoring and Alerting (Chapter 73)**\n",
    "\n",
    "Our IoT analytics system can trigger alerts when anomalies are detected or when RUL falls below a threshold. We can reuse the `AlertManager` from Chapter 73.\n",
    "\n",
    "```python\n",
    "from alerting import AlertManager, AlertRule, SlackChannel\n",
    "\n",
    "# Initialize alert manager\n",
    "alert_manager = AlertManager()\n",
    "alert_manager.register_channel('slack', SlackChannel(webhook_url='https://hooks.slack.com/...'))\n",
    "\n",
    "# Define an alert rule for low RUL\n",
    "def low_rul_condition(row):\n",
    "    return row.get('rul_predicted', 1000) < 60  # less than 60 minutes\n",
    "\n",
    "low_rul_rule = AlertRule(\n",
    "    name=\"Low RUL Warning\",\n",
    "    condition=low_rul_condition,\n",
    "    severity=\"P1\",\n",
    "    channels=[\"slack\"],\n",
    "    cooldown_minutes=30,\n",
    "    description=\"Machine predicted to fail within 1 hour.\"\n",
    ")\n",
    "alert_manager.add_rule(low_rul_rule)\n",
    "\n",
    "# In the streaming consumer, after each prediction:\n",
    "# if features is not None:\n",
    "#     row = pd.Series({**features, 'rul_predicted': rul_pred})\n",
    "#     alert_manager.process_row(row)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The alert manager checks each prediction against rules and sends notifications.\n",
    "- Cooldown prevents flooding.\n",
    "- Integration with Slack or email ensures that maintenance teams are notified immediately.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.10 Case Study: Predictive Maintenance for a Fleet of Pumps**\n",
    "\n",
    "Let's combine all components into a complete case study.\n",
    "\n",
    "**Scenario**: A water treatment plant has 10 pumps. Each pump is equipped with vibration, temperature, and current sensors sampling at 1 Hz. We want to predict RUL and detect anomalies to schedule maintenance.\n",
    "\n",
    "**Steps**:\n",
    "\n",
    "1. **Data Generation**: Simulate 10 machines with different degradation rates.\n",
    "2. **Offline Training**: Use historical run\u2011to\u2011failure data to train a RUL regression model.\n",
    "3. **Streaming Pipeline**: Deploy Kafka, a consumer per machine (or partition), feature extraction, model inference.\n",
    "4. **Alerting**: Integrate with Slack to notify when RUL < 24 hours.\n",
    "5. **Dashboard**: Visualise current RUL and anomaly scores (using Grafana or a custom dashboard).\n",
    "\n",
    "We'll simulate the offline training with data from multiple machines.\n",
    "\n",
    "```python\n",
    "# Generate data for multiple machines\n",
    "def generate_fleet_data(num_machines=10, duration_hours=1000):\n",
    "    dfs = []\n",
    "    for mid in range(1, num_machines+1):\n",
    "        # Slight variation in degradation rates\n",
    "        df = generate_machine_data(machine_id=mid, duration_hours=duration_hours * np.random.uniform(0.8,1.2))\n",
    "        dfs.append(df)\n",
    "    return pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "fleet_df = generate_fleet_data(num_machines=10, duration_hours=800)\n",
    "# Save to parquet for later use\n",
    "fleet_df.to_parquet(\"fleet_data.parquet\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Each machine has its own lifespan, but the degradation pattern is similar.\n",
    "- We can train a global model across all machines, which may generalise better.\n",
    "\n",
    "Now we run the streaming pipeline (simulated here with a loop through the data).\n",
    "\n",
    "```python\n",
    "# Load pre\u2011trained model and feature extractor\n",
    "# (Assume we have already trained and saved them)\n",
    "model = joblib.load(\"rul_model.pkl\")\n",
    "feature_cols = joblib.load(\"feature_cols.pkl\")\n",
    "extractor = StreamingFeatureExtractor(window_size=60, step_size=1, sampling_rate=1.0)\n",
    "\n",
    "# Simulate streaming by iterating through the fleet data in time order\n",
    "fleet_df = fleet_df.sort_values('timestamp')\n",
    "for _, row in fleet_df.iterrows():\n",
    "    features = extractor.process_sample(\n",
    "        row['machine_id'],\n",
    "        row['timestamp'],\n",
    "        row['vibration'],\n",
    "        row['temperature'],\n",
    "        row['current']\n",
    "    )\n",
    "    if features:\n",
    "        # Prepare feature vector\n",
    "        X = np.array([[features[col] for col in feature_cols]])\n",
    "        rul_pred = model.predict(X)[0]\n",
    "        print(f\"Machine {row['machine_id']} at {row['timestamp']}: RUL={rul_pred:.1f} min\")\n",
    "        # Optionally check alert\n",
    "        if rul_pred < 60:\n",
    "            # Send alert (using alert manager)\n",
    "            pass\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This simulates real\u2011time processing. In production, the loop would be replaced by a Kafka consumer.\n",
    "- The feature extractor maintains per\u2011machine state internally.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.11 Lessons Learned from IoT Analytics**\n",
    "\n",
    "1. **Data quality is paramount**: Sensor drift, missing data, and outliers must be handled early.\n",
    "2. **Feature engineering is domain\u2011specific**: Understanding the physics of the machine helps design meaningful features (e.g., bearing fault frequencies).\n",
    "3. **Stream processing requires careful state management**: Our sliding window buffer is stateful; in distributed systems, use state stores (e.g., Flink's keyed state).\n",
    "4. **Model degradation over time**: Machines change, and models may need periodic retraining. Monitor prediction errors and trigger retraining when drift is detected.\n",
    "5. **Edge vs. cloud trade\u2011offs**: Edge reduces latency and bandwidth but limits model complexity. Choose based on requirements.\n",
    "6. **Alerting must be actionable**: Too many false alarms lead to alert fatigue; tune thresholds carefully.\n",
    "\n",
    "---\n",
    "\n",
    "## **78.12 Future Improvements**\n",
    "\n",
    "- **Incorporate more sensors**: e.g., acoustic emissions, oil debris monitoring.\n",
    "- **Use deep learning for end\u2011to\u2011end feature learning**: Convolutional or recurrent networks on raw time series.\n",
    "- **Multi\u2011machine correlation**: Learn from the fleet to improve individual predictions.\n",
    "- **Remaining useful life with uncertainty**: Provide prediction intervals using quantile regression or Bayesian methods.\n",
    "- **Automated retraining pipeline**: When new failure data arrives, retrain and deploy models automatically.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we built a complete IoT analytics system for predictive maintenance. We generated synthetic sensor data, designed a streaming feature extractor, trained models for RUL regression and anomaly detection, and deployed them in a real\u2011time pipeline. We integrated alerting from Chapter 73 and discussed edge deployment. The system demonstrates how time\u2011series prediction extends to high\u2011velocity sensor data, with unique challenges in stateful stream processing and real\u2011time inference.\n",
    "\n",
    "This chapter concludes our series of domain\u2011specific adaptations. The next chapter, **Energy Demand Forecasting**, will apply similar principles to the energy sector.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 78**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='77. healthcare_prediction_systems.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='79. energy_demand_forecasting.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}