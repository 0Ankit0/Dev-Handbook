{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 20: Data Splitting Strategies**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand why standard random splitting fails for time\u2011series data\n",
    "- Recognize the problems caused by data leakage in temporal settings\n",
    "- Implement proper train\u2011validation\u2011test splits using time\u2011based cutoffs\n",
    "- Apply walk\u2011forward, rolling window, and expanding window validation techniques\n",
    "- Use purging and embargoing to prevent leakage in cross\u2011validation\n",
    "- Execute time\u2011series cross\u2011validation with `TimeSeriesSplit` and its variants\n",
    "- Handle multiple assets (symbols) correctly when splitting\n",
    "- Choose the right splitting strategy for your forecasting problem\n",
    "- Avoid common pitfalls that lead to overoptimistic performance estimates\n",
    "\n",
    "---\n",
    "\n",
    "## **20.1 Why Time\u2011Series Splitting is Different**\n",
    "\n",
    "In standard machine learning, data points are assumed to be independent and identically distributed (i.i.d.). Therefore, we can randomly shuffle the data and split it into training, validation, and test sets without worrying about temporal order. However, time\u2011series data violates the i.i.d. assumption because observations are ordered in time and exhibit autocorrelation, trends, and seasonality.\n",
    "\n",
    "Using a random split on time\u2011series data would allow the model to be trained on future data and tested on past data, which is impossible in a real\u2011world deployment. This leads to **data leakage**: the model sees information from the future during training, artificially inflating performance metrics. When deployed, the model fails because it cannot access future data.\n",
    "\n",
    "For the NEPSE prediction system, a random split might place January 2024 data in the training set and December 2023 data in the test set. The model would then \"learn\" patterns that rely on future events, like a market reaction to a budget announcement that happened in June, and then \"predict\" prices in December as if it knew the budget outcome. This is nonsense and would never work in live trading.\n",
    "\n",
    "Therefore, we must always respect the temporal order: **training data must come before validation data, which must come before test data**.\n",
    "\n",
    "### **20.1.1 The Arrow of Time**\n",
    "\n",
    "The fundamental principle of time\u2011series splitting is that we can only use past information to predict the future. When we split, we choose a cutoff date. All data before that date is used for training; all data after is used for validation or testing. This mimics the real scenario where we train a model on historical data and then use it to forecast future, unseen periods.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load NEPSE data\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# For simplicity, we'll work with one symbol (e.g., 'NEPSE' or a specific stock)\n",
    "symbol = df['Symbol'].unique()[0]  # pick first symbol\n",
    "df_one = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Plot the closing price to see the time order\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(df_one['Date'], df_one['Close'])\n",
    "plt.axvline(x=pd.to_datetime('2023-06-01'), color='r', linestyle='--', label='Potential split date')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Close Price (NPR)')\n",
    "plt.title(f'{symbol} - Closing Price Over Time')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This plot shows the natural progression of time. Any split must preserve this order. The red dashed line indicates a possible cutoff: data to the left (past) is training, to the right (future) is testing.\n",
    "- In practice, we would choose a cutoff that leaves enough test data for reliable evaluation (e.g., the last 20% of the period).\n",
    "\n",
    "---\n",
    "\n",
    "## **20.2 Random Split Problems**\n",
    "\n",
    "Let's demonstrate why random splitting is disastrous for time\u2011series. We'll create a simple feature (lagged return) and target (next day's return), then perform a random split and evaluate a model. The performance will look artificially good because the model uses future information.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Create a simple feature: yesterday's return\n",
    "df_one['Return'] = df_one['Close'].pct_change()\n",
    "df_one['Feature'] = df_one['Return'].shift(1)  # yesterday's return\n",
    "df_one['Target'] = df_one['Return'].shift(-1)  # tomorrow's return\n",
    "\n",
    "# Drop NaN rows\n",
    "df_ml = df_one[['Feature', 'Target']].dropna()\n",
    "\n",
    "# Random split (WRONG for time series)\n",
    "X = df_ml[['Feature']]\n",
    "y = df_ml['Target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a model\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Random split RMSE: {rmse:.6f}\")\n",
    "\n",
    "# Check temporal order in test set\n",
    "test_indices = X_test.index\n",
    "print(f\"Test set indices (should be random): {sorted(test_indices)[:10]}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The random split shuffles the data, so the training set contains both early and late observations, and the test set also contains a mix. This means the model can learn patterns that depend on the future relative to some training points. For example, if a similar pattern occurred in both 2022 and 2023, the model might see the 2023 pattern during training and then be tested on the 2022 pattern \u2013 but in reality, we can't know 2023 when predicting 2022.\n",
    "- The RMSE might look reasonable, but it's an illusion. When we deploy such a model, it will fail because future data is not available at prediction time.\n",
    "- The printed indices confirm that the test set includes a mix of early and late dates \u2013 exactly the problem.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.3 Train\u2011Validation\u2011Test Split**\n",
    "\n",
    "The standard approach in machine learning is to have three sets: training (for model fitting), validation (for hyperparameter tuning and model selection), and test (for final unbiased evaluation). For time\u2011series, we create these by choosing two cutoff dates.\n",
    "\n",
    "```python\n",
    "# Define cutoff dates\n",
    "train_end = '2022-12-31'\n",
    "val_end = '2023-06-30'\n",
    "\n",
    "# Split\n",
    "train = df_one[df_one['Date'] < train_end]\n",
    "val = df_one[(df_one['Date'] >= train_end) & (df_one['Date'] < val_end)]\n",
    "test = df_one[df_one['Date'] >= val_end]\n",
    "\n",
    "print(f\"Train: {train['Date'].min()} to {train['Date'].max()} ({len(train)} rows)\")\n",
    "print(f\"Validation: {val['Date'].min()} to {val['Date'].max()} ({len(val)} rows)\")\n",
    "print(f\"Test: {test['Date'].min()} to {test['Date'].max()} ({len(test)} rows)\")\n",
    "\n",
    "# Now create features and targets for each set\n",
    "def prepare_data(df, feature_cols, target_col):\n",
    "    df = df.copy()\n",
    "    # Create features and target (assuming they are already computed)\n",
    "    X = df[feature_cols]\n",
    "    y = df[target_col]\n",
    "    return X, y\n",
    "\n",
    "# Example: use lagged return as feature, next day's return as target\n",
    "feature_cols = ['Return_Lag1']  # assume we created this column\n",
    "target_col = 'Target_Return_t+1'\n",
    "\n",
    "# Ensure columns exist (we need to compute them for each split separately to avoid leakage)\n",
    "# For demonstration, we compute them globally first, but in practice compute within each split.\n",
    "# Here we assume they are already in df_one.\n",
    "X_train, y_train = prepare_data(train, feature_cols, target_col)\n",
    "X_val, y_val = prepare_data(val, feature_cols, target_col)\n",
    "X_test, y_test = prepare_data(test, feature_cols, target_col)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We choose two dates: one to mark the end of training, another to mark the end of validation. Everything after the second date is test.\n",
    "- The training set is the earliest period. The validation set comes next, and the test set is the most recent.\n",
    "- This respects the temporal order: the model is trained on past data, tuned on a later period (but still before test), and finally evaluated on the most recent, unseen data.\n",
    "- When computing features that require lookback (e.g., rolling means), we must ensure that for the validation and test sets, we only use information available up to that point. In practice, this means we should not compute features globally; instead, we should compute them within each split using only the data available at that time. For example, for a validation point, the rolling mean should be computed using only data from before that point (including training data, but not future validation data). This is a subtle but important point \u2013 we'll address it later.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.4 Time\u2011Based Splitting**\n",
    "\n",
    "Time\u2011based splitting is the simplest and most common method for time\u2011series. You choose a single cutoff date and split into training and test (or training, validation, and test as above). The key is to ensure the test set is chronologically after the training set.\n",
    "\n",
    "### **20.4.1 Simple Single Cutoff**\n",
    "\n",
    "```python\n",
    "cutoff_date = '2023-01-01'\n",
    "train = df_one[df_one['Date'] < cutoff_date]\n",
    "test = df_one[df_one['Date'] >= cutoff_date]\n",
    "\n",
    "print(f\"Train size: {len(train)}, Test size: {len(test)}\")\n",
    "```\n",
    "\n",
    "### **20.4.2 Multiple Cutoffs for Train/Val/Test**\n",
    "\n",
    "As shown in 20.3, we can use two cutoffs to create three sets. This is typical when you need to tune hyperparameters.\n",
    "\n",
    "### **20.4.3 Importance of Gap**\n",
    "\n",
    "Sometimes, to avoid any potential leakage from very recent history, we introduce a gap between training and validation/test. For example, if we use features that require a lookback window (e.g., 20-day moving average), we might want to ensure that the validation set does not include the first few points that depend on training data that is too close to the cutoff. But careful: if we skip data, we lose information. Usually, it's fine to have contiguous splits as long as we compute features correctly.\n",
    "\n",
    "```python\n",
    "# Introduce a 1-month gap between train and validation\n",
    "gap_start = '2023-01-01'\n",
    "gap_end = '2023-02-01'\n",
    "\n",
    "train = df_one[df_one['Date'] < gap_start]\n",
    "gap = df_one[(df_one['Date'] >= gap_start) & (df_one['Date'] < gap_end)]  # discard\n",
    "val = df_one[df_one['Date'] >= gap_end]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The gap period is not used for either training or validation. This can be useful if there is a known structural break (e.g., a regulatory change) or to ensure that validation truly starts after all training\u2011derived features are stable. However, it reduces the amount of data available.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.5 Walk\u2011Forward Validation**\n",
    "\n",
    "Walk\u2011forward validation (also known as rolling window validation) simulates how a model would be used in practice: we train on an initial window, then predict the next period, then expand the window (or move it) and repeat. This gives multiple performance estimates and helps assess model stability over time.\n",
    "\n",
    "### **20.5.1 Expanding Window Walk\u2011Forward**\n",
    "\n",
    "In expanding window, we start with an initial training set, then add more data as we move forward.\n",
    "\n",
    "```python\n",
    "# Define initial training size and test size (in days)\n",
    "initial_train_days = 500\n",
    "test_days = 50\n",
    "step = 50  # move forward by 50 days each time\n",
    "\n",
    "dates = df_one['Date'].values\n",
    "total_days = len(dates)\n",
    "\n",
    "scores = []\n",
    "start = 0\n",
    "\n",
    "while start + initial_train_days + test_days <= total_days:\n",
    "    train_end = start + initial_train_days\n",
    "    test_end = train_end + test_days\n",
    "    \n",
    "    train_indices = range(start, train_end)\n",
    "    test_indices = range(train_end, test_end)\n",
    "    \n",
    "    X_train = df_one.iloc[train_indices][['Feature']]  # feature\n",
    "    y_train = df_one.iloc[train_indices]['Target']\n",
    "    X_test = df_one.iloc[test_indices][['Feature']]\n",
    "    y_test = df_one.iloc[test_indices]['Target']\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    scores.append(rmse)\n",
    "    \n",
    "    print(f\"Window {start}: train {dates[start]} to {dates[train_end-1]}, \"\n",
    "          f\"test {dates[train_end]} to {dates[test_end-1]}, RMSE: {rmse:.6f}\")\n",
    "    \n",
    "    start += step\n",
    "\n",
    "print(f\"Average RMSE across windows: {np.mean(scores):.6f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We slide a window of fixed test size across the data. The training set expands each time because we keep all previous data (`start` increases, but we always train from the beginning up to `train_end`).\n",
    "- This mimics a realistic retraining scenario: as new data arrives, we retrain on all available history and forecast the next period.\n",
    "- The average RMSE gives a robust estimate of model performance under different market conditions. If scores vary widely, the model is unstable.\n",
    "\n",
    "### **20.5.2 Rolling Window Walk\u2011Forward**\n",
    "\n",
    "In rolling window, the training set size remains fixed; we drop the oldest data as we add new data.\n",
    "\n",
    "```python\n",
    "window_size = 500\n",
    "test_days = 50\n",
    "step = 50\n",
    "\n",
    "scores_rolling = []\n",
    "\n",
    "for start in range(0, total_days - window_size - test_days, step):\n",
    "    train_start = start\n",
    "    train_end = start + window_size\n",
    "    test_end = train_end + test_days\n",
    "    \n",
    "    train_indices = range(train_start, train_end)\n",
    "    test_indices = range(train_end, test_end)\n",
    "    \n",
    "    # Same fitting as above...\n",
    "    X_train = df_one.iloc[train_indices][['Feature']]\n",
    "    y_train = df_one.iloc[train_indices]['Target']\n",
    "    X_test = df_one.iloc[test_indices][['Feature']]\n",
    "    y_test = df_one.iloc[test_indices]['Target']\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    scores_rolling.append(rmse)\n",
    "\n",
    "print(f\"Rolling window average RMSE: {np.mean(scores_rolling):.6f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Rolling window is useful when the relationship between features and target may change over time (concept drift). By using only recent data, the model can adapt to new regimes.\n",
    "- However, it discards older data, which might contain valuable long\u2011term patterns. For the NEPSE market, where structural changes (e.g., new regulations) occur, rolling window might be more appropriate.\n",
    "- The choice between expanding and rolling depends on the stationarity of the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.6 Rolling Window Validation**\n",
    "\n",
    "Rolling window validation is essentially the same as walk\u2011forward with a fixed training window. The term is often used interchangeably. In the context of hyperparameter tuning, we might use a rolling window approach to evaluate different parameter sets.\n",
    "\n",
    "### **20.6.1 Example with Hyperparameter Tuning**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# We'll define a custom time-series splitter for rolling windows\n",
    "class RollingWindowSplit:\n",
    "    def __init__(self, window_size, test_size, step=1):\n",
    "        self.window_size = window_size\n",
    "        self.test_size = test_size\n",
    "        self.step = step\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        for start in range(0, n_samples - self.window_size - self.test_size, self.step):\n",
    "            train_end = start + self.window_size\n",
    "            test_end = train_end + self.test_size\n",
    "            train_indices = list(range(start, train_end))\n",
    "            test_indices = list(range(train_end, test_end))\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "# Assume X and y are the full dataset (with features and target)\n",
    "window_split = RollingWindowSplit(window_size=500, test_size=50, step=50)\n",
    "\n",
    "# Use this split in GridSearchCV\n",
    "param_grid = {'n_estimators': [50, 100], 'max_depth': [5, 10]}\n",
    "model = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# Note: GridSearchCV expects a cross-validation generator; we can use our custom splitter\n",
    "# But we need to ensure it yields indices correctly. We'll use a simple loop instead for clarity.\n",
    "best_score = -np.inf\n",
    "best_params = None\n",
    "\n",
    "for params in [{'n_estimators': 50, 'max_depth': 5}, {'n_estimators': 100, 'max_depth': 10}]:\n",
    "    scores = []\n",
    "    for train_idx, test_idx in window_split.split(X):\n",
    "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "        model.set_params(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        scores.append(model.score(X_test, y_test))  # R\u00b2\n",
    "    avg_score = np.mean(scores)\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_params = params\n",
    "\n",
    "print(f\"Best params: {best_params}, average validation R\u00b2: {best_score:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We define a custom splitter that yields training and test indices for each rolling window.\n",
    "- We then loop over hyperparameter combinations, compute the average performance across windows, and select the best.\n",
    "- This gives a robust estimate of how the model would perform over time, reducing the risk of overfitting to a specific validation period.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.7 Expanding Window Validation**\n",
    "\n",
    "Expanding window validation is similar, but the training set grows over time.\n",
    "\n",
    "```python\n",
    "class ExpandingWindowSplit:\n",
    "    def __init__(self, min_train_size, test_size, step=1):\n",
    "        self.min_train_size = min_train_size\n",
    "        self.test_size = test_size\n",
    "        self.step = step\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        for train_end in range(self.min_train_size, n_samples - self.test_size, self.step):\n",
    "            train_indices = list(range(train_end))\n",
    "            test_indices = list(range(train_end, train_end + self.test_size))\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "# Use similar to above\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Expanding window is appropriate when you believe that more data always helps (i.e., the process is stationary) and you want to use all available history.\n",
    "- In financial markets, both expanding and rolling are used; expanding is more common for long\u2011term trend models, rolling for short\u2011term adaptive models.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.8 Purging and Embargoing**\n",
    "\n",
    "When using cross\u2011validation that involves overlapping windows (like rolling windows), there is a risk of leakage between training and test sets if the test set contains data that is temporally close to the training set. For example, if you have a 20\u2011day moving average feature, the first few days of the test set might depend on data from the end of the training set. This is a form of leakage because the feature for the first test day uses training data, which is fine, but if the same data point appears in both training and test sets? Actually, no \u2013 they are separate indices. The issue is more subtle: when you have autocorrelated errors, the test error may be underestimated if the test set immediately follows the training set because the market conditions are similar.\n",
    "\n",
    "To mitigate this, we introduce **purging** and **embargoing**.\n",
    "\n",
    "- **Purging** means removing from the training set any data that overlaps in time with the test set (e.g., if you use lagged features, the first few test observations might have features computed from data that is also in the training set? Actually no, because features are computed only from past data. The real issue is that if the test set includes the day immediately after the training set, the model might be evaluated on a day that is very similar to the last training day, giving an optimistic estimate. This is not leakage per se, but it can lead to overfitting to the most recent regime.\n",
    "- **Embargoing** means excluding from the training set a period immediately before the test set (e.g., a few days or weeks) to ensure that the test set is truly out\u2011of\u2011sample and not too similar to the training data.\n",
    "\n",
    "In practice, purging is often automatic if we compute features correctly. Embargoing is a common practice in financial cross\u2011validation.\n",
    "\n",
    "```python\n",
    "# Example of embargo: exclude a gap before each test set\n",
    "embargo_days = 5\n",
    "\n",
    "class RollingWindowWithEmbargo:\n",
    "    def __init__(self, window_size, test_size, embargo, step=1):\n",
    "        self.window_size = window_size\n",
    "        self.test_size = test_size\n",
    "        self.embargo = embargo\n",
    "        self.step = step\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        for start in range(0, n_samples - self.window_size - self.test_size, self.step):\n",
    "            train_end = start + self.window_size\n",
    "            # Apply embargo: remove last 'embargo' days from training\n",
    "            train_indices = list(range(start, train_end - self.embargo))\n",
    "            test_indices = list(range(train_end, train_end + self.test_size))\n",
    "            if len(train_indices) > 0:\n",
    "                yield train_indices, test_indices\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- By removing the last `embargo` days from the training set before the test set, we ensure that the test set is not immediately adjacent to the training data. This reduces the chance of overfitting to short\u2011term autocorrelation.\n",
    "- The embargo period is discarded from training (not used in either set). This is a conservative approach that simulates a real\u2011world gap (e.g., we train on data up to a point, then wait a few days before making predictions).\n",
    "\n",
    "---\n",
    "\n",
    "## **20.9 Cross\u2011Validation for Time\u2011Series**\n",
    "\n",
    "Cross\u2011validation (CV) is a resampling technique to estimate model performance. For time\u2011series, we cannot use standard k\u2011fold CV because it shuffles the data. Instead, we use specialized methods.\n",
    "\n",
    "### **20.9.1 TimeSeriesSplit (Scikit\u2011Learn)**\n",
    "\n",
    "Scikit\u2011learn provides `TimeSeriesSplit`, which is an expanding window cross\u2011validator.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "for fold, (train_idx, test_idx) in enumerate(tscv.split(X)):\n",
    "    print(f\"Fold {fold+1}:\")\n",
    "    print(f\"  Train indices: {train_idx[0]}:{train_idx[-1]} (size {len(train_idx)})\")\n",
    "    print(f\"  Test indices: {test_idx[0]}:{test_idx[-1]} (size {len(test_idx)})\")\n",
    "    # Train and evaluate model here\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `TimeSeriesSplit` creates successive training sets that are supersets of previous ones (expanding window). The test sets are non\u2011overlapping and sequential.\n",
    "- This is a good default for time\u2011series CV, but it does not include an embargo. For many applications, it's sufficient.\n",
    "\n",
    "### **20.9.2 Blocked Cross\u2011Validation**\n",
    "\n",
    "Blocked CV divides the time series into contiguous blocks and uses some blocks for training, others for testing, in a way that respects order. For example, we might use the first 3 blocks for training, the next block for testing, then move forward.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import PredefinedSplit\n",
    "\n",
    "# Example: create 5 blocks of equal size\n",
    "n_blocks = 5\n",
    "block_size = len(X) // n_blocks\n",
    "\n",
    "test_fold = np.full(len(X), -1)  # -1 indicates training\n",
    "\n",
    "for i in range(1, n_blocks):\n",
    "    test_start = i * block_size\n",
    "    test_end = (i+1) * block_size if i < n_blocks-1 else len(X)\n",
    "    test_fold[test_start:test_end] = i  # fold number for test\n",
    "\n",
    "# Now PredefinedSplit will use these assignments\n",
    "ps = PredefinedSplit(test_fold)\n",
    "\n",
    "for train_idx, test_idx in ps.split():\n",
    "    print(f\"Train indices: {train_idx[:5]}...{train_idx[-5:]}\")\n",
    "    print(f\"Test indices: {test_idx[:5]}...{test_idx[-5:]}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This approach creates non\u2011overlapping test sets. It can be useful when you want to evaluate on distinct time periods.\n",
    "- However, it may not be ideal if the time series is short because the test sets may be small.\n",
    "\n",
    "### **20.9.3 Modified Cross\u2011Validation (Purged, Embargoed)**\n",
    "\n",
    "The financial literature (e.g., Marcos L\u00f3pez de Prado's \"Advances in Financial Machine Learning\") introduces purged and embargoed cross\u2011validation. The idea is to remove from the training set any data that overlaps in time with the test set due to feature lags (purging), and also to exclude a gap (embargo) to prevent leakage from very recent observations.\n",
    "\n",
    "Implementing full purged CV is complex and beyond the scope of this chapter, but we can approximate it with the embargo idea shown earlier.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.10 Multiple Asset Splitting**\n",
    "\n",
    "When we have multiple symbols (stocks) in the dataset, we must decide how to split. The options are:\n",
    "\n",
    "1. **Split by time globally:** Choose a cutoff date and use that for all symbols. This ensures that the model is tested on the same time period for all stocks. This is the most common approach.\n",
    "2. **Split by symbol:** Train on some symbols, test on others. This tests cross\u2011sectional generalization (whether patterns learned on one stock apply to another). This is less common for time\u2011series forecasting but can be useful if you have many stocks.\n",
    "3. **Split by both time and symbol:** For example, train on symbols A, B, C for years 2010\u20112019, test on symbol D for 2020. This is a more rigorous test.\n",
    "\n",
    "For the NEPSE system, we likely want to predict each symbol individually, so a time\u2011based split applied per symbol is appropriate. However, we must ensure that for each symbol, the split dates are aligned (e.g., use the same calendar cutoff).\n",
    "\n",
    "```python\n",
    "# Define a global cutoff date\n",
    "cutoff_date = '2023-01-01'\n",
    "\n",
    "# Create train/test for each symbol\n",
    "train_list = []\n",
    "test_list = []\n",
    "\n",
    "for symbol, group in df.groupby('Symbol'):\n",
    "    train = group[group['Date'] < cutoff_date].copy()\n",
    "    test = group[group['Date'] >= cutoff_date].copy()\n",
    "    train_list.append(train)\n",
    "    test_list.append(test)\n",
    "\n",
    "train_all = pd.concat(train_list)\n",
    "test_all = pd.concat(test_list)\n",
    "\n",
    "print(f\"Total train: {len(train_all)}, Total test: {len(test_all)}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This preserves the temporal order per symbol and uses the same cutoff for all. The model will be trained on a mix of symbols from earlier periods and tested on later periods.\n",
    "- When computing features that require lookback (e.g., rolling means), we must compute them within each symbol group separately, as we did in Chapter 19.\n",
    "\n",
    "---\n",
    "\n",
    "## **20.11 Splitting Best Practices**\n",
    "\n",
    "To wrap up, here are the key best practices for splitting time\u2011series data, especially for the NEPSE prediction system:\n",
    "\n",
    "### **20.11.1 Always Respect Temporal Order**\n",
    "\n",
    "Never shuffle your data before splitting. Use date\u2011based cutoffs.\n",
    "\n",
    "### **20.11.2 Use Expanding or Rolling Windows for Validation**\n",
    "\n",
    "For hyperparameter tuning, use walk\u2011forward validation (expanding or rolling) rather than a single validation set. This gives a more robust estimate.\n",
    "\n",
    "### **20.11.3 Purge and Embargo When Necessary**\n",
    "\n",
    "If your features have long lookback windows, consider adding an embargo period to avoid evaluating on data too similar to the training set.\n",
    "\n",
    "### **20.11.4 Maintain Consistent Feature Computation**\n",
    "\n",
    "Compute features within each training fold using only the data available up to that point. Do not use global statistics. This is especially important for rolling means, normalization, and any transformation that depends on the data distribution.\n",
    "\n",
    "```python\n",
    "# Example: correct way to compute rolling mean within a training fold\n",
    "def compute_features_for_fold(train_idx, test_idx, df, window=20):\n",
    "    # Combine train and test in order, but we will only use train data for the rolling calc\n",
    "    combined = df.iloc[np.r_[train_idx, test_idx]].copy()\n",
    "    \n",
    "    # Compute rolling mean using expanding window that only sees past data\n",
    "    # For training indices, we need rolling mean that includes only previous training points\n",
    "    # This is tricky; simpler: compute on the full combined but ensure that for test,\n",
    "    # the rolling mean uses data only up to that point (including train). This is fine.\n",
    "    combined['rolling_mean'] = combined['Close'].rolling(window, min_periods=1).mean()\n",
    "    \n",
    "    # Now split back\n",
    "    train_features = combined.iloc[:len(train_idx)][['rolling_mean']]\n",
    "    test_features = combined.iloc[len(train_idx):][['rolling_mean']]\n",
    "    return train_features, test_features\n",
    "```\n",
    "\n",
    "### **20.11.5 Document Your Splitting Strategy**\n",
    "\n",
    "Always record the split dates and method used. This ensures reproducibility and helps diagnose why a model might fail in production.\n",
    "\n",
    "### **20.11.6 Test on the Most Recent Data**\n",
    "\n",
    "Your final evaluation should always be on the most recent data available, as this best represents the conditions the model will face in deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we explored the critical topic of data splitting for time\u2011series prediction systems. Using the NEPSE dataset, we demonstrated:\n",
    "\n",
    "- **Why random splitting fails** \u2013 it leads to data leakage and overoptimistic performance.\n",
    "- **Time\u2011based splitting** \u2013 using cutoff dates to create train, validation, and test sets that respect the arrow of time.\n",
    "- **Walk\u2011forward validation** \u2013 expanding and rolling windows that simulate realistic retraining scenarios.\n",
    "- **Purging and embargoing** \u2013 techniques to further reduce leakage from temporal proximity.\n",
    "- **Time\u2011series cross\u2011validation** \u2013 using `TimeSeriesSplit` and custom splitters.\n",
    "- **Multiple asset splitting** \u2013 handling multiple symbols with consistent time cutoffs.\n",
    "- **Best practices** \u2013 a checklist for robust evaluation.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Always split by time, never randomly.\n",
    "- Use walk\u2011forward validation to tune models and estimate performance over different market regimes.\n",
    "- For final evaluation, use a hold\u2011out set from the most recent period.\n",
    "- When computing features, ensure they are calculated using only data available at the time (no lookahead).\n",
    "- If you have many symbols, apply the same temporal split to all, but compute features per symbol.\n",
    "\n",
    "With a solid splitting strategy, we can now confidently evaluate models. In the next chapter, **Chapter 21: Traditional Statistical Models**, we will apply these splitting techniques to evaluate classical time\u2011series models like ARIMA, Exponential Smoothing, and others on the NEPSE data.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 20**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='19. defining_prediction_targets.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='21. traditional_statistical_models.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}