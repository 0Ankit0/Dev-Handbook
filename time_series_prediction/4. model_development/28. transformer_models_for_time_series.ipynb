{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 28: Transformer Models for Time\u2011Series\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamental limitations of RNNs and CNNs that led to the development of Transformers\n",
    "- Explain the core components of the Transformer architecture: self\u2011attention, multi\u2011head attention, and positional encoding\n",
    "- Recognize why the attention mechanism is particularly suited for capturing long\u2011range dependencies in time\u2011series data\n",
    "- Implement a simplified Transformer block for time\u2011series forecasting using TensorFlow/Keras\n",
    "- Explore specialized Transformer variants (Informer, Autoformer, FEDformer) designed for long sequence forecasting\n",
    "- Discuss pre\u2011training strategies and how they can be applied to time\u2011series\n",
    "- Apply fine\u2011tuning techniques to adapt pre\u2011trained models to the NEPSE prediction task\n",
    "- Address practical considerations such as computational complexity, overfitting, and data requirements\n",
    "- Compare Transformer\u2011based models with RNNs, CNNs, and traditional statistical methods on the NEPSE dataset\n",
    "\n",
    "---\n",
    "\n",
    "## **28.1 Introduction to the Transformer Architecture**\n",
    "\n",
    "The Transformer model, introduced by Vaswani et al. in 2017, revolutionized natural language processing (NLP) by replacing recurrent and convolutional layers with a mechanism called **self\u2011attention**. Unlike RNNs, which process sequences step\u2011by\u2011step, Transformers process all elements of a sequence in parallel, making them highly efficient and capable of capturing long\u2011range dependencies without the vanishing gradient problem.\n",
    "\n",
    "In time\u2011series forecasting, Transformers have gained popularity because they can model complex temporal patterns across many time steps. For the NEPSE prediction system, a Transformer could theoretically learn relationships between events far apart in time\u2014for example, linking a price movement in January to a movement in June\u2014without the need for carefully engineered lag features.\n",
    "\n",
    "### **28.1.1 Why Attention for Time\u2011Series?**\n",
    "\n",
    "- **Long\u2011range dependencies:** Financial markets often exhibit dependencies that span weeks or months. RNNs struggle with such long horizons; Transformers handle them naturally.\n",
    "- **Parallelization:** Training on sequences of hundreds of time steps is much faster with Transformers than with RNNs.\n",
    "- **Interpretability:** Attention weights can be visualized to understand which past time points the model focuses on when making a prediction.\n",
    "\n",
    "However, Transformers come with challenges: they are data\u2011hungry, computationally intensive for very long sequences (quadratic complexity in sequence length), and prone to overfitting on small datasets. For a single NEPSE stock with a few thousand daily observations, we must apply strong regularization and potentially use pre\u2011trained models or transfer learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **28.2 Attention Mechanism**\n",
    "\n",
    "The core innovation of the Transformer is the **scaled dot\u2011product attention**. Given a query `Q`, a key `K`, and a value `V` (all matrices), the attention output is computed as:\n",
    "\n",
    "`Attention(Q, K, V) = softmax( (Q K\u1d40) / \u221ad\u2096 ) V`\n",
    "\n",
    "Intuitively, the dot product between a query and all keys measures how much each value should contribute to the output. The scaling factor `\u221ad\u2096` prevents the dot products from growing too large, which would push the softmax into regions of extremely small gradients.\n",
    "\n",
    "In self\u2011attention, the queries, keys, and values all come from the same input sequence\u2014each element attends to all others. This allows the model to weigh the importance of other time steps when encoding a particular time step.\n",
    "\n",
    "### **28.2.1 Self\u2011Attention for Time\u2011Series**\n",
    "\n",
    "For a time series, each time step (e.g., a vector of features at day `t`) becomes a token. Self\u2011attention computes a weighted sum of all time steps for each time step, where the weights are determined by the similarity between the steps. This can capture patterns like: \"today's return should be influenced by the returns of the past three days, but especially by the day that had a similar volatility spike.\"\n",
    "\n",
    "---\n",
    "\n",
    "## **28.3 Multi\u2011Head Attention**\n",
    "\n",
    "Instead of performing a single attention function, the Transformer uses **multi\u2011head attention**: it linearly projects the queries, keys, and values `h` times with different learned projections, performs attention in parallel, concatenates the results, and projects again. Each head can focus on different types of relationships (e.g., one head might capture short\u2011term momentum, another might capture weekly seasonality).\n",
    "\n",
    "Mathematically:\n",
    "\n",
    "`MultiHead(Q, K, V) = Concat(head\u2081, \u2026, head_h) W\u1d3c`  \n",
    "where `head\u1d62 = Attention(Q W\u1d62^Q, K W\u1d62^K, V W\u1d62^V)`\n",
    "\n",
    "---\n",
    "\n",
    "## **28.4 Positional Encoding**\n",
    "\n",
    "Since Transformers have no inherent notion of order (they process the sequence as a set), we must inject information about the position of each time step. The original Transformer adds **positional encodings** to the input embeddings. These encodings are usually sine and cosine functions of different frequencies, which allow the model to easily learn to attend by relative position.\n",
    "\n",
    "For time\u2011series, we can also use learned positional embeddings or simply concatenate the time index (e.g., day number) as a feature. However, sine\u2011cosine encodings have the advantage of extrapolating to sequence lengths not seen during training.\n",
    "\n",
    "---\n",
    "\n",
    "## **28.5 Encoder\u2011Decoder Architecture**\n",
    "\n",
    "The original Transformer was designed for sequence\u2011to\u2011sequence tasks (e.g., machine translation). It consists of:\n",
    "\n",
    "- **Encoder:** Processes the input sequence and produces a sequence of hidden representations.\n",
    "- **Decoder:** Generates the output sequence autoregressively, attending to the encoder's outputs and its own previous outputs.\n",
    "\n",
    "For time\u2011series forecasting, we often need only the encoder for tasks like predicting the next value from a window (many\u2011to\u2011one). For multi\u2011step forecasting, we can use an encoder\u2011decoder where the decoder produces a sequence of future values.\n",
    "\n",
    "---\n",
    "\n",
    "## **28.6 Building a Simple Transformer for Time\u2011Series Forecasting**\n",
    "\n",
    "We will implement a simplified Transformer encoder for the NEPSE return prediction task (predict next day's return from a window of past returns). We'll use TensorFlow/Keras, building custom layers for multi\u2011head attention and feed\u2011forward networks.\n",
    "\n",
    "### **28.6.1 Data Preparation (Same as Chapter 26)**\n",
    "\n",
    "We'll reuse the sequence data from Chapter 26: 20\u2011day windows of returns, scaled.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load and prepare NEPSE data (single symbol)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Compute returns\n",
    "df_stock['Return'] = df_stock['Close'].pct_change() * 100\n",
    "\n",
    "# Use only returns as feature for simplicity\n",
    "feature_columns = ['Return']\n",
    "df_stock = df_stock.dropna(subset=feature_columns)\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :])\n",
    "        y.append(data[i+seq_length, 0])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 20\n",
    "data = df_stock[feature_columns].values\n",
    "X, y = create_sequences(data, seq_length)\n",
    "\n",
    "# Temporal split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Scale features\n",
    "original_shape = X_train.shape\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_2d = scaler.fit_transform(X_train_2d)\n",
    "X_train_scaled = X_train_scaled_2d.reshape(original_shape)\n",
    "\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_test_scaled_2d = scaler.transform(X_test_2d)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(X_test.shape)\n",
    "\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "```\n",
    "\n",
    "### **28.6.2 Defining the Transformer Encoder Block**\n",
    "\n",
    "We'll implement a single encoder block with multi\u2011head self\u2011attention and a feed\u2011forward network, followed by layer normalization and residual connections.\n",
    "\n",
    "```python\n",
    "def transformer_encoder(inputs, head_size, num_heads, ff_dim, dropout=0):\n",
    "    \"\"\"\n",
    "    A single transformer encoder block.\n",
    "    Args:\n",
    "        inputs: (batch, time, features)\n",
    "        head_size: dimension of each attention head\n",
    "        num_heads: number of attention heads\n",
    "        ff_dim: hidden units in feed-forward network\n",
    "        dropout: dropout rate\n",
    "    Returns:\n",
    "        (batch, time, features) after self-attention and FFN\n",
    "    \"\"\"\n",
    "    # Multi-head self-attention\n",
    "    attention = layers.MultiHeadAttention(\n",
    "        key_dim=head_size, num_heads=num_heads, dropout=dropout\n",
    "    )(inputs, inputs)\n",
    "    attention = layers.Dropout(dropout)(attention)\n",
    "    attention = layers.LayerNormalization(epsilon=1e-6)(inputs + attention)  # residual + norm\n",
    "\n",
    "    # Feed-forward network\n",
    "    ff = layers.Conv1D(filters=ff_dim, kernel_size=1, activation='relu')(attention)\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    ff = layers.Conv1D(filters=inputs.shape[-1], kernel_size=1)(ff)  # project back to original dim\n",
    "    ff = layers.Dropout(dropout)(ff)\n",
    "    outputs = layers.LayerNormalization(epsilon=1e-6)(attention + ff)  # second residual + norm\n",
    "    return outputs\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `layers.MultiHeadAttention` is a built\u2011in Keras layer that efficiently computes scaled dot\u2011product attention with multiple heads. We pass the same sequence as query, key, and value for self\u2011attention.\n",
    "- Residual connections and layer normalization are applied after each sub\u2011layer (pre\u2011norm variant, common in modern Transformers).\n",
    "- The feed\u2011forward network uses two 1D convolutions with kernel size 1 (equivalent to position\u2011wise dense layers). The first expands the dimension, the second projects back to the original feature size.\n",
    "- Dropout is applied for regularization.\n",
    "\n",
    "### **28.6.3 Building the Complete Model**\n",
    "\n",
    "We'll stack several encoder blocks and then add a global pooling layer and a dense output.\n",
    "\n",
    "```python\n",
    "def build_transformer_model(input_shape, head_size, num_heads, ff_dim, num_blocks, dropout=0):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    # We need to produce a single prediction per sample. Options:\n",
    "    # - Use global average pooling over time\n",
    "    # - Take the last time step (since the model has seen all, both are valid)\n",
    "    # Here we use global average pooling.\n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1)(x)\n",
    "    \n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train_scaled.shape[1], X_train_scaled.shape[2])\n",
    "model = build_transformer_model(\n",
    "    input_shape=input_shape,\n",
    "    head_size=64,\n",
    "    num_heads=4,\n",
    "    ff_dim=128,\n",
    "    num_blocks=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We stack two transformer encoder blocks (`num_blocks=2`). Each block has 4 attention heads, each of dimension 64.\n",
    "- After the encoder, we apply global average pooling over the time dimension to obtain a fixed\u2011size vector, then a final dense layer.\n",
    "- Dropout is applied inside each block and after pooling to reduce overfitting.\n",
    "- The total number of parameters is relatively modest, suitable for our dataset size.\n",
    "\n",
    "### **28.6.4 Training and Evaluation**\n",
    "\n",
    "```python\n",
    "# Validation split (temporal)\n",
    "val_size = int(len(X_train_scaled) * 0.1)\n",
    "X_val = X_train_scaled[-val_size:]\n",
    "y_val = y_train[-val_size:]\n",
    "X_train_final = X_train_scaled[:-val_size]\n",
    "y_train_final = y_train[:-val_size]\n",
    "\n",
    "# Callbacks\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop, reduce_lr],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Test evaluation\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "rmse_transformer = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Transformer Test RMSE: {rmse_transformer:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We train with early stopping and learning rate reduction on plateau.\n",
    "- The test RMSE can be compared with LSTM and CNN models from previous chapters.\n",
    "\n",
    "---\n",
    "\n",
    "## **28.7 Time\u2011Series Transformer Variants**\n",
    "\n",
    "The vanilla Transformer, while powerful, has limitations for long sequence time\u2011series forecasting (LSTF): the quadratic complexity of self\u2011attention with respect to sequence length makes it expensive for sequences of hundreds or thousands of steps. Several variants have been proposed to address this:\n",
    "\n",
    "### **28.7.1 Informer**\n",
    "\n",
    "Informer (Zhou et al., 2021) introduces:\n",
    "\n",
    "- **ProbSparse attention:** Instead of computing attention for all query\u2011key pairs, it selects only the most important queries based on a sparsity measurement, reducing complexity to O(L log L).\n",
    "- **Self\u2011attention distilling:** Downsamples the input by half each layer, further reducing dimensions.\n",
    "- **Generative style decoder:** Produces long sequences in one forward pass rather than step\u2011by\u2011step.\n",
    "\n",
    "Informer is particularly suited for datasets where long\u2011term forecasting is needed (e.g., electricity consumption, traffic). For NEPSE, with only daily data, the sequence lengths may not be long enough to benefit from these optimizations, but the ideas are worth noting.\n",
    "\n",
    "### **28.7.2 Autoformer**\n",
    "\n",
    "Autoformer (Wu et al., 2021) replaces the self\u2011attention mechanism with an **auto\u2011correlation** mechanism that discovers period\u2011based dependencies. It uses:\n",
    "\n",
    "- **Series decomposition:** Decomposes the series into trend and seasonal components.\n",
    "- **Auto\u2011correlation:** Computes series similarity based on time delays, which is more efficient and interpretable for seasonality.\n",
    "\n",
    "Autoformer often outperforms standard Transformers on datasets with clear seasonal patterns. For NEPSE, we might have weak seasonality (e.g., day\u2011of\u2011week effects), so Autoformer could be beneficial.\n",
    "\n",
    "### **28.7.3 FEDformer**\n",
    "\n",
    "FEDformer (Zhou et al., 2022) applies a **frequency\u2011enhanced** approach, using Fourier and wavelet transforms to capture global and local patterns in the frequency domain. It is even more efficient and effective for long sequences.\n",
    "\n",
    "### **28.7.4 When to Use These Variants**\n",
    "\n",
    "For the NEPSE dataset with daily data and a few thousand samples, the vanilla Transformer with moderate sequence length (e.g., 60 days) is feasible. However, if we were to use higher frequency data (e.g., minute\u2011by\u2011minute) or very long windows, these variants would become necessary. Implementing them from scratch is complex; usually we rely on existing implementations (e.g., in GitHub repositories or libraries like `tsai`).\n",
    "\n",
    "---\n",
    "\n",
    "## **28.8 Pre\u2011training and Fine\u2011tuning for Time\u2011Series**\n",
    "\n",
    "Transformers have benefited enormously from pre\u2011training on large corpora in NLP. For time\u2011series, pre\u2011training is an emerging area. The idea is to train a model on a large collection of time series (e.g., many stocks, different markets) to learn general temporal patterns, then fine\u2011tune on a target series (e.g., a specific NEPSE stock).\n",
    "\n",
    "### **28.8.1 Pre\u2011training Strategies**\n",
    "\n",
    "- **Masked autoencoding:** Mask a portion of the time steps and train the model to reconstruct them (similar to BERT).\n",
    "- **Contrastive learning:** Learn representations such that different views of the same series are close, while views of different series are far apart.\n",
    "- **Forecasting pre\u2011training:** Train on a large dataset to predict future values, then fine\u2011tune on the target.\n",
    "\n",
    "### **28.8.2 Fine\u2011tuning on NEPSE**\n",
    "\n",
    "If we have a pre\u2011trained model (e.g., trained on US stock data), we could load its weights, replace the final layer, and fine\u2011tune on NEPSE data. However, such models are not yet widely available for time\u2011series. In practice, for NEPSE, we would likely train from scratch.\n",
    "\n",
    "---\n",
    "\n",
    "## **28.9 Implementation Considerations**\n",
    "\n",
    "### **28.9.1 Data Requirements**\n",
    "\n",
    "Transformers generally require more data than RNNs or CNNs. For a single stock with a few thousand samples, a small Transformer (2\u20114 layers, 4\u20118 heads) with strong regularization may work, but we risk overfitting. We can augment data by:\n",
    "\n",
    "- Using multiple stocks as separate training samples (treat each stock as independent).\n",
    "- Applying time\u2011series augmentation techniques (e.g., jittering, scaling, time warping) \u2013 though care must be taken not to break temporal relationships.\n",
    "\n",
    "### **28.9.2 Regularization**\n",
    "\n",
    "- Dropout in attention and feed\u2011forward layers (0.1\u20110.3).\n",
    "- Layer normalization (already used).\n",
    "- Weight decay (L2 regularization) on dense layers.\n",
    "- Early stopping.\n",
    "\n",
    "### **28.9.3 Computational Complexity**\n",
    "\n",
    "For sequence length `L`, self\u2011attention is O(L\u00b2). With `L=60`, L\u00b2=3600, which is fine. For `L=500`, L\u00b2=250,000, which becomes heavy. Use efficient variants if needed.\n",
    "\n",
    "### **28.9.4 Positional Encoding**\n",
    "\n",
    "We used the default sine\u2011cosine encodings in the Keras `MultiHeadAttention` layer (it adds them internally when `use_positional_encoding=True`? Actually, the Keras layer does not add positional encodings automatically; we must add them ourselves. In our implementation, we omitted them because with only one feature, the model might learn positional information implicitly through the residual connections? Better to explicitly add them. We can add a positional encoding layer.\n",
    "\n",
    "```python\n",
    "class PositionalEncoding(layers.Layer):\n",
    "    def __init__(self, sequence_length, d_model):\n",
    "        super().__init__()\n",
    "        self.pos_encoding = self.positional_encoding(sequence_length, d_model)\n",
    "    \n",
    "    def positional_encoding(self, length, d_model):\n",
    "        angle_rads = self.get_angles(np.arange(length)[:, np.newaxis],\n",
    "                                     np.arange(d_model)[np.newaxis, :],\n",
    "                                     d_model)\n",
    "        # apply sin to even indices, cos to odd\n",
    "        angle_rads[:, 0::2] = np.sin(angle_rads[:, 0::2])\n",
    "        angle_rads[:, 1::2] = np.cos(angle_rads[:, 1::2])\n",
    "        pos_encoding = angle_rads[np.newaxis, ...]  # (1, length, d_model)\n",
    "        return tf.cast(pos_encoding, dtype=tf.float32)\n",
    "    \n",
    "    def get_angles(self, pos, i, d_model):\n",
    "        angle_rates = 1 / np.power(10000, (2 * (i//2)) / np.float32(d_model))\n",
    "        return pos * angle_rates\n",
    "    \n",
    "    def call(self, inputs):\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n",
    "```\n",
    "\n",
    "Then add it after the input layer: `x = PositionalEncoding(seq_length, d_model)(x)` where `d_model` is the feature dimension after an initial linear projection (we may need to project to a higher dimension first).\n",
    "\n",
    "---\n",
    "\n",
    "## **28.10 Practical Example: NEPSE Direction Prediction with Transformer**\n",
    "\n",
    "Let's adapt the model for binary classification (direction). We'll change the output layer to sigmoid and use binary crossentropy loss.\n",
    "\n",
    "```python\n",
    "# Binary target\n",
    "y_binary = (y > 0).astype(int)\n",
    "y_train_bin, y_test_bin = y_binary[:split_idx], y_binary[split_idx:]\n",
    "\n",
    "# Build transformer classifier (same encoder, but output with sigmoid)\n",
    "def build_transformer_classifier(input_shape, head_size, num_heads, ff_dim, num_blocks, dropout=0):\n",
    "    inputs = keras.Input(shape=input_shape)\n",
    "    x = inputs\n",
    "    # Optional: initial projection to d_model if features != d_model\n",
    "    # x = layers.Dense(head_size * num_heads)(x)  # project to d_model\n",
    "    # x = PositionalEncoding(seq_length, head_size * num_heads)(x)\n",
    "    \n",
    "    for _ in range(num_blocks):\n",
    "        x = transformer_encoder(x, head_size, num_heads, ff_dim, dropout)\n",
    "    \n",
    "    x = layers.GlobalAveragePooling1D()(x)\n",
    "    x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(1, activation='sigmoid')(x)\n",
    "    model = keras.Model(inputs, outputs)\n",
    "    return model\n",
    "\n",
    "model_clf = build_transformer_classifier(\n",
    "    input_shape=input_shape,\n",
    "    head_size=32,\n",
    "    num_heads=4,\n",
    "    ff_dim=64,\n",
    "    num_blocks=2,\n",
    "    dropout=0.2\n",
    ")\n",
    "model_clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train\n",
    "history_clf = model_clf.fit(\n",
    "    X_train_final, y_train_bin[:-val_size] if val_size>0 else y_train_bin,  # align with validation split\n",
    "    validation_data=(X_val, y_binary[split_idx - val_size:split_idx] if val_size>0 else None),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    callbacks=[early_stop],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Test accuracy\n",
    "y_pred_prob = model_clf.predict(X_test_scaled).flatten()\n",
    "y_pred_class = (y_pred_prob > 0.5).astype(int)\n",
    "accuracy = np.mean(y_pred_class == y_test_bin)\n",
    "print(f\"Transformer classifier test accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **28.11 Chapter Summary**\n",
    "\n",
    "In this chapter, we introduced Transformer models and their application to time\u2011series forecasting, using the NEPSE dataset as a concrete example.\n",
    "\n",
    "- **Transformer architecture:** We explained self\u2011attention, multi\u2011head attention, positional encoding, and the encoder\u2011decoder structure.\n",
    "- **Implementation:** We built a simplified Transformer encoder in Keras and applied it to predict next\u2011day returns and direction.\n",
    "- **Efficient variants:** We discussed Informer, Autoformer, and FEDformer, which are designed for long sequence forecasting.\n",
    "- **Pre\u2011training and fine\u2011tuning:** We touched on the potential of transfer learning for time\u2011series.\n",
    "- **Practical considerations:** Data requirements, regularization, and computational complexity are key when using Transformers on relatively small financial datasets.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- For a single stock with a few thousand daily observations, a small Transformer (2\u20113 layers, 4\u20118 heads) with strong dropout and early stopping can be a competitive model.\n",
    "- Use positional encodings to retain temporal order.\n",
    "- Compare Transformer performance with LSTM and CNN baselines; often Transformers perform similarly or slightly better, but require more careful tuning.\n",
    "- If longer sequences (e.g., 100+ days) are used, consider efficient variants or reduce sequence length.\n",
    "- For multi\u2011step forecasting, an encoder\u2011decoder Transformer may be appropriate, but the increased complexity must be justified by performance gains.\n",
    "\n",
    "In the next chapter, **Chapter 29: Specialized Time\u2011Series Architectures**, we will explore other advanced models such as N\u2011BEATS, DeepAR, Temporal Fusion Transformers, and Gaussian Processes, which are specifically designed for probabilistic forecasting and hierarchical time series.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 28**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='27. convolutional_neural_networks_for_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='29. specialized_time_series_architectures.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}