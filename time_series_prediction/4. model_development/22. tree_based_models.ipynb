{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 22: Tree-Based Models**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamentals of decision trees and how they partition the feature space\n",
    "- Build and interpret decision trees for regression and classification on NEPSE data\n",
    "- Explain how random forests reduce overfitting through bagging and feature randomness\n",
    "- Train random forest models and analyze feature importance\n",
    "- Grasp the concept of gradient boosting and how it sequentially corrects errors\n",
    "- Implement gradient boosting machines (GBM) using popular libraries\n",
    "- Compare and contrast XGBoost, LightGBM, and CatBoost for time\u2011series forecasting\n",
    "- Tune hyperparameters effectively to avoid overfitting\n",
    "- Apply tree\u2011based models to predict NEPSE stock returns and direction\n",
    "- Understand the strengths and limitations of tree\u2011based models in financial time\u2011series\n",
    "\n",
    "---\n",
    "\n",
    "## **22.1 Decision Trees Fundamentals**\n",
    "\n",
    "Decision trees are intuitive, interpretable models that learn a series of if\u2011then\u2011else rules from the data. They partition the feature space into rectangular regions and assign a prediction (mean for regression, majority class for classification) to each region. In the context of the NEPSE prediction system, a decision tree might learn rules like: \"If the 5\u2011day moving average is above the 20\u2011day moving average **and** yesterday's return was positive, then predict an up move.\"\n",
    "\n",
    "### **22.1.1 Tree Construction**\n",
    "\n",
    "A decision tree is built recursively:\n",
    "\n",
    "1. Start with all training samples at the root node.\n",
    "2. For each feature, evaluate possible split points and choose the one that best separates the target according to a criterion (e.g., Gini impurity for classification, mean squared error for regression).\n",
    "3. Split the node into two child nodes.\n",
    "4. Repeat recursively on each child node until a stopping condition is met (e.g., maximum depth, minimum samples per leaf).\n",
    "\n",
    "The splitting criterion for regression is typically the reduction in variance (or MSE). For classification, common criteria are Gini impurity or entropy.\n",
    "\n",
    "### **22.1.2 Building a Decision Tree for NEPSE Data**\n",
    "\n",
    "Let's start by preparing a feature set for a single NEPSE stock. We'll create lag features, rolling statistics, and technical indicators as we did in earlier chapters.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.tree import DecisionTreeRegressor, DecisionTreeClassifier, plot_tree\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, accuracy_score\n",
    "\n",
    "# Load NEPSE data\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Use a single symbol for simplicity\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Create basic features (as in previous chapters)\n",
    "df_stock['Return'] = df_stock['Close'].pct_change() * 100\n",
    "\n",
    "# Lag features\n",
    "for lag in [1, 2, 3, 5]:\n",
    "    df_stock[f'Return_Lag{lag}'] = df_stock['Return'].shift(lag)\n",
    "    df_stock[f'Volume_Lag{lag}'] = df_stock['Vol'].shift(lag)\n",
    "\n",
    "# Rolling features\n",
    "for window in [5, 10, 20]:\n",
    "    df_stock[f'MA_{window}'] = df_stock['Close'].rolling(window).mean()\n",
    "    df_stock[f'Volatility_{window}'] = df_stock['Return'].rolling(window).std()\n",
    "\n",
    "# Technical indicators (simplified RSI)\n",
    "delta = df_stock['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(14).mean()\n",
    "avg_loss = loss.rolling(14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "df_stock['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Target: next day's return (regression) or direction (classification)\n",
    "df_stock['Target_Return'] = df_stock['Return'].shift(-1)\n",
    "df_stock['Target_Direction'] = (df_stock['Target_Return'] > 0).astype(int)\n",
    "\n",
    "# Drop NaN rows\n",
    "df_stock = df_stock.dropna()\n",
    "\n",
    "# Define features (exclude target and metadata)\n",
    "feature_cols = [col for col in df_stock.columns if col not in \n",
    "                ['Date', 'Symbol', 'S.No', 'Conf.', 'Open', 'High', 'Low', 'Close', 'LTP',\n",
    "                 'VWAP', 'Prev. Close', 'Turnover', 'Trans.', 'Diff', 'Range', 'Diff %',\n",
    "                 'Range %', 'VWAP %', '52 Weeks High', '52 Weeks Low',\n",
    "                 'Return', 'Target_Return', 'Target_Direction']]\n",
    "\n",
    "X = df_stock[feature_cols]\n",
    "y_reg = df_stock['Target_Return']\n",
    "y_clf = df_stock['Target_Direction']\n",
    "\n",
    "# Train/test split (temporal)\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train_reg, y_test_reg = y_reg.iloc[:split_idx], y_reg.iloc[split_idx:]\n",
    "y_train_clf, y_test_clf = y_clf.iloc[:split_idx], y_clf.iloc[split_idx:]\n",
    "\n",
    "print(f\"Training samples: {len(X_train)}, Test samples: {len(X_test)}\")\n",
    "print(f\"Feature list: {feature_cols[:5]}...\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We engineer a set of features typical for stock prediction: lagged returns, lagged volume, moving averages, volatility, and RSI.\n",
    "- The target for regression is the next day's return; for classification, a binary indicator of positive return.\n",
    "- A temporal split (80% train, 20% test) respects time order. We'll use this throughout the chapter.\n",
    "\n",
    "Now let's fit a decision tree regressor.\n",
    "\n",
    "```python\n",
    "# Decision Tree for regression\n",
    "dt_reg = DecisionTreeRegressor(max_depth=5, min_samples_split=10, random_state=42)\n",
    "dt_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = dt_reg.predict(X_train)\n",
    "y_pred_test = dt_reg.predict(X_test)\n",
    "\n",
    "print(f\"Train RMSE: {np.sqrt(mean_squared_error(y_train_reg, y_pred_train)):.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_test)):.4f}\")\n",
    "\n",
    "# Visualize a small part of the tree\n",
    "plt.figure(figsize=(20,10))\n",
    "plot_tree(dt_reg, max_depth=2, feature_names=feature_cols, filled=True, fontsize=10)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We set `max_depth=5` to keep the tree relatively simple and avoid overfitting. `min_samples_split=10` ensures that internal nodes have at least 10 samples before they can be split.\n",
    "- The train RMSE is lower than test RMSE, as expected. If the gap is large, the tree is overfitting; we could reduce depth or increase `min_samples_split`.\n",
    "- The tree visualization (limited to depth 2) shows the first few splits. For example, the root might split on `RSI` or `MA_20`, indicating which features are most important.\n",
    "\n",
    "For classification, we use `DecisionTreeClassifier`.\n",
    "\n",
    "```python\n",
    "dt_clf = DecisionTreeClassifier(max_depth=5, min_samples_split=10, random_state=42)\n",
    "dt_clf.fit(X_train, y_train_clf)\n",
    "\n",
    "y_pred_clf = dt_clf.predict(X_test)\n",
    "accuracy = accuracy_score(y_test_clf, y_pred_clf)\n",
    "print(f\"Classification accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The classifier aims to predict direction. Accuracy above 0.5 indicates some predictive power, but we must compare to a baseline (e.g., always predicting \"up\" if the market is trending up). In our temporal split, we should compute the baseline accuracy on the test set.\n",
    "\n",
    "### **22.1.3 Interpreting Decision Trees**\n",
    "\n",
    "Decision trees are highly interpretable. We can extract the rules:\n",
    "\n",
    "```python\n",
    "# Get feature importances\n",
    "importances = dt_reg.feature_importances_\n",
    "feature_imp = pd.DataFrame({'feature': feature_cols, 'importance': importances})\n",
    "feature_imp = feature_imp.sort_values('importance', ascending=False).head(10)\n",
    "print(feature_imp)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_imp['feature'], feature_imp['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances from Decision Tree')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Feature importance is calculated based on how much each feature reduces the splitting criterion (e.g., MSE) across all nodes. The top features give insight into what drives predictions.\n",
    "- For NEPSE, we might see that recent returns (e.g., `Return_Lag1`) and volatility measures dominate.\n",
    "\n",
    "### **22.1.4 Limitations of a Single Tree**\n",
    "\n",
    "- **High variance:** Small changes in data can lead to completely different trees.\n",
    "- **Instability:** Trees are sensitive to the specific training sample.\n",
    "- **Overfitting:** Without pruning, trees can grow deep and memorize noise.\n",
    "- **Piecewise constant predictions:** They produce step functions, not smooth approximations.\n",
    "\n",
    "These limitations motivate ensemble methods like random forests and gradient boosting.\n",
    "\n",
    "---\n",
    "\n",
    "## **22.2 Random Forest**\n",
    "\n",
    "Random Forest is an ensemble of decision trees, each trained on a bootstrap sample of the data (bagging) and with a random subset of features considered at each split. This decorrelates the trees and reduces variance while maintaining low bias.\n",
    "\n",
    "### **22.2.1 Bootstrap Aggregating (Bagging)**\n",
    "\n",
    "- **Bootstrap:** Create B random samples (with replacement) from the training set, each of the same size as the original.\n",
    "- **Aggregating:** Train a tree on each bootstrap sample, then average predictions (regression) or take majority vote (classification).\n",
    "- Bagging reduces variance without increasing bias much.\n",
    "\n",
    "Random forest adds an extra layer of randomness: at each split, only a random subset of features is considered (typically sqrt(n_features) for classification, n_features/3 for regression). This makes the trees even less correlated.\n",
    "\n",
    "### **22.2.2 Building a Random Forest for NEPSE**\n",
    "\n",
    "We'll use `RandomForestRegressor` and `RandomForestClassifier` from scikit\u2011learn.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor, RandomForestClassifier\n",
    "\n",
    "# Random Forest Regressor\n",
    "rf_reg = RandomForestRegressor(n_estimators=100, max_depth=5, min_samples_split=10,\n",
    "                               random_state=42, n_jobs=-1)\n",
    "rf_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "# Predict\n",
    "y_pred_rf_train = rf_reg.predict(X_train)\n",
    "y_pred_rf_test = rf_reg.predict(X_test)\n",
    "\n",
    "print(f\"Random Forest Train RMSE: {np.sqrt(mean_squared_error(y_train_reg, y_pred_rf_train)):.4f}\")\n",
    "print(f\"Random Forest Test RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_rf_test)):.4f}\")\n",
    "\n",
    "# Compare with single tree\n",
    "print(f\"Single Tree Test RMSE: {np.sqrt(mean_squared_error(y_test_reg, y_pred_test)):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `n_estimators=100` creates 100 trees. More trees generally improve performance but increase computation.\n",
    "- `max_depth=5` limits tree depth, controlling complexity. Without depth limit, trees can grow fully, potentially overfitting.\n",
    "- The test RMSE is often lower than a single tree, demonstrating the benefit of ensembling.\n",
    "\n",
    "For classification:\n",
    "\n",
    "```python\n",
    "rf_clf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf_clf.fit(X_train, y_train_clf)\n",
    "y_pred_rf_clf = rf_clf.predict(X_test)\n",
    "accuracy_rf = accuracy_score(y_test_clf, y_pred_rf_clf)\n",
    "print(f\"Random Forest Accuracy: {accuracy_rf:.4f}\")\n",
    "```\n",
    "\n",
    "### **22.2.3 Feature Importance from Random Forest**\n",
    "\n",
    "Random forest provides a more stable feature importance measure than a single tree, averaged over many trees.\n",
    "\n",
    "```python\n",
    "importances = rf_reg.feature_importances_\n",
    "feature_imp_rf = pd.DataFrame({'feature': feature_cols, 'importance': importances})\n",
    "feature_imp_rf = feature_imp_rf.sort_values('importance', ascending=False).head(10)\n",
    "print(feature_imp_rf)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feature_imp_rf['feature'], feature_imp_rf['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances from Random Forest')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Importance is computed as the average reduction in impurity (e.g., MSE) across all trees for each feature.\n",
    "- This ranking helps identify which features are most predictive. For NEPSE, we might see that lagged returns and volatility dominate.\n",
    "\n",
    "### **22.2.4 Out\u2011of\u2011Bag (OOB) Score**\n",
    "\n",
    "Since each tree is trained on a bootstrap sample, about one\u2011third of the data is left out (out\u2011of\u2011bag). These OOB samples can be used as a validation set without needing a separate validation split.\n",
    "\n",
    "```python\n",
    "rf_reg_oob = RandomForestRegressor(n_estimators=100, max_depth=5, oob_score=True, random_state=42)\n",
    "rf_reg_oob.fit(X_train, y_train_reg)\n",
    "print(f\"OOB R\u00b2 score: {rf_reg_oob.oob_score_:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `oob_score=True` computes the R\u00b2 score on the OOB samples. This gives an unbiased estimate of test performance, useful for hyperparameter tuning.\n",
    "\n",
    "---\n",
    "\n",
    "## **22.3 Gradient Boosting Machines**\n",
    "\n",
    "Gradient boosting builds an ensemble of trees sequentially, where each new tree tries to correct the errors of the previous ones. Instead of bagging, it uses boosting: trees are added one at a time, and each new tree is trained on the residual errors of the current ensemble.\n",
    "\n",
    "### **22.3.1 Boosting Concept**\n",
    "\n",
    "1. Start with a simple model (e.g., a constant prediction).\n",
    "2. Compute the residuals (errors) of the current model.\n",
    "3. Fit a new tree to predict the residuals.\n",
    "4. Add the new tree to the ensemble with a small learning rate (shrinkage).\n",
    "5. Repeat steps 2\u20114 for M iterations.\n",
    "\n",
    "The final prediction is the sum of all tree predictions (times the learning rate). This sequential approach can achieve high accuracy but is prone to overfitting if not carefully regularized.\n",
    "\n",
    "### **22.3.2 Gradient Boosting in Scikit\u2011Learn**\n",
    "\n",
    "Scikit\u2011learn provides `GradientBoostingRegressor` and `GradientBoostingClassifier`.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\n",
    "\n",
    "# Gradient Boosting Regressor\n",
    "gb_reg = GradientBoostingRegressor(n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "                                    random_state=42)\n",
    "gb_reg.fit(X_train, y_train_reg)\n",
    "\n",
    "y_pred_gb = gb_reg.predict(X_test)\n",
    "rmse_gb = np.sqrt(mean_squared_error(y_test_reg, y_pred_gb))\n",
    "print(f\"Gradient Boosting Test RMSE: {rmse_gb:.4f}\")\n",
    "\n",
    "# Gradient Boosting Classifier\n",
    "gb_clf = GradientBoostingClassifier(n_estimators=100, learning_rate=0.1, max_depth=3,\n",
    "                                     random_state=42)\n",
    "gb_clf.fit(X_train, y_train_clf)\n",
    "y_pred_gb_clf = gb_clf.predict(X_test)\n",
    "acc_gb = accuracy_score(y_test_clf, y_pred_gb_clf)\n",
    "print(f\"Gradient Boosting Accuracy: {acc_gb:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `n_estimators`: number of boosting stages.\n",
    "- `learning_rate`: shrinks the contribution of each tree; a lower rate requires more trees but often improves generalization.\n",
    "- `max_depth`: typically kept small (3\u20115) to keep trees weak, which is beneficial for boosting.\n",
    "\n",
    "### **22.3.3 Early Stopping**\n",
    "\n",
    "To avoid overfitting, we can use early stopping: monitor validation performance and stop adding trees when it no longer improves.\n",
    "\n",
    "```python\n",
    "# Split training data into train and validation (temporal)\n",
    "val_split = int(len(X_train) * 0.8)\n",
    "X_train_gb, X_val_gb = X_train.iloc[:val_split], X_train.iloc[val_split:]\n",
    "y_train_gb, y_val_gb = y_train_reg.iloc[:val_split], y_train_reg.iloc[val_split:]\n",
    "\n",
    "# Fit with early stopping\n",
    "gb_early = GradientBoostingRegressor(n_estimators=500, validation_fraction=0.2,\n",
    "                                      n_iter_no_change=10, tol=0.001, random_state=42)\n",
    "gb_early.fit(X_train_gb, y_train_gb)\n",
    "\n",
    "print(f\"Optimal number of trees: {gb_early.n_estimators_}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `validation_fraction` reserves part of the training data for validation.\n",
    "- `n_iter_no_change` stops training if validation score doesn't improve for that many iterations.\n",
    "- The final model uses the best number of trees.\n",
    "\n",
    "### **22.3.4 Feature Importance**\n",
    "\n",
    "Like random forest, gradient boosting also provides feature importances.\n",
    "\n",
    "```python\n",
    "importances_gb = gb_reg.feature_importances_\n",
    "feature_imp_gb = pd.DataFrame({'feature': feature_cols, 'importance': importances_gb})\n",
    "feature_imp_gb = feature_imp_gb.sort_values('importance', ascending=False).head(10)\n",
    "print(feature_imp_gb)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.4 XGBoost**\n",
    "\n",
    "XGBoost (Extreme Gradient Boosting) is an optimized implementation of gradient boosting that has become the go\u2011to algorithm for many machine learning competitions. It includes enhancements like regularization, handling of missing values, and efficient tree pruning.\n",
    "\n",
    "### **22.4.1 System Overview**\n",
    "\n",
    "XGBoost features:\n",
    "\n",
    "- **Regularization:** L1 and L2 penalties on leaf weights to reduce overfitting.\n",
    "- **Sparsity awareness:** Handles missing values automatically by learning the best direction to go when a value is missing.\n",
    "- **Weighted quantile sketch:** Efficiently finds optimal split points.\n",
    "- **Cross\u2011validation and early stopping** built in.\n",
    "- **Parallel processing** (though boosting is sequential, tree construction can be parallelized).\n",
    "\n",
    "### **22.4.2 Installing and Using XGBoost**\n",
    "\n",
    "```bash\n",
    "pip install xgboost\n",
    "```\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "\n",
    "# Prepare data in DMatrix format (optional but efficient)\n",
    "dtrain = xgb.DMatrix(X_train, label=y_train_reg)\n",
    "dtest = xgb.DMatrix(X_test, label=y_test_reg)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'reg:squarederror',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'reg_lambda': 1.0,\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "model_xgb = xgb.train(params, dtrain, num_boost_round=1000,\n",
    "                      evals=[(dtest, 'test')],\n",
    "                      early_stopping_rounds=10,\n",
    "                      verbose_eval=False)\n",
    "\n",
    "# Predict\n",
    "y_pred_xgb = model_xgb.predict(dtest)\n",
    "rmse_xgb = np.sqrt(mean_squared_error(y_test_reg, y_pred_xgb))\n",
    "print(f\"XGBoost Test RMSE: {rmse_xgb:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `subsample` fraction of samples used per tree (stochastic gradient boosting).\n",
    "- `colsample_bytree` fraction of features used per tree.\n",
    "- `reg_alpha` and `reg_lambda` are L1 and L2 regularization on weights.\n",
    "- `early_stopping_rounds` stops if test error doesn't improve for 10 rounds.\n",
    "- The model automatically uses the best iteration.\n",
    "\n",
    "For classification:\n",
    "\n",
    "```python\n",
    "params_clf = {\n",
    "    'objective': 'binary:logistic',\n",
    "    'max_depth': 3,\n",
    "    'learning_rate': 0.1,\n",
    "    'subsample': 0.8,\n",
    "    'colsample_bytree': 0.8,\n",
    "    'reg_alpha': 0.1,\n",
    "    'eval_metric': 'logloss',\n",
    "    'random_state': 42\n",
    "}\n",
    "\n",
    "dtrain_clf = xgb.DMatrix(X_train, label=y_train_clf)\n",
    "dtest_clf = xgb.DMatrix(X_test, label=y_test_clf)\n",
    "\n",
    "model_xgb_clf = xgb.train(params_clf, dtrain_clf, num_boost_round=1000,\n",
    "                           evals=[(dtest_clf, 'test')],\n",
    "                           early_stopping_rounds=10,\n",
    "                           verbose_eval=False)\n",
    "\n",
    "y_pred_xgb_clf_prob = model_xgb_clf.predict(dtest_clf)\n",
    "y_pred_xgb_clf = (y_pred_xgb_clf_prob > 0.5).astype(int)\n",
    "acc_xgb = accuracy_score(y_test_clf, y_pred_xgb_clf)\n",
    "print(f\"XGBoost Accuracy: {acc_xgb:.4f}\")\n",
    "```\n",
    "\n",
    "### **22.4.3 Feature Importance in XGBoost**\n",
    "\n",
    "XGBoost provides several types of importance: weight (number of times a feature is used), gain (average gain when the feature is used), and cover (average coverage).\n",
    "\n",
    "```python\n",
    "importance = model_xgb.get_score(importance_type='gain')\n",
    "# Convert to DataFrame\n",
    "imp_df = pd.DataFrame(list(importance.items()), columns=['feature', 'importance'])\n",
    "imp_df = imp_df.sort_values('importance', ascending=False).head(10)\n",
    "print(imp_df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `importance_type='gain'` is often more informative than 'weight' because it reflects how much the feature improves the model.\n",
    "- The results can be plotted similarly.\n",
    "\n",
    "### **22.4.4 Hyperparameter Tuning with XGBoost**\n",
    "\n",
    "We can use `GridSearchCV` or `RandomizedSearchCV` with XGBoost's scikit\u2011learn API.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7],\n",
    "    'learning_rate': [0.01, 0.1, 0.2],\n",
    "    'n_estimators': [100, 200],\n",
    "    'subsample': [0.8, 1.0]\n",
    "}\n",
    "\n",
    "grid = GridSearchCV(xgb_model, param_grid, cv=3, scoring='neg_mean_squared_error', verbose=1)\n",
    "grid.fit(X_train, y_train_reg)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best CV score: {grid.best_score_}\")\n",
    "```\n",
    "\n",
    "**Note:** When using time\u2011series data, the CV folds should be time\u2011based, not random. We can pass a custom time\u2011series splitter to `GridSearchCV`.\n",
    "\n",
    "---\n",
    "\n",
    "## **22.5 LightGBM**\n",
    "\n",
    "LightGBM (Light Gradient Boosting Machine) is another gradient boosting framework developed by Microsoft. It is designed for efficiency and speed, especially with large datasets. It introduces two novel techniques: **Gradient\u2011based One\u2011Side Sampling (GOSS)** and **Exclusive Feature Bundling (EFB)**.\n",
    "\n",
    "### **22.5.1 Leaf\u2011Wise Growth**\n",
    "\n",
    "Unlike level\u2011wise growth (XGBoost's default), LightGBM grows trees leaf\u2011wise: it splits the leaf with the highest loss reduction, leading to deeper, asymmetric trees. This can converge faster but may overfit on small datasets.\n",
    "\n",
    "### **22.5.2 Installing and Using LightGBM**\n",
    "\n",
    "```bash\n",
    "pip install lightgbm\n",
    "```\n",
    "\n",
    "```python\n",
    "import lightgbm as lgb\n",
    "\n",
    "# Prepare dataset (LightGBM has its own Dataset format)\n",
    "train_data = lgb.Dataset(X_train, label=y_train_reg)\n",
    "test_data = lgb.Dataset(X_test, label=y_test_reg, reference=train_data)\n",
    "\n",
    "# Set parameters\n",
    "params = {\n",
    "    'objective': 'regression',\n",
    "    'metric': 'rmse',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "# Train with early stopping\n",
    "model_lgb = lgb.train(params,\n",
    "                      train_data,\n",
    "                      valid_sets=[test_data],\n",
    "                      num_boost_round=1000,\n",
    "                      callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)])\n",
    "\n",
    "# Predict\n",
    "y_pred_lgb = model_lgb.predict(X_test, num_iteration=model_lgb.best_iteration)\n",
    "rmse_lgb = np.sqrt(mean_squared_error(y_test_reg, y_pred_lgb))\n",
    "print(f\"LightGBM Test RMSE: {rmse_lgb:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `num_leaves` controls the complexity (similar to `max_depth` in level\u2011wise trees). Typical values 31, 63, etc.\n",
    "- `feature_fraction` is analogous to `colsample_bytree`.\n",
    "- `bagging_fraction` and `bagging_freq` implement bagging (subsampling) to reduce overfitting.\n",
    "- Early stopping monitors the validation metric (here RMSE) and stops if no improvement.\n",
    "\n",
    "For classification:\n",
    "\n",
    "```python\n",
    "params_clf = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'binary_logloss',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.1,\n",
    "    'feature_fraction': 0.8,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'verbose': -1,\n",
    "    'seed': 42\n",
    "}\n",
    "\n",
    "train_clf = lgb.Dataset(X_train, label=y_train_clf)\n",
    "test_clf = lgb.Dataset(X_test, label=y_test_clf, reference=train_clf)\n",
    "\n",
    "model_lgb_clf = lgb.train(params_clf, train_clf, valid_sets=[test_clf],\n",
    "                          num_boost_round=1000,\n",
    "                          callbacks=[lgb.early_stopping(10), lgb.log_evaluation(0)])\n",
    "\n",
    "y_pred_lgb_prob = model_lgb_clf.predict(X_test)\n",
    "y_pred_lgb_clf = (y_pred_lgb_prob > 0.5).astype(int)\n",
    "acc_lgb = accuracy_score(y_test_clf, y_pred_lgb_clf)\n",
    "print(f\"LightGBM Accuracy: {acc_lgb:.4f}\")\n",
    "```\n",
    "\n",
    "### **22.5.3 Feature Importance in LightGBM**\n",
    "\n",
    "LightGBM provides importance via `feature_importance()`.\n",
    "\n",
    "```python\n",
    "importance_lgb = model_lgb.feature_importance(importance_type='gain')\n",
    "feature_imp_lgb = pd.DataFrame({'feature': feature_cols, 'importance': importance_lgb})\n",
    "feature_imp_lgb = feature_imp_lgb.sort_values('importance', ascending=False).head(10)\n",
    "print(feature_imp_lgb)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.6 CatBoost**\n",
    "\n",
    "CatBoost (Categorical Boosting) is a gradient boosting library developed by Yandex. It excels at handling categorical features automatically (hence the name) and is known for its robustness and reduced need for hyperparameter tuning.\n",
    "\n",
    "### **22.6.1 Ordered Boosting and Categorical Handling**\n",
    "\n",
    "- **Ordered boosting:** A permutation\u2011driven approach to avoid target leakage when dealing with categorical features.\n",
    "- **Automatic categorical feature processing:** You can specify which columns are categorical, and CatBoost applies various encoding schemes (e.g., one\u2011hot, mean encoding) with counter\u2011measures against overfitting.\n",
    "\n",
    "### **22.6.2 Installing and Using CatBoost**\n",
    "\n",
    "```bash\n",
    "pip install catboost\n",
    "```\n",
    "\n",
    "```python\n",
    "from catboost import CatBoostRegressor, CatBoostClassifier\n",
    "\n",
    "# CatBoost Regressor\n",
    "cb_reg = CatBoostRegressor(iterations=1000,\n",
    "                           learning_rate=0.1,\n",
    "                           depth=3,\n",
    "                           loss_function='RMSE',\n",
    "                           verbose=100,\n",
    "                           early_stopping_rounds=10,\n",
    "                           random_seed=42)\n",
    "\n",
    "cb_reg.fit(X_train, y_train_reg,\n",
    "           eval_set=(X_test, y_test_reg),\n",
    "           plot=False)  # plot=True for interactive visualization\n",
    "\n",
    "y_pred_cb = cb_reg.predict(X_test)\n",
    "rmse_cb = np.sqrt(mean_squared_error(y_test_reg, y_pred_cb))\n",
    "print(f\"CatBoost Test RMSE: {rmse_cb:.4f}\")\n",
    "\n",
    "# CatBoost Classifier\n",
    "cb_clf = CatBoostClassifier(iterations=1000,\n",
    "                            learning_rate=0.1,\n",
    "                            depth=3,\n",
    "                            loss_function='Logloss',\n",
    "                            verbose=100,\n",
    "                            early_stopping_rounds=10,\n",
    "                            random_seed=42)\n",
    "\n",
    "cb_clf.fit(X_train, y_train_clf,\n",
    "           eval_set=(X_test, y_test_clf),\n",
    "           plot=False)\n",
    "\n",
    "y_pred_cb_clf = cb_clf.predict(X_test)\n",
    "acc_cb = accuracy_score(y_test_clf, y_pred_cb_clf)\n",
    "print(f\"CatBoost Accuracy: {acc_cb:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `iterations` is the number of trees (boosting rounds).\n",
    "- `depth` is the tree depth (similar to `max_depth`).\n",
    "- CatBoost automatically handles missing values and categorical features. If we had categorical columns (like stock symbol), we would pass them via the `cat_features` parameter.\n",
    "- The `verbose` parameter controls output; `early_stopping_rounds` stops if validation metric doesn't improve.\n",
    "\n",
    "### **22.6.3 Feature Importance in CatBoost**\n",
    "\n",
    "```python\n",
    "importance_cb = cb_reg.get_feature_importance()\n",
    "feature_imp_cb = pd.DataFrame({'feature': feature_cols, 'importance': importance_cb})\n",
    "feature_imp_cb = feature_imp_cb.sort_values('importance', ascending=False).head(10)\n",
    "print(feature_imp_cb)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **22.7 Comparison of Tree Models**\n",
    "\n",
    "Let's compare the performance of the different tree\u2011based models on the NEPSE regression task.\n",
    "\n",
    "```python\n",
    "models = {\n",
    "    'Decision Tree': dt_reg,\n",
    "    'Random Forest': rf_reg,\n",
    "    'Gradient Boosting': gb_reg,\n",
    "    'XGBoost': model_xgb,\n",
    "    'LightGBM': model_lgb,\n",
    "    'CatBoost': cb_reg\n",
    "}\n",
    "\n",
    "results = []\n",
    "for name, model in models.items():\n",
    "    if name in ['XGBoost', 'LightGBM', 'CatBoost']:\n",
    "        # These models have their own predict methods\n",
    "        if name == 'XGBoost':\n",
    "            y_pred = model.predict(xgb.DMatrix(X_test))\n",
    "        elif name == 'LightGBM':\n",
    "            y_pred = model.predict(X_test, num_iteration=model.best_iteration)\n",
    "        else:\n",
    "            y_pred = model.predict(X_test)\n",
    "    else:\n",
    "        y_pred = model.predict(X_test)\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y_test_reg, y_pred))\n",
    "    results.append({'Model': name, 'Test RMSE': rmse})\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('Test RMSE')\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The comparison shows which model performs best on this particular dataset. Typically, boosting methods (XGBoost, LightGBM, CatBoost) outperform bagging (Random Forest) and single trees, but the ranking can vary with data.\n",
    "- Note that we haven't tuned hyperparameters extensively; a fair comparison would involve tuning each model.\n",
    "\n",
    "**Key Differences:**\n",
    "\n",
    "| Feature | Random Forest | Gradient Boosting | XGBoost | LightGBM | CatBoost |\n",
    "|---------|---------------|-------------------|---------|----------|----------|\n",
    "| Tree growth | Level-wise | Level-wise | Level-wise (histogram) | Leaf-wise | Symmetric |\n",
    "| Handling categoricals | One-hot encoding needed | One-hot encoding needed | One-hot encoding needed | One-hot encoding or native | Native, excellent |\n",
    "| Speed on large data | Moderate | Moderate | Fast | Very fast | Fast |\n",
    "| Overfitting tendency | Low | High (needs tuning) | Moderate | Moderate | Low |\n",
    "| Default performance | Good | Good | Very good | Very good | Very good |\n",
    "| Interpretability | High (importances) | Medium | Medium | Medium | Medium |\n",
    "\n",
    "---\n",
    "\n",
    "## **22.8 Best Practices for Tree\u2011Based Models in Time\u2011Series**\n",
    "\n",
    "### **22.8.1 Feature Engineering**\n",
    "\n",
    "Tree models can handle non\u2011linear relationships and interactions automatically, but they still benefit from good feature engineering. For NEPSE, include:\n",
    "\n",
    "- Lagged returns and volumes.\n",
    "- Rolling statistics (mean, std, min, max, skew).\n",
    "- Technical indicators (RSI, MACD, Bollinger Bands).\n",
    "- Calendar features (day of week, month, fiscal quarter).\n",
    "- Domain\u2011specific features (circuit breaker proximity).\n",
    "\n",
    "### **22.8.2 Preventing Look\u2011Ahead Bias**\n",
    "\n",
    "All features must be computable at prediction time. Ensure that rolling windows use only past data (`shift()` and closed windows). For example, when computing a 5\u2011day moving average for today's features, use `rolling(5).mean().shift(1)` to exclude today's value.\n",
    "\n",
    "### **22.8.3 Handling Missing Values**\n",
    "\n",
    "Tree models can handle missing values internally (XGBoost, LightGBM, CatBoost learn a default direction). However, it's good practice to either forward\u2011fill or drop rows with too many missing values.\n",
    "\n",
    "### **22.8.4 Temporal Cross\u2011Validation**\n",
    "\n",
    "Use time\u2011series cross\u2011validation (e.g., `TimeSeriesSplit`) to tune hyperparameters. Random shuffling will leak future information.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit, GridSearchCV\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "param_grid = {'max_depth': [3, 5, 7], 'n_estimators': [100, 200]}\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "grid = GridSearchCV(rf, param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
    "grid.fit(X, y_reg)  # use full dataset with temporal CV\n",
    "print(grid.best_params_)\n",
    "```\n",
    "\n",
    "### **22.8.5 Regularization and Early Stopping**\n",
    "\n",
    "- For boosting models, always use early stopping on a validation set to prevent overfitting.\n",
    "- Use `subsample`, `colsample_bytree`, `reg_alpha`, `reg_lambda` to add regularization.\n",
    "- For random forests, `max_depth`, `min_samples_split`, and `min_samples_leaf` control complexity.\n",
    "\n",
    "### **22.8.6 Interpretability**\n",
    "\n",
    "Use feature importances to understand which features drive predictions. This can also help in feature selection and debugging.\n",
    "\n",
    "### **22.8.7 Scalability**\n",
    "\n",
    "For large datasets (many stocks and long history), LightGBM and XGBoost are faster. Consider using GPU acceleration if available.\n",
    "\n",
    "---\n",
    "\n",
    "## **22.9 Application to NEPSE: Predicting Next\u2011Day Direction**\n",
    "\n",
    "Let's put it all together: we'll build a classifier to predict whether the NEPSE index (or a specific stock) will go up the next day. We'll use CatBoost for its ease of use and good default performance.\n",
    "\n",
    "```python\n",
    "# Assume X_train, y_train_clf, X_test, y_test_clf are already defined from temporal split\n",
    "\n",
    "# CatBoost classifier with some tuning\n",
    "model = CatBoostClassifier(iterations=500,\n",
    "                           learning_rate=0.05,\n",
    "                           depth=4,\n",
    "                           loss_function='Logloss',\n",
    "                           eval_metric='Accuracy',\n",
    "                           verbose=200,\n",
    "                           early_stopping_rounds=20,\n",
    "                           random_seed=42)\n",
    "\n",
    "model.fit(X_train, y_train_clf,\n",
    "          eval_set=(X_test, y_test_clf),\n",
    "          plot=False)\n",
    "\n",
    "# Predict on test\n",
    "y_pred = model.predict(X_test)\n",
    "acc = accuracy_score(y_test_clf, y_pred)\n",
    "print(f\"Test accuracy: {acc:.4f}\")\n",
    "\n",
    "# Feature importance\n",
    "feat_imp = pd.DataFrame({\n",
    "    'feature': feature_cols,\n",
    "    'importance': model.get_feature_importance()\n",
    "}).sort_values('importance', ascending=False).head(10)\n",
    "print(feat_imp)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(feat_imp['feature'], feat_imp['importance'])\n",
    "plt.xlabel('Importance')\n",
    "plt.title('Top 10 Feature Importances - CatBoost Classifier')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The model achieves a certain accuracy. If it's above 0.5, it has predictive power. But we must compare to a baseline: e.g., the proportion of up days in the test set (if >0.5, always predicting up gives that accuracy).\n",
    "- Feature importance reveals which features the model relies on. For NEPSE, we might see that recent returns, volatility, and RSI are top.\n",
    "\n",
    "---\n",
    "\n",
    "## **22.10 Chapter Summary**\n",
    "\n",
    "In this chapter, we explored tree\u2011based models for time\u2011series prediction, using the NEPSE dataset as a concrete example.\n",
    "\n",
    "- **Decision trees** provide interpretable rules but are prone to overfitting.\n",
    "- **Random forests** reduce variance by bagging and feature subsampling, yielding robust models.\n",
    "- **Gradient boosting** builds trees sequentially to correct errors, often achieving state\u2011of\u2011the\u2011art performance.\n",
    "- **XGBoost, LightGBM, and CatBoost** are optimized implementations with additional features like regularization, native categorical support, and speed.\n",
    "- We compared their strengths and weaknesses and provided best practices for time\u2011series applications.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Tree models are excellent for capturing non\u2011linear relationships and interactions among features like lagged returns, volume, and technical indicators.\n",
    "- Always use temporal cross\u2011validation to avoid look\u2011ahead bias.\n",
    "- Regularization and early stopping are crucial to prevent overfitting, especially in boosting.\n",
    "- Feature importance helps interpret what drives predictions, which is valuable for understanding market dynamics.\n",
    "- CatBoost and LightGBM are good starting points due to their ease of use and performance.\n",
    "\n",
    "In the next chapter, **Chapter 23: Linear Models for Time\u2011Series**, we will revisit linear models but with a focus on regularization and their application to financial forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 22**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='21. traditional_statistical_models.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='23. linear_models_for_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}