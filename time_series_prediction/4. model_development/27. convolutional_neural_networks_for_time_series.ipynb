{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 27: Convolutional Neural Networks for Time\u2011Series\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand how convolutional neural networks (CNNs) can be applied to sequential data\n",
    "- Explain the mechanics of 1D convolutions and how they differ from 2D convolutions used in images\n",
    "- Design and implement 1D CNN models for time\u2011series forecasting using the NEPSE dataset\n",
    "- Utilize pooling layers to reduce dimensionality and extract salient features\n",
    "- Grasp the concept of dilated convolutions and their role in capturing long\u2011range dependencies\n",
    "- Build Temporal Convolutional Networks (TCNs) with residual connections for improved performance\n",
    "- Combine CNN and RNN layers in hybrid architectures to leverage both local patterns and sequential memory\n",
    "- Apply best practices for training CNNs on time\u2011series data, including regularization and validation\n",
    "- Compare CNN\u2011based models with MLPs, RNNs, and traditional models on the NEPSE prediction task\n",
    "\n",
    "---\n",
    "\n",
    "## **27.1 Introduction to CNNs for Sequential Data**\n",
    "\n",
    "Convolutional Neural Networks (CNNs) have revolutionized computer vision, but they are also highly effective for time\u2011series analysis. When applied to sequences, 1D convolutions slide a filter (kernel) across the time dimension to detect local patterns\u2014such as short\u2011term trends, spikes, or repeated motifs. These learned patterns are then combined hierarchically to recognize more complex structures.\n",
    "\n",
    "For the NEPSE prediction system, CNNs can automatically extract relevant features from raw sequences of returns, volume, or technical indicators. Unlike RNNs, which process sequences step\u2011by\u2011step, CNNs are **parallelizable** and often faster to train. They have also been shown to achieve state\u2011of\u2011the\u2011art results on various time\u2011series benchmarks when properly designed (e.g., WaveNet, Temporal Convolutional Networks).\n",
    "\n",
    "### **27.1.1 Why Use CNNs for Time\u2011Series?**\n",
    "\n",
    "- **Local pattern detection:** Convolutions capture short\u2011term dependencies (e.g., the shape of a price spike) regardless of where they occur in the sequence.\n",
    "- **Hierarchical features:** Stacking convolutional layers allows the network to learn increasingly abstract representations (from raw movements to patterns like \"head and shoulders\").\n",
    "- **Efficiency:** Convolutions can be computed in parallel, making them faster than recurrent layers, especially on GPUs.\n",
    "- **Flexible receptive field:** Through dilation, CNNs can cover long time spans without a huge number of parameters.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.2 1D Convolutions**\n",
    "\n",
    "A 1D convolution operates over a single dimension (time). For an input sequence of length `L` with `C` channels (features), a convolution kernel of size `k` slides across time, computing a dot product between the kernel weights and the corresponding window of data. The result is a new sequence (feature map) of length `L - k + 1` (with no padding) or `L` (with padding). Multiple kernels produce multiple output channels.\n",
    "\n",
    "### **27.2.1 Understanding the Operation**\n",
    "\n",
    "Imagine we have a sequence of daily returns `r\u2081, r\u2082, \u2026, r_T`. A kernel of size 3 might learn to detect a pattern like \"down, up, up\". At each position, it computes:\n",
    "\n",
    "`output[t] = w\u2081 * r_t + w\u2082 * r_{t+1} + w\u2083 * r_{t+2} + b`\n",
    "\n",
    "where `w` are the learned weights and `b` is a bias. After training, certain kernels become activated by specific local shapes.\n",
    "\n",
    "### **27.2.2 Implementing a 1D CNN in Keras**\n",
    "\n",
    "We'll build a simple CNN for the NEPSE return prediction task. We'll use the same sequence data as in Chapter 26 (20\u2011day windows of returns). We'll add a convolutional layer followed by a dense output layer.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load and prepare data (same as in Chapter 26)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Compute returns\n",
    "df_stock['Return'] = df_stock['Close'].pct_change() * 100\n",
    "\n",
    "# Use only returns as feature for simplicity\n",
    "feature_columns = ['Return']\n",
    "df_stock = df_stock.dropna(subset=feature_columns)\n",
    "\n",
    "# Function to create sequences (same as before)\n",
    "def create_sequences(data, seq_length):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :])\n",
    "        y.append(data[i+seq_length, 0])  # target is the next return\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "seq_length = 20\n",
    "data = df_stock[feature_columns].values\n",
    "X, y = create_sequences(data, seq_length)\n",
    "\n",
    "# Temporal split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Scale features (reshape to 2D, scale, reshape back)\n",
    "original_shape = X_train.shape\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_2d = scaler.fit_transform(X_train_2d)\n",
    "X_train_scaled = X_train_scaled_2d.reshape(original_shape)\n",
    "\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_test_scaled_2d = scaler.transform(X_test_2d)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(X_test.shape)\n",
    "\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")  # (samples, timesteps, features)\n",
    "```\n",
    "\n",
    "Now we build a simple 1D CNN model:\n",
    "\n",
    "```python\n",
    "model_cnn = keras.Sequential([\n",
    "    layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, activation='relu'),\n",
    "    layers.GlobalAveragePooling1D(),  # or Flatten()\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_cnn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model_cnn.summary()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `Conv1D` with 64 filters and kernel size 3. The input shape is `(timesteps=20, features=1)`. The layer outputs a tensor of shape `(batch, timesteps - kernel_size + 1, 64)` if no padding, but Keras uses 'valid' padding by default (no padding), so the time dimension reduces. To keep the length the same, we can add `padding='same'`.\n",
    "- `MaxPooling1D` with pool size 2 reduces the time dimension by half, extracting the most salient features.\n",
    "- A second `Conv1D` layer with 32 filters.\n",
    "- `GlobalAveragePooling1D` averages over the time dimension, producing a fixed\u2011size vector for each sample, which is then fed to a dense output layer. Alternatively, we could use `Flatten()`, but global pooling reduces parameters and helps prevent overfitting.\n",
    "- The output layer is a single neuron for regression.\n",
    "\n",
    "### **27.2.3 Training and Evaluation**\n",
    "\n",
    "```python\n",
    "# Validation split (temporal)\n",
    "val_size = int(len(X_train_scaled) * 0.1)\n",
    "X_val = X_train_scaled[-val_size:]\n",
    "y_val = y_train[-val_size:]\n",
    "X_train_final = X_train_scaled[:-val_size]\n",
    "y_train_final = y_train[:-val_size]\n",
    "\n",
    "history = model_cnn.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Test evaluation\n",
    "y_pred = model_cnn.predict(X_test_scaled).flatten()\n",
    "rmse_cnn = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"CNN Test RMSE: {rmse_cnn:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We train similarly to the LSTM model. The CNN often trains faster because convolutions are parallelizable.\n",
    "- The performance may be comparable to the LSTM; the CNN captures local patterns but does not have an explicit memory of long\u2011term dependencies beyond what the stacked convolutions can cover (receptive field).\n",
    "\n",
    "---\n",
    "\n",
    "## **27.3 Pooling Layers**\n",
    "\n",
    "Pooling layers downsample the feature maps, reducing dimensionality and providing translation invariance. Common types:\n",
    "\n",
    "- **Max pooling:** Takes the maximum value over a window. Preserves the most activated features.\n",
    "- **Average pooling:** Takes the average. Smoother but may lose sharp features.\n",
    "\n",
    "In time\u2011series, pooling helps to condense the representation and focus on the most important local patterns. However, excessive pooling can discard useful temporal information. For forecasting, global pooling (over the whole time dimension) is often used before the final dense layer.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.4 Temporal Convolutional Networks (TCN)**\n",
    "\n",
    "A Temporal Convolutional Network (TCN) is a specific architecture that combines several principles:\n",
    "\n",
    "- **Causal convolutions:** Ensure that predictions at time `t` depend only on times `\u2264 t` (no look\u2011ahead). Achieved by padding only on the left.\n",
    "- **Dilated convolutions:** Introduce gaps in the kernel to exponentially increase the receptive field without increasing the number of parameters.\n",
    "- **Residual connections:** Allow training of very deep networks by adding skip connections.\n",
    "\n",
    "### **27.4.1 Dilated Convolutions**\n",
    "\n",
    "In a standard convolution, the kernel is applied to contiguous time steps. With dilation, we skip steps. For example, a kernel of size 3 with dilation rate 2 covers positions `t, t+2, t+4`. By stacking layers with exponentially increasing dilation (1,2,4,8,\u2026), the receptive field grows quickly, enabling the network to capture long\u2011range dependencies.\n",
    "\n",
    "### **27.4.2 Building a Simple TCN Block**\n",
    "\n",
    "We can implement a TCN using Keras with custom layers, but there are also libraries like `keras-tcn`. Here's a simplified version of a residual block with dilated convolutions:\n",
    "\n",
    "```python\n",
    "def residual_block(x, dilation_rate, filters, kernel_size=3):\n",
    "    # Save input for skip connection\n",
    "    shortcut = x\n",
    "    \n",
    "    # First dilated convolution\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Second dilated convolution\n",
    "    x = layers.Conv1D(filters, kernel_size, padding='causal', dilation_rate=dilation_rate, activation='relu')(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    \n",
    "    # Add residual (if dimensions match; otherwise use 1x1 conv)\n",
    "    if shortcut.shape[-1] != filters:\n",
    "        shortcut = layers.Conv1D(filters, 1, padding='same')(shortcut)\n",
    "    x = layers.add([x, shortcut])\n",
    "    x = layers.Activation('relu')(x)\n",
    "    return x\n",
    "\n",
    "# Build TCN model\n",
    "inputs = keras.Input(shape=(X_train_scaled.shape[1], X_train_scaled.shape[2]))\n",
    "x = inputs\n",
    "filters = 32\n",
    "for dilation_rate in [1, 2, 4, 8]:\n",
    "    x = residual_block(x, dilation_rate, filters)\n",
    "    filters *= 2  # double filters each block (optional)\n",
    "\n",
    "x = layers.GlobalAveragePooling1D()(x)\n",
    "x = layers.Dense(64, activation='relu')(x)\n",
    "outputs = layers.Dense(1)(x)\n",
    "\n",
    "model_tcn = keras.Model(inputs, outputs)\n",
    "model_tcn.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model_tcn.summary()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Each residual block contains two dilated convolutional layers with `padding='causal'` (ensures no leakage from future).\n",
    "- Dilation rates increase exponentially (1,2,4,8), giving a large receptive field with few layers.\n",
    "- Filters are doubled in each block to increase capacity.\n",
    "- A global average pooling compresses the time dimension, followed by a dense layer.\n",
    "- The model is trained similarly to the simple CNN.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.5 CNN\u2011RNN Hybrids**\n",
    "\n",
    "Combining CNNs and RNNs can leverage the strengths of both: CNNs extract local features, and RNNs model temporal dependencies. A common pattern is to use one or more convolutional layers to preprocess the input sequence, then feed the resulting feature maps into an LSTM or GRU.\n",
    "\n",
    "```python\n",
    "model_hybrid = keras.Sequential([\n",
    "    layers.Conv1D(filters=64, kernel_size=5, activation='relu', padding='same', input_shape=(seq_length, 1)),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same'),\n",
    "    layers.MaxPooling1D(pool_size=2),\n",
    "    layers.LSTM(50, return_sequences=False),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model_hybrid.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model_hybrid.summary()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The convolutional layers reduce the time dimension (via pooling) and extract local features.\n",
    "- The LSTM then processes the reduced sequence, capturing longer\u2011term dependencies.\n",
    "- This hybrid can be more efficient than a pure LSTM on long sequences because the CNN reduces the sequence length.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.6 Architectural Patterns**\n",
    "\n",
    "When designing CNNs for time\u2011series, consider these patterns:\n",
    "\n",
    "- **Early vs. late fusion:** If you have multiple features (e.g., returns, volume, RSI), you can either process them together from the start (early fusion) or have separate branches that are combined later (late fusion).\n",
    "- **Dilated stacks:** Stacking dilated convolutions (like TCN) often works well for long sequences.\n",
    "- **Residual connections:** Essential for deep networks to avoid vanishing gradients.\n",
    "- **Global pooling vs. flattening:** Global average pooling is parameter\u2011efficient and often generalizes better than flattening followed by dense layers.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.7 Implementation Strategies**\n",
    "\n",
    "### **27.7.1 Data Preparation**\n",
    "\n",
    "- Ensure sequences are constructed without look\u2011ahead.\n",
    "- Scale features per channel (as shown).\n",
    "- For multi\u2011step forecasting, you can have multiple output neurons (one per future step) or use a sequence\u2011to\u2011sequence architecture.\n",
    "\n",
    "### **27.7.2 Regularization**\n",
    "\n",
    "- Use dropout after convolutional layers (spatial dropout can be applied across channels).\n",
    "- Apply batch normalization to stabilize training.\n",
    "- Use L2 weight regularization.\n",
    "- Early stopping.\n",
    "\n",
    "### **27.7.3 Hyperparameter Tuning**\n",
    "\n",
    "Important hyperparameters:\n",
    "\n",
    "- Number of filters\n",
    "- Kernel size\n",
    "- Number of layers\n",
    "- Dilation rates\n",
    "- Pooling size\n",
    "- Learning rate and optimizer\n",
    "\n",
    "Use walk\u2011forward validation to tune these, as with any time\u2011series model.\n",
    "\n",
    "### **27.7.4 Comparing with RNNs and MLPs**\n",
    "\n",
    "CNNs often train faster and can achieve competitive accuracy. For the NEPSE dataset, they might capture short\u2011term patterns (e.g., 2\u20113 day reversals) effectively. Long\u2011term dependencies (e.g., monthly cycles) may require deeper networks or larger dilation rates.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.8 Practical Considerations for NEPSE**\n",
    "\n",
    "- **Feature engineering:** Even with CNNs, providing derived features (RSI, moving averages) as additional channels often helps. The CNN can learn to combine them.\n",
    "- **Sequence length:** Experiment with lengths from 10 to 60 days. Use validation performance to decide.\n",
    "- **Multi\u2011step forecasting:** For predicting multiple days ahead, a TCN with multiple output neurons or a sequence\u2011to\u2011sequence model can be used.\n",
    "- **Ensemble:** Combining CNN and LSTM predictions (simple average) may improve robustness.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.9 Comparison with Other Models**\n",
    "\n",
    "We can benchmark the CNN against the LSTM and MLP from previous chapters. Typically, on financial returns, all models may have similar RMSE, but CNNs often train faster. The choice may come down to computational resources and whether the data has strong local patterns.\n",
    "\n",
    "---\n",
    "\n",
    "## **27.10 Chapter Summary**\n",
    "\n",
    "In this chapter, we explored the application of Convolutional Neural Networks to time\u2011series forecasting, using the NEPSE dataset as a running example.\n",
    "\n",
    "- **1D convolutions** detect local patterns in sequences and are highly parallelizable.\n",
    "- **Pooling layers** reduce dimensionality and provide a degree of invariance.\n",
    "- **Temporal Convolutional Networks (TCNs)** use dilated convolutions and residual connections to capture long\u2011range dependencies efficiently.\n",
    "- **CNN\u2011RNN hybrids** combine the strengths of both architectures.\n",
    "- We implemented simple CNN, TCN, and hybrid models in Keras and evaluated them on the return prediction task.\n",
    "- Regularization and proper temporal validation are essential to avoid overfitting.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Start with a simple 1D CNN as a fast baseline.\n",
    "- If the sequence is long, consider a TCN with dilation.\n",
    "- Use multiple feature channels (returns, volume, technical indicators) to enrich the input.\n",
    "- Compare CNN performance with LSTM and MLP; often CNNs are competitive and faster.\n",
    "- Always validate with walk\u2011forward to ensure the model generalizes to new market regimes.\n",
    "\n",
    "In the next chapter, **Chapter 28: Transformer Models for Time\u2011Series**, we will delve into attention\u2011based architectures that have recently achieved state\u2011of\u2011the\u2011art results in many sequence\u2011to\u2011sequence tasks.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 27**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='26. recurrent_neural_networks.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='28. transformer_models_for_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}