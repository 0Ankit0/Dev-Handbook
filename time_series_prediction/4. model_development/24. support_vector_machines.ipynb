{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 24: Support Vector Machines\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamental concepts of Support Vector Machines (SVMs) for classification and regression\n",
    "- Explain the role of margins, support vectors, and the kernel trick\n",
    "- Apply SVM for classification (SVC) to predict NEPSE price direction\n",
    "- Use SVM for regression (SVR) to forecast returns or prices\n",
    "- Choose appropriate kernel functions (linear, polynomial, RBF) based on data characteristics\n",
    "- Tune SVM hyperparameters (C, gamma, degree) using time‑series cross‑validation\n",
    "- Scale features appropriately for SVM\n",
    "- Interpret SVM models and understand their limitations\n",
    "- Compare SVM performance with linear models and tree‑based models on the NEPSE dataset\n",
    "\n",
    "---\n",
    "\n",
    "## **24.1 SVM Fundamentals**\n",
    "\n",
    "Support Vector Machines (SVMs) are a class of powerful supervised learning algorithms originally developed for binary classification. The core idea is to find a hyperplane that separates the classes with the largest possible margin. The “support vectors” are the data points closest to the decision boundary; they alone determine the hyperplane. SVMs can also be extended to regression tasks (SVR) and, through the **kernel trick**, can capture non‑linear relationships without explicitly computing features in a high‑dimensional space.\n",
    "\n",
    "In the context of the NEPSE prediction system, we can use SVM to classify whether the next day's return will be positive (up) or negative (down). Alternatively, we can use SVR to forecast the exact return magnitude.\n",
    "\n",
    "### **24.1.1 Maximum Margin Classification**\n",
    "\n",
    "For a binary classification problem with linearly separable data, there are infinitely many separating hyperplanes. SVM chooses the one that maximizes the distance (margin) between the two classes. The decision function is:\n",
    "\n",
    "`f(x) = w·x + b`\n",
    "\n",
    "where `w` is the weight vector and `b` is the bias. The class is determined by the sign of `f(x)`. The margin is `2/||w||`, so maximizing the margin is equivalent to minimizing `||w||²` subject to the constraints that all points are correctly classified.\n",
    "\n",
    "In practice, data is often not perfectly separable. SVM introduces **slack variables** to allow some points to be on the wrong side of the margin (or even misclassified). This is controlled by the hyperparameter `C`:\n",
    "\n",
    "- Large `C` gives a hard margin (fewer misclassifications, but may overfit).\n",
    "- Small `C` gives a soft margin (more tolerance, potentially better generalization).\n",
    "\n",
    "### **24.1.2 The Kernel Trick**\n",
    "\n",
    "To handle non‑linear decision boundaries, SVM can map the original features into a higher‑dimensional space where the classes become linearly separable. The **kernel trick** allows this mapping to be performed implicitly by replacing dot products with a kernel function `K(xᵢ, xⱼ) = φ(xᵢ)·φ(xⱼ)`. Common kernels:\n",
    "\n",
    "- **Linear:** `K(x, y) = x·y` (equivalent to no mapping).\n",
    "- **Polynomial:** `K(x, y) = (γ x·y + r)ᵈ` where `d` is the degree.\n",
    "- **RBF (Radial Basis Function):** `K(x, y) = exp(-γ ||x - y||²)`, the most popular non‑linear kernel.\n",
    "- **Sigmoid:** `K(x, y) = tanh(γ x·y + r)`.\n",
    "\n",
    "For time‑series financial data, RBF is often a good starting point because it can model complex, non‑linear relationships.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.2 SVM for Classification (SVC)**\n",
    "\n",
    "We'll first apply SVM to the binary direction prediction task from previous chapters. The target is 1 if tomorrow's return > 0, else 0.\n",
    "\n",
    "### **24.2.1 Data Preparation and Scaling**\n",
    "\n",
    "SVM is sensitive to feature scales because it relies on distances. Therefore, we **must** scale features, typically to zero mean and unit variance.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load and prepare NEPSE data (same as previous chapters)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Use a single symbol for simplicity\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Create features and target (as in Chapter 23)\n",
    "df_stock['Return'] = df_stock['Close'].pct_change() * 100\n",
    "df_stock['Return_Lag1'] = df_stock['Return'].shift(1)\n",
    "df_stock['Return_Lag2'] = df_stock['Return'].shift(2)\n",
    "df_stock['Volume_Lag1'] = df_stock['Vol'].shift(1)\n",
    "df_stock['MA_5'] = df_stock['Close'].rolling(5).mean()\n",
    "df_stock['Volatility_5'] = df_stock['Return'].rolling(5).std()\n",
    "# RSI (simplified)\n",
    "delta = df_stock['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(14).mean()\n",
    "avg_loss = loss.rolling(14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "df_stock['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Target: direction (1 if next day's return > 0, else 0)\n",
    "df_stock['Target'] = (df_stock['Return'].shift(-1) > 0).astype(int)\n",
    "\n",
    "# Drop NaN\n",
    "df_stock = df_stock.dropna()\n",
    "\n",
    "# Feature columns\n",
    "feature_cols = ['Return_Lag1', 'Return_Lag2', 'Volume_Lag1', 'MA_5', 'Volatility_5', 'RSI']\n",
    "X = df_stock[feature_cols]\n",
    "y = df_stock['Target']\n",
    "\n",
    "# Temporal split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Scale features (fit on training only)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Scaling is crucial: without it, features with larger ranges (e.g., volume) would dominate the distance calculations, leading to poor performance.\n",
    "- We fit the scaler only on the training set to avoid data leakage.\n",
    "\n",
    "### **24.2.2 Training an SVM Classifier**\n",
    "\n",
    "We'll start with a linear SVM (linear kernel) as a baseline, then move to RBF.\n",
    "\n",
    "```python\n",
    "# Linear SVM\n",
    "svm_linear = SVC(kernel='linear', C=1.0, random_state=42)\n",
    "svm_linear.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_linear = svm_linear.predict(X_test_scaled)\n",
    "acc_linear = accuracy_score(y_test, y_pred_linear)\n",
    "print(f\"Linear SVM test accuracy: {acc_linear:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `C=1.0` is the default. Larger C tries to classify every training point correctly, which may lead to overfitting.\n",
    "- The linear SVM is similar to logistic regression but with a margin objective. It can serve as a baseline.\n",
    "\n",
    "### **24.2.3 RBF Kernel SVM**\n",
    "\n",
    "The RBF kernel can model non‑linear boundaries. It has two important hyperparameters: `C` (regularization) and `gamma` (kernel coefficient). A small gamma means a large similarity radius, leading to smoother decision boundaries; large gamma makes each point have high influence, potentially overfitting.\n",
    "\n",
    "```python\n",
    "# RBF SVM with default gamma\n",
    "svm_rbf = SVC(kernel='rbf', C=1.0, gamma='scale', random_state=42)\n",
    "svm_rbf.fit(X_train_scaled, y_train)\n",
    "\n",
    "y_pred_rbf = svm_rbf.predict(X_test_scaled)\n",
    "acc_rbf = accuracy_score(y_test, y_pred_rbf)\n",
    "print(f\"RBF SVM test accuracy: {acc_rbf:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `gamma='scale'` uses `1 / (n_features * X.var())` as the gamma value, a reasonable default. `gamma='auto'` uses `1 / n_features`.\n",
    "- The RBF kernel often outperforms linear if the true decision boundary is non‑linear.\n",
    "\n",
    "### **24.2.4 Hyperparameter Tuning with Grid Search**\n",
    "\n",
    "To get the best performance, we must tune `C` and `gamma`. We'll use `GridSearchCV` with time‑series cross‑validation.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 'scale', 'auto'],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Grid search\n",
    "svm = SVC(random_state=42)\n",
    "grid = GridSearchCV(svm, param_grid, cv=tscv, scoring='accuracy', verbose=1, n_jobs=-1)\n",
    "grid.fit(X_train_scaled, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid.best_params_}\")\n",
    "print(f\"Best cross-validation accuracy: {grid.best_score_:.4f}\")\n",
    "\n",
    "# Evaluate on test set\n",
    "y_pred_best = grid.predict(X_test_scaled)\n",
    "acc_best = accuracy_score(y_test, y_pred_best)\n",
    "print(f\"Test accuracy with best model: {acc_best:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We search over a range of `C` and `gamma` values. The grid should be chosen based on data scale; a logarithmic scale is common.\n",
    "- `TimeSeriesSplit` ensures that the validation sets are temporally after the training folds, preventing look‑ahead.\n",
    "- The best model is then evaluated on the held‑out test set.\n",
    "\n",
    "### **24.2.5 Interpreting SVM Results**\n",
    "\n",
    "SVM models are not as interpretable as linear models or decision trees. However, we can gain some insight by:\n",
    "\n",
    "- Examining the support vectors (the points that define the margin).\n",
    "- For linear SVM, the coefficients (weights) indicate feature importance, though they are affected by scaling.\n",
    "\n",
    "```python\n",
    "# Support vectors\n",
    "print(f\"Number of support vectors: {len(grid.best_estimator_.support_)}\")\n",
    "print(f\"Support vector indices: {grid.best_estimator_.support_[:5]}\")  # first five\n",
    "\n",
    "# For linear kernel, we can get coefficients\n",
    "if grid.best_estimator_.kernel == 'linear':\n",
    "    coef = grid.best_estimator_.coef_[0]\n",
    "    imp_df = pd.DataFrame({'feature': feature_cols, 'coef': coef})\n",
    "    print(imp_df.sort_values('coef', key=abs, ascending=False))\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Support vectors are the critical data points. If there are many, the model may be overfitting.\n",
    "- For linear SVM, the magnitude of coefficients indicates feature importance, but careful: features were scaled, so coefficients are comparable.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.3 SVM for Regression (SVR)**\n",
    "\n",
    "Support Vector Regression (SVR) aims to find a function that approximates the target values while keeping errors within a margin (epsilon). Points with errors larger than epsilon are penalized.\n",
    "\n",
    "The hyperparameters are:\n",
    "- `epsilon`: width of the insensitive tube.\n",
    "- `C`: regularization (trade‑off between flatness and tolerance of errors).\n",
    "- Kernel parameters (e.g., `gamma` for RBF).\n",
    "\n",
    "### **24.3.1 Applying SVR to NEPSE Return Prediction**\n",
    "\n",
    "We'll use the same features but now the target is the continuous next‑day return.\n",
    "\n",
    "```python\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Target: next day's return\n",
    "y_reg = df_stock['Return'].shift(-1).dropna()  # align with X\n",
    "\n",
    "# Ensure X and y have same index after dropping NaN in y\n",
    "X_reg = X.loc[y_reg.index]\n",
    "y_reg = y_reg\n",
    "\n",
    "# Temporal split (same split index as before, but ensure lengths match)\n",
    "split_idx_reg = int(len(X_reg) * 0.8)\n",
    "X_train_reg, X_test_reg = X_reg.iloc[:split_idx_reg], X_reg.iloc[split_idx_reg:]\n",
    "y_train_reg, y_test_reg = y_reg.iloc[:split_idx_reg], y_reg.iloc[split_idx_reg:]\n",
    "\n",
    "# Scale\n",
    "scaler_reg = StandardScaler()\n",
    "X_train_reg_scaled = scaler_reg.fit_transform(X_train_reg)\n",
    "X_test_reg_scaled = scaler_reg.transform(X_test_reg)\n",
    "\n",
    "# SVR with RBF kernel (default)\n",
    "svr = SVR(kernel='rbf', C=1.0, epsilon=0.1)\n",
    "svr.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "y_pred_svr = svr.predict(X_test_reg_scaled)\n",
    "rmse_svr = np.sqrt(mean_squared_error(y_test_reg, y_pred_svr))\n",
    "print(f\"SVR test RMSE: {rmse_svr:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `epsilon=0.1` means errors less than 0.1% are not penalized. This is domain‑specific; for returns, a small epsilon (e.g., 0.1‑0.5) is typical.\n",
    "- `C` and `gamma` should be tuned similarly to classification.\n",
    "\n",
    "### **24.3.2 Tuning SVR Hyperparameters**\n",
    "\n",
    "We'll use `GridSearchCV` again.\n",
    "\n",
    "```python\n",
    "param_grid_svr = {\n",
    "    'C': [0.1, 1, 10, 100],\n",
    "    'gamma': [0.001, 0.01, 0.1, 1, 'scale'],\n",
    "    'epsilon': [0.01, 0.1, 0.2, 0.5],\n",
    "    'kernel': ['rbf']\n",
    "}\n",
    "\n",
    "svr_tune = SVR()\n",
    "grid_svr = GridSearchCV(svr_tune, param_grid_svr, cv=tscv, scoring='neg_mean_squared_error', \n",
    "                        verbose=1, n_jobs=-1)\n",
    "grid_svr.fit(X_train_reg_scaled, y_train_reg)\n",
    "\n",
    "print(f\"Best SVR params: {grid_svr.best_params_}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-grid_svr.best_score_):.4f}\")\n",
    "\n",
    "y_pred_svr_best = grid_svr.predict(X_test_reg_scaled)\n",
    "rmse_svr_best = np.sqrt(mean_squared_error(y_test_reg, y_pred_svr_best))\n",
    "print(f\"SVR test RMSE (tuned): {rmse_svr_best:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We use negative MSE as the scoring metric; the grid search maximizes score, so we take negative.\n",
    "- The best model is then evaluated on the test set.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.4 Kernel Selection and Characteristics**\n",
    "\n",
    "Choosing the right kernel depends on the data:\n",
    "\n",
    "- **Linear kernel:** Use when data is approximately linearly separable, or as a baseline. Fast, interpretable.\n",
    "- **Polynomial kernel:** Can model interactions but has more hyperparameters (degree, coef0). May overfit with high degree.\n",
    "- **RBF kernel:** Most flexible; can model any non‑linear boundary if `gamma` is chosen well. Generally a good default.\n",
    "\n",
    "For NEPSE data, the relationship between features and returns is likely non‑linear, so RBF is a strong candidate. However, we can compare performance across kernels.\n",
    "\n",
    "```python\n",
    "kernels = ['linear', 'poly', 'rbf', 'sigmoid']\n",
    "results = {}\n",
    "\n",
    "for kernel in kernels:\n",
    "    if kernel == 'poly':\n",
    "        svm = SVC(kernel=kernel, degree=2, C=1, random_state=42)  # default degree 3; try 2 for simplicity\n",
    "    else:\n",
    "        svm = SVC(kernel=kernel, C=1, random_state=42)\n",
    "    svm.fit(X_train_scaled, y_train)\n",
    "    y_pred = svm.predict(X_test_scaled)\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    results[kernel] = acc\n",
    "    print(f\"{kernel} kernel accuracy: {acc:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- For polynomial, we set `degree=2` to avoid too much complexity. In practice, tuning degree is also needed.\n",
    "- Sigmoid kernel may behave similarly to RBF for some parameters but is less common.\n",
    "- The results guide which kernel family is promising.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.5 Scaling and Preprocessing**\n",
    "\n",
    "SVM requires careful scaling. Always:\n",
    "\n",
    "- Scale features to zero mean and unit variance (StandardScaler) or to a [0,1] range (MinMaxScaler). StandardScaler is more common.\n",
    "- Fit the scaler on the training set only.\n",
    "- Apply the same transformation to test/validation sets.\n",
    "\n",
    "If the data contains outliers, robust scaling (using median and quantiles) may be beneficial, but SVM with RBF is somewhat sensitive to outliers anyway.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "robust_scaler = RobustScaler()\n",
    "X_train_robust = robust_scaler.fit_transform(X_train)\n",
    "X_test_robust = robust_scaler.transform(X_test)\n",
    "# Then train SVM...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **24.6 Strengths and Limitations of SVM**\n",
    "\n",
    "### **Strengths**\n",
    "\n",
    "- **Effective in high‑dimensional spaces** – even when number of features exceeds samples (though careful with overfitting).\n",
    "- **Memory efficient** – only support vectors are stored, not all data points.\n",
    "- **Versatile** through different kernel functions.\n",
    "- **Good theoretical foundations** – maximum margin principle often leads to good generalization.\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "- **Not suitable for large datasets** – training time can be O(n²) or O(n³). For NEPSE with thousands of rows, it's fine, but for millions, it's slow.\n",
    "- **Sensitive to feature scaling** – must scale carefully.\n",
    "- **Difficult to interpret** – especially with non‑linear kernels.\n",
    "- **Choice of kernel and hyperparameters** can be non‑intuitive; extensive tuning required.\n",
    "- **No direct probability estimates** – though Platt scaling can be applied (SVC has `probability=True` option, but it's slower).\n",
    "- **Can perform poorly on very noisy data** – if classes overlap heavily, SVM may not separate well.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.7 Comparison with Other Models**\n",
    "\n",
    "Let's compare SVM with logistic regression and random forest on the same classification task.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Logistic Regression (with scaling)\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_train_scaled, y_train)\n",
    "y_pred_lr = lr.predict(X_test_scaled)\n",
    "acc_lr = accuracy_score(y_test, y_pred_lr)\n",
    "\n",
    "# Random Forest (no scaling needed)\n",
    "rf = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=42)\n",
    "rf.fit(X_train, y_train)  # note: uses unscaled data\n",
    "y_pred_rf = rf.predict(X_test)\n",
    "acc_rf = accuracy_score(y_test, y_pred_rf)\n",
    "\n",
    "# Best SVM (from grid search)\n",
    "acc_svm = acc_best\n",
    "\n",
    "print(\"Model Comparison:\")\n",
    "print(f\"Logistic Regression: {acc_lr:.4f}\")\n",
    "print(f\"Random Forest:       {acc_rf:.4f}\")\n",
    "print(f\"SVM (RBF, tuned):    {acc_svm:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Logistic regression is a linear baseline; SVM may outperform if non‑linearity is present.\n",
    "- Random forest is non‑linear and often competitive; comparing helps decide which model family suits the data.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.8 Practical Considerations for NEPSE**\n",
    "\n",
    "### **24.8.1 Dealing with Imbalanced Classes**\n",
    "\n",
    "If the market has more up days than down days (or vice versa), the classes may be imbalanced. SVM with unbalanced data can be adjusted using the `class_weight` parameter.\n",
    "\n",
    "```python\n",
    "svm_balanced = SVC(kernel='rbf', class_weight='balanced', random_state=42)\n",
    "```\n",
    "\n",
    "This assigns higher penalty to misclassifications of the minority class.\n",
    "\n",
    "### **24.8.2 Probability Outputs**\n",
    "\n",
    "If you need probability estimates (e.g., for position sizing), set `probability=True` (fits an additional Platt scaling model). This increases training time.\n",
    "\n",
    "```python\n",
    "svm_prob = SVC(kernel='rbf', probability=True, random_state=42)\n",
    "svm_prob.fit(X_train_scaled, y_train)\n",
    "probs = svm_prob.predict_proba(X_test_scaled)[:, 1]  # probability of class 1\n",
    "```\n",
    "\n",
    "### **24.8.3 Computational Efficiency**\n",
    "\n",
    "For larger datasets, consider using `LinearSVC` (for linear kernel) which is more scalable. For non‑linear, you may need to sample data or use a different algorithm.\n",
    "\n",
    "### **24.8.4 Feature Engineering**\n",
    "\n",
    "SVM with RBF can capture non‑linear interactions automatically, so extensive feature engineering (like polynomial features) is less critical. However, domain‑specific features (e.g., RSI, moving averages) still help because they encode domain knowledge.\n",
    "\n",
    "---\n",
    "\n",
    "## **24.9 Chapter Summary**\n",
    "\n",
    "In this chapter, we explored Support Vector Machines for both classification and regression, using the NEPSE dataset as a concrete example.\n",
    "\n",
    "- **SVM fundamentals:** maximum margin, support vectors, and the kernel trick.\n",
    "- **SVC for direction prediction:** we trained linear and RBF SVMs, tuned hyperparameters (`C`, `gamma`) with time‑series cross‑validation, and evaluated on a test set.\n",
    "- **SVR for return forecasting:** we applied SVR to continuous targets, tuning `C`, `gamma`, and `epsilon`.\n",
    "- **Kernel selection:** RBF is a flexible default; we compared linear, polynomial, and sigmoid.\n",
    "- **Scaling is mandatory** – we used `StandardScaler`.\n",
    "- **Strengths:** effective in high dimensions, versatile, good generalization.\n",
    "- **Limitations:** slow on large data, sensitive to scaling, hard to interpret.\n",
    "- **Comparison with other models** helps contextualize SVM performance.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- SVM can be a powerful tool for predicting stock direction, especially if the decision boundary is non‑linear.\n",
    "- Always scale features and tune hyperparameters using time‑series CV.\n",
    "- For large datasets, consider linear SVM or sampling; for small to medium, RBF is fine.\n",
    "- SVM does not provide native feature importance, but for linear SVM coefficients can be examined.\n",
    "- Combine SVM with domain‑specific features for best results.\n",
    "\n",
    "In the next chapter, **Chapter 25: Neural Network Fundamentals**, we will dive into the basics of neural networks, building the foundation for deep learning models in time‑series forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 24**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
