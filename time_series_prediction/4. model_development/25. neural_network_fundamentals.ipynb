{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 25: Neural Network Fundamentals**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the biological inspiration and basic architecture of artificial neural networks\n",
    "- Explain the role of perceptrons and activation functions in introducing non\u2011linearity\n",
    "- Build a multi\u2011layer perceptron (MLP) for regression and classification tasks using the NEPSE dataset\n",
    "- Grasp the concept of backpropagation and how neural networks learn from data\n",
    "- Choose appropriate loss functions for different prediction problems\n",
    "- Implement common optimization algorithms (SGD, Adam, RMSprop) and understand their differences\n",
    "- Apply regularization techniques (dropout, batch normalization, early stopping) to prevent overfitting\n",
    "- Train a neural network effectively, including data preparation, scaling, and monitoring\n",
    "- Recognize and avoid common pitfalls when applying neural networks to time\u2011series data\n",
    "\n",
    "---\n",
    "\n",
    "## **25.1 Introduction to Neural Networks**\n",
    "\n",
    "Artificial neural networks (ANNs) are a class of machine learning models inspired by the biological neural networks that constitute animal brains. They consist of interconnected units called **neurons** (or nodes) organized in layers. Each connection has a weight that is adjusted during learning. Neural networks are universal function approximators \u2013 given enough neurons and layers, they can represent any continuous function.\n",
    "\n",
    "In the context of the NEPSE prediction system, neural networks can capture complex, non\u2011linear relationships between engineered features (lagged returns, volume, technical indicators) and future returns or price direction. They often outperform traditional models when sufficient data is available, but they require careful tuning and regularization to avoid overfitting, especially with financial time\u2011series.\n",
    "\n",
    "### **25.1.1 The Building Blocks**\n",
    "\n",
    "A neural network is composed of:\n",
    "\n",
    "- **Input layer:** Each neuron corresponds to a feature (e.g., `Return_Lag1`, `RSI`, etc.).\n",
    "- **Hidden layers:** One or more layers between input and output, where the network learns representations.\n",
    "- **Output layer:** Produces the final prediction (e.g., a single neuron for regression, or multiple for classification).\n",
    "\n",
    "Each neuron computes a weighted sum of its inputs, adds a bias, and passes the result through an **activation function** to introduce non\u2011linearity.\n",
    "\n",
    "---\n",
    "\n",
    "## **25.2 Perceptrons and Activation Functions**\n",
    "\n",
    "### **25.2.1 The Perceptron**\n",
    "\n",
    "The perceptron is the simplest form of a neural network \u2013 a single neuron. It takes input vector `x`, multiplies by weights `w`, adds bias `b`, and outputs:\n",
    "\n",
    "`output = activation(w\u00b7x + b)`\n",
    "\n",
    "For binary classification, the activation is often a step function (e.g., output 1 if `w\u00b7x + b > 0`, else 0). However, step functions are not differentiable, so modern networks use smooth activation functions.\n",
    "\n",
    "### **25.2.2 Common Activation Functions**\n",
    "\n",
    "- **Sigmoid:** `\u03c3(z) = 1 / (1 + e\u207b\u1dbb)`. Outputs between 0 and 1. Used for binary classification output. Suffers from vanishing gradient for large |z|.\n",
    "- **Tanh:** `tanh(z) = (e\u1dbb - e\u207b\u1dbb) / (e\u1dbb + e\u207b\u1dbb)`. Outputs between -1 and 1. Zero\u2011centered, but still saturates.\n",
    "- **ReLU (Rectified Linear Unit):** `ReLU(z) = max(0, z)`. Most popular for hidden layers. Non\u2011saturating, computationally efficient. Can cause dead neurons (if always negative, gradient is zero).\n",
    "- **Leaky ReLU:** `max(\u03b1z, z)` with small \u03b1 (e.g., 0.01) to allow gradient for negative inputs.\n",
    "- **Softmax:** Used in output layer for multi\u2011class classification; converts logits to probabilities summing to 1.\n",
    "\n",
    "For the NEPSE prediction, we typically use ReLU for hidden layers. For regression output (predicting return), we use a linear activation (no activation). For binary classification (direction), we use sigmoid.\n",
    "\n",
    "---\n",
    "\n",
    "## **25.3 Multi\u2011Layer Perceptrons (MLP)**\n",
    "\n",
    "An MLP is a feedforward neural network with one or more hidden layers. Each layer is fully connected to the next. The network learns by adjusting weights to minimize a loss function.\n",
    "\n",
    "### **25.3.1 Building an MLP for NEPSE Return Prediction**\n",
    "\n",
    "We'll use TensorFlow/Keras to build a simple MLP for regression (predicting next day's return). We'll reuse the same feature set as in previous chapters.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load and prepare NEPSE data (as before)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Use a single symbol for simplicity\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Create features and target\n",
    "df_stock['Return'] = df_stock['Close'].pct_change() * 100\n",
    "df_stock['Return_Lag1'] = df_stock['Return'].shift(1)\n",
    "df_stock['Return_Lag2'] = df_stock['Return'].shift(2)\n",
    "df_stock['Volume_Lag1'] = df_stock['Vol'].shift(1)\n",
    "df_stock['MA_5'] = df_stock['Close'].rolling(5).mean()\n",
    "df_stock['Volatility_5'] = df_stock['Return'].rolling(5).std()\n",
    "# RSI\n",
    "delta = df_stock['Close'].diff()\n",
    "gain = delta.where(delta > 0, 0)\n",
    "loss = -delta.where(delta < 0, 0)\n",
    "avg_gain = gain.rolling(14).mean()\n",
    "avg_loss = loss.rolling(14).mean()\n",
    "rs = avg_gain / avg_loss\n",
    "df_stock['RSI'] = 100 - (100 / (1 + rs))\n",
    "\n",
    "# Target: next day's return\n",
    "df_stock['Target'] = df_stock['Return'].shift(-1)\n",
    "\n",
    "# Drop NaN\n",
    "df_stock = df_stock.dropna()\n",
    "\n",
    "# Feature columns\n",
    "feature_cols = ['Return_Lag1', 'Return_Lag2', 'Volume_Lag1', 'MA_5', 'Volatility_5', 'RSI']\n",
    "X = df_stock[feature_cols].values\n",
    "y = df_stock['Target'].values\n",
    "\n",
    "# Temporal split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Scale features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Build MLP model\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)  # linear activation for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We define a sequential model with two hidden layers (64 and 32 neurons) using ReLU activation.\n",
    "- The output layer has a single neuron with linear activation (default), suitable for regression.\n",
    "- We compile with Adam optimizer, mean squared error loss, and track mean absolute error as a metric.\n",
    "- `model.summary()` prints the architecture, showing the number of parameters.\n",
    "\n",
    "### **25.3.2 Training the Model**\n",
    "\n",
    "We'll train for a fixed number of epochs, monitoring validation loss to avoid overfitting.\n",
    "\n",
    "```python\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,  # use 20% of training for validation\n",
    "    epochs=100,\n",
    "    batch_size=32,\n",
    "    verbose=0  # set to 1 to see progress\n",
    ")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(history.history['mae'], label='train_mae')\n",
    "plt.plot(history.history['val_mae'], label='val_mae')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('MAE')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Evaluate on test set\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test MSE: {test_loss:.4f}, Test MAE: {test_mae:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Test RMSE: {rmse:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `validation_split=0.2` automatically holds out the last 20% of training data for validation. In time\u2011series, we should ensure this split is temporal; Keras's `validation_split` takes the last part, which is correct for time order (since our data is already ordered). However, it's better to manually split to control exactly.\n",
    "- Training for 100 epochs; we watch validation loss to see if it plateaus or starts increasing (overfitting).\n",
    "- The history plot shows learning curves.\n",
    "- Test evaluation gives final performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **25.4 Backpropagation**\n",
    "\n",
    "Backpropagation is the algorithm used to train neural networks. It computes the gradient of the loss function with respect to each weight by applying the chain rule of calculus, propagating errors backward from the output layer to the input layer. These gradients are then used by an optimizer to update weights.\n",
    "\n",
    "While we don't implement backpropagation manually (frameworks like TensorFlow do it automatically), understanding it helps in debugging and choosing hyperparameters.\n",
    "\n",
    "**Key steps:**\n",
    "\n",
    "1. **Forward pass:** Compute predictions and loss.\n",
    "2. **Backward pass:** Compute gradients of loss w.r.t. each weight.\n",
    "3. **Update weights:** Adjust weights in the opposite direction of the gradient (gradient descent).\n",
    "\n",
    "---\n",
    "\n",
    "## **25.5 Loss Functions**\n",
    "\n",
    "The loss function measures how well the model's predictions match the true targets. Choosing the right loss is crucial.\n",
    "\n",
    "### **25.5.1 Regression Losses**\n",
    "\n",
    "- **Mean Squared Error (MSE):** `(y_true - y_pred)\u00b2`. Sensitive to outliers.\n",
    "- **Mean Absolute Error (MAE):** `|y_true - y_pred|`. More robust to outliers.\n",
    "- **Huber loss:** Combination of MSE and MAE; quadratic for small errors, linear for large errors. Less sensitive to outliers than MSE.\n",
    "\n",
    "For NEPSE return prediction, MSE is common, but Huber may be better if there are extreme returns.\n",
    "\n",
    "### **25.5.2 Classification Losses**\n",
    "\n",
    "- **Binary Crossentropy:** For binary classification (direction). Measures the difference between true labels (0/1) and predicted probabilities.\n",
    "- **Categorical Crossentropy:** For multi\u2011class classification.\n",
    "\n",
    "For direction prediction, we use binary crossentropy.\n",
    "\n",
    "### **25.5.3 Example: Binary Classification with MLP**\n",
    "\n",
    "```python\n",
    "# Prepare binary target\n",
    "y_binary = (df_stock['Target'] > 0).astype(int).values\n",
    "y_train_bin, y_test_bin = y_binary[:split_idx], y_binary[split_idx:]\n",
    "\n",
    "# Build classifier\n",
    "model_clf = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1, activation='sigmoid')  # sigmoid for probability\n",
    "])\n",
    "\n",
    "model_clf.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "history_clf = model_clf.fit(\n",
    "    X_train_scaled, y_train_bin,\n",
    "    validation_split=0.2,\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss_clf, test_acc = model_clf.evaluate(X_test_scaled, y_test_bin, verbose=0)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **25.6 Optimization Algorithms**\n",
    "\n",
    "Optimizers update the weights to minimize the loss. They differ in how they use gradients and adapt learning rates.\n",
    "\n",
    "### **25.6.1 Stochastic Gradient Descent (SGD)**\n",
    "\n",
    "Basic SGD updates weights using the gradient of the loss on a mini\u2011batch:  \n",
    "`w = w - \u03b7 * \u2207w`\n",
    "\n",
    "where `\u03b7` is the learning rate. SGD can be slow and may oscillate.\n",
    "\n",
    "### **25.6.2 Adam (Adaptive Moment Estimation)**\n",
    "\n",
    "Adam combines momentum and adaptive learning rates. It maintains moving averages of gradients and squared gradients. It often works well out\u2011of\u2011the\u2011box and is a good default choice.\n",
    "\n",
    "### **25.6.3 RMSprop**\n",
    "\n",
    "Similar to Adam but without the momentum component. Also adapts learning rates per parameter.\n",
    "\n",
    "In Keras, we can easily use different optimizers:\n",
    "\n",
    "```python\n",
    "# SGD\n",
    "model.compile(optimizer=keras.optimizers.SGD(learning_rate=0.01), loss='mse')\n",
    "\n",
    "# Adam (default lr=0.001)\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# RMSprop\n",
    "model.compile(optimizer=keras.optimizers.RMSprop(learning_rate=0.001), loss='mse')\n",
    "```\n",
    "\n",
    "### **25.6.4 Learning Rate Scheduling**\n",
    "\n",
    "Adjusting the learning rate during training can improve convergence. Common schedules:\n",
    "\n",
    "- Step decay: reduce by factor every few epochs.\n",
    "- Exponential decay.\n",
    "- Reduce on plateau: reduce when validation loss stagnates.\n",
    "\n",
    "```python\n",
    "# Example: reduce learning rate when validation loss plateaus\n",
    "reduce_lr = keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6\n",
    ")\n",
    "\n",
    "history = model.fit(..., callbacks=[reduce_lr])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **25.7 Regularization in Neural Networks**\n",
    "\n",
    "Neural networks are prone to overfitting, especially with small datasets. Regularization techniques help.\n",
    "\n",
    "### **25.7.1 Dropout**\n",
    "\n",
    "During training, randomly drops a fraction of neurons (setting their output to zero) each forward pass. This prevents co\u2011adaptation and acts as an ensemble of networks.\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(n_features,)),\n",
    "    layers.Dropout(0.3),  # 30% dropout\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### **25.7.2 Batch Normalization**\n",
    "\n",
    "Normalizes the inputs to each layer, stabilizing training and allowing higher learning rates. It also has a slight regularizing effect.\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, input_shape=(n_features,)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(32),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Activation('relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### **25.7.3 Early Stopping**\n",
    "\n",
    "Stop training when validation performance stops improving, preventing overfitting.\n",
    "\n",
    "```python\n",
    "early_stop = keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss', patience=10, restore_best_weights=True\n",
    ")\n",
    "\n",
    "history = model.fit(..., callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "### **25.7.4 L1/L2 Weight Regularization**\n",
    "\n",
    "Add a penalty to the loss for large weights, similar to Lasso/Ridge.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import regularizers\n",
    "\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.001), input_shape=(n_features,)),\n",
    "    layers.Dense(32, activation='relu', kernel_regularizer=regularizers.l2(0.001)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **25.8 Training Neural Networks**\n",
    "\n",
    "### **25.8.1 Data Preparation**\n",
    "\n",
    "- Scale features to zero mean and unit variance (or to [0,1] range). Neural networks benefit from normalized inputs.\n",
    "- For time\u2011series, maintain temporal order; do not shuffle across time.\n",
    "- Create a validation set that is temporally after the training set.\n",
    "\n",
    "### **25.8.2 Choosing Hyperparameters**\n",
    "\n",
    "- **Number of layers and neurons:** Start simple (1\u20112 hidden layers) and increase if underfitting. Too many neurons can overfit.\n",
    "- **Batch size:** Typical values 32, 64, 128. Smaller batches introduce noise, which can help generalization; larger batches are more stable.\n",
    "- **Learning rate:** Critical; too high causes divergence, too low leads to slow convergence. Often tuned via trial or learning rate finder.\n",
    "- **Epochs:** Use early stopping to determine automatically.\n",
    "\n",
    "### **25.8.3 Monitoring Training**\n",
    "\n",
    "Always monitor both training and validation loss. If validation loss starts increasing while training loss continues decreasing, you're overfitting. Apply regularization or stop earlier.\n",
    "\n",
    "### **25.8.4 Example with All Techniques**\n",
    "\n",
    "```python\n",
    "# Build a regularized MLP\n",
    "model = keras.Sequential([\n",
    "    layers.Dense(128, activation='relu', input_shape=(X_train_scaled.shape[1],)),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(64, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True),\n",
    "    keras.callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "]\n",
    "\n",
    "history = model.fit(\n",
    "    X_train_scaled, y_train,\n",
    "    validation_split=0.2,\n",
    "    epochs=200,\n",
    "    batch_size=32,\n",
    "    callbacks=callbacks,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Evaluate\n",
    "test_loss, test_mae = model.evaluate(X_test_scaled, y_test, verbose=0)\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **25.9 Common Pitfalls**\n",
    "\n",
    "### **25.9.1 Look\u2011Ahead Bias**\n",
    "\n",
    "When using features that require future information (e.g., using tomorrow's high in today's features), the model will appear accurate but fail in production. Ensure all features are computable at prediction time (use `shift()` and rolling windows that exclude the current point).\n",
    "\n",
    "### **25.9.2 Data Leakage**\n",
    "\n",
    "Scaling on the entire dataset before splitting leaks information. Always fit scaler on training set only.\n",
    "\n",
    "### **25.9.3 Overfitting to Noise**\n",
    "\n",
    "Financial data is noisy. A complex network can easily memorize noise. Use strong regularization, simple architectures, and monitor validation loss.\n",
    "\n",
    "### **25.9.4 Non\u2011Stationarity**\n",
    "\n",
    "Financial markets change over time. A model trained on old data may not generalize to new regimes. Use walk\u2011forward validation and consider retraining frequently.\n",
    "\n",
    "### **25.9.5 Vanishing/Exploding Gradients**\n",
    "\n",
    "Deep networks can suffer from vanishing or exploding gradients, especially with sigmoid/tanh. Use ReLU activations, batch normalization, and proper weight initialization (e.g., He initialization).\n",
    "\n",
    "### **25.9.6 Insufficient Data**\n",
    "\n",
    "Neural networks typically require large datasets. For a single stock, we may have only a few thousand rows. Consider using data from multiple stocks (pooling) or transfer learning.\n",
    "\n",
    "### **25.9.7 Hyperparameter Tuning on Test Set**\n",
    "\n",
    "Never tune hyperparameters based on test set performance. Use a separate validation set or time\u2011series CV. The test set should be used only once, at the end.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we covered the fundamentals of neural networks, with applications to the NEPSE prediction system.\n",
    "\n",
    "- **Neural network architecture:** input, hidden, output layers; neurons and activation functions.\n",
    "- **MLP for regression and classification:** we built models to predict next\u2011day returns and direction.\n",
    "- **Backpropagation** is the learning algorithm; frameworks handle it automatically.\n",
    "- **Loss functions:** MSE/MAE for regression, binary crossentropy for classification.\n",
    "- **Optimizers:** SGD, Adam, RMSprop; Adam is a good default.\n",
    "- **Regularization:** dropout, batch normalization, early stopping, weight decay prevent overfitting.\n",
    "- **Training best practices:** scale data, monitor validation loss, use callbacks.\n",
    "- **Common pitfalls:** look\u2011ahead bias, leakage, overfitting, non\u2011stationarity.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Start with a simple MLP (1\u20112 hidden layers) as a baseline.\n",
    "- Use ReLU activation, Adam optimizer, and early stopping.\n",
    "- Always scale features using training set statistics.\n",
    "- Regularize heavily given the noisy nature of financial returns.\n",
    "- Validate with temporal splits and walk\u2011forward to ensure robustness.\n",
    "- Compare performance with simpler models (linear, tree\u2011based) to justify complexity.\n",
    "\n",
    "In the next chapter, **Chapter 26: Recurrent Neural Networks**, we will explore networks designed for sequential data, such as LSTMs and GRUs, which are particularly suited for time\u2011series forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 25**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='24. support_vector_machines.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='26. recurrent_neural_networks.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}