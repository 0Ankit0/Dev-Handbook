{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 29: Specialized Time‑Series Architectures\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the motivation behind specialized architectures designed for time‑series forecasting\n",
    "- Describe the N‑BEATS architecture and how it uses backward and forward residual connections\n",
    "- Implement a simplified version of N‑BEATS for univariate time‑series prediction\n",
    "- Explain how DeepAR produces probabilistic forecasts using autoregressive RNNs with negative binomial likelihood\n",
    "- Build a probabilistic forecasting model using DeepAR‑like principles with TensorFlow Probability\n",
    "- Grasp the key components of the Temporal Fusion Transformer (TFT) – an attention‑based model with explainability\n",
    "- Recognize when neural hierarchical interpolation (N-HiTS) is appropriate for long‑horizon forecasting\n",
    "- Apply Gaussian Processes to time‑series and understand their uncertainty estimates\n",
    "- Use state space models for interpretable trend‑seasonal decomposition\n",
    "- Combine multiple architectures into hybrid models for improved performance\n",
    "- Develop a systematic approach to architecture selection based on data characteristics and forecasting requirements\n",
    "\n",
    "---\n",
    "\n",
    "## **29.1 Introduction to Specialized Time‑Series Architectures**\n",
    "\n",
    "While generic architectures like LSTMs, CNNs, and Transformers can be applied to time‑series, researchers have developed specialized models that incorporate inductive biases specific to temporal data. These models often achieve state‑of‑the‑art performance on forecasting benchmarks and offer additional benefits such as interpretability, probabilistic outputs, or efficient handling of long sequences.\n",
    "\n",
    "For the NEPSE prediction system, understanding these architectures helps us choose the right tool for the job. For example, if we need not just point forecasts but also prediction intervals (uncertainty), DeepAR or Gaussian Processes are suitable. If we require interpretability to explain predictions to stakeholders, Temporal Fusion Transformers provide attention‑based explanations. If we are forecasting many related time series (e.g., all stocks in the NEPSE index), N‑BEATS or DeepAR can be adapted for multivariate or multi‑series settings.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.2 N‑BEATS (Neural Basis Expansion Analysis for Time‑Series Forecasting)**\n",
    "\n",
    "N‑BEATS, introduced by Oreshkin et al. (2019), is a pure deep learning architecture that does not rely on time‑specific components like RNNs or convolutions. Instead, it uses a stack of fully connected blocks with backward and forward residual connections. Each block consists of two parts: a **backcast** (the part of the input that is \"removed\" to focus on residuals) and a **forecast** (the contribution of that block to the final prediction). The outputs of all blocks are summed to produce the final forecast.\n",
    "\n",
    "### **29.2.1 Architecture Overview**\n",
    "\n",
    "- The input is a window of past observations of length `L`.\n",
    "- The network has multiple stacks, each containing several blocks.\n",
    "- Each block has a fully connected network that outputs two vectors: `backcast` (same length as input) and `forecast` (length equal to forecast horizon `H`).\n",
    "- The backcast is subtracted from the input before passing to the next block, allowing subsequent blocks to model the residual.\n",
    "- The forecasts from all blocks are summed to obtain the overall prediction.\n",
    "\n",
    "This design forces the model to learn a hierarchical decomposition of the time series, making it interpretable and often very accurate.\n",
    "\n",
    "### **29.2.2 Implementing a Simplified N‑BEATS in Keras**\n",
    "\n",
    "We'll implement a basic N‑BEATS model for univariate forecasting (predict next 5 days from past 20 days). This is a simplified version; the original uses multiple stacks and specialized basis functions.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NBeatsBlock(layers.Layer):\n",
    "    def __init__(self, units, theta_dim, backcast_length, forecast_length, **kwargs):\n",
    "        super().__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.theta_dim = theta_dim\n",
    "        self.backcast_length = backcast_length\n",
    "        self.forecast_length = forecast_length\n",
    "        \n",
    "        # Fully connected layers to produce theta\n",
    "        self.fc1 = layers.Dense(units, activation='relu')\n",
    "        self.fc2 = layers.Dense(units, activation='relu')\n",
    "        self.fc3 = layers.Dense(units, activation='relu')\n",
    "        self.theta = layers.Dense(theta_dim, activation='linear')\n",
    "        \n",
    "        # Basis layers: map theta to backcast and forecast\n",
    "        # In the original, these are fixed basis functions (trend, seasonality)\n",
    "        # Here we use learnable linear projections (simple but less interpretable)\n",
    "        self.backcast_basis = layers.Dense(backcast_length, use_bias=False)\n",
    "        self.forecast_basis = layers.Dense(forecast_length, use_bias=False)\n",
    "        \n",
    "    def call(self, inputs):\n",
    "        # inputs: (batch, backcast_length)\n",
    "        x = self.fc1(inputs)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        theta = self.theta(x)  # (batch, theta_dim)\n",
    "        backcast = self.backcast_basis(theta)\n",
    "        forecast = self.forecast_basis(theta)\n",
    "        return backcast, forecast\n",
    "\n",
    "def build_nbeats(backcast_length, forecast_length, stack_sizes=[3, 3], units=64, theta_dim=16):\n",
    "    \"\"\"\n",
    "    Build a simple N-BEATS model.\n",
    "    stack_sizes: number of blocks per stack (list of ints)\n",
    "    \"\"\"\n",
    "    input_layer = layers.Input(shape=(backcast_length,))\n",
    "    x = input_layer\n",
    "    forecast_sum = 0\n",
    "    \n",
    "    for i, n_blocks in enumerate(stack_sizes):\n",
    "        for _ in range(n_blocks):\n",
    "            block = NBeatsBlock(units, theta_dim, backcast_length, forecast_length)\n",
    "            backcast, forecast = block(x)\n",
    "            x = layers.Subtract()([x, backcast])  # residual connection\n",
    "            forecast_sum = layers.Add()([forecast_sum, forecast])\n",
    "    \n",
    "    model = keras.Model(inputs=input_layer, outputs=forecast_sum)\n",
    "    return model\n",
    "\n",
    "# Example usage\n",
    "backcast_len = 20\n",
    "forecast_len = 5\n",
    "model_nbeats = build_nbeats(backcast_len, forecast_len, stack_sizes=[2, 2], units=32, theta_dim=8)\n",
    "model_nbeats.compile(optimizer='adam', loss='mse')\n",
    "model_nbeats.summary()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `NBeatsBlock` is a custom layer that computes theta from the input using three dense layers, then projects theta to backcast and forecast via learnable linear layers (in the original, these are often fixed basis functions like trend polynomials or Fourier terms). Our version uses learnable projections, which is simpler but still effective.\n",
    "- The main model loops through stacks and blocks, subtracting each block's backcast from the input, and summing the forecasts.\n",
    "- The final output is the sum of all block forecasts.\n",
    "\n",
    "### **29.2.3 Training on NEPSE Data**\n",
    "\n",
    "We'll prepare data for multi‑step forecasting (predict next 5 returns from past 20 returns).\n",
    "\n",
    "```python\n",
    "# Load and prepare NEPSE data (as before)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "df_stock['Return'] = df_stock['Close'].pct_change() * 100\n",
    "df_stock = df_stock.dropna(subset=['Return'])\n",
    "\n",
    "# Create sequences for multi-step forecasting\n",
    "def create_multi_output_sequences(data, backcast_len, forecast_len):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - backcast_len - forecast_len + 1):\n",
    "        X.append(data[i:i+backcast_len])\n",
    "        y.append(data[i+backcast_len:i+backcast_len+forecast_len])\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "backcast_len = 20\n",
    "forecast_len = 5\n",
    "data = df_stock['Return'].values.reshape(-1, 1)\n",
    "X, y = create_multi_output_sequences(data, backcast_len, forecast_len)\n",
    "X = X.squeeze(axis=-1)  # (samples, backcast_len)\n",
    "y = y.squeeze(axis=-1)  # (samples, forecast_len)\n",
    "\n",
    "# Temporal split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Scale (fit on training)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler_X = StandardScaler()\n",
    "scaler_y = StandardScaler()\n",
    "X_train_scaled = scaler_X.fit_transform(X_train)\n",
    "X_test_scaled = scaler_X.transform(X_test)\n",
    "y_train_scaled = scaler_y.fit_transform(y_train)\n",
    "y_test_scaled = scaler_y.transform(y_test)\n",
    "\n",
    "# Build and train model\n",
    "model_nbeats = build_nbeats(backcast_len, forecast_len, stack_sizes=[2, 2], units=32, theta_dim=8)\n",
    "model_nbeats.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "early_stop = keras.callbacks.EarlyStopping(monitor='loss', patience=5, restore_best_weights=True)\n",
    "model_nbeats.fit(X_train_scaled, y_train_scaled, epochs=50, batch_size=32, callbacks=[early_stop], verbose=0)\n",
    "\n",
    "# Predict and inverse transform\n",
    "y_pred_scaled = model_nbeats.predict(X_test_scaled)\n",
    "y_pred = scaler_y.inverse_transform(y_pred_scaled)\n",
    "rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "print(f\"N-BEATS test RMSE (averaged over 5 steps): {rmse:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We reshape the data to have multiple output steps.\n",
    "- The model is trained to predict the next 5 returns. Scaling is applied to both input and output.\n",
    "- The RMSE is averaged across the 5 forecast steps; we could also compute per‑step metrics.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.3 DeepAR**\n",
    "\n",
    "DeepAR (Salinas et al., 2020) is a probabilistic forecasting model based on autoregressive RNNs. It uses a recurrent neural network (LSTM or GRU) to model the conditional distribution of each time step given past values and covariates. Instead of predicting a point value, DeepAR outputs parameters of a distribution (e.g., mean and variance for Gaussian, or mean and dispersion for negative binomial for count data). It is trained by maximizing the likelihood.\n",
    "\n",
    "Key features:\n",
    "\n",
    "- Handles multiple related time series (e.g., many stocks) by learning shared patterns.\n",
    "- Produces probabilistic forecasts (prediction intervals).\n",
    "- Incorporates covariates (e.g., day of week, month) easily.\n",
    "\n",
    "For the NEPSE system, DeepAR could be used to forecast the entire distribution of future returns, which is valuable for risk management.\n",
    "\n",
    "### **29.3.1 DeepAR Conceptual Overview**\n",
    "\n",
    "- Input: at each time step `t`, the model receives the previous target value `z_{t-1}` (if available) and covariates `x_t`.\n",
    "- The RNN updates its state.\n",
    "- At prediction time, the model is used autoregressively: it samples from the predicted distribution at each step and feeds the sample back as input for the next step.\n",
    "\n",
    "### **29.3.2 Implementing a Simplified DeepAR‑like Model in TensorFlow**\n",
    "\n",
    "We'll build a model that predicts the parameters of a Gaussian distribution for each future step. We'll use a many‑to‑many RNN with a custom loss function (negative log‑likelihood).\n",
    "\n",
    "```python\n",
    "import tensorflow_probability as tfp\n",
    "\n",
    "def gaussian_nll(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Negative log-likelihood for Gaussian distribution.\n",
    "    y_pred: (batch, horizon, 2) where last dim = [mean, log_std]\n",
    "    \"\"\"\n",
    "    mean = y_pred[..., 0]\n",
    "    log_std = y_pred[..., 1]\n",
    "    std = tf.exp(log_std)\n",
    "    return -tf.reduce_mean(tfp.distributions.Normal(mean, std).log_prob(y_true))\n",
    "\n",
    "def build_deepar_model(backcast_len, forecast_len, n_features=1, lstm_units=32):\n",
    "    \"\"\"\n",
    "    Simplified DeepAR: encoder-decoder with LSTM.\n",
    "    During training, we use teacher forcing (true previous target).\n",
    "    During inference, we would sample and feed back.\n",
    "    \"\"\"\n",
    "    # Encoder: process backcast window\n",
    "    encoder_inputs = layers.Input(shape=(backcast_len, n_features))\n",
    "    encoder_lstm = layers.LSTM(lstm_units, return_state=True)\n",
    "    encoder_outputs, state_h, state_c = encoder_lstm(encoder_inputs)\n",
    "    encoder_states = [state_h, state_c]\n",
    "    \n",
    "    # Decoder: we'll use teacher forcing during training\n",
    "    decoder_inputs = layers.Input(shape=(forecast_len, n_features))\n",
    "    decoder_lstm = layers.LSTM(lstm_units, return_sequences=True)\n",
    "    decoder_outputs = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "    \n",
    "    # Output layer: predict mean and log_std for each time step\n",
    "    outputs = layers.TimeDistributed(layers.Dense(2))(decoder_outputs)\n",
    "    \n",
    "    model = keras.Model(inputs=[encoder_inputs, decoder_inputs], outputs=outputs)\n",
    "    model.compile(optimizer='adam', loss=gaussian_nll)\n",
    "    return model\n",
    "\n",
    "# Prepare data for teacher forcing: decoder inputs are the target sequence shifted\n",
    "# In training, we use the actual future values as decoder inputs (teacher forcing)\n",
    "def prepare_deepar_data(data, backcast_len, forecast_len):\n",
    "    X_encoder = []\n",
    "    X_decoder = []\n",
    "    y = []\n",
    "    for i in range(len(data) - backcast_len - forecast_len):\n",
    "        X_encoder.append(data[i:i+backcast_len])\n",
    "        # decoder input is the target sequence (but we need values for each step)\n",
    "        # In DeepAR, decoder input at step t is the target at t-1 (starting with the last encoder value)\n",
    "        # For simplicity, we'll use the target sequence as decoder input (teacher forcing)\n",
    "        X_decoder.append(data[i+backcast_len:i+backcast_len+forecast_len])\n",
    "        y.append(data[i+backcast_len:i+backcast_len+forecast_len])\n",
    "    return np.array(X_encoder), np.array(X_decoder), np.array(y)\n",
    "\n",
    "# Prepare data\n",
    "data_vals = df_stock['Return'].values.reshape(-1, 1)\n",
    "X_enc, X_dec, y_multi = prepare_deepar_data(data_vals, backcast_len=20, forecast_len=5)\n",
    "\n",
    "# Temporal split\n",
    "split = int(len(X_enc) * 0.8)\n",
    "X_enc_train, X_enc_test = X_enc[:split], X_enc[split:]\n",
    "X_dec_train, X_dec_test = X_dec[:split], X_dec[split:]\n",
    "y_train, y_test = y_multi[:split], y_multi[split:]\n",
    "\n",
    "# Build and train\n",
    "model_deepar = build_deepar_model(backcast_len=20, forecast_len=5, lstm_units=32)\n",
    "model_deepar.fit([X_enc_train, X_dec_train], y_train, epochs=30, batch_size=32, validation_split=0.1, verbose=0)\n",
    "\n",
    "# Prediction (during inference, we need to sample autoregressively; here we'll just use teacher forcing for evaluation)\n",
    "y_pred_params = model_deepar.predict([X_enc_test, X_dec_test])\n",
    "mean_pred = y_pred_params[..., 0]\n",
    "std_pred = tf.exp(y_pred_params[..., 1]).numpy()\n",
    "rmse = np.sqrt(np.mean((mean_pred - y_test)**2))\n",
    "print(f\"DeepAR-like RMSE: {rmse:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The model uses an LSTM encoder to process the backcast window, and an LSTM decoder that receives the true future values during training (teacher forcing). The decoder output is passed through a `TimeDistributed` dense layer with 2 units (mean and log standard deviation).\n",
    "- The loss function is the negative log‑likelihood of a Gaussian distribution with predicted mean and standard deviation. This encourages the model to output both a point forecast and its uncertainty.\n",
    "- During inference, we would need to run the model autoregressively: start with the last observed value, sample from the predicted distribution, feed the sample as next input, and repeat. Our evaluation above uses teacher forcing (feeding true values) which gives an optimistic estimate. A proper evaluation would implement autoregressive sampling.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.4 Temporal Fusion Transformers (TFT)**\n",
    "\n",
    "Temporal Fusion Transformers (Lim et al., 2021) combine LSTM layers with attention mechanisms to provide interpretable and accurate forecasts. Key components:\n",
    "\n",
    "- **Variable selection network:** Chooses relevant input features at each time step.\n",
    "- **LSTM encoder‑decoder:** Processes the sequence.\n",
    "- **Multi‑head attention:** Captures long‑term dependencies.\n",
    "- **Interpretable outputs:** Attention weights and variable selection provide insight into predictions.\n",
    "\n",
    "TFT is designed for multi‑horizon forecasting with exogenous variables and can handle multiple time series. For NEPSE, we could use it to forecast returns using past returns, volume, and other covariates, while also obtaining explanations of which features matter most.\n",
    "\n",
    "Implementing TFT from scratch is complex; we can use existing libraries like `pytorch-forecasting` (PyTorch) or `darts` (which has a TFT implementation). However, for demonstration, we'll outline the structure and show how to use a high‑level library.\n",
    "\n",
    "```python\n",
    "# Example using darts (if installed)\n",
    "from darts.models import TFTModel\n",
    "from darts import TimeSeries\n",
    "from darts.dataprocessing.transformers import Scaler\n",
    "\n",
    "# Convert to darts TimeSeries\n",
    "series = TimeSeries.from_dataframe(df_stock, 'Date', 'Return')\n",
    "\n",
    "# Scale\n",
    "scaler = Scaler()\n",
    "series_scaled = scaler.fit_transform(series)\n",
    "\n",
    "# Define TFT model\n",
    "tft = TFTModel(\n",
    "    input_chunk_length=20,\n",
    "    output_chunk_length=5,\n",
    "    hidden_size=32,\n",
    "    lstm_layers=2,\n",
    "    num_attention_heads=4,\n",
    "    dropout=0.1,\n",
    "    batch_size=32,\n",
    "    n_epochs=50,\n",
    "    add_relative_index=False,  # no need if we have datetime\n",
    "    add_encoders={'cyclic': {'future': ['month', 'dayofweek']}},  # example covariates\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Train (using historical data)\n",
    "tft.fit(series_scaled, val_series=series_scaled[-100:])  # simple validation\n",
    "\n",
    "# Forecast\n",
    "forecast = tft.predict(n=5)\n",
    "forecast = scaler.inverse_transform(forecast)\n",
    "print(forecast)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `TFTModel` from `darts` encapsulates the full architecture.\n",
    "- We specify input length (20 days) and output length (5 days).\n",
    "- We can add covariates (e.g., month, day of week) via `add_encoders`.\n",
    "- The model trains and then produces a probabilistic forecast.\n",
    "- This is a high‑level example; in practice, we would tune hyperparameters and validate properly.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.5 Neural Hierarchical Interpolation (N‑HiTS)**\n",
    "\n",
    "N‑HiTS (Challu et al., 2023) is an extension of N‑BEATS that introduces hierarchical interpolation to capture patterns at different scales. It uses:\n",
    "\n",
    "- **Hierarchical downsampling:** The input is downsampled at multiple rates to create a pyramid of scales.\n",
    "- **Interpolation:** Predictions at different scales are interpolated back to the original resolution and combined.\n",
    "- **Efficiency:** It can handle very long sequences efficiently.\n",
    "\n",
    "For NEPSE, if we wanted to forecast long horizons (e.g., 100 days), N‑HiTS would be more efficient than a standard Transformer.\n",
    "\n",
    "Implementation is similar to N‑BEATS but with added downsampling blocks. We won't implement from scratch but note its existence.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.6 Gaussian Processes**\n",
    "\n",
    "Gaussian Processes (GPs) are non‑parametric probabilistic models that define a distribution over functions. They are well‑suited for time‑series with limited data because they provide uncertainty estimates and can incorporate prior knowledge through the kernel function. However, they scale poorly (O(n³)) and are rarely used for large datasets.\n",
    "\n",
    "For a small subset of NEPSE data (e.g., one stock with 500 points), a GP could be a good baseline.\n",
    "\n",
    "```python\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, WhiteKernel, Matern\n",
    "\n",
    "# Prepare data (simple: predict next value using lagged values)\n",
    "X_gp = np.arange(len(df_stock)).reshape(-1, 1)  # time index\n",
    "y_gp = df_stock['Return'].values\n",
    "\n",
    "# Use last 80% for training\n",
    "train_size = int(0.8 * len(X_gp))\n",
    "X_train_gp, X_test_gp = X_gp[:train_size], X_gp[train_size:]\n",
    "y_train_gp, y_test_gp = y_gp[:train_size], y_gp[train_size:]\n",
    "\n",
    "# Kernel: RBF + WhiteKernel for noise\n",
    "kernel = 1.0 * RBF(length_scale=1.0) + WhiteKernel(noise_level=0.1)\n",
    "gp = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10)\n",
    "gp.fit(X_train_gp, y_train_gp)\n",
    "\n",
    "# Predict and get uncertainty\n",
    "y_pred_gp, sigma = gp.predict(X_test_gp, return_std=True)\n",
    "rmse_gp = np.sqrt(np.mean((y_pred_gp - y_test_gp)**2))\n",
    "print(f\"GP RMSE: {rmse_gp:.4f}\")\n",
    "print(f\"Uncertainty (avg std): {sigma.mean():.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We use a simple RBF kernel to model smoothness, plus a white noise kernel to capture observation noise.\n",
    "- The GP provides both point predictions and uncertainty (standard deviation).\n",
    "- This model assumes the time index is the only input; we could also include lagged features, but then the input dimension increases, making GP slower.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.7 State Space Models**\n",
    "\n",
    "State space models (SSMs) are a classical approach that models a time series as the combination of a latent state evolving over time and an observation model. They are highly interpretable and can incorporate trends, seasonality, and regression effects. The `statsmodels` library provides tools like `UnobservedComponents` for this.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "\n",
    "# Fit a local linear trend model\n",
    "ssm = UnobservedComponents(y_train_gp, level='local linear trend', seasonal=5)\n",
    "ssm_res = ssm.fit()\n",
    "\n",
    "# Forecast next 5 steps\n",
    "forecast_ssm = ssm_res.forecast(5)\n",
    "print(forecast_ssm)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The model decomposes the series into level, trend, and seasonal components.\n",
    "- It is interpretable and often works well when the data has clear structure.\n",
    "- For NEPSE returns, seasonality may be weak, but the trend component might capture local momentum.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.8 Hybrid Models**\n",
    "\n",
    "Hybrid models combine two or more architectures to leverage their strengths. For example:\n",
    "\n",
    "- **ARIMA + Neural Network:** Use ARIMA to capture linear patterns and a neural network to model non‑linear residuals.\n",
    "- **CNN + LSTM:** Use CNN to extract local features, LSTM to model long‑term dependencies.\n",
    "- **Transformer + LSTM:** Use Transformer for long‑range attention and LSTM for sequential processing.\n",
    "\n",
    "For NEPSE, a hybrid of a statistical model (e.g., ARIMA) and a neural network could be effective. We can implement a simple residual hybrid:\n",
    "\n",
    "```python\n",
    "# Step 1: Fit ARIMA on training returns\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "arima = ARIMA(y_train_gp, order=(1,0,1))\n",
    "arima_fit = arima.fit()\n",
    "arima_pred_train = arima_fit.predict()\n",
    "arima_pred_test = arima_fit.forecast(steps=len(y_test_gp))\n",
    "\n",
    "# Step 2: Train neural network on ARIMA residuals\n",
    "residuals = y_train_gp - arima_pred_train\n",
    "# (prepare features for NN, e.g., lagged returns)\n",
    "# ... train NN to predict residuals\n",
    "\n",
    "# Step 3: Combine forecasts\n",
    "final_pred = arima_pred_test + nn_pred_test\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The ARIMA captures the linear autocorrelation; the NN learns to correct its errors.\n",
    "- This can improve accuracy over either model alone.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.9 Architecture Selection**\n",
    "\n",
    "Choosing the right architecture depends on:\n",
    "\n",
    "- **Data size:** N‑BEATS, DeepAR, and Transformers need more data than ARIMA or GP. For a single stock, simple models may suffice.\n",
    "- **Forecast horizon:** For long horizons, N‑HiTS or Informer are more efficient.\n",
    "- **Interpretability:** TFT and state space models offer explanations; deep learning black boxes do not.\n",
    "- **Uncertainty requirements:** DeepAR, GP, and TFT provide probabilistic forecasts.\n",
    "- **Multiple series:** DeepAR and TFT naturally handle multiple related time series.\n",
    "\n",
    "For the NEPSE system, a pragmatic approach is to start with simple models (ARIMA, ETS) and progress to more complex ones only if they significantly improve validation performance. We should maintain a suite of models and use cross‑validation to select the best.\n",
    "\n",
    "---\n",
    "\n",
    "## **29.10 Chapter Summary**\n",
    "\n",
    "In this chapter, we surveyed specialized time‑series architectures, each with unique strengths:\n",
    "\n",
    "- **N‑BEATS:** A pure deep learning model that decomposes the series via residual blocks; simple and accurate.\n",
    "- **DeepAR:** Probabilistic forecasting with RNNs; handles multiple series and provides uncertainty.\n",
    "- **Temporal Fusion Transformers:** Attention‑based model with interpretability; supports exogenous variables.\n",
    "- **N‑HiTS:** Hierarchical version of N‑BEATS for long sequences.\n",
    "- **Gaussian Processes:** Non‑parametric probabilistic model; good for small datasets.\n",
    "- **State Space Models:** Classical interpretable models; useful as baselines.\n",
    "- **Hybrid Models:** Combine statistical and ML components for potentially improved accuracy.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- For point forecasts of a single stock, N‑BEATS or a simple LSTM may be sufficient.\n",
    "- If you need prediction intervals, try DeepAR or a Gaussian Process.\n",
    "- If interpretability is critical, consider TFT or state space models.\n",
    "- Always start with a simple baseline; only add complexity if it demonstrably helps on out‑of‑sample validation.\n",
    "- Use time‑series cross‑validation to compare architectures fairly.\n",
    "\n",
    "In the next chapter, **Chapter 30: Model Training Best Practices**, we will consolidate all we've learned about training neural networks effectively, covering data preparation, batch size selection, learning rate scheduling, loss function selection, early stopping, checkpointing, mixed precision, distributed training, monitoring, and debugging.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 29**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
