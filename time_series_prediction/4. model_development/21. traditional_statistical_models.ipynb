{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 21: Traditional Statistical Models**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the foundational concepts of classical time‑series models\n",
    "- Explain the difference between autoregressive (AR), moving average (MA), and integrated (I) components\n",
    "- Identify the appropriate model order using autocorrelation (ACF) and partial autocorrelation (PACF) plots\n",
    "- Build, estimate, and diagnose ARIMA and SARIMA models for the NEPSE dataset\n",
    "- Apply exponential smoothing methods for forecasting\n",
    "- Recognize when to use vector autoregression (VAR) for multivariate series\n",
    "- Interpret model diagnostics and ensure residuals are white noise\n",
    "- Compare the strengths and limitations of statistical models versus machine learning approaches\n",
    "- Implement these models in Python using `statsmodels`\n",
    "\n",
    "---\n",
    "\n",
    "## **21.1 Introduction to Statistical Models**\n",
    "\n",
    "Before the rise of machine learning, forecasting time‑series was dominated by statistical models that explicitly describe the stochastic process generating the data. These models are built on probability theory and assume that the time series has a certain structure (e.g., linear dependence on past values, moving average of past errors, seasonality). The most famous family is the **ARIMA** (Autoregressive Integrated Moving Average) model, popularized by Box and Jenkins. Other important models include **Exponential Smoothing** (Holt‑Winters) and **Vector Autoregression** (VAR) for multivariate series.\n",
    "\n",
    "For the NEPSE prediction system, statistical models serve as excellent baselines. They are interpretable, have well‑established theory, and often perform surprisingly well, especially on shorter horizons. Moreover, they force us to think carefully about the data generating process, which helps in understanding the market dynamics.\n",
    "\n",
    "### **21.1.1 When to Use Statistical Models**\n",
    "\n",
    "- When the series exhibits clear linear autocorrelation structure.\n",
    "- When interpretability is important (e.g., for regulatory reporting).\n",
    "- When data is limited; statistical models generally require fewer observations than deep learning.\n",
    "- As a benchmark to compare against more complex models.\n",
    "\n",
    "### **21.1.2 The Box‑Jenkins Methodology**\n",
    "\n",
    "The Box‑Jenkins approach for ARIMA modeling consists of three steps:\n",
    "\n",
    "1. **Identification** – using plots, ACF, and PACF to guess the model order (p, d, q).\n",
    "2. **Estimation** – fitting the model parameters via maximum likelihood or least squares.\n",
    "3. **Diagnostic checking** – verifying that residuals are white noise (no autocorrelation) and that the model is adequate.\n",
    "\n",
    "We will follow this methodology throughout the chapter.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.2 Autoregressive Models (AR)**\n",
    "\n",
    "An autoregressive model of order p, denoted AR(p), assumes that the current value of the series depends linearly on its own previous p values plus a random error.\n",
    "\n",
    "**Mathematically:**  \n",
    "`y_t = c + φ₁ y_{t-1} + φ₂ y_{t-2} + ... + φ_p y_{t-p} + ε_t`  \n",
    "where `ε_t` is white noise.\n",
    "\n",
    "In the context of NEPSE, if we model daily returns, an AR(1) model would say that today's return is a fraction of yesterday's return plus a random shock. This captures momentum or mean‑reversion effects.\n",
    "\n",
    "### **21.2.1 Identifying AR Order with PACF**\n",
    "\n",
    "The partial autocorrelation function (PACF) measures the correlation between `y_t` and `y_{t-k}` after removing the effects of the intermediate lags. For an AR(p) process, the PACF cuts off after lag p. That is, it becomes zero for all lags > p.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# Load NEPSE data for a single symbol (e.g., first symbol)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "symbol = df['Symbol'].unique()[0]  # pick first symbol\n",
    "df_one = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Compute daily returns (more stationary than prices)\n",
    "df_one['Return'] = df_one['Close'].pct_change() * 100  # percentage returns\n",
    "returns = df_one['Return'].dropna()\n",
    "\n",
    "# Plot PACF to identify AR order\n",
    "fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(12, 8))\n",
    "plot_acf(returns, lags=30, ax=ax1, title='ACF of Returns')\n",
    "plot_pacf(returns, lags=30, ax=ax2, title='PACF of Returns')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The ACF of a stationary series typically decays gradually, while the PACF has a sharp cutoff at the AR order. In practice, we look for significant spikes in the PACF up to lag p, after which they become insignificant (within the confidence bands).\n",
    "- For financial returns, often the PACF shows significance only at lag 1, suggesting an AR(1) model. However, this may vary by stock and period.\n",
    "\n",
    "### **21.2.2 Fitting an AR Model in Python**\n",
    "\n",
    "We can fit an AR(p) model using `statsmodels.tsa.ar_model.AutoReg`.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.ar_model import AutoReg\n",
    "\n",
    "# Choose p based on PACF (here we'll try p=1,2,3)\n",
    "for p in [1, 2, 3]:\n",
    "    model = AutoReg(returns, lags=p, trend='c')  # 'c' includes constant\n",
    "    result = model.fit()\n",
    "    print(f\"\\nAR({p}) AIC: {result.aic:.2f}\")\n",
    "    print(result.summary().tables[1])\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `AutoReg` fits the model using OLS. The `trend='c'` adds a constant term.\n",
    "- We compare models using AIC (Akaike Information Criterion); lower is better. AIC balances goodness‑of‑fit with model complexity.\n",
    "- The summary provides coefficient estimates, standard errors, and p‑values. Significant coefficients suggest those lags are useful.\n",
    "\n",
    "### **21.2.3 Forecasting with AR Model**\n",
    "\n",
    "```python\n",
    "# Forecast next 5 days\n",
    "forecast = result.forecast(steps=5)\n",
    "print(\"Forecasted returns (%):\")\n",
    "print(forecast)\n",
    "\n",
    "# If we want to forecast prices, we need to invert the return transformation\n",
    "last_price = df_one['Close'].iloc[-1]\n",
    "forecast_prices = last_price * (1 + forecast.cumsum() / 100)  # assuming returns are percentages\n",
    "print(\"\\nForecasted prices:\")\n",
    "print(forecast_prices)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `forecast` returns point predictions for the next `steps` periods.\n",
    "- To convert return forecasts back to price, we apply the cumulative product. However, this is approximate because we are ignoring compounding effects; a more accurate method would be to simulate.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.3 Moving Average Models (MA)**\n",
    "\n",
    "A moving average model of order q, MA(q), expresses the current value as a linear combination of current and past error terms.\n",
    "\n",
    "**Mathematically:**  \n",
    "`y_t = μ + ε_t + θ₁ ε_{t-1} + θ₂ ε_{t-2} + ... + θ_q ε_{t-q}`  \n",
    "where `ε_t` is white noise.\n",
    "\n",
    "MA models are useful for series that are influenced by random shocks that persist for a few periods (e.g., news events affecting stock prices for a couple of days).\n",
    "\n",
    "### **21.3.1 Identifying MA Order with ACF**\n",
    "\n",
    "For an MA(q) process, the ACF cuts off after lag q, while the PACF decays gradually. This is the opposite of AR.\n",
    "\n",
    "```python\n",
    "# Plot ACF and PACF again; if ACF cuts off at lag q, consider MA(q)\n",
    "# For financial returns, often ACF shows no significant lags (white noise) or a small spike at lag 1.\n",
    "```\n",
    "\n",
    "### **21.3.2 Fitting an MA Model**\n",
    "\n",
    "In `statsmodels`, MA models are part of the ARIMA framework. We can fit a pure MA model using `ARIMA` with p=0, d=0.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# MA(1) model\n",
    "model_ma1 = ARIMA(returns, order=(0,0,1))  # (p,d,q) = (0,0,1)\n",
    "result_ma1 = model_ma1.fit()\n",
    "print(result_ma1.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `order=(0,0,1)` specifies AR=0, differencing=0, MA=1.\n",
    "- The summary includes the MA coefficient (ma.L1) and its significance.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.4 ARMA Models**\n",
    "\n",
    "Combining AR and MA terms gives the ARMA(p,q) model, which is often more parsimonious (fewer parameters) than a pure AR or MA of high order.\n",
    "\n",
    "**Mathematically:**  \n",
    "`y_t = c + φ₁ y_{t-1} + ... + φ_p y_{t-p} + ε_t + θ₁ ε_{t-1} + ... + θ_q ε_{t-q}`\n",
    "\n",
    "For stationary series (no trend), ARMA is appropriate. For financial returns, ARMA(1,1) is a common candidate.\n",
    "\n",
    "### **21.4.1 Fitting ARMA Models**\n",
    "\n",
    "```python\n",
    "# Try ARMA(1,1)\n",
    "model_arma11 = ARIMA(returns, order=(1,0,1))\n",
    "result_arma11 = model_arma11.fit()\n",
    "print(result_arma11.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The model includes both AR(1) and MA(1) terms. Often one of them may be insignificant; we can drop it if so.\n",
    "- Compare AIC with pure AR or MA models to select the best.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.5 ARIMA Models**\n",
    "\n",
    "ARIMA (Autoregressive Integrated Moving Average) extends ARMA to non‑stationary series by including a differencing step. The \"I\" stands for integrated, meaning we difference the series d times to make it stationary.\n",
    "\n",
    "**Notation:** ARIMA(p,d,q)  \n",
    "- p = order of autoregressive part  \n",
    "- d = degree of differencing  \n",
    "- q = order of moving average part\n",
    "\n",
    "For stock prices, which are typically non‑stationary (they trend), we often set d=1 to work with returns (which are stationary). That is, an ARIMA(p,1,q) on prices is equivalent to an ARMA(p,q) on returns.\n",
    "\n",
    "### **21.5.1 Integration (I) and Stationarity**\n",
    "\n",
    "We must check if the series is stationary. For NEPSE closing prices, an Augmented Dickey‑Fuller test will likely fail to reject the null of non‑stationarity. Differencing once usually makes it stationary.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "\n",
    "# Test on prices\n",
    "prices = df_one['Close'].dropna()\n",
    "adf_result = adfuller(prices)\n",
    "print(f\"Prices ADF p-value: {adf_result[1]:.4f}\")\n",
    "\n",
    "# Test on returns\n",
    "adf_result_ret = adfuller(returns.dropna())\n",
    "print(f\"Returns ADF p-value: {adf_result_ret[1]:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- A p‑value < 0.05 indicates stationarity. Prices are usually non‑stationary; returns are stationary.\n",
    "- Therefore, for price forecasting, we would use d=1. For return forecasting, d=0.\n",
    "\n",
    "### **21.5.2 Parameter Selection**\n",
    "\n",
    "Selecting p and q for ARIMA on returns is the same as for ARMA. We can use ACF/PACF of the differenced series (returns) to guide initial choices, then compare AIC.\n",
    "\n",
    "```python\n",
    "# ACF/PACF of returns already plotted; suggest possible p,q\n",
    "# For example, if PACF cuts at 1 and ACF decays, maybe AR(1). If ACF cuts at 1 and PACF decays, maybe MA(1).\n",
    "\n",
    "# Fit candidate models and compare AIC\n",
    "candidates = [(1,0,0), (0,0,1), (1,0,1), (2,0,1), (1,0,2)]\n",
    "results = []\n",
    "for order in candidates:\n",
    "    model = ARIMA(returns, order=order)\n",
    "    result = model.fit()\n",
    "    results.append({'order': order, 'AIC': result.aic})\n",
    "    \n",
    "results_df = pd.DataFrame(results).sort_values('AIC')\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We loop over several plausible orders and pick the one with the smallest AIC.\n",
    "- Note: This is on returns (d=0). If we wanted to model prices directly, we would use order (p,1,q) and fit on prices.\n",
    "\n",
    "### **21.5.3 Model Fitting and Interpretation**\n",
    "\n",
    "Let's fit the best model (lowest AIC) and examine its coefficients.\n",
    "\n",
    "```python\n",
    "best_order = results_df.iloc[0]['order']\n",
    "best_model = ARIMA(returns, order=best_order)\n",
    "best_result = best_model.fit()\n",
    "print(best_result.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The summary shows coefficients, standard errors, and p‑values. If a coefficient is not significant (p > 0.05), we might consider dropping that term.\n",
    "- Also check the Ljung‑Box test on residuals to ensure no autocorrelation remains (see diagnostics).\n",
    "\n",
    "### **21.5.4 Forecasting with ARIMA**\n",
    "\n",
    "```python\n",
    "# Forecast next 5 returns\n",
    "forecast_returns = best_result.forecast(steps=5)\n",
    "print(forecast_returns)\n",
    "\n",
    "# Convert to price forecast if needed\n",
    "last_price = df_one['Close'].iloc[-1]\n",
    "price_forecast = last_price * (1 + forecast_returns.cumsum() / 100)  # again approximate\n",
    "print(price_forecast)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `forecast` returns point predictions and confidence intervals if we use `get_forecast()`.\n",
    "- For price forecasting, we can also fit ARIMA directly on prices with d=1.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.6 SARIMA Models (Seasonal ARIMA)**\n",
    "\n",
    "If the series exhibits seasonality (e.g., weekly, monthly, yearly patterns), we need a seasonal ARIMA model, denoted SARIMA(p,d,q)(P,D,Q)s, where s is the seasonal period.\n",
    "\n",
    "For NEPSE, we might see weekly patterns (e.g., Monday effect) or monthly patterns (e.g., end‑of‑month). The seasonal component captures these.\n",
    "\n",
    "### **21.6.1 Identifying Seasonality**\n",
    "\n",
    "Plot the series and look for repeating patterns. Also, examine the ACF: significant spikes at seasonal lags (e.g., lag 5 for weekly, lag 20 for monthly) suggest seasonality.\n",
    "\n",
    "```python\n",
    "# For daily data with potential weekly seasonality (5 trading days), we might check ACF at lags 5,10,...\n",
    "# We'll use returns data\n",
    "plot_acf(returns, lags=40)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- If we see a spike at lag 5 (one week) and possibly at lag 10 (two weeks), that indicates weekly seasonality.\n",
    "- However, financial returns often have weak seasonality; it's more common in volumes or volatility.\n",
    "\n",
    "### **21.6.2 Fitting a SARIMA Model**\n",
    "\n",
    "We use `SARIMAX` from `statsmodels` (which can also handle exogenous variables).\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "\n",
    "# Example: SARIMA(1,0,1)(1,0,1,5) - weekly seasonality\n",
    "seasonal_order = (1,0,1,5)  # (P,D,Q,s)\n",
    "model_sarima = SARIMAX(returns, order=(1,0,1), seasonal_order=seasonal_order)\n",
    "result_sarima = model_sarima.fit(disp=False)\n",
    "print(result_sarima.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The `seasonal_order` tuple specifies the seasonal AR, differencing, MA, and the period s.\n",
    "- For NEPSE, s=5 for weekly (trading days). If we had monthly data, s=12. For daily with monthly effects, s could be 20 (approx trading days per month).\n",
    "- The model can become quite complex; we must ensure we have enough data to estimate all parameters.\n",
    "\n",
    "### **21.6.3 Model Selection for SARIMA**\n",
    "\n",
    "Because the parameter space is large, we often use automated search (e.g., `pmdarima` library) to find the best SARIMA order.\n",
    "\n",
    "```python\n",
    "# Install pmdarima if needed: pip install pmdarima\n",
    "import pmdarima as pm\n",
    "\n",
    "# Auto-arima to find best SARIMA model\n",
    "auto_model = pm.auto_arima(returns, seasonal=True, m=5,  # m=5 for weekly\n",
    "                            start_p=0, start_q=0, max_p=3, max_q=3,\n",
    "                            start_P=0, start_Q=0, max_P=2, max_Q=2,\n",
    "                            trace=True, error_action='ignore', suppress_warnings=True)\n",
    "print(auto_model.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `pmdarima` automatically searches over combinations of p,d,q,P,D,Q with given constraints.\n",
    "- The `m` parameter is the seasonal period. For daily data with weekly seasonality, m=5 (trading days). For monthly seasonality, m=20 or m=22.\n",
    "- The output includes the best model order and its AIC.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.7 Exponential Smoothing**\n",
    "\n",
    "Exponential smoothing methods forecast as weighted averages of past observations, with weights decaying exponentially as observations get older. They are intuitive and often perform well for series with trend and/or seasonality.\n",
    "\n",
    "### **21.7.1 Simple Exponential Smoothing (SES)**\n",
    "\n",
    "For series with no trend or seasonality, SES forecasts as a weighted average of all past values, with weights decaying exponentially. The smoothing parameter α (0<α<1) controls the decay.\n",
    "\n",
    "**Model:**  \n",
    "`ŷ_{t+1} = α y_t + (1-α) ŷ_t`\n",
    "\n",
    "In Python, we can use `SimpleExpSmoothing` from `statsmodels`.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing\n",
    "\n",
    "# Fit SES on returns (though returns may not need smoothing, but as demo)\n",
    "ses_model = SimpleExpSmoothing(returns).fit(smoothing_level=0.2, optimized=False)\n",
    "# or let it optimize alpha:\n",
    "ses_model_opt = SimpleExpSmoothing(returns).fit()\n",
    "print(f\"Optimized alpha: {ses_model_opt.params['smoothing_level']:.4f}\")\n",
    "\n",
    "# Forecast next 5\n",
    "forecast_ses = ses_model_opt.forecast(5)\n",
    "print(forecast_ses)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `optimized=False` with a given alpha; `optimized=True` (default) finds the best alpha by minimizing SSE.\n",
    "- SES is rarely used for financial returns because returns are often unpredictable (white noise), but it can be used for volatility or other smoother series.\n",
    "\n",
    "### **21.7.2 Double Exponential Smoothing (Holt's Method)**\n",
    "\n",
    "Adds a trend component. Suitable for series with trend but no seasonality.\n",
    "\n",
    "**Model:** level and trend equations.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.holtwinters import Holt\n",
    "\n",
    "holt_model = Holt(returns).fit()\n",
    "print(holt_model.summary())\n",
    "forecast_holt = holt_model.forecast(5)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Holt's method estimates a level and a trend, both smoothed exponentially.\n",
    "- For returns, which have no trend (mean zero), this may not be appropriate, but for prices it could capture trends.\n",
    "\n",
    "### **21.7.3 Triple Exponential Smoothing (Holt‑Winters)**\n",
    "\n",
    "Adds a seasonal component. Suitable for series with both trend and seasonality.\n",
    "\n",
    "**Model:** level, trend, and seasonal components.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "# For daily returns, if we believe weekly seasonality exists\n",
    "hw_model = ExponentialSmoothing(returns, seasonal_periods=5, trend='add', seasonal='add').fit()\n",
    "forecast_hw = hw_model.forecast(5)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `seasonal_periods` is the number of periods in a season (e.g., 5 for weekly trading days).\n",
    "- `trend` and `seasonal` can be 'add' or 'mul' for additive or multiplicative components.\n",
    "- For financial data, additive seasonality is more common.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.8 State Space Models**\n",
    "\n",
    "State space models provide a unified framework for many time‑series models, including ARIMA and exponential smoothing. They represent the observed series as a function of an unobserved state vector that evolves over time.\n",
    "\n",
    "In `statsmodels`, `SARIMAX` is actually a state space model. Also, `UnobservedComponents` allows flexible specification of trend, seasonality, and cycle.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.statespace.structural import UnobservedComponents\n",
    "\n",
    "# Model with a local level (random walk) and a seasonal component\n",
    "ss_model = UnobservedComponents(returns, level='local level', seasonal=5)\n",
    "ss_result = ss_model.fit()\n",
    "print(ss_result.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This is a sophisticated approach that can capture many features. It's beyond the scope of this chapter, but worth knowing that such tools exist.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.9 Vector Autoregression (VAR)**\n",
    "\n",
    "When we have multiple related time series (e.g., several stocks, or a stock and an index), we might want to model them jointly. VAR models each variable as a linear function of past values of itself and past values of all other variables.\n",
    "\n",
    "**Mathematically:**  \n",
    "`Y_t = c + A₁ Y_{t-1} + ... + A_p Y_{t-p} + ε_t`  \n",
    "where `Y_t` is a vector of variables, `A_i` are coefficient matrices.\n",
    "\n",
    "### **21.9.1 When to Use VAR**\n",
    "\n",
    "- To capture interdependencies (e.g., how NEPSE bank stocks influence each other).\n",
    "- For forecasting multiple series simultaneously.\n",
    "- For Granger causality testing (whether one series helps predict another).\n",
    "\n",
    "### **21.9.2 Fitting a VAR Model**\n",
    "\n",
    "We'll select a few stocks from the NEPSE dataset.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.api import VAR\n",
    "\n",
    "# Select a few symbols\n",
    "symbols = df['Symbol'].unique()[:3]  # first 3 symbols\n",
    "dfs = []\n",
    "for sym in symbols:\n",
    "    sym_df = df[df['Symbol'] == sym].set_index('Date')['Close'].rename(sym)\n",
    "    dfs.append(sym_df)\n",
    "\n",
    "# Combine into a single DataFrame (may have missing dates; we'll inner join)\n",
    "multi = pd.concat(dfs, axis=1).dropna()\n",
    "returns_multi = multi.pct_change().dropna() * 100\n",
    "\n",
    "# Fit VAR model (select lag order by AIC)\n",
    "var_model = VAR(returns_multi)\n",
    "lag_order = var_model.select_order(maxlags=10)\n",
    "print(lag_order.summary())\n",
    "\n",
    "# Fit with chosen lag (e.g., AIC suggested lag)\n",
    "results_var = var_model.fit(lag_order.aic)\n",
    "print(results_var.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `select_order` computes information criteria for different lags. We choose the lag that minimizes AIC.\n",
    "- `fit` estimates the model. The summary shows coefficients for each equation (each stock's return as a function of lagged returns of all stocks).\n",
    "- Forecasting with VAR uses `forecast` method, which requires the last `p` observations.\n",
    "\n",
    "### **21.9.3 Forecasting with VAR**\n",
    "\n",
    "```python\n",
    "# Forecast next 5 days\n",
    "lag_order = results_var.k_ar\n",
    "last_obs = returns_multi.values[-lag_order:]\n",
    "forecast_var = results_var.forecast(y=last_obs, steps=5)\n",
    "print(forecast_var)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `forecast` returns an array of shape (steps, n_variables).\n",
    "- We can then convert to price forecasts for each stock.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.10 Model Diagnostics**\n",
    "\n",
    "After fitting any statistical model, we must check whether the residuals resemble white noise. If they don't, the model is misspecified.\n",
    "\n",
    "### **21.10.1 Residual Analysis**\n",
    "\n",
    "- Plot residuals over time: should look random, no pattern.\n",
    "- ACF of residuals: should show no significant autocorrelation.\n",
    "- Ljung‑Box test: tests whether any group of autocorrelations is significantly different from zero.\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "\n",
    "residuals = best_result.resid\n",
    "\n",
    "# Plot residuals\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(residuals)\n",
    "plt.title('Residuals of ARIMA Model')\n",
    "plt.show()\n",
    "\n",
    "# ACF of residuals\n",
    "plot_acf(residuals, lags=30)\n",
    "plt.show()\n",
    "\n",
    "# Ljung-Box test\n",
    "lb_test = acorr_ljungbox(residuals, lags=[10, 20, 30], return_df=True)\n",
    "print(lb_test)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- For a good model, p‑values of the Ljung‑Box test should be > 0.05, indicating no significant autocorrelation at those lags.\n",
    "- If residuals show autocorrelation, we need to increase the model order or consider seasonality.\n",
    "\n",
    "### **21.10.2 Normality of Residuals (Optional)**\n",
    "\n",
    "Financial returns often have fat tails, so normality is not required, but extreme non‑normality may indicate outliers or model inadequacy. We can check with a Q‑Q plot.\n",
    "\n",
    "```python\n",
    "import scipy.stats as stats\n",
    "\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **21.10.3 Out‑of‑Sample Validation**\n",
    "\n",
    "Ultimately, the best diagnostic is out‑of‑sample forecasting performance. We should use the splitting techniques from Chapter 20 to evaluate the model on unseen data.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.11 Strengths and Limitations**\n",
    "\n",
    "### **Strengths of Statistical Models**\n",
    "\n",
    "- **Interpretability:** Coefficients have clear meanings (e.g., φ₁ measures persistence).\n",
    "- **Theoretical foundation:** Well‑understood properties, confidence intervals for forecasts.\n",
    "- **Parsimony:** Often require few parameters, reducing overfitting risk.\n",
    "- **Benchmarking:** Serve as natural baselines for ML models.\n",
    "- **Handling of uncertainty:** Provide prediction intervals naturally (via model assumptions).\n",
    "\n",
    "### **Limitations**\n",
    "\n",
    "- **Linearity:** Assume linear relationships, which may not hold in financial markets.\n",
    "- **Stationarity:** Require series to be stationary (or differenced), which may discard long‑term information.\n",
    "- **Univariate focus:** Basic ARIMA models don't incorporate external regressors easily (though ARIMAX does).\n",
    "- **Limited flexibility:** Cannot capture complex non‑linear patterns or interactions.\n",
    "- **Sensitivity to outliers:** Parameter estimates can be distorted by extreme events.\n",
    "\n",
    "### **Comparison with Machine Learning Models**\n",
    "\n",
    "| Aspect | Statistical Models | Machine Learning Models |\n",
    "|--------|-------------------|------------------------|\n",
    "| Interpretability | High | Low (black box) |\n",
    "| Data requirements | Moderate | High |\n",
    "| Handling non‑linearity | Poor | Excellent |\n",
    "| Feature engineering | Minimal | Extensive |\n",
    "| Uncertainty quantification | Natural | Requires special methods |\n",
    "| Computational cost | Low | High |\n",
    "| Performance on simple series | Good | May overfit |\n",
    "\n",
    "For the NEPSE system, we might start with a statistical model as a baseline, then see if ML models can improve upon it.\n",
    "\n",
    "---\n",
    "\n",
    "## **21.12 Implementation on NEPSE Data: Step‑by‑Step**\n",
    "\n",
    "Let's walk through a complete example of building an ARIMA model for a single NEPSE stock to forecast the next day's closing price.\n",
    "\n",
    "### **21.12.1 Data Preparation**\n",
    "\n",
    "```python\n",
    "# Load and prepare data for a specific symbol\n",
    "symbol = \"NEPSE\"  # or a specific stock ticker\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "df_stock = df_stock.set_index('Date').sort_index()\n",
    "\n",
    "# Use closing price\n",
    "prices = df_stock['Close'].dropna()\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(prices)\n",
    "plt.title(f'{symbol} Closing Price')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **21.12.2 Stationarity Check and Differencing**\n",
    "\n",
    "```python\n",
    "# ADF test on prices\n",
    "adf_p = adfuller(prices)\n",
    "print(f\"Prices ADF p-value: {adf_p[1]:.4f}\")\n",
    "\n",
    "# If p > 0.05, difference once\n",
    "if adf_p[1] > 0.05:\n",
    "    prices_diff = prices.diff().dropna()\n",
    "    # Test again\n",
    "    adf_diff = adfuller(prices_diff)\n",
    "    print(f\"1st difference ADF p-value: {adf_diff[1]:.4f}\")\n",
    "    d = 1\n",
    "else:\n",
    "    prices_diff = prices\n",
    "    d = 0\n",
    "```\n",
    "\n",
    "### **21.12.3 Identify AR and MA Orders**\n",
    "\n",
    "```python\n",
    "# Plot ACF and PACF of differenced series\n",
    "fig, (ax1, ax2) = plt.subplots(2,1, figsize=(12,8))\n",
    "plot_acf(prices_diff, lags=30, ax=ax1)\n",
    "plot_pacf(prices_diff, lags=30, ax=ax2)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **21.12.4 Fit Candidate ARIMA Models**\n",
    "\n",
    "```python\n",
    "# Based on plots, try a few orders\n",
    "orders = [(1,d,0), (0,d,1), (1,d,1), (2,d,1), (1,d,2)]\n",
    "results = []\n",
    "for order in orders:\n",
    "    try:\n",
    "        model = ARIMA(prices, order=order)\n",
    "        fit = model.fit()\n",
    "        results.append({'order': order, 'AIC': fit.aic})\n",
    "    except:\n",
    "        continue\n",
    "\n",
    "results_df = pd.DataFrame(results).sort_values('AIC')\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "### **21.12.5 Choose Best Model and Diagnose**\n",
    "\n",
    "```python\n",
    "best_order = results_df.iloc[0]['order']\n",
    "best_model = ARIMA(prices, order=best_order)\n",
    "best_fit = best_model.fit()\n",
    "print(best_fit.summary())\n",
    "\n",
    "# Residual diagnostics\n",
    "resid = best_fit.resid\n",
    "plot_acf(resid, lags=30)\n",
    "plt.show()\n",
    "\n",
    "lb_test = acorr_ljungbox(resid, lags=[10,20], return_df=True)\n",
    "print(lb_test)\n",
    "```\n",
    "\n",
    "### **21.12.6 Forecasting**\n",
    "\n",
    "```python\n",
    "# Forecast next 5 days\n",
    "forecast_result = best_fit.get_forecast(steps=5)\n",
    "forecast_mean = forecast_result.predicted_mean\n",
    "forecast_ci = forecast_result.conf_int()\n",
    "\n",
    "# Plot historical and forecast\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(prices.index[-100:], prices.values[-100:], label='Historical')\n",
    "forecast_index = pd.date_range(start=prices.index[-1] + pd.Timedelta(days=1), periods=5, freq='B')\n",
    "plt.plot(forecast_index, forecast_mean, label='Forecast', color='red')\n",
    "plt.fill_between(forecast_index, forecast_ci.iloc[:,0], forecast_ci.iloc[:,1], color='red', alpha=0.2)\n",
    "plt.legend()\n",
    "plt.title(f'{symbol} Price Forecast')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **21.13 Chapter Summary**\n",
    "\n",
    "In this chapter, we covered the essential traditional statistical models for time‑series forecasting, with applications to the NEPSE dataset.\n",
    "\n",
    "- **AR, MA, ARMA, ARIMA** models form the core of the Box‑Jenkins approach. We learned how to identify orders using ACF/PACF, fit models, and diagnose residuals.\n",
    "- **SARIMA** extends ARIMA to handle seasonality, which may be present in daily trading data.\n",
    "- **Exponential smoothing** methods (Holt‑Winters) provide an intuitive alternative for series with trend and seasonality.\n",
    "- **Vector Autoregression (VAR)** models multivariate time series, capturing interdependencies among multiple stocks.\n",
    "- **Model diagnostics** (Ljung‑Box test, residual ACF) are crucial to ensure model adequacy.\n",
    "- We compared strengths and limitations of statistical models with machine learning, highlighting that statistical models are excellent baselines.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Always start with a simple statistical model as a baseline (e.g., ARIMA on returns).\n",
    "- Use AIC to guide model selection, but also validate out‑of‑sample.\n",
    "- Check residuals carefully; if patterns remain, consider more complex models or ML.\n",
    "- For multivariate forecasting (e.g., multiple stocks), VAR can be a powerful tool.\n",
    "- Statistical models provide interpretable insights into market dynamics (e.g., persistence, mean reversion).\n",
    "\n",
    "In the next chapter, **Chapter 22: Tree‑Based Models**, we will explore how decision trees, random forests, and gradient boosting can be applied to the same forecasting problems, often yielding higher accuracy at the cost of interpretability.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 21**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
