{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 18: Machine Learning Fundamentals for Time-Series**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the core concepts of supervised learning as applied to time-series prediction\n",
    "- Differentiate between regression, classification, and multi-step forecasting problems\n",
    "- Grasp the implications of the No-Free-Lunch theorem for model selection\n",
    "- Explain the bias-variance tradeoff and its role in model performance\n",
    "- Identify and diagnose overfitting and underfitting in time-series models\n",
    "- Understand model capacity and its relationship to data complexity\n",
    "- Distinguish between training and test performance\n",
    "- Appreciate the importance of generalization in financial forecasting\n",
    "- Develop a systematic model selection strategy for the NEPSE prediction system\n",
    "\n",
    "---\n",
    "\n",
    "## **18.1 Supervised Learning Basics**\n",
    "\n",
    "Supervised learning is the machine learning paradigm where we train models on labeled data \u2013 that is, data that contains both input features and corresponding output targets. For time-series prediction, this means we have historical observations and we want to predict future values. The model learns a mapping function `f` from input features `X` to target `y` such that `y \u2248 f(X)`.\n",
    "\n",
    "### **18.1.1 The Supervised Learning Framework for Time-Series**\n",
    "\n",
    "In time-series, we transform the sequential data into a supervised learning format by creating lagged features. For the NEPSE dataset, we want to predict tomorrow's closing price (or return) using today's and past days' information.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Load and prepare NEPSE data\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values('Date').reset_index(drop=True)\n",
    "\n",
    "# Create supervised learning format: predict next day's Close\n",
    "# using today's features\n",
    "df['Target'] = df['Close'].shift(-1)  # tomorrow's close\n",
    "\n",
    "# Create feature matrix (using only information available today)\n",
    "feature_cols = ['Open', 'High', 'Low', 'Close', 'Vol', 'VWAP']\n",
    "df_features = df[feature_cols].copy()\n",
    "\n",
    "# Add some lag features\n",
    "df_features['Close_Lag1'] = df['Close'].shift(1)\n",
    "df_features['Volume_Lag1'] = df['Vol'].shift(1)\n",
    "\n",
    "# Drop rows with NaN (created by shift and target)\n",
    "df_clean = pd.concat([df_features, df['Target']], axis=1).dropna()\n",
    "\n",
    "# Define X (features) and y (target)\n",
    "X = df_clean[feature_cols + ['Close_Lag1', 'Volume_Lag1']]\n",
    "y = df_clean['Target']\n",
    "\n",
    "print(f\"Feature matrix shape: {X.shape}\")\n",
    "print(f\"Target vector shape: {y.shape}\")\n",
    "print(f\"\\nFirst 3 rows of features:\")\n",
    "print(X.head(3))\n",
    "print(f\"\\nCorresponding targets (next day close):\")\n",
    "print(y.head(3))\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Target creation:** `df['Target'] = df['Close'].shift(-1)` aligns each row with the next day's closing price. This is the fundamental transformation that turns a time-series into a supervised learning problem. The `shift(-1)` moves future values upward, so row `i` contains today's features and tomorrow's target.\n",
    "- **Feature matrix:** We select columns that are available at prediction time. Notice we also create `Close_Lag1` \u2013 yesterday's close \u2013 as an additional feature. In practice, we would include many more lags and derived features.\n",
    "- **Data cleaning:** After shifting, the last row will have a NaN target (since there is no \"tomorrow\"), and the first few rows will have NaN for lag features. We drop these to create a clean dataset.\n",
    "- **Result:** We now have a standard supervised learning dataset where each row is independent (though in reality, rows are temporally ordered). Models like Random Forest can be trained on this data.\n",
    "\n",
    "### **18.1.2 The Learning Task**\n",
    "\n",
    "The supervised learning task is to find a function `f` that minimizes the expected prediction error. For regression (predicting a continuous value like price), we typically minimize the mean squared error between predictions and actual values. For classification (predicting a discrete class like up/down), we minimize cross-entropy or misclassification rate.\n",
    "\n",
    "The model learns patterns from the training data: for example, that when the RSI is below 30 and volume is high, the next day's price tends to increase. These patterns are captured in the model's parameters (e.g., coefficients in linear regression, split points in decision trees).\n",
    "\n",
    "---\n",
    "\n",
    "## **18.2 Prediction Problem Types**\n",
    "\n",
    "Time-series prediction can be framed in several ways depending on the business goal. For the NEPSE system, we might want to predict the exact future price, the direction of movement, or a sequence of future values.\n",
    "\n",
    "### **18.2.1 Regression**\n",
    "\n",
    "Regression predicts a continuous numerical value. In our context, this could be the next day's closing price, the percentage return, or the price range.\n",
    "\n",
    "```python\n",
    "# Example: Regression to predict next day's close price\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# Train-test split (temporal, not random!)\n",
    "train_size = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "regressor.fit(X_train, y_train)\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"Regression RMSE: {rmse:.2f} NPR\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This uses a Random Forest to predict the exact closing price. The error metric RMSE tells us, on average, how far off our predictions are in Nepalese Rupees.\n",
    "- Regression is appropriate when the exact value matters \u2013 for example, in algorithmic trading where entry/exit prices determine profit.\n",
    "\n",
    "### **18.2.2 Classification**\n",
    "\n",
    "Classification predicts a discrete class. For the NEPSE system, a common classification task is predicting the direction of price movement: up, down, or unchanged.\n",
    "\n",
    "```python\n",
    "# Create binary classification target: 1 if price increases, 0 otherwise\n",
    "df['Target_Direction'] = (df['Close'].shift(-1) > df['Close']).astype(int)\n",
    "\n",
    "# Prepare data (using same features as before)\n",
    "df_clean = pd.concat([df_features, df['Target_Direction']], axis=1).dropna()\n",
    "X = df_clean[feature_cols + ['Close_Lag1', 'Volume_Lag1']]\n",
    "y = df_clean['Target_Direction']\n",
    "\n",
    "# Train a classifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "clf = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "\n",
    "# Temporal split\n",
    "X_train, X_test = X.iloc[:train_size], X.iloc[train_size:]\n",
    "y_train, y_test = y.iloc[:train_size], y.iloc[train_size:]\n",
    "\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Direction prediction accuracy: {accuracy:.3f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The binary target is 1 if tomorrow's close is higher than today's, 0 otherwise.\n",
    "- Classification can be easier than regression because small price movements (noise) are less problematic \u2013 as long as we get the direction right, we can make profitable trades.\n",
    "- Accuracy measures the fraction of correct direction predictions. A value above 0.5 indicates the model has some predictive power beyond random guessing.\n",
    "\n",
    "We could also create multi-class targets: e.g., \"large increase\" (>2%), \"small increase\" (0-2%), \"small decrease\" (-2-0%), \"large decrease\" (<-2%). This might be useful for position sizing.\n",
    "\n",
    "### **18.2.3 Multi-Step Forecasting**\n",
    "\n",
    "Often we need to predict several steps ahead \u2013 for example, the next 5 days of prices. This is more challenging because errors accumulate. There are two main approaches:\n",
    "\n",
    "1. **Direct multi-step:** Train separate models for each horizon (model for t+1, t+2, etc.).\n",
    "2. **Recursive multi-step:** Use one-step model and feed predictions back as features.\n",
    "\n",
    "```python\n",
    "# Example: Direct multi-step forecasting (predict t+1, t+2, t+3)\n",
    "horizons = [1, 2, 3]\n",
    "models = {}\n",
    "predictions = {}\n",
    "\n",
    "for h in horizons:\n",
    "    # Create target shifted by h days\n",
    "    df[f'Target_{h}'] = df['Close'].shift(-h)\n",
    "    \n",
    "    # Prepare data\n",
    "    df_clean = pd.concat([df_features, df[f'Target_{h}']], axis=1).dropna()\n",
    "    X = df_clean[feature_cols + ['Close_Lag1', 'Volume_Lag1']]\n",
    "    y = df_clean[f'Target_{h}']\n",
    "    \n",
    "    # Train model for this horizon\n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X.iloc[:train_size], y.iloc[:train_size])\n",
    "    models[h] = model\n",
    "    \n",
    "    # Predict on test set\n",
    "    preds = model.predict(X.iloc[train_size:])\n",
    "    predictions[h] = preds\n",
    "    \n",
    "    rmse = np.sqrt(mean_squared_error(y.iloc[train_size:], preds))\n",
    "    print(f\"Horizon {h} day(s) ahead - RMSE: {rmse:.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- For each horizon `h`, we create a target `Close.shift(-h)`. This gives, for each date, the price `h` days in the future.\n",
    "- We train a separate model for each horizon. This is the **direct** approach. Its advantage is that each model can specialize in its own horizon; disadvantage is that it ignores dependencies between the forecasts (e.g., t+2 might be related to t+1).\n",
    "- The **recursive** approach would use the t+1 model, then feed its prediction back as a feature to predict t+2, and so on. This is more efficient (only one model) but can compound errors.\n",
    "\n",
    "For the NEPSE system, we might need both short-term (1-3 day) and medium-term (5-20 day) forecasts depending on the trading strategy.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.3 The No-Free-Lunch Theorem**\n",
    "\n",
    "The No-Free-Lunch (NFL) theorem states that no single machine learning algorithm is universally better than any other across all possible problems. In other words, the performance of an algorithm on one type of problem does not guarantee its performance on another. For the NEPSE prediction system, this means we cannot assume that a model that works well for US stocks will work for NEPSE, or that a model that performs well during a bull market will perform well during a bear market.\n",
    "\n",
    "### **18.3.1 Implications for Model Selection**\n",
    "\n",
    "The NFL theorem guides us to:\n",
    "\n",
    "- **Experiment with multiple algorithms:** Try linear models, tree-based models, neural networks, and statistical methods (ARIMA, etc.). What works best depends on the specific characteristics of NEPSE data.\n",
    "- **Use cross-validation:** Compare models on the same data to see which generalizes best.\n",
    "- **Consider the data regime:** NEPSE may have different properties (low liquidity, high volatility, specific seasonality) than other markets. A model that accounts for these may outperform a generic one.\n",
    "\n",
    "```python\n",
    "# Example: Compare multiple algorithms on NEPSE data\n",
    "from sklearn.linear_model import LinearRegression, Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "import time\n",
    "\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Ridge Regression': Ridge(alpha=1.0),\n",
    "    'Lasso': Lasso(alpha=0.01),\n",
    "    'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(n_estimators=100, random_state=42),\n",
    "    'SVR': SVR(kernel='rbf')\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    start = time.time()\n",
    "    model.fit(X_train, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    y_pred = model.predict(X_test)\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    \n",
    "    results[name] = {\n",
    "        'MAE': mae,\n",
    "        'Train Time (s)': train_time\n",
    "    }\n",
    "\n",
    "# Display results\n",
    "results_df = pd.DataFrame(results).T.sort_values('MAE')\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This code trains six different regression models on the same NEPSE dataset and compares their Mean Absolute Error (MAE) on a temporally separated test set.\n",
    "- The results will likely show that some models (e.g., Random Forest) outperform others (e.g., Linear Regression) on this particular dataset. However, the NFL theorem reminds us that this ranking might change if we alter the features, the target horizon, or the time period.\n",
    "- The practical takeaway: always benchmark multiple models. Do not assume that the latest deep learning architecture is automatically best for NEPSE.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.4 Bias-Variance Tradeoff**\n",
    "\n",
    "The bias-variance tradeoff is a fundamental concept in machine learning that describes the tension between a model's ability to fit the training data (low bias) and its ability to generalize to unseen data (low variance).\n",
    "\n",
    "- **Bias** is the error introduced by approximating a real-world problem with a simplified model. High bias models underfit the data \u2013 they miss important patterns.\n",
    "- **Variance** is the error introduced by the model's sensitivity to fluctuations in the training set. High variance models overfit \u2013 they memorize noise instead of learning the true signal.\n",
    "\n",
    "### **18.4.1 Visualizing Bias and Variance**\n",
    "\n",
    "Imagine we are trying to model the relationship between past returns and future returns. A linear model might have high bias (it cannot capture non-linear patterns), while a high-degree polynomial might have high variance (it will fit every wiggle in the training data).\n",
    "\n",
    "```python\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Generate synthetic data that mimics a noisy NEPSE return pattern\n",
    "np.random.seed(42)\n",
    "X = np.linspace(0, 10, 100).reshape(-1, 1)\n",
    "y_true = 2 * np.sin(X).ravel() + 0.5 * X.ravel()\n",
    "y = y_true + np.random.normal(0, 0.5, size=len(X))\n",
    "\n",
    "# Fit models with different complexity\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "degrees = [1, 3, 15]\n",
    "plt.figure(figsize=(15, 5))\n",
    "\n",
    "for i, degree in enumerate(degrees):\n",
    "    ax = plt.subplot(1, 3, i + 1)\n",
    "    \n",
    "    model = make_pipeline(PolynomialFeatures(degree), LinearRegression())\n",
    "    model.fit(X, y)\n",
    "    y_pred = model.predict(X)\n",
    "    \n",
    "    plt.scatter(X, y, s=10, alpha=0.5, label='Data')\n",
    "    plt.plot(X, y_true, 'k--', label='True function')\n",
    "    plt.plot(X, y_pred, 'r-', label='Model')\n",
    "    plt.title(f'Degree {degree} Polynomial')\n",
    "    plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- **Degree 1 (linear):** High bias \u2013 the model is too simple and cannot capture the curvature. It underfits.\n",
    "- **Degree 3:** Good balance \u2013 captures the main shape without fitting the noise.\n",
    "- **Degree 15:** High variance \u2013 the model wiggles wildly to pass through every training point. It has memorized the noise and will not generalize well to new data.\n",
    "\n",
    "### **18.4.2 Applying to NEPSE Prediction**\n",
    "\n",
    "In the context of the NEPSE system:\n",
    "\n",
    "- **High bias (underfitting):** Using only linear features (e.g., today's price) to predict tomorrow's price. The model may miss important patterns like momentum or mean reversion. Training error will be high, and test error will also be high.\n",
    "- **High variance (overfitting):** Using hundreds of features (e.g., all tsfresh features) without regularization. The model will achieve very low training error but perform poorly on new data because it has learned spurious patterns specific to the training period.\n",
    "- **Optimal complexity:** A model with enough capacity to capture the true underlying patterns but not so much that it fits noise. This is achieved through careful feature selection, regularization, and validation.\n",
    "\n",
    "```python\n",
    "# Demonstration of overfitting with a complex model\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "\n",
    "# Train a decision tree with unlimited depth (high complexity)\n",
    "tree_deep = DecisionTreeRegressor(max_depth=None, random_state=42)\n",
    "tree_deep.fit(X_train, y_train)\n",
    "\n",
    "# Train a shallow tree (low complexity)\n",
    "tree_shallow = DecisionTreeRegressor(max_depth=3, random_state=42)\n",
    "tree_shallow.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate\n",
    "print(\"Deep Tree (max_depth=None):\")\n",
    "print(f\"  Train R\u00b2: {tree_deep.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R\u00b2: {tree_deep.score(X_test, y_test):.4f}\")\n",
    "\n",
    "print(\"\\nShallow Tree (max_depth=3):\")\n",
    "print(f\"  Train R\u00b2: {tree_shallow.score(X_train, y_train):.4f}\")\n",
    "print(f\"  Test R\u00b2: {tree_shallow.score(X_test, y_test):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The deep tree will likely have perfect or near-perfect training R\u00b2 (it can memorize the training data) but much lower test R\u00b2 \u2013 a classic sign of overfitting.\n",
    "- The shallow tree may have lower training R\u00b2 (higher bias) but better test R\u00b2 because it has learned more generalizable patterns.\n",
    "- In practice, we would use cross-validation to find the optimal depth that balances bias and variance.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.5 Overfitting and Underfitting**\n",
    "\n",
    "Overfitting and underfitting are the practical manifestations of the bias-variance tradeoff.\n",
    "\n",
    "### **18.5.1 Identifying Overfitting**\n",
    "\n",
    "Overfitting occurs when the model performs well on training data but poorly on unseen data. In time-series, overfitting can be especially insidious because of temporal dependencies \u2013 a model might appear to perform well in backtesting but fail in live trading due to overfitting to past market regimes.\n",
    "\n",
    "**Signs of overfitting:**\n",
    "- Large gap between training and test performance.\n",
    "- Model weights or feature importances that are unstable across different time periods.\n",
    "- Predictions that are overly sensitive to small changes in input.\n",
    "\n",
    "```python\n",
    "# Example: Monitoring overfitting during training\n",
    "from sklearn.model_selection import learning_curve\n",
    "\n",
    "train_sizes, train_scores, test_scores = learning_curve(\n",
    "    RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "    X_train, y_train,\n",
    "    train_sizes=np.linspace(0.1, 1.0, 10),\n",
    "    cv=5,\n",
    "    scoring='r2'\n",
    ")\n",
    "\n",
    "train_mean = np.mean(train_scores, axis=1)\n",
    "test_mean = np.mean(test_scores, axis=1)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(train_sizes, train_mean, 'o-', label='Training score')\n",
    "plt.plot(train_sizes, test_mean, 'o-', label='Cross-validation score')\n",
    "plt.xlabel('Training examples')\n",
    "plt.ylabel('R\u00b2 score')\n",
    "plt.legend()\n",
    "plt.title('Learning Curves - Detecting Overfitting')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Learning curves show model performance as a function of training set size.\n",
    "- If the training score remains high while the validation score plateaus or decreases, it suggests overfitting (the model is not generalizing).\n",
    "- For the NEPSE dataset, if we see a large and persistent gap, we need to reduce model complexity or add regularization.\n",
    "\n",
    "### **18.5.2 Identifying Underfitting**\n",
    "\n",
    "Underfitting happens when the model is too simple to capture the underlying structure. Signs include:\n",
    "- Low training and test performance.\n",
    "- Performance that does not improve with more data.\n",
    "- Residuals that show clear patterns (e.g., the model consistently misses turning points).\n",
    "\n",
    "```python\n",
    "# Check for underfitting: are residuals random?\n",
    "y_pred_simple = LinearRegression().fit(X_train, y_train).predict(X_test)\n",
    "residuals = y_test - y_pred_simple\n",
    "\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(y_test.values, label='Actual')\n",
    "plt.plot(y_pred_simple, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title('Simple Linear Model - Actual vs Predicted')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.scatter(y_pred_simple, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residual Plot')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- If the residuals show a pattern (e.g., they are positive when predictions are low and negative when predictions are high), the model is underfitting \u2013 it's missing a systematic relationship.\n",
    "- For the NEPSE system, underfitting might mean the model fails to capture volatility clustering or momentum effects.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.6 Model Capacity**\n",
    "\n",
    "Model capacity refers to a model's ability to fit a variety of functions. High-capacity models (e.g., deep neural networks, random forests with many trees) can approximate very complex relationships. Low-capacity models (e.g., linear regression) are restricted to linear functions.\n",
    "\n",
    "### **18.6.1 Matching Capacity to Data**\n",
    "\n",
    "The optimal model capacity depends on:\n",
    "- **Amount of data:** With more data, we can safely use higher-capacity models.\n",
    "- **Noise level:** In noisy environments (like financial markets), high-capacity models are more likely to overfit.\n",
    "- **Problem complexity:** If the true relationship is simple, a low-capacity model suffices.\n",
    "\n",
    "```python\n",
    "# Demonstrate increasing model capacity\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "\n",
    "capacities = [\n",
    "    ('Linear Regression', LinearRegression()),\n",
    "    ('Small NN (10 neurons)', MLPRegressor(hidden_layer_sizes=(10,), max_iter=1000, random_state=42)),\n",
    "    ('Large NN (100,50 neurons)', MLPRegressor(hidden_layer_sizes=(100,50), max_iter=1000, random_state=42))\n",
    "]\n",
    "\n",
    "results = {}\n",
    "for name, model in capacities:\n",
    "    model.fit(X_train, y_train)\n",
    "    train_score = model.score(X_train, y_train)\n",
    "    test_score = model.score(X_test, y_test)\n",
    "    results[name] = {'Train R\u00b2': train_score, 'Test R\u00b2': test_score}\n",
    "\n",
    "results_df = pd.DataFrame(results).T\n",
    "print(results_df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Linear Regression has the lowest capacity. It may underfit if the relationship is non-linear.\n",
    "- The small neural network has moderate capacity. It may capture some non-linearity without overfitting.\n",
    "- The large neural network has high capacity. It might overfit, especially with limited NEPSE data.\n",
    "- By comparing train and test R\u00b2, we can see which capacity level generalizes best.\n",
    "\n",
    "### **18.6.2 Capacity Control in Practice**\n",
    "\n",
    "We control capacity through:\n",
    "- **Model choice:** Linear models < tree-based < neural networks.\n",
    "- **Hyperparameters:** Tree depth, number of trees, neural network layers/neurons, regularization strength.\n",
    "- **Feature engineering:** More features increase effective capacity; feature selection reduces it.\n",
    "\n",
    "For the NEPSE system, start with a modest capacity (e.g., Random Forest with depth 5-10) and increase only if validation performance improves.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.7 Training vs. Test Performance**\n",
    "\n",
    "Understanding the difference between training and test performance is crucial for building reliable models.\n",
    "\n",
    "### **18.7.1 The Training-Test Split**\n",
    "\n",
    "In time-series, the standard random train-test split is invalid because it would use future data to predict the past. Instead, we must use a temporal split: train on earlier data, test on later data.\n",
    "\n",
    "```python\n",
    "# Temporal split for NEPSE data\n",
    "split_date = '2023-06-01'  # example cutoff\n",
    "\n",
    "train = df[df['Date'] < split_date]\n",
    "test = df[df['Date'] >= split_date]\n",
    "\n",
    "print(f\"Train period: {train['Date'].min()} to {train['Date'].max()}\")\n",
    "print(f\"Train samples: {len(train)}\")\n",
    "print(f\"Test period: {test['Date'].min()} to {test['Date'].max()}\")\n",
    "print(f\"Test samples: {len(test)}\")\n",
    "\n",
    "# Prepare features and targets for both sets\n",
    "# (assuming we already created features)\n",
    "X_train = train[feature_cols + lags]\n",
    "y_train = train['Target']\n",
    "X_test = test[feature_cols + lags]\n",
    "y_test = test['Target']\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This split respects the temporal order: the model is trained on past data and evaluated on future data, simulating real deployment.\n",
    "- The cutoff date should be chosen so that the test period is representative of future conditions (e.g., includes both bull and bear phases).\n",
    "\n",
    "### **18.7.2 Interpreting Performance Metrics**\n",
    "\n",
    "- **High training accuracy, low test accuracy:** Overfitting. The model has memorized the training data but not learned generalizable patterns.\n",
    "- **Low training and test accuracy:** Underfitting. The model is too simple.\n",
    "- **Both high and close:** Good generalization \u2013 the sweet spot.\n",
    "\n",
    "For the NEPSE system, we might see training R\u00b2 of 0.95 and test R\u00b2 of 0.30 on a complex model \u2013 a classic overfitting signal. A simpler model might show 0.60 and 0.55, which is much better for actual deployment.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.8 Generalization**\n",
    "\n",
    "Generalization is the model's ability to perform well on unseen data. It is the ultimate goal of machine learning. In financial forecasting, generalization is particularly challenging because markets evolve \u2013 a model that generalizes well must capture stable relationships, not ephemeral patterns.\n",
    "\n",
    "### **18.8.1 Factors Affecting Generalization**\n",
    "\n",
    "- **Data quality:** Clean, representative data leads to better generalization.\n",
    "- **Feature relevance:** Irrelevant features hurt generalization.\n",
    "- **Model complexity:** Appropriate capacity (not too high, not too low).\n",
    "- **Regularization:** Techniques that constrain the model to prefer simpler explanations.\n",
    "- **Training regime:** Enough data to learn patterns, not just noise.\n",
    "\n",
    "### **18.8.2 Testing Generalization**\n",
    "\n",
    "We can test generalization by evaluating the model on:\n",
    "- **Out-of-time data:** A later period not used in training.\n",
    "- **Out-of-sample data:** Different stocks (if we trained on some stocks, test on others).\n",
    "- **Cross-validation:** Time-series CV gives multiple estimates of generalization.\n",
    "\n",
    "```python\n",
    "# Time-series cross-validation to estimate generalization\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "scores = []\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(tscv.split(X)):\n",
    "    X_train_fold, X_val_fold = X.iloc[train_idx], X.iloc[val_idx]\n",
    "    y_train_fold, y_val_fold = y.iloc[train_idx], y.iloc[val_idx]\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "    model.fit(X_train_fold, y_train_fold)\n",
    "    score = model.score(X_val_fold, y_val_fold)\n",
    "    scores.append(score)\n",
    "    print(f\"Fold {fold+1} validation R\u00b2: {score:.4f}\")\n",
    "\n",
    "print(f\"\\nAverage validation R\u00b2: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `TimeSeriesSplit` creates expanding windows: each fold uses more training data and tests on a subsequent period.\n",
    "- The average score across folds gives a robust estimate of how the model will generalize to new, unseen time periods.\n",
    "- The standard deviation tells us about stability \u2013 high variability suggests the model is sensitive to the specific test period.\n",
    "\n",
    "---\n",
    "\n",
    "## **18.9 Model Selection Strategy**\n",
    "\n",
    "Model selection is the process of choosing the best model among candidates. For the NEPSE prediction system, we need a systematic strategy that balances performance, interpretability, and robustness.\n",
    "\n",
    "### **18.9.1 Steps in Model Selection**\n",
    "\n",
    "1. **Define evaluation metric:** Choose a metric that aligns with business goals (e.g., RMSE for price prediction, accuracy for direction, or a custom financial metric like Sharpe ratio).\n",
    "\n",
    "2. **Create candidate models:** Include diverse families \u2013 linear, tree-based, neural networks, statistical.\n",
    "\n",
    "3. **Use time-series cross-validation:** Evaluate each model on multiple train-test splits.\n",
    "\n",
    "4. **Compare performance:** Use the mean and variance of the chosen metric across folds.\n",
    "\n",
    "5. **Consider complexity:** Simpler models are preferable if performance is similar (Occam's razor).\n",
    "\n",
    "6. **Check for stability:** Model coefficients or feature importances should be reasonably consistent across folds.\n",
    "\n",
    "7. **Final test:** After selecting a model, evaluate it on a completely held-out final test set (the most recent data) to confirm generalization.\n",
    "\n",
    "```python\n",
    "# Example model selection framework\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "# Define candidate models with hyperparameter grids\n",
    "models_to_try = {\n",
    "    'Ridge': {\n",
    "        'model': Ridge(),\n",
    "        'params': {'alpha': [0.1, 1.0, 10.0]}\n",
    "    },\n",
    "    'Random Forest': {\n",
    "        'model': RandomForestRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'max_depth': [5, 10, None]\n",
    "        }\n",
    "    },\n",
    "    'Gradient Boosting': {\n",
    "        'model': GradientBoostingRegressor(random_state=42),\n",
    "        'params': {\n",
    "            'n_estimators': [50, 100],\n",
    "            'learning_rate': [0.05, 0.1],\n",
    "            'max_depth': [3, 5]\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Use TimeSeriesSplit for cross-validation\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "best_models = {}\n",
    "\n",
    "for name, config in models_to_try.items():\n",
    "    print(f\"\\nTuning {name}...\")\n",
    "    grid = GridSearchCV(\n",
    "        config['model'],\n",
    "        config['params'],\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    grid.fit(X, y)  # Note: In practice, use a separate validation set or nested CV\n",
    "    best_models[name] = grid.best_estimator_\n",
    "    print(f\"Best params: {grid.best_params_}\")\n",
    "    print(f\"Best CV score (negative MSE): {grid.best_score_:.4f}\")\n",
    "\n",
    "# Compare on a separate test set (final evaluation)\n",
    "test_scores = {}\n",
    "for name, model in best_models.items():\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    test_scores[name] = mse\n",
    "\n",
    "print(\"\\nFinal Test MSE:\")\n",
    "for name, mse in sorted(test_scores.items(), key=lambda x: x[1]):\n",
    "    print(f\"  {name}: {mse:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We define a dictionary of models and their hyperparameter grids.\n",
    "- `GridSearchCV` with `TimeSeriesSplit` performs hyperparameter tuning and cross-validation in one go. Note: For a completely unbiased estimate, we should use nested cross-validation (an inner loop for tuning, outer loop for evaluation), but that's computationally expensive.\n",
    "- After tuning, we evaluate the best models on a final test set that was not used during tuning/CV. This gives a realistic estimate of how each model would perform in production.\n",
    "- The model with the lowest test MSE is our final choice, but we should also consider interpretability and stability.\n",
    "\n",
    "### **18.9.2 Avoiding Common Pitfalls**\n",
    "\n",
    "- **Lookahead:** Ensure all preprocessing (scaling, feature engineering) is done within each CV fold using only training data.\n",
    "- **Multiple comparisons:** If you try many models, the best one might outperform by chance. Use a validation set or nested CV to get unbiased performance estimates.\n",
    "- **Overfitting to CV:** If you tune too aggressively, you might overfit to the validation folds. Keep the search space reasonable.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we established the machine learning foundations necessary for building time-series prediction systems, using the NEPSE stock prediction as our guiding example.\n",
    "\n",
    "**Key concepts covered:**\n",
    "\n",
    "- **Supervised learning framework:** Transforming time-series into a supervised format by creating lagged features and future targets.\n",
    "- **Problem types:** Regression (predicting exact prices), classification (predicting direction), and multi-step forecasting (direct vs. recursive approaches).\n",
    "- **No-Free-Lunch theorem:** No single model is universally best; we must experiment and compare multiple algorithms on our specific NEPSE dataset.\n",
    "- **Bias-variance tradeoff:** The tension between underfitting (high bias) and overfitting (high variance). We saw how model complexity affects this balance.\n",
    "- **Overfitting and underfitting:** Practical signs and diagnostic tools like learning curves and residual analysis.\n",
    "- **Model capacity:** Matching model complexity to data size and problem difficulty.\n",
    "- **Training vs. test performance:** The importance of temporal splits and the interpretation of performance gaps.\n",
    "- **Generalization:** The ultimate goal \u2013 models must perform well on unseen future data, not just past data.\n",
    "- **Model selection strategy:** A systematic approach using time-series cross-validation, hyperparameter tuning, and final hold-out evaluation.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "1. Always use temporal splits \u2013 never random splits \u2013 when evaluating time-series models.\n",
    "2. Start with simple models (linear regression, shallow trees) as baselines.\n",
    "3. Gradually increase complexity, monitoring the gap between train and test performance.\n",
    "4. Use time-series cross-validation to get robust estimates of generalization.\n",
    "5. Compare multiple model families \u2013 what works for US stocks may not work for NEPSE.\n",
    "6. Be wary of overfitting: in financial markets, yesterday's patterns may not repeat tomorrow.\n",
    "\n",
    "With these fundamentals in place, we are ready to dive into the specifics of model development. In **Chapter 19: Defining Prediction Targets**, we will explore how to design target variables that align with trading goals, whether we want to predict exact prices, directional moves, or risk-adjusted returns.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 18**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../3. feature_engineering/17. advanced_feature_engineering_techniques.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='19. defining_prediction_targets.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}