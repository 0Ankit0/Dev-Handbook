{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 26: Recurrent Neural Networks**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand why standard feedforward networks (MLPs) are insufficient for sequential data\n",
    "- Explain the architecture of a basic recurrent neural network (RNN) and how it processes sequences\n",
    "- Identify the vanishing/exploding gradient problem and how it limits simple RNNs\n",
    "- Implement Long Short\u2011Term Memory (LSTM) and Gated Recurrent Unit (GRU) networks using Keras\n",
    "- Prepare time\u2011series data in the correct format (samples, timesteps, features) for RNNs\n",
    "- Build and train RNN models for predicting NEPSE stock returns\n",
    "- Use bidirectional RNNs to capture patterns from both past and future (when applicable)\n",
    "- Stack multiple RNN layers to increase model capacity\n",
    "- Apply proper validation techniques (walk\u2011forward) to avoid look\u2011ahead bias\n",
    "- Compare RNN performance with MLPs and traditional models on the NEPSE dataset\n",
    "\n",
    "---\n",
    "\n",
    "## **26.1 Sequential Data Processing**\n",
    "\n",
    "Time\u2011series data, by its nature, is sequential: the order of observations matters. In previous chapters, we used **lag features** to feed past information into models like MLPs, random forests, or linear regression. For example, we created columns like `Return_Lag1`, `Return_Lag2`, etc., and treated them as independent features. While this works, it has limitations:\n",
    "\n",
    "- The model does not explicitly learn the **temporal dynamics**; it treats lags as separate dimensions without capturing the evolving pattern.\n",
    "- For longer sequences, the number of lag features grows, leading to high dimensionality and potential overfitting.\n",
    "- The model cannot easily learn patterns that depend on the **order** of the sequence, such as trends or repeating motifs.\n",
    "\n",
    "Recurrent neural networks (RNNs) are designed to handle sequential data by maintaining a **hidden state** that is updated as the network processes each time step. This hidden state acts as a memory, allowing information to persist across the sequence.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.2 Basic RNN Architecture**\n",
    "\n",
    "A simple RNN processes a sequence one element at a time. At each time step `t`, it takes the current input `x_t` and the previous hidden state `h_{t-1}`, and computes a new hidden state `h_t`:\n",
    "\n",
    "`h_t = tanh(W_xh * x_t + W_hh * h_{t-1} + b_h)`\n",
    "\n",
    "The output at each step can be `h_t` itself (if we want a sequence of outputs) or just the final hidden state (if we only need a single prediction after the whole sequence).\n",
    "\n",
    "For time\u2011series forecasting, we often use the **many\u2011to\u2011one** architecture: we feed a sequence of past observations (e.g., last 20 days of returns) and predict the next value.\n",
    "\n",
    "### **26.2.1 Vanishing/Exploding Gradients**\n",
    "\n",
    "During training, gradients are propagated back through time (Backpropagation Through Time \u2013 BPTT). For long sequences, repeated multiplication of the same weight matrix can cause gradients to vanish (become very small) or explode (become very large). Vanishing gradients prevent the network from learning long\u2011range dependencies; exploding gradients cause unstable training.\n",
    "\n",
    "This is why simple RNNs are rarely used in practice. Instead, we use gated architectures like LSTM and GRU.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.3 Long Short\u2011Term Memory (LSTM)**\n",
    "\n",
    "LSTMs introduce a **cell state** and three gates (input, forget, output) that regulate the flow of information. This design allows the network to remember information over long periods and avoid the vanishing gradient problem.\n",
    "\n",
    "- **Forget gate:** decides what to discard from the cell state.\n",
    "- **Input gate:** decides what new information to store.\n",
    "- **Output gate:** decides what to output based on the cell state.\n",
    "\n",
    "The equations are more complex, but conceptually, LSTMs can learn which past information is relevant and for how long to keep it.\n",
    "\n",
    "### **26.3.1 Gated Recurrent Units (GRU)**\n",
    "\n",
    "GRUs are a simplified version of LSTMs with two gates (reset and update). They have fewer parameters and often perform similarly to LSTMs on many tasks. GRUs are a good default for time\u2011series forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.4 Preparing Data for RNNs**\n",
    "\n",
    "RNNs expect input in the shape: `(samples, timesteps, features)`. For the NEPSE dataset, we need to create sequences of past returns (and possibly other features) to predict the next value.\n",
    "\n",
    "Suppose we decide to use a window of 20 past days to predict the next day's return. For each day `t`, we create a sequence of the previous 20 days' returns (and possibly other features like volume, RSI, etc.). The target is the return at day `t+1`.\n",
    "\n",
    "### **26.4.1 Creating Sequences**\n",
    "\n",
    "We'll write a function to generate these sequences from our DataFrame.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "\n",
    "# Load and prepare NEPSE data (single symbol)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Compute returns\n",
    "df_stock['Return'] = df_stock['Close'].pct_change() * 100\n",
    "\n",
    "# Optionally, add other features (e.g., volume change, RSI, etc.)\n",
    "# For simplicity, we'll start with only returns\n",
    "feature_columns = ['Return']  # we can add more later\n",
    "\n",
    "# Drop NaN (first row of returns)\n",
    "df_stock = df_stock.dropna(subset=feature_columns)\n",
    "\n",
    "# Function to create sequences\n",
    "def create_sequences(data, seq_length, target_col_idx=0):\n",
    "    \"\"\"\n",
    "    data: numpy array of shape (n_samples, n_features)\n",
    "    seq_length: number of past steps to use\n",
    "    target_col_idx: which column is the target (0 for first column)\n",
    "    Returns:\n",
    "        X: (n_samples - seq_length, seq_length, n_features)\n",
    "        y: (n_samples - seq_length,)\n",
    "    \"\"\"\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - seq_length):\n",
    "        X.append(data[i:i+seq_length, :])          # all features\n",
    "        y.append(data[i+seq_length, target_col_idx]) # target (next value)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# Choose sequence length (e.g., 20 days)\n",
    "seq_length = 20\n",
    "\n",
    "# Get data as numpy array (only the feature columns)\n",
    "data = df_stock[feature_columns].values\n",
    "\n",
    "# Create sequences\n",
    "X, y = create_sequences(data, seq_length)\n",
    "\n",
    "print(f\"X shape: {X.shape}\")  # (samples, timesteps, features)\n",
    "print(f\"y shape: {y.shape}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `create_sequences` slides a window of length `seq_length` over the data. For each window, it takes all features in that window as input `X`, and the next value (target column) as output `y`.\n",
    "- This produces a 3D array `X` of shape `(n_samples - seq_length, seq_length, n_features)`.\n",
    "- We must ensure we do not use future information; the sequences are constructed only from past data relative to each prediction point.\n",
    "\n",
    "### **26.4.2 Train/Test Split (Temporal)**\n",
    "\n",
    "We need to split the sequences temporally. Since the sequences are overlapping, we cannot randomly shuffle; we must take the first part as training, later part as test.\n",
    "\n",
    "```python\n",
    "# Number of samples\n",
    "n_samples = X.shape[0]\n",
    "split_idx = int(n_samples * 0.8)\n",
    "\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "print(f\"Train samples: {X_train.shape[0]}, Test samples: {X_test.shape[0]}\")\n",
    "```\n",
    "\n",
    "### **26.4.3 Scaling**\n",
    "\n",
    "RNNs also benefit from scaled inputs. However, we must be careful: scaling should be applied per feature across time, but using a scaler fitted on the training set **only**. Importantly, we should **not** scale across the time dimension; we scale each feature independently.\n",
    "\n",
    "We can reshape the data to 2D (samples * timesteps, features), fit the scaler, then reshape back.\n",
    "\n",
    "```python\n",
    "# Reshape to 2D: (samples * timesteps, features)\n",
    "original_shape = X_train.shape\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "\n",
    "# Fit scaler on training 2D data\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled_2d = scaler.fit_transform(X_train_2d)\n",
    "\n",
    "# Reshape back to 3D\n",
    "X_train_scaled = X_train_scaled_2d.reshape(original_shape)\n",
    "\n",
    "# Transform test data using same scaler\n",
    "X_test_2d = X_test.reshape(-1, X_test.shape[-1])\n",
    "X_test_scaled_2d = scaler.transform(X_test_2d)\n",
    "X_test_scaled = X_test_scaled_2d.reshape(X_test.shape)\n",
    "\n",
    "print(f\"Scaled X_train shape: {X_train_scaled.shape}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We reshape to 2D so that the scaler sees all time steps as independent observations. This is valid because we assume the distribution of each feature is stationary across time (for the training period). This is a common approach.\n",
    "- Alternatively, we could scale each feature using its mean and std across the entire training set without reshaping; reshaping achieves the same result.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.5 Building an LSTM Model in Keras**\n",
    "\n",
    "We'll build a simple LSTM model with one hidden layer.\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.LSTM(50, activation='tanh', return_sequences=False, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n",
    "    layers.Dense(1)  # output layer for regression\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model.summary()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- `LSTM(50, ...)` creates an LSTM layer with 50 units. The default activation for the recurrent step is `tanh`, and for the output it's also `tanh` (but we use a separate Dense layer for the final output).\n",
    "- `return_sequences=False` means this layer returns only the last output (after processing the whole sequence). This is typical for many\u2011to\u2011one forecasting.\n",
    "- `input_shape` is `(timesteps, n_features)`.\n",
    "- The output `Dense(1)` produces the predicted return.\n",
    "\n",
    "### **26.5.1 Training with Validation**\n",
    "\n",
    "We'll use a validation split that respects time order. Since our data is already in temporal order, we can use `validation_split` (which takes the last part) or manually create a validation set from the end of the training data.\n",
    "\n",
    "```python\n",
    "# Use last 10% of training as validation\n",
    "val_size = int(len(X_train_scaled) * 0.1)\n",
    "X_val = X_train_scaled[-val_size:]\n",
    "y_val = y_train[-val_size:]\n",
    "X_train_final = X_train_scaled[:-val_size]\n",
    "y_train_final = y_train[:-val_size]\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train_final, y_train_final,\n",
    "    validation_data=(X_val, y_val),\n",
    "    epochs=50,\n",
    "    batch_size=32,\n",
    "    verbose=0\n",
    ")\n",
    "\n",
    "# Plot loss\n",
    "plt.figure(figsize=(10,4))\n",
    "plt.plot(history.history['loss'], label='train_loss')\n",
    "plt.plot(history.history['val_loss'], label='val_loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss (MSE)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We manually split the training data into a final training set and a validation set, preserving temporal order. This ensures that validation data is always later than training data.\n",
    "- Training for 50 epochs; we can add early stopping.\n",
    "\n",
    "### **26.5.2 Evaluation on Test Set**\n",
    "\n",
    "```python\n",
    "y_pred = model.predict(X_test_scaled).flatten()\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "print(f\"LSTM Test RMSE: {rmse:.4f}\")\n",
    "\n",
    "# Compare with a simple baseline (e.g., predicting 0)\n",
    "baseline_rmse = np.sqrt(mean_squared_error(y_test, np.zeros_like(y_test)))\n",
    "print(f\"Baseline (predict 0) RMSE: {baseline_rmse:.4f}\")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **26.6 Stacked RNNs**\n",
    "\n",
    "We can stack multiple LSTM layers to learn higher\u2011level temporal abstractions. When stacking, intermediate layers must return sequences (`return_sequences=True`) so that the next LSTM layer receives a sequence.\n",
    "\n",
    "```python\n",
    "model_stacked = keras.Sequential([\n",
    "    layers.LSTM(50, return_sequences=True, input_shape=(X_train_scaled.shape[1], X_train_scaled.shape[2])),\n",
    "    layers.LSTM(50, return_sequences=False),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "model_stacked.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "model_stacked.summary()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The first LSTM returns a sequence (same length as input) to feed into the second LSTM.\n",
    "- The second LSTM returns only the last output.\n",
    "- Stacking increases capacity but also risk of overfitting; use with sufficient data and regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.7 Bidirectional RNNs**\n",
    "\n",
    "A bidirectional RNN runs two independent RNNs \u2013 one forward through the sequence, one backward \u2013 and concatenates their outputs. This allows the network to use both past and future context. However, for time\u2011series forecasting, using future context at prediction time is impossible (would be look\u2011ahead). Therefore, **bidirectional RNNs are not suitable for forecasting** where we only have past data. They are useful for tasks like sequence labeling (where the whole sequence is available) or for imputation, but not for predicting the next step. We mention it for completeness and to avoid misapplication.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.8 Training Tips for RNNs**\n",
    "\n",
    "### **26.8.1 Sequence Length**\n",
    "\n",
    "Choosing the sequence length is crucial. Too short, and the model may miss important longer\u2011term dependencies. Too long, and it may overfit or be computationally expensive. For daily stock returns, common lengths are 20 (one trading month) to 60 (one quarter). Experiment with different values and validate.\n",
    "\n",
    "### **26.8.2 Stateful RNNs**\n",
    "\n",
    "By default, Keras RNNs are **stateless**: the hidden state is reset after each batch. For very long sequences, you can use **stateful** RNNs where the state persists across batches, but this requires careful data handling and is rarely needed for typical time\u2011series forecasting where we use fixed\u2011length windows.\n",
    "\n",
    "### **26.8.3 Regularization**\n",
    "\n",
    "- **Dropout:** Apply dropout to inputs or recurrent connections. In Keras, LSTM has `dropout` (for input) and `recurrent_dropout` (for recurrent state). Use small values (0.2\u20110.3).\n",
    "- **Weight regularization:** Add L1/L2 penalties to kernels.\n",
    "- **Early stopping** as always.\n",
    "\n",
    "```python\n",
    "model = keras.Sequential([\n",
    "    layers.LSTM(50, dropout=0.2, recurrent_dropout=0.2, return_sequences=False, input_shape=(timesteps, n_features)),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "```\n",
    "\n",
    "### **26.8.4 Learning Rate and Optimizer**\n",
    "\n",
    "Adam with default settings usually works well. You can also try RMSprop.\n",
    "\n",
    "### **26.8.5 Scaling Targets**\n",
    "\n",
    "If the target (returns) has a very wide range, scaling it (e.g., to zero mean unit variance) can help. However, for returns, it's usually fine to leave as is.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.9 Example: Multi\u2011step Ahead Forecasting**\n",
    "\n",
    "Sometimes we want to predict multiple steps ahead (e.g., next 5 days). There are several strategies:\n",
    "\n",
    "- **Direct multi\u2011step:** Train a model to predict t+1, another for t+2, etc. (or a model with multiple outputs).\n",
    "- **Recursive:** Use the one\u2011step model iteratively, feeding its predictions back as input for the next step.\n",
    "- **Sequence\u2011to\u2011sequence:** Encode the input sequence and decode a sequence of future values (encoder\u2011decoder).\n",
    "\n",
    "We'll demonstrate a simple direct multi\u2011step with multiple outputs.\n",
    "\n",
    "```python\n",
    "# Create target for next 5 days\n",
    "y_multi = []\n",
    "for i in range(1, 6):\n",
    "    y_multi.append(df_stock['Return'].shift(-i).values)\n",
    "\n",
    "# Stack into array (samples, 5)\n",
    "y_multi = np.column_stack(y_multi)\n",
    "\n",
    "# Remove the last 5 rows where target is NaN\n",
    "y_multi = y_multi[:-5] if seq_length else y_multi\n",
    "\n",
    "# Align X (must also remove last 5 rows)\n",
    "X_multi = data[:len(y_multi)]\n",
    "\n",
    "# Create sequences (same function, but now y is multi-dimensional)\n",
    "X_multi_seq, y_multi_seq = create_sequences_multi(X_multi, y_multi, seq_length)\n",
    "# We need a modified create_sequences that handles multi-dimensional y\n",
    "# (simplified: assume y_multi is aligned with the end of each window)\n",
    "\n",
    "# Build model with 5 output units\n",
    "model_multi = keras.Sequential([\n",
    "    layers.LSTM(50, input_shape=(seq_length, n_features)),\n",
    "    layers.Dense(5)  # 5 outputs\n",
    "])\n",
    "model_multi.compile(optimizer='adam', loss='mse')\n",
    "model_multi.fit(...)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The model outputs 5 values simultaneously, predicting the next 5 days. This is a direct multi\u2011step approach.\n",
    "- The loss is MSE computed across all 5 steps.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.10 Comparison with MLP and Other Models**\n",
    "\n",
    "We can compare the LSTM with a simple MLP that uses lagged features as input (like in Chapter 25). Typically, on the NEPSE dataset, the LSTM might perform slightly better if there are meaningful sequential patterns, but the difference may not be huge due to the noisy nature of returns. The key advantage of RNNs is their ability to learn from sequences of arbitrary length without explicitly creating many lag features.\n",
    "\n",
    "```python\n",
    "# Build an MLP on the same data (but using flattened sequences)\n",
    "X_train_flat = X_train_scaled.reshape(X_train_scaled.shape[0], -1)\n",
    "X_test_flat = X_test_scaled.reshape(X_test_scaled.shape[0], -1)\n",
    "\n",
    "mlp = keras.Sequential([\n",
    "    layers.Dense(64, activation='relu', input_shape=(X_train_flat.shape[1],)),\n",
    "    layers.Dense(32, activation='relu'),\n",
    "    layers.Dense(1)\n",
    "])\n",
    "mlp.compile(optimizer='adam', loss='mse')\n",
    "mlp.fit(X_train_flat, y_train, validation_split=0.1, epochs=50, batch_size=32, verbose=0)\n",
    "\n",
    "y_pred_mlp = mlp.predict(X_test_flat).flatten()\n",
    "rmse_mlp = np.sqrt(mean_squared_error(y_test, y_pred_mlp))\n",
    "print(f\"MLP Test RMSE: {rmse_mlp:.4f}\")\n",
    "print(f\"LSTM Test RMSE: {rmse:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The MLP sees the same sequence but as a flat vector; it has no notion of temporal order. The LSTM explicitly models the order.\n",
    "- In practice, the LSTM may have a slight edge, especially with longer sequences.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.11 Practical Considerations for NEPSE**\n",
    "\n",
    "- **Data size:** For a single stock, we may have only a few thousand trading days. LSTMs typically require more data; consider pooling multiple stocks or using smaller models.\n",
    "- **Feature engineering:** While LSTMs can learn from raw sequences, providing additional features (volume, technical indicators) often helps. Include them in the feature dimension.\n",
    "- **Regularization is crucial:** Use dropout, recurrent dropout, and early stopping.\n",
    "- **Walk\u2011forward validation:** Use time\u2011series CV to tune hyperparameters (sequence length, units, dropout). Implement a manual loop over expanding windows.\n",
    "- **Benchmarking:** Always compare with simpler models (ARIMA, linear regression, MLP) to justify the added complexity.\n",
    "\n",
    "---\n",
    "\n",
    "## **26.12 Chapter Summary**\n",
    "\n",
    "In this chapter, we explored Recurrent Neural Networks for time\u2011series forecasting, using the NEPSE dataset as an example.\n",
    "\n",
    "- **RNN basics:** They maintain a hidden state to process sequences, but simple RNNs suffer from vanishing gradients.\n",
    "- **LSTM and GRU** are gated architectures that can learn long\u2011term dependencies.\n",
    "- **Data preparation:** We must create sequences of shape `(samples, timesteps, features)`.\n",
    "- **Building models in Keras:** We implemented an LSTM for one\u2011step ahead return prediction, and discussed multi\u2011step strategies.\n",
    "- **Stacked and bidirectional RNNs:** Stacking increases capacity; bidirectional is not suitable for forecasting.\n",
    "- **Training tips:** Scale data, use dropout, early stopping, and proper temporal validation.\n",
    "- **Comparison:** LSTMs may outperform MLPs if sequential patterns exist, but require careful tuning.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Start with a simple LSTM with one hidden layer, using a sequence length of 20\u201160 days.\n",
    "- Include additional features (volume, RSI, volatility) as extra channels.\n",
    "- Use walk\u2011forward validation to tune hyperparameters and avoid overfitting.\n",
    "- Regularize heavily; financial returns are noisy, and LSTMs can easily overfit.\n",
    "- Compare with MLP and ARIMA to determine if the sequential modeling adds value.\n",
    "\n",
    "In the next chapter, **Chapter 27: Convolutional Neural Networks for Time\u2011Series**, we will see how CNNs can also be applied to sequential data, often with competitive results and faster training.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 26**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='25. neural_network_fundamentals.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='27. convolutional_neural_networks_for_time_series.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}