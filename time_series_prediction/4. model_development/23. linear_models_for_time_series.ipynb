{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 23: Linear Models for Time‑Series**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamentals of linear regression and its assumptions\n",
    "- Apply ordinary least squares (OLS) to time‑series data and interpret coefficients\n",
    "- Recognize the limitations of OLS in the presence of multicollinearity and high dimensionality\n",
    "- Implement Ridge (L2) and Lasso (L1) regularization to improve generalization\n",
    "- Use Elastic Net to combine the benefits of Ridge and Lasso\n",
    "- Build polynomial regression models to capture non‑linear trends\n",
    "- Apply generalized linear models (GLMs) for non‑normal targets (e.g., binary direction)\n",
    "- Use regularization for feature selection in the NEPSE prediction system\n",
    "- Diagnose model fit through residual analysis and hypothesis tests\n",
    "- Understand the role of linear models as interpretable baselines in time‑series forecasting\n",
    "\n",
    "---\n",
    "\n",
    "## **23.1 Linear Regression Fundamentals**\n",
    "\n",
    "Linear regression is one of the simplest and most interpretable models in machine learning. It assumes a linear relationship between the input features `X` and the target `y`:\n",
    "\n",
    "`y = β₀ + β₁x₁ + β₂x₂ + ... + βₚxₚ + ε`\n",
    "\n",
    "where `ε` is the error term. The coefficients `β` are estimated by minimizing the sum of squared residuals (ordinary least squares, OLS).\n",
    "\n",
    "In the context of the NEPSE prediction system, we might try to predict tomorrow's return using today's features (lagged returns, volume, technical indicators) with a linear model. While the true relationship is unlikely to be perfectly linear, linear models provide a simple, interpretable baseline.\n",
    "\n",
    "### **23.1.1 Assumptions of Linear Regression**\n",
    "\n",
    "1. **Linearity:** The relationship between features and target is linear.\n",
    "2. **Independence:** Observations are independent (often violated in time‑series).\n",
    "3. **Homoscedasticity:** Constant variance of errors.\n",
    "4. **Normality:** Errors are normally distributed (for inference, not required for prediction).\n",
    "5. **No perfect multicollinearity:** Features are not perfectly correlated.\n",
    "\n",
    "Time‑series data typically violates the independence assumption (autocorrelation) and may exhibit heteroscedasticity (volatility clustering). Despite these violations, linear models can still produce reasonable forecasts, but we must be cautious with inference.\n",
    "\n",
    "### **23.1.2 Applying OLS to NEPSE Data**\n",
    "\n",
    "We'll use the same feature set as in Chapter 22 (lagged returns, moving averages, RSI, etc.) to predict next day's return.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# Load and prepare data (same as Chapter 22)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Use a single symbol\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Create features (simplified set for clarity)\n",
    "df_stock['Return'] = df_stock['Close'].pct_change() * 100\n",
    "df_stock['Return_Lag1'] = df_stock['Return'].shift(1)\n",
    "df_stock['Return_Lag2'] = df_stock['Return'].shift(2)\n",
    "df_stock['Volume_Lag1'] = df_stock['Vol'].shift(1)\n",
    "df_stock['MA_5'] = df_stock['Close'].rolling(5).mean()\n",
    "df_stock['Volatility_5'] = df_stock['Return'].rolling(5).std()\n",
    "df_stock['RSI'] = 100 - (100 / (1 + (df_stock['Close'].diff().where(lambda x: x>0, 0).rolling(14).mean() / \n",
    "                                     (-df_stock['Close'].diff().where(lambda x: x<0, 0).rolling(14).mean()))))\n",
    "\n",
    "# Target: next day's return\n",
    "df_stock['Target'] = df_stock['Return'].shift(-1)\n",
    "\n",
    "# Drop NaN\n",
    "df_stock = df_stock.dropna()\n",
    "\n",
    "# Features and target\n",
    "feature_cols = ['Return_Lag1', 'Return_Lag2', 'Volume_Lag1', 'MA_5', 'Volatility_5', 'RSI']\n",
    "X = df_stock[feature_cols]\n",
    "y = df_stock['Target']\n",
    "\n",
    "# Temporal split\n",
    "split_idx = int(len(X) * 0.8)\n",
    "X_train, X_test = X.iloc[:split_idx], X.iloc[split_idx:]\n",
    "y_train, y_test = y.iloc[:split_idx], y.iloc[split_idx:]\n",
    "\n",
    "# Fit OLS using scikit-learn\n",
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train = lr.predict(X_train)\n",
    "y_pred_test = lr.predict(X_test)\n",
    "\n",
    "print(f\"Train RMSE: {np.sqrt(mean_squared_error(y_train, y_pred_train)):.4f}\")\n",
    "print(f\"Test RMSE: {np.sqrt(mean_squared_error(y_test, y_pred_test)):.4f}\")\n",
    "\n",
    "# Coefficients\n",
    "coef_df = pd.DataFrame({'feature': feature_cols, 'coefficient': lr.coef_})\n",
    "print(coef_df)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We use a simple set of features to demonstrate linear regression. The model is trained on 80% of the data (temporally split) and evaluated on the most recent 20%.\n",
    "- The RMSE gives a baseline. Later we'll compare with regularized versions.\n",
    "- The coefficients show the estimated effect of each feature. For example, a positive coefficient on `Return_Lag1` would indicate that higher returns yesterday tend to lead to higher returns today (momentum), while a negative coefficient would suggest mean reversion.\n",
    "- Note that these coefficients are conditional on the other features in the model.\n",
    "\n",
    "### **23.1.3 Statistical Inference with OLS**\n",
    "\n",
    "For inference (confidence intervals, p‑values), we can use `statsmodels` which provides detailed regression output.\n",
    "\n",
    "```python\n",
    "# Add constant for intercept\n",
    "X_train_const = sm.add_constant(X_train)\n",
    "X_test_const = sm.add_constant(X_test)\n",
    "\n",
    "# Fit OLS using statsmodels\n",
    "ols_model = sm.OLS(y_train, X_train_const).fit()\n",
    "print(ols_model.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The summary includes R², adjusted R², F‑statistic, and coefficients with standard errors, t‑stats, and p‑values.\n",
    "- A low p‑value (<0.05) suggests the feature is statistically significant. However, in time‑series, these p‑values may be biased due to autocorrelation.\n",
    "- The Durbin‑Watson statistic (around 2) indicates no autocorrelation in residuals; values far from 2 suggest autocorrelation, which would violate the independence assumption.\n",
    "\n",
    "### **23.1.4 Limitations of OLS in Time‑Series**\n",
    "\n",
    "- **Multicollinearity:** Features like `MA_5` and `RSI` may be correlated, inflating standard errors.\n",
    "- **Non‑stationarity:** If the target or features are non‑stationary, coefficients may be unstable.\n",
    "- **Autocorrelation:** Residuals may be correlated over time, leading to inefficient estimates.\n",
    "- **Overfitting:** With many features, OLS can overfit, especially if the number of features is large relative to samples.\n",
    "\n",
    "These limitations motivate regularized regression methods.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.2 Ridge Regression (L2)**\n",
    "\n",
    "Ridge regression adds a penalty term to the OLS objective: it minimizes the sum of squared residuals plus `α` times the sum of squared coefficients (L2 norm). This shrinks coefficients toward zero but does not set them exactly to zero. Ridge is effective when there are many correlated features.\n",
    "\n",
    "**Objective:** minimize `∑(yᵢ - ŷᵢ)² + α ∑βⱼ²`\n",
    "\n",
    "The hyperparameter `α` controls the strength of regularization. As `α` increases, coefficients shrink more.\n",
    "\n",
    "### **23.2.1 Implementing Ridge with scikit‑learn**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Ridge regression\n",
    "ridge = Ridge()\n",
    "param_grid = {'alpha': [0.01, 0.1, 1.0, 10.0, 100.0]}\n",
    "# Use time-series cross-validation\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "grid_ridge = GridSearchCV(ridge, param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
    "grid_ridge.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best alpha: {grid_ridge.best_params_['alpha']:.4f}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-grid_ridge.best_score_):.4f}\")\n",
    "\n",
    "# Evaluate on test\n",
    "y_pred_ridge = grid_ridge.predict(X_test)\n",
    "rmse_ridge = np.sqrt(mean_squared_error(y_test, y_pred_ridge))\n",
    "print(f\"Test RMSE: {rmse_ridge:.4f}\")\n",
    "\n",
    "# Coefficients\n",
    "ridge_best = grid_ridge.best_estimator_\n",
    "coef_ridge = pd.DataFrame({'feature': feature_cols, 'coefficient': ridge_best.coef_})\n",
    "print(coef_ridge)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We use `GridSearchCV` with `TimeSeriesSplit` to tune `α`. The CV folds respect temporal order.\n",
    "- The best `α` is chosen based on average negative MSE (converted to RMSE).\n",
    "- Ridge coefficients are smaller than OLS coefficients, and some may be nearly zero but not exactly zero.\n",
    "\n",
    "### **23.2.2 Effect of Regularization**\n",
    "\n",
    "As `α` increases, coefficients shrink. This reduces variance but may increase bias. The optimal `α` balances bias and variance, leading to better out‑of‑sample performance.\n",
    "\n",
    "We can visualize the coefficient paths:\n",
    "\n",
    "```python\n",
    "alphas = np.logspace(-2, 2, 50)\n",
    "coefs = []\n",
    "for a in alphas:\n",
    "    ridge = Ridge(alpha=a)\n",
    "    ridge.fit(X_train, y_train)\n",
    "    coefs.append(ridge.coef_)\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(len(feature_cols)):\n",
    "    plt.plot(alphas, [c[i] for c in coefs], label=feature_cols[i])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficient')\n",
    "plt.title('Ridge coefficient paths')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- As `α` increases, coefficients smoothly shrink toward zero. This plot helps see which features are most affected by regularization.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.3 Lasso Regression (L1)**\n",
    "\n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) adds an L1 penalty: `α ∑|βⱼ|`. This penalty can shrink some coefficients exactly to zero, effectively performing feature selection. Lasso is useful when we suspect only a subset of features are relevant.\n",
    "\n",
    "**Objective:** minimize `∑(yᵢ - ŷᵢ)² + α ∑|βⱼ|`\n",
    "\n",
    "### **23.3.1 Implementing Lasso with scikit‑learn**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso\n",
    "\n",
    "lasso = Lasso(max_iter=10000)\n",
    "param_grid = {'alpha': [0.001, 0.01, 0.1, 1.0, 10.0]}\n",
    "\n",
    "grid_lasso = GridSearchCV(lasso, param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
    "grid_lasso.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best alpha: {grid_lasso.best_params_['alpha']:.4f}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-grid_lasso.best_score_):.4f}\")\n",
    "\n",
    "y_pred_lasso = grid_lasso.predict(X_test)\n",
    "rmse_lasso = np.sqrt(mean_squared_error(y_test, y_pred_lasso))\n",
    "print(f\"Test RMSE: {rmse_lasso:.4f}\")\n",
    "\n",
    "# Coefficients (many will be zero)\n",
    "lasso_best = grid_lasso.best_estimator_\n",
    "coef_lasso = pd.DataFrame({'feature': feature_cols, 'coefficient': lasso_best.coef_})\n",
    "print(coef_lasso[coef_lasso['coefficient'] != 0])\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Lasso may set some coefficients to exactly zero, indicating those features are not selected. This provides a form of automatic feature selection.\n",
    "- The remaining non‑zero coefficients are the most important predictors.\n",
    "- For the NEPSE data, Lasso might retain only a few features like `Return_Lag1` and `Volatility_5`, discarding others.\n",
    "\n",
    "### **23.3.2 Lasso Path**\n",
    "\n",
    "We can visualize how coefficients change with `α`:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import lasso_path\n",
    "\n",
    "alphas_lasso, coefs_lasso, _ = lasso_path(X_train, y_train, alphas=np.logspace(-3, 1, 50))\n",
    "plt.figure(figsize=(10,6))\n",
    "for i in range(len(feature_cols)):\n",
    "    plt.plot(alphas_lasso, coefs_lasso[i, :], label=feature_cols[i])\n",
    "plt.xscale('log')\n",
    "plt.xlabel('alpha')\n",
    "plt.ylabel('coefficient')\n",
    "plt.title('Lasso path')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- As `α` increases, coefficients drop to zero at different rates. The order in which they drop indicates feature importance.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.4 Elastic Net**\n",
    "\n",
    "Elastic Net combines L1 and L2 penalties, controlled by a mixing parameter `l1_ratio`. It can select groups of correlated features (unlike Lasso, which picks one from a correlated group) and still perform feature selection.\n",
    "\n",
    "**Objective:** minimize `∑(yᵢ - ŷᵢ)² + α * (l1_ratio * ∑|βⱼ| + (1-l1_ratio)/2 * ∑βⱼ²)`\n",
    "\n",
    "When `l1_ratio = 1`, it's Lasso; when `l1_ratio = 0`, it's Ridge. Values in between give a mix.\n",
    "\n",
    "### **23.4.1 Implementing Elastic Net**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNet\n",
    "\n",
    "elastic = ElasticNet(max_iter=10000)\n",
    "param_grid = {\n",
    "    'alpha': [0.001, 0.01, 0.1, 1.0, 10.0],\n",
    "    'l1_ratio': [0.1, 0.3, 0.5, 0.7, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "grid_elastic = GridSearchCV(elastic, param_grid, cv=tscv, scoring='neg_mean_squared_error')\n",
    "grid_elastic.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best params: {grid_elastic.best_params_}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-grid_elastic.best_score_):.4f}\")\n",
    "\n",
    "y_pred_elastic = grid_elastic.predict(X_test)\n",
    "rmse_elastic = np.sqrt(mean_squared_error(y_test, y_pred_elastic))\n",
    "print(f\"Test RMSE: {rmse_elastic:.4f}\")\n",
    "\n",
    "elastic_best = grid_elastic.best_estimator_\n",
    "coef_elastic = pd.DataFrame({'feature': feature_cols, 'coefficient': elastic_best.coef_})\n",
    "print(coef_elastic[coef_elastic['coefficient'] != 0])\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Elastic Net often performs well when there are groups of correlated features (e.g., multiple lagged returns). It can include all of them with shrunk coefficients rather than picking one.\n",
    "- The `l1_ratio` balances between Ridge and Lasso.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.5 Polynomial Regression**\n",
    "\n",
    "Linear models can capture non‑linear relationships by including polynomial terms (e.g., `x²`, `x³`) or interactions. This is still linear in the parameters, so we can use the same estimation methods.\n",
    "\n",
    "For time‑series, polynomial terms of time can model trends. For example, we might include `day²` to capture acceleration in prices. However, extrapolating polynomials can be dangerous.\n",
    "\n",
    "### **23.5.1 Creating Polynomial Features**\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# Create a time index (days since start)\n",
    "df_stock['Day'] = (df_stock['Date'] - df_stock['Date'].min()).dt.days\n",
    "\n",
    "# Select base features (include time)\n",
    "base_features = ['Day', 'Return_Lag1', 'Volume_Lag1']\n",
    "X_base = df_stock[base_features].dropna()  # ensure alignment\n",
    "\n",
    "# Generate polynomial features up to degree 2\n",
    "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
    "X_poly = poly.fit_transform(X_base)\n",
    "feature_names_poly = poly.get_feature_names_out(base_features)\n",
    "\n",
    "# Convert to DataFrame\n",
    "X_poly_df = pd.DataFrame(X_poly, columns=feature_names_poly, index=X_base.index)\n",
    "\n",
    "# Merge back with target (aligning indices)\n",
    "df_model = df_stock.loc[X_poly_df.index].copy()\n",
    "y_poly = df_model['Target']\n",
    "\n",
    "# Train/test split (temporal)\n",
    "split_idx_poly = int(len(X_poly_df) * 0.8)\n",
    "X_train_poly, X_test_poly = X_poly_df.iloc[:split_idx_poly], X_poly_df.iloc[split_idx_poly:]\n",
    "y_train_poly, y_test_poly = y_poly.iloc[:split_idx_poly], y_poly.iloc[split_idx_poly:]\n",
    "\n",
    "# Fit linear regression on polynomial features\n",
    "lr_poly = LinearRegression()\n",
    "lr_poly.fit(X_train_poly, y_train_poly)\n",
    "\n",
    "y_pred_poly = lr_poly.predict(X_test_poly)\n",
    "rmse_poly = np.sqrt(mean_squared_error(y_test_poly, y_pred_poly))\n",
    "print(f\"Polynomial regression test RMSE: {rmse_poly:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We include a time trend `Day` and its square to capture non‑linear trends. The square term allows the model to fit a parabolic trend.\n",
    "- Interaction terms like `Day * Return_Lag1` are also created, which could capture how the effect of past returns changes over time.\n",
    "- Polynomial regression can easily overfit, especially with high degrees. Regularization is recommended.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.6 Generalized Linear Models (GLMs)**\n",
    "\n",
    "GLMs extend linear regression to targets with non‑normal distributions (e.g., binary, count, positive‑valued). They consist of three components:\n",
    "\n",
    "- **Random component:** distribution of the target (e.g., Binomial for classification, Poisson for counts).\n",
    "- **Systematic component:** linear predictor `η = Xβ`.\n",
    "- **Link function:** connects the mean of the target to the linear predictor, e.g., logit for binary, log for counts.\n",
    "\n",
    "For the NEPSE system, we might use a GLM with Binomial distribution and logit link to predict the probability of an up move (logistic regression). This is a special case of GLM.\n",
    "\n",
    "### **23.6.1 Logistic Regression for Direction Prediction**\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Prepare binary target\n",
    "y_binary = (df_stock['Target'] > 0).astype(int)\n",
    "\n",
    "# Align with features (ensure same index as X)\n",
    "# Use same feature set as before (without polynomial)\n",
    "X_binary = X  # from earlier\n",
    "y_binary = y_binary.loc[X_binary.index]\n",
    "\n",
    "# Temporal split\n",
    "X_train_bin, X_test_bin = X_binary.iloc[:split_idx], X_binary.iloc[split_idx:]\n",
    "y_train_bin, y_test_bin = y_binary.iloc[:split_idx], y_binary.iloc[split_idx:]\n",
    "\n",
    "# Logistic regression with L2 regularization (default)\n",
    "logreg = LogisticRegression(penalty='l2', C=1.0, solver='lbfgs', max_iter=1000)\n",
    "logreg.fit(X_train_bin, y_train_bin)\n",
    "\n",
    "y_pred_bin = logreg.predict(X_test_bin)\n",
    "accuracy = accuracy_score(y_test_bin, y_pred_bin)\n",
    "print(f\"Logistic regression accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Coefficients\n",
    "coef_log = pd.DataFrame({'feature': feature_cols, 'coefficient': logreg.coef_[0]})\n",
    "print(coef_log)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Logistic regression models the log‑odds of an up move as a linear function of the features.\n",
    "- The coefficients indicate how a one‑unit change in a feature affects the log‑odds (and thus the probability).\n",
    "- Regularization parameter `C` is the inverse of `α`; smaller `C` means stronger regularization. We could tune it via cross‑validation.\n",
    "\n",
    "### **23.6.2 Other GLMs**\n",
    "\n",
    "For count data (e.g., number of trades), we might use Poisson regression. For positive‑valued targets (e.g., volatility), Gamma regression with log link could be appropriate. `statsmodels` provides extensive GLM capabilities.\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "import statsmodels.formula.api as smf\n",
    "\n",
    "# Example: Poisson regression (not directly applicable to NEPSE, but for illustration)\n",
    "# Assume we have a count variable 'Transactions' (Trans.)\n",
    "df_stock['Trans'] = df_stock['Trans.'].fillna(0).astype(int)\n",
    "# Use log link\n",
    "poisson_model = sm.GLM(df_stock['Trans'], sm.add_constant(df_stock[['Return_Lag1']]), \n",
    "                        family=sm.families.Poisson()).fit()\n",
    "print(poisson_model.summary())\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **23.7 Regularization Strategies**\n",
    "\n",
    "Regularization is essential when using many features, especially in time‑series where the risk of overfitting is high. The main strategies are:\n",
    "\n",
    "- **Ridge (L2):** Shrinks coefficients, good for many small/medium effects.\n",
    "- **Lasso (L1):** Performs feature selection, good when only a few features matter.\n",
    "- **Elastic Net:** Compromise, handles correlated features well.\n",
    "\n",
    "### **23.7.1 Choosing the Regularization Strength**\n",
    "\n",
    "We use time‑series cross‑validation to select `α` (and `l1_ratio` for Elastic Net). The example above with `GridSearchCV` and `TimeSeriesSplit` demonstrates this.\n",
    "\n",
    "### **23.7.2 Scaling Features**\n",
    "\n",
    "Regularization methods are sensitive to the scale of features. Always standardize features (center and scale) before applying Ridge, Lasso, or Elastic Net.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Now use scaled data in regularized models\n",
    "ridge_scaled = Ridge(alpha=1.0)\n",
    "ridge_scaled.fit(X_train_scaled, y_train)\n",
    "# ... etc.\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Without scaling, features with larger magnitudes would be penalized more, which is undesirable.\n",
    "- The scaler is fitted on the training set only and applied to the test set to avoid leakage.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.8 Feature Selection with Linear Models**\n",
    "\n",
    "Linear models with L1 regularization (Lasso) provide an embedded feature selection method. The features with non‑zero coefficients are the selected ones. This can be used to reduce dimensionality before applying other models.\n",
    "\n",
    "```python\n",
    "# After fitting Lasso with optimal alpha\n",
    "selected_features = feature_cols[lasso_best.coef_ != 0]\n",
    "print(f\"Selected features: {selected_features}\")\n",
    "\n",
    "# Now train a model (e.g., Random Forest) on only these features\n",
    "X_train_sel = X_train[selected_features]\n",
    "X_test_sel = X_test[selected_features]\n",
    "# ... train another model\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- This two‑step approach (Lasso for selection, then another model) can be effective, but care must be taken to avoid selection bias. The selection should be done within cross‑validation loops to get unbiased performance estimates.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.9 Time‑Series Linear Models**\n",
    "\n",
    "Some linear models are specifically designed for time‑series:\n",
    "\n",
    "- **Autoregressive (AR) models:** Essentially linear regression on lagged values of the target. We covered this in Chapter 21.\n",
    "- **ARIMAX / SARIMAX:** ARIMA with exogenous variables (features). These can be seen as linear models with time‑series errors.\n",
    "\n",
    "### **23.9.1 ARIMAX with Exogenous Features**\n",
    "\n",
    "We can include our engineered features as exogenous variables in an ARIMA model. This combines the time‑series dynamics with external predictors.\n",
    "\n",
    "```python\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "# Use the same features (scaled) as exogenous\n",
    "exog_train = X_train_scaled\n",
    "exog_test = X_test_scaled\n",
    "\n",
    "# Fit ARIMAX: order (p,d,q) on the target (returns) with exog\n",
    "# We'll use a simple AR(1) as example, but could tune p,d,q\n",
    "arimax_model = ARIMA(y_train, order=(1,0,0), exog=exog_train)\n",
    "arimax_fit = arimax_model.fit()\n",
    "print(arimax_fit.summary())\n",
    "\n",
    "# Forecast\n",
    "forecast = arimax_fit.forecast(steps=len(y_test), exog=exog_test)\n",
    "rmse_arimax = np.sqrt(mean_squared_error(y_test, forecast))\n",
    "print(f\"ARIMAX test RMSE: {rmse_arimax:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- ARIMAX models the target as a linear function of its own lags and the exogenous variables. The errors follow an ARMA process.\n",
    "- This can capture autocorrelation that a pure linear regression on features misses.\n",
    "- However, it is more complex to estimate and requires careful selection of ARIMA order.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.10 Interpretation and Diagnostics**\n",
    "\n",
    "Linear models are prized for interpretability. We can examine coefficients, confidence intervals, and residuals.\n",
    "\n",
    "### **23.10.1 Coefficient Interpretation**\n",
    "\n",
    "In a linear regression, a coefficient `βⱼ` represents the expected change in the target for a one‑unit change in feature `xⱼ`, holding all other features constant. For example, if `Return_Lag1` has a coefficient of 0.05, a 1% increase in yesterday's return is associated with a 0.05% increase in today's return, on average.\n",
    "\n",
    "In logistic regression, the coefficient represents the change in log‑odds of the event (up move) per unit change in the feature.\n",
    "\n",
    "### **23.10.2 Residual Diagnostics**\n",
    "\n",
    "We should check residuals for patterns that indicate model inadequacy.\n",
    "\n",
    "```python\n",
    "# Residuals from best linear model (e.g., Ridge)\n",
    "residuals = y_test - y_pred_ridge\n",
    "\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.subplot(1,2,1)\n",
    "plt.plot(residuals)\n",
    "plt.title('Residuals over time')\n",
    "\n",
    "plt.subplot(1,2,2)\n",
    "plt.scatter(y_pred_ridge, residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residuals')\n",
    "plt.title('Residuals vs Predicted')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# ACF of residuals\n",
    "from statsmodels.graphics.tsaplots import plot_acf\n",
    "plot_acf(residuals, lags=20)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- Residuals should have no trend, constant variance (homoscedasticity), and no autocorrelation.\n",
    "- If residuals show autocorrelation, the model may benefit from including lagged target terms (AR terms) or using ARIMAX.\n",
    "- If variance changes over time (volatility clustering), consider models that account for heteroscedasticity (e.g., GARCH).\n",
    "\n",
    "### **23.10.3 Hypothesis Tests (Cautions)**\n",
    "\n",
    "In time‑series, standard errors from OLS are often biased due to autocorrelation and heteroscedasticity. Use heteroscedasticity‑ and autocorrelation‑consistent (HAC) standard errors if inference is important.\n",
    "\n",
    "```python\n",
    "# Using statsmodels with robust standard errors\n",
    "model_hac = sm.OLS(y_train, X_train_const).fit(cov_type='HAC', cov_kwds={'maxlags': 5})\n",
    "print(model_hac.summary())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- The `cov_type='HAC'` option uses Newey‑West standard errors, which are robust to autocorrelation and heteroscedasticity.\n",
    "\n",
    "---\n",
    "\n",
    "## **23.11 Chapter Summary**\n",
    "\n",
    "In this chapter, we explored linear models for time‑series forecasting, with applications to the NEPSE dataset.\n",
    "\n",
    "- **Ordinary least squares (OLS)** provides a simple, interpretable baseline, but suffers from multicollinearity and overfitting with many features.\n",
    "- **Ridge regression (L2)** shrinks coefficients to reduce variance, useful when many features have small effects.\n",
    "- **Lasso (L1)** performs feature selection by setting some coefficients to zero, helpful when only a few features matter.\n",
    "- **Elastic Net** combines L1 and L2 penalties, handling correlated features well.\n",
    "- **Polynomial regression** extends linear models to capture non‑linear trends and interactions.\n",
    "- **Generalized linear models (GLMs)** like logistic regression handle non‑normal targets (e.g., binary direction).\n",
    "- Regularization strength must be tuned via time‑series cross‑validation.\n",
    "- Features should be standardized before applying regularized methods.\n",
    "- Linear models can be used for feature selection before feeding into more complex models.\n",
    "- **ARIMAX** incorporates exogenous variables into a time‑series model, capturing autocorrelation.\n",
    "- Interpretation and residual diagnostics are essential to validate model assumptions.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Start with a simple linear model as a baseline; its RMSE or accuracy sets a lower bound.\n",
    "- Use Lasso to identify the most predictive features – this can reveal market drivers.\n",
    "- Logistic regression for direction prediction gives interpretable probabilities of up/down moves.\n",
    "- Always scale features and use time‑series CV to tune regularization.\n",
    "- Combine linear models with time‑series components (ARIMAX) if residuals show autocorrelation.\n",
    "- Linear models are not the most accurate, but their transparency makes them valuable for understanding and as a benchmark.\n",
    "\n",
    "In the next chapter, **Chapter 24: Support Vector Machines**, we will explore how SVMs can capture non‑linear patterns through kernel tricks, and apply them to the NEPSE prediction problem.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 23**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
