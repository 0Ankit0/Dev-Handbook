{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 30: Model Training Best Practices\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Prepare time‑series data correctly for neural network training (batching, shuffling, normalization)\n",
    "- Choose an appropriate batch size and understand its impact on convergence\n",
    "- Implement learning rate schedules and adaptive methods to improve training\n",
    "- Select loss functions that align with your prediction task (regression, classification, probabilistic)\n",
    "- Apply early stopping to prevent overfitting and save the best model\n",
    "- Use model checkpointing to preserve training progress\n",
    "- Leverage mixed precision training to speed up computation on compatible hardware\n",
    "- Scale training across multiple GPUs with distributed strategies\n",
    "- Monitor training metrics effectively with tools like TensorBoard\n",
    "- Diagnose and debug common training issues (vanishing gradients, poor convergence, NaN losses)\n",
    "\n",
    "---\n",
    "\n",
    "## **30.1 Data Preparation**\n",
    "\n",
    "Proper data preparation is the foundation of successful model training. For time‑series forecasting, we must respect temporal order while still providing the model with enough variety to generalize.\n",
    "\n",
    "### **30.1.1 Temporal Shuffling**\n",
    "\n",
    "In standard machine learning, we shuffle the dataset to ensure that each batch represents the overall distribution. However, for time‑series, shuffling would break the temporal dependencies. Instead, we typically **do not shuffle** the training data when using RNNs or any model where the sequence order matters. For models like MLPs that use lagged features, we can shuffle because each sample is already a fixed window. But for sequences, we must maintain order.\n",
    "\n",
    "**When to shuffle:**\n",
    "- For MLPs on lagged features: shuffle is okay (each row is independent).\n",
    "- For RNNs/LSTMs/CNNs on sequences: do **not** shuffle across time; instead, we can shuffle batches **within** the training set if we ensure each batch contains complete sequences, but we must not mix sequences from different time periods. In practice, we often train without shuffling for stateful models, but for stateless models (like our sequence models), we can shuffle the **order of samples** (the windows) because each window is independent of the next window. This is acceptable as long as the windows themselves are constructed without look‑ahead.\n",
    "\n",
    "For our NEPSE sequence data, we can safely shuffle the training windows because they are independent samples (each is a fixed 20‑day window). The validation and test sets must remain in temporal order to evaluate true out‑of‑sample performance.\n",
    "\n",
    "```python\n",
    "# Example: shuffling training windows\n",
    "indices = np.arange(len(X_train))\n",
    "np.random.shuffle(indices)\n",
    "X_train_shuffled = X_train[indices]\n",
    "y_train_shuffled = y_train[indices]\n",
    "```\n",
    "\n",
    "**Explanation:** Shuffling helps break any sequential correlation between windows and leads to more stable training. However, we must ensure that windows do not overlap in time in a way that leaks information – if windows overlap heavily, shuffling might still be okay because each window is a valid input‑output pair. But if there is risk of overlapping test data, we must keep train/test strictly temporal.\n",
    "\n",
    "### **30.1.2 Batching**\n",
    "\n",
    "We feed data to the model in mini‑batches. The batch size affects training dynamics:\n",
    "\n",
    "- **Larger batches:** more stable gradient estimates, can use higher learning rates, but require more memory and may converge to sharper minima.\n",
    "- **Smaller batches:** noisier gradients, can help escape local minima, but training may be less stable.\n",
    "\n",
    "For time‑series, we often use batch sizes of 32, 64, or 128. The optimal size depends on the dataset size and model complexity.\n",
    "\n",
    "```python\n",
    "# In Keras, batch size is set during fit\n",
    "history = model.fit(X_train, y_train, batch_size=32, epochs=50)\n",
    "```\n",
    "\n",
    "### **30.1.3 Normalization/Standardization**\n",
    "\n",
    "Neural networks train best when inputs are scaled to similar ranges. For time‑series, we typically scale each feature independently using statistics computed on the **training set only**. We then apply the same transformation to validation and test sets.\n",
    "\n",
    "- **Standardization (Z‑score):** `(x - mean) / std` – good for features with different units.\n",
    "- **Min‑max scaling:** `(x - min) / (max - min)` – scales to [0,1]; sensitive to outliers.\n",
    "\n",
    "For financial returns, standardization is common because returns are already centered near zero. For prices, standardization is also used.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "# Fit on training data (2D: samples * timesteps, features)\n",
    "X_train_2d = X_train.reshape(-1, X_train.shape[-1])\n",
    "scaler.fit(X_train_2d)\n",
    "X_train_scaled = scaler.transform(X_train_2d).reshape(X_train.shape)\n",
    "X_test_scaled = scaler.transform(X_test.reshape(-1, X_test.shape[-1])).reshape(X_test.shape)\n",
    "```\n",
    "\n",
    "If the target is also scaled, remember to inverse‑transform predictions for final evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.2 Batch Size Selection**\n",
    "\n",
    "The batch size is a critical hyperparameter. Larger batches provide more accurate gradient estimates but require more memory and may lead to overfitting. Smaller batches introduce noise that can act as a regularizer.\n",
    "\n",
    "**Guidelines:**\n",
    "- Start with a batch size of 32 or 64.\n",
    "- If you have a lot of memory, try 128 or 256.\n",
    "- Monitor training and validation loss; if validation loss is much higher than training, you may be overfitting – try reducing batch size or increasing regularization.\n",
    "- For time‑series, avoid extremely large batches that may not represent the full variety of temporal patterns.\n",
    "\n",
    "```python\n",
    "# Experiment with different batch sizes\n",
    "batch_sizes = [16, 32, 64]\n",
    "histories = {}\n",
    "for bs in batch_sizes:\n",
    "    model = create_model()  # your model definition\n",
    "    history = model.fit(X_train, y_train, batch_size=bs, epochs=50, validation_data=(X_val, y_val), verbose=0)\n",
    "    histories[bs] = history.history['val_loss'][-1]  # final validation loss\n",
    "    print(f\"Batch size {bs}: final val loss = {histories[bs]:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:** This simple loop can help you choose a batch size that gives the lowest validation loss. However, batch size interacts with learning rate; often you need to adjust the learning rate when changing batch size.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.3 Learning Rate Scheduling**\n",
    "\n",
    "The learning rate determines how much we adjust the weights in response to the gradient. Too high and training diverges; too low and training stalls. Using a schedule that reduces the learning rate over time can improve convergence.\n",
    "\n",
    "### **30.3.1 Fixed Learning Rate with Decay**\n",
    "\n",
    "We can start with a higher learning rate and exponentially decay it.\n",
    "\n",
    "```python\n",
    "def lr_scheduler(epoch, lr):\n",
    "    if epoch < 10:\n",
    "        return lr\n",
    "    else:\n",
    "        return lr * tf.math.exp(-0.1)\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n",
    "model.fit(..., callbacks=[callback])\n",
    "```\n",
    "\n",
    "### **30.3.2 Reduce on Plateau**\n",
    "\n",
    "Reduce the learning rate when validation loss stops improving.\n",
    "\n",
    "```python\n",
    "reduce_lr = tf.keras.callbacks.ReduceLROnPlateau(\n",
    "    monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6\n",
    ")\n",
    "```\n",
    "\n",
    "### **30.3.3 Cyclical Learning Rates**\n",
    "\n",
    "Cyclical learning rates vary the learning rate between bounds, which can help escape local minima.\n",
    "\n",
    "```python\n",
    "# Using custom callback or tensorflow_addons\n",
    "```\n",
    "\n",
    "### **30.3.4 Adaptive Optimizers**\n",
    "\n",
    "Optimizers like Adam, RMSprop, and AdaGrad automatically adjust learning rates per parameter. They are often good defaults, but still benefit from global scheduling.\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
    "model.compile(optimizer=optimizer, loss='mse')\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **30.4 Loss Function Selection**\n",
    "\n",
    "The loss function should match the prediction task and the desired output distribution.\n",
    "\n",
    "### **30.4.1 Regression**\n",
    "\n",
    "- **Mean Squared Error (MSE):** `'mse'` – sensitive to outliers.\n",
    "- **Mean Absolute Error (MAE):** `'mae'` – more robust to outliers.\n",
    "- **Huber Loss:** combination of MSE and MAE; less sensitive to outliers.\n",
    "\n",
    "```python\n",
    "model.compile(loss='huber', optimizer='adam')\n",
    "```\n",
    "\n",
    "### **30.4.2 Classification**\n",
    "\n",
    "- **Binary crossentropy:** for binary classification (direction).\n",
    "- **Categorical crossentropy:** for multi‑class (e.g., up/flat/down).\n",
    "- **Sparse categorical crossentropy:** when labels are integers.\n",
    "\n",
    "```python\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "```\n",
    "\n",
    "### **30.4.3 Probabilistic Forecasting**\n",
    "\n",
    "Use negative log‑likelihood of a chosen distribution. For example, for Gaussian output:\n",
    "\n",
    "```python\n",
    "def negative_log_likelihood(y_true, y_pred):\n",
    "    mean = y_pred[..., 0]\n",
    "    log_std = y_pred[..., 1]\n",
    "    std = tf.exp(log_std)\n",
    "    return -tf.reduce_mean(tfp.distributions.Normal(mean, std).log_prob(y_true))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **30.5 Early Stopping**\n",
    "\n",
    "Early stopping halts training when a monitored metric (e.g., validation loss) stops improving for a certain number of epochs (patience). This prevents overfitting and saves time.\n",
    "\n",
    "```python\n",
    "early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "    monitor='val_loss',\n",
    "    patience=10,\n",
    "    restore_best_weights=True  # revert to best model\n",
    ")\n",
    "\n",
    "history = model.fit(..., callbacks=[early_stop])\n",
    "```\n",
    "\n",
    "**Explanation:** `restore_best_weights=True` ensures that after stopping, the model weights are set to the epoch with the lowest validation loss. This is crucial for getting the best model.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.6 Checkpointing**\n",
    "\n",
    "Model checkpointing saves the model (or just weights) periodically. This is useful for resuming interrupted training or for later analysis.\n",
    "\n",
    "```python\n",
    "checkpoint = tf.keras.callbacks.ModelCheckpoint(\n",
    "    'best_model.h5',\n",
    "    monitor='val_loss',\n",
    "    save_best_only=True,\n",
    "    mode='min',\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.fit(..., callbacks=[checkpoint])\n",
    "```\n",
    "\n",
    "**Explanation:** `save_best_only=True` saves only when the monitored metric improves, so you end up with the best model on disk.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.7 Mixed Precision Training**\n",
    "\n",
    "Mixed precision training uses both float16 and float32 to speed up training on compatible GPUs (NVIDIA Volta, Turing, Ampere). It reduces memory usage and can double training speed with minimal loss in accuracy.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras import mixed_precision\n",
    "mixed_precision.set_global_policy('mixed_float16')\n",
    "\n",
    "# Build model normally; outputs may need to be cast back to float32 for loss\n",
    "# For most Keras layers, this is handled automatically.\n",
    "```\n",
    "\n",
    "**Note:** Ensure your GPU supports mixed precision. You may need to adjust loss scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.8 Distributed Training**\n",
    "\n",
    "When you have multiple GPUs, you can distribute training to speed up large models or datasets.\n",
    "\n",
    "### **30.8.1 MirroredStrategy**\n",
    "\n",
    "The simplest strategy for synchronous training on a single machine with multiple GPUs.\n",
    "\n",
    "```python\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "with strategy.scope():\n",
    "    model = create_model()\n",
    "    model.compile(...)\n",
    "\n",
    "model.fit(...)\n",
    "```\n",
    "\n",
    "**Explanation:** Inside the strategy scope, all variables are mirrored across GPUs, and gradients are aggregated.\n",
    "\n",
    "### **30.8.2 MultiWorkerMirroredStrategy**\n",
    "\n",
    "For multi‑machine training.\n",
    "\n",
    "```python\n",
    "strategy = tf.distribute.MultiWorkerMirroredStrategy()\n",
    "with strategy.scope():\n",
    "    # build model\n",
    "```\n",
    "\n",
    "### **30.8.3 ParameterServerStrategy**\n",
    "\n",
    "For asynchronous training with parameter servers.\n",
    "\n",
    "For NEPSE, with a small dataset, distributed training is overkill, but it's good to know for scaling.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.9 Training Monitoring**\n",
    "\n",
    "Monitoring training helps you detect issues early. Use TensorBoard to visualize losses, metrics, and even histograms of weights.\n",
    "\n",
    "```python\n",
    "tensorboard = tf.keras.callbacks.TensorBoard(log_dir='./logs', histogram_freq=1)\n",
    "\n",
    "model.fit(..., callbacks=[tensorboard])\n",
    "```\n",
    "\n",
    "Then run `tensorboard --logdir ./logs` in the terminal.\n",
    "\n",
    "You can also log custom metrics.\n",
    "\n",
    "### **30.9.1 What to Monitor**\n",
    "\n",
    "- Training and validation loss – look for overfitting (val loss increasing).\n",
    "- Learning rate (if scheduled).\n",
    "- Gradient norms – if they become very small or very large, there may be vanishing/exploding gradients.\n",
    "- Weights and biases distributions – can indicate saturation.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.10 Debugging Training**\n",
    "\n",
    "Common issues and how to fix them:\n",
    "\n",
    "### **30.10.1 Loss Not Decreasing**\n",
    "\n",
    "- Learning rate too low or too high.\n",
    "- Data not normalized.\n",
    "- Wrong loss function.\n",
    "- Model too shallow (underfitting).\n",
    "\n",
    "### **30.10.2 NaN Loss**\n",
    "\n",
    "- Exploding gradients – reduce learning rate, add gradient clipping, check data for NaNs.\n",
    "- Numerical instability – use mixed precision correctly, add epsilon in denominators.\n",
    "\n",
    "### **30.10.3 Overfitting**\n",
    "\n",
    "- Add regularization (dropout, weight decay).\n",
    "- Reduce model capacity.\n",
    "- Increase training data (augmentation).\n",
    "- Early stopping.\n",
    "\n",
    "### **30.10.4 Underfitting**\n",
    "\n",
    "- Increase model capacity.\n",
    "- Train longer.\n",
    "- Adjust learning rate.\n",
    "\n",
    "### **30.10.5 Vanishing Gradients**\n",
    "\n",
    "- Use ReLU activations instead of sigmoid/tanh.\n",
    "- Add batch normalization.\n",
    "- Use residual connections.\n",
    "- Check initial weights.\n",
    "\n",
    "### **30.10.6 Gradient Clipping**\n",
    "\n",
    "To prevent exploding gradients, clip gradients during optimization.\n",
    "\n",
    "```python\n",
    "optimizer = tf.keras.optimizers.Adam(clipnorm=1.0)  # clip by norm\n",
    "# or\n",
    "optimizer = tf.keras.optimizers.Adam(clipvalue=0.5)  # clip by value\n",
    "```\n",
    "\n",
    "### **30.10.7 Debugging with a Small Subset**\n",
    "\n",
    "If the model doesn't learn, try overfitting a single batch. If it can't overfit one batch, there's a bug.\n",
    "\n",
    "```python\n",
    "# Take one batch\n",
    "x_batch, y_batch = next(iter(train_dataset.take(1)))\n",
    "model.fit(x_batch, y_batch, epochs=50, verbose=0)\n",
    "# Check if loss goes to near zero\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **30.11 Putting It All Together: NEPSE Training Pipeline**\n",
    "\n",
    "Let's combine the best practices into a complete training pipeline for an LSTM on NEPSE returns.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, callbacks\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np\n",
    "\n",
    "# Assume X_train, y_train, X_val, y_val, X_test, y_test are prepared and scaled\n",
    "\n",
    "# Define model\n",
    "def create_lstm_model(input_shape):\n",
    "    model = tf.keras.Sequential([\n",
    "        layers.LSTM(64, return_sequences=True, input_shape=input_shape),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.LSTM(32, return_sequences=False),\n",
    "        layers.Dropout(0.3),\n",
    "        layers.Dense(1)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "input_shape = (X_train.shape[1], X_train.shape[2])\n",
    "model = create_lstm_model(input_shape)\n",
    "\n",
    "# Learning rate schedule\n",
    "lr_schedule = tf.keras.optimizers.schedules.ExponentialDecay(\n",
    "    initial_learning_rate=0.001,\n",
    "    decay_steps=1000,\n",
    "    decay_rate=0.9\n",
    ")\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=lr_schedule)\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='huber', metrics=['mae'])\n",
    "\n",
    "# Callbacks\n",
    "early_stop = callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "reduce_lr = callbacks.ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "checkpoint = callbacks.ModelCheckpoint('best_lstm.h5', monitor='val_loss', save_best_only=True)\n",
    "tensorboard = callbacks.TensorBoard(log_dir='./logs/lstm')\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    batch_size=32,\n",
    "    epochs=100,\n",
    "    validation_data=(X_val, y_val),\n",
    "    callbacks=[early_stop, reduce_lr, checkpoint, tensorboard],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate on test\n",
    "test_loss, test_mae = model.evaluate(X_test, y_test)\n",
    "print(f\"Test MAE: {test_mae:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "- We use an exponential decay learning rate schedule combined with ReduceLROnPlateau for robustness.\n",
    "- Dropout is applied after each LSTM layer.\n",
    "- Huber loss is less sensitive to outliers than MSE.\n",
    "- Early stopping prevents overfitting; checkpoint saves the best model.\n",
    "- TensorBoard logs for monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "## **30.12 Chapter Summary**\n",
    "\n",
    "In this chapter, we covered essential best practices for training neural networks, particularly for time‑series forecasting with the NEPSE dataset.\n",
    "\n",
    "- **Data preparation:** correct scaling, batching, and shuffling strategies.\n",
    "- **Batch size selection:** impact on training dynamics and how to choose.\n",
    "- **Learning rate scheduling:** fixed decay, reduce on plateau, adaptive optimizers.\n",
    "- **Loss functions:** choosing based on task (regression, classification, probabilistic).\n",
    "- **Early stopping** to avoid overfitting.\n",
    "- **Checkpointing** to preserve the best model.\n",
    "- **Mixed precision** for faster training on compatible hardware.\n",
    "- **Distributed training** for scaling.\n",
    "- **Monitoring** with TensorBoard.\n",
    "- **Debugging** common training issues.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Always scale features using training set statistics.\n",
    "- Start with Adam and a moderate learning rate (0.001).\n",
    "- Use early stopping with patience 10‑20.\n",
    "- Monitor both training and validation loss; if they diverge, increase regularization.\n",
    "- Experiment with batch size (32, 64) and learning rate schedules.\n",
    "- For small datasets like a single stock, avoid overly complex models and heavy regularization is key.\n",
    "\n",
    "In the next chapter, **Chapter 31: Evaluation Metrics for Regression**, we will dive into the metrics used to assess forecast accuracy, including MAE, RMSE, MAPE, and specialized financial metrics.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 30**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
