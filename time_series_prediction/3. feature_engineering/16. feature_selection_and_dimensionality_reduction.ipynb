{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 16: Feature Selection and Dimensionality Reduction**\n",
    "\n",
    "## **16.1 The Need for Feature Selection**\n",
    "\n",
    "Feature selection is the process of identifying and retaining the most predictive subset of features while eliminating redundant or irrelevant ones. In the context of NEPSE (Nepal Stock Exchange) prediction systems, feature selection addresses the \"curse of dimensionality\" where an abundance of technical indicators, fundamental ratios, and macroeconomic variables can degrade model performance rather than improve it.\n",
    "\n",
    "**The Dimensionality Problem in NEPSE:**\n",
    "\n",
    "A comprehensive NEPSE prediction system might generate 200+ features per stock:\n",
    "- Price action features (Open, High, Low, Close, VWAP) \u2192 20+ indicators\n",
    "- Volume metrics (Vol, Turnover, Trans.) \u2192 15+ transformations  \n",
    "- Technical indicators (RSI, MACD, Bollinger, ATR) \u2192 30+ calculations\n",
    "- Lagged values (t-1, t-5, t-20) \u2192 60+ time-shifted features\n",
    "- Cross-sectional rankings \u2192 10+ relative metrics\n",
    "- Fundamental data (PE, PB, EPS growth) \u2192 20+ ratios\n",
    "\n",
    "**Consequences of High Dimensionality:**\n",
    "1. **Overfitting**: With only 252 trading days per year, 200 features provide near-infinite combinations to fit noise. A NEPSE model with 200 features and 500 training samples has high variance and will fail on out-of-sample data.\n",
    "2. **Multicollinearity**: NEPSE technical indicators are highly correlated. RSI(14) and Stochastic(14) share 0.85+ correlation; including both creates redundant parameters and unstable coefficient estimates in linear models.\n",
    "3. **Computational Cost**: Training XGBoost or LSTM models on 200 features requires 10x more time than on 20 carefully selected features, complicating real-time NEPSE trading systems.\n",
    "4. **Interpretability**: A model using 5 interpretable features (Price momentum, Volume spike, RSI divergence, VWAP deviation, 52-week position) is actionable for traders; a black box with 200 features is not.\n",
    "\n",
    "**Feature Selection vs. Dimensionality Reduction:**\n",
    "- **Selection**: Chooses a subset of original features (e.g., select RSI but not Stochastic). Preserves interpretability and feature names.\n",
    "- **Reduction**: Creates new synthetic features (e.g., PCA components combining RSI, MACD, and Volume). Loses direct interpretability but may capture latent patterns.\n",
    "\n",
    "**NEPSE-Specific Considerations:**\n",
    "- **Regime Dependency**: Features predictive in NEPSE bull markets (e.g., momentum) may fail in bear markets (where mean-reversion dominates). Selection must test stability across 2019-2020 (crash), 2021-2022 (recovery), and 2023-2024 (bull run) regimes.\n",
    "- **Low Signal-to-Noise**: NEPSE has lower liquidity than major exchanges, creating random price movements. Feature selection must distinguish true predictive signals from spurious correlations that arise by chance in small samples.\n",
    "\n",
    "---\n",
    "\n",
    "## **16.2 Filter Methods**\n",
    "\n",
    "Filter methods evaluate features based on statistical measures independent of any machine learning algorithm. They are computationally efficient and provide a preprocessing step to eliminate obviously irrelevant features before model-specific selection.\n",
    "\n",
    "### **16.2.1 Correlation-Based Selection**\n",
    "\n",
    "Correlation analysis identifies and removes redundant features that provide duplicate information, common with NEPSE technical indicators derived from the same price series.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def correlation_based_selection(df, features, target_col='Target_Return', \n",
    "                               threshold=0.85, method='spearman'):\n",
    "    \"\"\"\n",
    "    Select features based on correlation analysis for NEPSE data.\n",
    "    \n",
    "    Removes highly correlated features to reduce multicollinearity.\n",
    "    Uses Spearman (rank) correlation to capture non-linear monotonic relationships\n",
    "    common in financial time-series.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        NEPSE feature matrix\n",
    "    features : list\n",
    "        Candidate feature columns\n",
    "    target_col : str\n",
    "        Target variable for predictive correlation check\n",
    "    threshold : float\n",
    "        Correlation threshold for redundancy (0.85 = 85% correlated)\n",
    "    method : str\n",
    "        'spearman' (rank-based, robust to outliers) or 'pearson' (linear)\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    selected_features : list\n",
    "        Non-redundant features with high target correlation\n",
    "    corr_matrix : pd.DataFrame\n",
    "        Full correlation matrix for inspection\n",
    "    \"\"\"\n",
    "    # Calculate correlation matrix\n",
    "    X = df[features]\n",
    "    if method == 'spearman':\n",
    "        corr_matrix = X.corr(method='spearman')\n",
    "    else:\n",
    "        corr_matrix = X.corr(method='pearson')\n",
    "    \n",
    "    # Find highly correlated pairs\n",
    "    upper_triangle = corr_matrix.where(\n",
    "        np.triu(np.ones(corr_matrix.shape), k=1).astype(bool)\n",
    "    )\n",
    "    \n",
    "    # Identify features to drop (above threshold)\n",
    "    to_drop = []\n",
    "    for col in upper_triangle.columns:\n",
    "        high_corr = upper_triangle[col][abs(upper_triangle[col]) > threshold]\n",
    "        if not high_corr.empty:\n",
    "            # Keep the one with higher correlation to target, drop the other\n",
    "            for idx, corr_val in high_corr.items():\n",
    "                if idx in to_drop or col in to_drop:\n",
    "                    continue\n",
    "                \n",
    "                # Compare correlation with target\n",
    "                corr_idx_target = abs(df[idx].corr(df[target_col], method=method))\n",
    "                corr_col_target = abs(df[col].corr(df[target_col], method=method))\n",
    "                \n",
    "                if corr_col_target > corr_idx_target:\n",
    "                    to_drop.append(idx)\n",
    "                else:\n",
    "                    to_drop.append(col)\n",
    "    \n",
    "    selected_features = [f for f in features if f not in to_drop]\n",
    "    \n",
    "    # Additional filter: Remove features with low target correlation (< 0.05)\n",
    "    target_corrs = {}\n",
    "    for feat in selected_features:\n",
    "        corr, p_value = spearmanr(df[feat], df[target_col], nan_policy='omit')\n",
    "        target_corrs[feat] = {'correlation': corr, 'p_value': p_value}\n",
    "    \n",
    "    # Keep only statistically significant features (p < 0.05)\n",
    "    significant_features = [\n",
    "        f for f in selected_features \n",
    "        if target_corrs[f]['p_value'] < 0.05 and abs(target_corrs[f]['correlation']) > 0.03\n",
    "    ]\n",
    "    \n",
    "    return significant_features, corr_matrix, target_corrs\n",
    "\n",
    "def visualize_nepse_correlation_network(df, features, threshold=0.7):\n",
    "    \"\"\"\n",
    "    Visualize feature correlations as a network graph for NEPSE indicators.\n",
    "    \n",
    "    Helps identify clusters of redundant indicators (e.g., all momentum indicators\n",
    "    grouping together, all volatility indicators grouping together).\n",
    "    \"\"\"\n",
    "    import networkx as nx\n",
    "    \n",
    "    corr_matrix = df[features].corr().abs()\n",
    "    \n",
    "    # Create graph\n",
    "    G = nx.Graph()\n",
    "    \n",
    "    # Add edges for correlations above threshold\n",
    "    for i in range(len(features)):\n",
    "        for j in range(i+1, len(features)):\n",
    "            if corr_matrix.iloc[i, j] > threshold:\n",
    "                G.add_edge(features[i], features[j], weight=corr_matrix.iloc[i, j])\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    pos = nx.spring_layout(G, k=3, iterations=50)\n",
    "    \n",
    "    edges = G.edges(data=True)\n",
    "    weights = [d['weight'] for (u, v, d) in edges]\n",
    "    \n",
    "    nx.draw_networkx_nodes(G, pos, node_color='lightblue', node_size=500)\n",
    "    nx.draw_networkx_edges(G, pos, width=[w*2 for w in weights], alpha=0.5)\n",
    "    nx.draw_networkx_labels(G, pos, font_size=8)\n",
    "    \n",
    "    plt.title('NEPSE Feature Correlation Network\\n(Edges > 0.7 correlation)')\n",
    "    plt.show()\n",
    "    \n",
    "    # Identify clusters (connected components)\n",
    "    clusters = list(nx.connected_components(G))\n",
    "    print(f\"Identified {len(clusters)} feature clusters:\")\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f\"  Cluster {i+1}: {cluster}\")\n",
    "    \n",
    "    return clusters\n",
    "\n",
    "# Detailed Explanation:\n",
    "#\n",
    "# 1. Spearman vs Pearson for NEPSE:\n",
    "#    - Pearson measures linear correlation (sensitive to outliers)\n",
    "#    - Spearman measures rank correlation (monotonic relationships)\n",
    "#    - NEPSE has non-linear relationships (e.g., RSI 30\u219240 is bullish, \n",
    "#      but RSI 70\u219280 is bearish/overbought). Spearman captures the \n",
    "#      ordinal relationship better than Pearson.\n",
    "#\n",
    "# 2. Target Correlation Threshold (0.03):\n",
    "#    - In noisy financial data, correlations < 0.03 are often spurious\n",
    "#    - With 252 samples/year, correlation of 0.03 has p-value ~ 0.10\n",
    "#    - We use 0.03 as minimum predictive signal, but require p < 0.05\n",
    "#      for statistical significance\n",
    "#\n",
    "# 3. Handling Multicollinearity:\n",
    "#    - When RSI and Stochastic correlate at 0.90, we keep the one with\n",
    "#      higher target correlation (better predictive power)\n",
    "#    - This greedy approach ensures we retain the most informative \n",
    "#      member of each correlated cluster\n",
    "#\n",
    "# 4. Network Visualization:\n",
    "#    - Clusters reveal feature redundancy (e.g., all Bollinger Band \n",
    "#      features cluster together; all MACD components cluster)\n",
    "#    - Between-cluster features provide orthogonal information\n",
    "#    - Ideal NEPSE model uses one representative from each cluster\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `correlation_based_selection` function implements a two-stage filter for NEPSE features. **Stage 1** removes multicollinear features using a greedy algorithm: when two features correlate above 0.85 (threshold), it retains whichever has stronger predictive correlation with the target (next-day returns) and discards the other. This prevents the \"double counting\" of information common in technical analysis where indicators like RSI and Stochastic Oscillator measure similar momentum concepts.\n",
    "\n",
    "**Spearman vs. Pearson:**\n",
    "The function defaults to Spearman (rank) correlation because NEPSE relationships are often non-linear. For example, the relationship between RSI and future returns is inverse-U shaped (extreme values predict reversals, middle values predict continuation). Pearson correlation might show near-zero linear relationship, while Spearman captures the monotonic predictive structure.\n",
    "\n",
    "**Statistical Significance:**\n",
    "Even after removing redundancy, features must pass individual predictive tests. With typical NEPSE sample sizes (500-1000 trading days), correlations below 0.03 often occur by chance (p > 0.05). The function filters these out, ensuring only statistically robust predictors remain.\n",
    "\n",
    "### **16.2.2 Variance Threshold**\n",
    "\n",
    "Variance thresholding removes features with near-zero variance (constant or nearly constant values), which provide no discriminative power for NEPSE prediction.\n",
    "\n",
    "```python\n",
    "def variance_threshold_selection(df, features, threshold=0.01, target_aware=True):\n",
    "    \"\"\"\n",
    "    Remove low-variance features from NEPSE dataset.\n",
    "    \n",
    "    In financial time-series, low variance can indicate:\n",
    "    - Illiquid stocks with constant prices for days\n",
    "    - Technical indicators stuck at bounds (RSI = 100 for weeks)\n",
    "    - Calculated features with numerical errors\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    threshold : float\n",
    "        Minimum normalized variance to retain feature\n",
    "    target_aware : bool\n",
    "        If True, only remove features with low variance AND low target correlation\n",
    "        ( preserves low-variance features that are strongly predictive)\n",
    "    \"\"\"\n",
    "    from sklearn.feature_selection import VarianceThreshold\n",
    "    \n",
    "    X = df[features]\n",
    "    \n",
    "    # Normalize variance by mean (coefficient of variation) for scale-invariance\n",
    "    cv_threshold = threshold  # Coefficient of variation = std / |mean|\n",
    "    \n",
    "    variances = X.var()\n",
    "    means = X.abs().mean()\n",
    "    cv = variances / (means + 1e-8)  # Coefficient of variation\n",
    "    \n",
    "    low_var_features = cv[cv < cv_threshold].index.tolist()\n",
    "    \n",
    "    if target_aware and 'Target' in df.columns:\n",
    "        # Check if low-variance features still predict target (rare but possible)\n",
    "        for feat in low_var_features[:]:  # Copy list to allow removal\n",
    "            corr = abs(df[feat].corr(df['Target']))\n",
    "            if corr > 0.05:  # If predictive despite low variance, keep it\n",
    "                low_var_features.remove(feat)\n",
    "                print(f\"Keeping low-variance feature {feat} due to high target correlation ({corr:.3f})\")\n",
    "    \n",
    "    selected = [f for f in features if f not in low_var_features]\n",
    "    \n",
    "    print(f\"Removed {len(low_var_features)} low-variance features:\")\n",
    "    print(f\"  {low_var_features}\")\n",
    "    print(f\"Retained {len(selected)} features\")\n",
    "    \n",
    "    return selected\n",
    "\n",
    "# NEPSE Example:\n",
    "# Feature \"Is_Holiday\" might have variance 0.02 (2% of days are holidays)\n",
    "# But it has zero predictive power for returns \u2192 Remove\n",
    "# \n",
    "# Feature \"RSI_Above_70\" might have variance 0.03 (rarely >70)\n",
    "# But when True, predicts -2% returns \u2192 Keep despite low variance\n",
    "```\n",
    "\n",
    "### **16.2.3 Mutual Information**\n",
    "\n",
    "Mutual Information (MI) measures the reduction in uncertainty about the target variable given knowledge of a feature, capturing non-linear relationships that correlation misses.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import mutual_info_regression, mutual_info_classif\n",
    "\n",
    "def mutual_information_selection(df, features, target_col, n_neighbors=3, \n",
    "                                task='regression', n_features=20):\n",
    "    \"\"\"\n",
    "    Select features using Mutual Information for NEPSE prediction.\n",
    "    \n",
    "    MI measures: I(X;Y) = H(Y) - H(Y|X)\n",
    "    (Reduction in entropy of target given feature)\n",
    "    \n",
    "    Advantages for NEPSE:\n",
    "    - Captures non-linear relationships (e.g., V-shaped patterns)\n",
    "    - Detects interactions (e.g., Volume * Volatility)\n",
    "    - No assumption of Gaussian distribution\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    n_neighbors : int\n",
    "        Number of neighbors for k-NN based MI estimation (3-5 for financial data)\n",
    "    task : str\n",
    "        'regression' for continuous returns, 'classification' for direction\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Handle missing values\n",
    "    X = X.fillna(X.median())\n",
    "    \n",
    "    if task == 'regression':\n",
    "        mi_scores = mutual_info_regression(X, y, n_neighbors=n_neighbors, random_state=42)\n",
    "    else:\n",
    "        mi_scores = mutual_info_classif(X, y, n_neighbors=n_neighbors, random_state=42)\n",
    "    \n",
    "    mi_results = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'MI_Score': mi_scores\n",
    "    }).sort_values('MI_Score', ascending=False)\n",
    "    \n",
    "    # Select top N features\n",
    "    selected = mi_results.head(n_features)['Feature'].tolist()\n",
    "    \n",
    "    # MI per unit complexity (penalize redundant features)\n",
    "    # Calculate conditional redundancy\n",
    "    redundancy_penalty = {}\n",
    "    for feat in selected:\n",
    "        redundancies = []\n",
    "        for other_feat in selected:\n",
    "            if feat != other_feat:\n",
    "                # Approximate redundancy via feature-feature MI\n",
    "                mi_ff = mutual_info_regression(X[[feat]], X[other_feat], n_neighbors=3)[0]\n",
    "                redundancies.append(mi_ff)\n",
    "        redundancy_penalty[feat] = np.mean(redundancies) if redundancies else 0\n",
    "    \n",
    "    mi_results['Redundancy'] = mi_results['Feature'].map(redundancy_penalty)\n",
    "    mi_results['Adjusted_MI'] = mi_results['MI_Score'] - 0.5 * mi_results['Redundancy']\n",
    "    \n",
    "    # Re-rank by adjusted MI\n",
    "    selected_adjusted = mi_results.sort_values('Adjusted_MI', ascending=False).head(n_features)['Feature'].tolist()\n",
    "    \n",
    "    return selected_adjusted, mi_results\n",
    "\n",
    "# Explanation:\n",
    "#\n",
    "# 1. Why MI for NEPSE:\n",
    "#    - Correlation misses non-linear patterns like \"extreme RSI predicts reversal\"\n",
    "#    - MI captures: \"When RSI is between 30-70, returns are random (low MI)\n",
    "#      but when RSI < 20 or > 80, returns have high negative/positive skew (high MI)\"\n",
    "#\n",
    "# 2. Adjusted MI:\n",
    "#    - Pure MI selects features that are individually informative but redundant\n",
    "#    - Adjusted MI penalizes features that are highly mutual with other selected features\n",
    "#    - Ensures diverse information set (e.g., selects RSI OR Stochastic, not both)\n",
    "```\n",
    "\n",
    "### **16.2.4 Chi-Square Test**\n",
    "\n",
    "Chi-square tests evaluate independence between categorical features and categorical targets, useful for NEPSE regime classification (bull/bear/sideways markets).\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "\n",
    "def chi_square_feature_selection(df, features, target_col, n_bins=5):\n",
    "    \"\"\"\n",
    "    Apply Chi-Square test for categorical feature selection in NEPSE.\n",
    "    \n",
    "    Best for: Discrete features (candlestick patterns, gap types, volume spikes)\n",
    "    predicting discrete targets (Up/Down/Flat next day).\n",
    "    \n",
    "    Requires binning continuous features into quantiles first.\n",
    "    \"\"\"\n",
    "    # Discretize continuous features into quintiles\n",
    "    X = df[features].copy()\n",
    "    discretizer = KBinsDiscretizer(n_bins=n_bins, encode='ordinal', strategy='quantile')\n",
    "    X_discrete = discretizer.fit_transform(X)\n",
    "    X_discrete = pd.DataFrame(X_discrete, columns=features, index=X.index)\n",
    "    \n",
    "    # Ensure target is discrete (0, 1, 2 for Down, Flat, Up)\n",
    "    y = df[target_col]\n",
    "    if y.nunique() > 5:\n",
    "        # Bin continuous target into classes\n",
    "        y = pd.qcut(y, q=3, labels=[0, 1, 2])\n",
    "    \n",
    "    # Chi-square test (requires non-negative features)\n",
    "    X_discrete = X_discrete.abs()\n",
    "    \n",
    "    chi_scores, p_values = chi2(X_discrete, y)\n",
    "    \n",
    "    results = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Chi2_Score': chi_scores,\n",
    "        'P_Value': p_values\n",
    "    }).sort_values('Chi2_Score', ascending=False)\n",
    "    \n",
    "    # Select significant features (p < 0.05)\n",
    "    selected = results[results['P_Value'] < 0.05]['Feature'].tolist()\n",
    "    \n",
    "    return selected, results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **16.3 Wrapper Methods**\n",
    "\n",
    "Wrapper methods evaluate feature subsets by training and testing actual machine learning models, selecting the combination that optimizes predictive performance. They are computationally expensive but capture feature interactions missed by filter methods.\n",
    "\n",
    "### **16.3.1 Recursive Feature Elimination (RFE)**\n",
    "\n",
    "RFE iteratively removes the weakest features according to model coefficients or feature importance, starting with all features and pruning backward.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFE, RFECV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "def recursive_feature_elimination(df, features, target_col, n_features=15, \n",
    "                                 model_type='ridge', step=1):\n",
    "    \"\"\"\n",
    "    Apply Recursive Feature Elimination for NEPSE feature selection.\n",
    "    \n",
    "    Process:\n",
    "    1. Train model on all features\n",
    "    2. Rank features by importance (coefficients for linear, feature_importances_ for trees)\n",
    "    3. Remove weakest feature(s)\n",
    "    4. Repeat until n_features remain\n",
    "    \n",
    "    Time-Series Aware: Uses TimeSeriesSplit to prevent look-ahead bias.\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Choose estimator\n",
    "    if model_type == 'ridge':\n",
    "        estimator = Ridge(alpha=1.0)\n",
    "    elif model_type == 'random_forest':\n",
    "        estimator = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported model type\")\n",
    "    \n",
    "    # Time-series cross-validation\n",
    "    tscv = TimeSeriesSplit(n_splits=5)\n",
    "    \n",
    "    # RFE with cross-validation to find optimal number of features\n",
    "    selector = RFECV(\n",
    "        estimator=estimator,\n",
    "        step=step,  # Remove 1 feature at a time\n",
    "        cv=tscv,\n",
    "        scoring='neg_mean_squared_error',\n",
    "        n_jobs=-1,\n",
    "        min_features_to_select=5\n",
    "    )\n",
    "    \n",
    "    selector.fit(X, y)\n",
    "    \n",
    "    selected_features = [f for f, selected in zip(features, selector.support_) if selected]\n",
    "    ranking = dict(zip(features, selector.ranking_))\n",
    "    \n",
    "    print(f\"Optimal number of features: {selector.n_features_}\")\n",
    "    print(f\"Selected: {selected_features}\")\n",
    "    \n",
    "    return selected_features, ranking, selector\n",
    "\n",
    "# Explanation:\n",
    "#\n",
    "# 1. Why RFE for NEPSE:\n",
    "#    - Considers feature interactions (e.g., RSI might be useless alone but \n",
    "#      powerful when combined with Volume)\n",
    "#    - Model-specific selection (Ridge prefers uncorrelated features, \n",
    "#      Random Forest handles redundancy better)\n",
    "#\n",
    "# 2. TimeSeriesSplit:\n",
    "#    - Standard KFold leaks future information into training set\n",
    "#    - TimeSeriesSplit ensures training set always before validation set\n",
    "#    - Critical for NEPSE where feature importance varies by regime\n",
    "#\n",
    "# 3. Step Size:\n",
    "#    - step=1 is accurate but slow for 200+ features\n",
    "#    - step=5 (remove 5 at a time) faster but might miss optimal combinations\n",
    "```\n",
    "\n",
    "### **16.3.2 Forward Selection**\n",
    "\n",
    "Forward selection starts with no features and iteratively adds the feature that most improves model performance until a stopping criterion is met.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "def forward_feature_selection(df, features, target_col, max_features=20, \n",
    "                             model_type='ridge', cv_folds=5):\n",
    "    \"\"\"\n",
    "    Forward Selection for NEPSE: Start empty, add best feature each iteration.\n",
    "    \n",
    "    More conservative than RFE (backward), less prone to overfitting with small NEPSE samples.\n",
    "    Stops when CV score improvement < 0.1%.\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    if model_type == 'ridge':\n",
    "        model = Ridge(alpha=1.0)\n",
    "    \n",
    "    selected = []\n",
    "    remaining = set(features)\n",
    "    best_score = -np.inf\n",
    "    scores_history = []\n",
    "    \n",
    "    while remaining and len(selected) < max_features:\n",
    "        scores = []\n",
    "        \n",
    "        for feature in remaining:\n",
    "            # Try adding this feature\n",
    "            trial_features = selected + [feature]\n",
    "            X_trial = X[trial_features]\n",
    "            \n",
    "            # Time-series CV\n",
    "            tscv = TimeSeriesSplit(n_splits=cv_folds)\n",
    "            cv_scores = cross_val_score(model, X_trial, y, cv=tscv, \n",
    "                                       scoring='neg_mean_squared_error')\n",
    "            scores.append((feature, cv_scores.mean()))\n",
    "        \n",
    "        # Find best feature to add\n",
    "        scores.sort(key=lambda x: x[1], reverse=True)\n",
    "        best_feature, best_new_score = scores[0]\n",
    "        \n",
    "        # Stopping criterion: improvement > 0.1%\n",
    "        if best_new_score > best_score + 0.001:\n",
    "            selected.append(best_feature)\n",
    "            remaining.remove(best_feature)\n",
    "            best_score = best_new_score\n",
    "            scores_history.append(best_score)\n",
    "            print(f\"Added {best_feature}, CV Score: {best_score:.4f}\")\n",
    "        else:\n",
    "            print(\"No significant improvement, stopping.\")\n",
    "            break\n",
    "    \n",
    "    return selected, scores_history\n",
    "```\n",
    "\n",
    "### **16.3.3 Backward Elimination**\n",
    "\n",
    "Backward elimination starts with all features and removes the least significant feature iteratively (similar to RFE but often using statistical significance rather than importance).\n",
    "\n",
    "```python\n",
    "import statsmodels.api as sm\n",
    "\n",
    "def backward_elimination(df, features, target_col, significance_level=0.05):\n",
    "    \"\"\"\n",
    "    Statistical backward elimination using p-values from OLS regression.\n",
    "    \n",
    "    Appropriate for NEPSE linear factor models where statistical significance\n",
    "    and interpretability matter.\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Add constant for intercept\n",
    "    X = sm.add_constant(X)\n",
    "    \n",
    "    features = list(features)\n",
    "    \n",
    "    while len(features) > 0:\n",
    "        model = sm.OLS(y, X[features + ['const']]).fit()\n",
    "        p_values = model.pvalues.drop('const')\n",
    "        max_p = p_values.max()\n",
    "        \n",
    "        if max_p > significance_level:\n",
    "            excluded_feature = p_values.idxmax()\n",
    "            features.remove(excluded_feature)\n",
    "            print(f\"Removed {excluded_feature}, p-value: {max_p:.4f}\")\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return features, model\n",
    "```\n",
    "\n",
    "### **16.3.4 Stepwise Selection**\n",
    "\n",
    "Stepwise selection combines forward and backward steps, allowing features to be added and removed dynamically based on statistical criteria.\n",
    "\n",
    "```python\n",
    "def stepwise_selection(df, features, target_col, initial_list=[], \n",
    "                      threshold_in=0.01, threshold_out=0.05, verbose=True):\n",
    "    \"\"\"\n",
    "    Stepwise selection for NEPSE: Add significant features, remove insignificant ones.\n",
    "    \n",
    "    Balances forward and backward passes to find optimal subset.\n",
    "    \"\"\"\n",
    "    included = list(initial_list)\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    while True:\n",
    "        changed = False\n",
    "        \n",
    "        # Forward step\n",
    "        excluded = list(set(features) - set(included))\n",
    "        new_pvals = pd.Series(index=excluded, dtype=float)\n",
    "        \n",
    "        for new_col in excluded:\n",
    "            model = sm.OLS(y, sm.add_constant(X[included + [new_col]])).fit()\n",
    "            new_pvals[new_col] = model.pvalues[new_col]\n",
    "        \n",
    "        best_pval = new_pvals.min()\n",
    "        if best_pval < threshold_in:\n",
    "            best_feature = new_pvals.idxmin()\n",
    "            included.append(best_feature)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f\"Add  {best_feature:30s} | p-value: {best_pval:.4f}\")\n",
    "        \n",
    "        # Backward step\n",
    "        model = sm.OLS(y, sm.add_constant(X[included])).fit()\n",
    "        pvalues = model.pvalues.iloc[1:]  # Exclude intercept\n",
    "        worst_pval = pvalues.max()\n",
    "        \n",
    "        if worst_pval > threshold_out:\n",
    "            worst_feature = pvalues.idxmax()\n",
    "            included.remove(worst_feature)\n",
    "            changed = True\n",
    "            if verbose:\n",
    "                print(f\"Drop {worst_feature:30s} | p-value: {worst_pval:.4f}\")\n",
    "        \n",
    "        if not changed:\n",
    "            break\n",
    "    \n",
    "    return included\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **16.4 Embedded Methods**\n",
    "\n",
    "Embedded methods perform feature selection during model training, penalizing complex models with many features. They are efficient and model-specific.\n",
    "\n",
    "### **16.4.1 LASSO (L1 Regularization)**\n",
    "\n",
    "LASSO (Least Absolute Shrinkage and Selection Operator) adds a penalty proportional to the absolute value of coefficients, driving some coefficients to exactly zero (automatic feature selection).\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import Lasso, LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def lasso_feature_selection(df, features, target_col, alphas=None, \n",
    "                           cv_folds=5, max_features=30):\n",
    "    \"\"\"\n",
    "    LASSO regression for NEPSE feature selection.\n",
    "    \n",
    "    L1 penalty: Loss = MSE + alpha * sum(|coefficients|)\n",
    "    As alpha increases, more coefficients become exactly zero.\n",
    "    \n",
    "    Advantages for NEPSE:\n",
    "    - Automatic feature selection (sparse models)\n",
    "    - Handles multicollinearity by selecting one from correlated group\n",
    "    - Interpretable coefficients (Rs per unit feature change)\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Standardize features (LASSO requires scaling)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Cross-validated LASSO to find optimal alpha\n",
    "    if alphas is None:\n",
    "        alphas = np.logspace(-4, 0, 50)  # 0.0001 to 1.0\n",
    "    \n",
    "    lasso_cv = LassoCV(alphas=alphas, cv=TimeSeriesSplit(cv_folds), \n",
    "                       max_iter=10000, random_state=42)\n",
    "    lasso_cv.fit(X_scaled, y)\n",
    "    \n",
    "    # Get selected features (non-zero coefficients)\n",
    "    coefs = pd.Series(lasso_cv.coef_, index=features)\n",
    "    selected = coefs[coefs != 0].index.tolist()\n",
    "    \n",
    "    print(f\"Optimal alpha: {lasso_cv.alpha_:.6f}\")\n",
    "    print(f\"Selected {len(selected)} features out of {len(features)}\")\n",
    "    print(f\"R\u00b2 Score: {lasso_cv.score(X_scaled, y):.4f}\")\n",
    "    \n",
    "    # Feature importance by coefficient magnitude\n",
    "    importance = coefs.abs().sort_values(ascending=False)\n",
    "    \n",
    "    return selected, importance, lasso_cv\n",
    "\n",
    "# Explanation:\n",
    "#\n",
    "# 1. Alpha Tuning:\n",
    "#    - Low alpha (0.0001): Almost OLS, keeps many features, overfitting risk\n",
    "#    - High alpha (1.0): Aggressive sparsity, may underfit\n",
    "#    - LassoCV finds sweet spot via cross-validation on TimeSeriesSplit\n",
    "#\n",
    "# 2. Multicollinearity Handling:\n",
    "#    - When RSI and Stochastic are correlated (0.9), LASSO tends to pick\n",
    "#      one and zero out the other, avoiding redundant features\n",
    "#    - Which one survives depends on minor noise; use Elastic Net for stability\n",
    "#\n",
    "# 3. NEPSE Interpretation:\n",
    "#    - If \"RSI_Oversold\" has coefficient +0.02, it means:\n",
    "#      \"When RSI < 30, expected next-day return increases by 0.02% (2 basis points)\"\n",
    "#    - Sparse models allow traders to understand exactly which signals matter\n",
    "```\n",
    "\n",
    "### **16.4.2 Ridge (L2 Regularization)**\n",
    "\n",
    "Ridge regression uses L2 penalty (squared coefficients) which shrinks coefficients toward zero but rarely to exactly zero. It handles multicollinearity by distributing coefficients among correlated features.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "def ridge_feature_selection(df, features, target_col, alphas=None):\n",
    "    \"\"\"\n",
    "    Ridge regression for NEPSE: Shrinkage without elimination.\n",
    "    \n",
    "    Useful when all features have some predictive power and you want\n",
    "    to keep them all with reduced weights (e.g., ensemble models).\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    if alphas is None:\n",
    "        alphas = np.logspace(-2, 2, 50)\n",
    "    \n",
    "    ridge = RidgeCV(alphas=alphas, cv=TimeSeriesSplit(5))\n",
    "    ridge.fit(X_scaled, y)\n",
    "    \n",
    "    coefs = pd.Series(ridge.coef_, index=features)\n",
    "    \n",
    "    # Ridge keeps all features but ranks them by coefficient magnitude\n",
    "    # Consider removing features with |coef| < 1e-4 (numerical zero)\n",
    "    selected = coefs[abs(coefs) > 1e-4].index.tolist()\n",
    "    \n",
    "    return selected, coefs, ridge\n",
    "```\n",
    "\n",
    "### **16.4.3 Elastic Net**\n",
    "\n",
    "Elastic Net combines L1 (LASSO) and L2 (Ridge) penalties, providing both feature selection and handling of correlated groups.\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "def elastic_net_selection(df, features, target_col, l1_ratio=0.5):\n",
    "    \"\"\"\n",
    "    Elastic Net for NEPSE: Balance between LASSO (selection) and Ridge (grouping).\n",
    "    \n",
    "    l1_ratio=0.5 means equal L1 and L2 penalty.\n",
    "    l1_ratio=1.0 is pure LASSO, 0.0 is pure Ridge.\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Search over alphas and l1_ratios\n",
    "    enet = ElasticNetCV(\n",
    "        l1_ratio=[0.1, 0.3, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n",
    "        cv=TimeSeriesSplit(5),\n",
    "        max_iter=10000,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    enet.fit(X_scaled, y)\n",
    "    \n",
    "    coefs = pd.Series(enet.coef_, index=features)\n",
    "    selected = coefs[coefs != 0].index.tolist()\n",
    "    \n",
    "    print(f\"Optimal l1_ratio: {enet.l1_ratio_}\")\n",
    "    print(f\"Optimal alpha: {enet.alpha_:.6f}\")\n",
    "    \n",
    "    return selected, coefs, enet\n",
    "```\n",
    "\n",
    "### **16.4.4 Tree-Based Importance**\n",
    "\n",
    "Tree ensembles (Random Forest, XGBoost) provide built-in feature importance measures based on impurity reduction or permutation.\n",
    "\n",
    "```python\n",
    "import xgboost as xgb\n",
    "from sklearn.inspection import permutation_importance\n",
    "\n",
    "def tree_based_selection(df, features, target_col, method='gain', \n",
    "                        n_features=20, n_permutations=10):\n",
    "    \"\"\"\n",
    "    Feature selection using XGBoost importance scores.\n",
    "    \n",
    "    Methods:\n",
    "    - 'gain': Average gain of splits using feature (default)\n",
    "    - 'cover': Average coverage (number of samples affected)\n",
    "    - 'weight': Number of times feature appears in trees\n",
    "    - 'permutation': Shuffle feature and measure performance drop (most accurate)\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    y = df[target_col]\n",
    "    \n",
    "    # Train XGBoost\n",
    "    model = xgb.XGBRegressor(\n",
    "        n_estimators=100,\n",
    "        max_depth=5,\n",
    "        learning_rate=0.05,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.8,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    model.fit(X, y)\n",
    "    \n",
    "    if method == 'permutation':\n",
    "        # Permutation importance (model-agnostic, more reliable)\n",
    "        perm_importance = permutation_importance(\n",
    "            model, X, y, \n",
    "            n_repeats=n_permutations,\n",
    "            random_state=42,\n",
    "            scoring='neg_mean_squared_error'\n",
    "        )\n",
    "        importance_scores = perm_importance.importances_mean\n",
    "    else:\n",
    "        # Built-in importance\n",
    "        importance_dict = model.get_booster().get_score(importance_type=method)\n",
    "        importance_scores = [importance_dict.get(f, 0) for f in features]\n",
    "    \n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': features,\n",
    "        'Importance': importance_scores\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    selected = importance_df.head(n_features)['Feature'].tolist()\n",
    "    \n",
    "    return selected, importance_df, model\n",
    "\n",
    "# Explanation:\n",
    "#\n",
    "# 1. Gain vs Permutation:\n",
    "#    - Gain is biased toward high-cardinality features (continuous vs categorical)\n",
    "#    - Permutation shuffles each feature and measures performance drop\n",
    "#    - Permutation is slower but more accurate for NEPSE feature selection\n",
    "#\n",
    "# 2. Tree Importance Bias:\n",
    "#    - Correlated features (RSI, Stochastic) share importance\n",
    "#    - May show both as important even though they're redundant\n",
    "#    - Combine with RFE or correlation filtering for best results\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **16.5 Dimensionality Reduction Techniques**\n",
    "\n",
    "When features are highly correlated or when the feature space is too large for the sample size, dimensionality reduction creates new composite features that capture the majority of variance in fewer dimensions.\n",
    "\n",
    "### **16.5.1 Principal Component Analysis (PCA)**\n",
    "\n",
    "PCA transforms features into orthogonal components ranked by explained variance, removing multicollinearity and reducing noise.\n",
    "\n",
    "```python\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def apply_pca_reduction(df, features, variance_threshold=0.95, \n",
    "                       n_components=None, visualize=True):\n",
    "    \"\"\"\n",
    "    Apply PCA to NEPSE features for dimensionality reduction.\n",
    "    \n",
    "    Critical for:\n",
    "    - Removing multicollinearity among technical indicators\n",
    "    - Reducing noise (lower components often capture signal, higher capture noise)\n",
    "    - Visualization (plotting stocks in 2D/3D factor space)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    variance_threshold : float\n",
    "        Retain components explaining 95% of total variance\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    \n",
    "    # Standardize (essential for PCA)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Determine number of components\n",
    "    if n_components is None:\n",
    "        pca_full = PCA()\n",
    "        pca_full.fit(X_scaled)\n",
    "        cumsum = np.cumsum(pca_full.explained_variance_ratio_)\n",
    "        n_components = np.argmax(cumsum >= variance_threshold) + 1\n",
    "    \n",
    "    # Fit PCA\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    # Create component DataFrame\n",
    "    components_df = pd.DataFrame(\n",
    "        X_pca, \n",
    "        columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    # Component interpretation (loadings)\n",
    "    loadings = pd.DataFrame(\n",
    "        pca.components_.T,\n",
    "        columns=[f'PC{i+1}' for i in range(n_components)],\n",
    "        index=features\n",
    "    )\n",
    "    \n",
    "    print(f\"Reduced from {len(features)} to {n_components} dimensions\")\n",
    "    print(f\"Explained variance: {pca.explained_variance_ratio_.sum():.2%}\")\n",
    "    \n",
    "    # Interpret top components for NEPSE\n",
    "    for i in range(min(3, n_components)):\n",
    "        top_features = loadings[f'PC{i+1}'].abs().sort_values(ascending=False).head(5)\n",
    "        print(f\"\\nPC{i+1} top loadings:\")\n",
    "        print(top_features)\n",
    "    \n",
    "    if visualize and n_components >= 2:\n",
    "        plt.figure(figsize=(10, 6))\n",
    "        plt.scatter(X_pca[:, 0], X_pca[:, 1], alpha=0.5, c=df.index.map(lambda x: x.year))\n",
    "        plt.xlabel('PC1')\n",
    "        plt.ylabel('PC2')\n",
    "        plt.title('NEPSE Stocks in PCA Space (colored by year)')\n",
    "        plt.colorbar(label='Year')\n",
    "        plt.show()\n",
    "    \n",
    "    return components_df, loadings, pca\n",
    "\n",
    "# Explanation:\n",
    "#\n",
    "# 1. PCA for NEPSE Technical Indicators:\n",
    "#    - PC1 often represents \"Market Trend\" (positive loadings on all price MAs)\n",
    "#    - PC2 often represents \"Volatility\" (positive on ATR, Range, Volume)\n",
    "#    - PC3 often represents \"Momentum\" (RSI, MACD, Stochastic)\n",
    "#    - Instead of 30 correlated indicators, use 3-5 orthogonal components\n",
    "#\n",
    "# 2. Variance Threshold:\n",
    "#    - 95% retains most information while removing noise\n",
    "#    - For NEPSE with 200 features, typically reduces to 15-30 components\n",
    "#    - Check scree plot to identify \"elbow\" (optimal cutoff)\n",
    "#\n",
    "# 3. Limitations:\n",
    "#    - Components are linear combinations (lose interpretability)\n",
    "#    - \"PC1\" is hard to explain to traders vs \"RSI_Oversold\"\n",
    "#    - Use for intermediate modeling, not final interpretable models\n",
    "```\n",
    "\n",
    "### **16.5.2 t-SNE**\n",
    "\n",
    "t-Distributed Stochastic Neighbor Embedding is a non-linear technique for visualizing high-dimensional data in 2D or 3D, useful for identifying clusters of similar market regimes.\n",
    "\n",
    "```python\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "def apply_tsne_visualization(df, features, perplexity=30, n_iter=1000):\n",
    "    \"\"\"\n",
    "    t-SNE for visualizing NEPSE market regimes and stock similarities.\n",
    "    \n",
    "    Non-linear dimensionality reduction preserving local neighborhoods.\n",
    "    Good for identifying:\n",
    "    - Clusters of similar trading days (bull vs bear regimes)\n",
    "    - Outlier days (crashes, circuit breakers)\n",
    "    - Evolution of market structure over time\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    tsne = TSNE(n_components=2, perplexity=perplexity, \n",
    "                n_iter=n_iter, random_state=42)\n",
    "    X_tsne = tsne.fit_transform(X_scaled)\n",
    "    \n",
    "    tsne_df = pd.DataFrame(X_tsne, columns=['TSNE1', 'TSNE2'], index=df.index)\n",
    "    \n",
    "    # Plot with market regimes colored\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    scatter = plt.scatter(X_tsne[:, 0], X_tsne[:, 1], \n",
    "                         c=df['Close'], cmap='viridis', alpha=0.6)\n",
    "    plt.colorbar(scatter, label='Close Price')\n",
    "    plt.title('NEPSE Market Regimes (t-SNE)')\n",
    "    plt.show()\n",
    "    \n",
    "    return tsne_df\n",
    "```\n",
    "\n",
    "### **16.5.3 UMAP**\n",
    "\n",
    "Uniform Manifold Approximation and Projection is a modern technique that often preserves both local and global structure better than t-SNE.\n",
    "\n",
    "```python\n",
    "import umap\n",
    "\n",
    "def apply_umap_reduction(df, features, n_neighbors=15, min_dist=0.1, n_components=2):\n",
    "    \"\"\"\n",
    "    UMAP for NEPSE feature reduction and visualization.\n",
    "    \n",
    "    Faster than t-SNE and better preserves global structure (relative distances\n",
    "    between clusters, not just local neighborhoods).\n",
    "    \"\"\"\n",
    "    X = df[features]\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    reducer = umap.UMAP(\n",
    "        n_neighbors=n_neighbors,\n",
    "        min_dist=min_dist,\n",
    "        n_components=n_components,\n",
    "        random_state=42\n",
    "    )\n",
    "    \n",
    "    X_umap = reducer.fit_transform(X_scaled)\n",
    "    \n",
    "    return pd.DataFrame(X_umap, columns=[f'UMAP{i+1}' for i in range(n_components)])\n",
    "```\n",
    "\n",
    "### **16.5.4 Autoencoders**\n",
    "\n",
    "Neural network-based dimensionality reduction that learns non-linear compressed representations.\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "\n",
    "def build_feature_autoencoder(input_dim, encoding_dim=10):\n",
    "    \"\"\"\n",
    "    Build autoencoder for non-linear dimensionality reduction of NEPSE features.\n",
    "    \n",
    "    Useful when PCA fails to capture non-linear relationships between\n",
    "    technical indicators (e.g., RSI thresholds are non-linear).\n",
    "    \"\"\"\n",
    "    # Encoder\n",
    "    input_layer = layers.Input(shape=(input_dim,))\n",
    "    encoded = layers.Dense(64, activation='relu')(input_layer)\n",
    "    encoded = layers.Dense(32, activation='relu')(encoded)\n",
    "    encoded = layers.Dense(encoding_dim, activation='linear', name='bottleneck')(encoded)\n",
    "    \n",
    "    # Decoder\n",
    "    decoded = layers.Dense(32, activation='relu')(encoded)\n",
    "    decoded = layers.Dense(64, activation='relu')(decoded)\n",
    "    decoded = layers.Dense(input_dim, activation='linear')(decoded)\n",
    "    \n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    encoder = Model(input_layer, encoded)\n",
    "    \n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    \n",
    "    return autoencoder, encoder\n",
    "\n",
    "def train_autoencoder(df, features, encoding_dim=10, epochs=100):\n",
    "    \"\"\"\n",
    "    Train autoencoder to compress NEPSE features into latent space.\n",
    "    \"\"\"\n",
    "    X = df[features].values\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    autoencoder, encoder = build_feature_autoencoder(len(features), encoding_dim)\n",
    "    \n",
    "    # Train with early stopping\n",
    "    early_stop = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss', patience=10, restore_best_weights=True\n",
    "    )\n",
    "    \n",
    "    autoencoder.fit(\n",
    "        X_scaled, X_scaled,\n",
    "        epochs=epochs,\n",
    "        batch_size=32,\n",
    "        validation_split=0.2,\n",
    "        callbacks=[early_stop],\n",
    "        verbose=0\n",
    "    )\n",
    "    \n",
    "    # Extract encoded features\n",
    "    encoded_features = encoder.predict(X_scaled)\n",
    "    encoded_df = pd.DataFrame(\n",
    "        encoded_features,\n",
    "        columns=[f'AE_{i+1}' for i in range(encoding_dim)],\n",
    "        index=df.index\n",
    "    )\n",
    "    \n",
    "    return encoded_df, autoencoder, encoder\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **16.6 Feature Selection for Time-Series**\n",
    "\n",
    "Time-series feature selection requires special handling to prevent lookahead bias and ensure temporal consistency.\n",
    "\n",
    "```python\n",
    "class TimeSeriesFeatureSelector:\n",
    "    \"\"\"\n",
    "    Feature selection pipeline designed specifically for NEPSE time-series.\n",
    "    \n",
    "    Enforces:\n",
    "    - No future data leakage in feature statistics\n",
    "    - Rolling/expanding windows for stability metrics\n",
    "    - Regime-aware selection (features stable across bull/bear markets)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, date_col='Date'):\n",
    "        self.df = df.sort_values(date_col)\n",
    "        self.selected_features = []\n",
    "        \n",
    "    def temporal_train_test_split(self, features, target, train_end_date):\n",
    "        \"\"\"\n",
    "        Split data temporally: train before date, test after.\n",
    "        \"\"\"\n",
    "        train_mask = self.df['Date'] <= train_end_date\n",
    "        test_mask = self.df['Date'] > train_end_date\n",
    "        \n",
    "        X_train = self.df.loc[train_mask, features]\n",
    "        y_train = self.df.loc[train_mask, target]\n",
    "        X_test = self.df.loc[test_mask, features]\n",
    "        y_test = self.df.loc[test_mask, target]\n",
    "        \n",
    "        return X_train, X_test, y_train, y_test\n",
    "    \n",
    "    def stability_selection(self, features, target, n_splits=5, threshold=0.7):\n",
    "        \"\"\"\n",
    "        Select features that are consistently important across time periods.\n",
    "        \n",
    "        A feature might work well in 2021 (bull market) but fail in 2022 (bear).\n",
    "        We want features robust across regimes.\n",
    "        \"\"\"\n",
    "        # Split into temporal folds\n",
    "        dates = self.df['Date'].quantile(np.linspace(0, 1, n_splits+1)).values\n",
    "        importances = pd.DataFrame(index=features)\n",
    "        \n",
    "        for i in range(n_splits):\n",
    "            train_end = dates[i+1]\n",
    "            X_train, X_test, y_train, y_test = self.temporal_train_test_split(\n",
    "                features, target, train_end\n",
    "            )\n",
    "            \n",
    "            # Train model and get importances\n",
    "            model = xgb.XGBRegressor(n_estimators=50, max_depth=3)\n",
    "            model.fit(X_train, y_train)\n",
    "            \n",
    "            importances[f'period_{i}'] = model.feature_importances_\n",
    "        \n",
    "        # Calculate stability (mean importance / std across periods)\n",
    "        importances['mean'] = importances.mean(axis=1)\n",
    "        importances['std'] = importances.std(axis=1)\n",
    "        importances['stability'] = importances['mean'] / (importances['std'] + 1e-8)\n",
    "        \n",
    "        # Select features with high mean importance and low variance (stable)\n",
    "        stable_features = importances[\n",
    "            (importances['mean'] > importances['mean'].quantile(0.5)) &\n",
    "            (importances['stability'] > threshold)\n",
    "        ].index.tolist()\n",
    "        \n",
    "        return stable_features, importances\n",
    "    \n",
    "    def select_features(self, features, target, method='mutual_info', \n",
    "                       n_features=20, validation_date='2023-01-01'):\n",
    "        \"\"\"\n",
    "        Main selection method with temporal validation.\n",
    "        \"\"\"\n",
    "        # Split data\n",
    "        X_train, X_val, y_train, y_val = self.temporal_train_test_split(\n",
    "            features, target, validation_date\n",
    "        )\n",
    "        \n",
    "        if method == 'mutual_info':\n",
    "            selector = SelectKBest(mutual_info_regression, k=n_features)\n",
    "        elif method == 'f_regression':\n",
    "            from sklearn.feature_selection import f_regression, SelectKBest\n",
    "            selector = SelectKBest(f_regression, k=n_features)\n",
    "        \n",
    "        selector.fit(X_train, y_train)\n",
    "        \n",
    "        # Get selected feature names\n",
    "        mask = selector.get_support()\n",
    "        selected = [f for f, m in zip(features, mask) if m]\n",
    "        \n",
    "        # Validate performance\n",
    "        train_score = selector.score(X_train, y_train).mean()\n",
    "        val_score = selector.score(X_val, y_val).mean()\n",
    "        \n",
    "        print(f\"Selected {len(selected)} features\")\n",
    "        print(f\"Train score: {train_score:.4f}, Val score: {val_score:.4f}\")\n",
    "        print(f\"Features: {selected}\")\n",
    "        \n",
    "        return selected\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **16.7 Stability Analysis**\n",
    "\n",
    "Feature stability measures how consistently a feature performs across different time periods and market regimes in NEPSE.\n",
    "\n",
    "```python\n",
    "def analyze_feature_stability(df, features, target, window=252, step=63):\n",
    "    \"\"\"\n",
    "    Analyze how feature importance changes over time in NEPSE.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    window : int\n",
    "        Rolling window size (252 = 1 year of NEPSE trading)\n",
    "    step : int\n",
    "        Step size between analyses (63 = quarterly re-evaluation)\n",
    "    \n",
    "    Returns stability metrics for each feature.\n",
    "    \"\"\"\n",
    "    stability_results = {}\n",
    "    \n",
    "    for start in range(0, len(df) - window, step):\n",
    "        end = start + window\n",
    "        period_data = df.iloc[start:end]\n",
    "        \n",
    "        # Calculate correlations for this period\n",
    "        period_corr = period_data[features].corrwith(period_data[target])\n",
    "        \n",
    "        period_label = f\"{df.index[start]} to {df.index[end]}\"\n",
    "        stability_results[period_label] = period_corr\n",
    "    \n",
    "    stability_df = pd.DataFrame(stability_results).T\n",
    "    \n",
    "    # Calculate stability metrics\n",
    "    metrics = pd.DataFrame(index=features)\n",
    "    metrics['mean_correlation'] = stability_df.mean()\n",
    "    metrics['std_correlation'] = stability_df.std()\n",
    "    metrics['stability_ratio'] = metrics['mean_correlation'].abs() / (metrics['std_correlation'] + 0.01)\n",
    "    metrics['percent_positive'] = (stability_df > 0).mean()\n",
    "    \n",
    "    # Rank by stability (high mean correlation, low variance)\n",
    "    metrics['stability_score'] = (\n",
    "        metrics['mean_correlation'].abs() * \n",
    "        (1 - metrics['std_correlation']) * \n",
    "        metrics['percent_positive']\n",
    "    )\n",
    "    \n",
    "    return metrics.sort_values('stability_score', ascending=False), stability_df\n",
    "\n",
    "# Interpretation for NEPSE:\n",
    "# - High stability features: Volume trends, 52-week position, long-term MAs\n",
    "#   (These work consistently across bull/bear markets)\n",
    "# - Low stability features: Short-term RSI, daily gaps, news sentiment\n",
    "#   (These work only in specific regimes)\n",
    "#\n",
    "# Strategy: Build ensemble using stable features as base, \n",
    "# add regime-specific features as overlays.\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **16.8 Automated Feature Selection**\n",
    "\n",
    "Automated pipelines combine multiple selection methods to find optimal feature subsets without manual intervention.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "def automated_feature_selection_pipeline(df, features, target, max_features=30):\n",
    "    \"\"\"\n",
    "    Automated multi-stage feature selection for NEPSE.\n",
    "    \n",
    "    Stage 1: Filter - Remove low variance and high correlation\n",
    "    Stage 2: Embedded - LASSO for initial screening\n",
    "    Stage 3: Wrapper - RFE with XGBoost for final selection\n",
    "    Stage 4: Stability - Test across time periods\n",
    "    \"\"\"\n",
    "    print(\"Stage 1: Filter Methods...\")\n",
    "    # Remove low variance\n",
    "    X = df[features]\n",
    "    selector_var = VarianceThreshold(threshold=0.01)\n",
    "    X_var = selector_var.fit_transform(X)\n",
    "    features_var = [f for f, m in zip(features, selector_var.get_support()) if m]\n",
    "    \n",
    "    # Remove high correlation\n",
    "    corr_matrix = X[features_var].corr().abs()\n",
    "    upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "    to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "    features_uncorr = [f for f in features_var if f not in to_drop]\n",
    "    print(f\"  Filtered from {len(features)} to {len(features_uncorr)}\")\n",
    "    \n",
    "    print(\"Stage 2: LASSO Screening...\")\n",
    "    X_scaled = StandardScaler().fit_transform(df[features_uncorr])\n",
    "    lasso = LassoCV(cv=TimeSeriesSplit(5), random_state=42)\n",
    "    lasso.fit(X_scaled, df[target])\n",
    "    features_lasso = [f for f, c in zip(features_uncorr, lasso.coef_) if c != 0]\n",
    "    print(f\"  LASSO selected {len(features_lasso)}\")\n",
    "    \n",
    "    print(\"Stage 3: RFE with XGBoost...\")\n",
    "    if len(features_lasso) > max_features:\n",
    "        rfe = RFE(\n",
    "            estimator=xgb.XGBRegressor(n_estimators=50, max_depth=3),\n",
    "            n_features_to_select=max_features,\n",
    "            step=1\n",
    "        )\n",
    "        rfe.fit(df[features_lasso], df[target])\n",
    "        final_features = [f for f, m in zip(features_lasso, rfe.support_) if m]\n",
    "    else:\n",
    "        final_features = features_lasso\n",
    "    \n",
    "    print(f\"Final selection: {len(final_features)} features\")\n",
    "    return final_features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **16.9 Evaluation and Validation**\n",
    "\n",
    "Validating feature selection ensures the chosen subset generalizes to unseen NEPSE data and future time periods.\n",
    "\n",
    "```python\n",
    "def evaluate_feature_selection(df, all_features, selected_features, target, \n",
    "                              test_start_date='2023-01-01'):\n",
    "    \"\"\"\n",
    "    Compare model performance with full feature set vs selected subset.\n",
    "    \"\"\"\n",
    "    from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "    \n",
    "    # Temporal split\n",
    "    train = df[df['Date'] < test_start_date]\n",
    "    test = df[df['Date'] >= test_start_date]\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    for feature_set, name in [(all_features, 'All_Features'), \n",
    "                              (selected_features, 'Selected')]:\n",
    "        X_train = train[feature_set]\n",
    "        X_test = test[feature_set]\n",
    "        y_train = train[target]\n",
    "        y_test = test[target]\n",
    "        \n",
    "        model = xgb.XGBRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        results[name] = {\n",
    "            'RMSE': np.sqrt(mean_squared_error(y_test, y_pred)),\n",
    "            'MAE': mean_absolute_error(y_test, y_pred),\n",
    "            'R2': r2_score(y_test, y_pred),\n",
    "            'Feature_Count': len(feature_set)\n",
    "        }\n",
    "    \n",
    "    comparison = pd.DataFrame(results).T\n",
    "    print(comparison)\n",
    "    \n",
    "    # Check for improvement\n",
    "    if comparison.loc['Selected', 'RMSE'] < comparison.loc['All_Features', 'RMSE']:\n",
    "        print(\"\\nFeature selection improved generalization!\")\n",
    "    else:\n",
    "        print(\"\\nWarning: Feature selection may have removed predictive features.\")\n",
    "    \n",
    "    return comparison\n",
    "\n",
    "# Key Metrics for NEPSE Feature Selection:\n",
    "# 1. Out-of-time R\u00b2 (should be positive and stable)\n",
    "# 2. RMSE improvement (selected should be equal or better with fewer features)\n",
    "# 3. Sharpe Ratio of strategy using selected features (if trading)\n",
    "# 4. Turnover stability (selected features should persist across retrainings)\n",
    "```\n",
    "\n",
    "**End of Chapter 16**\n",
    "\n",
    "This chapter covered systematic feature selection and dimensionality reduction techniques essential for managing the high-dimensional feature space in NEPSE prediction systems, from filter methods (correlation, mutual information) to wrapper methods (RFE, forward selection) and embedded methods (LASSO, tree importance), with specific attention to time-series validation and stability analysis across market regimes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='15. feature_scaling_and_normalization.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='17. advanced_feature_engineering_techniques.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}