{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 10: Introduction to Feature Engineering**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand what features represent in the context of time-series prediction systems\n",
    "- Identify different types of features applicable to financial time-series data, specifically for the NEPSE (Nepal Stock Exchange) stock prediction system\n",
    "- Apply domain knowledge from financial markets to engineer meaningful features from raw OHLCV (Open, High, Low, Close, Volume) data\n",
    "- Distinguish between feature engineering and feature selection processes\n",
    "- Recognize common pitfalls such as look-ahead bias and data leakage when working with stock market data\n",
    "- Document features effectively for reproducibility and team collaboration\n",
    "\n",
    "---\n",
    "\n",
    "## **10.1 What Are Features?**\n",
    "\n",
    "In machine learning, **features** (also known as variables, attributes, or predictors) are the measurable properties or characteristics of the phenomenon being observed. In the context of our NEPSE stock prediction system, features are the quantifiable attributes derived from historical market data that help the model learn patterns and predict future price movements.\n",
    "\n",
    "Raw data from the NEPSE CSV format contains columns like `Open`, `High`, `Low`, `Close`, `Volume`, `VWAP`, etc. While these raw values can serve as basic features, feature engineering transforms these raw inputs into more informative representations that capture temporal dynamics, statistical properties, and domain-specific patterns.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load NEPSE stock data\n",
    "# CSV Structure: S.No,Symbol,Conf.,Open,High,Low,Close,LTP,Close - LTP,\n",
    "# Close - LTP %,VWAP,Vol,Prev. Close,Turnover,Trans.,Diff,Range,Diff %,\n",
    "# Range %,VWAP %,52 Weeks High,52 Weeks Low\n",
    "\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "\n",
    "# Display basic information about the raw features\n",
    "print(\"Raw Feature Columns:\", df.columns.tolist())\n",
    "print(\"\\nFirst 5 rows of raw features:\")\n",
    "print(df[['Symbol', 'Open', 'High', 'Low', 'Close', 'Vol']].head())\n",
    "\n",
    "# Basic statistical features from raw data\n",
    "print(\"\\nBasic Statistics of Close Price:\")\n",
    "print(df['Close'].describe())\n",
    "\n",
    "# Creating a simple feature: Daily Return\n",
    "# This is a derived feature showing percentage change from previous close\n",
    "df['Daily_Return'] = ((df['Close'] - df['Prev. Close']) / df['Prev. Close']) * 100\n",
    "\n",
    "print(\"\\nDaily Return Feature (First 5 values):\")\n",
    "print(df[['Symbol', 'Close', 'Prev. Close', 'Daily_Return']].head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The code above demonstrates the fundamental concept of features using the NEPSE dataset. \n",
    "\n",
    "First, we load the CSV data containing the raw market data. The columns `Open`, `High`, `Low`, `Close`, and `Vol` represent the most basic features—raw price and volume data points recorded during the trading session.\n",
    "\n",
    "The `df['Close'].describe()` operation generates statistical features (mean, standard deviation, min, max) that characterize the distribution of closing prices. These statistical summaries serve as aggregate features that capture the central tendency and volatility of the stock.\n",
    "\n",
    "Finally, we engineer a new feature called `Daily_Return` by calculating the percentage change between the current `Close` and `Prev. Close`. This derived feature is crucial for time-series prediction because it normalizes price changes across different stocks and time periods, making patterns more comparable and learnable by machine learning models. The daily return captures the momentum and direction of price movement, which is often more predictive than absolute price levels.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.2 The Feature Engineering Process**\n",
    "\n",
    "Feature engineering is the systematic process of transforming raw data into features that better represent the underlying problem to the predictive models, resulting in improved model accuracy. For the NEPSE prediction system, this process involves a structured pipeline that converts raw OHLCV data into a rich feature set suitable for machine learning algorithms.\n",
    "\n",
    "The process typically follows these stages: **Data Understanding** (exploring the NEPSE CSV structure), **Data Cleaning** (handling missing values in volume or price data), **Feature Construction** (creating lag features, technical indicators), **Feature Transformation** (normalization, scaling), and **Feature Validation** (checking for data leakage and stationarity).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NEPSEFeatureEngineer:\n",
    "    \"\"\"\n",
    "    A comprehensive feature engineering pipeline for NEPSE stock data.\n",
    "    This class encapsulates the entire feature engineering process.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.raw_data = None\n",
    "        self.engineered_data = None\n",
    "        self.scaler = StandardScaler()\n",
    "        \n",
    "    def load_data(self):\n",
    "        \"\"\"Stage 1: Data Loading and Initial Understanding\"\"\"\n",
    "        self.raw_data = pd.read_csv(self.data_path)\n",
    "        \n",
    "        # Convert date columns if present, otherwise create sequence\n",
    "        if 'Date' in self.raw_data.columns:\n",
    "            self.raw_data['Date'] = pd.to_datetime(self.raw_data['Date'])\n",
    "            self.raw_data = self.raw_data.sort_values('Date')\n",
    "        else:\n",
    "            # If no date column, assume data is ordered by S.No\n",
    "            self.raw_data = self.raw_data.sort_values('S.No')\n",
    "            \n",
    "        print(f\"Data loaded: {len(self.raw_data)} records\")\n",
    "        print(f\"Columns: {self.raw_data.columns.tolist()}\")\n",
    "        return self\n",
    "    \n",
    "    def clean_data(self):\n",
    "        \"\"\"Stage 2: Data Cleaning and Preprocessing\"\"\"\n",
    "        # Handle missing values in critical columns\n",
    "        critical_columns = ['Open', 'High', 'Low', 'Close', 'Vol']\n",
    "        \n",
    "        for col in critical_columns:\n",
    "            if col in self.raw_data.columns:\n",
    "                # Forward fill for missing values (carry last known value)\n",
    "                self.raw_data[col] = self.raw_data[col].fillna(method='ffill')\n",
    "                # Backward fill for any remaining NaNs at the beginning\n",
    "                self.raw_data[col] = self.raw_data[col].fillna(method='bfill')\n",
    "        \n",
    "        # Remove outliers (prices that are unrealistic)\n",
    "        # Using IQR method for Close price\n",
    "        Q1 = self.raw_data['Close'].quantile(0.25)\n",
    "        Q3 = self.raw_data['Close'].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        # Filter out extreme outliers\n",
    "        self.raw_data = self.raw_data[\n",
    "            (self.raw_data['Close'] >= lower_bound) & \n",
    "            (self.raw_data['Close'] <= upper_bound)\n",
    "        ]\n",
    "        \n",
    "        print(f\"Data cleaned: {len(self.raw_data)} records remaining\")\n",
    "        return self\n",
    "    \n",
    "    def construct_features(self):\n",
    "        \"\"\"Stage 3: Feature Construction\"\"\"\n",
    "        df = self.raw_data.copy()\n",
    "        \n",
    "        # 3.1 Lag Features (Previous values)\n",
    "        # These capture temporal dependencies\n",
    "        for lag in [1, 3, 5]:\n",
    "            df[f'Close_Lag_{lag}'] = df['Close'].shift(lag)\n",
    "            df[f'Volume_Lag_{lag}'] = df['Vol'].shift(lag)\n",
    "        \n",
    "        # 3.2 Rolling Window Features (Moving statistics)\n",
    "        # These capture trends and volatility over time\n",
    "        windows = [5, 10, 20]\n",
    "        for window in windows:\n",
    "            # Moving averages (trend indicators)\n",
    "            df[f'SMA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "            df[f'EMA_{window}'] = df['Close'].ewm(span=window).mean()\n",
    "            \n",
    "            # Volatility indicators\n",
    "            df[f'Std_{window}'] = df['Close'].rolling(window=window).std()\n",
    "            df[f'Range_{window}'] = (\n",
    "                df['High'].rolling(window=window).max() - \n",
    "                df['Low'].rolling(window=window).min()\n",
    "            )\n",
    "            \n",
    "            # Volume indicators\n",
    "            df[f'Vol_SMA_{window}'] = df['Vol'].rolling(window=window).mean()\n",
    "        \n",
    "        # 3.3 Technical Indicators (Domain-specific features)\n",
    "        # Relative Strength Index (RSI)\n",
    "        delta = df['Close'].diff()\n",
    "        gain = (delta.where(delta > 0, 0)).rolling(window=14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(window=14).mean()\n",
    "        rs = gain / loss\n",
    "        df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD (Moving Average Convergence Divergence)\n",
    "        ema_12 = df['Close'].ewm(span=12).mean()\n",
    "        ema_26 = df['Close'].ewm(span=26).mean()\n",
    "        df['MACD'] = ema_12 - ema_26\n",
    "        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "        \n",
    "        # 3.4 Time-based Features\n",
    "        if 'Date' in df.columns:\n",
    "            df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "            df['Month'] = df['Date'].dt.month\n",
    "            df['Quarter'] = df['Date'].dt.quarter\n",
    "            df['Is_Month_Start'] = df['Date'].dt.is_month_start.astype(int)\n",
    "            df['Is_Month_End'] = df['Date'].dt.is_month_end.astype(int)\n",
    "        \n",
    "        # 3.5 Price-based Features\n",
    "        df['Daily_Return'] = df['Close'].pct_change()\n",
    "        df['Price_Range'] = df['High'] - df['Low']\n",
    "        df['Price_Change'] = df['Close'] - df['Open']\n",
    "        df['Upper_Shadow'] = df['High'] - df[['Close', 'Open']].max(axis=1)\n",
    "        df['Lower_Shadow'] = df[['Close', 'Open']].min(axis=1) - df['Low']\n",
    "        \n",
    "        # Remove rows with NaN values created by lag/rolling operations\n",
    "        df = df.dropna()\n",
    "        \n",
    "        self.engineered_data = df\n",
    "        print(f\"Features constructed: {len(df.columns)} features\")\n",
    "        print(f\"Feature columns: {df.columns.tolist()}\")\n",
    "        return self\n",
    "    \n",
    "    def transform_features(self):\n",
    "        \"\"\"Stage 4: Feature Transformation and Scaling\"\"\"\n",
    "        # Separate features and target\n",
    "        feature_cols = [col for col in self.engineered_data.columns \n",
    "                       if col not in ['Close', 'Symbol', 'Date', 'S.No', 'Conf.']]\n",
    "        \n",
    "        X = self.engineered_data[feature_cols]\n",
    "        y = self.engineered_data['Close']\n",
    "        \n",
    "        # Scale features\n",
    "        X_scaled = self.scaler.fit_transform(X)\n",
    "        X_scaled_df = pd.DataFrame(X_scaled, columns=feature_cols, index=X.index)\n",
    "        \n",
    "        print(f\"Features transformed and scaled\")\n",
    "        print(f\"Feature matrix shape: {X_scaled_df.shape}\")\n",
    "        return X_scaled_df, y\n",
    "    \n",
    "    def validate_features(self):\n",
    "        \"\"\"Stage 5: Feature Validation\"\"\"\n",
    "        # Check for data leakage (features that use future information)\n",
    "        # In a real system, this would be more sophisticated\n",
    "        \n",
    "        # Check for stationarity (important for time-series)\n",
    "        from scipy import stats\n",
    "        \n",
    "        close_prices = self.engineered_data['Close']\n",
    "        adf_result = stats.adfuller(close_prices)\n",
    "        \n",
    "        print(f\"ADF Statistic: {adf_result[0]}\")\n",
    "        print(f\"p-value: {adf_result[1]}\")\n",
    "        if adf_result[1] <= 0.05:\n",
    "            print(\"Series is stationary\")\n",
    "        else:\n",
    "            print(\"Series is non-stationary (consider differencing)\")\n",
    "        \n",
    "        # Check for multicollinearity\n",
    "        corr_matrix = self.engineered_data[['Close', 'SMA_5', 'SMA_10', 'RSI']].corr()\n",
    "        print(\"\\nCorrelation Matrix:\")\n",
    "        print(corr_matrix)\n",
    "        \n",
    "        return self\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the feature engineering pipeline\n",
    "    pipeline = NEPSEFeatureEngineer('nepse_data.csv')\n",
    "    \n",
    "    # Execute the pipeline stages\n",
    "    (pipeline\n",
    "     .load_data()\n",
    "     .clean_data()\n",
    "     .construct_features()\n",
    "     .validate_features())\n",
    "    \n",
    "    # Get transformed features for modeling\n",
    "    X, y = pipeline.transform_features()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This comprehensive example demonstrates the complete feature engineering process for the NEPSE stock prediction system through a structured pipeline class.\n",
    "\n",
    "**Stage 1: Data Loading and Understanding**\n",
    "The `load_data()` method initializes the pipeline by loading the NEPSE CSV data. It handles the specific structure of NEPSE data, sorting by `S.No` (serial number) if no date column exists, or by `Date` if available. This ensures temporal ordering is maintained, which is critical for time-series feature engineering.\n",
    "\n",
    "**Stage 2: Data Cleaning**\n",
    "The `clean_data()` method addresses data quality issues specific to financial time-series. It uses forward-fill (`ffill`) and backward-fill (`bfill`) methods to handle missing values, which is appropriate for stock data where the last known price is a reasonable estimate for brief gaps. It also implements outlier detection using the Interquartile Range (IQR) method to remove erroneous price spikes that could distort feature calculations.\n",
    "\n",
    "**Stage 3: Feature Construction**\n",
    "This is the core stage where raw NEPSE data is transformed into predictive features:\n",
    "\n",
    "- **Lag Features**: The code creates `Close_Lag_1`, `Close_Lag_3`, and `Close_Lag_5`, which represent the closing price 1, 3, and 5 periods ago. These capture temporal dependencies and autocorrelation in stock prices.\n",
    "\n",
    "- **Rolling Window Features**: Simple Moving Averages (`SMA_5`, `SMA_10`, `SMA_20`) and Exponential Moving Averages (`EMA`) are calculated to identify trends. Standard deviation (`Std`) and range features capture volatility over different time horizons.\n",
    "\n",
    "- **Technical Indicators**: The Relative Strength Index (RSI) measures momentum and overbought/oversold conditions. MACD (Moving Average Convergence Divergence) identifies trend changes and momentum shifts. These are domain-specific features derived from financial technical analysis.\n",
    "\n",
    "- **Time-Based Features**: Day of week, month, quarter, and month-start/end indicators capture calendar effects and seasonal patterns in trading behavior.\n",
    "\n",
    "- **Price-Based Features**: Daily returns, price ranges, upper/lower shadows (candlestick patterns), and price changes provide normalized measures of price action.\n",
    "\n",
    "**Stage 4: Feature Transformation**\n",
    "The `transform_features()` method applies `StandardScaler` to normalize feature distributions. This is essential because features like volume (in millions) and RSI (0-100) operate on vastly different scales. Standardization ensures that the model treats all features equally during training.\n",
    "\n",
    "**Stage 5: Feature Validation**\n",
    "The `validate_features()` method checks for stationarity using the Augmented Dickey-Fuller (ADF) test, which is crucial for time-series models. It also examines the correlation matrix to detect multicollinearity between features (e.g., high correlation between `Close` and `SMA_5`), which could cause instability in linear models.\n",
    "\n",
    "This structured pipeline ensures that the NEPSE stock prediction system receives high-quality, informative features that capture the complex temporal dynamics of the Nepalese stock market while avoiding common pitfalls like data leakage and look-ahead bias.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.4 Domain Knowledge Integration**\n",
    "\n",
    "Domain knowledge integration is the process of incorporating expert understanding of a specific field into the feature engineering pipeline. For the NEPSE stock prediction system, this means leveraging financial market theories, trading practices, and economic principles specific to the Nepalese stock market to create features that capture meaningful market behaviors rather than just statistical patterns.\n",
    "\n",
    "The Nepalese stock market has unique characteristics: it operates Sunday through Thursday (unlike Western markets), has specific regulatory constraints, experiences high volatility due to limited liquidity, and shows strong seasonal patterns around fiscal year endings (mid-July). Understanding these nuances allows us to engineer features that a generic time-series approach might miss.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "class NEPSEDomainFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Feature engineering class that integrates Nepalese stock market domain knowledge.\n",
    "    NEPSE-specific characteristics:\n",
    "    - Trading days: Sunday to Thursday (5 days a week)\n",
    "    - High volatility, low liquidity compared to major exchanges\n",
    "    - Strong fiscal year effects (fiscal year ends mid-July)\n",
    "    - Circuit breakers: 4% (first), 5% (second), 6% (third) daily limits\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self._ensure_datetime()\n",
    "        \n",
    "    def _ensure_datetime(self):\n",
    "        \"\"\"Ensure Date column is datetime type\"\"\"\n",
    "        if 'Date' in self.df.columns:\n",
    "            self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
    "            self.df = self.df.sort_values('Date')\n",
    "    \n",
    "    def add_nepse_trading_calendar_features(self):\n",
    "        \"\"\"\n",
    "        NEPSE trades Sunday-Thursday.\n",
    "        These features capture trading day effects specific to Nepalese market.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Day of week (0=Sunday, 4=Thursday in NEPSE context)\n",
    "        # Note: In pandas, Monday=0, Sunday=6, so we adjust\n",
    "        df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "        df['Is_Sunday'] = (df['Day_of_Week'] == 6).astype(int)  # Sunday often has different volume\n",
    "        df['Is_Thursday'] = (df['Day_of_Week'] == 4).astype(int)  # Thursday is last trading day\n",
    "        \n",
    "        # Weekend gap (days since last trading day)\n",
    "        # In NEPSE, Friday and Saturday are weekend\n",
    "        df['Days_Since_Last_Trade'] = df['Date'].diff().dt.days\n",
    "        \n",
    "        # Trading week of month (NEPSE has ~4 trading weeks per month)\n",
    "        df['Trading_Week'] = (df['Date'].dt.day - 1) // 7 + 1\n",
    "        \n",
    "        self.df = df\n",
    "        return self\n",
    "    \n",
    "    def add_fiscal_year_features(self):\n",
    "        \"\"\"\n",
    "        Nepal's fiscal year runs mid-July to mid-July (roughly July 16 - July 15).\n",
    "        This creates unique seasonal patterns in stock trading due to:\n",
    "        - Year-end portfolio adjustments\n",
    "        - Tax considerations\n",
    "        - Quarterly reporting cycles\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Nepali fiscal year (approximate mapping using Gregorian calendar)\n",
    "        # FY starts ~July 16\n",
    "        month = df['Date'].dt.month\n",
    "        day = df['Date'].dt.day\n",
    "        \n",
    "        # Determine fiscal year (year starts in July)\n",
    "        fiscal_year = df['Date'].dt.year\n",
    "        # If before July 16, it's part of previous fiscal year\n",
    "        mask = (month < 7) | ((month == 7) & (day < 16))\n",
    "        fiscal_year[mask] = fiscal_year[mask] - 1\n",
    "        \n",
    "        df['Fiscal_Year'] = fiscal_year\n",
    "        \n",
    "        # Fiscal quarter (Q1: Jul-Sep, Q2: Oct-Dec, Q3: Jan-Mar, Q4: Apr-Jul)\n",
    "        quarter_mapping = {7: 1, 8: 1, 9: 1, \n",
    "                          10: 2, 11: 2, 12: 2,\n",
    "                          1: 3, 2: 3, 3: 3,\n",
    "                          4: 4, 5: 4, 6: 4, 7: 4}\n",
    "        \n",
    "        # Adjust for fiscal year quarters\n",
    "        df['Fiscal_Quarter'] = month.map(lambda m: \n",
    "            1 if m in [7, 8, 9] else\n",
    "            2 if m in [10, 11, 12] else\n",
    "            3 if m in [1, 2, 3] else 4\n",
    "        )\n",
    "        \n",
    "        # Days to fiscal year end (important for tax-loss harvesting patterns)\n",
    "        # FY ends ~July 15\n",
    "        year_end = pd.to_datetime(df['Date'].dt.year.astype(str) + '-07-15')\n",
    "        # Adjust for dates after July 15 (belong to next FY end)\n",
    "        year_end[df['Date'].dt.month >= 7] = pd.to_datetime(\n",
    "            (df['Date'].dt.year + 1).astype(str) + '-07-15'\n",
    "        )\n",
    "        \n",
    "        df['Days_to_FY_End'] = (year_end - df['Date']).dt.days\n",
    "        \n",
    "        self.df = df\n",
    "        return self\n",
    "    \n",
    "    def add_circuit_breaker_features(self):\n",
    "        \"\"\"\n",
    "        NEPSE has circuit breakers at 4%, 5%, and 6% daily price limits.\n",
    "        These features identify when stocks are hitting these limits,\n",
    "        which indicates extreme sentiment and potential reversal points.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Calculate daily price change percentage\n",
    "        df['Daily_Change_Pct'] = ((df['Close'] - df['Prev. Close']) / df['Prev. Close']) * 100\n",
    "        \n",
    "        # Circuit breaker levels (NEPSE specific)\n",
    "        df['Hit_Upper_Circuit'] = (df['Daily_Change_Pct'] >= 4).astype(int)\n",
    "        df['Hit_Lower_Circuit'] = (df['Daily_Change_Pct'] <= -4).astype(int)\n",
    "        \n",
    "        # Proximity to circuit breakers (how close to 4% limit)\n",
    "        df['Upper_Circuit_Distance'] = 4 - df['Daily_Change_Pct']\n",
    "        df['Lower_Circuit_Distance'] = df['Daily_Change_Pct'] - (-4)\n",
    "        \n",
    "        # Volatility regime (based on how often circuit breakers are hit)\n",
    "        df['Circuit_Breaker_Regime'] = df['Hit_Upper_Circuit'].rolling(window=20).sum() - \\\n",
    "                                       df['Hit_Lower_Circuit'].rolling(window=20).sum()\n",
    "        \n",
    "        self.df = df\n",
    "        return self\n",
    "    \n",
    "    def get_feature_summary(self):\n",
    "        \"\"\"Return summary of engineered features\"\"\"\n",
    "        feature_categories = {\n",
    "            'Raw_Price': ['Open', 'High', 'Low', 'Close', 'LTP'],\n",
    "            'Volume': ['Vol', 'Turnover', 'Trans.'],\n",
    "            'Calendar': ['Day_of_Week', 'Is_Sunday', 'Is_Thursday', 'Fiscal_Year', 'Fiscal_Quarter'],\n",
    "            'Lag': [col for col in self.df.columns if 'Lag_' in col],\n",
    "            'Rolling': [col for col in self.df.columns if 'SMA_' in col or 'EMA_' in col or 'Std_' in col],\n",
    "            'Technical': ['RSI', 'MACD', 'MACD_Signal'],\n",
    "            'Circuit_Breaker': ['Daily_Change_Pct', 'Hit_Upper_Circuit', 'Circuit_Breaker_Regime']\n",
    "        }\n",
    "        \n",
    "        summary = {}\n",
    "        for category, features in feature_categories.items():\n",
    "            available = [f for f in features if f in self.df.columns]\n",
    "            if available:\n",
    "                summary[category] = len(available)\n",
    "        \n",
    "        return summary\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize the feature engineering pipeline\n",
    "    engineer = NEPSEFeatureEngineer('nepse_data.csv')\n",
    "    \n",
    "    # Execute the complete pipeline\n",
    "    (engineer\n",
    "     .load_data()\n",
    "     .clean_data()\n",
    "     .add_nepse_trading_calendar_features()\n",
    "     .add_fiscal_year_features()\n",
    "     .add_circuit_breaker_features()\n",
    "     .construct_features())\n",
    "    \n",
    "    # Get feature summary\n",
    "    summary = engineer.get_feature_summary()\n",
    "    print(\"\\nFeature Engineering Summary:\")\n",
    "    for category, count in summary.items():\n",
    "        print(f\"  {category}: {count} features\")\n",
    "    \n",
    "    # Display sample of engineered features\n",
    "    print(\"\\nSample of Engineered Features:\")\n",
    "    feature_cols = [col for col in engineer.df.columns \n",
    "                   if col not in ['S.No', 'Symbol', 'Conf.']]\n",
    "    print(engineer.df[feature_cols[:10]].head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This comprehensive example demonstrates the feature engineering process specifically tailored for the NEPSE stock prediction system, integrating domain knowledge unique to the Nepalese stock market.\n",
    "\n",
    "**The Pipeline Architecture:**\n",
    "The `NEPSEFeatureEngineer` class encapsulates the entire feature engineering workflow as a fluent interface (method chaining), allowing stages to be executed sequentially. This design pattern ensures reproducibility and makes the pipeline easy to modify or extend.\n",
    "\n",
    "**Stage 1: Data Loading and Understanding**\n",
    "The `load_data()` method handles the specific NEPSE CSV structure. It recognizes that NEPSE data may or may not have explicit Date columns, falling back to `S.No` (serial number) for temporal ordering. This is crucial because NEPSE data exports sometimes lack proper timestamps, relying instead on sequential ordering.\n",
    "\n",
    "**Stage 2: Data Cleaning**\n",
    "The `clean_data()` method addresses real-world data quality issues in NEPSE feeds. It uses forward-fill and backward-fill for missing values, which is appropriate for financial time-series where the last traded price remains valid until the next trade. It also implements the Interquartile Range (IQR) method to remove extreme outliers that might represent data entry errors or flash crashes not representative of normal market conditions.\n",
    "\n",
    "**Stage 3: Domain-Specific Feature Engineering**\n",
    "This is where deep NEPSE domain knowledge is integrated:\n",
    "\n",
    "*Trading Calendar Features*: Unlike major international exchanges that trade Monday-Friday, NEPSE operates Sunday through Thursday. The code creates binary indicators `Is_Sunday` and `Is_Thursday` because these days often show different trading patterns (Sunday being the first day after the weekend gap, Thursday being the pre-weekend close). The `Days_Since_Last_Trade` feature accounts for the Friday-Saturday weekend gap unique to NEPSE.\n",
    "\n",
    "*Fiscal Year Features*: Nepal's fiscal year runs from mid-July to mid-July (Shrawan to Ashad in the Nepali calendar). This creates distinct seasonal patterns in stock trading due to fiscal year-end portfolio adjustments, tax-loss harvesting, and quarterly earnings cycles aligned with this calendar. The code calculates `Fiscal_Year`, `Fiscal_Quarter`, and `Days_to_FY_End` to capture these seasonal effects.\n",
    "\n",
    "*Circuit Breaker Features*: NEPSE implements circuit breakers at 4%, 5%, and 6% daily price movement limits. When a stock hits these limits, it indicates extreme sentiment and potential reversal points. The code creates binary indicators for upper/lower circuit hits and calculates proximity to these limits, which serve as regime indicators for volatility clustering.\n",
    "\n",
    "**Stage 4: Feature Transformation**\n",
    "After construction, features are standardized using `StandardScaler` to ensure all features contribute equally to the model regardless of their original scales (e.g., volume in millions vs. RSI in 0-100).\n",
    "\n",
    "**Stage 5: Validation**\n",
    "The pipeline includes validation checks for stationarity using the Augmented Dickey-Fuller test (essential for time-series models) and correlation analysis to detect multicollinearity between derived features.\n",
    "\n",
    "This domain-integrated approach ensures that the NEPSE prediction model leverages not just statistical patterns but the specific structural characteristics of the Nepalese stock market, from its unique trading calendar to its regulatory circuit breaker mechanisms.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.3 Feature Types in Time-Series**\n",
    "\n",
    "Time-series feature engineering requires specialized approaches that account for temporal dependencies, sequential patterns, and the non-stationary nature of financial data. Unlike cross-sectional data where observations are independent, time-series features must respect the arrow of time and capture the evolving statistical properties of the series.\n",
    "\n",
    "For the NEPSE prediction system, we categorize features into several distinct types, each capturing different aspects of market behavior: **Lag Features** (historical values), **Rolling Window Features** (moving statistics), **Expanding Window Features** (cumulative statistics), **Date/Time Features** (temporal context), **Domain-Specific Indicators** (technical analysis), and **Interaction Features** (combinations of raw features).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import talib  # Technical Analysis Library\n",
    "from scipy.stats import skew, kurtosis\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class NEPSETimeSeriesFeatures:\n",
    "    \"\"\"\n",
    "    Comprehensive time-series feature engineering for NEPSE stock data.\n",
    "    Implements various feature types specific to financial time-series.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.feature_metadata = {}\n",
    "        \n",
    "    def create_lag_features(self, columns: List[str], lags: List[int]) -> 'NEPSETimeSeriesFeatures':\n",
    "        \"\"\"\n",
    "        Create lag features (autoregressive features).\n",
    "        \n",
    "        Lag features are the most fundamental time-series features, representing\n",
    "        the value of a variable at previous time steps. They capture temporal\n",
    "        dependencies and autocorrelation structures.\n",
    "        \n",
    "        For NEPSE: Lag-1 of Close price represents yesterday's closing price,\n",
    "        which is often the strongest predictor of today's price.\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "                \n",
    "            for lag in lags:\n",
    "                feature_name = f'{col}_Lag_{lag}'\n",
    "                self.df[feature_name] = self.df[col].shift(lag)\n",
    "                \n",
    "                # Store metadata\n",
    "                self.feature_metadata[feature_name] = {\n",
    "                    'type': 'lag',\n",
    "                    'source_column': col,\n",
    "                    'lag_periods': lag,\n",
    "                    'description': f'{col} value from {lag} periods ago'\n",
    "                }\n",
    "        \n",
    "        print(f\"Created {len(lags) * len(columns)} lag features\")\n",
    "        return self\n",
    "    \n",
    "    def create_rolling_window_features(self, columns: List[str], \n",
    "                                     windows: List[int]) -> 'NEPSETimeSeriesFeatures':\n",
    "        \"\"\"\n",
    "        Create rolling window (moving) statistics.\n",
    "        \n",
    "        Rolling window features calculate statistics over a fixed window of\n",
    "        recent observations. They capture local trends, volatility, and\n",
    "        momentum while adapting to changing market conditions.\n",
    "        \n",
    "        For NEPSE: A 20-day rolling average represents the monthly trend\n",
    "        (since NEPSE trades ~20 days per month).\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "                \n",
    "            for window in windows:\n",
    "                # Statistical measures\n",
    "                self.df[f'{col}_SMA_{window}'] = self.df[col].rolling(window=window).mean()\n",
    "                self.df[f'{col}_EMA_{window}'] = self.df[col].ewm(span=window, adjust=False).mean()\n",
    "                self.df[f'{col}_Std_{window}'] = self.df[col].rolling(window=window).std()\n",
    "                self.df[f'{col}_Min_{window}'] = self.df[col].rolling(window=window).min()\n",
    "                self.df[f'{col}_Max_{window}'] = self.df[col].rolling(window=window).max()\n",
    "                self.df[f'{col}_Range_{window}'] = (\n",
    "                    self.df[f'{col}_Max_{window}'] - self.df[f'{col}_Min_{window}']\n",
    "                )\n",
    "                \n",
    "                # Higher-order statistics\n",
    "                self.df[f'{col}_Skew_{window}'] = self.df[col].rolling(window=window).skew()\n",
    "                self.df[f'{col}_Kurt_{window}'] = self.df[col].rolling(window=window).kurt()\n",
    "                \n",
    "                # Technical indicators derived from rolling windows\n",
    "                # Rate of Change (ROC)\n",
    "                self.df[f'{col}_ROC_{window}'] = (\n",
    "                    (self.df[col] - self.df[col].shift(window)) / \n",
    "                    self.df[col].shift(window) * 100\n",
    "                )\n",
    "                \n",
    "        print(f\"Created rolling window features for windows: {windows}\")\n",
    "        return self\n",
    "    \n",
    "    def create_expanding_window_features(self, columns: List[str]) -> 'NEPSETimeSeriesFeatures':\n",
    "        \"\"\"\n",
    "        Create expanding window (cumulative) statistics.\n",
    "        \n",
    "        Expanding window features calculate statistics from the beginning of\n",
    "        the series up to the current point. They capture long-term trends\n",
    "        and historical context that rolling windows might miss.\n",
    "        \n",
    "        For NEPSE: Expanding mean shows the all-time average price trend,\n",
    "        useful for identifying long-term support/resistance levels.\n",
    "        \"\"\"\n",
    "        for col in columns:\n",
    "            if col not in self.df.columns:\n",
    "                continue\n",
    "            \n",
    "            # Cumulative statistics\n",
    "            self.df[f'{col}_Exp_Mean'] = self.df[col].expanding().mean()\n",
    "            self.df[f'{col}_Exp_Std'] = self.df[col].expanding().std()\n",
    "            self.df[f'{col}_Exp_Min'] = self.df[col].expanding().min()\n",
    "            self.df[f'{col}_Exp_Max'] = self.df[col].expanding().max()\n",
    "            \n",
    "            # Cumulative returns (since inception)\n",
    "            self.df[f'{col}_Cumulative_Return'] = (\n",
    "                (self.df[col] / self.df[col].iloc[0]) - 1\n",
    "            ) * 100\n",
    "            \n",
    "            # Running count of observations\n",
    "            self.df[f'{col}_Count'] = self.df[col].expanding().count()\n",
    "            \n",
    "        print(\"Created expanding window features\")\n",
    "        return self\n",
    "    \n",
    "    def create_datetime_features(self) -> 'NEPSETimeSeriesFeatures':\n",
    "        \"\"\"\n",
    "        Create time-based features specific to NEPSE trading patterns.\n",
    "        \n",
    "        Temporal features capture cyclical patterns, seasonality, and\n",
    "        calendar effects that influence stock prices.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        if 'Date' not in df.columns:\n",
    "            print(\"No Date column found, skipping datetime features\")\n",
    "            return self\n",
    "        \n",
    "        # Basic temporal features\n",
    "        df['Year'] = df['Date'].dt.year\n",
    "        df['Month'] = df['Date'].dt.month\n",
    "        df['Day'] = df['Date'].dt.day\n",
    "        df['Day_of_Week'] = df['Date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "        df['Quarter'] = df['Date'].dt.quarter\n",
    "        df['Day_of_Year'] = df['Date'].dt.dayofyear\n",
    "        \n",
    "        # NEPSE-specific trading patterns\n",
    "        # NEPSE trades Sunday-Thursday (Friday-Saturday weekend)\n",
    "        df['Is_Weekend_Proximity'] = ((df['Day_of_Week'] == 4) |  # Thursday (last trading day)\n",
    "                                       (df['Day_of_Week'] == 6)).astype(int)  # Sunday (first trading day)\n",
    "        \n",
    "        # Month-end effects (common in emerging markets)\n",
    "        df['Is_Month_End'] = (df['Date'].dt.is_month_end).astype(int)\n",
    "        df['Days_to_Month_End'] = (df['Date'] + pd.offsets.MonthEnd(0) - df['Date']).dt.days\n",
    "        \n",
    "        # Fiscal year effects (Nepal: mid-July to mid-July)\n",
    "        # Q4 (April-July) often shows different patterns due to year-end accounting\n",
    "        df['Is_Fiscal_Q4'] = (df['Fiscal_Quarter'] == 4).astype(int)\n",
    "        \n",
    "        # Cyclical encoding (convert cyclical features to sine/cosine)\n",
    "        # This preserves the cyclical nature (e.g., December is close to January)\n",
    "        df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "        df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "        \n",
    "        df['Day_of_Week_Sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "        df['Day_of_Week_Cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "        \n",
    "        self.df = df\n",
    "        print(\"Created datetime features including NEPSE-specific patterns\")\n",
    "        return self\n",
    "\n",
    "# Example usage with NEPSE data\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample NEPSE data\n",
    "    dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='B')\n",
    "    # Filter for Sunday-Thursday (simplified for example)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    sample_data = pd.DataFrame({\n",
    "        'Date': dates[:100],\n",
    "        'Open': np.random.uniform(100, 200, 100),\n",
    "        'High': np.random.uniform(200, 210, 100),\n",
    "        'Low': np.random.uniform(90, 100, 100),\n",
    "        'Close': np.random.uniform(150, 200, 100),\n",
    "        'Vol': np.random.randint(1000, 10000, 100),\n",
    "        'Prev. Close': np.random.uniform(150, 200, 100)\n",
    "    })\n",
    "    \n",
    "    # Initialize feature engineer\n",
    "    fe = NEPSEDomainFeatureEngineer(sample_data)\n",
    "    \n",
    "    # Create all feature types\n",
    "    (fe\n",
    "     .create_lag_features(['Close', 'Vol'], [1, 3, 5])\n",
    "     .create_rolling_window_features(['Close'], [5, 10, 20])\n",
    "     .create_expanding_window_features(['Close'])\n",
    "     .create_datetime_features())\n",
    "    \n",
    "    print(\"\\nFinal Feature Set Shape:\", fe.df.shape)\n",
    "    print(\"\\nFeature Categories:\")\n",
    "    for col in fe.df.columns:\n",
    "        print(f\"  - {col}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This comprehensive implementation demonstrates domain knowledge integration for the NEPSE stock prediction system by creating specialized features that capture the unique characteristics of the Nepalese stock market.\n",
    "\n",
    "**NEPSE-Specific Domain Knowledge Implementation:**\n",
    "\n",
    "The `NEPSEDomainFeatureEngineer` class encapsulates specialized knowledge about the Nepalese stock market that generic time-series libraries would not automatically handle. \n",
    "\n",
    "**Trading Calendar Features:**\n",
    "Unlike major exchanges (NYSE, NASDAQ) that trade Monday-Friday, NEPSE operates Sunday through Thursday with Friday-Saturday weekends. The `add_nepse_trading_calendar_features()` method (integrated into `create_datetime_features()`) creates binary indicators `Is_Sunday` and `Is_Thursday` because these boundary days exhibit different trading patterns—Sunday often sees gap openings reacting to weekend news, while Thursday shows position squaring before the weekend. The `Days_Since_Last_Trade` feature accounts for the Friday-Saturday gap, which is longer than overnight gaps and often leads to higher volatility.\n",
    "\n",
    "**Fiscal Year Features:**\n",
    "Nepal's fiscal year runs from mid-July to mid-July (Shrawan to Ashad in the Nepali calendar), differing from the calendar year used by most international markets. The `add_fiscal_year_features()` method calculates `Fiscal_Year`, `Fiscal_Quarter`, and `Is_Fiscal_Q4`. Q4 (April-July) is particularly important in NEPSE because companies finalize annual reports, dividends are announced, and institutional investors rebalance portfolios for fiscal year-end reporting. This creates distinct seasonal patterns that calendar-year-based features would miss.\n",
    "\n",
    "**Circuit Breaker Features:**\n",
    "NEPSE implements circuit breakers at 4%, 5%, and 6% daily price movement limits. The `add_circuit_breaker_features()` method creates `Hit_Upper_Circuit` and `Hit_Lower_Circuit` binary indicators, along with `Circuit_Breaker_Regime` which tracks the rolling count of circuit breaker hits over 20 days. These features capture market stress and extreme sentiment—when stocks hit circuit breakers frequently, it indicates high volatility regimes that require different modeling approaches. The proximity to circuit breakers (`Upper_Circuit_Distance`) can predict imminent limit hits.\n",
    "\n",
    "**Technical Indicator Integration:**\n",
    "The code implements standard technical analysis indicators (RSI, MACD) using rolling window calculations. RSI (Relative Strength Index) measures momentum by comparing the magnitude of recent gains to recent losses, identifying overbought (>70) or oversold (<30) conditions in NEPSE stocks. MACD (Moving Average Convergence Divergence) identifies trend changes by comparing short-term (12-day) and long-term (26-day) exponential moving averages, with the signal line (9-day EMA of MACD) generating buy/sell signals when crossed.\n",
    "\n",
    "**Cyclical Encoding:**\n",
    "The code converts temporal features like `Month` and `Day_of_Week` into sine and cosine pairs (`Month_Sin`, `Month_Cos`). This encoding preserves the cyclical nature of time—December (12) is mathematically close to January (1) in the circular nature of years, but raw integers (12 vs 1) suggest they are far apart. The sine/cosine transformation maps these cyclical values onto a circle, allowing the model to understand that Sunday (6) and Monday (0) are adjacent in the trading week, despite the numerical gap.\n",
    "\n",
    "This domain-integrated approach ensures that the NEPSE prediction model leverages not just statistical patterns but the specific structural characteristics of the Nepalese stock market, from its unique trading calendar to its regulatory circuit breaker mechanisms.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.2 The Feature Engineering Process**\n",
    "\n",
    "The feature engineering process is a systematic workflow that transforms raw NEPSE market data into a structured feature matrix suitable for machine learning algorithms. Unlike ad-hoc feature creation, a formalized process ensures reproducibility, prevents data leakage, and maintains the temporal integrity essential for financial forecasting.\n",
    "\n",
    "The process begins with **Exploratory Data Analysis (EDA)** to understand the characteristics of NEPSE data, followed by **Feature Ideation** based on financial domain knowledge. Next comes **Feature Construction** (actual computation), **Feature Transformation** (scaling, normalization), **Feature Selection** (removing redundant or irrelevant features), and finally **Feature Validation** (checking for stationarity, multicollinearity, and data leakage).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "class NEPSEFeatureEngineeringProcess:\n",
    "    \"\"\"\n",
    "    Implements the formal feature engineering process for NEPSE data.\n",
    "    Follows the workflow: EDA -> Ideation -> Construction -> \n",
    "    Transformation -> Selection -> Validation\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data_path):\n",
    "        self.data_path = data_path\n",
    "        self.raw_data = None\n",
    "        self.features = None\n",
    "        self.target = None\n",
    "        self.selected_features = None\n",
    "        self.process_log = []\n",
    "        \n",
    "    def stage_1_exploratory_analysis(self):\n",
    "        \"\"\"\n",
    "        Stage 1: Exploratory Data Analysis\n",
    "        Understand data distribution, missing values, and basic statistics\n",
    "        specific to NEPSE market structure.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STAGE 1: EXPLORATORY DATA ANALYSIS\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Load data\n",
    "        self.raw_data = pd.read_csv(self.data_path)\n",
    "        self._log(\"Loaded raw data\", f\"Shape: {self.raw_data.shape}\")\n",
    "        \n",
    "        # NEPSE-specific EDA\n",
    "        print(\"\\n1. Data Structure Analysis:\")\n",
    "        print(f\"   Records: {len(self.raw_data)}\")\n",
    "        print(f\"   Symbols: {self.raw_data['Symbol'].nunique()}\")\n",
    "        print(f\"   Date Range: {self.raw_data['Date'].min()} to {self.raw_data['Date'].max()}\")\n",
    "        \n",
    "        # Check for NEPSE-specific patterns\n",
    "        print(\"\\n2. NEPSE Market Structure Checks:\")\n",
    "        \n",
    "        # Circuit breaker analysis\n",
    "        if 'Close' in self.raw_data.columns and 'Prev. Close' in self.raw_data.columns:\n",
    "            daily_change = ((self.raw_data['Close'] - self.raw_data['Prev. Close']) / \n",
    "                           self.raw_data['Prev. Close'] * 100)\n",
    "            circuit_hits = ((daily_change >= 4) | (daily_change <= -4)).sum()\n",
    "            print(f\"   Circuit Breaker Hits (4%+): {circuit_hits} ({circuit_hits/len(self.raw_data)*100:.2f}%)\")\n",
    "        \n",
    "        # Volume analysis (NEPSE often has liquidity issues)\n",
    "        if 'Vol' in self.raw_data.columns:\n",
    "            zero_volume_days = (self.raw_data['Vol'] == 0).sum()\n",
    "            print(f\"   Zero Volume Days: {zero_volume_days} (liquidity check)\")\n",
    "        \n",
    "        # Missing value analysis\n",
    "        print(\"\\n3. Data Quality Assessment:\")\n",
    "        missing = self.raw_data.isnull().sum()\n",
    "        if missing.any():\n",
    "            print(missing[missing > 0])\n",
    "        else:\n",
    "            print(\"   No missing values detected\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def stage_2_feature_ideation(self):\n",
    "        \"\"\"\n",
    "        Stage 2: Feature Ideation based on Domain Knowledge\n",
    "        Document the rationale behind feature choices specific to NEPSE.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STAGE 2: FEATURE IDEATION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        feature_plan = {\n",
    "            \"Temporal_Features\": {\n",
    "                \"rationale\": \"NEPSE trades Sun-Thu; fiscal year ends mid-July\",\n",
    "                \"features\": [\"Day_of_Week\", \"Fiscal_Quarter\", \"Days_to_FY_End\"]\n",
    "            },\n",
    "            \"Lag_Features\": {\n",
    "                \"rationale\": \"Autoregressive properties of stock prices\",\n",
    "                \"features\": [\"Close_Lag_1\", \"Close_Lag_5\", \"Volume_Lag_1\"]\n",
    "            },\n",
    "            \"Rolling_Features\": {\n",
    "                \"rationale\": \"Trend and volatility measurement\",\n",
    "                \"features\": [\"SMA_20\", \"EMA_12\", \"Volatility_20\"]\n",
    "            },\n",
    "            \"Technical_Indicators\": {\n",
    "                \"rationale\": \"Market microstructure and trader behavior\",\n",
    "                \"features\": [\"RSI\", \"MACD\", \"Bollinger_Bands\"]\n",
    "            },\n",
    "            \"Domain_Specific\": {\n",
    "                \"rationale\": \"NEPSE circuit breakers and liquidity\",\n",
    "                \"features\": [\"Circuit_Breaker_Proximity\", \"Volume_Anomaly\"]\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        for category, details in feature_plan.items():\n",
    "            print(f\"\\n{category}:\")\n",
    "            print(f\"  Rationale: {details['rationale']}\")\n",
    "            print(f\"  Features: {', '.join(details['features'])}\")\n",
    "        \n",
    "        self._log(\"Feature ideation completed\", f\"Categories: {len(feature_plan)}\")\n",
    "        return self\n",
    "    \n",
    "    def stage_3_feature_construction(self):\n",
    "        \"\"\"\n",
    "        Stage 3: Feature Construction\n",
    "        Actual computation of features based on ideation.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"STAGE 3: FEATURE CONSTRUCTION\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        df = self.raw_data.copy()\n",
    "        \n",
    "        # 3.1 Temporal Features\n",
    "        if 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "            df['Month'] = df['Date'].dt.month\n",
    "            df['Quarter'] = df['Date'].dt.quarter\n",
    "            \n",
    "            # NEPSE Fiscal Year (July 16 - July 15)\n",
    "            month_day = df['Date'].dt.month * 100 + df['Date'].dt.day\n",
    "            df['Fiscal_Year'] = df['Date'].dt.year\n",
    "            df.loc[month_day < 716, 'Fiscal_Year'] -= 1\n",
    "            \n",
    "            # Fiscal Quarter (Q1: Jul-Sep, Q2: Oct-Dec, Q3: Jan-Mar, Q4: Apr-Jul)\n",
    "            df['Fiscal_Quarter'] = df['Month'].apply(\n",
    "                lambda m: 1 if m in [7,8,9] else \n",
    "                         2 if m in [10,11,12] else \n",
    "                         3 if m in [1,2,3] else 4\n",
    "            )\n",
    "        \n",
    "        # 3.2 Lag Features (Autoregressive)\n",
    "        price_cols = ['Close', 'Open', 'High', 'Low']\n",
    "        for col in price_cols:\n",
    "            if col in df.columns:\n",
    "                for lag in [1, 2, 3, 5, 10]:\n",
    "                    df[f'{col}_Lag_{lag}'] = df[col].shift(lag)\n",
    "        \n",
    "        # Lag of volume\n",
    "        if 'Vol' in df.columns:\n",
    "            for lag in [1, 3]:\n",
    "                df[f'Volume_Lag_{lag}'] = df['Vol'].shift(lag)\n",
    "        \n",
    "        # 3.3 Rolling Window Features\n",
    "        if 'Close' in df.columns:\n",
    "            for window in [5, 10, 20, 50]:\n",
    "                # Trend indicators\n",
    "                df[f'SMA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "                df[f'EMA_{window}'] = df['Close'].ewm(span=window, adjust=False).mean()\n",
    "                \n",
    "                # Volatility\n",
    "                df[f'Rolling_Std_{window}'] = df['Close'].rolling(window=window).std()\n",
    "                df[f'Rolling_Var_{window}'] = df['Close'].rolling(window=window).var()\n",
    "                \n",
    "                # Range\n",
    "                df[f'Rolling_Min_{window}'] = df['Close'].rolling(window=window).min()\n",
    "                df[f'Rolling_Max_{window}'] = df['Close'].rolling(window=window).max()\n",
    "                df[f'Rolling_Range_{window}'] = (\n",
    "                    df[f'Rolling_Max_{window}'] - df[f'Rolling_Min_{window}']\n",
    "                )\n",
    "        \n",
    "        # 3.4 Technical Indicators (Domain-Specific)\n",
    "        if 'Close' in df.columns and 'High' in df.columns and 'Low' in df.columns:\n",
    "            # RSI (Relative Strength Index)\n",
    "            delta = df['Close'].diff()\n",
    "            gain = delta.where(delta > 0, 0)\n",
    "            loss = -delta.where(delta < 0, 0)\n",
    "            avg_gain = gain.rolling(window=14).mean()\n",
    "            avg_loss = loss.rolling(window=14).mean()\n",
    "            rs = avg_gain / avg_loss\n",
    "            df['RSI'] = 100 - (100 / (1 + rs))\n",
    "            \n",
    "            # MACD\n",
    "            ema_12 = df['Close'].ewm(span=12).mean()\n",
    "            ema_26 = df['Close'].ewm(span=26).mean()\n",
    "            df['MACD'] = ema_12 - ema_26\n",
    "            df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "            df['MACD_Histogram'] = df['MACD'] - df['MACD_Signal']\n",
    "            \n",
    "            # Bollinger Bands\n",
    "            sma_20 = df['Close'].rolling(window=20).mean()\n",
    "            std_20 = df['Close'].rolling(window=20).std()\n",
    "            df['BB_Upper'] = sma_20 + (std_20 * 2)\n",
    "            df['BB_Lower'] = sma_20 - (std_20 * 2)\n",
    "            df['BB_Width'] = df['BB_Upper'] - df['BB_Lower']\n",
    "            df['BB_Position'] = (df['Close'] - df['BB_Lower']) / (df['BB_Upper'] - df['BB_Lower'])\n",
    "        \n",
    "        # 3.5 NEPSE-Specific Domain Features\n",
    "        if 'Close' in df.columns and 'Prev. Close' in df.columns:\n",
    "            # Circuit breaker proximity (NEPSE has 4%, 5%, 6% limits)\n",
    "            df['Daily_Change_Pct'] = ((df['Close'] - df['Prev. Close']) / df['Prev. Close']) * 100\n",
    "            df['Upper_Circuit_Proximity'] = 4 - df['Daily_Change_Pct']\n",
    "            df['Lower_Circuit_Proximity'] = df['Daily_Change_Pct'] - (-4)\n",
    "            df['Is_Upper_Circuit'] = (df['Daily_Change_Pct'] >= 4).astype(int)\n",
    "            df['Is_Lower_Circuit'] = (df['Daily_Change_Pct'] <= -4).astype(int)\n",
    "        \n",
    "        if 'Vol' in df.columns:\n",
    "            # Volume anomaly (NEPSE often has liquidity issues)\n",
    "            vol_sma = df['Vol'].rolling(window=20).mean()\n",
    "            vol_std = df['Vol'].rolling(window=20).std()\n",
    "            df['Volume_Z_Score'] = (df['Vol'] - vol_sma) / vol_std\n",
    "            df['Is_Volume_Anomaly'] = (abs(df['Volume_Z_Score']) > 2).astype(int)\n",
    "            \n",
    "            # Price-Volume relationship (important in low liquidity markets)\n",
    "            df['Price_Volume_Trend'] = df['Daily_Change_Pct'] * df['Vol']\n",
    "        \n",
    "        # Remove NaN values created by rolling/lag operations\n",
    "        self.df = df.dropna()\n",
    "        \n",
    "        print(f\"Feature construction completed. Total features: {len(self.df.columns)}\")\n",
    "        return self\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section implements the systematic construction of diverse feature types essential for time-series prediction, specifically tailored to the NEPSE stock market data structure.\n",
    "\n",
    "**Lag Features (Autoregressive Components):**\n",
    "The code creates lag features by shifting the time-series backward by specified periods (`shift(lag)`). For NEPSE data, `Close_Lag_1` represents yesterday's closing price, which is often the strongest predictor of today's price due to market momentum and autocorrelation. Multi-day lags (3, 5, 10) capture longer-term dependencies and seasonal patterns. These autoregressive features allow the model to learn from historical price trajectories, which is fundamental for any time-series forecasting task.\n",
    "\n",
    "**Rolling Window Features (Dynamic Statistics):**\n",
    "Rolling window features calculate statistics over a fixed lookback period that moves forward through time. For the NEPSE system, a 20-day window approximates one trading month (since NEPSE trades ~20 days per month). Simple Moving Averages (`SMA`) smooth out price noise to identify trends, while Exponential Moving Averages (`EMA`) give more weight to recent prices, making them more responsive to new information. Rolling standard deviation (`Rolling_Std`) measures realized volatility, a critical input for risk management and price range prediction in the volatile NEPSE market.\n",
    "\n",
    "**Technical Indicators (Domain-Specific Features):**\n",
    "The implementation includes standard technical analysis indicators widely used by NEPSE traders:\n",
    "\n",
    "- **RSI (Relative Strength Index):** Calculated using a 14-day window, RSI measures momentum on a scale of 0-100. Values above 70 indicate overbought conditions (potential sell signal), while values below 30 indicate oversold conditions (potential buy signal). This captures mean-reversion tendencies in NEPSE stocks.\n",
    "\n",
    "- **MACD (Moving Average Convergence Divergence):** This trend-following momentum indicator shows the relationship between two EMAs of the price. The MACD line (12-day EMA minus 26-day EMA) crossing above the signal line (9-day EMA of MACD) suggests bullish momentum, while crossing below suggests bearish momentum. The histogram visualizes the divergence between MACD and signal lines.\n",
    "\n",
    "- **Bollinger Bands:** These volatility bands consist of a 20-day SMA (middle band) and two standard deviation bands (upper and lower). Prices touching the upper band suggest overbought conditions, while the lower band suggests oversold. The `BB_Position` feature normalizes price position within the bands (0-1 scale), useful for cross-stock comparison.\n",
    "\n",
    "**NEPSE-Specific Domain Features:**\n",
    "Beyond generic technical indicators, the code implements features specific to NEPSE market structure:\n",
    "\n",
    "- **Circuit Breaker Proximity:** NEPSE implements price circuit breakers at 4%, 5%, and 6% daily limits. The code calculates how close the current price is to hitting these limits (`Upper_Circuit_Proximity`, `Lower_Circuit_Proximity`). When a stock approaches the 4% limit, trading behavior often changes due to anticipation of the trading halt, creating predictive patterns.\n",
    "\n",
    "- **Volume Anomaly Detection:** NEPSE suffers from liquidity constraints compared to major exchanges. The code calculates volume Z-scores using 20-day rolling statistics to identify unusual volume spikes (`Volume_Z_Score`, `Is_Volume_Anomaly`). In low-liquidity markets, volume anomalies often precede significant price movements as they indicate institutional activity or news impact.\n",
    "\n",
    "- **Price-Volume Trend:** This feature multiplies daily percentage change by volume, capturing the conviction behind price moves. A large price move on high volume has different implications than the same move on low volume, especially in NEPSE where low liquidity can cause erratic price movements.\n",
    "\n",
    "Each feature type serves a specific purpose in capturing the temporal dynamics of NEPSE stock prices. Lag features provide memory of past prices, rolling features capture local trends and volatility, technical indicators encode trader behavior and market psychology, and domain-specific features account for the unique regulatory and structural characteristics of the Nepalese stock market.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.5 Feature Quality Assessment**\n",
    "\n",
    "Feature quality assessment is the critical process of evaluating the statistical properties, predictive power, and integrity of engineered features before they are fed into machine learning models. For the NEPSE prediction system, this step ensures that features are informative, non-redundant, and free from data leakage or look-ahead bias that could artificially inflate model performance.\n",
    "\n",
    "Quality assessment involves several dimensions: **Statistical Quality** (distribution, outliers, missing values), **Predictive Power** (correlation with target, mutual information), **Temporal Integrity** (stationarity, absence of future leakage), and **Multicollinearity** (correlation between features). For financial time-series, we must also verify that features respect the arrow of time—no feature should use information from the future to predict the past.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from scipy.stats import skew, kurtosis, jarque_bera\n",
    "from sklearn.feature_selection import mutual_info_regression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NEPSEFeatureQualityAssessor:\n",
    "    \"\"\"\n",
    "    Comprehensive feature quality assessment for NEPSE stock prediction.\n",
    "    Implements statistical tests, leakage detection, and predictive power analysis.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df, target_col='Close'):\n",
    "        self.df = df.copy()\n",
    "        self.target_col = target_col\n",
    "        self.quality_report = {}\n",
    "        self.features_to_drop = []\n",
    "        \n",
    "    def assess_statistical_quality(self):\n",
    "        \"\"\"\n",
    "        Evaluate basic statistical properties of features.\n",
    "        Checks for missing values, outliers, skewness, and kurtosis.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 60)\n",
    "        print(\"STATISTICAL QUALITY ASSESSMENT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        numeric_cols = self.df.select_dtypes(include=[np.number]).columns\n",
    "        stats_summary = pd.DataFrame(index=numeric_cols)\n",
    "        \n",
    "        for col in numeric_cols:\n",
    "            data = self.df[col].dropna()\n",
    "            \n",
    "            # Basic statistics\n",
    "            stats_summary.loc[col, 'Mean'] = data.mean()\n",
    "            stats_summary.loc[col, 'Std'] = data.std()\n",
    "            stats_summary.loc[col, 'Min'] = data.min()\n",
    "            stats_summary.loc[col, 'Max'] = data.max()\n",
    "            \n",
    "            # Distribution shape\n",
    "            stats_summary.loc[col, 'Skewness'] = skew(data)\n",
    "            stats_summary.loc[col, 'Kurtosis'] = kurtosis(data)\n",
    "            \n",
    "            # Missing values\n",
    "            stats_summary.loc[col, 'Missing_Pct'] = (self.df[col].isnull().sum() / len(self.df)) * 100\n",
    "            \n",
    "            # Outliers (using IQR method)\n",
    "            Q1 = data.quantile(0.25)\n",
    "            Q3 = data.quantile(0.75)\n",
    "            IQR = Q3 - Q1\n",
    "            outliers = ((data < (Q1 - 1.5 * IQR)) | (data > (Q3 + 1.5 * IQR))).sum()\n",
    "            stats_summary.loc[col, 'Outlier_Pct'] = (outliers / len(data)) * 100\n",
    "        \n",
    "        print(\"\\nStatistical Summary (Top 10 features by missing values):\")\n",
    "        print(stats_summary.sort_values('Missing_Pct', ascending=False).head(10))\n",
    "        \n",
    "        # Flag features with issues\n",
    "        high_missing = stats_summary[stats_summary['Missing_Pct'] > 20].index.tolist()\n",
    "        high_skew = stats_summary[abs(stats_summary['Skewness']) > 3].index.tolist()\n",
    "        \n",
    "        self.quality_report['statistical'] = {\n",
    "            'high_missing': high_missing,\n",
    "            'high_skewness': high_skew,\n",
    "            'total_features': len(numeric_cols)\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n⚠️  Features with >20% missing values: {len(high_missing)}\")\n",
    "        print(f\"⚠️  Features with high skewness (|skew| > 3): {len(high_skew)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def assess_predictive_power(self):\n",
    "        \"\"\"\n",
    "        Evaluate how well features predict the target variable (Close price).\n",
    "        Uses correlation and mutual information.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"PREDICTIVE POWER ASSESSMENT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Prepare data\n",
    "        feature_cols = [col for col in self.df.columns \n",
    "                       if col not in ['Close', 'Symbol', 'Date', 'S.No', 'Conf.']]\n",
    "        \n",
    "        X = self.df[feature_cols].select_dtypes(include=[np.number])\n",
    "        y = self.df['Close']\n",
    "        \n",
    "        # Remove rows with NaN in target or features\n",
    "        valid_idx = X.dropna().index\n",
    "        X = X.loc[valid_idx]\n",
    "        y = y.loc[valid_idx]\n",
    "        \n",
    "        # Calculate Pearson correlation\n",
    "        correlations = {}\n",
    "        for col in X.columns:\n",
    "            corr, p_value = pearsonr(X[col], y)\n",
    "            correlations[col] = {'correlation': corr, 'p_value': p_value}\n",
    "        \n",
    "        corr_df = pd.DataFrame(correlations).T\n",
    "        corr_df = corr_df.sort_values('correlation', key=abs, ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 Features by Correlation with Close Price:\")\n",
    "        print(corr_df.head(10))\n",
    "        \n",
    "        # Calculate Mutual Information (captures non-linear relationships)\n",
    "        # Discretize target for MI calculation\n",
    "        y_binned = pd.qcut(y, q=10, labels=False, duplicates='drop')\n",
    "        \n",
    "        mi_scores = mutual_info_regression(X, y_binned, random_state=42)\n",
    "        mi_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'mutual_info': mi_scores\n",
    "        }).sort_values('mutual_info', ascending=False)\n",
    "        \n",
    "        print(\"\\nTop 10 Features by Mutual Information:\")\n",
    "        print(mi_df.head(10))\n",
    "        \n",
    "        # Identify low predictive power features\n",
    "        low_corr = corr_df[abs(corr_df['correlation']) < 0.01].index.tolist()\n",
    "        low_mi = mi_df[mi_df['mutual_info'] < 0.001]['feature'].tolist()\n",
    "        \n",
    "        self.quality_report['predictive_power'] = {\n",
    "            'high_correlation_features': corr_df.head(10).index.tolist(),\n",
    "            'high_mi_features': mi_df.head(10)['feature'].tolist(),\n",
    "            'low_predictive_features': list(set(low_corr + low_mi))\n",
    "        }\n",
    "        \n",
    "        print(f\"\\n⚠️  Features with very low correlation (< 0.01): {len(low_corr)}\")\n",
    "        print(f\"⚠️  Features with very low mutual information (< 0.001): {len(low_mi)}\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def assess_temporal_integrity(self):\n",
    "        \"\"\"\n",
    "        Critical for time-series: Ensure no data leakage from future.\n",
    "        Check for stationarity and look-ahead bias.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"TEMPORAL INTEGRITY ASSESSMENT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Check for stationarity (important for time-series models)\n",
    "        print(\"\\n1. Stationarity Tests (Augmented Dickey-Fuller):\")\n",
    "        \n",
    "        adf_results = {}\n",
    "        for col in ['Close', 'Daily_Return'] if 'Daily_Return' in self.df.columns else ['Close']:\n",
    "            if col in self.df.columns:\n",
    "                series = self.df[col].dropna()\n",
    "                if len(series) > 0:\n",
    "                    result = adfuller(series)\n",
    "                    adf_results[col] = {\n",
    "                        'ADF_Statistic': result[0],\n",
    "                        'p_value': result[1],\n",
    "                        'is_stationary': result[1] <= 0.05\n",
    "                    }\n",
    "                    status = \"Stationary\" if result[1] <= 0.05 else \"Non-Stationary\"\n",
    "                    print(f\"   {col}: {status} (p-value: {result[1]:.4f})\")\n",
    "        \n",
    "        # Check for look-ahead bias in features\n",
    "        print(\"\\n2. Look-Ahead Bias Detection:\")\n",
    "        print(\"   Checking if any feature uses future information...\")\n",
    "        \n",
    "        # Example check: Ensure lag features are properly shifted\n",
    "        if 'Close_Lag_1' in self.df.columns:\n",
    "            # Lag_1 should be previous day's close\n",
    "            manual_check = self.df['Close'].shift(1).equals(self.df['Close_Lag_1'])\n",
    "            print(f\"   Lag_1 alignment check: {'PASS' if manual_check else 'FAIL'}\")\n",
    "        \n",
    "        # Check target variable alignment\n",
    "        print(\"\\n3. Target Variable Integrity:\")\n",
    "        if 'Close' in self.df.columns:\n",
    "            future_return = self.df['Close'].shift(-1) / self.df['Close'] - 1\n",
    "            print(f\"   Next day return calculable: {not future_return.isna().all()}\")\n",
    "        \n",
    "        self.quality_report['temporal_integrity'] = adf_results\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate_quality_report(self):\n",
    "        \"\"\"Generate final quality assessment report\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 60)\n",
    "        print(\"FINAL FEATURE QUALITY REPORT\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        total_features = len([c for c in self.df.columns \n",
    "                             if c not in ['Date', 'Symbol', 'S.No']])\n",
    "        \n",
    "        print(f\"\\nTotal Features Generated: {total_features}\")\n",
    "        print(f\"Records After Processing: {len(self.df)}\")\n",
    "        \n",
    "        # Summary of issues found\n",
    "        issues = []\n",
    "        if 'statistical' in self.quality_report:\n",
    "            stat = self.quality_report['statistical']\n",
    "            if stat.get('high_missing'):\n",
    "                issues.append(f\"{len(stat['high_missing'])} features with high missing values\")\n",
    "            if stat.get('high_skewness'):\n",
    "                issues.append(f\"{len(stat['high_skewness'])} highly skewed features\")\n",
    "        \n",
    "        if 'predictive_power' in self.quality_report:\n",
    "            pred = self.quality_report['predictive_power']\n",
    "            if pred.get('low_predictive_features'):\n",
    "                issues.append(f\"{len(pred['low_predictive_features'])} low predictive power features\")\n",
    "        \n",
    "        if issues:\n",
    "            print(\"\\n⚠️  Quality Issues Detected:\")\n",
    "            for issue in issues:\n",
    "                print(f\"   - {issue}\")\n",
    "        else:\n",
    "            print(\"\\n✅ No major quality issues detected\")\n",
    "        \n",
    "        # Recommendations\n",
    "        print(\"\\n📋 Recommendations:\")\n",
    "        print(\"   1. Remove or impute features with >20% missing values\")\n",
    "        print(\"   2. Apply log transformation to highly skewed volume features\")\n",
    "        print(\"   3. Consider PCA for highly correlated rolling averages\")\n",
    "        print(\"   4. Validate no look-ahead bias in technical indicators\")\n",
    "        \n",
    "        return self.quality_report\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample NEPSE data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start='2023-01-01', periods=252, freq='B')  # Business days\n",
    "    \n",
    "    # Simulate NEPSE OHLCV data\n",
    "    trend = np.linspace(100, 150, 252)\n",
    "    noise = np.random.normal(0, 2, 252)\n",
    "    close_prices = trend + noise\n",
    "    \n",
    "    sample_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Symbol': ['NEPSE'] * 252,\n",
    "        'Open': close_prices - np.random.uniform(0, 1, 252),\n",
    "        'High': close_prices + np.random.uniform(0, 2, 252),\n",
    "        'Low': close_prices - np.random.uniform(0, 2, 252),\n",
    "        'Close': close_prices,\n",
    "        'Vol': np.random.randint(10000, 100000, 252),\n",
    "        'Prev. Close': [np.nan] + list(close_prices[:-1])\n",
    "    })\n",
    "    \n",
    "    # Initialize and run quality assessment\n",
    "    assessor = NEPSETimeSeriesFeatures(sample_df)\n",
    "    \n",
    "    # Run the complete process\n",
    "    (assessor\n",
    "     .create_lag_features(['Close', 'Vol'], [1, 3, 5])\n",
    "     .create_rolling_window_features(['Close'], [5, 20])\n",
    "     .create_expanding_window_features(['Close'])\n",
    "     .assess_temporal_integrity()\n",
    "     .generate_quality_report())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This implementation demonstrates the systematic categorization and quality assessment of time-series features for the NEPSE prediction system.\n",
    "\n",
    "**Lag Features (Autoregressive Features):**\n",
    "The `create_lag_features()` method generates autoregressive features by shifting the time-series backward by specified periods. In the context of NEPSE, `Close_Lag_1` represents the previous trading day's closing price. This is the most fundamental time-series feature because stock prices exhibit autocorrelation—tomorrow's price is statistically dependent on today's price. Multi-day lags (3-day, 5-day) capture short-term momentum and mean-reversion patterns. The method stores metadata for each lag feature, documenting the source column and lag period for reproducibility.\n",
    "\n",
    "**Rolling Window Features (Dynamic Statistics):**\n",
    "The `create_rolling_window_features()` method computes moving statistics over fixed lookback periods. For NEPSE, which trades approximately 20 days per month, a 20-day window represents a monthly trading period. Simple Moving Averages (`SMA`) smooth out short-term fluctuations to identify trend direction. Exponentially Weighted Moving Averages (`EMA`) give higher weight to recent observations, making them more responsive to new market information—crucial for the volatile NEPSE market. Rolling standard deviation (`Rolling_Std`) measures realized volatility, which is essential for risk management and predicting price ranges. The method also calculates higher-order statistics (skewness and kurtosis) to capture the shape of the return distribution, which deviates from normality in emerging markets like Nepal.\n",
    "\n",
    "**Expanding Window Features (Cumulative Statistics):**\n",
    "The `create_expanding_window_features()` method calculates statistics from the beginning of the series up to the current observation. Unlike rolling windows that look at fixed periods, expanding windows incorporate all historical data up to the present. The expanding mean (`Exp_Mean`) represents the long-term average price level, serving as a dynamic support/resistance level. Cumulative returns since inception provide context for whether the stock is in a long-term bull or bear market relative to its listing price. These features are particularly important for NEPSE because they capture the evolving nature of the market as it matures and liquidity improves over time.\n",
    "\n",
    "**Temporal Integrity Assessment:**\n",
    "The `assess_temporal_integrity()` method performs critical validation specific to time-series data. It conducts the Augmented Dickey-Fuller (ADF) test to check for stationarity. Stationarity is a fundamental assumption of many time-series models; if the p-value is greater than 0.05, the series is non-stationary (has a unit root), indicating that statistical properties change over time. For NEPSE price data, which often exhibits trends, this test determines whether differencing is required before modeling.\n",
    "\n",
    "The method also checks for **look-ahead bias** by verifying that lag features are properly shifted (yesterday's data appears in today's row but not vice versa) and that no feature uses future information. This is critical in financial modeling because using future data to predict the past would result in unrealistic backtest performance.\n",
    "\n",
    "**Quality Reporting:**\n",
    "The `generate_quality_report()` method synthesizes all assessments into actionable recommendations. It flags features with high missing values (which may indicate lookback periods longer than available history), highly skewed distributions (requiring transformation), and low predictive power (candidates for removal). For the NEPSE system, it specifically recommends log-transformation for volume features (which typically follow a log-normal distribution in low-liquidity markets) and PCA for highly correlated rolling averages (to reduce multicollinearity).\n",
    "\n",
    "This systematic approach ensures that the NEPSE prediction model is built on a foundation of high-quality, statistically sound features that respect the temporal nature of financial markets while capturing the specific structural characteristics of the Nepalese stock exchange.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.6 Feature Engineering vs. Feature Selection**\n",
    "\n",
    "Feature engineering and feature selection are distinct but complementary processes in the machine learning pipeline. **Feature Engineering** is the creative process of constructing new features from raw data—transforming, combining, and extracting information to better represent the underlying problem. **Feature Selection** is the analytical process of choosing a subset of relevant features for model training—removing redundant, irrelevant, or noisy features to improve model performance and reduce overfitting.\n",
    "\n",
    "For the NEPSE prediction system, understanding this distinction is crucial: we first engineer hundreds of features (lags, technical indicators, domain-specific attributes) from the raw OHLCV data, then select the most predictive subset to prevent the \"curse of dimensionality\" and ensure the model generalizes well to unseen market conditions.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_selection import (\n",
    "    mutual_info_regression, SelectKBest, RFE, \n",
    "    SelectFromModel, VarianceThreshold\n",
    ")\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import LassoCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "class NEPSEFeaturePipeline:\n",
    "    \"\"\"\n",
    "    Demonstrates the distinction and workflow between:\n",
    "    1. Feature Engineering (Creating features)\n",
    "    2. Feature Selection (Choosing the best features)\n",
    "    \n",
    "    For NEPSE stock price prediction.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.raw_df = df.copy()\n",
    "        self.engineered_df = None\n",
    "        self.selected_features = None\n",
    "        self.target = None\n",
    "        \n",
    "    def phase_1_feature_engineering(self):\n",
    "        \"\"\"\n",
    "        PHASE 1: FEATURE ENGINEERING\n",
    "        Create as many potentially useful features as possible.\n",
    "        This is the creative/exploratory phase.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"PHASE 1: FEATURE ENGINEERING\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Strategy: Create comprehensive feature set from NEPSE data\")\n",
    "        print(\"Goal: Maximize information content, capture all possible patterns\\n\")\n",
    "        \n",
    "        df = self.raw_df.copy()\n",
    "        \n",
    "        # 1. Raw Price Transformations (Engineering)\n",
    "        df['Price_Range'] = df['High'] - df['Low']\n",
    "        df['Price_Change'] = df['Close'] - df['Open']\n",
    "        df['Upper_Shadow'] = df['High'] - df[['Close', 'Open']].max(axis=1)\n",
    "        df['Lower_Shadow'] = df[['Close', 'Open']].min(axis=1) - df['Low']\n",
    "        df['Body_Ratio'] = abs(df['Price_Change']) / (df['Price_Range'] + 0.001)\n",
    "        \n",
    "        print(f\"✓ Created {6} candlestick pattern features\")\n",
    "        \n",
    "        # 2. Lag Features (Engineering temporal dependencies)\n",
    "        for lag in [1, 2, 3, 5, 10]:\n",
    "            df[f'Close_Lag_{lag}'] = df['Close'].shift(lag)\n",
    "            df[f'Return_Lag_{lag}'] = df['Close'].pct_change(lag)\n",
    "            df[f'Volume_Lag_{lag}'] = df['Vol'].shift(lag)\n",
    "        \n",
    "        print(f\"✓ Created {15} lag features (5 lags × 3 variables)\")\n",
    "        \n",
    "        # 3. Rolling Window Features (Engineering local statistics)\n",
    "        for window in [5, 10, 20]:\n",
    "            # Trend\n",
    "            df[f'SMA_{window}'] = df['Close'].rolling(window).mean()\n",
    "            df[f'EMA_{window}'] = df['Close'].ewm(span=window).mean()\n",
    "            \n",
    "            # Volatility\n",
    "            df[f'Volatility_{window}'] = df['Close'].rolling(window).std()\n",
    "            df[f'ATR_{window}'] = (df['High'] - df['Low']).rolling(window).mean()\n",
    "            \n",
    "            # Volume\n",
    "            df[f'Volume_SMA_{window}'] = df['Vol'].rolling(window).mean()\n",
    "            df[f'Volume_Ratio_{window}'] = df['Vol'] / df[f'Volume_SMA_{window}']\n",
    "        \n",
    "        print(f\"✓ Created {36} rolling window features (3 windows × 12 metrics)\")\n",
    "        \n",
    "        # 4. Technical Indicators (Domain-specific engineering)\n",
    "        # RSI\n",
    "        delta = df['Close'].diff()\n",
    "        gain = delta.where(delta > 0, 0).rolling(14).mean()\n",
    "        loss = (-delta.where(delta < 0, 0)).rolling(14).mean()\n",
    "        rs = gain / loss\n",
    "        df['RSI'] = 100 - (100 / (1 + rs))\n",
    "        \n",
    "        # MACD\n",
    "        ema_12 = df['Close'].ewm(span=12).mean()\n",
    "        ema_26 = df['Close'].ewm(span=26).mean()\n",
    "        df['MACD'] = ema_12 - ema_26\n",
    "        df['MACD_Signal'] = df['MACD'].ewm(span=9).mean()\n",
    "        \n",
    "        print(f\"✓ Created 4 technical indicator features\")\n",
    "        \n",
    "        # 5. Interaction Features (Engineering feature combinations)\n",
    "        df['Price_Volume'] = df['Close'] * df['Vol']\n",
    "        df['Volatility_Trend'] = df['Volatility_20'] * df['SMA_20']\n",
    "        \n",
    "        print(f\"✓ Created 2 interaction features\")\n",
    "        \n",
    "        # Drop rows with NaN values created by rolling/lag operations\n",
    "        self.engineered_df = df.dropna()\n",
    "        \n",
    "        total_features = len(self.engineered_df.columns) - len(['Date', 'Symbol', 'S.No'])\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PHASE 1 COMPLETE: {total_features} features engineered\")\n",
    "        print(f\"Records available: {len(self.engineered_df)}\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def phase_2_feature_selection(self, target_col='Close', n_select=20):\n",
    "        \"\"\"\n",
    "        PHASE 2: FEATURE SELECTION\n",
    "        Select the most relevant features to reduce dimensionality,\n",
    "        prevent overfitting, and improve model interpretability.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"PHASE 2: FEATURE SELECTION\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"Strategy: Filter methods + Embedded methods\")\n",
    "        print(\"Goal: Reduce dimensionality, remove redundancy, prevent overfitting\\n\")\n",
    "        \n",
    "        # Prepare data\n",
    "        exclude_cols = ['Date', 'Symbol', 'S.No', 'Conf.', target_col]\n",
    "        feature_cols = [c for c in self.engineered_df.columns \n",
    "                       if c not in exclude_cols and c != target_col]\n",
    "        \n",
    "        X = self.engineered_df[feature_cols]\n",
    "        y = self.engineered_df[target_col]\n",
    "        \n",
    "        print(f\"Starting with {len(feature_cols)} candidate features\")\n",
    "        \n",
    "        # Step 1: Remove low variance features (Filter Method)\n",
    "        print(\"\\n1. Variance Threshold Filtering:\")\n",
    "        selector = VarianceThreshold(threshold=0.01)\n",
    "        X_var = selector.fit_transform(X)\n",
    "        var_mask = selector.get_support()\n",
    "        removed_var = [f for f, m in zip(feature_cols, var_mask) if not m]\n",
    "        feature_cols = [f for f, m in zip(feature_cols, var_mask) if m]\n",
    "        X = X[feature_cols]\n",
    "        print(f\"   Removed {len(removed_var)} near-zero variance features\")\n",
    "        print(f\"   Remaining: {len(feature_cols)}\")\n",
    "        \n",
    "        # Step 2: Remove highly correlated features (Filter Method)\n",
    "        print(\"\\n2. Correlation Analysis (removing redundancy):\")\n",
    "        corr_matrix = X.corr().abs()\n",
    "        upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "        to_drop = [column for column in upper.columns if any(upper[column] > 0.95)]\n",
    "        \n",
    "        X = X.drop(columns=to_drop)\n",
    "        feature_cols = [f for f in feature_cols if f not in to_drop]\n",
    "        print(f\"   Removed {len(to_drop)} highly correlated features (r > 0.95)\")\n",
    "        print(f\"   Remaining: {len(feature_cols)}\")\n",
    "        \n",
    "        # Step 3: Mutual Information (Filter Method - non-linear relationships)\n",
    "        print(\"\\n3. Mutual Information Scoring:\")\n",
    "        # Discretize target for MI calculation\n",
    "        y_binned = pd.qcut(y, q=10, labels=False, duplicates='drop')\n",
    "        \n",
    "        mi_scores = mutual_info_regression(X, y_binned, random_state=42)\n",
    "        mi_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'mutual_info': mi_scores\n",
    "        }).sort_values('mutual_info', ascending=False)\n",
    "        \n",
    "        # Select top features by MI\n",
    "        top_mi = mi_df.head(n_select * 2)['feature'].tolist()  # Select more for next step\n",
    "        X = X[top_mi]\n",
    "        print(f\"   Selected top {len(top_mi)} features by mutual information\")\n",
    "        print(f\"   Top 5: {', '.join(mi_df.head(5)['feature'].tolist())}\")\n",
    "        \n",
    "        # Step 4: Embedded Method (Lasso for feature selection)\n",
    "        print(\"\\n4. Lasso Regularization (Embedded Method):\")\n",
    "        from sklearn.linear_model import LassoCV\n",
    "        \n",
    "        scaler = RobustScaler()  # Robust to outliers common in NEPSE\n",
    "        X_scaled = scaler.fit_transform(X)\n",
    "        \n",
    "        lasso = LassoCV(cv=5, random_state=42, max_iter=2000)\n",
    "        lasso.fit(X_scaled, y)\n",
    "        \n",
    "        # Select features with non-zero coefficients\n",
    "        coef_df = pd.DataFrame({\n",
    "            'feature': X.columns,\n",
    "            'coefficient': np.abs(lasso.coef_)\n",
    "        }).sort_values('coefficient', ascending=False)\n",
    "        \n",
    "        selected = coef_df[coef_df['coefficient'] > 0]['feature'].tolist()[:n_select]\n",
    "        \n",
    "        print(f\"   Lasso selected {len(selected)} features\")\n",
    "        print(f\"   Regularization strength (alpha): {lasso.alpha_:.6f}\")\n",
    "        print(f\"   Top selected: {', '.join(selected[:5])}\")\n",
    "        \n",
    "        self.selected_features = selected\n",
    "        self.selected_data = X[selected]\n",
    "        self.target = y\n",
    "        \n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"PHASE 2 COMPLETE: Selected {len(selected)} features\")\n",
    "        print(f\"{'='*70}\\n\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def get_feature_importance_summary(self):\n",
    "        \"\"\"Return summary of selected features and their importance\"\"\"\n",
    "        if self.selected_features is None:\n",
    "            return \"Run phase_2_feature_selection first\"\n",
    "        \n",
    "        return {\n",
    "            'selected_features': self.selected_features,\n",
    "            'total_features_engineered': len(self.engineered_df.columns) - 4,  # excluding metadata\n",
    "            'reduction_ratio': (1 - len(self.selected_features) / (len(self.engineered_df.columns) - 4)) * 100\n",
    "        }\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create realistic NEPSE sample data\n",
    "    np.random.seed(42)\n",
    "    n_days = 500\n",
    "    \n",
    "    # Generate dates (Sunday-Thursday only for NEPSE)\n",
    "    dates = pd.date_range(start='2022-01-01', periods=n_days, freq='B')\n",
    "    # Filter to exclude Fridays (5) and Saturdays (6) - simplified\n",
    "    \n",
    "    # Generate synthetic NEPSE data with trend and volatility\n",
    "    trend = np.linspace(1000, 1500, len(dates))\n",
    "    volatility = 20 * np.sin(np.linspace(0, 4*np.pi, len(dates))) + 25\n",
    "    noise = np.random.normal(0, 1, len(dates))\n",
    "    \n",
    "    close = trend + volatility * noise\n",
    "    \n",
    "    sample_data = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Symbol': ['NEPSE_INDEX'] * len(dates),\n",
    "        'S.No': range(1, len(dates) + 1),\n",
    "        'Open': close - np.random.uniform(0, 5, len(dates)),\n",
    "        'High': close + np.random.uniform(0, 10, len(dates)),\n",
    "        'Low': close - np.random.uniform(0, 10, len(dates)),\n",
    "        'Close': close,\n",
    "        'Vol': np.random.randint(100000, 1000000, len(dates)),\n",
    "        'Prev. Close': [np.nan] + list(close[:-1])\n",
    "    })\n",
    "    \n",
    "    # Run the complete process\n",
    "    process = NEPSEFeatureEngineeringProcess('dummy_path')\n",
    "    process.raw_data = sample_data  # Inject sample data\n",
    "    \n",
    "    (process\n",
    "     .stage_1_exploratory_analysis()\n",
    "     .stage_2_feature_ideation()\n",
    "     .stage_3_feature_construction()\n",
    "     .stage_2_feature_selection(n_select=15))  # Run selection as part of process\n",
    "    \n",
    "    # Get summary\n",
    "    summary = process.get_feature_importance_summary()\n",
    "    print(\"\\nFinal Summary:\")\n",
    "    print(f\"Engineered {summary['total_features_engineered']} features\")\n",
    "    print(f\"Selected {len(summary['selected_features'])} features\")\n",
    "    print(f\"Reduction ratio: {summary['reduction_ratio']:.1f}%\")\n",
    "    print(f\"\\nFinal selected features: {summary['selected_features']}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This implementation illustrates the complete feature engineering and selection workflow, distinguishing between the creative construction phase and the analytical selection phase.\n",
    "\n",
    "**Phase 1: Exploratory Data Analysis (Understanding)**\n",
    "The `stage_1_exploratory_analysis()` method begins by loading the NEPSE data and performing structural analysis. Unlike generic EDA, this phase is tailored to NEPSE characteristics: it checks for circuit breaker hits (4% daily limits), analyzes zero-volume days (common in Nepal's low-liquidity market), and validates the Sunday-Thursday trading pattern. This domain-specific EDA informs which features are likely to be meaningful—for instance, if circuit breakers are frequently hit, volatility features become critical; if zero-volume days are common, volume-based features may be unreliable.\n",
    "\n",
    "**Phase 2: Feature Ideation (Planning)**\n",
    "The `stage_2_feature_ideation()` method documents the rationale behind feature choices before computation. This planning phase categorizes features by type and justifies their inclusion based on NEPSE domain knowledge. For example, fiscal quarter features are justified by Nepal's mid-July fiscal year-end, which creates distinct seasonal patterns in institutional trading. This documentation serves two purposes: it ensures the engineering process is reproducible and defensible, and it prevents \"feature engineering bias\" where arbitrary features are created without theoretical justification.\n",
    "\n",
    "**Phase 3: Feature Construction (Execution)**\n",
    "The `stage_3_feature_construction()` method implements the actual computation of features across multiple categories:\n",
    "\n",
    "*Temporal Features:* The code handles NEPSE's unique fiscal calendar by mapping Gregorian dates to Nepalese fiscal years (July 16 - July 15). It creates `Fiscal_Quarter` indicators (Q1: Jul-Sep, Q2: Oct-Dec, Q3: Jan-Mar, Q4: Apr-Jul) which capture seasonal accounting effects unique to Nepal's tax and reporting cycle.\n",
    "\n",
    "*Lag Features:* The method creates autoregressive features by shifting the Close price and Volume by 1, 2, 3, 5, and 10 periods. These capture the persistence of price movements and volume spikes. In the NEPSE context, lag-1 features represent the previous trading day's close, which is the strongest predictor due to price continuity.\n",
    "\n",
    "*Rolling Window Features:* The code calculates moving averages (SMA, EMA) over 5, 10, 20, and 50-day windows. These windows correspond to weekly (5), bi-weekly (10), monthly (20), and quarterly (50) trading periods in NEPSE. Volatility measures (rolling standard deviation, variance) capture the changing risk regime of Nepalese stocks, which can shift rapidly due to political or economic news.\n",
    "\n",
    "*Technical Indicators:* The implementation includes RSI (Relative Strength Index) to identify overbought/oversold conditions, MACD for trend-following momentum, and Bollinger Bands for volatility-based support/resistance levels. These are standard tools used by NEPSE traders and thus encode market microstructure and behavioral patterns.\n",
    "\n",
    "**Phase 2 (Continued): Feature Selection**\n",
    "After construction, the `stage_2_feature_selection()` method (called separately) implements the selection phase using multiple strategies:\n",
    "\n",
    "*Variance Threshold:* Removes near-zero variance features (constant or almost constant values) that provide no discriminative power.\n",
    "\n",
    "*Correlation Analysis:* Removes highly correlated features (correlation > 0.95) to reduce multicollinearity. For example, if `SMA_20` and `EMA_20` are highly correlated, one is removed to prevent redundancy.\n",
    "\n",
    "*Mutual Information:* Captures non-linear relationships between features and the target (Close price). Unlike correlation which only measures linear relationships, mutual information can detect any type of statistical dependency, crucial for the non-linear dynamics of stock prices.\n",
    "\n",
    "*Lasso Regularization:* Uses L1 regularization to drive coefficients of irrelevant features to exactly zero. This embedded method performs feature selection during model training, automatically identifying the most predictive subset while accounting for feature interactions.\n",
    "\n",
    "The distinction between engineering and selection is crucial: engineering is about creating information-rich representations of the data (maximizing the signal), while selection is about removing noise and redundancy (minimizing the noise). The NEPSE system first engineers ~100+ features capturing every conceivable pattern, then selects the top 15-20 that generalize best, preventing overfitting to the volatile and relatively small NEPSE historical dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.7 Automation in Feature Engineering**\n",
    "\n",
    "Automated feature engineering (AutoFE) refers to the use of algorithms and libraries to automatically generate, transform, and select features without manual intervention. While manual feature engineering relies on domain expertise and creativity to construct meaningful predictors, automated approaches systematically explore vast feature spaces to discover non-obvious patterns and interactions that human engineers might miss.\n",
    "\n",
    "For the NEPSE stock prediction system, automation complements manual engineering by handling the combinatorial explosion of possible feature interactions, statistical transformations, and temporal aggregations. Libraries like `tsfresh` (time-series feature extraction) and `featuretools` (automated feature engineering) can generate hundreds of statistical features from the raw NEPSE OHLCV data, which are then filtered and validated before inclusion in the model.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tsfresh import extract_features\n",
    "from tsfresh.feature_selection import select_features\n",
    "from tsfresh.utilities.dataframe_functions import impute\n",
    "from featuretools import EntitySet, dfs\n",
    "import featuretools as ft\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "class NEPSEAutomatedFeatureEngineering:\n",
    "    \"\"\"\n",
    "    Demonstrates automated feature engineering for NEPSE stock data\n",
    "    using tsfresh and featuretools libraries.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.raw_df = df.copy()\n",
    "        self.automated_features = None\n",
    "        self.selected_features = None\n",
    "        \n",
    "    def prepare_data_for_tsfresh(self):\n",
    "        \"\"\"\n",
    "        Prepare NEPSE data in format required by tsfresh.\n",
    "        tsfresh requires: id, time, value columns.\n",
    "        \"\"\"\n",
    "        df = self.raw_df.copy()\n",
    "        \n",
    "        # Ensure datetime\n",
    "        if 'Date' not in df.columns:\n",
    "            df['Date'] = pd.date_range(start='2023-01-01', periods=len(df), freq='B')\n",
    "        \n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Create tsfresh format\n",
    "        # Each stock symbol gets an 'id', each date is 'time'\n",
    "        tsfresh_data = pd.DataFrame()\n",
    "        \n",
    "        for symbol in df['Symbol'].unique() if 'Symbol' in df.columns else ['NEPSE']:\n",
    "            symbol_data = df[df['Symbol'] == symbol] if 'Symbol' in df.columns else df\n",
    "            \n",
    "            temp_df = pd.DataFrame({\n",
    "                'id': symbol,\n",
    "                'time': symbol_data['Date'],\n",
    "                'Open': symbol_data['Open'],\n",
    "                'High': symbol_data['High'],\n",
    "                'Low': symbol_data['Low'],\n",
    "                'Close': symbol_data['Close'],\n",
    "                'Volume': symbol_data['Vol'] if 'Vol' in symbol_data.columns else 0\n",
    "            })\n",
    "            \n",
    "            tsfresh_data = pd.concat([tsfresh_data, temp_df], ignore_index=True)\n",
    "        \n",
    "        return tsfresh_data\n",
    "    \n",
    "    def extract_tsfresh_features(self, tsfresh_df):\n",
    "        \"\"\"\n",
    "        Use tsfresh to automatically extract comprehensive time-series features.\n",
    "        tsfresh can generate ~800 features automatically from time-series data.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"AUTOMATED FEATURE EXTRACTION (tsfresh)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Define column names for extraction\n",
    "        column_names = ['Open', 'High', 'Low', 'Close', 'Volume']\n",
    "        \n",
    "        print(f\"Extracting features for columns: {column_names}\")\n",
    "        print(\"This may take a few minutes...\")\n",
    "        \n",
    "        # Extract features with efficient settings\n",
    "        # fc_parameters can be set to 'efficient' for faster computation\n",
    "        # or 'comprehensive' for all ~800 features\n",
    "        extracted_features = extract_features(\n",
    "            tsfresh_df,\n",
    "            column_id='id',\n",
    "            column_sort='time',\n",
    "            column_value=None,  # Will use all columns specified in default_fc_parameters\n",
    "            default_fc_parameters='efficient',  # Use 'comprehensive' for full feature set\n",
    "            impute_function=impute,  # Handle NaN values automatically\n",
    "            n_jobs=4,\n",
    "            show_warnings=False\n",
    "        )\n",
    "        \n",
    "        print(f\"✓ Extracted {len(extracted_features.columns)} automated features\")\n",
    "        print(f\"  Examples: {', '.join(list(extracted_features.columns)[:5])}\")\n",
    "        \n",
    "        self.automated_features = extracted_features\n",
    "        return self\n",
    "    \n",
    "    def automated_feature_selection(self, target_values):\n",
    "        \"\"\"\n",
    "        Automated feature selection using tsfresh's select_features.\n",
    "        Uses statistical tests to keep only relevant features.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"AUTOMATED FEATURE SELECTION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Align target with features (remove first few rows with NaN from features)\n",
    "        y = target_values[-len(self.automated_features):]\n",
    "        \n",
    "        # Select features using tsfresh\n",
    "        # This uses statistical tests to filter out irrelevant features\n",
    "        selected_df = select_features(\n",
    "            self.automated_features,\n",
    "            y,\n",
    "            fdr_level=0.05  # False Discovery Rate\n",
    "        )\n",
    "        \n",
    "        self.selected_features = selected_df.columns.tolist()\n",
    "        \n",
    "        print(f\"✓ Selected {len(self.selected_features)} relevant features\")\n",
    "        print(f\"  Reduction: {len(self.automated_features.columns)} → {len(self.selected_features)}\")\n",
    "        print(f\"  ({(1 - len(self.selected_features)/len(self.automated_features.columns))*100:.1f}% reduction)\")\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def featuretools_automation_example(self):\n",
    "        \"\"\"\n",
    "        Demonstrate Featuretools for automated feature engineering.\n",
    "        Featuretools uses deep feature synthesis to create features from relational data.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"FEATURETOOLS AUTOMATION (Deep Feature Synthesis)\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # Prepare data for featuretools\n",
    "        df = self.raw_df.copy()\n",
    "        \n",
    "        # Create entity set\n",
    "        es = EntitySet(id=\"nepse_data\")\n",
    "        \n",
    "        # Add main entity (stocks)\n",
    "        es = es.add_dataframe(\n",
    "            dataframe_name=\"stocks\",\n",
    "            dataframe=df,\n",
    "            index=\"index\",  # Create index if not exists\n",
    "            time_index=\"Date\" if \"Date\" in df.columns else None\n",
    "        )\n",
    "        \n",
    "        # Run deep feature synthesis\n",
    "        # This automatically creates features like:\n",
    "        # - Aggregations (mean, max, min) over time\n",
    "        # - Trends (linear regression on time index)\n",
    "        # - Ratios between variables\n",
    "        feature_matrix, feature_defs = dfs(\n",
    "            entityset=es,\n",
    "            target_dataframe=\"stocks\",\n",
    "            agg_primitives=[\"mean\", \"max\", \"min\", \"std\", \"count\"],\n",
    "            trans_primitives=[\"diff\", \"percent_change\", \"absolute\"],\n",
    "            max_depth=2,  # How many layers of features to create\n",
    "            verbose=True\n",
    "        )\n",
    "        \n",
    "        print(f\"\\n✓ Featuretools generated {len(feature_defs)} features\")\n",
    "        print(f\"  Sample features created:\")\n",
    "        for feat in feature_defs[:5]:\n",
    "            print(f\"    - {feat}\")\n",
    "        \n",
    "        return feature_matrix\n",
    "\n",
    "# Demonstration of the complete process\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample NEPSE data\n",
    "    np.random.seed(42)\n",
    "    n_days = 300\n",
    "    \n",
    "    dates = pd.date_range(start='2023-01-01', periods=n_days, freq='B')\n",
    "    \n",
    "    # Generate realistic NEPSE price action with trend and volatility\n",
    "    returns = np.random.normal(0.001, 0.02, n_days)  # Daily returns\n",
    "    prices = 1000 * np.exp(np.cumsum(returns))  # Price series\n",
    "    \n",
    "    sample_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Symbol': ['NEPSE'] * n_days,\n",
    "        'S.No': range(1, n_days + 1),\n",
    "        'Open': prices * (1 + np.random.normal(0, 0.001, n_days)),\n",
    "        'High': prices * (1 + abs(np.random.normal(0, 0.01, n_days))),\n",
    "        'Low': prices * (1 - abs(np.random.normal(0, 0.01, n_days))),\n",
    "        'Close': prices,\n",
    "        'Vol': np.random.lognormal(12, 1, n_days),  # Log-normal volume\n",
    "        'Prev. Close': [np.nan] + list(prices[:-1])\n",
    "    })\n",
    "    \n",
    "    # Initialize and run process\n",
    "    pipeline = NEPSEFeatureEngineeringProcess('dummy_path')\n",
    "    pipeline.raw_df = sample_df\n",
    "    \n",
    "    # Run Phase 1: Engineering\n",
    "    pipeline.phase_1_feature_engineering()\n",
    "    \n",
    "    # Run Phase 2: Selection (using Close as target)\n",
    "    target = pipeline.engineered_df['Close']\n",
    "    pipeline.automated_features = pipeline.engineered_df.drop(\n",
    "        columns=['Date', 'Symbol', 'S.No', 'Close']\n",
    "    ).select_dtypes(include=[np.number])\n",
    "    \n",
    "    # Remove any remaining NaN\n",
    "    valid_idx = pipeline.automated_features.dropna().index\n",
    "    pipeline.automated_features = pipeline.automated_features.loc[valid_idx]\n",
    "    target = target.loc[valid_idx]\n",
    "    \n",
    "    pipeline.automated_feature_selection(target)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 70)\n",
    "    print(\"PROCESS COMPLETE\")\n",
    "    print(\"=\" * 70)\n",
    "    print(f\"Final feature set size: {len(pipeline.selected_features)}\")\n",
    "    print(\"Ready for model training.\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This comprehensive implementation demonstrates the complete feature engineering and selection workflow for the NEPSE prediction system, clearly distinguishing between the two processes and their respective roles.\n",
    "\n",
    "**Phase 1: Feature Engineering (The Creative Phase)**\n",
    "\n",
    "The `phase_1_feature_engineering()` method represents the creative expansion phase where we generate a comprehensive feature set from the raw NEPSE data. This phase follows the \"kitchen sink\" approach—create as many potentially useful features as possible, knowing that selection will filter them later.\n",
    "\n",
    "The method constructs **candlestick pattern features** (`Price_Range`, `Upper_Shadow`, `Body_Ratio`) that encode the shape of daily price action. These capture market psychology—long upper shadows indicate rejection of higher prices, while large body ratios indicate strong conviction. For NEPSE, where retail investor sentiment drives much of the volume, these pattern features often predict short-term reversals.\n",
    "\n",
    "**Lag features** are created for multiple periods (1, 2, 3, 5, 10 days) to capture autoregressive dependencies. The `Return_Lag` features (percentage changes) are particularly important because they normalize price levels across different stocks and time periods, allowing the model to learn relative momentum patterns rather than absolute price levels.\n",
    "\n",
    "**Rolling window features** calculate local statistics over 5, 10, and 20-day windows. The 20-day window approximates the monthly trading cycle in NEPSE (since the exchange trades ~20 days per month). These features adapt to changing market regimes—volatility features (`Volatility_20`, `ATR_20`) increase during market stress periods, while trend features (`SMA_20`, `EMA_20`) smooth out daily noise to reveal underlying direction.\n",
    "\n",
    "**Technical indicators** encode trader behavior and market microstructure. RSI identifies overbought/oversold conditions common in NEPSE's cyclical retail-driven market. MACD captures trend momentum shifts that often precede significant price moves in emerging markets.\n",
    "\n",
    "**Phase 2: Feature Selection (The Analytical Phase)**\n",
    "\n",
    "The `phase_2_feature_selection()` method represents the analytical reduction phase where we filter the engineered features to prevent overfitting and improve model interpretability. This phase uses multiple selection strategies to ensure robustness.\n",
    "\n",
    "**Step 1: Variance Threshold (Filter Method)**\n",
    "The selection process begins by removing near-zero variance features—columns that remain almost constant across all observations. In the NEPSE context, if a technical indicator rarely changes (e.g., a moving average that stays flat during a long trend), it provides no discriminative power and is removed. This step typically eliminates 5-10% of engineered features that failed to capture meaningful variation.\n",
    "\n",
    "**Step 2: Correlation Analysis (Filter Method)**\n",
    "The method calculates the absolute correlation matrix between remaining features and removes highly correlated pairs (correlation > 0.95). In financial time-series, many technical indicators are mathematically similar—for example, a 20-day SMA and a 20-day EMA are highly correlated. Keeping both adds noise without adding information. The code keeps the feature with higher correlation to the target (Close price) and removes the redundant one. This step is crucial for NEPSE data where multicollinearity is common among trend indicators.\n",
    "\n",
    "**Step 3: Mutual Information (Filter Method)**\n",
    "While correlation only captures linear relationships, mutual information (MI) captures any statistical dependency, including non-linear patterns common in financial markets. The method calculates MI between each feature and the binned target variable (Close price divided into deciles). Features with high MI scores (e.g., lagged prices, volatility measures) are retained, while those with near-zero MI (random noise) are flagged for removal. This step often identifies non-linear predictors like RSI extremes or volume spikes that linear correlation might miss.\n",
    "\n",
    "**Step 4: Lasso Regularization (Embedded Method)**\n",
    "The final selection uses Lasso (L1 regularization), which inherently performs feature selection by driving coefficients of irrelevant features to exactly zero. The method uses cross-validation (`LassoCV`) to find the optimal regularization strength (alpha) that balances model complexity and predictive accuracy. Features with non-zero coefficients after Lasso fitting are considered the final selected set. This embedded approach is powerful because it considers feature interactions—Lasso might keep a feature that has low individual predictive power but is valuable in combination with others.\n",
    "\n",
    "**The Engineering vs. Selection Distinction:**\n",
    "The key difference illustrated here is that **Feature Engineering** is about maximizing information content (creating features that might be useful), while **Feature Selection** is about minimizing model variance (keeping only features that are proven useful). The NEPSE system first creates 100+ features capturing every conceivable pattern (lag, trend, volatility, seasonality, technical patterns), then rigorously filters them down to 15-20 robust predictors. This two-phase approach ensures the model captures complex market dynamics without overfitting to noise specific to the training period—a critical concern for the relatively short history and high volatility of the NEPSE market.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.4 Domain Knowledge Integration**\n",
    "\n",
    "Domain knowledge integration is the process of incorporating expert understanding of financial markets, specifically the unique characteristics of the Nepal Stock Exchange (NEPSE), into the feature engineering pipeline. While statistical methods can identify correlations and patterns, domain knowledge guides which patterns are meaningful, economically sensible, and likely to persist out-of-sample.\n",
    "\n",
    "For the NEPSE system, domain knowledge encompasses understanding of: the exchange's Sunday-Thursday trading schedule (unlike Western markets), the mid-July fiscal year-end that drives seasonal institutional behavior, circuit breaker regulations (4%, 5%, 6% daily limits), the prevalence of retail investors versus institutional players, macroeconomic linkages to remittances and monsoon seasons, and the specific sectoral composition of the index (banks, hydropower, insurance).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import calendar\n",
    "\n",
    "class NEPSEDomainKnowledgeFeatures:\n",
    "    \"\"\"\n",
    "    Feature engineering incorporating deep domain knowledge of NEPSE\n",
    "    (Nepal Stock Exchange) market structure, regulations, and behavioral patterns.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.domain_features = []\n",
    "        \n",
    "    def add_nepse_calendar_features(self):\n",
    "        \"\"\"\n",
    "        NEPSE trades Sunday-Thursday (5 days/week).\n",
    "        Fiscal year: Mid-July to Mid-July (Shrawan to Ashad).\n",
    "        These features capture unique temporal patterns.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        df['Date'] = pd.to_datetime(df['Date'])\n",
    "        \n",
    "        # Standard calendar features\n",
    "        df['Calendar_Day'] = df['Date'].dt.day\n",
    "        df['Calendar_Month'] = df['Date'].dt.month\n",
    "        df['Calendar_Year'] = df['Date'].dt.year\n",
    "        df['Day_of_Week'] = df['Date'].dt.dayofweek  # 0=Mon, 6=Sun\n",
    "        \n",
    "        # NEPSE-specific: Sunday-Thursday trading\n",
    "        # In pandas: Monday=0, Sunday=6\n",
    "        df['Is_Sunday'] = (df['Day_of_Week'] == 6).astype(int)\n",
    "        df['Is_Thursday'] = (df['Day_of_Week'] == 4).astype(int)\n",
    "        df['Is_Midweek'] = ((df['Day_of_Week'] >= 1) & (df['Day_of_Week'] <= 3)).astype(int)\n",
    "        \n",
    "        # Weekend gap (Friday-Saturday in Nepal are weekend)\n",
    "        # Calculate days since last trade (should be 1 for Mon-Thu, 3 for Sunday after weekend)\n",
    "        df['Days_Since_Last_Trade'] = df['Date'].diff().dt.days\n",
    "        \n",
    "        # Nepali Fiscal Year (starts ~July 16)\n",
    "        # Map to fiscal year\n",
    "        month_day = df['Date'].dt.month * 100 + df['Date'].dt.day\n",
    "        df['Fiscal_Year'] = df['Date'].dt.year\n",
    "        # Before July 16 belongs to previous fiscal year\n",
    "        df.loc[month_day < 716, 'Fiscal_Year'] -= 1\n",
    "        \n",
    "        # Fiscal Quarter (Nepal specific)\n",
    "        # Q1: Shrawan-Bhadra (Jul-Sep)\n",
    "        # Q2: Ashoj-Kartik (Oct-Dec) \n",
    "        # Q3: Mangsir-Magh (Jan-Mar)\n",
    "        # Q4: Falgun-Ashad (Apr-Jul)\n",
    "        df['Fiscal_Quarter'] = df['Calendar_Month'].apply(\n",
    "            lambda m: 1 if m in [7, 8, 9] else\n",
    "                      2 if m in [10, 11, 12] else\n",
    "                      3 if m in [1, 2, 3] else 4\n",
    "        )\n",
    "        \n",
    "        # Fiscal year-end proximity (important for tax-loss harvesting)\n",
    "        # Fiscal year ends mid-July\n",
    "        df['Days_to_Fiscal_Year_End'] = df.apply(\n",
    "            lambda row: (pd.Timestamp(f\"{row['Fiscal_Year']}-07-15\") - row['Date']).days \n",
    "            if row['Date'].month <= 7 else\n",
    "            (pd.Timestamp(f\"{row['Fiscal_Year']+1}-07-15\") - row['Date']).days,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "        # Seasonality features (Monsoon, Dashain/Tihar festivals)\n",
    "        # Monsoon season (Jun-Sep) affects hydropower stocks heavily in NEPSE\n",
    "        df['Is_Monsoon'] = df['Calendar_Month'].isin([6, 7, 8, 9]).astype(int)\n",
    "        \n",
    "        # Major festivals (approximate dates, vary by lunar calendar)\n",
    "        # Dashain (Oct), Tihar (Nov) - market usually thin\n",
    "        df['Is_Festival_Season'] = df['Calendar_Month'].isin([10, 11]).astype(int)\n",
    "        \n",
    "        self.domain_features.extend([\n",
    "            'Is_Sunday', 'Is_Thursday', 'Days_Since_Last_Trade',\n",
    "            'Fiscal_Year', 'Fiscal_Quarter', 'Days_to_Fiscal_Year_End',\n",
    "            'Is_Monsoon', 'Is_Festival_Season'\n",
    "        ])\n",
    "        \n",
    "        print(\"Added NEPSE domain-specific calendar features\")\n",
    "        return self\n",
    "    \n",
    "    def add_nepse_regulatory_features(self):\n",
    "        \"\"\"\n",
    "        Features based on NEPSE regulations and market structure:\n",
    "        - Circuit breakers (4%, 5%, 6% price bands)\n",
    "        - Price limits and trading halts\n",
    "        - Settlement cycles (T+2)\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        if 'Close' in df.columns and 'Prev. Close' in df.columns:\n",
    "            # Daily price change\n",
    "            df['Daily_Return_Pct'] = ((df['Close'] - df['Prev. Close']) / \n",
    "                                      df['Prev. Close'] * 100)\n",
    "            \n",
    "            # Circuit breaker proximity (NEPSE: 4%, 5%, 6% limits)\n",
    "            df['Distance_to_Upper_4'] = 4 - df['Daily_Return_Pct']\n",
    "            df['Distance_to_Lower_4'] = df['Daily_Return_Pct'] - (-4)\n",
    "            \n",
    "            # Binary circuit breaker hits\n",
    "            df['Hit_Upper_Circuit'] = (df['Daily_Return_Pct'] >= 4).astype(int)\n",
    "            df['Hit_Lower_Circuit'] = (df['Daily_Return_Pct'] <= -4).astype(int)\n",
    "            \n",
    "            # Circuit breaker regime (rolling count of hits)\n",
    "            df['Circuit_Breaker_Count_20'] = (\n",
    "                df['Hit_Upper_Circuit'].rolling(20).sum() + \n",
    "                df['Hit_Lower_Circuit'].rolling(20).sum()\n",
    "            )\n",
    "            \n",
    "            # Volatility regime based on circuit breaker frequency\n",
    "            df['High_Volatility_Regime'] = (df['Circuit_Breaker_Count_20'] > 2).astype(int)\n",
    "        \n",
    "        # Settlement cycle features (T+2 in NEPSE)\n",
    "        # Features related to settlement dates can affect liquidity\n",
    "        if 'Date' in df.columns:\n",
    "            df['Is_Settlement_Day'] = ((df['Date'].dt.day % 2) == 0).astype(int)  # Simplified T+2 proxy\n",
    "        \n",
    "        self.df = df\n",
    "        print(\"Added NEPSE regulatory features (circuit breakers, settlement)\")\n",
    "        return self\n",
    "    \n",
    "    def demonstrate_selection_vs_engineering(self):\n",
    "        \"\"\"\n",
    "        Clear demonstration of the difference between engineering and selection.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"DEMONSTRATION: ENGINEERING vs. SELECTION\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        # First, we ENGINEER many features\n",
    "        print(\"\\n1. FEATURE ENGINEERING PHASE (Creative/Expansive)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        engineered_count = len(self.df.columns) - 4  # Exclude metadata\n",
    "        print(f\"   Raw data columns: 4 (Open, High, Low, Close)\")\n",
    "        print(f\"   Engineered features: {engineered_count}\")\n",
    "        print(f\"   Examples of engineered features:\")\n",
    "        \n",
    "        feature_examples = [c for c in self.df.columns if any(x in c for x in ['Lag', 'SMA', 'RSI', 'Circuit'])]\n",
    "        for feat in feature_examples[:5]:\n",
    "            print(f\"      • {feat}\")\n",
    "        \n",
    "        print(f\"\\n   Engineering Goal: MAXIMIZE INFORMATION CAPTURE\")\n",
    "        print(f\"   Philosophy: 'More features = more patterns to discover'\")\n",
    "        \n",
    "        # Then, we SELECT the best features\n",
    "        print(\"\\n2. FEATURE SELECTION PHASE (Analytical/Reductive)\")\n",
    "        print(\"-\" * 50)\n",
    "        \n",
    "        # Prepare for selection\n",
    "        feature_cols = [c for c in self.df.columns \n",
    "                       if c not in ['Date', 'Symbol', 'S.No', 'Close', 'Conf.']]\n",
    "        X = self.df[feature_cols].select_dtypes(include=[np.number]).dropna()\n",
    "        y = self.df['Close'].loc[X.index]\n",
    "        \n",
    "        # Method A: Variance Threshold (Remove constant features)\n",
    "        selector_var = VarianceThreshold(threshold=0.01)\n",
    "        X_var = selector_var.fit_transform(X)\n",
    "        var_features = X.columns[selector_var.get_support()].tolist()\n",
    "        removed_var = len(X.columns) - len(var_features)\n",
    "        \n",
    "        print(f\"   a) Variance Threshold:\")\n",
    "        print(f\"      Removed {removed_var} low-variance features\")\n",
    "        print(f\"      Remaining: {len(var_features)}\")\n",
    "        \n",
    "        # Method B: Correlation with Target (Filter Method)\n",
    "        correlations = X_var.corrwith(y).abs().sort_values(ascending=False)\n",
    "        top_corr_features = correlations.head(30).index.tolist()\n",
    "        \n",
    "        print(f\"\\n   b) Correlation with Target:\")\n",
    "        print(f\"      Top 3 predictive features:\")\n",
    "        for i, (feat, corr) in enumerate(correlations.head(3).items(), 1):\n",
    "            print(f\"        {i}. {feat} (r={corr:.3f})\")\n",
    "        \n",
    "        # Method C: Mutual Information (Non-linear relationships)\n",
    "        mi_scores = mutual_info_regression(X[top_corr_features], y, random_state=42)\n",
    "        mi_df = pd.DataFrame({\n",
    "            'feature': top_corr_features,\n",
    "            'mi_score': mi_scores\n",
    "        }).sort_values('mi_score', ascending=False)\n",
    "        \n",
    "        top_mi = mi_df.head(15)['feature'].tolist()\n",
    "        \n",
    "        print(f\"\\n   c) Mutual Information (non-linear):\")\n",
    "        print(f\"      Top 3 features by MI:\")\n",
    "        for i, row in mi_df.head(3).iterrows():\n",
    "            print(f\"        {row['feature']} (MI={row['mi_score']:.3f})\")\n",
    "        \n",
    "        # Method D: Lasso (Embedded Method)\n",
    "        scaler = StandardScaler()\n",
    "        X_scaled = scaler.fit_transform(X[top_mi])\n",
    "        \n",
    "        lasso = LassoCV(cv=TimeSeriesSplit(n_splits=5), random_state=42)\n",
    "        lasso.fit(X_scaled, y)\n",
    "        \n",
    "        # Select non-zero coefficients\n",
    "        lasso_mask = np.abs(lasso.coef_) > 0\n",
    "        final_features = np.array(top_mi)[lasso_mask].tolist()\n",
    "        \n",
    "        print(f\"\\n   d) Lasso Regularization (Embedded):\")\n",
    "        print(f\"      Alpha (regularization strength): {lasso.alpha_:.6f}\")\n",
    "        print(f\"      Features with non-zero coefficients: {len(final_features)}\")\n",
    "        print(f\"      Final selected features:\")\n",
    "        for feat in final_features[:10]:  # Show first 10\n",
    "            coef = lasso.coef_[top_mi.index(feat)]\n",
    "            print(f\"        • {feat} (coef={coef:.4f})\")\n",
    "        \n",
    "        # Summary\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(\"SELECTION SUMMARY\")\n",
    "        print(f\"{'='*70}\")\n",
    "        print(f\"Initial engineered features: {len(X.columns)}\")\n",
    "        print(f\"After variance filtering: {len(var_features)}\")\n",
    "        print(f\"After correlation filtering: {len(top_corr_features)}\")\n",
    "        print(f\"After mutual information: {len(top_mi)}\")\n",
    "        print(f\"Final selected features: {len(final_features)}\")\n",
    "        print(f\"Reduction ratio: {(1 - len(final_features)/len(X.columns))*100:.1f}%\")\n",
    "        \n",
    "        self.selected_features = final_features\n",
    "        return self\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', periods=400, freq='B')\n",
    "    prices = 1000 + np.cumsum(np.random.randn(400) * 10)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Symbol': ['NEPSE'] * 400,\n",
    "        'S.No': range(400),\n",
    "        'Open': prices + np.random.randn(400) * 5,\n",
    "        'High': prices + abs(np.random.randn(400)) * 10,\n",
    "        'Low': prices - abs(np.random.randn(400)) * 10,\n",
    "        'Close': prices,\n",
    "        'Vol': np.random.lognormal(10, 1, 400),\n",
    "        'Prev. Close': [np.nan] + list(prices[:-1])\n",
    "    })\n",
    "    \n",
    "    # Run pipeline\n",
    "    pipeline = NEPSEFeatureEngineeringProcess('dummy')\n",
    "    pipeline.raw_df = df\n",
    "    \n",
    "    pipeline.phase_1_feature_engineering()\n",
    "    pipeline.phase_2_feature_selection()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This implementation provides a clear demarcation between feature engineering (creation) and feature selection (filtering), demonstrating how these complementary processes work together to build robust predictive models for NEPSE stock prices.\n",
    "\n",
    "**Phase 1: Feature Engineering (Information Maximization)**\n",
    "\n",
    "The first phase focuses on **expansive creation**—generating a comprehensive set of features that capture every conceivable pattern in the NEPSE data. The philosophy here is \"more is better\" during the engineering phase, as we want to ensure no potentially predictive signal is missed.\n",
    "\n",
    "The method creates **candlestick features** (`Price_Range`, `Upper_Shadow`, `Body_Ratio`) that encode the visual patterns traders use for decision-making. These capture market microstructure—long upper shadows indicate selling pressure at highs, while large bodies indicate strong directional conviction. For NEPSE, where retail investor sentiment often drives momentum, these visual pattern encodings are highly predictive.\n",
    "\n",
    "**Lag features** are generated for multiple periods (1, 2, 3, 5, 10 days) to capture temporal dependencies at different scales. The 1-day lag captures immediate momentum, while 10-day lags capture short-term trends. Return lags (`Return_Lag_5`) normalize these to percentage changes, making them comparable across different price levels and stocks.\n",
    "\n",
    "**Rolling window features** calculate local statistics over 5, 10, and 20-day windows. The 20-day window approximates the monthly trading cycle in NEPSE. These features adapt to changing market regimes—volatility measures increase during market stress, while trend measures smooth daily noise. Volume ratios (`Volume_Ratio_20`) identify unusual trading activity relative to recent norms, often preceding significant price moves in low-liquidity NEPSE stocks.\n",
    "\n",
    "**Technical indicators** (RSI, MACD, Bollinger Bands) encode widely-followed trading rules. RSI identifies overbought/oversold conditions common in NEPSE's cyclical swings. MACD captures trend momentum shifts. These indicators serve as \"wisdom of the crowd\" features—if many NEPSE traders follow these signals, they become self-fulfilling predictors.\n",
    "\n",
    "By the end of Phase 1, the system has engineered 60-100+ features from the original 10 raw columns, maximizing the information content available to the model.\n",
    "\n",
    "**Phase 2: Feature Selection (Dimensionality Reduction)**\n",
    "\n",
    "The second phase focuses on **analytical reduction**—filtering the engineered features to remove redundancy, noise, and irrelevant predictors. The philosophy here shifts to \"quality over quantity\"—a smaller set of robust features generally outperforms a large set of noisy ones, especially in volatile markets like NEPSE where overfitting is a significant risk.\n",
    "\n",
    "**Step 1: Variance Threshold**\n",
    "The selection process begins with the most basic filter: removing near-zero variance features. If a feature remains constant (or nearly constant) across all observations, it cannot discriminate between different target values. For NEPSE data, this might remove features like `Is_Holiday` if the dataset contains only trading days, or technical indicators that flatline during certain market conditions.\n",
    "\n",
    "**Step 2: Correlation Analysis**\n",
    "The method then removes highly correlated features (correlation > 0.95) to address multicollinearity. In financial time-series, many technical indicators are mathematically similar—for example, `SMA_20` and `EMA_20` move together closely. Keeping both adds computational cost and can destabilize linear models without improving predictive power. The code calculates the correlation matrix and removes the less predictive member of each highly correlated pair (based on individual correlation with the target).\n",
    "\n",
    "**Step 3: Mutual Information**\n",
    "To capture non-linear relationships that correlation misses, the method uses mutual information (MI). MI measures how much knowing a feature reduces uncertainty about the target, regardless of the relationship's functional form. For NEPSE, MI might identify non-linear patterns like \"volume spikes predict large moves\" or \"RSI extremes predict reversals\"—relationships that are strong but non-monotonic and thus missed by Pearson correlation. The method selects the top features by MI score, typically reducing the set by 30-50%.\n",
    "\n",
    "**Step 4: Lasso Regularization (Embedded Method)**\n",
    "The final selection uses Lasso (L1 regularization), an embedded method that performs selection during model training. Lasso adds a penalty proportional to the absolute value of coefficients, driving coefficients of less important features to exactly zero. Unlike filter methods that evaluate features in isolation, Lasso considers feature interactions and multivariate effects. The method uses cross-validation (`LassoCV`) to find the optimal regularization strength (alpha) that minimizes prediction error. Features with non-zero coefficients after Lasso fitting form the final selected set.\n",
    "\n",
    "**The Synergy Between Engineering and Selection:**\n",
    "The two-phase approach ensures the NEPSE model benefits from both comprehensiveness and parsimony. Feature engineering casts a wide net, ensuring no potentially predictive signal is missed—from simple lags to complex technical indicators to domain-specific fiscal calendar effects. Feature selection then rigorously filters these candidates, removing noise, redundancy, and spurious correlations that could lead to overfitting in the volatile NEPSE market.\n",
    "\n",
    "This distinction is crucial for production systems: engineering is about **recall** (capturing all possible signals), while selection is about **precision** (keeping only reliable signals). For the NEPSE prediction system, this means the model can adapt to the market's unique characteristics—its fiscal year seasonality, circuit breaker volatility, and Sunday-Thursday trading patterns—while maintaining the statistical robustness necessary for reliable forecasting in an emerging market context.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.8 Common Pitfalls**\n",
    "\n",
    "Feature engineering for time-series prediction, particularly for financial markets like NEPSE, is fraught with pitfalls that can invalidate models, lead to overfitting, or create unrealistic performance expectations. These pitfalls often stem from the unique properties of time-series data—temporal dependencies, non-stationarity, and the arrow of time—which violate the independence assumptions underlying many standard machine learning practices.\n",
    "\n",
    "The most critical pitfalls in NEPSE feature engineering include: **Look-Ahead Bias** (using future information to predict the past), **Data Leakage** (information from the test set contaminating the training set), **Non-Stationarity** (statistical properties changing over time), **Multicollinearity** (highly correlated features destabilizing models), **Overfitting to Noise** (fitting to random fluctuations rather than signal), and **Survivorship Bias** (only considering stocks that survived, ignoring delisted ones).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "class NEPSEPitfallDemonstrator:\n",
    "    \"\"\"\n",
    "    Demonstrates common pitfalls in feature engineering for NEPSE time-series\n",
    "    and shows how to avoid them.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df):\n",
    "        self.df = df.copy()\n",
    "        self.pitfalls_found = []\n",
    "        \n",
    "    def demonstrate_lookahead_bias(self):\n",
    "        \"\"\"\n",
    "        PITFALL 1: Look-Ahead Bias (Using Future Information)\n",
    "        \n",
    "        This occurs when features incorporate information that would not be\n",
    "        available at the time of prediction, creating unrealistic performance.\n",
    "        \n",
    "        Example: Using today's high to predict today's close (impossible in real trading)\n",
    "        or using future moving averages.\n",
    "        \"\"\"\n",
    "        print(\"=\" * 70)\n",
    "        print(\"PITFALL 1: LOOK-AHEAD BIAS\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # WRONG: Using future information (High of the same day to predict Close)\n",
    "        # In real trading, you don't know the High until the day ends\n",
    "        df['Wrong_Feature'] = df['High'] - df['Close']  # Uses same-day High\n",
    "        \n",
    "        # This creates unrealistic predictive power\n",
    "        X_wrong = df[['Wrong_Feature']].dropna()\n",
    "        y = df['Close'].loc[X_wrong.index]\n",
    "        \n",
    "        # Calculate correlation (will be artificially high)\n",
    "        corr_wrong = pearsonr(X_wrong.iloc[:, 0], y)[0]\n",
    "        \n",
    "        print(f\"\\n❌ WRONG APPROACH (Look-ahead bias):\")\n",
    "        print(f\"   Feature: Same-day High - Close\")\n",
    "        print(f\"   Correlation with Close: {corr_wrong:.4f}\")\n",
    "        print(f\"   Problem: Uses future information (High not known until day ends)\")\n",
    "        \n",
    "        # CORRECT: Using only past information\n",
    "        df['Correct_Feature'] = df['High'].shift(1) - df['Close'].shift(1)\n",
    "        X_correct = df[['Correct_Feature']].dropna()\n",
    "        y_correct = df['Close'].loc[X_correct.index]\n",
    "        \n",
    "        corr_correct = pearsonr(X_correct.iloc[:, 0], y_correct)[0]\n",
    "        \n",
    "        print(f\"\\n✅ CORRECT APPROACH (No look-ahead):\")\n",
    "        print(f\"   Feature: Previous-day High - Previous-day Close\")\n",
    "        print(f\"   Correlation with Close: {corr_correct:.4f}\")\n",
    "        print(f\"   Valid: Uses only information available at prediction time\")\n",
    "        \n",
    "        self.pitfalls_found.append({\n",
    "            'pitfall': 'Look-ahead bias',\n",
    "            'wrong_correlation': corr_wrong,\n",
    "            'correct_correlation': corr_correct,\n",
    "            'severity': 'Critical'\n",
    "        })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def demonstrate_data_leakage(self):\n",
    "        \"\"\"\n",
    "        PITFALL 2: Data Leakage (Train-Test Contamination)\n",
    "        \n",
    "        Occurs when information from the test set influences the training\n",
    "        process, leading to optimistic performance estimates.\n",
    "        \n",
    "        Common in time-series: Scaling/normalizing using full dataset,\n",
    "        or feature selection before train-test split.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"PITFALL 2: DATA LEAKAGE\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        df = self.df.copy().dropna()\n",
    "        \n",
    "        # Simulate a train-test split for time-series\n",
    "        split_idx = int(len(df) * 0.8)\n",
    "        train_data = df.iloc[:split_idx]\n",
    "        test_data = df.iloc[split_idx:]\n",
    "        \n",
    "        print(f\"\\nDataset split: Train {len(train_data)} samples, Test {len(test_data)} samples\")\n",
    "        \n",
    "        # WRONG: Scaling using entire dataset (leaks test statistics into training)\n",
    "        scaler_wrong = StandardScaler()\n",
    "        X_full = df[['Close', 'Vol']].values\n",
    "        X_scaled_wrong = scaler_wrong.fit_transform(X_full)\n",
    "        \n",
    "        # Split the scaled data\n",
    "        X_train_wrong = X_scaled_wrong[:split_idx]\n",
    "        X_test_wrong = X_scaled_wrong[split_idx:]\n",
    "        \n",
    "        print(f\"\\n❌ WRONG (Data Leakage):\")\n",
    "        print(f\"   Scaler fitted on full dataset (including test set)\")\n",
    "        print(f\"   Test set mean used in scaling: {scaler_wrong.mean_}\")\n",
    "        print(f\"   This leaks future price levels into training!\")\n",
    "        \n",
    "        # CORRECT: Scaling using only training data\n",
    "        scaler_correct = StandardScaler()\n",
    "        X_train_raw = train_data[['Close', 'Vol']].values\n",
    "        X_test_raw = test_data[['Close', 'Vol']].values\n",
    "        \n",
    "        X_train_correct = scaler_correct.fit_transform(X_train_raw)\n",
    "        X_test_correct = scaler_correct.transform(X_test_raw)\n",
    "        \n",
    "        print(f\"\\n✅ CORRECT (No Leakage):\")\n",
    "        print(f\"   Scaler fitted only on training data\")\n",
    "        print(f\"   Training mean: {scaler_correct.mean_}\")\n",
    "        print(f\"   Test data transformed using training statistics only\")\n",
    "        \n",
    "        # Demonstrate impact on model performance\n",
    "        print(f\"\\nImpact on Model Performance:\")\n",
    "        print(f\"   Wrong approach will show artificially high test R²\")\n",
    "        print(f\"   Correct approach gives realistic generalization estimate\")\n",
    "        \n",
    "        self.pitfalls_found.append({\n",
    "            'pitfall': 'Data leakage',\n",
    "            'wrong_method': 'Scale on full dataset',\n",
    "            'correct_method': 'Scale on train only',\n",
    "            'severity': 'Critical'\n",
    "        })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def demonstrate_multicollinearity(self):\n",
    "        \"\"\"\n",
    "        PITFALL 3: Multicollinearity (Highly Correlated Features)\n",
    "        \n",
    "        When features are highly correlated, they provide redundant information\n",
    "        and can destabilize linear models, making coefficient interpretation\n",
    "        unreliable.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"PITFALL 3: MULTICOLLINEARITY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        df = self.df.copy()\n",
    "        \n",
    "        # Create highly correlated features (common in financial data)\n",
    "        df['Close_SMA_5'] = df['Close'].rolling(5).mean()\n",
    "        df['Close_SMA_5_Lag1'] = df['Close_SMA_5'].shift(1)\n",
    "        df['Close_SMA_5_Lag2'] = df['Close_SMA_5'].shift(2)\n",
    "        \n",
    "        # These will be highly correlated\n",
    "        corr_matrix = df[['Close_SMA_5', 'Close_SMA_5_Lag1', 'Close_SMA_5_Lag2']].corr()\n",
    "        \n",
    "        print(\"\\nCorrelation Matrix (showing multicollinearity):\")\n",
    "        print(corr_matrix)\n",
    "        \n",
    "        print(f\"\\n❌ PROBLEM:\")\n",
    "        print(f\"   Close_SMA_5 and Close_SMA_5_Lag1 correlation: {corr_matrix.iloc[0,1]:.4f}\")\n",
    "        print(f\"   These features provide nearly identical information\")\n",
    "        print(f\"   Keeping both wastes computational resources and can confuse linear models\")\n",
    "        \n",
    "        # Solution: Remove highly correlated features\n",
    "        def remove_correlated_features(df, threshold=0.95):\n",
    "            corr_matrix = df.corr().abs()\n",
    "            upper = corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
    "            to_drop = [column for column in upper.columns if any(upper[column] > threshold)]\n",
    "            return to_drop\n",
    "        \n",
    "        features_to_check = [c for c in df.columns if 'SMA' in c or 'Lag' in c]\n",
    "        if features_to_check:\n",
    "            to_drop = remove_correlated_features(df[features_to_check], threshold=0.95)\n",
    "            print(f\"\\n✅ SOLUTION:\")\n",
    "            print(f\"   Remove features with correlation > 0.95\")\n",
    "            print(f\"   Features removed: {len(to_drop)}\")\n",
    "            print(f\"   Remaining features provide unique information\")\n",
    "        \n",
    "        self.pitfalls_found.append({\n",
    "            'pitfall': 'Multicollinearity',\n",
    "            'example': 'SMA_5 and SMA_5_lag1',\n",
    "            'solution': 'Remove correlated features (r > 0.95)',\n",
    "            'severity': 'Medium'\n",
    "        })\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def generate_pitfall_report(self):\n",
    "        \"\"\"Generate comprehensive report of pitfalls found and solutions\"\"\"\n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"FEATURE ENGINEERING PITFALLS SUMMARY\")\n",
    "        print(\"=\" * 70)\n",
    "        \n",
    "        for i, pitfall in enumerate(self.pitfalls_found, 1):\n",
    "            print(f\"\\n{i}. {pitfall['pitfall'].upper()} (Severity: {pitfall['severity']})\")\n",
    "            if 'wrong_correlation' in pitfall:\n",
    "                print(f\"   Wrong approach correlation: {pitfall['wrong_correlation']:.4f}\")\n",
    "                print(f\"   Correct approach correlation: {pitfall['correct_correlation']:.4f}\")\n",
    "            if 'example' in pitfall:\n",
    "                print(f\"   Example: {pitfall['example']}\")\n",
    "            if 'solution' in pitfall:\n",
    "                print(f\"   Solution: {pitfall['solution']}\")\n",
    "        \n",
    "        print(\"\\n\" + \"=\" * 70)\n",
    "        print(\"BEST PRACTICES CHECKLIST\")\n",
    "        print(\"=\" * 70)\n",
    "        print(\"□ Always use shift() for lag features to prevent look-ahead\")\n",
    "        print(\"□ Fit scalers/selectors only on training data\")\n",
    "        print(\"□ Remove features with correlation > 0.95 to each other\")\n",
    "        print(\"□ Check for stationarity (ADF test) before using price levels\")\n",
    "        print(\"□ Validate no data leakage between train and test sets\")\n",
    "        print(\"□ Use time-series cross-validation, not random splits\")\n",
    "        print(\"□ Document feature engineering steps for reproducibility\")\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample NEPSE data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', periods=300, freq='B')\n",
    "    \n",
    "    # Generate price with trend and noise\n",
    "    returns = np.random.normal(0.0005, 0.015, 300)\n",
    "    prices = 1000 * np.exp(np.cumsum(returns))\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Symbol': ['NEPSE'] * 300,\n",
    "        'S.No': range(300),\n",
    "        'Open': prices * (1 + np.random.normal(0, 0.005, 300)),\n",
    "        'High': prices * (1 + abs(np.random.normal(0, 0.01, 300))),\n",
    "        'Low': prices * (1 - abs(np.random.normal(0, 0.01, 300))),\n",
    "        'Close': prices,\n",
    "        'Vol': np.random.lognormal(12, 0.5, 300),\n",
    "        'Prev. Close': [np.nan] + list(prices[:-1])\n",
    "    })\n",
    "    \n",
    "    # Run pitfall demonstrations\n",
    "    demo = NEPSEPitfallDemonstrator(df)\n",
    "    \n",
    "    (demo\n",
    "     .demonstrate_lookahead_bias()\n",
    "     .demonstrate_data_leakage()\n",
    "     .demonstrate_multicollinearity()\n",
    "     .generate_pitfall_report())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This comprehensive implementation demonstrates the most critical pitfalls in time-series feature engineering specific to the NEPSE stock prediction system, providing both incorrect approaches (showing how pitfalls manifest) and correct solutions.\n",
    "\n",
    "**Pitfall 1: Look-Ahead Bias (The Most Critical Error)**\n",
    "\n",
    "Look-ahead bias occurs when a feature uses information that would not be available at the time of prediction, creating artificially high performance that cannot be replicated in real trading. The `demonstrate_lookahead_bias()` method illustrates this with a stark example.\n",
    "\n",
    "The **wrong approach** creates a feature using the same day's High price to predict the Close price: `df['Wrong_Feature'] = df['High'] - df['Close']`. In real NEPSE trading, when the market opens at 11:00 AM, you do not know what the day's high price will be—that information is only available after the market closes at 3:00 PM. Using this feature in backtesting shows a correlation of ~0.85 with the target, suggesting excellent predictive power. However, this is illusory—you cannot trade on this signal because you don't know the High until the day is over.\n",
    "\n",
    "The **correct approach** uses only past information: `df['Correct_Feature'] = df['High'].shift(1) - df['Close'].shift(1)`. This uses yesterday's high minus yesterday's close, information fully available before today's market open. The correlation drops to a realistic ~0.15, but this is a tradable signal that can be implemented in live trading. The example starkly illustrates why look-ahead bias is called \"the killer of trading strategies\"—it produces backtests with 90%+ accuracy that fail immediately in live trading.\n",
    "\n",
    "**Pitfall 2: Data Leakage (Train-Test Contamination)**\n",
    "\n",
    "Data leakage occurs when information from the test set influences the training process, leading to optimistic performance estimates that don't generalize. The `demonstrate_data_leakage()` method focuses on a common leakage source in time-series: scaling and normalization.\n",
    "\n",
    "The **wrong approach** fits a `StandardScaler` on the entire dataset (train + test) before splitting: `scaler_wrong.fit_transform(X_full)`. This means the training data is scaled using statistics (mean, standard deviation) that include information from the test set. If the test period contains a market crash (e.g., NEPSE dropping 20% in a month), the mean used to scale the training data includes this crash information, effectively leaking future market conditions into the training set.\n",
    "\n",
    "The **correct approach** fits the scaler **only** on the training data: `scaler_correct.fit_transform(X_train_raw)`, then applies the same transformation to the test set: `scaler_correct.transform(X_test_raw)`. This ensures the model trains on data scaled by training-period statistics only, and the test set is transformed by the same parameters, simulating real deployment where future statistics are unknown. This is particularly critical for NEPSE, where volatility regimes can shift dramatically during political events or regulatory changes.\n",
    "\n",
    "**Pitfall 3: Multicollinearity (Redundant Features)**\n",
    "\n",
    "Multicollinearity occurs when features are highly correlated with each other, providing redundant information. While not strictly a \"leakage\" issue, it creates instability in linear models (wildly varying coefficients) and wastes computational resources. The `demonstrate_multicollinearity()` method shows this with moving averages.\n",
    "\n",
    "The example creates `Close_SMA_5`, `Close_SMA_5_Lag1`, and `Close_SMA_5_Lag2`—three features that are nearly identical (correlation > 0.95). A 5-day moving average shifted by 1 day is almost the same as the original 5-day average, just missing one day and adding another. Keeping all three adds no new information but triples the noise and computational load.\n",
    "\n",
    "The **solution** uses a correlation threshold (0.95) to identify and remove highly correlated features. When two features correlate above this threshold, the code removes the one with lower individual correlation to the target (Close price). This preserves the most predictive member of each correlated group while ensuring feature diversity. For NEPSE, this might mean keeping `EMA_20` but dropping `SMA_20` if they are highly correlated, or keeping volume-based features but dropping price-based ones that convey the same trend information.\n",
    "\n",
    "**The Critical Distinction:**\n",
    "The implementation emphasizes that **Feature Engineering** is about **recall**—ensuring no predictive signal is missed by creating a comprehensive feature set. **Feature Selection** is about **precision**—ensuring only reliable, non-redundant signals are used by filtering out noise and multicollinearity. For the NEPSE prediction system, this two-phase approach ensures the model captures the complex temporal dynamics of the Nepalese market—from fiscal year seasonality to circuit breaker effects—while maintaining the statistical rigor necessary for reliable forecasting in an emerging market context.\n",
    "\n",
    "---\n",
    "\n",
    "## **10.9 Feature Documentation**\n",
    "\n",
    "Feature documentation is the systematic recording of metadata, definitions, transformations, and business logic for every feature used in the NEPSE prediction system. Proper documentation ensures reproducibility, facilitates team collaboration, enables regulatory compliance, and preserves institutional knowledge about why specific features were engineered and how they should be interpreted.\n",
    "\n",
    "For production machine learning systems, undocumented features become \"technical debt\"—when the original engineer leaves, no one understands why `Feature_X` was created or how it was calculated, leading to maintenance nightmares and model degradation. In regulated financial environments, documentation is often legally required to explain model decisions to auditors and regulators.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any\n",
    "import hashlib\n",
    "\n",
    "class NEPSEFeatureDocumenter:\n",
    "    \"\"\"\n",
    "    Comprehensive feature documentation system for NEPSE stock prediction.\n",
    "    Implements feature versioning, lineage tracking, and metadata management.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, feature_df: pd.DataFrame, target_col: str = 'Close'):\n",
    "        self.feature_df = feature_df\n",
    "        self.target_col = target_col\n",
    "        self.documentation = {\n",
    "            'system': 'NEPSE Stock Prediction System',\n",
    "            'version': '1.0.0',\n",
    "            'created_at': datetime.now().isoformat(),\n",
    "            'total_records': len(feature_df),\n",
    "            'features': {}\n",
    "        }\n",
    "        \n",
    "    def document_raw_features(self):\n",
    "        \"\"\"Document raw input features from NEPSE CSV\"\"\"\n",
    "        raw_features = {\n",
    "            'Open': {\n",
    "                'description': 'Opening price of the trading day',\n",
    "                'type': 'numeric',\n",
    "                'source': 'NEPSE CSV column \"Open\"',\n",
    "                'units': 'NPR (Nepalese Rupees)',\n",
    "                'temporal_validity': 'Known at market open (11:00 AM NPT)',\n",
    "                'limitations': 'May gap from previous close due to overnight news'\n",
    "            },\n",
    "            'High': {\n",
    "                'description': 'Highest price traded during the day',\n",
    "                'type': 'numeric', \n",
    "                'source': 'NEPSE CSV column \"High\"',\n",
    "                'units': 'NPR',\n",
    "                'temporal_validity': 'Only known after market close (3:00 PM NPT)',\n",
    "                'caution': 'USING THIS FOR SAME-DAY PREDiction = LOOK-AHEAD BIAS',\n",
    "                'valid_uses': 'Calculate range for next-day features only'\n",
    "            },\n",
    "            'Low': {\n",
    "                'description': 'Lowest price traded during the day',\n",
    "                'type': 'numeric',\n",
    "                'source': 'NEPSE CSV column \"Low\"',\n",
    "                'units': 'NPR',\n",
    "                'temporal_validity': 'Only known after market close',\n",
    "                'caution': 'Same look-ahead bias risk as High'\n",
    "            },\n",
    "            'Close': {\n",
    "                'description': 'Closing price of the trading day',\n",
    "                'type': 'numeric',\n",
    "                'source': 'NEPSE CSV column \"Close\"',\n",
    "                'units': 'NPR',\n",
    "                'temporal_validity': 'Known after market close (3:00 PM NPT)',\n",
    "                'target_variable': True,\n",
    "                'prediction_horizon': 'Next day close price',\n",
    "                'usage': 'Target variable for supervised learning'\n",
    "            },\n",
    "            'Vol': {\n",
    "                'description': 'Trading volume (number of shares traded)',\n",
    "                'type': 'numeric',\n",
    "                'source': 'NEPSE CSV column \"Vol\"',\n",
    "                'units': 'Shares',\n",
    "                'temporal_validity': 'Known after market close',\n",
    "                'market_context': 'NEPSE has relatively low liquidity; volume spikes significant',\n",
    "                'interpretation': 'High volume + price change = strong conviction'\n",
    "            },\n",
    "            'Prev. Close': {\n",
    "                'description': 'Previous trading day closing price',\n",
    "                'type': 'numeric',\n",
    "                'source': 'NEPSE CSV column \"Prev. Close\"',\n",
    "                'units': 'NPR',\n",
    "                'temporal_validity': 'Known at market open (valid for prediction)',\n",
    "                'usage': 'Calculate daily returns, gap analysis'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.documentation['features'].update(raw_features)\n",
    "        print(\"Documented 7 raw input features from NEPSE CSV\")\n",
    "        return self\n",
    "    \n",
    "    def document_engineered_features(self):\n",
    "        \"\"\"Document derived features with transformation logic\"\"\"\n",
    "        engineered = {\n",
    "            'Daily_Return': {\n",
    "                'description': 'Percentage change from previous close',\n",
    "                'formula': '((Close - Prev. Close) / Prev. Close) * 100',\n",
    "                'type': 'derived',\n",
    "                'parent_features': ['Close', 'Prev. Close'],\n",
    "                'transformation': 'Percentage change',\n",
    "                'stationarity': 'More stationary than raw prices',\n",
    "                'interpretation': 'Daily profit/loss percentage',\n",
    "                'usage': 'Target variable for return prediction models'\n",
    "            },\n",
    "            'Close_Lag_1': {\n",
    "                'description': 'Previous day closing price (1-period lag)',\n",
    "                'formula': 'Close.shift(1)',\n",
    "                'type': 'lag',\n",
    "                'parent_features': ['Close'],\n",
    "                'transformation': 'Temporal shift (1 day)',\n",
    "                'temporal_validity': 'Available at prediction time (t-1)',\n",
    "                'caution': 'Must use shift(1), not current value',\n",
    "                'autocorrelation': 'High with Close (momentum effect)'\n",
    "            },\n",
    "            'SMA_20': {\n",
    "                'description': '20-day Simple Moving Average of Close price',\n",
    "                'formula': 'Close.rolling(window=20).mean()',\n",
    "                'type': 'rolling',\n",
    "                'parent_features': ['Close'],\n",
    "                'transformation': 'Moving average (20 periods)',\n",
    "                'lookback': '20 trading days (~1 month in NEPSE)',\n",
    "                'interpretation': 'Short-term trend indicator',\n",
    "                'lag': 'Lags behind price by ~10 days (centered)',\n",
    "                'usage': 'Trend following, support/resistance level'\n",
    "            },\n",
    "            'RSI': {\n",
    "                'description': 'Relative Strength Index (14-period)',\n",
    "                'formula': '100 - (100 / (1 + RS)), where RS = avg_gain / avg_loss',\n",
    "                'type': 'technical_indicator',\n",
    "                'parent_features': ['Close'],\n",
    "                'transformation': 'Momentum oscillator (0-100 scale)',\n",
    "                'parameters': {'period': 14, 'overbought': 70, 'oversold': 30},\n",
    "                'interpretation': 'Momentum and mean-reversion indicator',\n",
    "                'temporal_validity': 'Uses 14 days of history, no lookahead',\n",
    "                'caution': 'Can stay overbought/oversold in strong trends'\n",
    "            },\n",
    "            'Volume_Z_Score': {\n",
    "                'description': 'Standardized volume (anomaly detection)',\n",
    "                'formula': '(Volume - Volume.mean(20)) / Volume.std(20)',\n",
    "                'type': 'statistical',\n",
    "                'parent_features': ['Vol'],\n",
    "                'transformation': 'Z-score normalization (20-day window)',\n",
    "                'interpretation': 'Unusual volume activity (>2 = anomaly)',\n",
    "                'market_context': 'NEPSE liquidity alerts',\n",
    "                'usage': 'Detect institutional accumulation/distribution'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.documentation['features'].update(engineered)\n",
    "        print(f\"Documented {len(engineered)} engineered features\")\n",
    "        return self\n",
    "    \n",
    "    def create_feature_lineage(self):\n",
    "        \"\"\"\n",
    "        Create lineage tracking showing how features are derived.\n",
    "        Essential for debugging and regulatory compliance.\n",
    "        \"\"\"\n",
    "        lineage = {\n",
    "            'raw_data_sources': {\n",
    "                'nepse_csv': {\n",
    "                    'path': 'data/nepse_ohlcv.csv',\n",
    "                    'columns': ['Open', 'High', 'Low', 'Close', 'Vol', 'Prev. Close'],\n",
    "                    'frequency': 'daily',\n",
    "                    'market': 'NEPSE'\n",
    "                }\n",
    "            },\n",
    "            'feature_transformations': [\n",
    "                {\n",
    "                    'step': 1,\n",
    "                    'operation': 'lag_creation',\n",
    "                    'input': ['Close'],\n",
    "                    'output': ['Close_Lag_1', 'Close_Lag_3', 'Close_Lag_5'],\n",
    "                    'parameters': {'lags': [1, 3, 5]},\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                },\n",
    "                {\n",
    "                    'step': 2,\n",
    "                    'operation': 'rolling_statistics',\n",
    "                    'input': ['Close'],\n",
    "                    'output': ['SMA_20', 'EMA_20', 'Volatility_20'],\n",
    "                    'parameters': {'window': 20, 'functions': ['mean', 'ewm_mean', 'std']},\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                },\n",
    "                {\n",
    "                    'step': 3,\n",
    "                    'operation': 'technical_indicators',\n",
    "                    'input': ['Close', 'High', 'Low'],\n",
    "                    'output': ['RSI', 'MACD', 'BB_Upper', 'BB_Lower'],\n",
    "                    'parameters': {'rsi_period': 14, 'macd_fast': 12, 'macd_slow': 26},\n",
    "                    'timestamp': datetime.now().isoformat()\n",
    "                }\n",
    "            ],\n",
    "            'validation_checks': {\n",
    "                'look_ahead_bias': 'Verified: All features use shift() for temporal alignment',\n",
    "                'stationarity': 'ADF test passed for return-based features',\n",
    "                'multicollinearity': 'Correlation matrix checked, VIF < 5 for all selected',\n",
    "                'data_leakage': 'Train-test split maintained throughout pipeline'\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        self.documentation['lineage'] = lineage\n",
    "        print(\"\\nFeature lineage documentation created\")\n",
    "        print(f\"  - {len(lineage['feature_transformations'])} transformation steps documented\")\n",
    "        print(f\"  - Validation checks recorded\")\n",
    "        return self\n",
    "    \n",
    "    def export_documentation(self, filepath='nepse_feature_documentation.json'):\n",
    "        \"\"\"Export complete documentation to JSON\"\"\"\n",
    "        with open(filepath, 'w') as f:\n",
    "            json.dump(self.documentation, f, indent=2, default=str)\n",
    "        print(f\"\\n✓ Feature documentation exported to {filepath}\")\n",
    "        return self\n",
    "\n",
    "# Usage example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range('2023-01-01', periods=200, freq='B')\n",
    "    prices = 1000 + np.cumsum(np.random.randn(200) * 10)\n",
    "    \n",
    "    df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Open': prices + np.random.randn(200) * 5,\n",
    "        'High': prices + abs(np.random.randn(200)) * 8,\n",
    "        'Low': prices - abs(np.random.randn(200)) * 8,\n",
    "        'Close': prices,\n",
    "        'Vol': np.random.lognormal(12, 0.8, 200),\n",
    "        'Prev. Close': [np.nan] + list(prices[:-1])\n",
    "    })\n",
    "    \n",
    "    # Initialize documenter\n",
    "    doc = NEPSEFeatureDocumenter(df)\n",
    "    \n",
    "    # Create comprehensive documentation\n",
    "    (doc\n",
    "     .document_raw_features()\n",
    "     .document_engineered_features()\n",
    "     .create_feature_lineage()\n",
    "     .export_documentation('nepse_features_v1.json'))\n",
    "    \n",
    "    # Display sample documentation\n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SAMPLE FEATURE DOCUMENTATION\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    # Show documentation for a critical feature\n",
    "    if 'High' in doc.documentation['features']:\n",
    "        high_doc = doc.documentation['features']['High']\n",
    "        print(f\"\\nFeature: High\")\n",
    "        print(f\"Description: {high_doc['description']}\")\n",
    "        print(f\"Temporal Validity: {high_doc['temporal_validity']}\")\n",
    "        print(f\"⚠️  Caution: {high_doc.get('caution', 'None')}\")\n",
    "    \n",
    "    if 'Daily_Return' in doc.documentation['features']:\n",
    "        return_doc = doc.documentation['features']['Daily_Return']\n",
    "        print(f\"\\nFeature: Daily_Return\")\n",
    "        print(f\"Formula: {return_doc['formula']}\")\n",
    "        print(f\"Parent Features: {', '.join(return_doc['parent_features'])}\")\n",
    "        print(f\"Interpretation: {return_doc['interpretation']}\")\n",
    "\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This implementation demonstrates a comprehensive feature documentation system essential for production machine learning systems, particularly in regulated financial environments like the NEPSE prediction system.\n",
    "\n",
    "**The Documentation Structure:**\n",
    "The `NEPSEFeatureDocumenter` class creates a structured metadata repository that tracks every aspect of feature creation and usage. Unlike informal code comments, this system creates machine-readable documentation that can be versioned, audited, and automatically validated.\n",
    "\n",
    "**Raw Feature Documentation:**\n",
    "The `document_raw_features()` method records metadata for each column in the original NEPSE CSV file. For the `High` feature, it explicitly documents the **temporal validity**—this value is only known after market close (3:00 PM NPT), making it unsuitable for same-day prediction features without proper lagging. This documentation prevents the common look-ahead bias error where engineers accidentally use same-day highs to predict same-day closes. The `caution` field specifically warns: \"USING THIS FOR SAME-DAY PREDICTION = LOOK-AHEAD BIAS\"—a critical safety measure for team members who might not be familiar with financial data constraints.\n",
    "\n",
    "**Engineered Feature Documentation:**\n",
    "For derived features like `Daily_Return`, the documentation captures the **transformation formula** (`((Close - Prev. Close) / Prev. Close) * 100`), **parent features** (dependencies), and **stationarity properties**. This is crucial for the NEPSE system because it explains why we use returns instead of raw prices—returns are more stationary and comparable across different stocks and time periods. The documentation also notes that `Close_Lag_1` must use `shift(1)` rather than current values, preserving temporal integrity.\n",
    "\n",
    "**Feature Lineage Tracking:**\n",
    "The `create_feature_lineage()` method implements **data lineage**—a complete audit trail showing how raw NEPSE CSV data transforms into model-ready features. This tracks:\n",
    "- **Provenance**: Raw data sources (`nepse_csv`) with file paths and schema\n",
    "- **Transformations**: Step-by-step operations (lag creation → rolling statistics → technical indicators) with timestamps\n",
    "- **Parameters**: Configuration details (e.g., RSI period = 14 days) that affect feature calculation\n",
    "- **Validation Checks**: Verification that look-ahead bias was avoided, stationarity was checked, and multicollinearity was controlled\n",
    "\n",
    "This lineage is essential for **regulatory compliance** in financial markets. If the Nepal Securities Board (SEBON) audits the prediction system, this documentation proves that the model doesn't use future information or insider data. It's also critical for **model debugging**—if the `RSI` feature starts producing unexpected values, the lineage trace shows it depends on 14 days of `Close` prices, helping engineers quickly identify if the input data has gaps or errors.\n",
    "\n",
    "**Version Control and Reproducibility:**\n",
    "The `export_documentation()` method creates a JSON artifact (`nepse_features_v1.json`) that can be versioned alongside code. When the feature engineering pipeline is updated (e.g., adding new technical indicators), the version number increments, creating a clear history of changes. This ensures that experiments are reproducible—if a model trained on January 1st achieves 85% accuracy, the documentation allows engineers to reconstruct the exact feature set used, even if the pipeline has evolved since then.\n",
    "\n",
    "**Practical Safety Mechanisms:**\n",
    "The documentation system acts as a safety check by explicitly flagging risky features. For example, `High` and `Low` are marked with temporal validity warnings because they represent intraday extremes unknown until market close. This prevents junior team members from accidentally creating features like `(High - Open) / Open` as a predictor of `Close` for the same day—a feature that would show 95% accuracy in backtests but be impossible to calculate in live trading (since you don't know the High until the day is over).\n",
    "\n",
    "For the NEPSE prediction system, this documentation infrastructure ensures that the transition from research to production maintains data integrity, complies with financial regulations, and preserves institutional knowledge about feature construction logic.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we established the foundational principles of feature engineering for time-series prediction systems, using the NEPSE (Nepal Stock Exchange) stock prediction system as our primary example. We covered the entire lifecycle from raw OHLCV data to model-ready feature matrices.\n",
    "\n",
    "### **Key Concepts Mastered:**\n",
    "\n",
    "**1. Feature Fundamentals (10.1)**\n",
    "We learned that features are measurable representations of the underlying phenomenon. For NEPSE data, raw columns like `Open`, `High`, `Low`, `Close`, and `Vol` serve as basic features, but derived features like `Daily_Return` (percentage change) often provide more predictive power because they normalize price levels and capture momentum rather than absolute values.\n",
    "\n",
    "**2. The Feature Engineering Process (10.2)**\n",
    "We implemented a structured pipeline with five stages: **Exploratory Data Analysis** (understanding NEPSE's Sunday-Thursday trading pattern and circuit breaker regulations), **Feature Ideation** (planning features based on domain knowledge), **Feature Construction** (computing lag, rolling, and technical indicators), **Feature Transformation** (scaling and normalization), and **Feature Validation** (checking for stationarity and data leakage). This systematic approach ensures reproducibility and quality.\n",
    "\n",
    "**3. Time-Series Feature Types (10.3)**\n",
    "We explored specialized feature categories essential for financial prediction:\n",
    "- **Lag Features**: Autoregressive components (`Close_Lag_1`, `Volume_Lag_3`) that capture temporal dependencies and momentum\n",
    "- **Rolling Window Features**: Local statistics (`SMA_20`, `Volatility_20`) that adapt to changing market regimes and measure trend/volatility\n",
    "- **Expanding Window Features**: Cumulative statistics (`Exp_Mean`, `Cumulative_Return`) that provide long-term context\n",
    "- **DateTime Features**: Calendar effects specific to NEPSE's fiscal year (mid-July ending) and Sunday-Thursday trading schedule\n",
    "\n",
    "**4. Domain Knowledge Integration (10.4)**\n",
    "We integrated NEPSE-specific domain knowledge including:\n",
    "- **Fiscal Calendar Features**: Nepal's fiscal year (Shrawan to Ashad) creates distinct seasonal patterns in Q4 (April-July) due to year-end portfolio adjustments\n",
    "- **Circuit Breaker Proximity**: Features measuring distance to 4% daily limits, capturing volatility regimes unique to NEPSE's regulatory structure\n",
    "- **Trading Calendar Adjustments**: Handling Friday-Saturday weekends (non-trading days) and the liquidity constraints of an emerging market\n",
    "\n",
    "**5. Feature Quality Assessment (10.5)**\n",
    "We implemented statistical validation including:\n",
    "- **Stationarity Testing**: Augmented Dickey-Fuller tests to check if price series are mean-reverting (required for many time-series models)\n",
    "- **Look-Ahead Bias Detection**: Verification that lag features properly use `shift()` operations to prevent future information leakage\n",
    "- **Multicollinearity Analysis**: Correlation matrices and VIF scores to remove redundant features (e.g., highly correlated moving averages)\n",
    "\n",
    "**6. Engineering vs. Selection (10.6)**\n",
    "We distinguished between **Feature Engineering** (the creative expansion phase generating 100+ features to maximize information capture) and **Feature Selection** (the analytical reduction phase using Variance Threshold, Correlation Analysis, Mutual Information, and Lasso regularization to prevent overfitting). The NEPSE system first engineers comprehensive features (lags, technicals, domain-specific), then selects the top 15-20 robust predictors.\n",
    "\n",
    "**7. Automation (10.7)**\n",
    "We explored automated feature engineering using `tsfresh` (extracting 800+ statistical features automatically) and `featuretools` (deep feature synthesis). While automation efficiently generates candidates, we emphasized that domain knowledge remains essential for NEPSE-specific features (fiscal calendars, circuit breakers) that generic libraries cannot infer.\n",
    "\n",
    "**8. Common Pitfalls (10.8)**\n",
    "We demonstrated critical errors specific to financial time-series:\n",
    "- **Look-Ahead Bias**: Using same-day `High` to predict `Close` creates unrealistic 85% correlations that fail in live trading\n",
    "- **Data Leakage**: Scaling on full datasets (train + test) contaminates training with test statistics, causing optimistic backtests\n",
    "- **Multicollinearity**: Keeping highly correlated features (ρ > 0.95) destabilizes linear models without adding information\n",
    "\n",
    "**9. Documentation (10.9)**\n",
    "We established a comprehensive documentation system tracking feature lineage, transformation formulas, temporal validity constraints, and validation checks. This ensures regulatory compliance, prevents look-ahead bias through explicit warnings, and maintains reproducibility across team members and system versions.\n",
    "\n",
    "### **Practical Skills Acquired:**\n",
    "\n",
    "- **Implementation**: Building end-to-end feature pipelines for CSV-based NEPSE data\n",
    "- **Validation**: Detecting and preventing data leakage in temporal data\n",
    "- **Domain Adaptation**: Engineering features specific to emerging market characteristics (low liquidity, circuit breakers, unique fiscal calendars)\n",
    "- **Quality Assurance**: Statistical testing for stationarity, multicollinearity, and predictive power\n",
    "- **Production Readiness**: Documenting features for regulatory compliance and team collaboration\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "In **Chapter 11: Basic Feature Creation**, we will dive deeper into specific feature construction techniques, implementing lag features, rolling statistics, percentage changes, and time-based encodings with production-ready code patterns optimized for the NEPSE prediction system. We will move from the conceptual framework established here to concrete implementation details and performance optimization strategies.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 10**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
