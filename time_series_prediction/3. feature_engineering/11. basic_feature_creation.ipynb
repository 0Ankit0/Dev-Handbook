{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 11: Basic Feature Creation**\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Extract raw value features from NEPSE CSV data and understand their temporal validity constraints\n",
    "- Construct difference features (absolute and relative) that capture daily price action and volatility\n",
    "- Calculate percentage change features (returns) normalized for cross-stock and cross-time comparison\n",
    "- Implement lag features correctly using time-series shift operations to prevent look-ahead bias\n",
    "- Compute rolling window statistics (moving averages, volatility) with appropriate window sizes for NEPSE's trading calendar\n",
    "- Generate expanding window features for cumulative trend analysis\n",
    "- Engineer time-based features specific to NEPSE's Sunday-Thursday trading schedule and Nepali fiscal calendar\n",
    "- Create interaction features combining price and volume information\n",
    "- Apply mathematical transformations (log, power) to handle skewed distributions common in financial data\n",
    "- Implement efficient computation patterns using vectorized pandas/numpy operations\n",
    "\n",
    "---\n",
    "\n",
    "## **11.1 Raw Value Features**\n",
    "\n",
    "Raw value features are the fundamental measurements present in the original NEPSE dataset without transformation. While seemingly simple, proper handling of raw features requires understanding their temporal characteristics, data types, and implicit information content. These features form the foundation upon which all derived features are built.\n",
    "\n",
    "The NEPSE CSV provides several raw price and volume measurements: `Open`, `High`, `Low`, `Close`, `LTP` (Last Traded Price), `VWAP` (Volume Weighted Average Price), `Vol` (Volume), `Turnover`, and `Trans.` (Number of Transactions). Each has distinct properties regarding when the information becomes available during the trading day and how it should be used in predictive modeling.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "class NEPSEBasicFeatureEngineer:\n",
    "    \"\"\"\n",
    "    Basic feature creation for NEPSE stock prediction system.\n",
    "    Handles raw value extraction and initial preprocessing.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, csv_path: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.df = None\n",
    "        self.feature_metadata = {}\n",
    "        \n",
    "    def load_and_validate_raw_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load NEPSE CSV and extract raw features with validation.\n",
    "        \n",
    "        NEPSE CSV Structure:\n",
    "        S.No, Symbol, Conf., Open, High, Low, Close, LTP, Close - LTP, \n",
    "        Close - LTP %, VWAP, Vol, Prev. Close, Turnover, Trans., Diff, \n",
    "        Range, Diff %, Range %, VWAP %, 52 Weeks High, 52 Weeks Low\n",
    "        \"\"\"\n",
    "        print(\"Loading NEPSE raw data...\")\n",
    "        self.df = pd.read_csv(self.csv_path)\n",
    "        \n",
    "        # Display initial structure\n",
    "        print(f\"Loaded {len(self.df)} records\")\n",
    "        print(f\"Available columns: {self.df.columns.tolist()}\")\n",
    "        \n",
    "        # Validate required raw features exist\n",
    "        required_features = ['Open', 'High', 'Low', 'Close', 'Vol', 'Prev. Close']\n",
    "        missing = [f for f in required_features if f not in self.df.columns]\n",
    "        \n",
    "        if missing:\n",
    "            raise ValueError(f\"Missing required features: {missing}\")\n",
    "        \n",
    "        # Data type validation and conversion\n",
    "        price_columns = ['Open', 'High', 'Low', 'Close', 'LTP', 'VWAP', \n",
    "                        'Prev. Close', '52 Weeks High', '52 Weeks Low']\n",
    "        \n",
    "        for col in price_columns:\n",
    "            if col in self.df.columns:\n",
    "                # Convert to float, handling any string formatting\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "                \n",
    "        # Volume and turnover as integers\n",
    "        volume_columns = ['Vol', 'Turnover', 'Trans.']\n",
    "        for col in volume_columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = pd.to_numeric(self.df[col], errors='coerce')\n",
    "        \n",
    "        # Handle missing values (forward fill for prices, 0 for volume)\n",
    "        for col in price_columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].ffill().bfill()  # Forward then backward fill\n",
    "        \n",
    "        for col in volume_columns:\n",
    "            if col in self.df.columns:\n",
    "                self.df[col] = self.df[col].fillna(0)\n",
    "        \n",
    "        # Sort by S.No to ensure temporal order\n",
    "        if 'S.No' in self.df.columns:\n",
    "            self.df = self.df.sort_values('S.No').reset_index(drop=True)\n",
    "        \n",
    "        print(f\"✓ Raw features validated and cleaned\")\n",
    "        return self.df\n",
    "    \n",
    "    def extract_price_features(self) -> Dict[str, pd.Series]:\n",
    "        \"\"\"\n",
    "        Extract and categorize raw price features by temporal availability.\n",
    "        \n",
    "        Critical for preventing look-ahead bias in feature engineering.\n",
    "        \"\"\"\n",
    "        features = {}\n",
    "        \n",
    "        # Group 1: Opening information (known at 11:00 AM NPT when market opens)\n",
    "        features['opening'] = {\n",
    "            'Open': self.df['Open'],\n",
    "            'Prev_Close': self.df['Prev. Close'],  # Yesterday's close\n",
    "            'Overnight_Gap': self.df['Open'] - self.df['Prev. Close']\n",
    "        }\n",
    "        \n",
    "        # Group 2: Intraday extremes (only known after 3:00 PM market close)\n",
    "        # WARNING: These cannot be used for same-day prediction without lagging\n",
    "        features['intraday_extremes'] = {\n",
    "            'High': self.df['High'],\n",
    "            'Low': self.df['Low'],\n",
    "            'True_Range': self.df['High'] - self.df['Low'],\n",
    "            'Mid_Price': (self.df['High'] + self.df['Low']) / 2\n",
    "        }\n",
    "        \n",
    "        # Group 3: Closing information (known after 3:00 PM)\n",
    "        features['closing'] = {\n",
    "            'Close': self.df['Close'],\n",
    "            'LTP': self.df['LTP'],  # Last Traded Price (usually = Close)\n",
    "            'VWAP': self.df['VWAP']  # Volume Weighted Average Price\n",
    "        }\n",
    "        \n",
    "        # Group 4: Volume information (cumulative during day, final after close)\n",
    "        features['volume'] = {\n",
    "            'Volume': self.df['Vol'],\n",
    "            'Turnover': self.df['Turnover'],  # Value traded\n",
    "            'Transactions': self.df['Trans.'],  # Number of trades\n",
    "            'Avg_Trade_Size': self.df['Turnover'] / (self.df['Trans.'] + 1)  # Avoid div by zero\n",
    "        }\n",
    "        \n",
    "        # Document temporal constraints\n",
    "        self.feature_metadata['temporal_groups'] = {\n",
    "            'opening': 'Available at market open (11:00 AM NPT)',\n",
    "            'intraday_extremes': 'Only available after market close (3:00 PM NPT)',\n",
    "            'closing': 'Available after market close (3:00 PM NPT)',\n",
    "            'volume': 'Cumulative during day, final after close'\n",
    "        }\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def create_basic_price_relationships(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create basic relationship features from raw prices.\n",
    "        These are still \"basic\" as they use only same-day information.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Position of Close within daily range (0=at low, 1=at high)\n",
    "        # Also known as \"Stochastic\" or \"%K\" in technical analysis\n",
    "        df['Close_Position'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'] + 0.0001)\n",
    "        \n",
    "        # Distance from VWAP (Volume Weighted Average Price)\n",
    "        # Positive = closed above average price (bullish)\n",
    "        # Negative = closed below average price (bearish)\n",
    "        df['VWAP_Distance'] = df['Close'] - df['VWAP']\n",
    "        df['VWAP_Distance_Pct'] = (df['VWAP_Distance'] / df['VWAP']) * 100\n",
    "        \n",
    "        # Spread between High and Low relative to Close (volatility proxy)\n",
    "        df['Daily_Range_Pct'] = ((df['High'] - df['Low']) / df['Close']) * 100\n",
    "        \n",
    "        # Open to Close relationship (intraday trend)\n",
    "        df['Open_Close_Diff'] = df['Close'] - df['Open']\n",
    "        df['Open_Close_Return'] = (df['Open_Close_Diff'] / df['Open']) * 100\n",
    "        \n",
    "        # Distance from 52-week extremes (long-term position)\n",
    "        df['Distance_52W_High'] = ((df['52 Weeks High'] - df['Close']) / df['52 Weeks High']) * 100\n",
    "        df['Distance_52W_Low'] = ((df['Close'] - df['52 Weeks Low']) / df['52 Weeks Low']) * 100\n",
    "        \n",
    "        print(f\"Created {6} basic relationship features\")\n",
    "        return df\n",
    "\n",
    "# Usage Example\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample NEPSE data matching the CSV structure\n",
    "    np.random.seed(42)\n",
    "    n_days = 100\n",
    "    \n",
    "    sample_data = pd.DataFrame({\n",
    "        'S.No': range(1, n_days + 1),\n",
    "        'Symbol': ['NEPSE'] * n_days,\n",
    "        'Conf.': [''] * n_days,\n",
    "        'Open': np.random.uniform(1800, 2000, n_days),\n",
    "        'High': np.random.uniform(1900, 2100, n_days),\n",
    "        'Low': np.random.uniform(1700, 1900, n_days),\n",
    "        'Close': np.random.uniform(1850, 2050, n_days),\n",
    "        'LTP': np.random.uniform(1850, 2050, n_days),\n",
    "        'Close - LTP': np.random.uniform(-5, 5, n_days),\n",
    "        'Close - LTP %': np.random.uniform(-0.5, 0.5, n_days),\n",
    "        'VWAP': np.random.uniform(1840, 2040, n_days),\n",
    "        'Vol': np.random.randint(1000000, 5000000, n_days),\n",
    "        'Prev. Close': np.random.uniform(1840, 2040, n_days),\n",
    "        'Turnover': np.random.randint(1000000000, 5000000000, n_days),\n",
    "        'Trans.': np.random.randint(1000, 5000, n_days),\n",
    "        'Diff': np.random.uniform(-50, 50, n_days),\n",
    "        'Range': np.random.uniform(100, 200, n_days),\n",
    "        'Diff %': np.random.uniform(-2, 2, n_days),\n",
    "        'Range %': np.random.uniform(1, 5, n_days),\n",
    "        'VWAP %': np.random.uniform(-1, 1, n_days),\n",
    "        '52 Weeks High': np.random.uniform(2500, 3000, n_days),\n",
    "        '52 Weeks Low': np.random.uniform(1200, 1600, n_days)\n",
    "    })\n",
    "    \n",
    "    # Save to CSV for demonstration\n",
    "    sample_data.to_csv('nepse_sample.csv', index=False)\n",
    "    \n",
    "    # Initialize engineer\n",
    "    engineer = NEPSEBasicFeatureEngineer('nepse_sample.csv')\n",
    "    \n",
    "    # Load and validate\n",
    "    engineer.load_and_validate_raw_features()\n",
    "    \n",
    "    # Extract categorized features\n",
    "    raw_groups = engineer.extract_price_features()\n",
    "    \n",
    "    print(\"\\nRaw Feature Groups:\")\n",
    "    for group, features in raw_groups.items():\n",
    "        print(f\"  {group}: {list(features.keys())}\")\n",
    "    \n",
    "    # Create basic relationships\n",
    "    engineer.create_basic_price_relationships()\n",
    "    \n",
    "    # Display sample\n",
    "    print(\"\\nSample of Basic Features:\")\n",
    "    display_cols = ['Close', 'Open', 'Close_Position', 'VWAP_Distance_Pct', \n",
    "                   'Daily_Range_Pct', 'Distance_52W_High']\n",
    "    print(engineer.df[display_cols].head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This implementation establishes the foundation of feature engineering by carefully handling raw NEPSE data with attention to temporal validity—critical for preventing look-ahead bias.\n",
    "\n",
    "**Temporal Grouping Strategy:**\n",
    "The `extract_price_features()` method categorizes raw features by when they become available during the trading day. This is essential because NEPSE operates from 11:00 AM to 3:00 PM Nepal Time, and features must respect this timeline:\n",
    "\n",
    "- **Opening Group**: `Open` and `Prev. Close` are known at 11:00 AM when the market opens. The `Overnight_Gap` (difference between today's open and yesterday's close) captures pre-market sentiment and overnight news impact. These are safe to use for predicting intraday movements because they're known before trading begins.\n",
    "\n",
    "- **Intraday Extremes Group**: `High` and `Low` represent the day's trading range but are **only known after 3:00 PM** when the market closes. The code explicitly warns that these cannot be used for same-day prediction without lagging (shifting by one period). The `True_Range` (High minus Low) measures daily volatility, while `Mid_Price` represents the equilibrium point between buyers and sellers.\n",
    "\n",
    "- **Closing Group**: `Close`, `LTP` (Last Traded Price), and `VWAP` are final values determined after market close. In NEPSE data, `LTP` usually equals `Close` for the last trade, but during the day, LTP represents the most recent transaction price. `VWAP` is particularly valuable because it represents the average price weighted by volume, indicating where the majority of trading occurred.\n",
    "\n",
    "- **Volume Group**: `Volume`, `Turnover`, and `Transactions` measure market activity. The `Avg_Trade_Size` (Turnover divided by number of transactions) reveals whether trading is dominated by large institutional orders or small retail trades—a key characteristic of the NEPSE market structure.\n",
    "\n",
    "**Basic Relationship Features:**\n",
    "The `create_basic_price_relationships()` method transforms raw absolute prices into relative metrics that are more predictive:\n",
    "\n",
    "- **Close_Position**: Normalizes the closing price within the daily range (0 to 1 scale). A value of 0.9 indicates the stock closed near its daily high (bullish sentiment), while 0.1 indicates a close near the low (bearish). This normalization allows comparison across different price levels.\n",
    "\n",
    "- **VWAP_Distance**: Measures how far the closing price deviated from the volume-weighted average. In NEPSE, closing significantly above VWAP suggests late-day buying pressure or institutional accumulation, while closing below suggests distribution. The percentage version normalizes this for cross-stock comparison.\n",
    "\n",
    "- **Daily_Range_Pct**: The intraday range (High-Low) as a percentage of the closing price. This is a pure volatility measure—high values indicate uncertainty or news-driven trading, while low values indicate consensus. For NEPSE, range percentages above 4% often indicate circuit breaker proximity.\n",
    "\n",
    "- **Distance_52W_High/Low**: Positions the current price within its 52-week trading range. These are mean-reversion indicators—stocks near 52-week highs may be overextended, while those near lows may be oversold. In the context of NEPSE's cyclical market behavior, these extremes often precede reversals.\n",
    "\n",
    "**Data Validation and Cleaning:**\n",
    "The `load_and_validate_raw_features()` method implements production-quality data handling. It converts all price columns to numeric types (handling potential string formatting issues in CSV exports), forward-fills missing price data (reasonable for time-series where the last known price remains valid), and fills missing volume data with zeros (assuming no trading occurred). This ensures the pipeline doesn't fail on real-world data quality issues common in NEPSE historical datasets.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.2 Difference Features**\n",
    "\n",
    "Difference features capture absolute changes and spreads between related price measurements. Unlike percentage changes (which we'll cover next), differences preserve the magnitude information and are particularly useful when the absolute scale of price movements carries predictive information—such as when analyzing support/resistance levels or volatility clustering in the NEPSE market.\n",
    "\n",
    "In the context of the Nepalese stock market, difference features help identify intraday momentum, opening gaps, and price compression/expansion patterns that are specific to the market's liquidity characteristics and trading behavior.\n",
    "\n",
    "```python\n",
    "class NEPSEDifferenceFeatures:\n",
    "    \"\"\"\n",
    "    Creation of absolute difference features for NEPSE time-series.\n",
    "    These capture spreads, gaps, and absolute price movements.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def create_intraday_spread_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create features based on spreads between OHLC values.\n",
    "        These capture intraday volatility and price action.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Basic range (High - Low)\n",
    "        # Large values indicate high volatility, small values consolidation\n",
    "        df['HL_Spread'] = df['High'] - df['Low']\n",
    "        \n",
    "        # Upper and lower shadows (wicks) of the candlestick\n",
    "        # These represent rejected price levels\n",
    "        df['Upper_Shadow'] = df['High'] - df[['Close', 'Open']].max(axis=1)\n",
    "        df['Lower_Shadow'] = df[['Close', 'Open']].min(axis=1) - df['Low']\n",
    "        \n",
    "        # Body of the candlestick (Open to Close)\n",
    "        # Large body = strong directional movement\n",
    "        # Small body = indecision (doji)\n",
    "        df['Body_Size'] = abs(df['Close'] - df['Open'])\n",
    "        \n",
    "        # Shadow to Body ratio\n",
    "        # High values indicate rejection of price levels (potential reversal)\n",
    "        # Low values indicate strong trend (marubozu candles)\n",
    "        df['Shadow_Body_Ratio'] = (df['Upper_Shadow'] + df['Lower_Shadow']) / (df['Body_Size'] + 0.0001)\n",
    "        \n",
    "        # True Range (accounts for gaps from previous close)\n",
    "        # More accurate volatility measure than simple HL range\n",
    "        prev_close = df['Prev. Close']\n",
    "        df['True_Range'] = np.maximum(\n",
    "            df['High'] - df['Low'],\n",
    "            np.maximum(\n",
    "                abs(df['High'] - prev_close),\n",
    "                abs(df['Low'] - prev_close)\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        print(\"Created intraday spread features:\")\n",
    "        print(\"  - HL_Spread: High-Low range\")\n",
    "        print(\"  - Upper/Lower_Shadow: Rejection wicks\")\n",
    "        print(\"  - Body_Size: Open-Close magnitude\")\n",
    "        print(\"  - Shadow_Body_Ratio: Rejection strength\")\n",
    "        print(\"  - True_Range: Gap-adjusted volatility\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_gap_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create features for overnight and intraday gaps.\n",
    "        Gaps in NEPSE often indicate significant news or sentiment shifts.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Overnight gap (Open - Previous Close)\n",
    "        # Positive = gap up (bullish overnight sentiment)\n",
    "        # Negative = gap down (bearish overnight sentiment)\n",
    "        df['Overnight_Gap'] = df['Open'] - df['Prev. Close']\n",
    "        \n",
    "        # Gap percentage (normalized)\n",
    "        df['Overnight_Gap_Pct'] = (df['Overnight_Gap'] / df['Prev. Close']) * 100\n",
    "        \n",
    "        # Gap fill indicator (did price return to previous close during the day?)\n",
    "        # Gap up + Close below Open = potential exhaustion\n",
    "        # Gap down + Close above Open = potential reversal\n",
    "        df['Gap_Fill'] = (\n",
    "            ((df['Overnight_Gap'] > 0) & (df['Close'] < df['Open'])) |  # Gap up, closed lower\n",
    "            ((df['Overnight_Gap'] < 0) & (df['Close'] > df['Open']))    # Gap down, closed higher\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Gap size categories (NEPSE context: >2% is significant)\n",
    "        df['Gap_Size_Category'] = pd.cut(\n",
    "            df['Overnight_Gap_Pct'].abs(),\n",
    "            bins=[0, 0.5, 2, 4, 100],\n",
    "            labels=['Small', 'Medium', 'Large', 'Extreme']\n",
    "        )\n",
    "        \n",
    "        # Intraday extension (how far did price move beyond open?)\n",
    "        df['Up_Extension'] = df['High'] - df['Open']  # Buying pressure\n",
    "        df['Down_Extension'] = df['Open'] - df['Low']  # Selling pressure\n",
    "        \n",
    "        # Extension balance (buying vs selling pressure)\n",
    "        df['Extension_Balance'] = df['Up_Extension'] - df['Down_Extension']\n",
    "        \n",
    "        print(\"\\nCreated gap features:\")\n",
    "        print(\"  - Overnight_Gap: Open vs Previous Close\")\n",
    "        print(\"  - Gap_Fill: Boolean if gap was filled\")\n",
    "        print(\"  - Gap_Size_Category: Discretized gap magnitude\")\n",
    "        print(\"  - Up/Down_Extension: Intraday movement beyond open\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_momentum_divergence_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create features showing divergence between price and volume.\n",
    "        Divergences often predict trend changes.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Price change vs Volume change divergence\n",
    "        # Rising price on falling volume = weak trend (potential reversal)\n",
    "        # Falling price on rising volume = strong trend (continuation likely)\n",
    "        \n",
    "        # Calculate differences\n",
    "        price_diff = df['Close'].diff()\n",
    "        volume_diff = df['Vol'].diff()\n",
    "        \n",
    "        # Divergence flags\n",
    "        df['Price_Volume_Divergence'] = (\n",
    "            ((price_diff > 0) & (volume_diff < 0)) |  # Price up, volume down\n",
    "            ((price_diff < 0) & (volume_diff > 0))    # Price down, volume up\n",
    "        ).astype(int)\n",
    "        \n",
    "        # VWAP divergence (price vs volume-weighted price)\n",
    "        # Price above VWAP on increasing volume = accumulation\n",
    "        # Price below VWAP on increasing volume = distribution\n",
    "        df['VWAP_Diff'] = df['Close'] - df['VWAP']\n",
    "        \n",
    "        # Change in VWAP difference (acceleration)\n",
    "        df['VWAP_Diff_Change'] = df['VWAP_Diff'].diff()\n",
    "        \n",
    "        print(\"\\nCreated divergence features:\")\n",
    "        print(\"  - Price_Volume_Divergence: Inverse relationship flag\")\n",
    "        print(\"  - VWAP_Diff: Deviation from volume-weighted price\")\n",
    "        print(\"  - VWAP_Diff_Change: Change in deviation\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Use the engineer from previous section\n",
    "    engineer = NEPSEBasicFeatureEngineer('nepse_sample.csv')\n",
    "    engineer.load_and_validate_raw_features()\n",
    "    engineer.create_basic_price_relationships()\n",
    "    \n",
    "    # Create difference features\n",
    "    diff_engineer = NEPSEDifferenceFeatures(engineer.df)\n",
    "    \n",
    "    diff_engineer.create_intraday_spread_features()\n",
    "    diff_engineer.create_gap_features()\n",
    "    diff_engineer.create_momentum_divergence_features()\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\nSample Difference Features:\")\n",
    "    display_cols = ['Open', 'High', 'Low', 'Close', 'HL_Spread', \n",
    "                   'Upper_Shadow', 'Body_Size', 'Overnight_Gap', \n",
    "                   'Gap_Fill', 'VWAP_Diff']\n",
    "    print(diff_engineer.df[display_cols].head())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section focuses on **absolute difference features** that capture the magnitude of price movements and spreads without normalizing to percentages. These features are particularly valuable for NEPSE analysis because they preserve the scale of price action, which can indicate the intensity of buying/selling pressure.\n",
    "\n",
    "**Intraday Spread Features:**\n",
    "The `create_intraday_spread_features()` method analyzes the internal structure of each trading day's price bar (candlestick). In technical analysis, the relationship between the body (Open-to-Close range) and shadows (wicks) reveals market psychology:\n",
    "\n",
    "- **HL_Spread (High-Low Spread)**: This is the raw daily trading range in Nepalese Rupees (NPR). Unlike percentage ranges, the absolute spread indicates the actual money at stake during the day's volatility. For NEPSE, spreads above NPR 50 on mid-cap stocks often indicate institutional participation or significant news events.\n",
    "\n",
    "- **Upper_Shadow and Lower_Shadow**: These represent price rejections. The `Upper_Shadow` measures how far the price rose above the Open/Close before being rejected (selling pressure). The `Lower_Shadow` measures how far it fell below before recovering (buying support). In the NEPSE context, long upper shadows on high-volume days often indicate distribution by large holders at resistance levels.\n",
    "\n",
    "- **Body_Size**: The absolute difference between Open and Close. Large bodies (relative to recent history) indicate strong conviction—either buying (green/bullish) or selling (red/bearish). Small bodies suggest indecision or equilibrium between buyers and sellers.\n",
    "\n",
    "- **Shadow_Body_Ratio**: A high ratio (>2.0) indicates that the market tested levels significantly beyond the open/close but failed to hold them—a potential reversal signal. Low ratios (<0.5) indicate strong trending days where the price moved consistently in one direction (marubozu candles).\n",
    "\n",
    "- **True_Range**: This is a more sophisticated volatility measure than simple High-Low range because it accounts for gaps from the previous day. If NEPSE opens with a large gap up (due to overnight news), the True Range captures the full extent of the movement, including the gap. This is essential for volatility-based position sizing and stop-loss calculations.\n",
    "\n",
    "**Gap Analysis Features:**\n",
    "The `create_gap_features()` method analyzes discontinuities between trading sessions, which are particularly significant in NEPSE due to the Friday-Saturday weekend and potential overnight news from the Nepali government or central bank (NRB).\n",
    "\n",
    "- **Overnight_Gap**: The absolute difference between today's Open and yesterday's Close. In NEPSE, gaps often occur due to:\n",
    "  - Policy announcements (monetary policy, fiscal budget)\n",
    "  - Quarterly earnings releases\n",
    "  - Regional market movements (Indian markets influence)\n",
    "  - Weekend news accumulation (Friday-Saturday gap)\n",
    "\n",
    "- **Gap_Fill**: A binary indicator showing whether the price returned to the previous day's close during the trading session. Gap fills suggest that the initial sentiment was temporary—gap up followed by selling (close below open) or gap down followed by buying (close above open). In mean-reversion strategies, unfilled gaps often indicate trend continuation.\n",
    "\n",
    "- **Gap_Size_Category**: Discretizes gaps into Small (<0.5%), Medium (0.5-2%), Large (2-4%), and Extreme (>4%). In NEPSE, Extreme gaps often trigger circuit breakers or indicate major corporate actions. This categorical encoding helps tree-based models handle non-linear gap effects.\n",
    "\n",
    "**Momentum Divergence Features:**\n",
    "The `create_momentum_divergence_features()` method identifies discrepancies between price movement and volume, which often precede trend changes:\n",
    "\n",
    "- **Price_Volume_Divergence**: A binary flag indicating when price and volume move in opposite directions (price up/volume down or price down/volume up). In low-liquidity NEPSE stocks, rising prices on declining volume suggest the move is not supported by broad participation and may reverse. Conversely, falling prices on rising volume indicates strong selling pressure likely to continue.\n",
    "\n",
    "- **VWAP_Diff**: The absolute difference between Close and VWAP. Since VWAP represents the \"fair\" average price where most trading occurred, deviations indicate end-of-day sentiment shifts. Large positive VWAP_Diff suggests late buying (bullish), while negative suggests late selling (bearish).\n",
    "\n",
    "These difference features provide the raw material for understanding market microstructure in NEPSE—how prices move, where they find support/resistance, and how volume validates (or invalidates) price movements.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.3 Percentage Change Features**\n",
    "\n",
    "Percentage change features (returns) normalize price movements relative to a base value, enabling cross-sectional comparison across stocks with different price levels and time periods. This normalization is essential for the NEPSE prediction system because it allows the model to learn patterns that generalize across different stocks and market regimes, rather than memorizing price levels specific to individual securities.\n",
    "\n",
    "Returns also have more desirable statistical properties than raw prices—they are closer to stationary (mean-reverting), exhibit less heteroscedasticity (changing variance over time), and approximate normal distributions more closely (though financial returns typically have fat tails).\n",
    "\n",
    "```python\n",
    "class NEPSEPercentageFeatures:\n",
    "    \"\"\"\n",
    "    Percentage change (return) features for NEPSE data.\n",
    "    Normalizes price movements for cross-stock and cross-time comparison.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def create_daily_returns(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create basic return features from closing prices.\n",
    "        Returns are the foundation of financial time-series analysis.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Simple return (arithmetic)\n",
    "        # Formula: (P_t - P_{t-1}) / P_{t-1}\n",
    "        df['Daily_Return'] = df['Close'].pct_change()\n",
    "        \n",
    "        # Log return (continuously compounded)\n",
    "        # Formula: ln(P_t / P_{t-1})\n",
    "        # Properties: Time-additive, symmetric for +/- moves\n",
    "        df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        \n",
    "        # Adjusted returns (accounting for splits/dividends if data available)\n",
    "        # For NEPSE, this is often just the simple return unless corporate actions file provided\n",
    "        df['Adj_Return'] = (df['Close'] - df['Prev. Close']) / df['Prev. Close']\n",
    "        \n",
    "        # Signed returns (positive/negative classification)\n",
    "        df['Return_Direction'] = np.sign(df['Daily_Return'])\n",
    "        df['Is_Positive_Return'] = (df['Daily_Return'] > 0).astype(int)\n",
    "        \n",
    "        # Return magnitude categories (for classification models)\n",
    "        df['Return_Magnitude'] = pd.cut(\n",
    "            df['Daily_Return'].abs(),\n",
    "            bins=[0, 0.01, 0.02, 0.05, 1.0],\n",
    "            labels=['Small', 'Medium', 'Large', 'Extreme']\n",
    "        )\n",
    "        \n",
    "        # Volatility regime (rolling standard deviation of returns)\n",
    "        df['Volatility_20'] = df['Daily_Return'].rolling(window=20).std()\n",
    "        \n",
    "        print(\"Created daily return features:\")\n",
    "        print(\"  - Daily_Return: Arithmetic return (P_t - P_{t-1})/P_{t-1}\")\n",
    "        print(\"  - Log_Return: Continuously compounded return ln(P_t/P_{t-1})\")\n",
    "        print(\"  - Return_Direction: Sign of return (-1, 0, +1)\")\n",
    "        print(\"  - Volatility_20: 20-day rolling standard deviation\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_intraday_returns(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns relative to different reference points during the day.\n",
    "        Captures intraday momentum and mean-reversion.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Return from Open to Close (intraday trend)\n",
    "        df['Intraday_Return'] = (df['Close'] - df['Open']) / df['Open']\n",
    "        \n",
    "        # Return from Low to Close (recovery strength)\n",
    "        # High values indicate strong buying from daily lows\n",
    "        df['Recovery_Strength'] = (df['Close'] - df['Low']) / (df['High'] - df['Low'] + 0.0001)\n",
    "        \n",
    "        # Return from High to Close (give-back)\n",
    "        # Low values indicate selling pressure from highs\n",
    "        df['Give_Back'] = (df['High'] - df['Close']) / (df['High'] - df['Low'] + 0.0001)\n",
    "        \n",
    "        # VWAP return (performance vs average price)\n",
    "        df['VWAP_Return'] = (df['Close'] - df['VWAP']) / df['VWAP']\n",
    "        \n",
    "        # Gap return (overnight performance)\n",
    "        df['Gap_Return'] = (df['Open'] - df['Prev. Close']) / df['Prev. Close']\n",
    "        \n",
    "        # Post-gap drift (how much did price move after the gap?)\n",
    "        df['Post_Gap_Drift'] = df['Intraday_Return'] - df['Gap_Return']\n",
    "        \n",
    "        print(\"\\nCreated intraday return features:\")\n",
    "        print(\"  - Intraday_Return: Open to Close performance\")\n",
    "        print(\"  - Recovery_Strength: Bounce from daily low (0-1)\")\n",
    "        print(\"  - VWAP_Return: Out/under-performance vs VWAP\")\n",
    "        print(\"  - Gap_Return: Overnight gap percentage\")\n",
    "        print(\"  - Post_Gap_Drift: Intraday move after gap\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_relative_returns(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns relative to long-term benchmarks.\n",
    "        Contextualizes daily performance within broader trends.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Return from 52-week high (drawdown)\n",
    "        df['Drawdown_52W'] = (df['Close'] - df['52 Weeks High']) / df['52 Weeks High']\n",
    "        \n",
    "        # Return from 52-week low (upside)\n",
    "        df['Upside_52W'] = (df['Close'] - df['52 Weeks Low']) / df['52 Weeks Low']\n",
    "        \n",
    "        # Position in 52-week range (0 = at low, 1 = at high)\n",
    "        df['Position_52W'] = (df['Close'] - df['52 Weeks Low']) / \\\n",
    "                            (df['52 Weeks High'] - df['52 Weeks Low'] + 0.0001)\n",
    "        \n",
    "        # Distance to 52-week high as percentage\n",
    "        df['Distance_To_High_Pct'] = ((df['52 Weeks High'] - df['Close']) / df['52 Weeks High']) * 100\n",
    "        \n",
    "        print(\"\\nCreated relative return features:\")\n",
    "        print(\"  - Drawdown_52W: Distance from 52-week high (negative)\")\n",
    "        print(\"  - Position_52W: Relative position in 52W range (0-1)\")\n",
    "        print(\"  - Distance_To_High_Pct: Percentage below 52-week high\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_volume_normalized_returns(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Returns weighted by volume activity.\n",
    "        Distinguishes between high-conviction and low-conviction moves.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Volume-weighted return (return * relative volume)\n",
    "        avg_volume = df['Vol'].rolling(window=20).mean()\n",
    "        rel_volume = df['Vol'] / avg_volume\n",
    "        \n",
    "        df['Volume_Weighted_Return'] = df['Daily_Return'] * rel_volume\n",
    "        \n",
    "        # Dollar volume (turnover) weighted features\n",
    "        df['Turnover_Normalized_Return'] = df['Daily_Return'] * (df['Turnover'] / df['Turnover'].rolling(20).mean())\n",
    "        \n",
    "        # Return efficiency (return per unit of volume)\n",
    "        # High return on low volume = efficient move (likely to continue)\n",
    "        # High return on high volume = exhausting move (likely to reverse)\n",
    "        df['Return_Efficiency'] = df['Daily_Return'] / (rel_volume + 0.001)\n",
    "        \n",
    "        print(\"\\nCreated volume-normalized return features:\")\n",
    "        print(\"  - Volume_Weighted_Return: Return scaled by relative volume\")\n",
    "        print(\"  - Return_Efficiency: Return per unit of volume\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Continue from previous example\n",
    "    engineer = NEPSEBasicFeatureEngineer('nepse_sample.csv')\n",
    "    engineer.load_and_validate_raw_features()\n",
    "    engineer.create_basic_price_relationships()\n",
    "    \n",
    "    # Add percentage features\n",
    "    pct_engineer = NEPSEPercentageFeatures(engineer.df)\n",
    "    \n",
    "    pct_engineer.create_daily_returns()\n",
    "    pct_engineer.create_intraday_returns()\n",
    "    pct_engineer.create_relative_returns()\n",
    "    pct_engineer.create_volume_normalized_returns()\n",
    "    \n",
    "    # Display comparison of return types\n",
    "    print(\"\\nComparison of Return Calculations:\")\n",
    "    return_cols = ['Close', 'Daily_Return', 'Log_Return', 'Intraday_Return', 'Gap_Return']\n",
    "    print(pct_engineer.df[return_cols].head(10))\n",
    "    \n",
    "    # Statistical properties\n",
    "    print(\"\\nStatistical Properties of Returns:\")\n",
    "    stats = pct_engineer.df[['Daily_Return', 'Log_Return']].describe()\n",
    "    print(stats)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section implements **percentage change features** (returns) which are fundamental to financial time-series analysis. Unlike raw price differences, returns normalize movements to a percentage of the base price, enabling meaningful comparison across different stocks and time periods in the NEPSE universe.\n",
    "\n",
    "**Daily Return Calculations:**\n",
    "The `create_daily_returns()` method implements the two primary return calculations used in quantitative finance:\n",
    "\n",
    "- **Arithmetic Return (Simple Return)**: Calculated as `(P_t - P_{t-1}) / P_{t-1}` using pandas' `pct_change()` method. This represents the actual percentage gain or loss for holding the stock for one day. For example, a move from NPR 100 to NPR 102 yields a 2% arithmetic return. This is the standard measure for reporting investment performance and calculating portfolio values.\n",
    "\n",
    "- **Log Return (Continuously Compounded Return)**: Calculated as `ln(P_t / P_{t-1})` where `ln` is the natural logarithm. Log returns have mathematical properties that make them preferable for statistical modeling: they are time-additive (the sum of daily log returns equals the total log return over the period), and they are symmetric—a 2% gain followed by a 2% loss returns exactly to the starting point (arithmetic returns do not: 100 → 102 → 99.96).\n",
    "\n",
    "The method also creates **directional indicators** (`Return_Direction` as -1, 0, +1, and `Is_Positive_Return` as binary) which are useful for classification models predicting whether the next day will be up or down, rather than the exact magnitude.\n",
    "\n",
    "**Intraday Return Decomposition:**\n",
    "The `create_intraday_returns()` method breaks down the daily return into components that reveal intraday dynamics:\n",
    "\n",
    "- **Intraday_Return**: The return from market open to close, `(Close - Open) / Open`. This measures the directional trend during the trading session, separate from overnight gaps. For NEPSE, intraday returns are often driven by local sentiment and order flow, while gaps reflect overnight news.\n",
    "\n",
    "- **Recovery_Strength**: Normalized as `(Close - Low) / (High - Low)`, this measures where the closing price sits within the daily range. Values near 1.0 indicate the stock recovered from its lows to close near highs (bullish), while values near 0.0 indicate it sold off from highs to close near lows (bearish). This is a key mean-reversion indicator in NEPSE's often range-bound market.\n",
    "\n",
    "- **VWAP_Return**: The return relative to the volume-weighted average price, `(Close - VWAP) / VWAP`. Closing above VWAP indicates that the \"smart money\" (institutional volume) was buying late in the day, while closing below suggests distribution. In NEPSE, where retail investors dominate, VWAP deviations can indicate institutional positioning.\n",
    "\n",
    "- **Gap_Return vs. Post_Gap_Drift**: The `Gap_Return` captures overnight movement (open vs. previous close), while `Post_Gap_Drift` measures the intraday continuation or reversal of that gap. In NEPSE, \"gap and go\" patterns (strong gap followed by continued move) often occur during earnings seasons, while \"gap and fade\" (gap filled during day) is common during low-volume periods.\n",
    "\n",
    "**Relative Performance Features:**\n",
    "The `create_relative_returns()` method contextualizes current performance within the 52-week trading range:\n",
    "\n",
    "- **Drawdown_52W**: The percentage decline from the 52-week high (negative values). This measures how far the stock has fallen from its peak—a drawdown of -20% indicates a bear market for that specific stock. In NEPSE, where many stocks experience deep drawdowns followed by strong recoveries, this feature helps identify oversold conditions.\n",
    "\n",
    "- **Position_52W**: A normalized position within the 52-week range (0 to 1). Values above 0.8 indicate the stock is near 52-week highs (overbought risk), while values below 0.2 indicate proximity to lows (oversold potential). This is a mean-reversion signal particularly effective in NEPSE's cyclical market structure.\n",
    "\n",
    "**Volume-Normalized Returns:**\n",
    "The `create_volume_normalized_returns()` method addresses a key aspect of NEPSE market microstructure—low liquidity. A 2% return on massive volume has different implications than a 2% return on minimal volume:\n",
    "\n",
    "- **Volume_Weighted_Return**: Multiplies the daily return by relative volume (today's volume divided by 20-day average). This amplifies returns that occur on high volume (strong conviction) and diminishes those on low volume (weak participation). For NEPSE stocks with irregular liquidity, this distinguishes genuine moves from noise.\n",
    "\n",
    "- **Return_Efficiency**: Calculates return per unit of volume, `Daily_Return / Relative_Volume`. High efficiency (large move on low volume) suggests thin markets and potential manipulation or news-driven gaps. Low efficiency (small move on high volume) suggests strong resistance/support levels where supply absorbed demand.\n",
    "\n",
    "These percentage features transform absolute price levels into relative performance metrics that are stationary (stable statistical properties over time) and comparable across the diverse universe of NEPSE stocks, from high-priced commercial banks to low-priced microfinance companies.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.4 Lag Features**\n",
    "\n",
    "Lag features (also called autoregressive features) are the values of a variable at previous time steps. They are the most fundamental time-series features because they directly model the temporal dependence inherent in sequential data—the idea that the past influences the future. In the context of NEPSE stock prediction, lag features capture momentum, mean-reversion, and cyclical patterns that repeat over time.\n",
    "\n",
    "Proper construction of lag features is critical because it is the primary defense against look-ahead bias. By explicitly shifting data backward using the `shift()` function, we ensure that when predicting time $t$, we only use information from times $t-1, t-2, ..., t-n$, never from $t$ or $t+1$.\n",
    "\n",
    "```python\n",
    "class NEPSELagFeatures:\n",
    "    \"\"\"\n",
    "    Lag (autoregressive) feature creation for NEPSE time-series.\n",
    "    Critical for capturing temporal dependencies while preventing look-ahead bias.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        self.lag_features_created = []\n",
    "        \n",
    "    def create_price_lags(self, lags: List[int] = [1, 2, 3, 5, 10, 20]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create lagged price features for autoregressive modeling.\n",
    "        \n",
    "        Args:\n",
    "            lags: List of periods to lag. \n",
    "                  For NEPSE: 1=yesterday, 5=last week, 20=last month (~20 trading days)\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        for lag in lags:\n",
    "            # Lag of Close price (most important for prediction)\n",
    "            col_name = f'Close_Lag_{lag}'\n",
    "            df[col_name] = df['Close'].shift(lag)\n",
    "            self.lag_features_created.append(col_name)\n",
    "            \n",
    "            # Lag of Open (captures opening sentiment)\n",
    "            df[f'Open_Lag_{lag}'] = df['Open'].shift(lag)\n",
    "            \n",
    "            # Lag of High/Low (captures previous support/resistance)\n",
    "            df[f'High_Lag_{lag}'] = df['High'].shift(lag)\n",
    "            df[f'Low_Lag_{lag}'] = df['Low'].shift(lag)\n",
    "            \n",
    "            # Lag of VWAP (institutional price memory)\n",
    "            if 'VWAP' in df.columns:\n",
    "                df[f'VWAP_Lag_{lag}'] = df['VWAP'].shift(lag)\n",
    "        \n",
    "        print(f\"Created price lag features for periods: {lags}\")\n",
    "        print(f\"Total price lag features: {len(lags) * 4}\")  # Close, Open, High, Low\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_return_lags(self, lags: List[int] = [1, 2, 3, 5]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create lagged return features.\n",
    "        Returns are more stationary than prices, often better for modeling.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Ensure we have daily returns calculated\n",
    "        if 'Daily_Return' not in df.columns:\n",
    "            df['Daily_Return'] = df['Close'].pct_change()\n",
    "        \n",
    "        for lag in lags:\n",
    "            # Lagged returns (past performance)\n",
    "            df[f'Return_Lag_{lag}'] = df['Daily_Return'].shift(lag)\n",
    "            \n",
    "            # Sign of lagged return (direction only)\n",
    "            df[f'Return_Direction_Lag_{lag}'] = np.sign(df[f'Return_Lag_{lag}'])\n",
    "            \n",
    "            # Absolute magnitude of lagged return (volatility memory)\n",
    "            df[f'Return_Abs_Lag_{lag}'] = df[f'Return_Lag_{lag}'].abs()\n",
    "        \n",
    "        # Create return autocorrelation features\n",
    "        # (how correlated is today's return with past returns?)\n",
    "        df['Return_AutoCorr_1'] = df['Daily_Return'].shift(1) * df['Daily_Return'].shift(2)\n",
    "        \n",
    "        print(f\"\\nCreated return lag features:\")\n",
    "        print(f\"  - Return lags for periods: {lags}\")\n",
    "        print(f\"  - Direction lags (sign)\")\n",
    "        print(f\"  - Absolute return lags (magnitude)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_volume_lags(self, lags: List[int] = [1, 3, 5]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create lagged volume features.\n",
    "        Volume often leads price (climax volume signals turns).\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        for lag in lags:\n",
    "            # Raw volume lag\n",
    "            df[f'Volume_Lag_{lag}'] = df['Vol'].shift(lag)\n",
    "            \n",
    "            # Relative volume (vs recent average)\n",
    "            avg_vol = df['Vol'].rolling(window=20).mean()\n",
    "            rel_vol = df['Vol'] / avg_vol\n",
    "            df[f'Rel_Volume_Lag_{lag}'] = rel_vol.shift(lag)\n",
    "            \n",
    "            # Volume change (acceleration)\n",
    "            df[f'Volume_Change_Lag_{lag}'] = df['Vol'].pct_change().shift(lag)\n",
    "        \n",
    "        print(f\"\\nCreated volume lag features for periods: {lags}\")\n",
    "        return df\n",
    "    \n",
    "    def create_lag_interactions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create interaction features between different lagged variables.\n",
    "        Captures complex temporal relationships.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Price momentum (difference between recent and older lags)\n",
    "        df['Momentum_1_5'] = df['Close_Lag_1'] - df['Close_Lag_5']  # Short-term momentum\n",
    "        df['Momentum_5_20'] = df['Close_Lag_5'] - df['Close_Lag_20']  # Medium-term momentum\n",
    "        \n",
    "        # Price acceleration (change in momentum)\n",
    "        df['Acceleration'] = df['Momentum_1_5'] - df['Momentum_1_5'].shift(1)\n",
    "        \n",
    "        # Volume-Price interaction lags\n",
    "        if 'Volume_Lag_1' in df.columns and 'Return_Lag_1' in df.columns:\n",
    "            df['Volume_Price_Interaction'] = df['Volume_Lag_1'] * df['Return_Lag_1']\n",
    "        \n",
    "        # Range expansion/contraction (volatility regime)\n",
    "        df['Range_Lag_1'] = (df['High_Lag_1'] - df['Low_Lag_1']) / df['Close_Lag_1']\n",
    "        df['Range_Lag_5'] = (df['High_Lag_5'] - df['Low_Lag_5']) / df['Close_Lag_5']\n",
    "        df['Range_Contraction'] = df['Range_Lag_1'] < df['Range_Lag_5']  # Volatility squeeze\n",
    "        \n",
    "        print(\"\\nCreated lag interaction features:\")\n",
    "        print(\"  - Momentum features (1-5 day, 5-20 day)\")\n",
    "        print(\"  - Acceleration (change in momentum)\")\n",
    "        print(\"  - Range contraction/expansion\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def validate_no_lookahead(self):\n",
    "        \"\"\"\n",
    "        Critical validation: Ensure no feature uses future information.\n",
    "        \"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"VALIDATING NO LOOK-AHEAD BIAS\")\n",
    "        print(\"=\"*60)\n",
    "        \n",
    "        # Check that lag features are properly shifted\n",
    "        if 'Close_Lag_1' in self.df.columns:\n",
    "            # Close_Lag_1 at row i should equal Close at row i-1\n",
    "            is_valid = (self.df['Close_Lag_1'].iloc[1:] == self.df['Close'].iloc[:-1]).all()\n",
    "            \n",
    "            if is_valid:\n",
    "                print(\"✓ Close_Lag_1 correctly shifted (no look-ahead)\")\n",
    "            else:\n",
    "                print(\"✗ ERROR: Close_Lag_1 may contain look-ahead bias!\")\n",
    "        \n",
    "        # Check for any correlation between current target and future features\n",
    "        # (should be near zero for properly constructed lags)\n",
    "        if 'Close_Lag_1' in self.df.columns:\n",
    "            corr = self.df['Close'].corr(self.df['Close_Lag_1'])\n",
    "            print(f\"  Correlation Close vs Close_Lag_1: {corr:.4f} (should be high, ~0.95)\")\n",
    "            \n",
    "        # Ensure no NaN values at the end (which would indicate future data)\n",
    "        nan_count = self.df['Close_Lag_1'].isna().sum()\n",
    "        print(f\"  NaN values in lag features (expected at start): {nan_count}\")\n",
    "        \n",
    "        return self.df\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup data with returns\n",
    "    engineer = NEPSEBasicFeatureEngineer('nepse_sample.csv')\n",
    "    engineer.load_and_validate_raw_features()\n",
    "    \n",
    "    pct_engineer = NEPSEPercentageFeatures(engineer.df)\n",
    "    df_with_returns = pct_engineer.create_daily_returns()\n",
    "    \n",
    "    # Create lag features\n",
    "    lag_engineer = NEPSELagFeatures(df_with_returns)\n",
    "    \n",
    "    lag_engineer.create_price_lags(lags=[1, 3, 5])\n",
    "    lag_engineer.create_return_lags(lags=[1, 2, 5])\n",
    "    lag_engineer.create_volume_lags(lags=[1, 3])\n",
    "    lag_engineer.create_lag_interactions()\n",
    "    lag_engineer.validate_no_lookahead()\n",
    "    \n",
    "    # Display lag structure\n",
    "    print(\"\\nLag Feature Structure (showing temporal alignment):\")\n",
    "    display_cols = ['Close', 'Close_Lag_1', 'Close_Lag_3', 'Return_Lag_1', 'Volume_Lag_1']\n",
    "    print(lag_engineer.df[display_cols].head(10))\n",
    "    \n",
    "    # Show correlation between current price and lags\n",
    "    print(\"\\nAutocorrelation of Close Price:\")\n",
    "    autocorr = lag_engineer.df[['Close', 'Close_Lag_1', 'Close_Lag_3', 'Close_Lag_5']].corr()\n",
    "    print(autocorr)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section implements **lag features**—the temporal backbone of any time-series prediction system. The code demonstrates proper construction techniques that prevent the most common and damaging error in financial machine learning: look-ahead bias.\n",
    "\n",
    "**Price Lag Construction:**\n",
    "The `create_price_lags()` method generates shifted versions of price data. When we execute `df['Close'].shift(1)`, pandas moves the entire series backward by one position, so that at row $t$, we see the value from row $t-1$. This is equivalent to $Close_{t-1}$ in mathematical notation.\n",
    "\n",
    "The method creates lags for multiple periods:\n",
    "- **Lag 1**: Yesterday's price (the strongest predictor of today's price due to market efficiency and momentum)\n",
    "- **Lag 3**: Price from 3 days ago (captures short-term patterns and weekly effects—since NEPSE trades Sun-Thu, lag 3 moves from Sunday → Wednesday)\n",
    "- **Lag 5**: Approximately one trading week in NEPSE (Sun-Thu schedule)\n",
    "- **Lag 20**: Approximately one trading month (~20 trading days in NEPSE)\n",
    "\n",
    "Creating lags for Open, High, Low, and VWAP provides additional context:\n",
    "- **Open_Lag_1**: Yesterday's opening price (indicates how the market started vs. how it ended)\n",
    "- **High_Lag_1/Low_Lag_1**: Yesterday's support and resistance levels (key technical analysis concepts where old highs become new resistance)\n",
    "\n",
    "**Return Lag Construction:**\n",
    "The `create_return_lags()` method shifts percentage returns rather than absolute prices. Returns are often preferred for modeling because they are closer to stationary (constant mean and variance over time). The method creates three variants:\n",
    "- **Return_Lag_n**: The actual percentage return (e.g., +2.3%)\n",
    "- **Return_Direction_Lag_n**: Just the sign (+1, 0, -1), useful for classification tasks predicting up/down\n",
    "- **Return_Abs_Lag_n**: The absolute magnitude (2.3%), capturing volatility memory regardless of direction\n",
    "\n",
    "The **Return_AutoCorr_1** feature multiplies yesterday's return by the day-before's return. Positive values indicate momentum (up followed by up, or down followed by down), while negative values indicate mean-reversion (up followed by down).\n",
    "\n",
    "**Volume Lag Construction:**\n",
    "Volume lags are particularly important in NEPSE because volume often leads price—climax volume (extremely high relative volume) frequently marks turning points. The `create_volume_lags()` method creates:\n",
    "- **Volume_Lag_n**: Raw volume shifted back\n",
    "- **Rel_Volume_Lag_n**: Volume relative to 20-day average (normalizes for different stocks' typical liquidity)\n",
    "- **Volume_Change_Lag_n**: Percentage change in volume (acceleration/deceleration of trading interest)\n",
    "\n",
    "**Lag Interactions:**\n",
    "The `create_lag_interactions()` method combines different lagged variables to capture dynamic relationships:\n",
    "- **Momentum_1_5**: The difference between yesterday's close and the close 5 days ago. Positive values indicate short-term upward momentum; negative indicates downward. This is essentially a 4-day return calculated using lag features.\n",
    "- **Momentum_5_20**: Medium-term trend (close 5 days ago vs. 20 days ago), capturing the \"monthly\" trend in NEPSE.\n",
    "- **Acceleration**: The change in momentum—whether the trend is speeding up or slowing down. This is the second derivative of price and often signals trend exhaustion.\n",
    "- **Range_Contraction**: A boolean indicator that triggers when yesterday's range (High-Low) is smaller than the range 5 days ago. This \"volatility squeeze\" often precedes explosive breakout moves in NEPSE stocks.\n",
    "\n",
    "**Critical Validation:**\n",
    "The `validate_no_lookahead()` method performs essential safety checks. It verifies that `Close_Lag_1` at row $i$ truly equals `Close` at row $i-1$, confirming the shift operation worked correctly. It also checks the correlation between current close and lagged close—if the lag is constructed correctly, this should be very high (~0.95 for daily stock prices), indicating strong autocorrelation. If the correlation were near zero or negative, it would indicate a bug in the shifting logic.\n",
    "\n",
    "For the NEPSE prediction system, these lag features capture the market's memory—how yesterday's price action influences today's opening sentiment, how last week's trend affects this week's direction, and how volume spikes three days ago might predict today's volatility. They form the autoregressive component of the model, allowing it to learn from historical patterns while strictly respecting the temporal arrow that prevents future information from contaminating predictions.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.5 Rolling Window Features**\n",
    "\n",
    "Rolling window features (also called moving window statistics) calculate aggregate metrics over a fixed-size window of recent observations that \"rolls\" forward through time. Unlike expanding windows (which grow indefinitely), rolling windows maintain a constant lookback period, making them adaptive to recent market regimes while ignoring distant history that may no longer be relevant.\n",
    "\n",
    "For the NEPSE prediction system, rolling windows are essential because the Nepalese stock market exhibits regime changes—periods of high volatility during political instability or policy announcements, followed by quiet consolidation phases. Rolling statistics adapt to these changes by focusing on recent behavior, providing dynamic benchmarks for trend and volatility measurement.\n",
    "\n",
    "```python\n",
    "class NEPSE rollingFeatures:\n",
    "    \"\"\"\n",
    "    Rolling window (moving) statistics for NEPSE time-series.\n",
    "    Captures local trends, volatility, and adaptive benchmarks.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def create_moving_averages(self, windows: List[int] = [5, 10, 20, 50]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create Simple and Exponential Moving Averages (SMA, EMA).\n",
    "        Core trend indicators for technical analysis.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        for window in windows:\n",
    "            # Simple Moving Average (SMA) - equal weight to all days\n",
    "            df[f'SMA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "            \n",
    "            # Exponential Moving Average (EMA) - more weight to recent days\n",
    "            # span = window approximates the center of mass\n",
    "            df[f'EMA_{window}'] = df['Close'].ewm(span=window, adjust=False).mean()\n",
    "            \n",
    "            # Moving average distance (price relative to trend)\n",
    "            # Positive = above average (bullish), Negative = below (bearish)\n",
    "            df[f'Dist_SMA_{window}'] = df['Close'] - df[f'SMA_{window}']\n",
    "            df[f'Dist_SMA_{window}_Pct'] = (df[f'Dist_SMA_{window}'] / df[f'SMA_{window}']) * 100\n",
    "            \n",
    "            # Price position within MA envelope (0-1 scale)\n",
    "            # Uses High/Low to create dynamic bands\n",
    "            ma_high = df['High'].rolling(window=window).max()\n",
    "            ma_low = df['Low'].rolling(window=window).min()\n",
    "            df[f'Position_MA_{window}'] = (df['Close'] - ma_low) / (ma_high - ma_low + 0.0001)\n",
    "        \n",
    "        print(f\"Created moving averages for windows: {windows}\")\n",
    "        print(f\"  - SMA (Simple Moving Average)\")\n",
    "        print(f\"  - EMA (Exponential Moving Average)\")\n",
    "        print(f\"  - Distance from MA (absolute and percentage)\")\n",
    "        print(f\"  - Position within MA range\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_volatility_features(self, windows: List[int] = [5, 10, 20]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Rolling volatility and dispersion measures.\n",
    "        Critical for risk management and regime detection in NEPSE.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        for window in windows:\n",
    "            # Standard deviation (volatility)\n",
    "            df[f'Volatility_{window}'] = df['Close'].rolling(window=window).std()\n",
    "            \n",
    "            # Variance (for some statistical models)\n",
    "            df[f'Variance_{window}'] = df['Close'].rolling(window=window).var()\n",
    "            \n",
    "            # Average True Range (ATR) - robust volatility measure\n",
    "            # Uses True Range which accounts for gaps\n",
    "            high_low = df['High'] - df['Low']\n",
    "            high_close = (df['High'] - df['Close'].shift()).abs()\n",
    "            low_close = (df['Low'] - df['Close'].shift()).abs()\n",
    "            true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "            df[f'ATR_{window}'] = true_range.rolling(window=window).mean()\n",
    "            \n",
    "            # Normalized ATR (ATR as percentage of price)\n",
    "            df[f'ATR_Pct_{window}'] = (df[f'ATR_{window}'] / df['Close']) * 100\n",
    "            \n",
    "            # Bollinger Bands (volatility envelopes)\n",
    "            sma = df['Close'].rolling(window=window).mean()\n",
    "            std = df['Close'].rolling(window=window).std()\n",
    "            df[f'BB_Upper_{window}'] = sma + (std * 2)\n",
    "            df[f'BB_Lower_{window}'] = sma - (std * 2)\n",
    "            df[f'BB_Width_{window}'] = df[f'BB_Upper_{window}'] - df[f'BB_Lower_{window}']\n",
    "            df[f'BB_Position_{window}'] = (df['Close'] - df[f'BB_Lower_{window}']) / \\\n",
    "                                          (df[f'BB_Upper_{window}'] - df[f'BB_Lower_{window}'] + 0.0001)\n",
    "        \n",
    "        print(f\"\\nCreated volatility features:\")\n",
    "        print(f\"  - Standard deviation (price volatility)\")\n",
    "        print(f\"  - Average True Range (ATR)\")\n",
    "        print(f\"  - Bollinger Bands (volatility envelopes)\")\n",
    "        print(f\"  - BB Position (relative location within bands)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_volume_rolling_features(self, windows: List[int] = [5, 10, 20]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Rolling statistics for volume analysis.\n",
    "        Identifies accumulation/distribution patterns.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        for window in windows:\n",
    "            # Average volume\n",
    "            df[f'Volume_SMA_{window}'] = df['Vol'].rolling(window=window).mean()\n",
    "            \n",
    "            # Volume standard deviation (identifies unusual activity)\n",
    "            df[f'Volume_Std_{window}'] = df['Vol'].rolling(window=window).std()\n",
    "            \n",
    "            # Relative Volume (today vs recent average)\n",
    "            df[f'Rel_Volume_{window}'] = df['Vol'] / df[f'Volume_SMA_{window}']\n",
    "            \n",
    "            # Volume trend (increasing or decreasing liquidity)\n",
    "            df[f'Volume_Trend_{window}'] = df[f'Volume_SMA_{window}'].diff(5)  # 5-day change\n",
    "            \n",
    "            # On-Balance Volume (OBV) - cumulative volume flow\n",
    "            if 'Daily_Return' in df.columns:\n",
    "                obv = (np.sign(df['Daily_Return']) * df['Vol']).cumsum()\n",
    "                df[f'OBV_{window}'] = obv.rolling(window=window).mean()\n",
    "        \n",
    "        print(f\"\\nCreated volume rolling features:\")\n",
    "        print(f\"  - Volume moving averages\")\n",
    "        print(f\"  - Relative volume (vs average)\")\n",
    "        print(f\"  - Volume trend (change in average)\")\n",
    "        print(f\"  - On-Balance Volume (OBV)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_rolling_statistics(self, windows: List[int] = [10, 20]) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Advanced rolling statistics (skewness, kurtosis, etc.).\n",
    "        Captures distribution shape changes.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        for window in windows:\n",
    "            # Skewness (asymmetry of returns)\n",
    "            # Positive = longer right tail (bigger up moves)\n",
    "            # Negative = longer left tail (bigger down moves)\n",
    "            df[f'Skew_{window}'] = df['Close'].rolling(window=window).skew()\n",
    "            \n",
    "            # Kurtosis (fat tails vs normal distribution)\n",
    "            # High kurtosis = extreme moves more likely (tail risk)\n",
    "            df[f'Kurt_{window}'] = df['Close'].rolling(window=window).kurt()\n",
    "            \n",
    "            # Min/Max (support and resistance levels)\n",
    "            df[f'Rolling_Min_{window}'] = df['Low'].rolling(window=window).min()\n",
    "            df[f'Rolling_Max_{window}'] = df['High'].rolling(window=window).max()\n",
    "            \n",
    "            # Range (volatility proxy)\n",
    "            df[f'Rolling_Range_{window}'] = df[f'Rolling_Max_{window}'] - df[f'Rolling_Min_{window}']\n",
    "            \n",
    "            # Percent rank (where does current price sit in recent range?)\n",
    "            # 0 = at 20-day low, 1 = at 20-day high\n",
    "            df[f'Percent_Rank_{window}'] = df['Close'].rolling(window=window).apply(\n",
    "                lambda x: pd.Series(x).rank(pct=True).iloc[-1], raw=True\n",
    "            )\n",
    "        \n",
    "        print(f\"\\nCreated advanced rolling statistics:\")\n",
    "        print(f\"  - Skewness and Kurtosis (distribution shape)\")\n",
    "        print(f\"  - Rolling Min/Max (support/resistance)\")\n",
    "        print(f\"  - Percent Rank (position in recent range)\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Continue from lag features example\n",
    "    lag_engineer = NEPSELagFeatures(lag_engineer.df)\n",
    "    \n",
    "    # Add rolling features\n",
    "    rolling_engineer = NEPSE rollingFeatures(lag_engineer.df)\n",
    "    \n",
    "    rolling_engineer.create_moving_averages(windows=[5, 20])\n",
    "    rolling_engineer.create_volatility_features(windows=[5, 20])\n",
    "    rolling_engineer.create_volume_rolling_features(windows=[10])\n",
    "    rolling_engineer.create_rolling_statistics(windows=[20])\n",
    "    \n",
    "    # Display rolling features\n",
    "    print(\"\\nRolling Window Features Sample:\")\n",
    "    display_cols = ['Close', 'SMA_20', 'EMA_20', 'Dist_SMA_20_Pct', \n",
    "                   'Volatility_20', 'ATR_20', 'BB_Position_20', 'Percent_Rank_20']\n",
    "    print(rolling_engineer.df[display_cols].tail(10))\n",
    "    \n",
    "    # Show how rolling stats adapt to regime changes\n",
    "    print(\"\\nRolling Statistics During Different Regimes:\")\n",
    "    regime_cols = ['Close', 'Volatility_20', 'Skew_20', 'Rel_Volume_10']\n",
    "    print(rolling_engineer.df[regime_cols].describe())\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section implements **rolling window features**—adaptive statistics that calculate metrics over the most recent $n$ observations, providing dynamic benchmarks that respond to changing market regimes in NEPSE.\n",
    "\n",
    "**Moving Averages (Trend Indicators):**\n",
    "The `create_moving_averages()` method implements both Simple Moving Averages (SMA) and Exponential Moving Averages (EMA). \n",
    "\n",
    "- **SMA**: Calculates the arithmetic mean over the window. For a 20-day window in NEPSE (approximately one trading month), `SMA_20` represents the average consensus price over the past month. Prices above the SMA indicate bullish sentiment; prices below indicate bearish.\n",
    "\n",
    "- **EMA**: Uses exponentially decaying weights, giving more importance to recent prices. The formula `ewm(span=window, adjust=False)` applies a smoothing factor of $2/(window+1)$. For NEPSE, EMA responds faster to new information than SMA, making it better for volatile emerging markets where trends change quickly.\n",
    "\n",
    "- **Distance from MA**: `Dist_SMA_20_Pct` measures how far the current price deviates from its 20-day average as a percentage. Values above +5% suggest overbought conditions (potential mean reversion), while values below -5% suggest oversold conditions. In NEPSE's cyclical market, these deviations often trigger algorithmic and institutional rebalancing.\n",
    "\n",
    "**Volatility Features:**\n",
    "The `create_volatility_features()` method measures price variability and risk:\n",
    "\n",
    "- **Standard Deviation**: The most common volatility measure, calculated as `df['Close'].rolling(window=20).std()`. For NEPSE, 20-day volatility above 2% daily indicates high uncertainty (often during political events or budget announcements), while below 1% indicates consolidation.\n",
    "\n",
    "- **Average True Range (ATR)**: A more robust volatility measure that accounts for gaps between trading sessions. The True Range is the maximum of: (1) current High minus current Low, (2) absolute value of current High minus previous Close, (3) absolute value of current Low minus previous Close. This is crucial for NEPSE because overnight gaps are common due to the Friday-Saturday weekend and overnight news.\n",
    "\n",
    "- **Bollinger Bands**: Volatility envelopes set at ±2 standard deviations from the moving average. The `BB_Position_20` feature (0-1 scale) indicates where the price sits within the bands—values near 0 touch the lower band (oversold), values near 1 touch the upper band (overbought). The `BB_Width` measures the distance between bands; narrowing bands predict volatility expansion (the \"squeeze\" pattern).\n",
    "\n",
    "**Volume Rolling Features:**\n",
    "Volume analysis is particularly important in NEPSE's relatively illiquid market:\n",
    "\n",
    "- **Relative Volume**: `Rel_Volume_10` compares today's volume to the 10-day average. Values above 2.0 indicate twice-normal activity, often signaling institutional participation or news-driven trading. In NEPSE, volume spikes above 3x average frequently mark trend reversals or breakouts.\n",
    "\n",
    "- **On-Balance Volume (OBV)**: A cumulative indicator that adds volume on up-days and subtracts on down-days. The rolling average of OBV (`OBV_10`) smooths this to show whether volume is flowing into or out of the stock. Rising OBV with flat price suggests accumulation (smart money buying before rally).\n",
    "\n",
    "**Advanced Statistics:**\n",
    "The `create_rolling_statistics()` method captures higher-order moments of the return distribution:\n",
    "\n",
    "- **Skewness**: Measures asymmetry. Positive skew indicates frequent small losses and occasional large gains (bullish asymmetry); negative skew indicates frequent small gains with occasional crashes. NEPSE stocks often exhibit negative skew during bear markets as selling accelerates.\n",
    "\n",
    "- **Kurtosis**: Measures \"fat tails\"—the likelihood of extreme moves compared to a normal distribution. High kurtosis (>3) indicates elevated tail risk, common in NEPSE during periods of political uncertainty.\n",
    "\n",
    "- **Percent Rank**: Indicates the current price's position within the recent range (0 = 20-day low, 1 = 20-day high). This is a pure mean-reversion indicator—values near 0 suggest bounce potential, values near 1 suggest pullback risk.\n",
    "\n",
    "These rolling features provide adaptive context that raw prices cannot. While the absolute price level of NPR 2000 for a NEPSE stock is meaningless without context, being \"5% above the 20-day average with volatility expanding and volume 2x normal\" provides a rich, actionable feature vector for machine learning models.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.6 Expanding Window Features**\n",
    "\n",
    "Expanding window features calculate cumulative statistics from the beginning of the series up to the current observation. Unlike rolling windows which use a fixed recent period, expanding windows incorporate all historical data up to time $t$, providing long-term context and historical benchmarks that remain stable over time.\n",
    "\n",
    "For the NEPSE prediction system, expanding features are valuable because they capture the evolving \"memory\" of the market since the stock's listing or the start of the dataset. They provide absolute benchmarks (all-time average, all-time high) against which current prices can be compared, helping identify when stocks reach historically significant levels.\n",
    "\n",
    "```python\n",
    "class NEPSEExpandingFeatures:\n",
    "    \"\"\"\n",
    "    Expanding window (cumulative) statistics for NEPSE.\n",
    "    Capture long-term trends and historical context.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def create_cumulative_statistics(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Create expanding (cumulative) statistics.\n",
    "        These grow with time, incorporating all history up to current point.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Cumulative mean (all-time average price)\n",
    "        # Shows the long-term equilibrium level\n",
    "        df['Cumulative_Mean'] = df['Close'].expanding().mean()\n",
    "        \n",
    "        # Cumulative standard deviation (long-term volatility)\n",
    "        df['Cumulative_Std'] = df['Close'].expanding().std()\n",
    "        \n",
    "        # Distance from cumulative mean (z-score using all history)\n",
    "        df['Dist_Cumulative_Mean'] = (df['Close'] - df['Cumulative_Mean']) / df['Cumulative_Std']\n",
    "        \n",
    "        # Cumulative min/max (all-time highs and lows)\n",
    "        df['Cumulative_Max'] = df['High'].expanding().max()\n",
    "        df['Cumulative_Min'] = df['Low'].expanding().min()\n",
    "        \n",
    "        # Drawdown from all-time high (peak-to-trough decline)\n",
    "        df['Drawdown'] = (df['Close'] - df['Cumulative_Max']) / df['Cumulative_Max']\n",
    "        \n",
    "        # Distance from all-time low (recovery measure)\n",
    "        df['Recovery'] = (df['Close'] - df['Cumulative_Min']) / df['Cumulative_Min']\n",
    "        \n",
    "        print(\"Created expanding window features:\")\n",
    "        print(\"  - Cumulative_Mean: All-time average price\")\n",
    "        print(\"  - Cumulative_Std: Long-term volatility\")\n",
    "        print(\"  - Drawdown: Distance from all-time high (negative)\")\n",
    "        print(\"  - Recovery: Distance from all-time low (positive)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_cumulative_returns(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Cumulative and compound return features.\n",
    "        Track total performance since inception.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Calculate daily returns if not present\n",
    "        if 'Daily_Return' not in df.columns:\n",
    "            df['Daily_Return'] = df['Close'].pct_change()\n",
    "        \n",
    "        # Cumulative return (total percentage gain since start)\n",
    "        # Formula: (P_t / P_0) - 1\n",
    "        df['Cumulative_Return'] = (df['Close'] / df['Close'].iloc[0]) - 1\n",
    "        \n",
    "        # Compound Annual Growth Rate (CAGR) to current point\n",
    "        # Approximate using trading days (assume 252 trading days/year)\n",
    "        days = np.arange(len(df))\n",
    "        df['CAGR'] = ((df['Close'] / df['Close'].iloc[0]) ** (252 / (days + 1))) - 1\n",
    "        \n",
    "        # Cumulative volume (total shares traded since start)\n",
    "        df['Cumulative_Volume'] = df['Vol'].expanding().sum()\n",
    "        \n",
    "        # Average daily volume since start\n",
    "        df['Avg_Volume_Since_Start'] = df['Vol'].expanding().mean()\n",
    "        \n",
    "        # Volume trend (is recent volume above or below historical average?)\n",
    "        df['Volume_vs_Historical'] = df['Vol'] / df['Avg_Volume_Since_Start']\n",
    "        \n",
    "        print(\"\\nCreated cumulative return features:\")\n",
    "        print(\"  - Cumulative_Return: Total return since start\")\n",
    "        print(\"  - CAGR: Compound annual growth rate\")\n",
    "        print(\"  - Cumulative_Volume: Total historical volume\")\n",
    "        print(\"  - Volume_vs_Historical: Recent volume relative to all-time average\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_running_counts(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Running counts and frequencies of specific events.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        if 'Daily_Return' in df.columns:\n",
    "            # Count of positive days so far\n",
    "            df['Positive_Days_Count'] = (df['Daily_Return'] > 0).expanding().sum()\n",
    "            \n",
    "            # Win rate (percentage of days that were positive)\n",
    "            days = np.arange(1, len(df) + 1)\n",
    "            df['Win_Rate'] = df['Positive_Days_Count'] / days\n",
    "            \n",
    "            # Count of extreme moves (>2% in NEPSE context)\n",
    "            extreme_moves = df['Daily_Return'].abs() > 0.02\n",
    "            df['Extreme_Move_Count'] = extreme_moves.expanding().sum()\n",
    "            \n",
    "            # Frequency of extreme moves (increasing = rising volatility regime)\n",
    "            df['Extreme_Move_Freq'] = df['Extreme_Move_Count'] / days\n",
    "        \n",
    "        # Days since all-time high (how long has it been since the peak?)\n",
    "        is_new_high = df['Close'] == df['Cumulative_Max']\n",
    "        df['Days_Since_High'] = (~is_new_high).expanding().sum()\n",
    "        \n",
    "        # Reset counter when new high is made\n",
    "        df.loc[is_new_high, 'Days_Since_High'] = 0\n",
    "        \n",
    "        print(\"\\nCreated running count features:\")\n",
    "        print(\"  - Positive_Days_Count: Total up days\")\n",
    "        print(\"  - Win_Rate: Percentage of up days\")\n",
    "        print(\"  - Extreme_Move_Count: Days with >2% moves\")\n",
    "        print(\"  - Days_Since_High: Duration since last all-time high\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Continue from previous features\n",
    "    expanding_engineer = NEPSEExpandingFeatures(rolling_engineer.df)\n",
    "    \n",
    "    expanding_engineer.create_cumulative_statistics()\n",
    "    expanding_engineer.create_cumulative_returns()\n",
    "    expanding_engineer.create_running_counts()\n",
    "    \n",
    "    # Display expanding features\n",
    "    print(\"\\nExpanding Window Features (showing evolution over time):\")\n",
    "    display_cols = ['Close', 'Cumulative_Mean', 'Drawdown', 'Cumulative_Return', \n",
    "                   'Win_Rate', 'Days_Since_High']\n",
    "    print(expanding_engineer.df[display_cols].iloc[::20])  # Show every 20th row\n",
    "    \n",
    "    # Show how features stabilize over time\n",
    "    print(\"\\nFeature Stability (first vs last 20 observations):\")\n",
    "    print(\"First 20:\")\n",
    "    print(expanding_engineer.df[['Cumulative_Mean', 'Win_Rate']].head(20))\n",
    "    print(\"\\nLast 20:\")\n",
    "    print(expanding_engineer.df[['Cumulative_Mean', 'Win_Rate']].tail(20))\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section implements **expanding window features**—cumulative statistics that incorporate all historical data up to the current point, providing long-term context and stable benchmarks for the NEPSE prediction system.\n",
    "\n",
    "**Cumulative Price Statistics:**\n",
    "The `create_cumulative_statistics()` method generates features that represent the \"full history\" context:\n",
    "\n",
    "- **Cumulative_Mean**: The average price since the beginning of the dataset. Unlike the 20-day moving average which fluctuates, this expands to include all data and becomes increasingly stable. For a NEPSE stock listed 5 years ago, the cumulative mean represents its long-term equilibrium value—prices significantly above this level may be overvalued relative to historical norms.\n",
    "\n",
    "- **Cumulative_Std**: The standard deviation calculated over the entire history. This measures the stock's inherent volatility characteristic. A stock with cumulative_std of NPR 50 is inherently more volatile than one with NPR 10, regardless of current price level.\n",
    "\n",
    "- **Dist_Cumulative_Mean**: A z-score using the cumulative statistics: $(Close - Cumulative\\_Mean) / Cumulative\\_Std$. This indicates how many standard deviations the current price is from its all-time average. Values above +2 suggest extreme overvaluation; below -2 suggest extreme undervaluation (mean reversion opportunities).\n",
    "\n",
    "- **Drawdown**: The percentage decline from the all-time high: $(Close - Cumulative\\_Max) / Cumulative\\_Max$. This is always negative or zero, measuring how far the stock has fallen from its peak. In NEPSE, drawdowns of -30% to -50% are common during bear markets and often mark good entry points for long-term investors.\n",
    "\n",
    "**Cumulative Return Metrics:**\n",
    "The `create_cumulative_returns()` method tracks total performance:\n",
    "\n",
    "- **Cumulative_Return**: The total percentage return since the start of the data: $(P_t / P_0) - 1$. If a NEPSE stock started at NPR 100 and is now NPR 250, the cumulative return is 150%. This provides absolute performance context—has this stock been a long-term winner or loser?\n",
    "\n",
    "- **CAGR (Compound Annual Growth Rate)**: The annualized return rate implied by the cumulative performance. Calculated as $(P_t / P_0)^{(252/t)} - 1$, where 252 represents the number of trading days in a year. This allows comparison across different time periods and stocks. A NEPSE stock with 15% CAGR is outperforming typical bank deposits and many fixed-income alternatives.\n",
    "\n",
    "- **Volume_vs_Historical**: Compares today's volume to the all-time average volume since listing. Values above 2.0 indicate today's activity is twice the historical norm, suggesting unusual interest or institutional activity in an otherwise quiet stock.\n",
    "\n",
    "**Running Count Features:**\n",
    "The `create_running_counts()` method tracks event frequencies:\n",
    "\n",
    "- **Win_Rate**: The cumulative percentage of trading days that closed positive. This stabilizes over time to reflect the stock's underlying trend bias. A win rate above 55% indicates a strong uptrend; below 45% indicates persistent downtrend. For NEPSE, win rates cluster around 50% in sideways markets but deviate during trending periods.\n",
    "\n",
    "- **Days_Since_High**: Counts how many days have passed since the last all-time high was made. In bull markets, this stays near 0 (constant new highs). During corrections, it accumulates, indicating duration of the pullback. In NEPSE, pullbacks lasting more than 60 days (3 months) often represent significant bear markets requiring fundamental reassessment.\n",
    "\n",
    "**Comparison with Rolling Windows:**\n",
    "While rolling windows (20-day) adapt quickly to recent changes, expanding windows provide stability and long-term context. For example:\n",
    "- **Rolling_20_Mean**: Changes significantly during a month-long rally\n",
    "- **Cumulative_Mean**: Moves slowly, reflecting the stock's long-term value anchor\n",
    "\n",
    "In the NEPSE prediction system, combining both is powerful: the rolling mean identifies short-term trends, while the expanding mean identifies deviations from historical value. When `SMA_20` crosses above `Cumulative_Mean`, it signals that recent momentum has pushed the price above its long-term average, potentially indicating the start of a new bull phase in the cyclical NEPSE market.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.7 Time-Based Features**\n",
    "\n",
    "Time-based features encode calendar effects, seasonal patterns, and temporal context that influence stock market behavior. Financial markets exhibit regular patterns tied to the calendar—day-of-week effects (weekends, Monday blues), month-end effects (portfolio rebalancing), and seasonal patterns (fiscal year-end tax selling).\n",
    "\n",
    "For the NEPSE prediction system, time-based features are particularly important because of Nepal's unique calendar structure: the Sunday-Thursday trading week (different from Western markets), the mid-July fiscal year-end (Shrawan to Ashad), and the influence of Nepali festivals (Dashain, Tihar) which create distinct seasonal liquidity patterns.\n",
    "\n",
    "```python\n",
    "class NEPSETimeFeatures:\n",
    "    \"\"\"\n",
    "    Time-based and calendar features specific to NEPSE.\n",
    "    Captures seasonal patterns, trading calendar effects, and fiscal year cycles.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        if 'Date' in self.df.columns:\n",
    "            self.df['Date'] = pd.to_datetime(self.df['Date'])\n",
    "        \n",
    "    def create_basic_calendar_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Standard calendar features (day, month, year).\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Ensure Date is datetime\n",
    "        if 'Date' not in df.columns:\n",
    "            print(\"Warning: No Date column found, creating sequence-based features\")\n",
    "            df['Date'] = pd.date_range(start='2023-01-01', periods=len(df), freq='B')\n",
    "        \n",
    "        # Extract components\n",
    "        df['Year'] = df['Date'].dt.year\n",
    "        df['Month'] = df['Date'].dt.month\n",
    "        df['Day'] = df['Date'].dt.day\n",
    "        df['Day_of_Week'] = df['Date'].dt.dayofweek  # 0=Monday, 6=Sunday\n",
    "        df['Day_of_Year'] = df['Date'].dt.dayofyear\n",
    "        df['Week_of_Year'] = df['Date'].dt.isocalendar().week\n",
    "        \n",
    "        # Quarter (calendar)\n",
    "        df['Quarter'] = df['Date'].dt.quarter\n",
    "        \n",
    "        print(\"Created basic calendar features:\")\n",
    "        print(f\"  - Date range: {df['Date'].min()} to {df['Date'].max()}\")\n",
    "        print(f\"  - Days: {df['Day_of_Week'].nunique()} unique days\")\n",
    "        print(f\"  - Months: {df['Month'].nunique()} unique months\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_nepse_trading_calendar(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        NEPSE-specific trading calendar features.\n",
    "        NEPSE trades Sunday (6) through Thursday (4).\n",
    "        Friday (5) and Saturday (0 in some systems, but 5=Friday, 6=Saturday in pandas) are weekend.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # In pandas: Monday=0, Sunday=6\n",
    "        # NEPSE trades Sunday(6), Monday(0), Tuesday(1), Wednesday(2), Thursday(3)\n",
    "        # Wait, pandas standard: Monday=0, Tuesday=1, Wednesday=2, Thursday=3, Friday=4, Saturday=5, Sunday=6\n",
    "        # So NEPSE trades: Sunday(6), Monday(0), Tuesday(1), Wednesday(2), Thursday(3)\n",
    "        \n",
    "        df['Is_Sunday'] = (df['Day_of_Week'] == 6).astype(int)\n",
    "        df['Is_Monday'] = (df['Day_of_Week'] == 0).astype(int)\n",
    "        df['Is_Thursday'] = (df['Day_of_Week'] == 3).astype(int)  # Thursday is last trading day\n",
    "        \n",
    "        # Weekend proximity (Friday/Saturday are weekend)\n",
    "        # Thursday is \"pre-weekend\", Sunday is \"post-weekend\"\n",
    "        df['Is_Pre_Weekend'] = df['Is_Thursday']  # Last trading day before weekend\n",
    "        df['Is_Post_Weekend'] = df['Is_Sunday']   # First trading day after weekend\n",
    "        \n",
    "        # Days since weekend (0 for Sunday, 4 for Thursday)\n",
    "        # Maps Sunday->0, Monday->1, Tuesday->2, Wednesday->3, Thursday->4\n",
    "        day_map = {6: 0, 0: 1, 1: 2, 2: 3, 3: 4}\n",
    "        df['Days_Since_Weekend'] = df['Day_of_Week'].map(day_map)\n",
    "        \n",
    "        # Weekend gap risk (Thursday close to Sunday open gap potential)\n",
    "        # Higher values indicate longer time since last trade (more news accumulation)\n",
    "        df['Weekend_Risk_Score'] = df['Days_Since_Weekend'] / 4  # Normalized 0-1\n",
    "        \n",
    "        print(\"\\nCreated NEPSE trading calendar features:\")\n",
    "        print(\"  - Is_Sunday: First trading day (gap risk)\")\n",
    "        print(\"  - Is_Thursday: Last trading day (position squaring)\")\n",
    "        print(\"  - Days_Since_Weekend: Trading day counter (0-4)\")\n",
    "        print(\"  - Weekend_Risk_Score: News accumulation proxy\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_nepali_fiscal_calendar(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Nepali fiscal year features.\n",
    "        Nepal's fiscal year: Shrawan (mid-July) to Ashad (mid-July)\n",
    "        Critical for tax and reporting seasonality.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Fiscal Year determination\n",
    "        # FY starts mid-July (approx July 16)\n",
    "        month_day = df['Date'].dt.month * 100 + df['Date'].dt.day\n",
    "        \n",
    "        # If before July 16, we're in previous fiscal year\n",
    "        # Example: Jan 2024 (1/15 = 115) < 716, so FY 2023/24 (FY 2080 in Nepali calendar)\n",
    "        df['Fiscal_Year'] = df['Date'].dt.year\n",
    "        df.loc[month_day < 716, 'Fiscal_Year'] -= 1\n",
    "        \n",
    "        # Fiscal Quarter (Nepal specific)\n",
    "        # Q1: Shrawan-Bhadra-Ashwin (Jul, Aug, Sep) - Monsoon, slow start\n",
    "        # Q2: Kartik-Mangsir-Poush (Oct, Nov, Dec) - Post-Dashain activity\n",
    "        # Q3: Magh-Falgun-Chaitra (Jan, Feb, Mar) - Winter, steady trading\n",
    "        # Q4: Baisakh-Jestha-Ashad (Apr, May, Jun) - Year-end rally/tax selling\n",
    "        df['Fiscal_Quarter'] = df['Month'].apply(\n",
    "            lambda m: 1 if m in [7, 8, 9] else\n",
    "                      2 if m in [10, 11, 12] else\n",
    "                      3 if m in [1, 2, 3] else 4\n",
    "        )\n",
    "        \n",
    "        # Fiscal month (1-12 starting from Shrawan/July)\n",
    "        # July=1, August=2, ..., June=12\n",
    "        fiscal_month_map = {7: 1, 8: 2, 9: 3, 10: 4, 11: 5, 12: 6,\n",
    "                           1: 7, 2: 8, 3: 9, 4: 10, 5: 11, 6: 12}\n",
    "        df['Fiscal_Month'] = df['Month'].map(fiscal_month_map)\n",
    "        \n",
    "        # Fiscal year-end proximity (days until mid-July)\n",
    "        # Critical for tax-loss harvesting and window dressing\n",
    "        year_end_date = pd.to_datetime(df['Fiscal_Year'].astype(str) + '-07-15')\n",
    "        # For dates after July 15, use next year\n",
    "        mask = df['Date'].dt.month > 7\n",
    "        year_end_date[mask] = pd.to_datetime((df['Fiscal_Year'][mask] + 1).astype(str) + '-07-15')\n",
    "        \n",
    "        df['Days_to_FY_End'] = (year_end_date - df['Date']).dt.days\n",
    "        \n",
    "        # Is year-end quarter (Q4: Apr-Jul)\n",
    "        df['Is_FY_End_Quarter'] = (df['Fiscal_Quarter'] == 4).astype(int)\n",
    "        \n",
    "        # Is fiscal year-end month (Ashad/June)\n",
    "        df['Is_FY_End_Month'] = (df['Month'] == 6).astype(int)\n",
    "        \n",
    "        print(\"\\nCreated Nepali fiscal calendar features:\")\n",
    "        print(f\"  - Fiscal Year range: {df['Fiscal_Year'].min()} to {df['Fiscal_Year'].max()}\")\n",
    "        print(f\"  - Fiscal_Quarter: Q1=Jul-Sep, Q2=Oct-Dec, Q3=Jan-Mar, Q4=Apr-Jul\")\n",
    "        print(f\"  - Days_to_FY_End: {df['Days_to_FY_End'].min()} to {df['Days_to_FY_End'].max()}\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_seasonal_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Seasonal and cyclical encoding.\n",
    "        Converts linear time into cyclical patterns.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Cyclical encoding of month (preserves circularity: Dec close to Jan)\n",
    "        df['Month_Sin'] = np.sin(2 * np.pi * df['Month'] / 12)\n",
    "        df['Month_Cos'] = np.cos(2 * np.pi * df['Month'] / 12)\n",
    "        \n",
    "        # Cyclical encoding of day of week\n",
    "        df['DOW_Sin'] = np.sin(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "        df['DOW_Cos'] = np.cos(2 * np.pi * df['Day_of_Week'] / 7)\n",
    "        \n",
    "        # Cyclical encoding of fiscal month\n",
    "        if 'Fiscal_Month' in df.columns:\n",
    "            df['Fiscal_Month_Sin'] = np.sin(2 * np.pi * df['Fiscal_Month'] / 12)\n",
    "            df['Fiscal_Month_Cos'] = np.cos(2 * np.pi * df['Fiscal_Month'] / 12)\n",
    "        \n",
    "        # Month-end effects (common in all markets)\n",
    "        df['Is_Month_Start'] = (df['Date'].dt.is_month_start).astype(int)\n",
    "        df['Is_Month_End'] = (df['Date'].dt.is_month_end).astype(int)\n",
    "        df['Days_to_Month_End'] = (df['Date'] + pd.offsets.MonthEnd(0) - df['Date']).dt.days\n",
    "        \n",
    "        # Festival seasons (approximate dates for NEPSE)\n",
    "        # Dashain (October), Tihar (November) - markets typically thin\n",
    "        df['Is_Dashain_Season'] = (df['Month'] == 10).astype(int)\n",
    "        df['Is_Tihar_Season'] = (df['Month'] == 11).astype(int)\n",
    "        \n",
    "        # Monsoon season (Jun-Sep) - affects hydropower stocks heavily\n",
    "        df['Is_Monsoon'] = df['Month'].isin([6, 7, 8, 9]).astype(int)\n",
    "        \n",
    "        print(\"\\nCreated seasonal features:\")\n",
    "        print(\"  - Cyclical encoding (sin/cos) for Month, Day of Week\")\n",
    "        print(\"  - Month start/end indicators\")\n",
    "        print(\"  - Festival season flags (Dashain, Tihar)\")\n",
    "        print(\"  - Monsoon season flag (hydropower sensitivity)\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data with dates spanning multiple years\n",
    "    np.random.seed(42)\n",
    "    dates = pd.date_range(start='2022-01-01', end='2023-12-31', freq='B')\n",
    "    # Filter for Sunday-Thursday (simplified)\n",
    "    \n",
    "    sample_df = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Close': np.random.uniform(1800, 2200, len(dates))\n",
    "    })\n",
    "    \n",
    "    time_engineer = NEPSETimeFeatures(sample_df)\n",
    "    \n",
    "    time_engineer.create_basic_calendar_features()\n",
    "    time_engineer.create_nepse_trading_calendar()\n",
    "    time_engineer.create_nepali_fiscal_calendar()\n",
    "    time_engineer.create_seasonal_features()\n",
    "    \n",
    "    # Display time features\n",
    "    print(\"\\nTime-Based Features Sample:\")\n",
    "    display_cols = ['Date', 'Day_of_Week', 'Is_Sunday', 'Is_Thursday', \n",
    "                   'Fiscal_Quarter', 'Days_to_FY_End', 'Month_Sin', 'Month_Cos']\n",
    "    print(time_engineer.df[display_cols].head(20))\n",
    "    \n",
    "    # Show fiscal year transition\n",
    "    print(\"\\nFiscal Year Transition (June-July):\")\n",
    "    jun_jul_mask = (time_engineer.df['Month'] == 6) | (time_engineer.df['Month'] == 7)\n",
    "    print(time_engineer.df[jun_jul_mask][['Date', 'Fiscal_Year', 'Fiscal_Month', 'Days_to_FY_End']].head(10))\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section implements **time-based features** that encode calendar effects specific to the NEPSE trading environment and Nepali fiscal calendar.\n",
    "\n",
    "**NEPSE Trading Calendar:**\n",
    "The `create_nepse_trading_calendar()` method addresses the unique Sunday-Thursday trading schedule of the Nepal Stock Exchange, which differs from the Monday-Friday schedule of Western markets:\n",
    "\n",
    "- **Is_Sunday**: Identifies the first trading day of the week. In NEPSE, Sunday often exhibits \"weekend gap\" behavior—prices opening significantly higher or lower than Thursday's close due to news accumulation over Friday and Saturday. This creates volatility and mean-reversion opportunities distinct from other trading days.\n",
    "\n",
    "- **Is_Thursday**: The last trading day before the weekend. In NEPSE, Thursdays often see \"position squaring\"—traders closing positions to avoid weekend risk (two days of potential news accumulation without ability to trade). This can create volume spikes and reversal patterns.\n",
    "\n",
    "- **Days_Since_Weekend**: A counter from 0 (Sunday) to 4 (Thursday) that captures the progression of the trading week. Studies of NEPSE show different volatility patterns—typically lower on Sunday (wait-and-see approach), building through Tuesday-Wednesday, and elevated on Thursday (position adjustments).\n",
    "\n",
    "**Nepali Fiscal Calendar:**\n",
    "The `create_nepali_fiscal_calendar()` method implements Nepal's unique fiscal year (Shrawan to Ashad, mid-July to mid-July), which drives distinct seasonal patterns in the stock market:\n",
    "\n",
    "- **Fiscal_Year**: Determined by checking if the date is before or after July 16. If January 15, 2024 (115 in MMDD format) is before July 16 (716), it belongs to Fiscal Year 2023 (which runs July 2023-July 2024). This is critical because institutional investors report performance by fiscal year.\n",
    "\n",
    "- **Fiscal_Quarter**: Mapped to Nepali months—Q1 (July-September) is the post-budget period when new government spending plans affect infrastructure and bank stocks. Q4 (April-July) is the fiscal year-end when tax-loss harvesting and \"window dressing\" (buying winners to show in portfolio reports) create distinct market dynamics.\n",
    "\n",
    "- **Days_to_FY_End**: Counts down to mid-July. As this approaches 0 (June), NEPSE typically sees increased volatility as investors realize tax losses (selling losers) and rebalance portfolios. Stocks down significantly for the fiscal year often face additional selling pressure in Ashad (June).\n",
    "\n",
    "**Cyclical Encoding:**\n",
    "The `create_seasonal_features()` method addresses a mathematical issue with raw month/day numbers: December (12) and January (1) are numerically far apart (difference of 11), but cyclically adjacent. The sine/cosine transformation maps months onto a circle:\n",
    "\n",
    "- **Month_Sin/Month_Cos**: Convert month 1-12 into coordinates on a unit circle. January (month 1) maps to specific (sin, cos) values, as does December (month 12), and these points are close together on the circle, correctly representing that December flows into January. This allows machine learning models to understand seasonal continuity.\n",
    "\n",
    "**Festival and Seasonal Effects:**\n",
    "- **Is_Dashain_Season/October**: Dashain (Nepal's biggest festival) typically occurs in October. During this period, NEPSE trading volume drops significantly as investors focus on celebrations rather than markets, creating liquidity crunches and erratic price movements.\n",
    "\n",
    "- **Is_Monsoon**: June-September marks the monsoon season in Nepal. This is particularly relevant for NEPSE because the index is heavily weighted toward hydropower companies (which generate more electricity during monsoon) and agricultural stocks. The monsoon feature captures sectoral seasonality that affects the broad market.\n",
    "\n",
    "These time-based features allow the NEPSE prediction model to account for structural calendar effects that pure price-based models would miss—from the weekly rhythm of Sunday-Thursday trading to the annual cycle of fiscal year-end portfolio adjustments unique to Nepal's tax and reporting calendar.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.8 Interaction Features**\n",
    "\n",
    "Interaction features capture the combined effect of two or more variables that is greater than the sum of their individual effects. In financial markets, the relationship between price and volume, or between volatility and trend, often provides more predictive power than either variable alone. These features model the \"chemistry\" between different market dimensions.\n",
    "\n",
    "For the NEPSE prediction system, interaction features are particularly valuable because they can identify regime-specific patterns—for example, a price increase on high volume has different implications than the same price increase on low volume in Nepal's relatively illiquid market structure.\n",
    "\n",
    "```python\n",
    "class NEPSEInteractionFeatures:\n",
    "    \"\"\"\n",
    "    Interaction features combining multiple variables.\n",
    "    Capture synergistic effects between price, volume, and volatility.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def create_price_volume_interactions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Interactions between price movements and volume activity.\n",
    "        Volume validates (or invalidates) price moves.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Ensure required columns exist\n",
    "        if 'Daily_Return' not in df.columns:\n",
    "            df['Daily_Return'] = df['Close'].pct_change()\n",
    "        \n",
    "        if 'Rel_Volume_10' not in df.columns:\n",
    "            avg_vol = df['Vol'].rolling(10).mean()\n",
    "            df['Rel_Volume_10'] = df['Vol'] / avg_vol\n",
    "        \n",
    "        # Volume-Confirmed Return (return weighted by relative volume)\n",
    "        # High values = strong move with high conviction\n",
    "        df['Volume_Confirmed_Return'] = df['Daily_Return'] * df['Rel_Volume_10']\n",
    "        \n",
    "        # Price change efficiency (return per unit of volume)\n",
    "        # High = efficient move (large price change on modest volume)\n",
    "        # Low = inefficient move (large volume for small price change)\n",
    "        df['Price_Efficiency'] = df['Daily_Return'] / (df['Rel_Volume_10'] + 0.001)\n",
    "        \n",
    "        # Volume-Price Trend (cumulative confirmation)\n",
    "        # Positive = rising prices on rising volume (strong uptrend)\n",
    "        # Negative = falling prices on rising volume (strong downtrend)\n",
    "        df['Volume_Price_Trend'] = np.sign(df['Daily_Return']) * np.log(df['Rel_Volume_10'] + 1)\n",
    "        \n",
    "        # VWAP deviation interaction (price vs average price, weighted by volume)\n",
    "        if 'VWAP_Distance_Pct' in df.columns:\n",
    "            df['VWAP_Volume_Interaction'] = df['VWAP_Distance_Pct'] * df['Rel_Volume_10']\n",
    "        \n",
    "        # Climactic volume indicator (extreme volume + extreme price)\n",
    "        # Often marks turning points\n",
    "        extreme_price = df['Daily_Return'].abs() > df['Daily_Return'].rolling(20).std() * 2\n",
    "        extreme_vol = df['Rel_Volume_10'] > 2.0\n",
    "        df['Climactic_Move'] = (extreme_price & extreme_vol).astype(int)\n",
    "        \n",
    "        print(\"Created price-volume interactions:\")\n",
    "        print(\"  - Volume_Confirmed_Return: Return weighted by volume\")\n",
    "        print(\"  - Price_Efficiency: Return per unit volume\")\n",
    "        print(\"  - Volume_Price_Trend: Signed volume confirmation\")\n",
    "        print(\"  - Climactic_Move: Extreme volume + price (reversal signal)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_volatility_trend_interactions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Interactions between volatility and trend direction.\n",
    "        Different volatility regimes favor different strategies.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Trend strength (distance from moving average)\n",
    "        if 'Dist_SMA_20_Pct' in df.columns:\n",
    "            trend_strength = df['Dist_SMA_20_Pct']\n",
    "        else:\n",
    "            sma = df['Close'].rolling(20).mean()\n",
    "            trend_strength = ((df['Close'] - sma) / sma) * 100\n",
    "        \n",
    "        # Volatility regime\n",
    "        if 'Volatility_20' not in df.columns:\n",
    "            df['Volatility_20'] = df['Daily_Return'].rolling(20).std()\n",
    "        \n",
    "        vol_percentile = df['Volatility_20'].rolling(252).apply(\n",
    "            lambda x: pd.Series(x).rank(pct=True).iloc[-1], raw=True\n",
    "        )\n",
    "        \n",
    "        # Volatility-Adjusted Trend (trend strength relative to noise)\n",
    "        # High values = strong trend, low noise (good for trend following)\n",
    "        df['Trend_SNR'] = trend_strength / (df['Volatility_20'] + 0.0001)  # Signal-to-noise ratio\n",
    "        \n",
    "        # Volatility Regime Trend\n",
    "        # How does trend behave in high vs low volatility?\n",
    "        df['High_Vol_Trend'] = trend_strength * (vol_percentile > 0.75).astype(int)\n",
    "        df['Low_Vol_Trend'] = trend_strength * (vol_percentile < 0.25).astype(int)\n",
    "        \n",
    "        # Expansion/Contraction interaction\n",
    "        # Price trend during volatility expansion vs contraction\n",
    "        vol_expanding = df['Volatility_20'] > df['Volatility_20'].shift(5)\n",
    "        df['Expansion_Trend'] = trend_strength * vol_expanding.astype(int)\n",
    "        df['Contraction_Trend'] = trend_strength * (~vol_expanding).astype(int)\n",
    "        \n",
    "        print(\"\\nCreated volatility-trend interactions:\")\n",
    "        print(\"  - Trend_SNR: Trend strength relative to volatility\")\n",
    "        print(\"  - High/Low_Vol_Trend: Trend in different volatility regimes\")\n",
    "        print(\"  - Expansion/Contraction_Trend: Trend during volatility changes\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_range_volume_interactions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Interactions between daily range (volatility) and volume.\n",
    "        Identifies breakouts vs false moves.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Range expansion on volume (true breakout)\n",
    "        daily_range = (df['High'] - df['Low']) / df['Close']\n",
    "        df['Range_Volume_Breakout'] = daily_range * df['Rel_Volume_10']\n",
    "        \n",
    "        # Narrow range on low volume (consolidation before expansion)\n",
    "        narrow_range = daily_range < daily_range.rolling(20).quantile(0.2)\n",
    "        low_volume = df['Rel_Volume_10'] < 0.8\n",
    "        df['Consolidation_Signal'] = (narrow_range & low_volume).astype(int)\n",
    "        \n",
    "        # Wide range on low volume (potential trap)\n",
    "        wide_range = daily_range > daily_range.rolling(20).quantile(0.8)\n",
    "        df['Trap_Signal'] = (wide_range & low_volume).astype(int)\n",
    "        \n",
    "        print(\"\\nCreated range-volume interactions:\")\n",
    "        print(\"  - Range_Volume_Breakout: Wide range on high volume\")\n",
    "        print(\"  - Consolidation_Signal: Tight range on low volume\")\n",
    "        print(\"  - Trap_Signal: Wide range on low volume (false breakout)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_multi_lag_interactions(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Interactions between different time lags.\n",
    "        Captures acceleration and momentum shifts.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Ensure lag features exist\n",
    "        if 'Close_Lag_1' not in df.columns:\n",
    "            for lag in [1, 5, 20]:\n",
    "                df[f'Close_Lag_{lag}'] = df['Close'].shift(lag)\n",
    "        \n",
    "        # Short-term vs Long-term momentum interaction\n",
    "        short_term = (df['Close'] - df['Close_Lag_1']) / df['Close_Lag_1']\n",
    "        long_term = (df['Close'] - df['Close_Lag_20']) / df['Close_Lag_20']\n",
    "        \n",
    "        # Momentum alignment (both positive or both negative = strong trend)\n",
    "        df['Momentum_Alignment'] = short_term * long_term\n",
    "        \n",
    "        # Momentum divergence (short term opposite to long term = potential reversal)\n",
    "        df['Momentum_Divergence'] = abs(short_term - long_term)\n",
    "        \n",
    "        # Acceleration (change in short-term momentum)\n",
    "        df['Momentum_Acceleration'] = short_term - (df['Close_Lag_1'] - df['Close_Lag_5']) / df['Close_Lag_5']\n",
    "        \n",
    "        print(\"\\nCreated multi-lag interactions:\")\n",
    "        print(\"  - Momentum_Alignment: Short-term × Long-term (trend strength)\")\n",
    "        print(\"  - Momentum_Divergence: Difference between timeframes\")\n",
    "        print(\"  - Momentum_Acceleration: Change in momentum rate\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup with previous features\n",
    "    interaction_engineer = NEPSEInteractionFeatures(expanding_engineer.df)\n",
    "    \n",
    "    interaction_engineer.create_price_volume_interactions()\n",
    "    interaction_engineer.create_volatility_trend_interactions()\n",
    "    interaction_engineer.create_range_volume_interactions()\n",
    "    interaction_engineer.create_multi_lag_interactions()\n",
    "    \n",
    "    # Display interaction features\n",
    "    print(\"\\nInteraction Features Sample:\")\n",
    "    display_cols = ['Close', 'Daily_Return', 'Rel_Volume_10', 'Volume_Confirmed_Return',\n",
    "                   'Trend_SNR', 'Climactic_Move', 'Momentum_Alignment']\n",
    "    print(interaction_engineer.df[display_cols].tail(10))\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section implements **interaction features** that combine multiple variables to capture synergistic effects not visible when examining features in isolation.\n",
    "\n",
    "**Price-Volume Interactions:**\n",
    "The `create_price_volume_interactions()` method addresses a fundamental principle of technical analysis: volume validates price. A price movement on high volume indicates broad participation and conviction, while the same movement on low volume suggests lack of participation and potential reversal:\n",
    "\n",
    "- **Volume_Confirmed_Return**: Multiplies the daily return by relative volume. A +2% return on 3x average volume scores +6, while the same +2% on 0.5x volume scores only +1. This amplifies signals that have institutional backing and diminishes noise from thin trading.\n",
    "\n",
    "- **Price_Efficiency**: Measures return generated per unit of volume. High efficiency (large price move on modest volume) suggests strong supply/demand imbalance and potential continuation. Low efficiency (large volume for small price move) suggests equilibrium and potential reversal. In NEPSE's illiquid market, efficiency spikes often precede trend changes.\n",
    "\n",
    "- **Climactic_Move**: A binary flag identifying extreme price moves (>2 standard deviations) occurring on extreme volume (>2x average). In NEPSE, these \"blow-off\" moves often mark trend exhaustion—parabolic rallies ending in volume spikes as the last buyers enter, or panic selling capitulations marking bottoms.\n",
    "\n",
    "**Volatility-Trend Interactions:**\n",
    "The `create_volatility_trend_interactions()` method combines trend strength with market noise levels:\n",
    "\n",
    "- **Trend_SNR (Signal-to-Noise Ratio)**: Divides trend strength (distance from 20-day MA) by volatility. High SNR indicates a strong, clean trend suitable for trend-following strategies. Low SNR indicates choppy, directionless markets where mean-reversion strategies work better. For NEPSE, SNR > 2.0 indicates strong trending conditions; < 0.5 indicates range-bound markets.\n",
    "\n",
    "- **High_Vol_Trend**: Isolates trend strength during high volatility regimes (top quartile). In NEPSE, trends during high volatility are often unsustainable and prone to sharp reversals, unlike low-volatility trends which tend to persist.\n",
    "\n",
    "**Range-Volume Interactions:**\n",
    "The `create_range_volume_interactions()` method distinguishes between genuine breakouts and false moves:\n",
    "\n",
    "- **Range_Volume_Breakout**: Multiplies daily range by relative volume. Wide ranges on high volume indicate genuine breakouts as consensus forms around new price levels. This is a high-conviction entry signal for NEPSE momentum strategies.\n",
    "\n",
    "- **Consolidation_Signal**: Identifies tight ranges (bottom 20% of 20-day range) on low volume (<0.8x average). This \"coiled spring\" pattern often precedes explosive moves in NEPSE stocks as pent-up supply/demand imbalances resolve.\n",
    "\n",
    "- **Trap_Signal**: Wide ranges on low volume suggest \"bull traps\" or \"bear traps\"—false breakouts that suck in traders before reversing. In low-liquidity NEPSE stocks, a few large orders can create wide ranges without broad participation, creating these trap patterns.\n",
    "\n",
    "**Multi-Lag Interactions:**\n",
    "The `create_multi_lag_interactions()` method combines different time horizons:\n",
    "\n",
    "- **Momentum_Alignment**: Multiplies short-term (1-day) momentum by long-term (20-day) momentum. Positive values indicate both short and long-term trends agree (strong directional move). Negative values indicate conflict (short-term counter-trend move within longer trend—potential pullback or reversal).\n",
    "\n",
    "- **Momentum_Divergence**: The absolute difference between short and long-term momentum. High divergence warns of potential trend changes as near-term action conflicts with established direction.\n",
    "\n",
    "These interaction features allow the NEPSE prediction model to understand context—a +2% move means different things depending on volume, volatility regime, and alignment with longer-term trends. This multi-dimensional view is essential for accurate prediction in complex, adaptive markets like NEPSE.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.9 Transformation Features**\n",
    "\n",
    "Transformation features apply mathematical functions to raw or engineered features to improve their statistical properties for machine learning algorithms. Financial data often exhibits skewed distributions, heteroscedasticity (changing variance), and non-linear relationships that can impair model performance. Transformations stabilize variance, reduce skewness, and linearize relationships.\n",
    "\n",
    "For the NEPSE prediction system, transformations are crucial because financial time-series typically have fat-tailed (leptokurtic) distributions with extreme outliers during market crashes or rallies. Transformations like log, square root, and power transforms make these distributions more normal (Gaussian), improving the performance of algorithms that assume normality, such as linear regression, neural networks, and SVMs.\n",
    "\n",
    "```python\n",
    "class NEPSETransformationFeatures:\n",
    "    \"\"\"\n",
    "    Mathematical transformation features for NEPSE data.\n",
    "    Improve statistical properties (normality, stationarity) for ML models.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, df: pd.DataFrame):\n",
    "        self.df = df.copy()\n",
    "        \n",
    "    def create_log_transforms(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Logarithmic transformations.\n",
    "        Compresses scale, stabilizes variance, linearizes exponential growth.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Log of price (turns multiplicative relationships into additive)\n",
    "        # Log(P_t) - Log(P_{t-1}) = Log(P_t / P_{t-1}) = log return\n",
    "        df['Log_Close'] = np.log(df['Close'])\n",
    "        df['Log_Volume'] = np.log(df['Vol'] + 1)  # +1 to handle zero volume\n",
    "        df['Log_Turnover'] = np.log(df['Turnover'] + 1)\n",
    "        \n",
    "        # Log of ranges (stabilizes volatility measures)\n",
    "        df['Log_Range'] = np.log((df['High'] - df['Low']) + 0.001)\n",
    "        \n",
    "        # Log differences (alternative return calculation)\n",
    "        df['Log_Diff_1'] = df['Log_Close'].diff(1)\n",
    "        df['Log_Diff_5'] = df['Log_Close'].diff(5)\n",
    "        \n",
    "        print(\"Created log transformation features:\")\n",
    "        print(\"  - Log_Close: Natural log of price\")\n",
    "        print(\"  - Log_Volume: Log of volume (handles skewness)\")\n",
    "        print(\"  - Log_Diff: Log differences (returns)\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_power_transforms(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Power transformations (Box-Cox, Yeo-Johnson approximations).\n",
    "        Reduces skewness in volume and volatility data.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Square root transformation (moderate skew reduction)\n",
    "        # Good for volume data which is often Poisson-like\n",
    "        df['Sqrt_Volume'] = np.sqrt(df['Vol'])\n",
    "        df['Sqrt_Turnover'] = np.sqrt(df['Turnover'])\n",
    "        \n",
    "        # Cube root (stronger than sqrt, weaker than log)\n",
    "        df['Cbrt_Volume'] = np.cbrt(df['Vol'])\n",
    "        \n",
    "        # Power transformation for returns (reduce kurtosis/fat tails)\n",
    "        # Sign(x) * |x|^0.5 compresses extreme values while preserving sign\n",
    "        if 'Daily_Return' in df.columns:\n",
    "            df['Signed_Sqrt_Return'] = np.sign(df['Daily_Return']) * np.sqrt(np.abs(df['Daily_Return']))\n",
    "            \n",
    "            # Tanh compression (sigmoid-like, strongly compresses outliers)\n",
    "            df['Tanh_Return'] = np.tanh(df['Daily_Return'] * 10)  # Scale factor 10 for daily returns\n",
    "        \n",
    "        print(\"\\nCreated power transformation features:\")\n",
    "        print(\"  - Sqrt_Volume: Square root of volume\")\n",
    "        print(\"  - Signed_Sqrt_Return: Signed square root of returns\")\n",
    "        print(\"  - Tanh_Return: Hyperbolic tangent compression\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_rank_and_quantile_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Rank and quantile transformations (robust to outliers).\n",
    "        Converts values to percentiles within recent history.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Percentile rank (0-1 scale) - robust to outliers\n",
    "        for window in [20, 60]:\n",
    "            # Price position in recent range\n",
    "            df[f'Price_Rank_{window}'] = df['Close'].rolling(window).apply(\n",
    "                lambda x: pd.Series(x).rank(pct=True).iloc[-1], raw=True\n",
    "            )\n",
    "            \n",
    "            # Volume rank (is today's volume high or low historically?)\n",
    "            df[f'Volume_Rank_{window}'] = df['Vol'].rolling(window).apply(\n",
    "                lambda x: pd.Series(x).rank(pct=True).iloc[-1], raw=True\n",
    "            )\n",
    "            \n",
    "            # Return rank (how extreme is today's move?)\n",
    "            if 'Daily_Return' in df.columns:\n",
    "                df[f'Return_Rank_{window}'] = df['Daily_Return'].rolling(window).apply(\n",
    "                    lambda x: pd.Series(x).rank(pct=True).iloc[-1], raw=True\n",
    "                )\n",
    "        \n",
    "        # Quantile bins (discretization)\n",
    "        # Reduces noise, captures non-linear relationships\n",
    "        df['Volume_Quintile'] = pd.qcut(df['Vol'], q=5, labels=['Very_Low', 'Low', 'Medium', 'High', 'Very_High'])\n",
    "        df['Return_Decile'] = pd.qcut(df['Daily_Return'].rank(method='first'), q=10, labels=False)\n",
    "        \n",
    "        print(\"\\nCreated rank/quantile features:\")\n",
    "        print(\"  - Price_Rank: Percentile position in recent range\")\n",
    "        print(\"  - Volume_Rank: Volume percentile (0=lowest, 1=highest)\")\n",
    "        print(\"  - Volume_Quintile: Discrete volume categories\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_differencing_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Differencing transformations for stationarity.\n",
    "        Removes trends and seasonal components.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # First-order differencing (removes linear trends)\n",
    "        df['Close_Diff_1'] = df['Close'].diff(1)\n",
    "        df['Volume_Diff_1'] = df['Vol'].diff(1)\n",
    "        \n",
    "        # Second-order differencing (removes curvature)\n",
    "        df['Close_Diff_2'] = df['Close'].diff(2)\n",
    "        \n",
    "        # Seasonal differencing (for fiscal year effects in NEPSE)\n",
    "        # Difference from same time last year (approx 252 trading days)\n",
    "        df['Close_Diff_252'] = df['Close'].diff(252)\n",
    "        \n",
    "        # Relative differencing (percentage change from n periods ago)\n",
    "        for period in [5, 20, 252]:\n",
    "            df[f'Close_Pct_Change_{period}'] = (df['Close'] - df['Close'].shift(period)) / df['Close'].shift(period)\n",
    "        \n",
    "        print(\"\\nCreated differencing features:\")\n",
    "        print(\"  - Close_Diff_1: First difference (daily change)\")\n",
    "        print(\"  - Close_Diff_2: Second difference (acceleration)\")\n",
    "        print(\"  - Close_Diff_252: Year-over-year difference\")\n",
    "        print(\"  - Pct_Change: Relative changes over 5, 20, 252 days\")\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def create_normalization_features(self) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Normalization and scaling transformations.\n",
    "        Standardize features for algorithms sensitive to scale.\n",
    "        \"\"\"\n",
    "        df = self.df\n",
    "        \n",
    "        # Z-score normalization (mean=0, std=1)\n",
    "        for window in [20, 60]:\n",
    "            # Rolling z-score (how many std devs from recent mean?)\n",
    "            rolling_mean = df['Close'].rolling(window).mean()\n",
    "            rolling_std = df['Close'].rolling(window).std()\n",
    "            df[f'Close_ZScore_{window}'] = (df['Close'] - rolling_mean) / rolling_std\n",
    "            \n",
    "            if 'Daily_Return' in df.columns:\n",
    "                ret_mean = df['Daily_Return'].rolling(window).mean()\n",
    "                ret_std = df['Daily_Return'].rolling(window).std()\n",
    "                df[f'Return_ZScore_{window}'] = (df['Daily_Return'] - ret_mean) / ret_std\n",
    "        \n",
    "        # Min-Max scaling (0-1 range)\n",
    "        for window in [20]:\n",
    "            rolling_min = df['Close'].rolling(window).min()\n",
    "            rolling_max = df['Close'].rolling(window).max()\n",
    "            df[f'Close_MinMax_{window}'] = (df['Close'] - rolling_min) / (rolling_max - rolling_min)\n",
    "        \n",
    "        # Robust scaling (using median and IQR, resistant to outliers)\n",
    "        for window in [20]:\n",
    "            rolling_median = df['Close'].rolling(window).median()\n",
    "            rolling_iqr = df['Close'].rolling(window).quantile(0.75) - df['Close'].rolling(window).quantile(0.25)\n",
    "            df[f'Close_Robust_{window}'] = (df['Close'] - rolling_median) / rolling_iqr\n",
    "        \n",
    "        print(\"\\nCreated normalization features:\")\n",
    "        print(\"  - Close_ZScore: Standard score (rolling)\")\n",
    "        print(\"  - Close_MinMax: Min-max scaling (0-1)\")\n",
    "        print(\"  - Close_Robust: Robust scaling (median/IQR)\")\n",
    "        \n",
    "        return df\n",
    "\n",
    "# Demonstration\n",
    "if __name__ == \"__main__\":\n",
    "    # Setup\n",
    "    transform_engineer = NEPSETransformationFeatures(interaction_engineer.df)\n",
    "    \n",
    "    transform_engineer.create_log_transforms()\n",
    "    transform_engineer.create_power_transforms()\n",
    "    transform_engineer.create_rank_and_quantile_features()\n",
    "    transform_engineer.create_differencing_features()\n",
    "    transform_engineer.create_normalization_features()\n",
    "    \n",
    "    # Display transformations\n",
    "    print(\"\\nTransformation Features Sample:\")\n",
    "    display_cols = ['Close', 'Log_Close', 'Sqrt_Volume', 'Price_Rank_20', \n",
    "                   'Close_ZScore_20', 'Close_Diff_1', 'Signed_Sqrt_Return']\n",
    "    print(transform_engineer.df[display_cols].tail(10))\n",
    "    \n",
    "    # Show distribution improvement\n",
    "    print(\"\\nDistribution Comparison (Skewness):\")\n",
    "    print(f\"Raw Volume Skew: {transform_engineer.df['Vol'].skew():.2f}\")\n",
    "    print(f\"Log Volume Skew: {transform_engineer.df['Log_Volume'].skew():.2f}\")\n",
    "    print(f\"Sqrt Volume Skew: {transform_engineer.df['Sqrt_Volume'].skew():.2f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section implements **transformation features** that apply mathematical functions to improve the statistical properties of raw financial data for machine learning.\n",
    "\n",
    "**Logarithmic Transformations:**\n",
    "The `create_log_transforms()` method applies the natural logarithm to compress scale and stabilize variance:\n",
    "\n",
    "- **Log_Close**: Converts price levels to log-prices. In finance, log-prices have the desirable property that differences equal log-returns: $Log(P_t) - Log(P_{t-1}) = Log(P_t/P_{t-1})$. This turns the exponential growth of stock prices (which compounds multiplicatively) into linear growth (which compounds additively), making trends easier to model.\n",
    "\n",
    "- **Log_Volume**: Volume data in NEPSE is typically right-skewed (few days with extremely high volume, many with moderate). The log transformation compresses the long right tail, making the distribution more symmetric and Gaussian-like. The `+1` handles potential zero-volume days (though rare in NEPSE index data).\n",
    "\n",
    "**Power Transformations:**\n",
    "The `create_power_transforms()` method uses power functions to reduce skewness and kurtosis (fat tails):\n",
    "\n",
    "- **Signed_Sqrt_Return**: Applies square root to the absolute value of returns while preserving the sign. This compresses extreme returns (e.g., ±5% moves become ±0.22 after square root) while maintaining directional information. This is particularly useful for NEPSE during volatile periods when circuit breakers (4% limits) create artificial ceilings but intraday swings can still be extreme.\n",
    "\n",
    "- **Tanh_Return**: Uses the hyperbolic tangent function to strongly compress outliers into the range (-1, 1). The scaling factor of 10 maps typical daily returns (±0.02) to the linear region of tanh, while extreme crashes (±0.10) are compressed toward ±1 without creating infinite outliers.\n",
    "\n",
    "**Rank and Quantile Features:**\n",
    "The `create_rank_and_quantile_features()` method converts absolute values into relative percentiles:\n",
    "\n",
    "- **Price_Rank_20**: Indicates the current price's percentile position within the last 20 days (0 = lowest, 1 = highest). Unlike raw prices which trend upward over time, rank features are inherently stationary and bounded, making them ideal for models that assume stable input distributions.\n",
    "\n",
    "- **Volume_Quintile**: Discretizes volume into 5 equal-sized buckets (quintiles). This categorical encoding can improve tree-based models by reducing noise and highlighting regime changes—moving from \"Very_Low\" to \"Very_High\" volume is more meaningful than the exact share count difference.\n",
    "\n",
    "**Differencing Features:**\n",
    "The `create_differencing_features()` method implements differencing to achieve stationarity:\n",
    "\n",
    "- **Close_Diff_1**: First-order difference ($P_t - P_{t-1}$), equivalent to the absolute price change. This removes linear trends from price series, converting a trending series into a stationary series of changes suitable for ARMA models.\n",
    "\n",
    "- **Close_Diff_252**: Seasonal differencing with a 252-day lag (approximately one trading year). This removes annual trends and seasonality, comparing today's price to the same date last year. For NEPSE, this captures year-over-year growth while removing fiscal year seasonality.\n",
    "\n",
    "**Normalization Features:**\n",
    "The `create_normalization_features()` method scales features to standard ranges:\n",
    "\n",
    "- **Close_ZScore_20**: Rolling z-score calculated over 20 days: $(Close - MA_{20}) / Std_{20}$. This measures how unusual today's price is relative to the recent month. Z-scores above +2 or below -2 indicate statistically significant deviations (potential mean reversion opportunities).\n",
    "\n",
    "- **Close_Robust_20**: Robust scaling using median and interquartile range (IQR) instead of mean and standard deviation. This is resistant to outliers—if NEPSE has a flash crash day, it doesn't permanently distort the scaling parameters as it would with z-score.\n",
    "\n",
    "These transformations ensure that the NEPSE prediction model receives inputs with desirable statistical properties: symmetric distributions, stable variance, and comparable scales across different features and time periods.\n",
    "\n",
    "---\n",
    "\n",
    "## **11.10 Implementation Patterns**\n",
    "\n",
    "This section consolidates best practices for implementing the basic feature creation pipeline efficiently and robustly. When working with large NEPSE datasets (potentially millions of rows across thousands of stocks), implementation details—vectorization, memory management, and pipeline architecture—significantly impact performance and maintainability.\n",
    "\n",
    "The patterns covered include: **Vectorized Operations** (avoiding loops), **Pipeline Architecture** (scikit-learn compatible), **Memory Efficiency** (categorical dtypes, chunking), **Feature Stores** (saving computed features), and **Validation Frameworks** (ensuring correctness).\n",
    "\n",
    "```python\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, List, Optional\n",
    "import joblib\n",
    "\n",
    "class NEPSEFeaturePipeline(BaseEstimator, TransformerMixin):\n",
    "    \"\"\"\n",
    "    Production-ready feature engineering pipeline for NEPSE.\n",
    "    Implements sklearn-compatible transformer for integration with ML workflows.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 price_lags: List[int] = [1, 5, 20],\n",
    "                 ma_windows: List[int] = [5, 20],\n",
    "                 volatility_windows: List[int] = [20],\n",
    "                 include_time_features: bool = True):\n",
    "        self.price_lags = price_lags\n",
    "        self.ma_windows = ma_windows\n",
    "        self.volatility_windows = volatility_windows\n",
    "        self.include_time_features = include_time_features\n",
    "        self.feature_names_ = None\n",
    "        \n",
    "    def fit(self, X, y=None):\n",
    "        \"\"\"Fit method (required for sklearn compatibility).\"\"\"\n",
    "        return self\n",
    "    \n",
    "    def transform(self, X: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Transform raw NEPSE data into engineered features.\n",
    "        Vectorized implementation for performance.\n",
    "        \"\"\"\n",
    "        df = X.copy()\n",
    "        \n",
    "        # 1. Basic Features (Vectorized)\n",
    "        df['Daily_Return'] = df['Close'].pct_change()\n",
    "        df['Log_Return'] = np.log(df['Close'] / df['Close'].shift(1))\n",
    "        df['High_Low_Pct'] = (df['High'] - df['Low']) / df['Close']\n",
    "        df['Open_Close_Pct'] = (df['Close'] - df['Open']) / df['Open']\n",
    "        \n",
    "        # 2. Lag Features (Vectorized using shift)\n",
    "        for lag in self.price_lags:\n",
    "            df[f'Close_Lag_{lag}'] = df['Close'].shift(lag)\n",
    "            df[f'Return_Lag_{lag}'] = df['Daily_Return'].shift(lag)\n",
    "            df[f'Volume_Lag_{lag}'] = df['Vol'].shift(lag)\n",
    "        \n",
    "        # 3. Rolling Features (Vectorized using rolling)\n",
    "        for window in self.ma_windows:\n",
    "            df[f'SMA_{window}'] = df['Close'].rolling(window=window).mean()\n",
    "            df[f'EMA_{window}'] = df['Close'].ewm(span=window).mean()\n",
    "            df[f'Dist_SMA_{window}'] = (df['Close'] - df[f'SMA_{window}']) / df[f'SMA_{window}']\n",
    "        \n",
    "        for window in self.volatility_windows:\n",
    "            df[f'Volatility_{window}'] = df['Daily_Return'].rolling(window=window).std()\n",
    "            df[f'ATR_{window}'] = (df['High'] - df['Low']).rolling(window=window).mean()\n",
    "        \n",
    "        # 4. Time Features (if enabled)\n",
    "        if self.include_time_features and 'Date' in df.columns:\n",
    "            df['Date'] = pd.to_datetime(df['Date'])\n",
    "            df['Day_of_Week'] = df['Date'].dt.dayofweek\n",
    "            df['Month'] = df['Date'].dt.month\n",
    "            \n",
    "            # NEPSE fiscal year\n",
    "            month_day = df['Date'].dt.month * 100 + df['Date'].dt.day\n",
    "            df['Fiscal_Year'] = df['Date'].dt.year\n",
    "            df.loc[month_day < 716, 'Fiscal_Year'] -= 1\n",
    "        \n",
    "        # 5. Interaction Features (Vectorized)\n",
    "        if 'Rel_Volume' not in df.columns:\n",
    "            df['Rel_Volume'] = df['Vol'] / df['Vol'].rolling(20).mean()\n",
    "        \n",
    "        df['Volume_Confirmed_Return'] = df['Daily_Return'] * df['Rel_Volume']\n",
    "        \n",
    "        # Store feature names\n",
    "        self.feature_names_ = [c for c in df.columns if c not in ['Date', 'Symbol', 'S.No']]\n",
    "        \n",
    "        return df\n",
    "    \n",
    "    def get_feature_names(self):\n",
    "        \"\"\"Return list of created feature names.\"\"\"\n",
    "        return self.feature_names_\n",
    "\n",
    "class NEPSEFeatureStore:\n",
    "    \"\"\"\n",
    "    Feature store for saving and retrieving engineered features.\n",
    "    Prevents recomputation and ensures consistency.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, store_path: str = 'features/'):\n",
    "        self.store_path = store_path\n",
    "        \n",
    "    def save_features(self, df: pd.DataFrame, symbol: str, date: str):\n",
    "        \"\"\"Save engineered features to parquet (efficient storage).\"\"\"\n",
    "        filepath = f\"{self.store_path}/{symbol}_{date}.parquet\"\n",
    "        df.to_parquet(filepath)\n",
    "        print(f\"Saved features to {filepath}\")\n",
    "        \n",
    "    def load_features(self, symbol: str, date: str) -> pd.DataFrame:\n",
    "        \"\"\"Load pre-computed features.\"\"\"\n",
    "        filepath = f\"{self.store_path}/{symbol}_{date}.parquet\"\n",
    "        return pd.read_parquet(filepath)\n",
    "\n",
    "def create_optimization_features(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Memory-optimized feature creation for large NEPSE datasets.\n",
    "    \"\"\"\n",
    "    # 1. Downcast numeric types to reduce memory\n",
    "    float_cols = df.select_dtypes(include=['float64']).columns\n",
    "    int_cols = df.select_dtypes(include=['int64']).columns\n",
    "    \n",
    "    df[float_cols] = df[float_cols].astype('float32')\n",
    "    df[int_cols] = df[int_cols].astype('int32')\n",
    "    \n",
    "    # 2. Use categorical for low-cardinality features\n",
    "    if 'Symbol' in df.columns:\n",
    "        df['Symbol'] = df['Symbol'].astype('category')\n",
    "    \n",
    "    # 3. Vectorized calculation (no loops)\n",
    "    # Calculate all lags at once using list comprehension\n",
    "    lags = [1, 5, 20]\n",
    "    lag_features = pd.concat(\n",
    "        [df['Close'].shift(lag).rename(f'Close_Lag_{lag}') for lag in lags],\n",
    "        axis=1\n",
    "    )\n",
    "    df = pd.concat([df, lag_features], axis=1)\n",
    "    \n",
    "    # 4. Efficient rolling with min_periods to handle start of series\n",
    "    df['SMA_20'] = df['Close'].rolling(window=20, min_periods=1).mean()\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Demonstration of complete pipeline\n",
    "if __name__ == \"__main__\":\n",
    "    # Create sample data\n",
    "    np.random.seed(42)\n",
    "    n = 1000\n",
    "    dates = pd.date_range('2022-01-01', periods=n, freq='B')\n",
    "    \n",
    "    data = pd.DataFrame({\n",
    "        'Date': dates,\n",
    "        'Symbol': ['NEPSE'] * n,\n",
    "        'Open': np.random.uniform(1800, 2000, n),\n",
    "        'High': np.random.uniform(1900, 2100, n),\n",
    "        'Low': np.random.uniform(1700, 1900, n),\n",
    "        'Close': np.random.uniform(1850, 2050, n),\n",
    "        'Vol': np.random.randint(1000000, 5000000, n)\n",
    "    })\n",
    "    \n",
    "    # Method 1: Sklearn Pipeline (Production)\n",
    "    print(\"Method 1: Sklearn Pipeline\")\n",
    "    pipeline = Pipeline([\n",
    "        ('features', NEPSEFeaturePipeline(\n",
    "            price_lags=[1, 5],\n",
    "            ma_windows=[5, 20],\n",
    "            include_time_features=True\n",
    "        ))\n",
    "    ])\n",
    "    \n",
    "    result = pipeline.fit_transform(data)\n",
    "    print(f\"Features created: {len(result.columns)}\")\n",
    "    print(f\"Feature names: {pipeline.named_steps['features'].get_feature_names()[:5]}...\")\n",
    "    \n",
    "    # Method 2: Optimized for Memory\n",
    "    print(\"\\nMethod 2: Memory Optimized\")\n",
    "    optimized = create_optimization_features(data)\n",
    "    print(f\"Memory usage: {optimized.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "    \n",
    "    # Method 3: Feature Store\n",
    "    print(\"\\nMethod 3: Feature Store\")\n",
    "    store = NEPSEFeatureStore()\n",
    "    # store.save_features(result, 'NEPSE', '2023-01-01')\n",
    "    \n",
    "    print(\"\\n✓ Implementation patterns demonstration complete\")\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This final section provides **production implementation patterns** that ensure the feature engineering pipeline is efficient, scalable, and maintainable for real-world NEPSE prediction systems.\n",
    "\n",
    "**Sklearn-Compatible Pipeline:**\n",
    "The `NEPSEFeaturePipeline` class inherits from `BaseEstimator` and `TransformerMixin`, making it compatible with scikit-learn's `Pipeline` and `GridSearchCV`. This integration is crucial for production ML workflows because it allows feature engineering to be treated as a first-class citizen in the modeling process—parameterized, cross-validated, and serialized along with the model.\n",
    "\n",
    "The `transform()` method implements **vectorized operations** throughout. Instead of Python loops (which are slow), it uses pandas' built-in vectorized methods (`shift()`, `rolling()`, `ewm()`) which are implemented in C and optimized for performance. For a dataset of 1 million NEPSE records, vectorized operations complete in seconds while loop-based implementations might take minutes.\n",
    "\n",
    "**Feature Store Pattern:**\n",
    "The `NEPSEFeatureStore` class implements a simple feature store using Apache Parquet format. In production NEPSE systems, feature stores prevent redundant computation—once features are engineered for a specific stock and date range, they are saved to disk and can be retrieved instantly for model retraining or backtesting. Parquet is chosen over CSV because it:\n",
    "- Stores data in columnar format (efficient for feature matrices where we often query specific columns)\n",
    "- Preserves data types (int32, float32, categorical)\n",
    "- Supports compression (reduces storage for large NEPSE historical datasets)\n",
    "\n",
    "**Memory Optimization:**\n",
    "The `create_optimization_features()` function demonstrates techniques for handling large datasets:\n",
    "- **Type Downcasting**: Converts `float64` to `float32` (halving memory usage) and `int64` to `int32`. For NEPSE prices (rarely exceeding NPR 10,000), float32 provides sufficient precision while reducing memory footprint by 50%.\n",
    "- **Categorical Encoding**: Converts `Symbol` (stock ticker) from string to categorical. With hundreds of NEPSE symbols repeated millions of times, this reduces memory from ~50 bytes per string to ~8 bytes per integer reference.\n",
    "- **Concatenation Pattern**: Instead of adding lag features one-by-one in a loop (which fragments DataFrame memory), it creates all lag columns in a list and concatenates them once, reducing memory fragmentation.\n",
    "\n",
    "**Vectorization Strategy:**\n",
    "The code emphasizes **vectorized calculations**—operations performed on entire arrays at once rather than element-by-element. For example:\n",
    "- **Bad (Slow)**: `for i in range(len(df)): df.loc[i, 'Lag_1'] = df.loc[i-1, 'Close']`\n",
    "- **Good (Fast)**: `df['Lag_1'] = df['Close'].shift(1)`\n",
    "\n",
    "The vectorized approach delegates the iteration to pandas' underlying C libraries, achieving 100x+ speedups on large NEPSE datasets spanning decades of daily data across hundreds of stocks.\n",
    "\n",
    "**Pipeline Validation:**\n",
    "The implementation includes `min_periods=1` in rolling calculations to handle the start of the time series gracefully. Without this, the first 19 rows of a 20-day moving average would be NaN, potentially causing model training failures. With `min_periods=1`, the SMA uses available data (even just 1 day) at the beginning of the series, ensuring continuous output.\n",
    "\n",
    "These implementation patterns ensure that the NEPSE feature engineering pipeline can handle production-scale data efficiently while maintaining code clarity and integration with standard ML tooling.\n",
    "\n",
    "---\n",
    "\n",
    "## **Chapter Summary**\n",
    "\n",
    "In this chapter, we implemented the foundational feature creation techniques for the NEPSE stock prediction system, transforming raw OHLCV data into informative predictors through systematic engineering.\n",
    "\n",
    "### **Key Accomplishments:**\n",
    "\n",
    "**1. Raw Value Features (11.1)**\n",
    "We established proper handling of NEPSE CSV data, categorizing features by temporal availability (opening vs. intraday vs. closing) to prevent look-ahead bias. We created basic relationships like `Close_Position` (within daily range) and `VWAP_Distance` (deviation from volume-weighted average) that encode market microstructure.\n",
    "\n",
    "**2. Difference Features (11.2)**\n",
    "We engineered absolute spread features including candlestick shadows (`Upper_Shadow`, `Lower_Shadow`), True Range (gap-adjusted volatility), and overnight gap analysis. These capture intraday dynamics and opening sentiment specific to NEPSE's Sunday-Thursday trading cycle.\n",
    "\n",
    "**3. Percentage Change Features (11.3)**\n",
    "We implemented arithmetic and logarithmic returns, intraday performance metrics (Open-to-Close), and volume-normalized returns. These transformations enable cross-sectional comparison across NEPSE's diverse universe of bank, hydropower, and insurance stocks with different price levels.\n",
    "\n",
    "**4. Lag Features (11.4)**\n",
    "We created autoregressive features using proper `shift()` operations to prevent look-ahead bias, including price lags (1, 3, 5, 20-day), return lags, and volume lags. We validated temporal integrity ensuring `Close_Lag_1` correctly references yesterday's price, forming the backbone of time-series prediction.\n",
    "\n",
    "**5. Rolling Window Features (11.5)**\n",
    "We computed adaptive statistics including Simple and Exponential Moving Averages, Bollinger Bands, Average True Range (ATR), and rolling skewness/kurtosis. These capture local trends and volatility regimes essential for NEPSE's cyclical market behavior.\n",
    "\n",
    "**6. Expanding Window Features (11.6)**\n",
    "We implemented cumulative statistics including all-time highs/lows, drawdown calculations, and cumulative returns since inception. These provide long-term context and historical benchmarks for identifying when NEPSE stocks reach extreme levels.\n",
    "\n",
    "**7. Time-Based Features (11.7)**\n",
    "We encoded Nepal's unique calendar structure: Sunday-Thursday trading week, mid-July fiscal year-end (Shrawan-Ashad), and seasonal effects (monsoon, Dashain festival). Cyclical encodings (sin/cos) preserved the continuity of seasonal patterns.\n",
    "\n",
    "**8. Interaction Features (11.8)**\n",
    "We combined variables multiplicatively to capture synergistic effects: Volume-Confirmed Returns (validating price moves with volume), Trend Signal-to-Noise ratios, and momentum alignment between short and long-term trends.\n",
    "\n",
    "**9. Transformation Features (11.9)**\n",
    "We applied mathematical transforms to improve statistical properties: log transforms for volume (reducing skew), signed square roots for returns (compressing fat tails), and z-score normalizations for stationarity.\n",
    "\n",
    "**10. Implementation Patterns (11.10)**\n",
    "We established production-ready patterns: sklearn-compatible pipelines for ML integration, vectorized operations for performance (100x speedup over loops), memory optimization (float32, categorical dtypes), and feature stores for persistence.\n",
    "\n",
    "### **Practical Skills Acquired:**\n",
    "\n",
    "- **Temporal Safety**: Implementing lag features with `shift()` to strictly prevent look-ahead bias in financial data\n",
    "- **Domain Adaptation**: Engineering features specific to NEPSE's fiscal calendar and Sunday-Thursday trading schedule\n",
    "- **Statistical Rigor**: Creating stationary features (returns, differences) suitable for time-series models\n",
    "- **Production Engineering**: Building memory-efficient, vectorized pipelines capable of processing millions of NEPSE records\n",
    "\n",
    "### **Next Steps:**\n",
    "\n",
    "In **Chapter 12: Advanced Rolling Window Features**, we will explore sophisticated window selection strategies, multiple window harmonics, adaptive windows that adjust to volatility regimes, and efficient computation techniques for handling large-scale NEPSE datasets with high-frequency features.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 11**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
