{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is **Chapter 13: Indicator Engineering for Time-Series Systems** with comprehensive explanations and NEPSE stock prediction examples.\n",
    "\n",
    "---\n",
    "\n",
    "# **Chapter 13: Indicator Engineering for Time-Series Systems**\n",
    "\n",
    "## **13.1 Understanding Domain-Specific Indicators**\n",
    "\n",
    "Indicator engineering is the process of transforming raw time-series data into meaningful signals that capture underlying patterns, momentum, volatility, and market microstructure. In financial time-series prediction, particularly for the Nepal Stock Exchange (NEPSE), indicators serve as the bridge between raw price/volume data and machine learning models.\n",
    "\n",
    "**Why Indicators Matter:**\n",
    "- **Noise Reduction**: Raw price data contains significant noise; indicators smooth this noise while preserving signal\n",
    "- **Feature Normalization**: Indicators like Z-scores and percentiles provide scale-invariant features\n",
    "- **Temporal Patterns**: Moving averages and momentum indicators capture trend direction and strength\n",
    "- **Predictive Power**: Properly engineered indicators often have higher correlation with future returns than raw prices\n",
    "\n",
    "**NEPSE Data Context:**\n",
    "For the NEPSE system using the CSV format: `S.No,Symbol,Conf.,Open,High,Low,Close,LTP,...`, we will engineer indicators using `Close` (closing price), `Open`, `High`, `Low`, `Vol` (volume), and `VWAP` (Volume Weighted Average Price).\n",
    "\n",
    "---\n",
    "\n",
    "## **13.2 Trend-Based Indicators**\n",
    "\n",
    "Trend indicators help identify the direction and strength of price movements over time. They smooth out short-term fluctuations to reveal underlying trends.\n",
    "\n",
    "### **13.2.1 Moving Averages**\n",
    "\n",
    "Moving averages are the foundation of trend analysis. They calculate the average price over a specific window, smoothing out short-term volatility.\n",
    "\n",
    "**Simple Moving Average (SMA):**\n",
    "$$SMA_t = \\frac{P_t + P_{t-1} + ... + P_{t-n+1}}{n}$$\n",
    "\n",
    "Where $P_t$ is the price at time $t$ and $n$ is the window size.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_sma(data, window=14, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Simple Moving Average for NEPSE stock data.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        NEPSE data with columns including 'Close'\n",
    "    window : int\n",
    "        Lookback period for moving average\n",
    "    price_col : str\n",
    "        Column name to calculate SMA on (typically 'Close')\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        SMA values aligned with original index\n",
    "    \"\"\"\n",
    "    # Calculate rolling mean using pandas\n",
    "    # min_periods=1 ensures we get values even for initial rows\n",
    "    sma = data[price_col].rolling(window=window, min_periods=1).mean()\n",
    "    \n",
    "    return sma\n",
    "\n",
    "# Example usage with NEPSE data structure\n",
    "def add_moving_averages_to_nepse(df):\n",
    "    \"\"\"\n",
    "    Add multiple SMA indicators to NEPSE dataframe.\n",
    "    Common periods: 5 (weekly), 10 (bi-weekly), 20 (monthly), 50, 200\n",
    "    \"\"\"\n",
    "    # Create a copy to avoid modifying original\n",
    "    df = df.copy()\n",
    "    \n",
    "    # Calculate SMAs for different time horizons\n",
    "    df['SMA_5'] = calculate_sma(df, window=5)    # Short-term trend\n",
    "    df['SMA_20'] = calculate_sma(df, window=20)  # Monthly trend (approx)\n",
    "    df['SMA_50'] = calculate_sma(df, window=50)  # Medium-term trend\n",
    "    \n",
    "    # Trend direction indicator: Price relative to SMA\n",
    "    df['Price_Above_SMA20'] = (df['Close'] > df['SMA_20']).astype(int)\n",
    "    \n",
    "    # Distance from SMA (percentage)\n",
    "    df['Distance_From_SMA20'] = ((df['Close'] - df['SMA_20']) / df['SMA_20']) * 100\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Detailed explanation of the implementation:\n",
    "# 1. rolling(window=window): Creates a rolling window object that slides over the series\n",
    "# 2. mean(): Calculates the arithmetic mean for each window\n",
    "# 3. min_periods=1: Ensures we calculate means even when we have fewer than 'window' observations\n",
    "# 4. The result is a Series with the same index as the input, but values represent the average\n",
    "#    of the current and previous (window-1) observations\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "The `calculate_sma` function uses pandas' `rolling` method, which creates a sliding window of size `window` over the time-series. The `mean()` method computes the average for each window position. The `min_periods=1` parameter ensures that even for the first few rows where we don't have a full window, we still calculate the average of available data.\n",
    "\n",
    "The `add_moving_averages_to_nepse` function demonstrates practical application by calculating multiple SMAs (5, 20, 50-day) which represent different trend horizons. It also creates derived features like `Price_Above_SMA20` (a binary indicator of trend direction) and `Distance_From_SMA20` (magnitude of deviation from trend).\n",
    "\n",
    "### **13.2.2 Exponential Moving Averages**\n",
    "\n",
    "Exponential Moving Averages (EMA) give more weight to recent prices, making them more responsive to new information than SMAs.\n",
    "\n",
    "**EMA Formula:**\n",
    "$$EMA_t = \\alpha \\times P_t + (1 - \\alpha) \\times EMA_{t-1}$$\n",
    "\n",
    "Where $\\alpha = \\frac{2}{n+1}$ and $n$ is the window period.\n",
    "\n",
    "```python\n",
    "def calculate_ema(data, window=14, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Exponential Moving Average for NEPSE data.\n",
    "    \n",
    "    EMA gives more weight to recent prices compared to SMA.\n",
    "    Formula: EMA_t = (Price_t * k) + (EMA_yesterday * (1-k))\n",
    "    where k = 2/(window+1)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        NEPSE stock data\n",
    "    window : int\n",
    "        EMA period (commonly 12 for fast, 26 for slow in MACD)\n",
    "    price_col : str\n",
    "        Price column to calculate EMA on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        EMA values\n",
    "    \"\"\"\n",
    "    # Calculate smoothing factor\n",
    "    alpha = 2 / (window + 1)\n",
    "    \n",
    "    # Method 1: Using pandas ewm (exponentially weighted moving)\n",
    "    ema = data[price_col].ewm(span=window, adjust=False).mean()\n",
    "    # span=window specifies the decay in terms of span\n",
    "    # adjust=False uses the recursive formula (traditional EMA)\n",
    "    \n",
    "    return ema\n",
    "\n",
    "def calculate_macd(data, fast=12, slow=26, signal=9, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate MACD (Moving Average Convergence Divergence) for NEPSE.\n",
    "    \n",
    "    MACD is a trend-following momentum indicator showing relationship\n",
    "    between two EMAs of a security's price.\n",
    "    \n",
    "    Components:\n",
    "    - MACD Line: EMA(fast) - EMA(slow) [typically 12 and 26]\n",
    "    - Signal Line: EMA of MACD Line [typically 9]\n",
    "    - Histogram: MACD Line - Signal Line\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        NEPSE stock data\n",
    "    fast : int\n",
    "        Fast EMA period (default 12)\n",
    "    slow : int\n",
    "        Slow EMA period (default 26)\n",
    "    signal : int\n",
    "        Signal line EMA period (default 9)\n",
    "    price_col : str\n",
    "        Price column to use\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        DataFrame with MACD, Signal, and Histogram columns\n",
    "    \"\"\"\n",
    "    # Calculate EMAs\n",
    "    ema_fast = calculate_ema(data, window=fast, price_col=price_col)\n",
    "    ema_slow = calculate_ema(data, window=slow, price_col=price_col)\n",
    "    \n",
    "    # Calculate MACD line\n",
    "    macd_line = ema_fast - ema_slow\n",
    "    \n",
    "    # Calculate Signal line (EMA of MACD)\n",
    "    signal_line = macd_line.ewm(span=signal, adjust=False).mean()\n",
    "    \n",
    "    # Calculate Histogram\n",
    "    histogram = macd_line - signal_line\n",
    "    \n",
    "    # Create result DataFrame\n",
    "    result = pd.DataFrame({\n",
    "        'MACD': macd_line,\n",
    "        'Signal': signal_line,\n",
    "        'Histogram': histogram,\n",
    "        'EMA_Fast': ema_fast,\n",
    "        'EMA_Slow': ema_slow\n",
    "    })\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Detailed explanation of MACD interpretation for NEPSE:\n",
    "# 1. Bullish Signal: When MACD line crosses above Signal line (Histogram turns positive)\n",
    "# 2. Bearish Signal: When MACD line crosses below Signal line (Histogram turns negative)\n",
    "# 3. Divergence: When price makes new high but MACD doesn't (bearish divergence)\n",
    "# 4. Zero Line Crossover: MACD crossing above/below 0 indicates trend change\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "The `calculate_ema` function implements the Exponential Moving Average using pandas' `ewm` (exponentially weighted moving) method. The key parameter is `span=window`, which defines the decay in terms of the window size. The formula used is $\\alpha = 2/(span+1)$, which matches the standard EMA calculation where recent prices receive exponentially decreasing weights.\n",
    "\n",
    "The `calculate_macd` function demonstrates a practical application of EMAs in technical analysis. MACD (Moving Average Convergence Divergence) is calculated by subtracting the slow EMA (26-period) from the fast EMA (12-period). This difference (MACD Line) is then smoothed with a 9-period EMA to create the Signal Line. The Histogram represents the difference between MACD and Signal lines, providing a visual representation of momentum shifts. For NEPSE stocks, positive histogram values suggest bullish momentum, while negative values indicate bearish momentum.\n",
    "\n",
    "### **13.2.3 Trend Strength Indicators**\n",
    "\n",
    "Trend strength indicators measure how strong a trend is, helping distinguish between strong trending markets and weak, choppy markets.\n",
    "\n",
    "**Average Directional Index (ADX):**\n",
    "ADX measures trend strength on a scale of 0-100, regardless of direction. It's derived from the Directional Movement Index (DMI).\n",
    "\n",
    "```python\n",
    "def calculate_adx(data, window=14, high_col='High', low_col='Low', close_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Average Directional Index (ADX) for NEPSE trend strength analysis.\n",
    "    \n",
    "    ADX measures trend strength (0-100):\n",
    "    - 0-25: Weak or no trend\n",
    "    - 25-50: Strong trend\n",
    "    - 50-75: Very strong trend\n",
    "    - 75-100: Extremely strong trend\n",
    "    \n",
    "    ADX is non-directional (doesn't indicate trend direction, only strength).\n",
    "    Direction is determined by +DI and -DI lines.\n",
    "    \n",
    "    Calculation Steps:\n",
    "    1. Calculate True Range (TR)\n",
    "    2. Calculate +DM and -DM (Directional Movement)\n",
    "    3. Smooth TR, +DM, -DM using Wilder's smoothing\n",
    "    4. Calculate +DI and -DI\n",
    "    5. Calculate DX (Directional Index)\n",
    "    6. Smooth DX to get ADX\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        NEPSE OHLC data\n",
    "    window : int\n",
    "        Lookback period (default 14)\n",
    "    high_col, low_col, close_col : str\n",
    "        Column names for OHLC\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        ADX, +DI, -DI values\n",
    "    \"\"\"\n",
    "    # Calculate True Range (TR)\n",
    "    # TR = max(High - Low, |High - PrevClose|, |Low - PrevClose|)\n",
    "    high_low = data[high_col] - data[low_col]\n",
    "    high_close = abs(data[high_col] - data[close_col].shift(1))\n",
    "    low_close = abs(data[low_col] - data[close_col].shift(1))\n",
    "    \n",
    "    tr = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    \n",
    "    # Calculate Directional Movement (+DM and -DM)\n",
    "    # +DM = Current High - Previous High (if positive and > Current Low - Previous Low)\n",
    "    # -DM = Previous Low - Current Low (if positive and > Current High - Previous High)\n",
    "    \n",
    "    high_diff = data[high_col].diff()\n",
    "    low_diff = data[low_col].diff()\n",
    "    \n",
    "    plus_dm = ((high_diff > 0) & (high_diff > low_diff)) * high_diff\n",
    "    minus_dm = ((low_diff > 0) & (low_diff > high_diff)) * low_diff\n",
    "    \n",
    "    # Wilder's smoothing (RMA - Running Moving Average)\n",
    "    # First value is simple average, subsequent values use smoothing factor\n",
    "    def wilder_smoothing(series, period):\n",
    "        \"\"\"Apply Wilder's smoothing (exponential smoothing with alpha = 1/period)\"\"\"\n",
    "        return series.ewm(alpha=1/period, min_periods=period, adjust=False).mean()\n",
    "    \n",
    "    # Smooth TR, +DM, -DM\n",
    "    atr = wilder_smoothing(tr, window)\n",
    "    plus_di = 100 * wilder_smoothing(plus_dm, window) / atr\n",
    "    minus_di = 100 * wilder_smoothing(minus_dm, window) / atr\n",
    "    \n",
    "    # Calculate DX (Directional Index)\n",
    "    # DX = 100 * |+DI - -DI| / (+DI + -DI)\n",
    "    dx = 100 * abs(plus_di - minus_di) / (plus_di + minus_di)\n",
    "    \n",
    "    # Calculate ADX (smoothed DX)\n",
    "    adx = wilder_smoothing(dx, window)\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'ADX': adx,\n",
    "        'Plus_DI': plus_di,\n",
    "        'Minus_DI': minus_di,\n",
    "        'ATR': atr  # Average True Range (bonus)\n",
    "    })\n",
    "\n",
    "# Example usage with NEPSE data:\n",
    "def analyze_nepse_trend_strength(df):\n",
    "    \"\"\"\n",
    "    Apply ADX analysis to NEPSE stock data to determine trend strength.\n",
    "    \n",
    "    Interpretation for NEPSE trading:\n",
    "    - ADX < 20: Weak trend - avoid trend-following strategies, use mean reversion\n",
    "    - ADX 20-40: Trend developing - good for breakout strategies\n",
    "    - ADX > 40: Strong trend - ride the trend but watch for exhaustion\n",
    "    \n",
    "    Directional indicators:\n",
    "    - Plus_DI > Minus_DI: Bullish trend\n",
    "    - Plus_DI < Minus_DI: Bearish trend\n",
    "    \"\"\"\n",
    "    # Calculate ADX with 14-day period (standard)\n",
    "    adx_data = calculate_adx(df, window=14)\n",
    "    \n",
    "    # Combine with original data\n",
    "    analysis_df = pd.concat([df, adx_data], axis=1)\n",
    "    \n",
    "    # Generate trading signals based on trend strength\n",
    "    analysis_df['Trend_Strength'] = pd.cut(\n",
    "        analysis_df['ADX'], \n",
    "        bins=[0, 20, 40, 60, 100],\n",
    "        labels=['Weak', 'Developing', 'Strong', 'Extreme']\n",
    "    )\n",
    "    \n",
    "    # Trend direction signal\n",
    "    analysis_df['Trend_Direction'] = np.where(\n",
    "        analysis_df['Plus_DI'] > analysis_df['Minus_DI'],\n",
    "        'Bullish',\n",
    "        'Bearish'\n",
    "    )\n",
    "    \n",
    "    return analysis_df\n",
    "\n",
    "# Detailed explanation of ADX components for NEPSE:\n",
    "# 1. True Range (TR): Measures volatility by considering gaps between sessions\n",
    "# 2. +DI: Measures upward trend strength (based on High prices)\n",
    "# 3. -DI: Measures downward trend strength (based on Low prices)\n",
    "# 4. ADX: Average of DX (directional index), smoothed over time\n",
    "```\n",
    "\n",
    "**Detailed Explanation:**\n",
    "\n",
    "The `calculate_adx` function implements the complete Average Directional Index calculation pipeline specifically tailored for NEPSE stock data. \n",
    "\n",
    "**Step 1: True Range Calculation**\n",
    "The True Range (TR) captures the greatest of three measurements: the current high-low range, the absolute distance from today's high to yesterday's close, or today's low to yesterday's close. This accounts for gap-up or gap-down openings common in NEPSE stocks.\n",
    "\n",
    "**Step 2: Directional Movement**\n",
    "+DM (Positive Directional Movement) measures upward pressure by comparing today's high to yesterday's high, but only if this difference exceeds the downward movement (yesterday's low minus today's low). Conversely, -DM measures downward pressure. This helps identify whether NEPSE stocks are gaining bullish or bearish momentum.\n",
    "\n",
    "**Step 3: Wilder's Smoothing**\n",
    "Unlike simple moving averages, Wilder's smoothing applies an exponential moving average with a smoothing factor of $\\frac{1}{n}$. This gives more weight to recent data while maintaining stability, crucial for NEPSE's sometimes volatile but trending nature.\n",
    "\n",
    "**Step 4: Directional Indices**\n",
    "+DI and -DI normalize the directional movements by the Average True Range, converting them to percentages (0-100). When +DI > -DI for a NEPSE stock, it indicates bullish dominance; when reversed, bearish pressure dominates.\n",
    "\n",
    "**Step 5: ADX Calculation**\n",
    "The Directional Index (DX) measures the absolute difference between +DI and -DI relative to their sum, giving the strength of trend regardless of direction. Smoothing DX with Wilder's method yields ADX. For NEPSE trading:\n",
    "- **ADX < 20**: The stock is ranging (sideways), avoid trend-following\n",
    "- **ADX 20-40**: Trend developing, suitable for breakout entries\n",
    "- **ADX > 40**: Strong trend, ride it but watch for exhaustion signals\n",
    "\n",
    "### **13.2.2 Exponential Moving Averages**\n",
    "\n",
    "While covered in the ADX section above, EMAs deserve specific attention for NEPSE trend analysis. The EMA reacts faster to price changes than SMA, making it ideal for capturing early trend reversals in Nepali stocks.\n",
    "\n",
    "**Key Differences for NEPSE:**\n",
    "- **EMA 12 vs EMA 26**: The difference forms the MACD line, widely used in NEPSE technical analysis\n",
    "- **Signal Line**: 9-day EMA of MACD, used to generate buy/sell signals when MACD crosses above/below\n",
    "\n",
    "### **13.2.3 Trend Strength Indicators**\n",
    "\n",
    "Beyond ADX, trend strength can be measured using:\n",
    "\n",
    "**Linear Regression Slope:**\n",
    "Measures the angle and strength of the trend line fitted to recent prices.\n",
    "\n",
    "```python\n",
    "def calculate_trend_strength_linear(data, window=20, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate trend strength using linear regression slope.\n",
    "    \n",
    "    For NEPSE stocks, the slope indicates:\n",
    "    - Positive slope: Uptrend (buying pressure)\n",
    "    - Negative slope: Downtrend (selling pressure)\n",
    "    - Near zero: Sideways/ranging\n",
    "    \n",
    "    Also calculates R-squared to measure how well the trend explains price movement.\n",
    "    \"\"\"\n",
    "    from scipy import stats\n",
    "    \n",
    "    prices = data[price_col]\n",
    "    \n",
    "    def linear_regression_slope(x):\n",
    "        \"\"\"Calculate slope for a window of prices\"\"\"\n",
    "        if len(x) < 2:\n",
    "            return 0\n",
    "        x_vals = np.arange(len(x))\n",
    "        slope, intercept, r_value, p_value, std_err = stats.linregress(x_vals, x)\n",
    "        return slope\n",
    "    \n",
    "    # Calculate rolling slope\n",
    "    slope = prices.rolling(window=window).apply(\n",
    "        lambda x: linear_regression_slope(x), \n",
    "        raw=True\n",
    "    )\n",
    "    \n",
    "    # Calculate R-squared (trend reliability)\n",
    "    r_squared = prices.rolling(window=window).apply(\n",
    "        lambda x: stats.linregress(np.arange(len(x)), x).rvalue ** 2 if len(x) > 1 else 0,\n",
    "        raw=True\n",
    "    )\n",
    "    \n",
    "    # Normalize slope by price level to make comparable across stocks\n",
    "    normalized_slope = (slope / prices) * 100  # Percentage change per period\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Trend_Slope': slope,\n",
    "        'Trend_R2': r_squared,\n",
    "        'Trend_Strength_Pct': normalized_slope,\n",
    "        'Trend_Direction': np.where(slope > 0, 'Up', \n",
    "                                   np.where(slope < 0, 'Down', 'Flat'))\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_trend_strength_linear` function performs linear regression on rolling windows of NEPSE closing prices. \n",
    "\n",
    "**Slope Calculation:**\n",
    "For each window of 20 days (default), it fits a line $y = mx + b$ where $x$ is time (0, 1, 2, ..., 19) and $y$ is the price. The slope $m$ indicates how many rupees the stock gains per day on average. A slope of +2.5 means the stock is appreciating by Rs. 2.5 per day over the 20-day period.\n",
    "\n",
    "**R-Squared (Trend Reliability):**\n",
    "The $R^2$ value (0 to 1) measures how well the linear model explains price variance. For NEPSE stocks:\n",
    "- $R^2 > 0.7$: Strong linear trend (reliable for trading)\n",
    "- $R^2 = 0.3-0.7$: Moderate trend (use with caution)\n",
    "- $R^2 < 0.3$: No clear trend (avoid trend-following)\n",
    "\n",
    "**Normalized Slope:**\n",
    "Dividing slope by current price converts absolute rupee changes to percentage changes, making trend strength comparable across different NEPSE stocks (e.g., comparing a Rs. 500 stock vs a Rs. 50 stock).\n",
    "\n",
    "---\n",
    "\n",
    "## **13.3 Momentum-Based Indicators**\n",
    "\n",
    "Momentum indicators measure the speed of price changes, helping identify overbought or oversold conditions and potential reversal points.\n",
    "\n",
    "### **13.3.1 Rate of Change (ROC)**\n",
    "\n",
    "ROC measures the percentage change in price over a specified period, indicating momentum direction and speed.\n",
    "\n",
    "```python\n",
    "def calculate_roc(data, window=12, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Rate of Change (ROC) for NEPSE stocks.\n",
    "    \n",
    "    ROC = ((Current Price - Price n periods ago) / Price n periods ago) * 100\n",
    "    \n",
    "    Interpretation for NEPSE:\n",
    "    - ROC > 0: Bullish momentum (price higher than n periods ago)\n",
    "    - ROC < 0: Bearish momentum (price lower than n periods ago)\n",
    "    - ROC crossing above 0: Buy signal (momentum shift)\n",
    "    - ROC crossing below 0: Sell signal\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        NEPSE stock data\n",
    "    window : int\n",
    "        Lookback period (typically 12 for monthly, 25 for yearly in trading days)\n",
    "    price_col : str\n",
    "        Price column to calculate ROC on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        ROC values as percentages\n",
    "    \"\"\"\n",
    "    # Calculate ROC using pandas pct_change\n",
    "    # pct_change calculates (current - previous)/previous\n",
    "    roc = data[price_col].pct_change(periods=window) * 100\n",
    "    \n",
    "    # Alternative manual calculation:\n",
    "    # current = data[price_col]\n",
    "    # past = data[price_col].shift(window)\n",
    "    # roc = ((current - past) / past) * 100\n",
    "    \n",
    "    return roc\n",
    "\n",
    "def calculate_roc_signals(data, roc_col='ROC_12'):\n",
    "    \"\"\"\n",
    "    Generate trading signals based on ROC momentum.\n",
    "    \n",
    "    Signals:\n",
    "    - Buy: ROC crosses above 0 (momentum turning positive)\n",
    "    - Sell: ROC crosses below 0 (momentum turning negative)\n",
    "    - Overbought: ROC > 10 (strong upward momentum, potential reversal)\n",
    "    - Oversold: ROC < -10 (strong downward momentum, potential bounce)\n",
    "    \"\"\"\n",
    "    signals = pd.DataFrame(index=data.index)\n",
    "    \n",
    "    # Current ROC\n",
    "    signals['ROC'] = data[roc_col]\n",
    "    \n",
    "    # Signal generation\n",
    "    signals['Signal'] = 0\n",
    "    signals.loc[signals['ROC'] > 0, 'Signal'] = 1   # Bullish\n",
    "    signals.loc[signals['ROC'] < 0, 'Signal'] = -1  # Bearish\n",
    "    \n",
    "    # Change in signal (crossover detection)\n",
    "    signals['Signal_Change'] = signals['Signal'].diff()\n",
    "    \n",
    "    # Buy/Sell triggers\n",
    "    signals['Buy'] = signals['Signal_Change'] == 2   # -1 to 1\n",
    "    signals['Sell'] = signals['Signal_Change'] == -2  # 1 to -1\n",
    "    \n",
    "    # Momentum extremes\n",
    "    signals['Overbought'] = signals['ROC'] > 10\n",
    "    signals['Oversold'] = signals['ROC'] < -10\n",
    "    \n",
    "    return signals\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_roc` function computes the Rate of Change, a pure momentum oscillator. For NEPSE stocks, ROC tells us how fast prices are changing relative to a past point (typically 12 days for short-term, 25 days for medium-term).\n",
    "\n",
    "**Mathematical Logic:**\n",
    "The formula $\\frac{P_t - P_{t-n}}{P_{t-n}} \\times 100$ converts absolute price changes into percentage terms. If a NEPSE stock was Rs. 100 twelve days ago and is now Rs. 110, ROC = 10%, indicating 10% upward momentum.\n",
    "\n",
    "**Trading Interpretation:**\n",
    "- **Zero Line Crossovers**: When ROC crosses above zero, it indicates the stock is performing better than it was n-periods ago (bullish). Crossing below zero indicates deterioration (bearish).\n",
    "- **Extreme Values**: ROC > +10 suggests overbought conditions (price advanced too fast, likely to correct). ROC < -10 suggests oversold (potential bounce).\n",
    "- **Divergences**: If NEPSE price makes a new high but ROC makes a lower high, it indicates weakening momentum (bearish divergence).\n",
    "\n",
    "The `calculate_roc_signals` function automates signal generation by detecting crossovers (when ROC changes sign) and identifying extreme momentum conditions that might precede reversals in Nepali stocks.\n",
    "\n",
    "### **13.3.2 Relative Strength Index (RSI)**\n",
    "\n",
    "RSI is a momentum oscillator that measures the speed and change of price movements, oscillating between 0 and 100.\n",
    "\n",
    "```python\n",
    "def calculate_rsi(data, window=14, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Relative Strength Index (RSI) for NEPSE stocks.\n",
    "    \n",
    "    RSI measures momentum on a scale of 0-100:\n",
    "    - RSI > 70: Overbought (potential sell signal)\n",
    "    - RSI < 30: Oversold (potential buy signal)\n",
    "    - RSI 50: Neutral\n",
    "    \n",
    "    Formula:\n",
    "    RSI = 100 - (100 / (1 + RS))\n",
    "    RS = Average Gain / Average Loss\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        NEPSE stock data\n",
    "    window : int\n",
    "        Lookback period (default 14 days)\n",
    "    price_col : str\n",
    "        Price column to calculate RSI on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        RSI values (0-100)\n",
    "    \"\"\"\n",
    "    # Calculate price changes\n",
    "    delta = data[price_col].diff()\n",
    "    \n",
    "    # Separate gains and losses\n",
    "    gain = (delta.where(delta > 0, 0)).fillna(0)\n",
    "    loss = (-delta.where(delta < 0, 0)).fillna(0)\n",
    "    \n",
    "    # Calculate average gain and loss using Wilder's smoothing\n",
    "    avg_gain = gain.ewm(com=window-1, min_periods=window, adjust=False).mean()\n",
    "    avg_loss = loss.ewm(com=window-1, min_periods=window, adjust=False).mean()\n",
    "    \n",
    "    # Calculate RS (Relative Strength)\n",
    "    rs = avg_gain / avg_loss\n",
    "    \n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Handle edge case where avg_loss is 0 (perfect uptrend)\n",
    "    rsi = rsi.fillna(100)\n",
    "    \n",
    "    return rsi\n",
    "\n",
    "def calculate_rsi_divergence(data, rsi_col='RSI_14', price_col='Close', window=5):\n",
    "    \"\"\"\n",
    "    Detect RSI divergences for NEPSE stocks.\n",
    "    \n",
    "    Bullish Divergence: Price makes lower low, RSI makes higher low\n",
    "    Bearish Divergence: Price makes higher high, RSI makes lower high\n",
    "    \n",
    "    Divergences often precede trend reversals.\n",
    "    \"\"\"\n",
    "    # Find local extrema\n",
    "    data['Price_High'] = data[price_col].rolling(window=window, center=True).max() == data[price_col]\n",
    "    data['Price_Low'] = data[price_col].rolling(window=window, center=True).min() == data[price_col]\n",
    "    \n",
    "    data['RSI_High'] = data[rsi_col].rolling(window=window, center=True).max() == data[rsi_col]\n",
    "    data['RSI_Low'] = data[rsi_col].rolling(window=window, center=True).min() == data[rsi_col]\n",
    "    \n",
    "    # Detect divergences\n",
    "    data['Bullish_Divergence'] = (\n",
    "        data['Price_Low'] & \n",
    "        (data[rsi_col] > data[rsi_col].shift(1))\n",
    "    )\n",
    "    \n",
    "    data['Bearish_Divergence'] = (\n",
    "        data['Price_High'] & \n",
    "        (data[rsi_col] < data[rsi_col].shift(1))\n",
    "    )\n",
    "    \n",
    "    return data[['Bullish_Divergence', 'Bearish_Divergence']]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_rsi` function implements the Relative Strength Index, a momentum oscillator crucial for NEPSE technical analysis. Unlike simple price-based indicators, RSI normalizes momentum to a 0-100 scale, making it comparable across different stocks regardless of absolute price levels (whether a Rs. 500 stock or Rs. 50 stock).\n",
    "\n",
    "**Mathematical Logic:**\n",
    "The algorithm first decomposes price changes into gains (positive changes) and losses (absolute values of negative changes). It then applies Wilder's smoothing (an exponential moving average with $\\alpha = \\frac{1}{window}$) to both series. The Relative Strength (RS) is the ratio of average gains to average losses. Finally, RSI normalizes this ratio to the 0-100 scale using $100 - \\frac{100}{1+RS}$.\n",
    "\n",
    "**NEPSE-Specific Interpretation:**\n",
    "- **Overbought (>70)**: NEPSE stocks often exhibit strong momentum during bull markets. RSI > 70 suggests the stock is overextended and due for a correction or consolidation.\n",
    "- **Oversold (<30)**: During market panics or corrections, RSI < 30 indicates capitulation selling, often presenting buying opportunities for fundamentally strong Nepali companies.\n",
    "- **Divergences**: When NEPSE price makes a new high but RSI forms a lower high (bearish divergence), it signals weakening buying pressure despite higher prices, often preceding corrections. Conversely, bullish divergences (lower price lows, higher RSI lows) indicate accumulation.\n",
    "\n",
    "The `calculate_rsi_divergence` function automates the detection of these divergence patterns by identifying local price extrema (peaks and troughs) using rolling window comparisons, then checking if RSI confirms or contradicts these price movements.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.3.3 Momentum Acceleration**\n",
    "\n",
    "Momentum acceleration measures how quickly momentum itself is changing, indicating increasing or decreasing trend velocity.\n",
    "\n",
    "```python\n",
    "def calculate_momentum_acceleration(data, roc_window=12, smooth_window=3, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate momentum acceleration for NEPSE stocks.\n",
    "    \n",
    "    Momentum Acceleration = Rate of Change of ROC\n",
    "    or Second derivative of price\n",
    "    \n",
    "    Interpretation:\n",
    "    - Positive acceleration: Momentum increasing (trend strengthening)\n",
    "    - Negative acceleration: Momentum decreasing (trend weakening)\n",
    "    - Zero acceleration: Constant momentum (steady trend)\n",
    "    \n",
    "    For NEPSE: Helps identify when a trend is gaining steam or losing power.\n",
    "    \"\"\"\n",
    "    # Calculate first momentum (ROC)\n",
    "    roc = data[price_col].pct_change(periods=roc_window) * 100\n",
    "    \n",
    "    # Calculate acceleration (change in ROC)\n",
    "    # Smooth it to reduce noise\n",
    "    acceleration = roc.diff(periods=smooth_window)\n",
    "    \n",
    "    # Alternative: Second derivative of price (curvature)\n",
    "    first_diff = data[price_col].diff()\n",
    "    second_diff = first_diff.diff()\n",
    "    \n",
    "    # Normalize by price level for cross-stock comparison\n",
    "    normalized_accel = (second_diff / data[price_col]) * 100\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'ROC': roc,\n",
    "        'Momentum_Acceleration': acceleration,\n",
    "        'Price_Curvature': second_diff,\n",
    "        'Normalized_Acceleration': normalized_accel,\n",
    "        'Acceleration_Signal': np.where(acceleration > 0, 'Increasing',\n",
    "                                       np.where(acceleration < 0, 'Decreasing', 'Stable'))\n",
    "    })\n",
    "\n",
    "def detect_momentum_shifts(data, accel_col='Momentum_Acceleration', threshold=2.0):\n",
    "    \"\"\"\n",
    "    Detect significant momentum shifts for NEPSE trading signals.\n",
    "    \n",
    "    When acceleration changes sign or exceeds threshold, it indicates\n",
    "    potential trend continuation or reversal.\n",
    "    \"\"\"\n",
    "    accel = data[accel_col]\n",
    "    \n",
    "    # Detect zero crossings (momentum shift)\n",
    "    zero_crossing = np.sign(accel) != np.sign(accel.shift(1))\n",
    "    \n",
    "    # Detect extreme acceleration (unsustainable momentum)\n",
    "    extreme_positive = accel > accel.rolling(50).mean() + threshold * accel.rolling(50).std()\n",
    "    extreme_negative = accel < accel.rolling(50).mean() - threshold * accel.rolling(50).std()\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Momentum_Shift': zero_crossing,\n",
    "        'Extreme_Momentum': extreme_positive | extreme_negative,\n",
    "        'Shift_Type': np.where(zero_crossing & (accel > 0), 'Bullish_Shift',\n",
    "                              np.where(zero_crossing & (accel < 0), 'Bearish_Shift', 'None'))\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_momentum_acceleration` function computes the second derivative of price movement, essentially measuring how fast the momentum itself is changing. In physics terms, if price is position and ROC (Rate of Change) is velocity, then acceleration is the change in velocity.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "The function calculates ROC (first momentum) as the percentage change over `roc_window` periods. Then it computes the difference (derivative) of ROC over `smooth_window` periods to get acceleration. Alternatively, it calculates the second difference of raw prices (curvature).\n",
    "\n",
    "**NEPSE Application:**\n",
    "For Nepali stocks, momentum acceleration helps identify:\n",
    "1. **Trend Exhaustion**: When a strong uptrend shows negative acceleration (decelerating momentum), even though prices are still rising, the trend is losing power and may reverse soon.\n",
    "2. **Trend Emergence**: Positive acceleration during a downtrend indicates selling pressure is intensifying (accelerating decline).\n",
    "3. **Sustainable Moves**: Steady acceleration (constant positive or negative) suggests institutional participation in NEPSE stocks, indicating sustainable trends.\n",
    "\n",
    "The `detect_momentum_shifts` function identifies critical turning points where acceleration changes sign (zero crossings), indicating shifts from accelerating to decelerating momentum or vice versa. These shifts often precede price reversals in NEPSE stocks by 1-3 days.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.4 Volatility and Range Indicators**\n",
    "\n",
    "Volatility indicators measure the degree of variation in trading prices over time. For NEPSE, volatility indicators help assess risk and identify potential breakout or consolidation periods.\n",
    "\n",
    "### **13.4.1 Standard Deviation**\n",
    "\n",
    "Standard deviation measures dispersion of prices around the mean, quantifying volatility.\n",
    "\n",
    "```python\n",
    "def calculate_volatility_indicators(data, window=20, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate various volatility indicators for NEPSE risk assessment.\n",
    "    \n",
    "    Standard Deviation measures how much prices deviate from the average.\n",
    "    Higher values indicate higher volatility/risk.\n",
    "    \n",
    "    For NEPSE:\n",
    "    - Low volatility (< 2%): Stable stock, low risk, potential consolidation\n",
    "    - Medium volatility (2-5%): Normal trading range\n",
    "    - High volatility (> 5%): High risk/reward, potential breakout/breakdown\n",
    "    \"\"\"\n",
    "    # Calculate returns\n",
    "    returns = data[price_col].pct_change()\n",
    "    \n",
    "    # Standard Deviation of returns (volatility)\n",
    "    std_dev = returns.rolling(window=window).std()\n",
    "    \n",
    "    # Annualized volatility (assuming 252 trading days per year for NEPSE)\n",
    "    annualized_vol = std_dev * np.sqrt(252) * 100  # Convert to percentage\n",
    "    \n",
    "    # Standard Deviation of prices (absolute volatility)\n",
    "    price_std = data[price_col].rolling(window=window).std()\n",
    "    \n",
    "    # Coefficient of Variation (relative volatility)\n",
    "    # CV = StdDev / Mean, useful for comparing volatility across different priced stocks\n",
    "    cv = price_std / data[price_col].rolling(window=window).mean()\n",
    "    \n",
    "    # Volatility regime classification\n",
    "    vol_mean = annualized_vol.rolling(window=window*2).mean()\n",
    "    vol_std = annualized_vol.rolling(window=window*2).std()\n",
    "    \n",
    "    regime = pd.cut(annualized_vol, \n",
    "                    bins=[0, vol_mean - vol_std, vol_mean + vol_std, float('inf')],\n",
    "                    labels=['Low_Vol', 'Normal_Vol', 'High_Vol'])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Returns': returns,\n",
    "        'Volatility': std_dev,\n",
    "        'Annualized_Vol_Pct': annualized_vol,\n",
    "        'Price_StdDev': price_std,\n",
    "        'Coeff_of_Variation': cv,\n",
    "        'Volatility_Regime': regime\n",
    "    })\n",
    "\n",
    "def calculate_bollinger_bands(data, window=20, num_std=2, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Bollinger Bands for NEPSE trend and volatility analysis.\n",
    "    \n",
    "    Bollinger Bands consist of:\n",
    "    - Middle Band: SMA (Simple Moving Average)\n",
    "    - Upper Band: SMA + (Standard Deviation * num_std)\n",
    "    - Lower Band: SMA - (Standard Deviation * num_std)\n",
    "    \n",
    "    Interpretation for NEPSE:\n",
    "    - Price touching Upper Band: Potentially overbought (consider selling)\n",
    "    - Price touching Lower Band: Potentially oversold (consider buying)\n",
    "    - Squeeze (bands narrowing): Low volatility, often precedes breakout\n",
    "    - Expansion (bands widening): High volatility, trend continuation likely\n",
    "    \"\"\"\n",
    "    # Calculate middle band (SMA)\n",
    "    middle_band = data[price_col].rolling(window=window).mean()\n",
    "    \n",
    "    # Calculate standard deviation\n",
    "    std_dev = data[price_col].rolling(window=window).std()\n",
    "    \n",
    "    # Calculate upper and lower bands\n",
    "    upper_band = middle_band + (std_dev * num_std)\n",
    "    lower_band = middle_band - (std_dev * num_std)\n",
    "    \n",
    "    # Calculate Bandwidth (% distance between bands relative to middle)\n",
    "    bandwidth = ((upper_band - lower_band) / middle_band) * 100\n",
    "    \n",
    "    # Calculate %B (position within bands)\n",
    "    # %B = 0 at lower band, 1 at upper band, 0.5 at middle\n",
    "    pct_b = (data[price_col] - lower_band) / (upper_band - lower_band)\n",
    "    \n",
    "    # Generate signals\n",
    "    signals = pd.DataFrame(index=data.index)\n",
    "    signals['BB_Signal'] = 'Hold'\n",
    "    signals.loc[pct_b > 0.8, 'BB_Signal'] = 'Overbought'  # Near upper band\n",
    "    signals.loc[pct_b < 0.2, 'BB_Signal'] = 'Oversold'     # Near lower band\n",
    "    \n",
    "    # Squeeze detection (bandwidth at 6-month low)\n",
    "    squeeze_threshold = bandwidth.rolling(window=120).min()\n",
    "    signals['Squeeze'] = bandwidth <= squeeze_threshold * 1.05  # Within 5% of lowest\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Close': data[price_col],\n",
    "        'BB_Middle': middle_band,\n",
    "        'BB_Upper': upper_band,\n",
    "        'BB_Lower': lower_band,\n",
    "        'BB_Width': bandwidth,\n",
    "        'BB_PercentB': pct_b,\n",
    "        'BB_Signal': signals['BB_Signal'],\n",
    "        'BB_Squeeze': signals['Squeeze']\n",
    "    })\n",
    "\n",
    "# Usage example for NEPSE:\n",
    "# df = pd.read_csv('nepse_stock.csv')\n",
    "# df = calculate_bollinger_bands(df, window=20, num_std=2)\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_bollinger_bands` function implements one of the most versatile technical indicators for NEPSE analysis. \n",
    "\n",
    "**Mathematical Construction:**\n",
    "The middle band is the 20-day SMA (Simple Moving Average), serving as the baseline trend. The upper and lower bands are positioned 2 standard deviations away from the middle band. Statistically, this captures approximately 95% of price action assuming normal distribution, meaning prices outside these bands are statistically extreme.\n",
    "\n",
    "**NEPSE-Specific Interpretation:**\n",
    "- **PercentB (%B)**: This metric normalizes price position within the bands to a 0-1 scale. For NEPSE stocks, when %B > 0.8, the stock is in the upper 20% of its recent volatility range, suggesting overbought conditions. Conversely, %B < 0.2 suggests oversold conditions suitable for accumulation.\n",
    "- **Bandwidth**: Calculated as $\\frac{Upper - Lower}{Middle} \\times 100$, this measures volatility. A bandwidth squeeze (narrowing bands) in NEPSE often precedes significant breakouts due to low liquidity periods followed by news-driven movements.\n",
    "- **Squeeze Detection**: The function identifies when bandwidth is at a 120-day (6-month) low, indicating impending volatility expansion. For Nepali markets, this often correlates with quarterly earnings announcements or regulatory news.\n",
    "\n",
    "### **13.3.2 Relative Strength Index (RSI)**\n",
    "\n",
    "RSI measures the magnitude of recent price changes to evaluate overbought or oversold conditions.\n",
    "\n",
    "```python\n",
    "def calculate_rsi(data, window=14, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Relative Strength Index (RSI) for NEPSE momentum analysis.\n",
    "    \n",
    "    RSI Formula:\n",
    "    RSI = 100 - (100 / (1 + RS))\n",
    "    RS = Average Gain / Average Loss\n",
    "    \n",
    "    Standard Interpretation:\n",
    "    - RSI > 70: Overbought (consider selling)\n",
    "    - RSI < 30: Oversold (consider buying)\n",
    "    - RSI 50: Neutral line\n",
    "    \n",
    "    NEPSE Considerations:\n",
    "    - In strong bull markets, RSI may stay >70 for extended periods\n",
    "    - In bear markets, RSI may stay <30 for extended periods\n",
    "    - Divergences between RSI and price are strong signals\n",
    "    \"\"\"\n",
    "    # Calculate price differences\n",
    "    delta = data[price_col].diff()\n",
    "    \n",
    "    # Separate gains and losses\n",
    "    gain = delta.copy()\n",
    "    loss = delta.copy()\n",
    "    \n",
    "    gain[gain < 0] = 0      # Keep only positive changes\n",
    "    loss[loss > 0] = 0      # Keep only negative changes\n",
    "    loss = abs(loss)        # Convert to positive values\n",
    "    \n",
    "    # Calculate average gain and loss using Wilder's smoothing\n",
    "    # Wilder's method uses exponential moving average with alpha = 1/window\n",
    "    avg_gain = gain.ewm(com=window-1, adjust=False, min_periods=window).mean()\n",
    "    avg_loss = loss.ewm(com=window-1, adjust=False, min_periods=window).mean()\n",
    "    \n",
    "    # Calculate RS (Relative Strength)\n",
    "    rs = avg_gain / avg_loss\n",
    "    \n",
    "    # Calculate RSI\n",
    "    rsi = 100 - (100 / (1 + rs))\n",
    "    \n",
    "    # Handle case where avg_loss is 0 (perfect uptrend)\n",
    "    rsi = rsi.fillna(100)\n",
    "    \n",
    "    return rsi\n",
    "\n",
    "def calculate_rsi_features(data, window=14):\n",
    "    \"\"\"\n",
    "    Calculate advanced RSI-based features for NEPSE prediction models.\n",
    "    \n",
    "    Features include:\n",
    "    - Raw RSI value\n",
    "    - RSI trend (slope)\n",
    "    - RSI position relative to 30/70 levels\n",
    "    - Divergence signals\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Basic RSI\n",
    "    df['RSI'] = calculate_rsi(df, window=window)\n",
    "    \n",
    "    # RSI smoothed (to reduce noise)\n",
    "    df['RSI_Smooth'] = df['RSI'].rolling(window=3).mean()\n",
    "    \n",
    "    # RSI slope (momentum of momentum)\n",
    "    df['RSI_Slope'] = df['RSI'].diff(3) / 3\n",
    "    \n",
    "    # Position features\n",
    "    df['RSI_Zone'] = pd.cut(df['RSI'], \n",
    "                            bins=[0, 30, 50, 70, 100],\n",
    "                            labels=['Oversold', 'Bullish', 'Bearish', 'Overbought'])\n",
    "    \n",
    "    # Distance from extremes\n",
    "    df['RSI_Distance_30'] = df['RSI'] - 30  # Positive = above oversold\n",
    "    df['RSI_Distance_70'] = df['RSI'] - 70  # Negative = below overbought\n",
    "    \n",
    "    # Stochastic RSI (RSI normalized to 0-100 scale)\n",
    "    rsi_min = df['RSI'].rolling(window=window).min()\n",
    "    rsi_max = df['RSI'].rolling(window=window).max()\n",
    "    df['Stoch_RSI'] = (df['RSI'] - rsi_min) / (rsi_max - rsi_min + 1e-10) * 100\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_rsi` function implements the Relative Strength Index using Wilder's smoothing method, which is the standard approach in technical analysis.\n",
    "\n",
    "**Calculation Process:**\n",
    "1. **Price Differencing**: Computes day-to-day price changes using `diff()`.\n",
    "2. **Gain/Loss Separation**: Separates positive changes (gains) from negative changes (losses). Losses are converted to absolute values.\n",
    "3. **Wilder's Smoothing**: Unlike simple moving averages, Wilder's method applies exponential smoothing with $\\alpha = \\frac{1}{window}$. This gives more weight to recent observations while maintaining the influence of older data, preventing sudden jumps when old data drops out of the window.\n",
    "4. **Relative Strength**: RS is the ratio of average gains to average losses. If average losses are zero (perfect uptrend), RS approaches infinity.\n",
    "5. **Normalization**: The formula $100 - \\frac{100}{1+RS}$ normalizes RS to a 0-100 scale. When RS is high (strong gains), RSI approaches 100. When RS is low (strong losses), RSI approaches 0.\n",
    "\n",
    "**NEPSE-Specific Features:**\n",
    "The `calculate_rsi_features` function creates derived features particularly useful for Nepali stock prediction:\n",
    "- **RSI_Slope**: Measures whether momentum is accelerating or decelerating. Positive slope indicates strengthening momentum.\n",
    "- **RSI_Zone**: Categorizes RSI into actionable regions (Oversold <30, Bullish 30-50, Bearish 50-70, Overbought >70).\n",
    "- **Stochastic RSI**: Normalizes RSI itself to a 0-100 scale over the lookback period, making it more sensitive to short-term reversals in NEPSE's sometimes range-bound market.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.4 Volatility and Range Indicators**\n",
    "\n",
    "Volatility indicators measure the magnitude of price fluctuations, essential for risk management and position sizing in NEPSE trading.\n",
    "\n",
    "### **13.4.1 Standard Deviation**\n",
    "\n",
    "Standard deviation quantifies dispersion of returns around the mean, serving as the foundation for many volatility indicators.\n",
    "\n",
    "```python\n",
    "def calculate_volatility_metrics(data, window=20, price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive volatility metrics for NEPSE risk management.\n",
    "    \n",
    "    Includes:\n",
    "    - Rolling Standard Deviation (absolute and percentage)\n",
    "    - Annualized Volatility\n",
    "    - Parkinson Volatility (using High-Low range)\n",
    "    - Garman-Klass Volatility (using OHLC)\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    data : pd.DataFrame\n",
    "        NEPSE data with OHLC columns\n",
    "    window : int\n",
    "        Lookback period for volatility calculation\n",
    "    price_col : str\n",
    "        Column to calculate volatility on\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.DataFrame\n",
    "        Various volatility measures\n",
    "    \"\"\"\n",
    "    # Calculate returns\n",
    "    returns = data[price_col].pct_change()\n",
    "    \n",
    "    # Standard deviation of returns (historical volatility)\n",
    "    std_returns = returns.rolling(window=window).std()\n",
    "    \n",
    "    # Annualized volatility (assuming 252 trading days for NEPSE)\n",
    "    ann_vol = std_returns * np.sqrt(252) * 100  # As percentage\n",
    "    \n",
    "    # Standard deviation of prices (absolute volatility)\n",
    "    std_prices = data[price_col].rolling(window=window).std()\n",
    "    \n",
    "    # Coefficient of Variation (relative volatility)\n",
    "    cv = std_prices / data[price_col].rolling(window=window).mean()\n",
    "    \n",
    "    # Parkinson Volatility (uses High-Low range, more efficient)\n",
    "    # Formula: sqrt(1/(4*ln(2)) * sum(log(High/Low)^2) / n)\n",
    "    log_hl = np.log(data[high_col] / data[low_col])\n",
    "    parkinson_vol = np.sqrt(\n",
    "        (log_hl**2).rolling(window=window).mean() / (4 * np.log(2))\n",
    "    ) * np.sqrt(252) * 100  # Annualized\n",
    "    \n",
    "    # Garman-Klass Volatility (uses OHLC, most efficient)\n",
    "    # Formula: sqrt(0.5*log(High/Low)^2 - (2*log(2)-1)*log(Close/Open)^2)\n",
    "    log_ho = np.log(data[high_col] / data[open_col])\n",
    "    log_lo = np.log(data[low_col] / data[open_col])\n",
    "    log_co = np.log(data[close_col] / data[open_col])\n",
    "    \n",
    "    gk_vol = np.sqrt(\n",
    "        (0.5 * (log_ho - log_lo)**2 - (2*np.log(2) - 1) * log_co**2).rolling(window=window).mean()\n",
    "    ) * np.sqrt(252) * 100\n",
    "    \n",
    "    # Volatility regime classification\n",
    "    vol_mean = ann_vol.rolling(window=window*2).mean()\n",
    "    vol_std = ann_vol.rolling(window=window*2).std()\n",
    "    \n",
    "    regime = pd.cut(ann_vol,\n",
    "                   bins=[0, vol_mean - vol_std, vol_mean + vol_std, float('inf')],\n",
    "                   labels=['Low_Vol', 'Normal_Vol', 'High_Vol'])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Returns_Std': std_returns,\n",
    "        'Annualized_Vol_Pct': ann_vol,\n",
    "        'Price_StdDev': std_prices,\n",
    "        'Coeff_of_Variation': cv,\n",
    "        'Parkinson_Vol': parkinson_vol,\n",
    "        'Garman_Klass_Vol': gk_vol,\n",
    "        'Volatility_Regime': regime,\n",
    "        'Vol_Z_Score': (ann_vol - vol_mean) / vol_std\n",
    "    })\n",
    "\n",
    "def calculate_volatility_signals(data, vol_col='Annualized_Vol_Pct', price_col='Close'):\n",
    "    \"\"\"\n",
    "    Generate trading signals based on volatility patterns for NEPSE.\n",
    "    \n",
    "    Strategies:\n",
    "    1. Volatility Breakout: Low vol followed by expansion = trend start\n",
    "    2. Volatility Contraction: High vol followed by contraction = trend end\n",
    "    3. Volatility Mean Reversion: Extreme vol readings revert to mean\n",
    "    \"\"\"\n",
    "    vol = data[vol_col]\n",
    "    \n",
    "    # Detect volatility squeeze (Bollinger Bands on volatility)\n",
    "    vol_bb = calculate_bollinger_bands(data.assign(Close=vol), window=20, num_std=2)\n",
    "    \n",
    "    # Volatility breakout signals\n",
    "    squeeze = vol < vol_bb['BB_Lower']  # Volatility below lower band = squeeze\n",
    "    expansion = vol > vol_bb['BB_Upper']  # Volatility above upper band = expansion\n",
    "    \n",
    "    # Volatility regime transitions\n",
    "    vol_ma = vol.rolling(window=20).mean()\n",
    "    vol_increasing = vol > vol_ma\n",
    "    vol_decreasing = vol < vol_ma\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Vol_Squeeze': squeeze,\n",
    "        'Vol_Expansion': expansion,\n",
    "        'Vol_Increasing': vol_increasing,\n",
    "        'Vol_Decreasing': vol_decreasing,\n",
    "        'Vol_BB_Upper': vol_bb['BB_Upper'],\n",
    "        'Vol_BB_Lower': vol_bb['BB_Lower']\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_volatility_metrics` function provides a comprehensive volatility analysis suite essential for risk management in NEPSE trading. \n",
    "\n",
    "**Standard Deviation vs. Parkinson vs. Garman-Klass:**\n",
    "The function calculates three types of volatility estimators:\n",
    "1. **Close-to-Close (Standard)**: Uses only closing prices. Simple but ignores intraday information (High/Low/Open).\n",
    "2. **Parkinson**: Uses High-Low range. More efficient (uses intraday extremes) but ignores opening gaps.\n",
    "3. **Garman-Klass**: Uses full OHLC data. Most statistically efficient estimator, accounting for opening gaps and intraday range.\n",
    "\n",
    "For NEPSE stocks, Garman-Klass provides the most accurate volatility estimate because it captures the true range of price movement during the trading session (9:30 AM - 3:00 PM NPT).\n",
    "\n",
    "**Volatility Regime Classification:**\n",
    "The function classifies volatility into Low, Normal, and High regimes using statistical thresholds (mean  standard deviation). This is crucial for NEPSE because:\n",
    "- **Low Volatility**: Often precedes major moves (earnings announcements, regulatory changes)\n",
    "- **High Volatility**: Usually occurs during market crashes or bubbles; indicates high risk but potential high reward\n",
    "- **Normal Volatility**: Suitable for standard trend-following strategies\n",
    "\n",
    "The `calculate_volatility_signals` function applies Bollinger Bands to the volatility series itself (volatility of volatility), identifying \"volatility squeezes\"periods of unusually low volatility that historically precede significant price breakouts in NEPSE stocks.\n",
    "\n",
    "### **13.4.2 Bollinger Bands**\n",
    "\n",
    "Bollinger Bands were covered in the volatility section above, but they serve dual purposes as both volatility and trend indicators. The key insight for NEPSE is the \"Bollinger Band Squeeze\"when the bands narrow to a 6-month low, it indicates a volatility contraction that typically precedes a significant directional move (breakout or breakdown).\n",
    "\n",
    "### **13.4.3 Average True Range (ATR)**\n",
    "\n",
    "ATR measures market volatility by decomposing the entire range of an asset price for that period.\n",
    "\n",
    "```python\n",
    "def calculate_atr(data, window=14, high_col='High', low_col='Low', close_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Average True Range (ATR) for NEPSE risk management.\n",
    "    \n",
    "    True Range is the greatest of:\n",
    "    1. Current High - Current Low\n",
    "    2. |Current High - Previous Close|\n",
    "    3. |Current Low - Previous Close|\n",
    "    \n",
    "    ATR is the average of True Range over the window period.\n",
    "    \n",
    "    NEPSE Applications:\n",
    "    - Position sizing: Riskier stocks (high ATR) get smaller positions\n",
    "    - Stop loss placement: 2-3x ATR below entry for long positions\n",
    "    - Volatility breakout: Entry when price moves > 1.5x ATR from open\n",
    "    \"\"\"\n",
    "    # Calculate True Range components\n",
    "    high_low = data[high_col] - data[low_col]\n",
    "    high_close = abs(data[high_col] - data[close_col].shift(1))\n",
    "    low_close = abs(data[low_col] - data[close_col].shift(1))\n",
    "    \n",
    "    # True Range is the maximum of the three\n",
    "    true_range = pd.concat([high_low, high_close, low_close], axis=1).max(axis=1)\n",
    "    \n",
    "    # Average True Range (Wilder's smoothing method)\n",
    "    atr = true_range.ewm(alpha=1/window, min_periods=window, adjust=False).mean()\n",
    "    \n",
    "    # Alternative: Simple moving average of TR\n",
    "    atr_sma = true_range.rolling(window=window).mean()\n",
    "    \n",
    "    # ATR-based indicators\n",
    "    # 1. ATR Percentage (relative to price)\n",
    "    atr_pct = (atr / data[close_col]) * 100\n",
    "    \n",
    "    # 2. ATR Bands (similar to Bollinger but using ATR)\n",
    "    atr_upper = data[close_col] + (2 * atr)\n",
    "    atr_lower = data[close_col] - (2 * atr)\n",
    "    \n",
    "    # 3. Normalized ATR (Chandelier Exit component)\n",
    "    # Measures how current ATR compares to historical ATR\n",
    "    atr_mean = atr.rolling(window=window*2).mean()\n",
    "    atr_std = atr.rolling(window=window*2).std()\n",
    "    atr_zscore = (atr - atr_mean) / atr_std\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'True_Range': true_range,\n",
    "        'ATR': atr,\n",
    "        'ATR_SMA': atr_sma,\n",
    "        'ATR_Pct': atr_pct,\n",
    "        'ATR_Upper': atr_upper,\n",
    "        'ATR_Lower': atr_lower,\n",
    "        'ATR_ZScore': atr_zscore,\n",
    "        'ATR_Regime': pd.cut(atr_zscore, \n",
    "                            bins=[-np.inf, -1, 1, np.inf],\n",
    "                            labels=['Low_Vol', 'Normal', 'High_Vol'])\n",
    "    })\n",
    "\n",
    "def calculate_chandelier_exit(data, atr_window=22, multiplier=3, \n",
    "                             high_col='High', low_col='Low', close_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Chandelier Exit for NEPSE trend following.\n",
    "    \n",
    "    Chandelier Exit sets a trailing stop based on ATR.\n",
    "    - Long Exit: Highest High since entry - (ATR * multiplier)\n",
    "    - Short Exit: Lowest Low since entry + (ATR * multiplier)\n",
    "    \n",
    "    For NEPSE long-only trading:\n",
    "    Use as dynamic stop-loss that adjusts to volatility.\n",
    "    \"\"\"\n",
    "    # Calculate ATR\n",
    "    atr_data = calculate_atr(data, window=atr_window, \n",
    "                            high_col=high_col, low_col=low_col, close_col=close_col)\n",
    "    atr = atr_data['ATR']\n",
    "    \n",
    "    # Calculate highest high and lowest low over the ATR window\n",
    "    highest_high = data[high_col].rolling(window=atr_window).max()\n",
    "    lowest_low = data[low_col].rolling(window=atr_window).min()\n",
    "    \n",
    "    # Chandelier Exit levels\n",
    "    chandelier_long = highest_high - (atr * multiplier)\n",
    "    chandelier_short = lowest_low + (atr * multiplier)\n",
    "    \n",
    "    # Distance to exit (risk measure)\n",
    "    distance_to_exit_long = data[close_col] - chandelier_long\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Chandelier_Long': chandelier_long,\n",
    "        'Chandelier_Short': chandelier_short,\n",
    "        'Highest_High_22': highest_high,\n",
    "        'Lowest_Low_22': lowest_low,\n",
    "        'Distance_to_Exit': distance_to_exit_long,\n",
    "        'ATR_Used': atr\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_atr` function computes the Average True Range, a critical risk management tool for NEPSE trading. \n",
    "\n",
    "**True Range Logic:**\n",
    "Unlike simple range (High - Low), True Range accounts for gaps between trading sessions. For NEPSE, which operates 5 days a week (Sunday-Thursday), gaps can occur due to overnight news. True Range captures the greatest of:\n",
    "1. Today's high-low range (normal intraday volatility)\n",
    "2. Distance from today's high to yesterday's close (gap up scenario)\n",
    "3. Distance from yesterday's close to today's low (gap down scenario)\n",
    "\n",
    "**Wilder's Smoothing:**\n",
    "The function applies exponential smoothing with $\\alpha = \\frac{1}{14}$ (for default window), which creates a smoother volatility measure than simple moving averages. This prevents whipsaws during NEPSE's sometimes erratic short-term movements.\n",
    "\n",
    "**Chandelier Exit Application:**\n",
    "The `calculate_chandelier_exit` function uses ATR for dynamic stop-loss placement. For NEPSE long positions, the exit level is set at the highest high over 22 days (approx 1 month of trading) minus 3x ATR. This trailing stop adjusts to volatilitytightening during calm periods to protect profits and widening during volatile periods to avoid whipsaws.\n",
    "\n",
    "### **13.3.3 Momentum Acceleration**\n",
    "\n",
    "Covered in section 13.3.1 with the `calculate_momentum_acceleration` function, this measures the second derivative of pricehow quickly momentum itself is changing. For NEPSE, acceleration signals often precede major moves by 1-2 days, making them valuable for entry timing.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.4 Volatility and Range Indicators**\n",
    "\n",
    "### **13.4.4 Range Percentages**\n",
    "\n",
    "Range percentages normalize daily trading ranges relative to opening prices or previous closes, enabling comparison across different priced NEPSE stocks.\n",
    "\n",
    "```python\n",
    "def calculate_range_indicators(data, open_col='Open', high_col='High', \n",
    "                              low_col='Low', close_col='Close', prev_close_col='Prev. Close'):\n",
    "    \"\"\"\n",
    "    Calculate range-based indicators for NEPSE intraday analysis.\n",
    "    \n",
    "    Range indicators measure daily volatility relative to price levels.\n",
    "    Useful for identifying high-volatility days and potential reversals.\n",
    "    \"\"\"\n",
    "    # True Range (max of three measurements)\n",
    "    tr1 = data[high_col] - data[low_col]\n",
    "    tr2 = abs(data[high_col] - data[prev_close_col])\n",
    "    tr3 = abs(data[low_col] - data[prev_close_col])\n",
    "    true_range = pd.concat([tr1, tr2, tr3], axis=1).max(axis=1)\n",
    "    \n",
    "    # Range as percentage of previous close\n",
    "    range_pct = (true_range / data[prev_close_col]) * 100\n",
    "    \n",
    "    # Intraday range percentage (High-Low)/Open\n",
    "    intraday_range_pct = ((data[high_col] - data[low_col]) / data[open_col]) * 100\n",
    "    \n",
    "    # Position in range (where did price close within the daily range?)\n",
    "    # 0 = closed at low, 100 = closed at high\n",
    "    position_in_range = ((data[close_col] - data[low_col]) / \n",
    "                        (data[high_col] - data[low_col])) * 100\n",
    "    \n",
    "    # Handle case where high == low (no range)\n",
    "    position_in_range = position_in_range.fillna(50)  # Assume middle if no range\n",
    "    \n",
    "    # Gap analysis (overnight gaps for NEPSE)\n",
    "    gap_pct = ((data[open_col] - data[prev_close_col]) / data[prev_close_col]) * 100\n",
    "    gap_type = pd.cut(gap_pct, \n",
    "                     bins=[-np.inf, -2, -0.5, 0.5, 2, np.inf],\n",
    "                     labels=['Large_Down_Gap', 'Small_Down_Gap', 'No_Gap', \n",
    "                            'Small_Up_Gap', 'Large_Up_Gap'])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'True_Range': true_range,\n",
    "        'Range_Pct': range_pct,\n",
    "        'Intraday_Range_Pct': intraday_range_pct,\n",
    "        'Position_in_Range': position_in_range,\n",
    "        'Gap_Pct': gap_pct,\n",
    "        'Gap_Type': gap_type,\n",
    "        'Is_Large_Range': range_pct > range_pct.rolling(window=20).mean() + \n",
    "                         2 * range_pct.rolling(window=20).std()\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_range_indicators` function provides granular analysis of daily price ranges for NEPSE stocks, crucial for understanding intraday volatility patterns.\n",
    "\n",
    "**True Range Calculation:**\n",
    "Unlike simple High-Low range, True Range accounts for gaps between sessions. For NEPSE, which trades Sunday-Thursday, overnight news can cause significant gaps. True Range captures the maximum of:\n",
    "1. Today's high minus today's low (normal day)\n",
    "2. Absolute distance from today's high to yesterday's close (gap up and pullback)\n",
    "3. Absolute distance from yesterday's close to today's low (gap down and bounce)\n",
    "\n",
    "**Position in Range:**\n",
    "This metric normalizes the closing price within the daily range to a 0-100 scale. Values near 100 indicate strong buying pressure (closed at daily high), typical of bullish sentiment in NEPSE. Values near 0 indicate distribution (closed at low). Values around 50 suggest indecision.\n",
    "\n",
    "**Gap Analysis:**\n",
    "For NEPSE, gaps >2% often indicate significant news (quarterly results, regulatory changes, sector news). The function categorizes gaps into five types, from large down gaps (panic selling) to large up gaps (euphoria), helping identify emotional extremes in the Nepali market.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.5 Volume/Intensity Indicators**\n",
    "\n",
    "Volume indicators confirm price movements and measure the intensity of trading activity, crucial for validating trends in NEPSE's relatively thin market.\n",
    "\n",
    "### **13.5.1 Volume Ratios**\n",
    "\n",
    "Volume ratios compare current volume to historical averages to identify unusual activity.\n",
    "\n",
    "```python\n",
    "def calculate_volume_indicators(data, vol_col='Vol', close_col='Close', \n",
    "                               open_col='Open', high_col='High', low_col='Low'):\n",
    "    \"\"\"\n",
    "    Calculate volume-based indicators for NEPSE liquidity analysis.\n",
    "    \n",
    "    Volume indicators help confirm price trends and identify accumulation/distribution.\n",
    "    Critical for NEPSE due to lower liquidity compared to major exchanges.\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    # Basic volume moving averages\n",
    "    df['Vol_SMA_5'] = df[vol_col].rolling(window=5).mean()\n",
    "    df['Vol_SMA_20'] = df[vol_col].rolling(window=20).mean()\n",
    "    df['Vol_SMA_50'] = df[vol_col].rolling(window=50).mean()\n",
    "    \n",
    "    # Volume Ratio (current vs average)\n",
    "    df['Volume_Ratio'] = df[vol_col] / df['Vol_SMA_20']\n",
    "    \n",
    "    # Volume intensity classification\n",
    "    df['Volume_Intensity'] = pd.cut(df['Volume_Ratio'],\n",
    "                                   bins=[0, 0.5, 1.0, 2.0, float('inf')],\n",
    "                                   labels=['Very_Low', 'Below_Avg', 'Above_Avg', 'Very_High'])\n",
    "    \n",
    "    # On-Balance Volume (OBV) - cumulative volume flow\n",
    "    df['Price_Change'] = df[close_col].diff()\n",
    "    df['Volume_Flow'] = np.where(df['Price_Change'] > 0, df[vol_col],\n",
    "                                  np.where(df['Price_Change'] < 0, -df[vol_col], 0))\n",
    "    df['OBV'] = df['Volume_Flow'].cumsum()\n",
    "    \n",
    "    # OBV moving average for signals\n",
    "    df['OBV_SMA'] = df['OBV'].rolling(window=20).mean()\n",
    "    df['OBV_Signal'] = np.where(df['OBV'] > df['OBV_SMA'], 'Accumulation', 'Distribution')\n",
    "    \n",
    "    # Volume-Weighted Average Price (VWAP) deviation\n",
    "    # VWAP is provided in NEPSE data, calculate deviation from it\n",
    "    if 'VWAP' in df.columns:\n",
    "        df['VWAP_Deviation'] = ((df[close_col] - df['VWAP']) / df['VWAP']) * 100\n",
    "        df['Above_VWAP'] = df[close_col] > df['VWAP']  # Bullish if above VWAP\n",
    "    \n",
    "    # Money Flow Index (MFI) - volume-weighted RSI\n",
    "    typical_price = (df[high_col] + df[low_col] + df[close_col]) / 3\n",
    "    raw_money_flow = typical_price * df[vol_col]\n",
    "    \n",
    "    money_flow_positive = np.where(typical_price > typical_price.shift(1), raw_money_flow, 0)\n",
    "    money_flow_negative = np.where(typical_price < typical_price.shift(1), raw_money_flow, 0)\n",
    "    \n",
    "    money_flow_ratio = pd.Series(money_flow_positive).rolling(window=14).sum() / \\\n",
    "                     pd.Series(money_flow_negative).rolling(window=14).sum()\n",
    "    \n",
    "    df['MFI'] = 100 - (100 / (1 + money_flow_ratio))\n",
    "    \n",
    "    # Klinger Volume Oscillator (KVO)\n",
    "    # Combines volume with trend direction\n",
    "    dm = ((df[high_col] + df[low_col] + df[close_col]) / 3) - \\\n",
    "         ((df[high_col].shift(1) + df[low_col].shift(1) + df[close_col].shift(1)) / 3)\n",
    "    cm = df[vol_col] * np.where(dm > 0, 1, -1) * abs(dm)\n",
    "    \n",
    "    df['KVO'] = cm.ewm(span=34, adjust=False).mean() - cm.ewm(span=55, adjust=False).mean()\n",
    "    \n",
    "    return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_volume_indicators` function provides a comprehensive volume analysis toolkit essential for NEPSE due to the exchange's lower liquidity compared to major international markets.\n",
    "\n",
    "**Volume Ratio Analysis:**\n",
    "The `Volume_Ratio` compares current trading volume to the 20-day average. In NEPSE, volume spikes (ratio > 2.0) often indicate:\n",
    "- **Institutional Activity**: Large block trades by mutual funds or insurance companies\n",
    "- **News Impact**: Earnings releases, regulatory announcements, or sector news\n",
    "- **Breakout Confirmation**: Price breaking resistance on high volume validates the breakout\n",
    "\n",
    "**On-Balance Volume (OBV):**\n",
    "OBV is a cumulative indicator that adds volume on up days and subtracts volume on down days. For NEPSE stocks:\n",
    "- **Rising OBV**: Accumulation phase (smart money buying)\n",
    "- **Falling OBV**: Distribution phase (smart money selling)\n",
    "- **OBV Divergence**: When price makes new high but OBV doesn't, it indicates weak buying pressure and potential reversal\n",
    "\n",
    "**Money Flow Index (MFI):**\n",
    "MFI combines price and volume to measure buying/selling pressure. Unlike RSI which only looks at price, MFI weights the RSI calculation by volume. MFI > 80 indicates strong buying pressure (overbought), while MFI < 20 indicates heavy selling (oversold).\n",
    "\n",
    "**Klinger Volume Oscillator (KVO):**\n",
    "KVO uses volume force (volume  trend  absolute daily range) and calculates the difference between short-term (34-period) and long-term (55-period) EMAs of volume force. Positive KVO indicates accumulation, negative indicates distribution. Crossovers signal trend changes.\n",
    "\n",
    "### **13.5.2 Volume-Weighted Metrics**\n",
    "\n",
    "Volume-weighted metrics give more importance to periods with higher trading activity.\n",
    "\n",
    "```python\n",
    "def calculate_volume_weighted_metrics(data, vol_col='Vol', close_col='Close', \n",
    "                                     high_col='High', low_col='Low', open_col='Open'):\n",
    "    \"\"\"\n",
    "    Calculate volume-weighted price metrics for NEPSE.\n",
    "    \n",
    "    Volume-weighted metrics are more reliable than simple averages because\n",
    "    they account for where most trading activity occurred.\n",
    "    \"\"\"\n",
    "    # Volume Weighted Average Price (VWAP) - cumulative\n",
    "    # VWAP = cumulative(TP * Volume) / cumulative(Volume)\n",
    "    # where TP = (High + Low + Close) / 3\n",
    "    typical_price = (data[high_col] + data[low_col] + data[close_col]) / 3\n",
    "    tp_vol = typical_price * data[vol_col]\n",
    "    \n",
    "    cum_tp_vol = tp_vol.cumsum()\n",
    "    cum_vol = data[vol_col].cumsum()\n",
    "    \n",
    "    vwap_cumulative = cum_tp_vol / cum_vol\n",
    "    \n",
    "    # Reset VWAP periodically (e.g., monthly for NEPSE)\n",
    "    # In practice, institutional traders reset VWAP at month start\n",
    "    data['Month'] = pd.to_datetime(data.index).to_period('M')\n",
    "    monthly_vwap = data.groupby('Month').apply(\n",
    "        lambda x: (x[close_col] * x[vol_col]).cumsum() / x[vol_col].cumsum()\n",
    "    ).reset_index(level=0, drop=True)\n",
    "    \n",
    "    # Volume Weighted Moving Average (VWMA)\n",
    "    # Similar to SMA but weighted by volume\n",
    "    vwma = (data[close_col] * data[vol_col]).rolling(window=20).sum() / \\\n",
    "           data[vol_col].rolling(window=20).sum()\n",
    "    \n",
    "    # Volume Weighted Standard Deviation\n",
    "    vw_mean = vwma\n",
    "    vw_variance = ((data[close_col] - vw_mean) ** 2 * data[vol_col]).rolling(window=20).sum() / \\\n",
    "                  data[vol_col].rolling(window=20).sum()\n",
    "    vw_std = np.sqrt(vw_variance)\n",
    "    \n",
    "    # Price relative to VWAP (intraday trend strength)\n",
    "    vwap_deviation = ((data[close_col] - vwap_cumulative) / vwap_cumulative) * 100\n",
    "    \n",
    "    # Volume Profile (where did most volume trade?)\n",
    "    # Simplified: Compare current volume to VWAP distance\n",
    "    volume_profile_signal = np.where(\n",
    "        (data[vol_col] > data[vol_col].rolling(20).mean()) & \n",
    "        (abs(vwap_deviation) < 0.5),\n",
    "        'High_Vol_At_VWAP',  # Accumulation/distribution at fair value\n",
    "        'Normal'\n",
    "    )\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'VWAP_Cumulative': vwap_cumulative,\n",
    "        'VWAP_Monthly': monthly_vwap,\n",
    "        'VWMA_20': vwma,\n",
    "        'VW_StdDev': vw_std,\n",
    "        'VWAP_Deviation_Pct': vwap_deviation,\n",
    "        'Volume_Profile_Signal': volume_profile_signal,\n",
    "        'Above_VWAP': data[close_col] > vwap_cumulative\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_volume_weighted_metrics` function computes Volume Weighted Average Price (VWAP) and related metrics, which are crucial for NEPSE because they identify where the majority of trading volume occurred, indicating \"fair value\" according to market participants.\n",
    "\n",
    "**VWAP Calculation:**\n",
    "VWAP is calculated as the cumulative sum of (Typical Price  Volume) divided by cumulative volume. Typical Price = (High + Low + Close)/3 represents the \"center of gravity\" for the day. Unlike simple moving averages, VWAP gives more weight to periods with heavy institutional trading.\n",
    "\n",
    "**NEPSE-Specific Insights:**\n",
    "- **Institutional Benchmark**: In NEPSE, mutual funds and insurance companies often use VWAP as a benchmark for execution quality. Prices above VWAP suggest aggressive buying (paying premium), while prices below suggest aggressive selling.\n",
    "- **Support/Resistance**: VWAP acts as dynamic support/resistance. In uptrends, price tends to bounce off VWAP; in downtrends, rallies often fail at VWAP.\n",
    "- **Monthly Reset**: The function calculates monthly VWAP because NEPSE institutional investors often rebalance monthly. The monthly VWAP serves as a fair value anchor for the month.\n",
    "\n",
    "**Volume Profile Analysis:**\n",
    "The function identifies \"High Volume at VWAP\" dayswhen volume exceeds the 20-day average but price stays within 0.5% of VWAP. These days indicate accumulation or distribution by large players at fair value, often preceding significant moves.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.5 Volume/Intensity Indicators**\n",
    "\n",
    "### **13.5.3 On-Balance Volume (OBV)**\n",
    "\n",
    "OBV is a cumulative indicator that uses volume flow to predict changes in stock price.\n",
    "\n",
    "```python\n",
    "def calculate_obv_advanced(data, vol_col='Vol', close_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate On-Balance Volume (OBV) and derived signals for NEPSE.\n",
    "    \n",
    "    OBV Formula:\n",
    "    - If Close > Previous Close: OBV = Previous OBV + Current Volume\n",
    "    - If Close < Previous Close: OBV = Previous OBV - Current Volume\n",
    "    - If Close = Previous Close: OBV = Previous OBV\n",
    "    \n",
    "    Interpretation:\n",
    "    - Rising OBV: Accumulation (volume on up days > volume on down days)\n",
    "    - Falling OBV: Distribution (volume on down days > volume on up days)\n",
    "    - Divergence: Price rising but OBV falling = weak trend (distribution)\n",
    "    \n",
    "    NEPSE Specific:\n",
    "    - OBV breaks above 20-day MA: Institutional accumulation\n",
    "    - OBV divergence lasting >10 days: High probability reversal\n",
    "    \"\"\"\n",
    "    # Calculate price direction\n",
    "    price_change = data[close_col].diff()\n",
    "    \n",
    "    # Calculate OBV\n",
    "    obv = pd.Series(index=data.index, dtype=float)\n",
    "    obv.iloc[0] = data[vol_col].iloc[0]  # Initialize with first volume\n",
    "    \n",
    "    for i in range(1, len(data)):\n",
    "        if price_change.iloc[i] > 0:\n",
    "            obv.iloc[i] = obv.iloc[i-1] + data[vol_col].iloc[i]\n",
    "        elif price_change.iloc[i] < 0:\n",
    "            obv.iloc[i] = obv.iloc[i-1] - data[vol_col].iloc[i]\n",
    "        else:\n",
    "            obv.iloc[i] = obv.iloc[i-1]\n",
    "    \n",
    "    # OBV Moving Average\n",
    "    obv_sma = obv.rolling(window=20).mean()\n",
    "    \n",
    "    # OBV Slope (momentum of volume flow)\n",
    "    obv_slope = obv.diff(5)  # 5-period change\n",
    "    \n",
    "    # OBV Divergence detection\n",
    "    price_slope = data[close_col].diff(5)\n",
    "    \n",
    "    # Bullish divergence: Price down, OBV up (accumulation during decline)\n",
    "    bullish_div = (price_slope < 0) & (obv_slope > 0)\n",
    "    \n",
    "    # Bearish divergence: Price up, OBV down (distribution during rally)\n",
    "    bearish_div = (price_slope > 0) & (obv_slope < 0)\n",
    "    \n",
    "    # OBV Trend confirmation\n",
    "    obv_above_ma = obv > obv_sma\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'OBV': obv,\n",
    "        'OBV_SMA20': obv_sma,\n",
    "        'OBV_Slope': obv_slope,\n",
    "        'OBV_Above_MA': obv_above_ma,\n",
    "        'OBV_Bullish_Div': bullish_div,\n",
    "        'OBV_Bearish_Div': bearish_div,\n",
    "        'OBV_Signal': np.where(bullish_div, 'Accumulation',\n",
    "                              np.where(bearish_div, 'Distribution', 'Neutral'))\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_obv_advanced` function implements On-Balance Volume, a cumulative flow-of-funds indicator essential for NEPSE due to the market's susceptibility to institutional manipulation and low float scenarios.\n",
    "\n",
    "**OBV Mechanics:**\n",
    "The algorithm maintains a running total where volume is added on up-days and subtracted on down-days. The logic is simple but powerful: if price closes higher than yesterday, all of today's volume is considered \"positive volume\" (buying pressure). If price closes lower, all volume is \"negative volume\" (selling pressure).\n",
    "\n",
    "**NEPSE Institutional Flow Detection:**\n",
    "In the Nepali market, OBV is particularly valuable because:\n",
    "1. **Smart Money Tracking**: When NEPSE price is declining but OBV is rising (bullish divergence), it indicates accumulation by informed investors (promoters, institutions) who know the stock is undervalued. This often precedes positive news announcements.\n",
    "2. **Distribution Warning**: When price makes new highs but OBV fails to confirm (bearish divergence), it suggests institutional distributionlarge players selling to retail investors at peaks. This is a high-probability sell signal in NEPSE.\n",
    "3. **Trend Confirmation**: When both price and OBV are making higher highs and higher lows, the uptrend is healthy and sustained by volume.\n",
    "\n",
    "**Advanced Features:**\n",
    "The function calculates OBV slope (5-period rate of change) to measure the intensity of accumulation/distribution. It also detects divergences by comparing price slope to OBV slopewhen they move in opposite directions for 5 consecutive periods, it flags potential reversal points.\n",
    "\n",
    "### **13.5.2 Volume-Weighted Metrics**\n",
    "\n",
    "Volume-weighted metrics provide a more accurate picture of where the majority of trading occurred and at what prices.\n",
    "\n",
    "```python\n",
    "def calculate_volume_profile(data, close_col='Close', vol_col='Vol', \n",
    "                            high_col='High', low_col='Low', n_bins=10):\n",
    "    \"\"\"\n",
    "    Calculate Volume Profile for NEPSE stocks.\n",
    "    \n",
    "    Volume Profile shows how much volume was traded at each price level.\n",
    "    Helps identify:\n",
    "    - Point of Control (POC): Price level with highest volume (fair value)\n",
    "    - Value Area: Price range containing 70% of volume\n",
    "    - Support/Resistance levels based on volume clusters\n",
    "    \n",
    "    NEPSE Application:\n",
    "    - POC acts as magnet for price (gravitational pull)\n",
    "    - Breakout above value area high with volume = strong bullish\n",
    "    - Breakdown below value area low with volume = strong bearish\n",
    "    \"\"\"\n",
    "    # Create price bins (levels)\n",
    "    price_min = data[low_col].min()\n",
    "    price_max = data[high_col].max()\n",
    "    bins = np.linspace(price_min, price_max, n_bins)\n",
    "    \n",
    "    # Assign each trade to a price level (using Close as proxy)\n",
    "    data['Price_Level'] = pd.cut(data[close_col], bins=bins, labels=False)\n",
    "    \n",
    "    # Calculate volume at each level\n",
    "    volume_profile = data.groupby('Price_Level')[vol_col].sum()\n",
    "    \n",
    "    # Point of Control (price level with max volume)\n",
    "    poc_level = volume_profile.idxmax()\n",
    "    poc_price = (bins[poc_level] + bins[poc_level + 1]) / 2\n",
    "    \n",
    "    # Calculate Value Area (70% of volume)\n",
    "    total_volume = volume_profile.sum()\n",
    "    target_volume = total_volume * 0.70\n",
    "    \n",
    "    # Sort levels by volume and accumulate until 70%\n",
    "    sorted_levels = volume_profile.sort_values(ascending=False)\n",
    "    cumsum = sorted_levels.cumsum()\n",
    "    value_area_levels = sorted_levels[cumsum <= target_volume].index\n",
    "    \n",
    "    value_area_low = bins[value_area_levels.min()]\n",
    "    value_area_high = bins[value_area_levels.max() + 1]\n",
    "    \n",
    "    # Current price position relative to Value Area\n",
    "    current_price = data[close_col].iloc[-1]\n",
    "    in_value_area = (current_price >= value_area_low) and (current_price <= value_area_high)\n",
    "    above_va = current_price > value_area_high\n",
    "    below_va = current_price < value_area_low\n",
    "    \n",
    "    return {\n",
    "        'Volume_Profile': volume_profile,\n",
    "        'Point_of_Control': poc_price,\n",
    "        'Value_Area_Low': value_area_low,\n",
    "        'Value_Area_High': value_area_high,\n",
    "        'Current_Position': {\n",
    "            'Price': current_price,\n",
    "            'In_Value_Area': in_value_area,\n",
    "            'Above_VA': above_va,\n",
    "            'Below_VA': below_va,\n",
    "            'Distance_from_POC': ((current_price - poc_price) / poc_price) * 100\n",
    "        }\n",
    "    }\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_volume_profile` function implements Volume Profile Analysis, a sophisticated technique that reveals where the majority of trading activity (volume) occurred at specific price levels, rather than just when.\n",
    "\n",
    "**Conceptual Framework:**\n",
    "Unlike time-based indicators that show what happened when, Volume Profile shows what happened where. It divides the price range into bins (levels) and sums the volume that occurred at each level. This creates a histogram showing \"fair value\" areas where buyers and sellers agreed most frequently.\n",
    "\n",
    "**Key Components:**\n",
    "1. **Point of Control (POC)**: The price level with the highest traded volume. In NEPSE, the POC acts as a \"magnet\" or gravitational center for price. When price moves away from POC but lacks volume support, it tends to revert back to this level. Institutional traders in Nepal often use POC as a reference for \"fair value\" entries.\n",
    "\n",
    "2. **Value Area (VA)**: The price range containing approximately 70% of total volume (1 standard deviation in volume distribution). The Value Area High (VAH) and Value Area Low (VAL) act as dynamic support and resistance levels. For NEPSE:\n",
    "   - Breakout above VAH with volume = Strong bullish, target next resistance\n",
    "   - Breakdown below VAL with volume = Strong bearish, target next support\n",
    "   - Price inside VA = Balanced, fair value, range-bound strategies work best\n",
    "\n",
    "3. **Volume Profile Shape**: The distribution shape reveals market structure:\n",
    "   - **D-shaped (bell curve)**: Balanced market, good for mean reversion\n",
    "   - **P-shaped (high volume at bottom)**: Accumulation, bullish\n",
    "   - **b-shaped (high volume at top)**: Distribution, bearish\n",
    "   - **Thin profile**: Low liquidity, avoid large positions\n",
    "\n",
    "**NEPSE Application:**\n",
    "For Nepali stocks, Volume Profile is particularly valuable because:\n",
    "- **Low Float Awareness**: Many NEPSE stocks have low free float. Volume Profile shows where major holders accumulated (thick volume areas) versus thin air (gaps) where price can move fast.\n",
    "- **Institutional Footprints**: Large volume nodes often represent institutional accumulation zones. When price returns to these levels, institutions often defend their positions, creating support.\n",
    "- **Breakout Validation**: In NEPSE, false breakouts are common. A breakout is only valid if price moves out of the Value Area on volume > 2x average, indicating genuine conviction rather than manipulation.\n",
    "\n",
    "### **13.5.3 On-Balance Volume (OBV)**\n",
    "\n",
    "OBV was covered in section 13.5.1. It serves as the primary volume trend indicator, cumulatively tracking whether volume is flowing into or out of a NEPSE stock.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.6 Position and Rank Indicators**\n",
    "\n",
    "Position and rank indicators normalize price data to statistical distributions, enabling comparison across different stocks and time periods regardless of absolute price levels.\n",
    "\n",
    "### **13.6.1 Percentile Ranks**\n",
    "\n",
    "Percentile ranks indicate where the current price stands relative to its historical range.\n",
    "\n",
    "```python\n",
    "def calculate_percentile_ranks(data, windows=[20, 50, 200], price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate percentile ranks for NEPSE stocks.\n",
    "    \n",
    "    Percentile Rank = (Number of values below current) / Total values * 100\n",
    "    \n",
    "    Interpretation:\n",
    "    - 0-20: Bottom quintile (oversold, potential support)\n",
    "    - 20-40: Lower quartile (weak)\n",
    "    - 40-60: Middle (fair value)\n",
    "    - 60-80: Upper quartile (strong)\n",
    "    - 80-100: Top quintile (overbought, potential resistance)\n",
    "    \n",
    "    NEPSE Use:\n",
    "    - Compare different stocks regardless of price level\n",
    "    - Identify extreme readings for mean reversion\n",
    "    - Sector rotation analysis (which sectors in top/bottom percentiles)\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    \n",
    "    for window in windows:\n",
    "        # Calculate rolling percentile rank\n",
    "        # Using scipy's percentileofscore for exact calculation\n",
    "        from scipy.stats import percentileofscore\n",
    "        \n",
    "        def rolling_percentile(x):\n",
    "            if len(x) < window:\n",
    "                return np.nan\n",
    "            return percentileofscore(x[:-1], x[-1], kind='rank')\n",
    "        \n",
    "        df[f'Percentile_{window}'] = df[price_col].rolling(window=window).apply(\n",
    "            rolling_percentile, raw=True\n",
    "        )\n",
    "        \n",
    "        # Alternative: Simple min-max scaling (0-100) within window\n",
    "        rolling_min = df[price_col].rolling(window=window).min()\n",
    "        rolling_max = df[price_col].rolling(window=window).max()\n",
    "        df[f'Position_Score_{window}'] = ((df[price_col] - rolling_min) / \n",
    "                                           (rolling_max - rolling_min)) * 100\n",
    "    \n",
    "    # Composite percentile (average of different timeframes)\n",
    "    df['Composite_Percentile'] = df[[f'Percentile_{w}' for w in windows]].mean(axis=1)\n",
    "    \n",
    "    # Percentile-based signals\n",
    "    df['Extreme_Low'] = df['Composite_Percentile'] < 10   # Bottom decile\n",
    "    df['Extreme_High'] = df['Composite_Percentile'] > 90  # Top decile\n",
    "    df['Fair_Value'] = (df['Composite_Percentile'] > 40) & (df['Composite_Percentile'] < 60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def calculate_percentile_divergence(data, price_col='Close', vol_col='Vol'):\n",
    "    \"\"\"\n",
    "    Detect divergences between price percentiles and volume percentiles.\n",
    "    \n",
    "    Bullish: Price in low percentile but volume in high percentile (accumulation)\n",
    "    Bearish: Price in high percentile but volume in high percentile (distribution)\n",
    "    \"\"\"\n",
    "    # Calculate rolling percentiles for price and volume\n",
    "    price_pct = data[price_col].rolling(20).apply(\n",
    "        lambda x: percentileofscore(x[:-1], x[-1]), raw=True\n",
    "    )\n",
    "    vol_pct = data[vol_col].rolling(20).apply(\n",
    "        lambda x: percentileofscore(x[:-1], x[-1]), raw=True\n",
    "    )\n",
    "    \n",
    "    # Divergence detection\n",
    "    bullish_div = (price_pct < 30) & (vol_pct > 70)  # Low price, high volume\n",
    "    bearish_div = (price_pct > 70) & (vol_pct > 70)  # High price, high volume\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Price_Percentile': price_pct,\n",
    "        'Volume_Percentile': vol_pct,\n",
    "        'Bullish_Divergence': bullish_div,\n",
    "        'Bearish_Divergence': bearish_div\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_percentile_ranks` function transforms absolute price levels into relative rankings within historical windows, solving a critical problem in NEPSE analysis: comparing stocks with different price levels (e.g., a Rs. 10,000 stock vs a Rs. 100 stock).\n",
    "\n",
    "**Percentile Calculation:**\n",
    "Using `percentileofscore` from scipy, the function calculates what percentage of historical prices (over the lookback window) are below the current price. A 90th percentile reading means the stock is trading higher than 90% of the past 20 (or 50, or 200) daysstatistically overbought.\n",
    "\n",
    "**NEPSE Multi-Timeframe Analysis:**\n",
    "The function calculates percentiles across three timeframes:\n",
    "- **20-day**: Short-term overbought/oversold (trading)\n",
    "- **50-day**: Medium-term positioning (swing trading)\n",
    "- **200-day**: Long-term secular trends (investing)\n",
    "\n",
    "The Composite Percentile averages these, providing a consensus view of where the stock stands across all time horizons.\n",
    "\n",
    "**Divergence Analysis:**\n",
    "The `calculate_percentile_divergence` function identifies smart money activity by comparing price percentiles to volume percentiles. In NEPSE:\n",
    "- **Bullish Divergence**: Price in bottom 30% (appears weak) but volume in top 30% (heavy accumulation). This suggests institutions are quietly accumulating shares without driving prices upoften precedes major rallies.\n",
    "- **Bearish Divergence**: Price in top 30% (appears strong) with volume in top 30% (heavy distribution). Indicates institutions selling to retail investors at peaksoften marks tops.\n",
    "\n",
    "### **13.6.2 Position in Range**\n",
    "\n",
    "Position in Range (also known as Stochastic Oscillator) measures where the current price sits relative to the recent high-low range.\n",
    "\n",
    "```python\n",
    "def calculate_stochastic_indicators(data, k_window=14, d_window=3, \n",
    "                                   high_col='High', low_col='Low', close_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Stochastic Oscillator (%K and %D) for NEPSE momentum.\n",
    "    \n",
    "    %K = (Current Close - Lowest Low) / (Highest High - Lowest Low) * 100\n",
    "    %D = 3-day SMA of %K (signal line)\n",
    "    \n",
    "    Interpretation:\n",
    "    - >80: Overbought (price near top of range)\n",
    "    - <20: Oversold (price near bottom of range)\n",
    "    - %K crossing above %D: Buy signal\n",
    "    - %K crossing below %D: Sell signal\n",
    "    \n",
    "    NEPSE Specific:\n",
    "    - Use 14-period for weekly analysis (2 weeks of NEPSE trading)\n",
    "    - Divergences highly predictive due to lower algorithmic trading\n",
    "    \"\"\"\n",
    "    # Calculate %K (Fast Stochastic)\n",
    "    lowest_low = data[low_col].rolling(window=k_window).min()\n",
    "    highest_high = data[high_col].rolling(window=k_window).max()\n",
    "    \n",
    "    # Handle case where high == low (division by zero)\n",
    "    range_hl = highest_high - lowest_low\n",
    "    range_hl = range_hl.replace(0, np.nan)  # Avoid division by zero\n",
    "    \n",
    "    pct_k = ((data[close_col] - lowest_low) / range_hl) * 100\n",
    "    \n",
    "    # Calculate %D (Slow Stochastic) - signal line\n",
    "    pct_d = pct_k.rolling(window=d_window).mean()\n",
    "    \n",
    "    # Slow Stochastic (smoothed %K)\n",
    "    pct_k_slow = pct_k.rolling(window=3).mean()\n",
    "    pct_d_slow = pct_k_slow.rolling(window=d_window).mean()\n",
    "    \n",
    "    # Stochastic RSI (Stochastics applied to RSI)\n",
    "    rsi = calculate_rsi(data, window=14)  # From previous section\n",
    "    rsi_low = rsi.rolling(window=k_window).min()\n",
    "    rsi_high = rsi.rolling(window=k_window).max()\n",
    "    stoch_rsi = ((rsi - rsi_low) / (rsi_high - rsi_low)) * 100\n",
    "    \n",
    "    # Signal generation\n",
    "    overbought = pct_k > 80\n",
    "    oversold = pct_k < 20\n",
    "    \n",
    "    # Crossovers\n",
    "    k_cross_above_d = (pct_k > pct_d) & (pct_k.shift(1) <= pct_d.shift(1))\n",
    "    k_cross_below_d = (pct_k < pct_d) & (pct_k.shift(1) >= pct_d.shift(1))\n",
    "    \n",
    "    # Divergences (price vs stochastic)\n",
    "    price_high = data[close_col].rolling(window=5).max() == data[close_col]\n",
    "    price_low = data[close_col].rolling(window=5).min() == data[close_col]\n",
    "    \n",
    "    stoch_high = pct_k.rolling(window=5).max() == pct_k\n",
    "    stoch_low = pct_k.rolling(window=5).min() == pct_k\n",
    "    \n",
    "    # Bearish divergence: Price higher high, Stoch lower high\n",
    "    bearish_div = price_high & (pct_k < pct_k.shift(5))\n",
    "    \n",
    "    # Bullish divergence: Price lower low, Stoch higher low\n",
    "    bullish_div = price_low & (pct_k > pct_k.shift(5))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Stoch_K': pct_k,\n",
    "        'Stoch_D': pct_d,\n",
    "        'Stoch_K_Slow': pct_k_slow,\n",
    "        'Stoch_D_Slow': pct_d_slow,\n",
    "        'Stoch_RSI': stoch_rsi,\n",
    "        'Stoch_Overbought': overbought,\n",
    "        'Stoch_Oversold': oversold,\n",
    "        'Stoch_Buy_Signal': k_cross_above_d & oversold,\n",
    "        'Stoch_Sell_Signal': k_cross_below_d & overbought,\n",
    "        'Stoch_Bullish_Div': bullish_div,\n",
    "        'Stoch_Bearish_Div': bearish_div\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_stochastic_indicators` function implements the Stochastic Oscillator family, including Fast Stochastic (%K), Slow Stochastic (%D), and Stochastic RSI. These indicators are particularly effective for NEPSE because they normalize price action to a 0-100 scale regardless of the stock's absolute price, enabling comparison across the diverse price ranges in the Nepali market (from Rs. 10 to Rs. 10,000+).\n",
    "\n",
    "**%K Calculation:**\n",
    "The raw Stochastic (%K) measures where the current close sits within the recent high-low range:\n",
    "$$\\%K = \\frac{Close - Lowest Low}{Highest High - Lowest Low} \\times 100$$\n",
    "\n",
    "A reading of 90 means the stock closed in the top 10% of the 14-day range (overbought). A reading of 10 means it closed in the bottom 10% (oversold).\n",
    "\n",
    "**Signal Generation:**\n",
    "The function generates buy signals when %K crosses above %D (signal line) while in oversold territory (<20), indicating the first sign of buying pressure after a decline. Sell signals occur when %K crosses below %D in overbought territory (>80).\n",
    "\n",
    "**Divergence Detection:**\n",
    "For NEPSE specifically, the function detects divergences by comparing price peaks/troughs to Stochastic peaks/troughs over 5-day windows. Bearish divergence (price higher high, Stochastic lower high) is a reliable reversal warning in Nepali stocks because it indicates weakening buying pressure despite higher pricesoften seen when retail investors chase highs while institutions distribute.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.6 Position and Rank Indicators**\n",
    "\n",
    "### **13.6.3 Z-Scores**\n",
    "\n",
    "Z-Scores standardize data points to indicate how many standard deviations they are from the mean, enabling statistical arbitrage and mean reversion strategies.\n",
    "\n",
    "```python\n",
    "def calculate_zscore_indicators(data, windows=[20, 50, 200], price_col='Close'):\n",
    "    \"\"\"\n",
    "    Calculate Z-Scores for statistical mean reversion analysis in NEPSE.\n",
    "    \n",
    "    Z-Score = (Current Price - Mean) / Standard Deviation\n",
    "    \n",
    "    Interpretation:\n",
    "    - |Z| < 1: Within normal range (68% of data)\n",
    "    - 1 < |Z| < 2: Moderate deviation (27% of data)\n",
    "    - |Z| > 2: Extreme deviation (5% of data) - mean reversion candidate\n",
    "    \n",
    "    NEPSE Strategy:\n",
    "    - Z > 2: Sell (price statistically extended, likely to revert)\n",
    "    - Z < -2: Buy (price statistically depressed, likely to bounce)\n",
    "    - Z crossing 0: Trend confirmation (momentum shift)\n",
    "    \"\"\"\n",
    "    df = data.copy()\n",
    "    results = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    for window in windows:\n",
    "        # Calculate rolling mean and std\n",
    "        rolling_mean = df[price_col].rolling(window=window).mean()\n",
    "        rolling_std = df[price_col].rolling(window=window).std()\n",
    "        \n",
    "        # Calculate Z-Score\n",
    "        zscore = (df[price_col] - rolling_mean) / rolling_std\n",
    "        \n",
    "        results[f'ZScore_{window}'] = zscore\n",
    "        \n",
    "        # Statistical thresholds\n",
    "        results[f'ZScore_Extreme_High_{window}'] = zscore > 2\n",
    "        results[f'ZScore_Extreme_Low_{window}'] = zscore < -2\n",
    "        results[f'ZScore_Normal_{window}'] = abs(zscore) < 1\n",
    "        \n",
    "        # Mean reversion signals\n",
    "        results[f'ZScore_MeanRev_Buy_{window}'] = (zscore < -2) & (zscore.diff() > 0)\n",
    "        results[f'ZScore_MeanRev_Sell_{window}'] = (zscore > 2) & (zscore.diff() < 0)\n",
    "    \n",
    "    # Composite Z-Score (average of multiple timeframes)\n",
    "    zscore_cols = [f'ZScore_{w}' for w in windows]\n",
    "    results['ZScore_Composite'] = results[zscore_cols].mean(axis=1)\n",
    "    \n",
    "    # Z-Score momentum (rate of change of Z-Score)\n",
    "    results['ZScore_Momentum'] = results['ZScore_Composite'].diff(5)\n",
    "    \n",
    "    # Z-Score trend (is it reverting or extending?)\n",
    "    results['ZScore_Trend'] = np.where(\n",
    "        results['ZScore_Composite'] > 1.5, 'Overbought',\n",
    "        np.where(results['ZScore_Composite'] < -1.5, 'Oversold', 'Neutral')\n",
    "    )\n",
    "    \n",
    "    return results\n",
    "\n",
    "def calculate_zscore_divergence(data, price_col='Close', window=20):\n",
    "    \"\"\"\n",
    "    Detect divergences between price and its Z-Score for NEPSE.\n",
    "    \n",
    "    Normal divergence: Price makes new high, Z-Score makes lower high\n",
    "    (Price extending but statistical momentum weakening)\n",
    "    \"\"\"\n",
    "    zscore = calculate_zscore_indicators(data, windows=[window])\n",
    "    zscore_col = f'ZScore_{window}'\n",
    "    \n",
    "    # Find peaks and troughs\n",
    "    price_high = data[price_col] == data[price_col].rolling(window=5, center=True).max()\n",
    "    price_low = data[price_col] == data[price_col].rolling(window=5, center=True).min()\n",
    "    \n",
    "    zscore_high = zscore[zscore_col] == zscore[zscore_col].rolling(window=5, center=True).max()\n",
    "    zscore_low = zscore[zscore_col] == zscore[zscore_col].rolling(window=5, center=True).min()\n",
    "    \n",
    "    # Divergences\n",
    "    bearish_div = price_high & (zscore[zscore_col] < zscore[zscore_col].shift(5))\n",
    "    bullish_div = price_low & (zscore[zscore_col] > zscore[zscore_col].shift(5))\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'ZScore': zscore[zscore_col],\n",
    "        'Bearish_Divergence': bearish_div,\n",
    "        'Bullish_Divergence': bullish_div,\n",
    "        'Is_Peak': price_high,\n",
    "        'Is_Trough': price_low\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_zscore_indicators` function standardizes price data to a normal distribution with mean 0 and standard deviation 1, enabling statistical arbitrage strategies in NEPSE.\n",
    "\n",
    "**Statistical Foundation:**\n",
    "The Z-Score formula $Z = \\frac{X - \\mu}{\\sigma}$ transforms raw prices into standard deviations from the mean. In a normal distribution:\n",
    "- 68% of data falls within $|Z| < 1$\n",
    "- 95% within $|Z| < 2$\n",
    "- 99.7% within $|Z| < 3$\n",
    "\n",
    "**NEPSE Mean Reversion Strategy:**\n",
    "For Nepali stocks, which often exhibit mean-reverting behavior due to limited float and retail participation, extreme Z-Scores present high-probability trading opportunities:\n",
    "- **Z < -2**: Price is 2+ standard deviations below the mean. Statistically, there's a 95% probability it will revert toward the mean within the lookback window. This is a high-probability buy signal for NEPSE mean reversion strategies.\n",
    "- **Z > +2**: Price is 2+ standard deviations above the mean. 95% probability of reversion. Sell signal or short opportunity (if available).\n",
    "\n",
    "**Multi-Timeframe Composite:**\n",
    "The function calculates Z-Scores across 20, 50, and 200-day windows, then averages them into a Composite Z-Score. This prevents false signals from single timeframes. For example, a stock might be overbought short-term (20-day Z > 2) but oversold long-term (200-day Z < -1). The composite provides a balanced view.\n",
    "\n",
    "**Z-Score Momentum:**\n",
    "The rate of change of the Z-Score (`ZScore_Momentum`) indicates whether the stock is accelerating toward extremes (momentum increasing) or reverting toward mean (momentum decreasing). Positive momentum while Z > 1.5 indicates strong trend continuation; negative momentum while Z > 1.5 indicates trend exhaustion.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.7 Cross-Domain Application**\n",
    "\n",
    "While this chapter focuses on NEPSE financial data, the indicator engineering principles apply across domains:\n",
    "\n",
    "**Weather Forecasting:**\n",
    "- **Trend Indicators**: Moving averages of temperature to detect climate change trends\n",
    "- **Momentum**: Rate of change in atmospheric pressure to predict storm intensity\n",
    "- **Volatility**: Standard deviation of temperature (weather instability)\n",
    "\n",
    "**Healthcare (Patient Monitoring):**\n",
    "- **Trend**: Moving average of heart rate to detect gradual deterioration\n",
    "- **Momentum**: Rate of change in blood pressure for acute event detection\n",
    "- **Z-Scores**: Standardized vital signs to detect anomalies across different patient baselines\n",
    "\n",
    "**IoT/Sensor Data:**\n",
    "- **Trend**: SMA of vibration sensors to detect bearing wear in machinery\n",
    "- **Volatility**: ATR equivalent for sensor range to detect instability\n",
    "- **Volume**: Event frequency (counts) as intensity indicator\n",
    "\n",
    "The mathematical foundations (rolling windows, standardization, momentum calculations) remain consistent; only the interpretation changes based on domain context.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.8 Indicator Computation Libraries**\n",
    "\n",
    "For production NEPSE prediction systems, use optimized libraries rather than custom implementations:\n",
    "\n",
    "```python\n",
    "# Recommended libraries for NEPSE indicator calculation\n",
    "\n",
    "# 1. TA-Lib (Technical Analysis Library)\n",
    "# Fast C implementation, industry standard\n",
    "import talib\n",
    "\n",
    "def calculate_with_talib(df):\n",
    "    \"\"\"Using TA-Lib for production NEPSE systems\"\"\"\n",
    "    # Moving Averages\n",
    "    sma20 = talib.SMA(df['Close'], timeperiod=20)\n",
    "    ema12 = talib.EMA(df['Close'], timeperiod=12)\n",
    "    ema26 = talib.EMA(df['Close'], timeperiod=26)\n",
    "    \n",
    "    # Momentum\n",
    "    rsi = talib.RSI(df['Close'], timeperiod=14)\n",
    "    macd, macd_signal, macd_hist = talib.MACD(\n",
    "        df['Close'], fastperiod=12, slowperiod=26, signalperiod=9\n",
    "    )\n",
    "    \n",
    "    # Volatility\n",
    "    upper, middle, lower = talib.BBANDS(\n",
    "        df['Close'], timeperiod=20, nbdevup=2, nbdevdn=2\n",
    "    )\n",
    "    atr = talib.ATR(df['High'], df['Low'], df['Close'], timeperiod=14)\n",
    "    \n",
    "    # Volume\n",
    "    obv = talib.OBV(df['Close'], df['Vol'])\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'SMA_20': sma20, 'EMA_12': ema12, 'EMA_26': ema26,\n",
    "        'RSI': rsi, 'MACD': macd, 'MACD_Signal': macd_signal,\n",
    "        'BB_Upper': upper, 'BB_Lower': lower, 'ATR': atr, 'OBV': obv\n",
    "    })\n",
    "\n",
    "# 2. pandas-ta (Pure Python, extensive indicator library)\n",
    "import pandas_ta as ta\n",
    "\n",
    "def calculate_with_pandas_ta(df):\n",
    "    \"\"\"Using pandas-ta for flexible NEPSE analysis\"\"\"\n",
    "    # Add all common indicators at once\n",
    "    df.ta.rsi(length=14, append=True)\n",
    "    df.ta.macd(fast=12, slow=26, signal=9, append=True)\n",
    "    df.ta.bbands(length=20, std=2, append=True)\n",
    "    df.ta.atr(length=14, append=True)\n",
    "    df.ta.obv(append=True)\n",
    "    df.ta.adx(length=14, append=True)\n",
    "    df.ta.stoch(k=14, d=3, append=True)\n",
    "    \n",
    "    # Custom strategy\n",
    "    my_strategy = ta.Strategy(\n",
    "        name=\"NEPSE Momentum\",\n",
    "        description=\"SMA, RSI, MACD, and BBands\",\n",
    "        ta=[\n",
    "            {\"kind\": \"sma\", \"length\": 20},\n",
    "            {\"kind\": \"sma\", \"length\": 50},\n",
    "            {\"kind\": \"rsi\", \"length\": 14},\n",
    "            {\"kind\": \"macd\", \"fast\": 12, \"slow\": 26},\n",
    "            {\"kind\": \"bbands\", \"length\": 20},\n",
    "        ],\n",
    "    )\n",
    "    \n",
    "    df.ta.strategy(my_strategy)\n",
    "    return df\n",
    "\n",
    "# 3. Custom optimized functions for production\n",
    "# Use Numba for JIT compilation if performance critical\n",
    "from numba import jit\n",
    "\n",
    "@jit(nopython=True)\n",
    "def fast_sma(prices, window):\n",
    "    \"\"\"Numba-accelerated SMA for high-frequency NEPSE data\"\"\"\n",
    "    n = len(prices)\n",
    "    result = np.empty(n)\n",
    "    result[:window-1] = np.nan\n",
    "    \n",
    "    # First value\n",
    "    result[window-1] = np.mean(prices[:window])\n",
    "    \n",
    "    # Efficient rolling calculation\n",
    "    for i in range(window, n):\n",
    "        result[i] = result[i-1] + (prices[i] - prices[i-window]) / window\n",
    "    \n",
    "    return result\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section provides production-ready implementations using industry-standard libraries. For NEPSE prediction systems handling large datasets or real-time streaming, performance optimization is crucial.\n",
    "\n",
    "**TA-Lib (Technical Analysis Library):**\n",
    "TA-Lib is the industry standard written in C, offering 200+ indicators with optimized performance. For NEPSE production systems, TA-Lib is preferred because:\n",
    "- **Speed**: C implementation is 100x faster than pure Python\n",
    "- **Accuracy**: Uses established financial formulas (Wilder's smoothing, etc.)\n",
    "- **Memory Efficiency**: Processes data in C arrays, not Python objects\n",
    "\n",
    "The example shows how to calculate core NEPSE indicators: SMA, EMA, RSI, MACD, Bollinger Bands, ATR, and OBV in a single efficient pass.\n",
    "\n",
    "**pandas-ta:**\n",
    "A pure Python alternative offering 130+ indicators with a pandas-native interface. Advantages for NEPSE analysis:\n",
    "- **Flexibility**: Easy to customize parameters for Nepali market conditions\n",
    "- **Strategy Objects**: Can bundle multiple indicators into reusable strategies\n",
    "- **Pandas Integration**: Returns DataFrames that integrate seamlessly with existing NEPSE data pipelines\n",
    "\n",
    "**Numba Optimization:**\n",
    "For high-frequency NEPSE data (tick data or 1-minute bars), the `@jit` decorator compiles Python functions to machine code at runtime. The `fast_sma` example shows a rolling mean calculation optimized for speedcritical when processing thousands of NEPSE stocks in real-time.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.9 Indicator Selection Framework**\n",
    "\n",
    "With hundreds of indicators available, selecting the right ones for NEPSE prediction requires a systematic framework.\n",
    "\n",
    "```python\n",
    "class NEPSEIndicatorSelector:\n",
    "    \"\"\"\n",
    "    Framework for selecting optimal indicators for NEPSE prediction models.\n",
    "    \n",
    "    Selection Criteria:\n",
    "    1. Predictive Power: Correlation with future returns\n",
    "    2. Non-Redundancy: Low correlation with other selected indicators\n",
    "    3. Robustness: Performance across different market regimes\n",
    "    4. Computational Efficiency: Calculation speed for real-time use\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        self.indicators = {}\n",
    "        self.performance_metrics = {}\n",
    "        \n",
    "    def generate_candidate_indicators(self):\n",
    "        \"\"\"Generate comprehensive set of indicators for NEPSE\"\"\"\n",
    "        # Trend indicators\n",
    "        self.indicators['SMA_20'] = self.data['Close'].rolling(20).mean()\n",
    "        self.indicators['EMA_12'] = self.data['Close'].ewm(span=12).mean()\n",
    "        self.indicators['ADX'] = calculate_adx(self.data)['ADX']\n",
    "        \n",
    "        # Momentum\n",
    "        self.indicators['RSI'] = calculate_rsi(self.data)\n",
    "        self.indicators['MACD'] = calculate_macd(self.data)['MACD']\n",
    "        self.indicators['ROC'] = calculate_roc(self.data)\n",
    "        \n",
    "        # Volatility\n",
    "        self.indicators['ATR'] = calculate_atr(self.data)['ATR']\n",
    "        self.indicators['BB_Width'] = calculate_bollinger_bands(self.data)['BB_Width']\n",
    "        \n",
    "        # Volume\n",
    "        self.indicators['OBV'] = calculate_volume_indicators(self.data)['OBV']\n",
    "        self.indicators['Volume_Ratio'] = calculate_volume_indicators(self.data)['Volume_Ratio']\n",
    "        \n",
    "        # Position\n",
    "        self.indicators['Percentile_20'] = calculate_percentile_ranks(self.data, windows=[20])['Percentile_20']\n",
    "        self.indicators['Stoch_K'] = calculate_stochastic_indicators(self.data)['Stoch_K']\n",
    "        \n",
    "        return pd.DataFrame(self.indicators)\n",
    "    \n",
    "    def evaluate_predictive_power(self, indicator_df, forward_periods=[1, 5, 10]):\n",
    "        \"\"\"\n",
    "        Evaluate how well each indicator predicts future returns.\n",
    "        \n",
    "        Uses Spearman correlation (rank correlation) to handle non-linear relationships.\n",
    "        \"\"\"\n",
    "        future_returns = {}\n",
    "        \n",
    "        for period in forward_periods:\n",
    "            future_returns[f'Return_{period}d'] = self.data['Close'].pct_change(period).shift(-period)\n",
    "        \n",
    "        returns_df = pd.DataFrame(future_returns)\n",
    "        \n",
    "        correlations = {}\n",
    "        for col in indicator_df.columns:\n",
    "            if indicator_df[col].isna().all():\n",
    "                continue\n",
    "                \n",
    "            correlations[col] = {}\n",
    "            for ret_col in returns_df.columns:\n",
    "                # Spearman correlation (monotonic relationships)\n",
    "                corr = indicator_df[col].corr(returns_df[ret_col], method='spearman')\n",
    "                correlations[col][ret_col] = corr\n",
    "        \n",
    "        return pd.DataFrame(correlations).T\n",
    "    \n",
    "    def select_non_redundant_indicators(self, indicator_df, correlation_threshold=0.85):\n",
    "        \"\"\"\n",
    "        Remove highly correlated indicators to avoid multicollinearity.\n",
    "        \n",
    "        Uses hierarchical clustering to group similar indicators and select\n",
    "        the best representative from each group.\n",
    "        \"\"\"\n",
    "        from scipy.cluster import hierarchy\n",
    "        from scipy.spatial.distance import squareform\n",
    "        \n",
    "        # Calculate correlation matrix\n",
    "        corr_matrix = indicator_df.corr().abs()\n",
    "        \n",
    "        # Convert to distance matrix\n",
    "        distance_matrix = 1 - corr_matrix\n",
    "        \n",
    "        # Hierarchical clustering\n",
    "        linkage = hierarchy.linkage(squareform(distance_matrix), method='average')\n",
    "        \n",
    "        # Form flat clusters\n",
    "        clusters = hierarchy.fcluster(linkage, t=correlation_threshold, criterion='distance')\n",
    "        \n",
    "        # Select one indicator per cluster (highest predictive power)\n",
    "        selected_indicators = []\n",
    "        for cluster_id in np.unique(clusters):\n",
    "            cluster_indicators = indicator_df.columns[clusters == cluster_id]\n",
    "            # Select the one with highest variance (most information)\n",
    "            variances = indicator_df[cluster_indicators].var()\n",
    "            selected = variances.idxmax()\n",
    "            selected_indicators.append(selected)\n",
    "        \n",
    "        return selected_indicators, clusters\n",
    "    \n",
    "    def get_optimal_indicator_set(self):\n",
    "        \"\"\"Execute full selection pipeline\"\"\"\n",
    "        # Generate all candidates\n",
    "        indicator_df = self.generate_candidate_indicators()\n",
    "        \n",
    "        # Evaluate predictive power\n",
    "        predictive_power = self.evaluate_predictive_power(indicator_df)\n",
    "        \n",
    "        # Remove redundant indicators\n",
    "        selected, clusters = self.select_non_redundant_indicators(indicator_df)\n",
    "        \n",
    "        return {\n",
    "            'selected_indicators': selected,\n",
    "            'predictive_power': predictive_power,\n",
    "            'clusters': clusters,\n",
    "            'indicator_data': indicator_df[selected]\n",
    "        }\n",
    "\n",
    "# Usage for NEPSE:\n",
    "# selector = NEPSEIndicatorSelector(nepse_df)\n",
    "# optimal_indicators = selector.get_optimal_indicator_set()\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `NEPSEIndicatorSelector` class provides a systematic framework for selecting the optimal subset of technical indicators for machine learning models, addressing the curse of dimensionality and multicollinearity common in financial prediction.\n",
    "\n",
    "**Phase 1: Candidate Generation**\n",
    "The framework generates 20+ standard indicators covering all categories (trend, momentum, volatility, volume, position). This ensures comprehensive coverage of potential predictive signals in NEPSE data.\n",
    "\n",
    "**Phase 2: Predictive Power Evaluation**\n",
    "Using Spearman correlation (which captures monotonic but non-linear relationships), the framework tests each indicator's correlation with forward returns (1, 5, and 10 days ahead). Spearman is preferred over Pearson because financial relationships are rarely linear but often rank-correlated. For example, extremely high RSI (>80) may consistently predict negative returns, but the relationship isn't linear across all RSI values.\n",
    "\n",
    "**Phase 3: Non-Redundancy Selection**\n",
    "Financial indicators are highly correlated (e.g., RSI and Stochastic often move together). Including all of them in a model creates multicollinearity, making coefficients unstable and reducing generalization. The framework uses hierarchical clustering on the correlation matrix to group similar indicators, then selects the highest-variance (most informative) member from each cluster. This ensures the final indicator set captures diverse aspects of market behavior without redundancy.\n",
    "\n",
    "**NEPSE Optimization:**\n",
    "For Nepali stocks, this selection process typically retains:\n",
    "- **Trend**: ADX (trend strength) rather than multiple MAs\n",
    "- **Momentum**: RSI (most robust) rather than Stochastic (noisy in low liquidity)\n",
    "- **Volatility**: ATR (used for position sizing) rather than Bollinger Width\n",
    "- **Volume**: Volume Ratio (simple but effective) rather than complex OBV derivatives\n",
    "\n",
    "This optimized set of 4-6 indicators provides maximum predictive power with minimum computational overhead and collinearity for NEPSE prediction models.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.7 Cross-Domain Application**\n",
    "\n",
    "While this chapter focuses on NEPSE financial data, the indicator engineering principles apply universally:\n",
    "\n",
    "**Retail Demand Forecasting:**\n",
    "- **Trend Indicators**: 7-day and 30-day moving averages of sales to detect seasonal trends\n",
    "- **Momentum**: Rate of change in foot traffic to predict inventory needs\n",
    "- **Volatility**: Standard deviation of daily sales to assess business risk\n",
    "- **Volume**: Transaction counts confirming price trends (average ticket size)\n",
    "\n",
    "**Energy Consumption:**\n",
    "- **Trend**: Moving averages of kWh usage to detect efficiency improvements\n",
    "- **Momentum**: Acceleration in consumption indicating equipment malfunction\n",
    "- **Z-Scores**: Statistical detection of anomalous consumption spikes\n",
    "\n",
    "**Manufacturing/IoT:**\n",
    "- **Bollinger Bands**: Sensor readings to detect when machines drift out of spec\n",
    "- **RSI**: Vibration sensor momentum to predict bearing failure\n",
    "- **ATR**: Range of temperature fluctuations indicating system instability\n",
    "\n",
    "The mathematical transformations (smoothing, normalization, rate-of-change, standardization) are domain-agnostic and form the foundation of time-series feature engineering across all industries.\n",
    "\n",
    "---\n",
    "\n",
    "## **13.8 Indicator Computation Libraries**\n",
    "\n",
    "For production NEPSE systems, leverage these optimized libraries:\n",
    "\n",
    "1. **TA-Lib**: C-based, 200+ indicators, fastest execution\n",
    "2. **pandas-ta**: Pure Python, 130+ indicators, pandas integration\n",
    "3. **ta**: Lightweight Python technical analysis library\n",
    "4. **Numba**: JIT compilation for custom indicator acceleration\n",
    "5. **Dask**: Parallel computation for multi-stock NEPSE screening\n",
    "\n",
    "---\n",
    "\n",
    "## **13.9 Indicator Selection Framework**\n",
    "\n",
    "(See detailed implementation in section 13.6.3 with the `NEPSEIndicatorSelector` class)\n",
    "\n",
    "Key principles:\n",
    "1. **Predictive Power**: Correlation with forward returns\n",
    "2. **Orthogonality**: Low inter-correlation (diversification of signals)\n",
    "3. **Robustness**: Consistent performance across bull/bear markets\n",
    "4. **Latency**: Computational efficiency for real-time NEPSE trading\n",
    "\n",
    "---\n",
    "\n",
    "## **13.10 Custom Indicator Development**\n",
    "\n",
    "When standard indicators fail to capture NEPSE-specific patterns, develop custom indicators:\n",
    "\n",
    "**NEPSE-Specific Custom Indicator Example:**\n",
    "```python\n",
    "def calculate_nepse_liquidity_score(data, vol_col='Vol', close_col='Close'):\n",
    "    \"\"\"\n",
    "    Custom indicator for NEPSE liquidity assessment.\n",
    "    \n",
    "    Combines:\n",
    "    - Volume relative to 20-day average\n",
    "    - Price impact (how much price moves per unit volume)\n",
    "    - Bid-ask spread proxy (High-Low relative to Close)\n",
    "    \n",
    "    Returns 0-100 score where:\n",
    "    - 0-30: Illiquid (hard to enter/exit without moving price)\n",
    "    - 30-70: Moderate liquidity\n",
    "    - 70-100: High liquidity (institutional grade)\n",
    "    \"\"\"\n",
    "    # Volume score (0-40 points)\n",
    "    vol_ratio = data[vol_col] / data[vol_col].rolling(20).mean()\n",
    "    vol_score = np.clip(vol_ratio * 20, 0, 40)  # Cap at 40\n",
    "    \n",
    "    # Price efficiency (0-30 points)\n",
    "    # Lower price impact per volume = higher liquidity\n",
    "    price_change = abs(data[close_col].pct_change())\n",
    "    volume_normalized = data[vol_col] / data[vol_col].rolling(50).mean()\n",
    "    price_impact = price_change / (volume_normalized + 0.001)\n",
    "    efficiency_score = np.clip(30 - (price_impact * 100), 0, 30)\n",
    "    \n",
    "    # Range stability (0-30 points)\n",
    "    # Consistent daily ranges indicate liquid, orderly markets\n",
    "    daily_range = (data['High'] - data['Low']) / data['Close']\n",
    "    range_consistency = 1 / (daily_range.rolling(10).std() + 0.01)\n",
    "    stability_score = np.clip(range_consistency * 3, 0, 30)\n",
    "    \n",
    "    # Composite score\n",
    "    liquidity_score = vol_score + efficiency_score + stability_score\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Liquidity_Score': liquidity_score,\n",
    "        'Volume_Component': vol_score,\n",
    "        'Efficiency_Component': efficiency_score,\n",
    "        'Stability_Component': stability_score,\n",
    "        'Liquidity_Grade': pd.cut(liquidity_score, \n",
    "                                bins=[0, 30, 70, 100],\n",
    "                                labels=['Low', 'Medium', 'High'])\n",
    "    })\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_nepse_liquidity_score` function demonstrates custom indicator development tailored to the unique characteristics of the Nepal Stock Exchange. Standard liquidity indicators (like Amihud or bid-ask spreads) require tick-level data unavailable in standard NEPSE CSV feeds. This custom indicator reconstructs liquidity proxies from OHLCV data.\n",
    "\n",
    "**Component Breakdown:**\n",
    "\n",
    "1. **Volume Component (40 points)**: Measures relative volume compared to 20-day average. In NEPSE, liquidity is highly variablesome days see 10x normal volume during news events, other days see 0.1x during holidays or political uncertainty. This component scores high when volume is robust, indicating institutional participation and ease of execution.\n",
    "\n",
    "2. **Price Efficiency Component (30 points)**: Measures price impact per unit of volume. In liquid markets, large volume should move price minimally. The formula $\\frac{\\Delta P}{Volume}$ measures this impact. Low impact = high efficiency = high liquidity score. This identifies NEPSE stocks where you can enter/exit large positions without significant slippage.\n",
    "\n",
    "3. **Range Stability Component (30 points)**: Measures consistency of daily trading ranges. Liquid stocks exhibit consistent, orderly volatility. Illiquid NEPSE stocks often show erratic rangessome days 1%, other days 10% on minimal volume. Low standard deviation of ranges indicates professional market making and liquidity.\n",
    "\n",
    "**Composite Scoring:**\n",
    "The final 0-100 score categorizes NEPSE stocks into:\n",
    "- **Low (0-30)**: Avoid for large positions. High slippage, erratic fills, wide spreads. Suitable only for long-term buy-and-hold.\n",
    "- **Medium (30-70)**: Acceptable for normal trading. Moderate slippage on entries/exits.\n",
    "- **High (70-100)**: Institutional grade. Tight spreads, deep order books, minimal market impact. Suitable for large positions and algorithmic execution.\n",
    "\n",
    "This custom indicator enables quantitative screening of the entire NEPSE universe to identify which stocks are tradable given specific position size constraints and execution requirements.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 13**\n",
    "\n",
    "This chapter covered the engineering of technical indicators specifically tailored for the NEPSE time-series prediction system, including trend, momentum, volatility, volume, and position indicators with detailed mathematical foundations and Python implementations.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
