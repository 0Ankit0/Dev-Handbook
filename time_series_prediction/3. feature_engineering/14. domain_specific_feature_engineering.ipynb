{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 14: Domain-Specific Feature Engineering**\n",
    "\n",
    "## **14.1 Financial Domain Features**\n",
    "\n",
    "Financial time-series feature engineering transforms raw market data into predictive signals that capture investor behavior, market microstructure, and economic regimes. While Chapter 13 covered technical indicators, this section focuses on domain-specific financial features tailored for the NEPSE (Nepal Stock Exchange) prediction system and general equity markets.\n",
    "\n",
    "### **14.1.1 Price Action Features**\n",
    "\n",
    "Price action features capture the behavioral patterns of market participants through the lens of OHLC (Open, High, Low, Close) data. For NEPSE, these features must account for the market's specific characteristics: lower liquidity, higher retail participation, and susceptibility to gap-up/gap-down openings due to overnight news.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def calculate_price_action_features(df, open_col='Open', high_col='High', \n",
    "                                   low_col='Low', close_col='Close', \n",
    "                                   prev_close_col='Prev. Close'):\n",
    "    \"\"\"\n",
    "    Calculate comprehensive price action features for NEPSE stocks.\n",
    "    \n",
    "    These features capture candlestick patterns, gap analysis, and \n",
    "    intraday momentum critical for Nepali market prediction.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Candlestick Body and Shadows\n",
    "    # Body represents the conviction of buyers vs sellers\n",
    "    features['Body_Size'] = abs(df[close_col] - df[open_col])\n",
    "    features['Body_Pct'] = (features['Body_Size'] / df[open_col]) * 100\n",
    "    \n",
    "    # Upper Shadow (selling pressure/rejection of higher prices)\n",
    "    features['Upper_Shadow'] = df[high_col] - np.maximum(df[open_col], df[close_col])\n",
    "    features['Upper_Shadow_Pct'] = (features['Upper_Shadow'] / df[open_col]) * 100\n",
    "    \n",
    "    # Lower Shadow (buying support/rejection of lower prices)\n",
    "    features['Lower_Shadow'] = np.minimum(df[open_col], df[close_col]) - df[low_col]\n",
    "    features['Lower_Shadow_Pct'] = (features['Lower_Shadow'] / df[open_col]) * 100\n",
    "    \n",
    "    # Total Range (volatility of the session)\n",
    "    features['Daily_Range'] = df[high_col] - df[low_col]\n",
    "    features['Range_Pct'] = (features['Daily_Range'] / df[open_col]) * 100\n",
    "    \n",
    "    # 2. Candlestick Pattern Recognition\n",
    "    # Doji (indecision, market equilibrium)\n",
    "    features['Is_Doji'] = features['Body_Pct'] < 0.3  # Body less than 0.3% of price\n",
    "    \n",
    "    # Hammer (potential reversal after downtrend)\n",
    "    # Small body at top, long lower shadow (2x body minimum)\n",
    "    features['Is_Hammer'] = (\n",
    "        (features['Body_Pct'] < features['Lower_Shadow_Pct'] / 2) & \n",
    "        (features['Upper_Shadow_Pct'] < features['Body_Pct'])\n",
    "    )\n",
    "    \n",
    "    # Shooting Star (potential reversal after uptrend)\n",
    "    # Small body at bottom, long upper shadow\n",
    "    features['Is_Shooting_Star'] = (\n",
    "        (features['Body_Pct'] < features['Upper_Shadow_Pct'] / 2) & \n",
    "        (features['Lower_Shadow_Pct'] < features['Body_Pct'])\n",
    "    )\n",
    "    \n",
    "    # Marubozu (strong conviction, no shadows)\n",
    "    # Indicates strong buying (white/green) or selling (black/red) pressure\n",
    "    features['Is_Marubozu'] = (\n",
    "        (features['Upper_Shadow_Pct'] < 0.1) & \n",
    "        (features['Lower_Shadow_Pct'] < 0.1) & \n",
    "        (features['Body_Pct'] > 1.0)  # Significant body\n",
    "    )\n",
    "    \n",
    "    # 3. Gap Analysis (Critical for NEPSE due to overnight news impact)\n",
    "    # Gap percentage from previous close\n",
    "    features['Gap_Pct'] = ((df[open_col] - df[prev_close_col]) / df[prev_close_col]) * 100\n",
    "    \n",
    "    # Gap types\n",
    "    features['Gap_Up'] = features['Gap_Pct'] > 0.5  # More than 0.5% gap up\n",
    "    features['Gap_Down'] = features['Gap_Pct'] < -0.5  # More than 0.5% gap down\n",
    "    features['Gap_Large'] = abs(features['Gap_Pct']) > 2.0  # Extreme gap\n",
    "    \n",
    "    # Gap fill analysis (did price return to previous close?)\n",
    "    features['Gap_Filled'] = (\n",
    "        (features['Gap_Up'] & (df[low_col] <= df[prev_close_col])) |\n",
    "        (features['Gap_Down'] & (df[high_col] >= df[prev_close_col]))\n",
    "    )\n",
    "    \n",
    "    # 4. Intraday Momentum (directional strength)\n",
    "    # Close location within range (0 = low, 100 = high)\n",
    "    features['Close_Location'] = ((df[close_col] - df[low_col]) / \n",
    "                                 (df[high_col] - df[low_col] + 0.0001)) * 100\n",
    "    \n",
    "    # Buy/Sell Pressure (based on close vs VWAP if available)\n",
    "    if 'VWAP' in df.columns:\n",
    "        features['Above_VWAP'] = (df[close_col] > df['VWAP']).astype(int)\n",
    "        features['VWAP_Distance'] = ((df[close_col] - df['VWAP']) / df['VWAP']) * 100\n",
    "    \n",
    "    # 5. Multi-day Patterns\n",
    "    # Three-day patterns (trend continuation/reversal)\n",
    "    features['Three_Day_High'] = df[high_col].rolling(3).max()\n",
    "    features['Three_Day_Low'] = df[low_col].rolling(3).min()\n",
    "    features['New_3Day_High'] = df[high_col] == features['Three_Day_High']\n",
    "    features['New_3Day_Low'] = df[low_col] == features['Three_Day_Low']\n",
    "    \n",
    "    # 6. Price Relative to 52-Week Range (if available)\n",
    "    if '52 Weeks High' in df.columns and '52 Weeks Low' in df.columns:\n",
    "        range_52w = df['52 Weeks High'] - df['52 Weeks Low']\n",
    "        features['Position_52W_Range'] = ((df[close_col] - df['52 Weeks Low']) / \n",
    "                                         (range_52w + 0.0001)) * 100\n",
    "        \n",
    "        # Proximity to extremes (mean reversion signals)\n",
    "        features['Near_52W_High'] = features['Position_52W_Range'] > 95\n",
    "        features['Near_52W_Low'] = features['Position_52W_Range'] < 5\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Detailed Explanation:\n",
    "# \n",
    "# 1. Candlestick Components:\n",
    "#    - Body_Size: Represents the net price movement from open to close. \n",
    "#      Large body = strong conviction, small body = indecision.\n",
    "#      In NEPSE, retail traders often panic on large red bodies, creating \n",
    "#      next-day reversals.\n",
    "#\n",
    "# 2. Shadow Analysis:\n",
    "#    - Upper_Shadow: Indicates rejection of higher prices. In NEPSE, long \n",
    "#      upper shadows often mean institutional selling into retail buying \n",
    "#      euphoria (distribution).\n",
    "#    - Lower_Shadow: Indicates buying support. Long lower shadows in NEPSE \n",
    "#      often represent value buying by informed investors after morning panic.\n",
    "#\n",
    "# 3. Gap Analysis:\n",
    "#    - NEPSE opens at 11:00 AM NPT, leaving room for overnight news gaps.\n",
    "#    - Gap_Up > 2% on high volume = Institutional accumulation (follow through likely)\n",
    "#    - Gap_Down > 2% on high volume = Institutional distribution (panic selling)\n",
    "#    - Gap_Filled: If a gap up fills same day (price falls to prev close), \n",
    "#      it indicates weak conviction (false breakout).\n",
    "#\n",
    "# 4. Close_Location:\n",
    "#    - Values near 100 (closed at high) = aggressive buying into close, \n",
    "#      often bullish for next day in NEPSE.\n",
    "#    - Values near 0 (closed at low) = capitulation, potential reversal signal.\n",
    "#\n",
    "# 5. 52-Week Position:\n",
    "#    - Stocks near 52-week highs in NEPSE often face profit booking (mean reversion).\n",
    "#    - Stocks near 52-week lows with high volume often indicate bottoming \n",
    "#      (value accumulation by promoters/institutions).\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "The `calculate_price_action_features` function transforms raw NEPSE OHLC data into behavioral signals that reflect market psychology. \n",
    "\n",
    "**Candlestick Anatomy:**\n",
    "The function decomposes each trading session into its constituent parts: body (open-to-close range) and shadows (wicks). In NEPSE's retail-dominated market, candlestick patterns carry significant predictive weight because they reflect the emotional state of traders. A `Hammer` pattern (long lower shadow, small body at top) indicates that despite aggressive selling during the session, buyers stepped in to close near the open—often marking short-term bottoms in Nepali stocks.\n",
    "\n",
    "**Gap Analysis:**\n",
    "NEPSE operates Sunday-Thursday with a session break overnight. This creates gap risk where opening prices differ significantly from previous closes due to corporate announcements, regulatory changes, or global market movements. The function categorizes gaps by magnitude (0.5% threshold for significance, 2% for extreme) and tracks whether gaps fill intraday. A gap up that fills the same day (price falls back to previous close) indicates false breakout—retail euphoria met with institutional selling.\n",
    "\n",
    "**Location Metrics:**\n",
    "`Close_Location` normalizes the closing price within the daily range to a 0-100 scale. In NEPSE, closes above 90% of the daily range (aggressive buying into the close) predict overnight continuation because Nepali traders often carry positions home based on end-of-day strength. Conversely, closes below 10% indicate panic selling that often exhausts itself, creating mean-reversion opportunities the next session.\n",
    "\n",
    "### **14.1.2 Volume Features**\n",
    "\n",
    "Volume features in financial markets measure the intensity and conviction behind price movements. For NEPSE, volume analysis is critical due to the market's liquidity constraints and the presence of large institutional block trades alongside retail participation.\n",
    "\n",
    "```python\n",
    "def calculate_volume_features(df, vol_col='Vol', close_col='Close', \n",
    "                             turnover_col='Turnover', trans_col='Trans.',\n",
    "                             open_col='Open', high_col='High', low_col='Low'):\n",
    "    \"\"\"\n",
    "    Calculate sophisticated volume features for NEPSE liquidity analysis.\n",
    "    \n",
    "    NEPSE-specific considerations:\n",
    "    - Lower float stocks show exaggerated volume spikes\n",
    "    - Turnover in NPR provides institutional flow clues\n",
    "    - Transaction count (Trans.) indicates retail vs institutional participation\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Volume Moving Averages and Ratios\n",
    "    features['Volume_SMA_5'] = df[vol_col].rolling(5).mean()\n",
    "    features['Volume_SMA_20'] = df[vol_col].rolling(20).mean()\n",
    "    features['Volume_SMA_50'] = df[vol_col].rolling(50).mean()\n",
    "    \n",
    "    # Relative Volume (current vs average) - key liquidity indicator\n",
    "    features['Relative_Volume'] = df[vol_col] / features['Volume_SMA_20']\n",
    "    features['Volume_Spike'] = features['Relative_Volume'] > 2.0  # 2x average\n",
    "    \n",
    "    # 2. Volume Trend (increasing/decreasing)\n",
    "    features['Volume_Trend'] = np.where(\n",
    "        df[vol_col] > features['Volume_SMA_5'], 'Increasing',\n",
    "        np.where(df[vol_col] < features['Volume_SMA_5'] * 0.8, 'Decreasing', 'Stable')\n",
    "    )\n",
    "    \n",
    "    # 3. Price-Volume Relationship (Ease of Movement)\n",
    "    # Measures how much volume is required to move price\n",
    "    distance_moved = ((df[high_col] + df[low_col]) / 2) - \\\n",
    "                     ((df[high_col].shift(1) + df[low_col].shift(1)) / 2)\n",
    "    box_ratio = (df[vol_col] / 1_000_000) / (df[high_col] - df[low_col] + 0.001)\n",
    "    features['Ease_of_Movement'] = distance_moved / box_ratio\n",
    "    \n",
    "    # 4. Volume Weighted Features (using VWAP if available)\n",
    "    if 'VWAP' in df.columns:\n",
    "        # Volume Profile relative to VWAP\n",
    "        features['Volume_at_VWAP'] = (abs(df[close_col] - df['VWAP']) < 0.01).astype(int)\n",
    "        \n",
    "        # Accumulation/Distribution based on close vs VWAP\n",
    "        features['Accumulation_Day'] = (df[close_col] > df['VWAP']) & (df[vol_col] > features['Volume_SMA_20'])\n",
    "        features['Distribution_Day'] = (df[close_col] < df['VWAP']) & (df[vol_col] > features['Volume_SMA_20'])\n",
    "    \n",
    "    # 5. Turnover Analysis (NEPSE specific - in NPR)\n",
    "    if turnover_col in df.columns:\n",
    "        features['Turnover_SMA_20'] = df[turnover_col].rolling(20).mean()\n",
    "        features['Turnover_Ratio'] = df[turnover_col] / features['Turnover_SMA_20']\n",
    "        \n",
    "        # Average Trade Size (proxy for institutional activity)\n",
    "        features['Avg_Trade_Size'] = df[turnover_col] / (df[trans_col] + 1)  # NPR per transaction\n",
    "        features['Avg_Trade_SMA'] = features['Avg_Trade_Size'].rolling(20).mean()\n",
    "        \n",
    "        # Institutional flow proxy (large avg trade size + high turnover)\n",
    "        features['Institutional_Flow'] = (\n",
    "            (features['Avg_Trade_Size'] > features['Avg_Trade_SMA'] * 1.5) & \n",
    "            (features['Turnover_Ratio'] > 1.5)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # 6. Transaction Count Analysis (Trans. column)\n",
    "    if trans_col in df.columns:\n",
    "        features['Trans_SMA_20'] = df[trans_col].rolling(20).mean()\n",
    "        features['Trans_Ratio'] = df[trans_col] / features['Trans_SMA_20']\n",
    "        \n",
    "        # Retail Participation (high transaction count, low average size)\n",
    "        features['Retail_Dominant'] = (\n",
    "            (features['Trans_Ratio'] > 1.5) & \n",
    "            (features['Avg_Trade_Size'] < features['Avg_Trade_SMA'] * 0.8)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Institutional Dominance (low transaction count, high size)\n",
    "        features['Institutional_Dominant'] = (\n",
    "            (features['Trans_Ratio'] < 0.8) & \n",
    "            (features['Avg_Trade_Size'] > features['Avg_Trade_SMA'] * 1.5)\n",
    "        ).astype(int)\n",
    "    \n",
    "    # 7. Volume-Price Divergence\n",
    "    price_change = df[close_col].pct_change()\n",
    "    volume_change = df[vol_col].pct_change()\n",
    "    \n",
    "    # Bullish divergence: Price down, Volume up (accumulation)\n",
    "    features['Bullish_Volume_Div'] = (price_change < -0.01) & (volume_change > 0.5)\n",
    "    \n",
    "    # Bearish divergence: Price up, Volume down (weak rally)\n",
    "    features['Bearish_Volume_Div'] = (price_change > 0.01) & (volume_change < -0.3)\n",
    "    \n",
    "    # 8. Force Index (volume * price change)\n",
    "    features['Force_Index'] = df[vol_col] * (df[close_col] - df[close_col].shift(1))\n",
    "    features['Force_Index_EMA'] = features['Force_Index'].ewm(span=13).mean()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Explanation of NEPSE Volume Dynamics:\n",
    "#\n",
    "# 1. Relative_Volume:\n",
    "#    In NEPSE, a volume spike (>2x average) often precedes major news.\n",
    "#    Unlike developed markets, volume spikes in NEPSE tend to persist \n",
    "#    for several days due to slow information diffusion.\n",
    "#\n",
    "# 2. Avg_Trade_Size:\n",
    "#    NEPSE has distinct retail vs institutional patterns.\n",
    "#    Retail: High Trans. count, low Avg_Trade_Size (< NPR 50,000)\n",
    "#    Institutional: Low Trans. count, high Avg_Trade_Size (> NPR 500,000)\n",
    "#    This proxy helps detect \"smart money\" accumulation.\n",
    "#\n",
    "# 3. Accumulation vs Distribution:\n",
    "#    Accumulation_Day: Close above VWAP on high volume = Institutional buying\n",
    "#    Distribution_Day: Close below VWAP on high volume = Institutional selling\n",
    "#    Three consecutive accumulation days in NEPSE often mark trend bottoms.\n",
    "#\n",
    "# 4. Force_Index:\n",
    "#    Combines volume and price momentum. Large positive values indicate \n",
    "#    strong buying pressure (institutional entry).\n",
    "#    In NEPSE, Force Index divergences (price new high, FI lower high) \n",
    "#    are highly predictive of corrections.\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_volume_features` function addresses the unique microstructure of NEPSE, where liquidity varies dramatically between stocks and institutional participation leaves distinct footprints.\n",
    "\n",
    "**Institutional vs Retail Proxies:**\n",
    "NEPSE provides both `Turnover` (total value in NPR) and `Trans.` (number of transactions). The ratio of these creates `Avg_Trade_Size`, a proxy for participant type. Retail-dominated stocks show high transaction counts with small average sizes (many small orders), while institutional accumulation shows low transaction counts with large average sizes (block trades). The function flags `Institutional_Dominant` days when average trade size exceeds 1.5x the norm—often preceding significant price moves as institutions accumulate positions ahead of corporate announcements.\n",
    "\n",
    "**Accumulation/Distribution Logic:**\n",
    "Using VWAP (Volume Weighted Average Price) provided in NEPSE data, the function identifies accumulation days (close above VWAP on above-average volume) and distribution days (close below VWAP on high volume). In NEPSE's thin market, three consecutive accumulation days often mark the end of a decline as institutional buyers exhaust available supply. Conversely, distribution days at market peaks indicate institutions selling to retail investors (the \"greater fool\" dynamic common in emerging markets).\n",
    "\n",
    "**Force Index:**\n",
    "The Elder Force Index multiplies volume by price change, creating a momentum oscillator that identifies the conviction behind moves. For NEPSE, extreme Force Index values (>2 standard deviations) often indicate \"blow-off\" tops or \"capitulation\" bottoms where exhausted participants exit en masse, creating mean-reversion opportunities.\n",
    "\n",
    "### **14.1.3 Market Microstructure**\n",
    "\n",
    "Market microstructure features estimate liquidity, spread, and depth from available data. Since NEPSE does not provide Level 2 order book data, we must infer microstructure from OHLCV and derived metrics.\n",
    "\n",
    "```python\n",
    "def calculate_microstructure_features(df, high_col='High', low_col='Low', \n",
    "                                     close_col='Close', open_col='Open',\n",
    "                                     vol_col='Vol', vwap_col='VWAP'):\n",
    "    \"\"\"\n",
    "    Estimate market microstructure features from NEPSE OHLCV data.\n",
    "    \n",
    "    Without Level 2 data, we proxy:\n",
    "    - Bid-ask spread from High-Low range\n",
    "    - Liquidity from Volume/Range ratio\n",
    "    - Volatility clustering from GARCH-like features\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Effective Spread Estimation\n",
    "    # Proxy for bid-ask spread using daily range and close location\n",
    "    daily_range = df[high_col] - df[low_col]\n",
    "    \n",
    "    # Quoted Spread Proxy: If close near high, spread compressed (bullish)\n",
    "    # If close near low, spread widened (bearish)\n",
    "    high_close_dist = df[high_col] - df[close_col]\n",
    "    low_close_dist = df[close_col] - df[low_col]\n",
    "    \n",
    "    # Effective spread estimate (simplified Roll's model adaptation)\n",
    "    features['Effective_Spread_Proxy'] = 2 * np.sqrt(abs(high_close_dist - low_close_dist) * daily_range + 0.0001)\n",
    "    features['Spread_Pct'] = (features['Effective_Spread_Proxy'] / df[close_col]) * 100\n",
    "    \n",
    "    # 2. Liquidity Measures\n",
    "    # Amihud Illiquidity (price impact per unit volume)\n",
    "    abs_return = abs(df[close_col].pct_change())\n",
    "    features['Amihud_Illiquidity'] = (abs_return / (df[vol_col] + 1)) * 1_000_000  # Scaled\n",
    "    features['Amihud_Smooth'] = features['Amihud_Illiquidity'].rolling(20).mean()\n",
    "    \n",
    "    # Liquidity Ratio (volume relative to price range)\n",
    "    # Higher = more liquid (more volume needed to move price)\n",
    "    features['Liquidity_Ratio'] = df[vol_col] / (daily_range + 0.001)\n",
    "    \n",
    "    # 3. Kyle's Lambda (Price Impact) approximation\n",
    "    # Measures how much prices change with volume\n",
    "    signed_volume = df[vol_col] * np.sign(df[close_col] - df[open_col])\n",
    "    price_change = df[close_col] - df[open_col]\n",
    "    \n",
    "    # Rolling regression slope of price change on signed volume\n",
    "    def rolling_kyle_lambda(x, y, window=20):\n",
    "        \"\"\"Calculate Kyle's lambda using rolling covariance/variance\"\"\"\n",
    "        cov = x.rolling(window).cov(y)\n",
    "        var = x.rolling(window).var()\n",
    "        return cov / (var + 1e-10)\n",
    "    \n",
    "    features['Kyle_Lambda'] = rolling_kyle_lambda(signed_volume, price_change)\n",
    "    features['High_Impact'] = features['Kyle_Lambda'] > features['Kyle_Lambda'].quantile(0.9)\n",
    "    \n",
    "    # 4. Order Flow Toxicity (VPIN approximation)\n",
    "    # Volume-synchronized probability of informed trading\n",
    "    buy_volume = df[vol_col] * (df[close_col] > df[open_col]).astype(float)\n",
    "    sell_volume = df[vol_col] * (df[close_col] <= df[open_col]).astype(float)\n",
    "    \n",
    "    features['Order_Imbalance'] = abs(buy_volume - sell_volume) / (df[vol_col] + 1)\n",
    "    features['VPIN_Proxy'] = features['Order_Imbalance'].rolling(20).mean()\n",
    "    \n",
    "    # 5. Volatility Clustering (GARCH effects)\n",
    "    # Current volatility predicted by past volatility\n",
    "    log_returns = np.log(df[close_col] / df[close_col].shift(1))\n",
    "    squared_returns = log_returns ** 2\n",
    "    \n",
    "    features['Abs_Return'] = abs(log_returns)\n",
    "    features['Vol_Clustering'] = features['Abs_Return'].rolling(5).mean() / features['Abs_Return'].rolling(20).mean()\n",
    "    \n",
    "    # ARCH effect: Squared returns autocorrelation\n",
    "    features['ARCH_Effect'] = squared_returns.rolling(5).mean() * 1000\n",
    "    \n",
    "    # 6. Market Depth Proxy\n",
    "    # Estimated from VWAP deviation and volume\n",
    "    if vwap_col in df.columns:\n",
    "        vwap_dev = abs(df[close_col] - df[vwap_col]) / df[vwap_col]\n",
    "        # Deep market: High volume with low VWAP deviation\n",
    "        features['Market_Depth'] = df[vol_col] / (vwap_dev + 0.001)\n",
    "        features['Shallow_Market'] = features['Market_Depth'] < features['Market_Depth'].quantile(0.2)\n",
    "    \n",
    "    # 7. Intraday Volatility Patterns (NEPSE specific)\n",
    "    # Estimate opening, midday, closing volatility from range decomposition\n",
    "    features['Opening_Range'] = df[high_col].rolling(3).max() - df[low_col].rolling(3).min()\n",
    "    features['Opening_Volatility'] = features['Opening_Range'] / df[close_col]\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Microstructure Interpretation for NEPSE:\n",
    "#\n",
    "# 1. Effective_Spread_Proxy:\n",
    "#    NEPSE stocks typically have wider spreads than developed markets.\n",
    "#    Spread > 1% indicates illiquid stock (avoid for large positions).\n",
    "#    Spread compression (decreasing) + volume increase = Institutional entry\n",
    "#\n",
    "# 2. Amihud_Illiquidity:\n",
    "#    Higher values = more illiquid (large price moves per unit volume).\n",
    "#    NEPSE micro-caps often show Amihud > 100 (extremely illiquid).\n",
    "#    Values < 10 indicate institutional-grade liquidity.\n",
    "#\n",
    "# 3. Kyle_Lambda:\n",
    "#    Measures price impact. High lambda means market is thin (small orders move price).\n",
    "#    In NEPSE, lambda spikes before earnings (informed trading).\n",
    "#    Lambda collapse after news = liquidity return.\n",
    "#\n",
    "# 4. VPIN_Proxy:\n",
    "#    High VPIN (Volume-synchronized PIN) indicates informed trading (toxic flow).\n",
    "#    VPIN > 0.6 in NEPSE suggests insiders are active (regulatory risk).\n",
    "#    VPIN spikes often precede major announcements.\n",
    "#\n",
    "# 5. Vol_Clustering:\n",
    "#    GARCH effect proxy. Values > 1.5 indicate volatile period likely to continue.\n",
    "#    In NEPSE, volatility clusters around quarter-ends (window dressing).\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_microstructure_features` function reconstructs high-frequency market structure from low-frequency NEPSE data, essential for understanding execution costs and market manipulation risks.\n",
    "\n",
    "**Spread Estimation:**\n",
    "Without Level 2 data, the function uses the relationship between daily range and close location to proxy bid-ask spreads. When closes occur near the high of the day, it suggests the closing auction absorbed ask liquidity (tight spreads). When closes occur near the low, it suggests wide spreads or aggressive market selling. For NEPSE, spreads exceeding 1% of price indicate illiquid conditions where market orders face significant slippage.\n",
    "\n",
    "**Amihud Illiquidity:**\n",
    "This classic metric divides absolute returns by volume, measuring the price impact per unit of trading activity. In NEPSE, Amihud values vary dramatically: blue-chip stocks like NTC (Nepal Telecom) show values < 5 (liquid), while micro-caps show values > 100 (illiquid). Sudden increases in Amihud during price declines indicate \"flight to quality\" or liquidity crises where market makers withdraw.\n",
    "\n",
    "**VPIN Proxy:**\n",
    "The Volume-synchronized Probability of Informed Trading estimates the presence of \"toxic flow\"—traders with superior information. High VPIN (>0.6) in NEPSE often indicates promoter activity or leaked corporate information ahead of quarterly results. Regulatory constraints in Nepal make this a valuable risk management signal.\n",
    "\n",
    "### **14.1.4 Sentiment Features**\n",
    "\n",
    "Sentiment features quantify market psychology using available NEPSE metrics. Since NEPSE lacks options data (put/call ratios) or short interest data, we construct sentiment proxies from price action, volume, and breadth.\n",
    "\n",
    "```python\n",
    "def calculate_sentiment_features(df, close_col='Close', open_col='Open', \n",
    "                                high_col='High', low_col='Low', vol_col='Vol',\n",
    "                                diff_pct_col='Diff %', range_pct_col='Range %'):\n",
    "    \"\"\"\n",
    "    Calculate market sentiment indicators from NEPSE data.\n",
    "    \n",
    "    Sentiment proxies:\n",
    "    - Put/Call proxy from skewness (panic vs euphoria)\n",
    "    - Fear/Greed from volatility expansion\n",
    "    - Breadth from advancing vs declining features\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Skewness-Based Sentiment (Fear vs Euphoria)\n",
    "    # Negative skew = Fear (tail risk to downside)\n",
    "    # Positive skew = Euphoria (tail risk to upside)\n",
    "    returns = df[close_col].pct_change()\n",
    "    features['Return_Skew_20'] = returns.rolling(20).skew()\n",
    "    \n",
    "    # Fear index: High negative skew + high volatility\n",
    "    volatility = returns.rolling(20).std()\n",
    "    features['Fear_Index'] = -features['Return_Skew_20'] * volatility * 100\n",
    "    \n",
    "    # 2. Euphoria Indicator (bubble detection)\n",
    "    # Price rising with increasing volatility and volume\n",
    "    price_sma = df[close_col].rolling(20).mean()\n",
    "    price_distance = (df[close_col] - price_sma) / price_sma\n",
    "    \n",
    "    features['Euphoria'] = (\n",
    "        (price_distance > 0.1) &  # Price 10% above MA\n",
    "        (volatility > volatility.rolling(50).mean()) &  # Vol expanding\n",
    "        (df[vol_col] > df[vol_col].rolling(20).mean())  # Volume high\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 3. Panic Indicator (capitulation)\n",
    "    # Sharp drops on massive volume\n",
    "    features['Panic'] = (\n",
    "        (returns < -0.05) &  # Down > 5%\n",
    "        (df[vol_col] > df[vol_col].rolling(20).mean() * 2)  # Volume 2x normal\n",
    "    ).astype(int)\n",
    "    \n",
    "    # 4. Breadth Proxy (from Diff % column if available)\n",
    "    if diff_pct_col in df.columns:\n",
    "        # Diff % represents change from previous close\n",
    "        features['Advancing'] = df[diff_pct_col] > 0\n",
    "        features['Declining'] = df[diff_pct_col] < 0\n",
    "        \n",
    "        # Advance/Decline ratio proxy (for single stock, use rolling)\n",
    "        features['AD_Momentum'] = features['Advancing'].rolling(5).sum() / 5\n",
    "    \n",
    "    # 5. Volatility Regime (Fear gauge proxy)\n",
    "    # VIX-like calculation from price swings\n",
    "    daily_range = df[high_col] - df[low_col]\n",
    "    features['Volatility_Index_Proxy'] = (daily_range / df[open_col] * 100).rolling(20).mean()\n",
    "    \n",
    "    # Volatility term structure (contango/backwardation proxy)\n",
    "    vol_short = returns.rolling(5).std()\n",
    "    vol_long = returns.rolling(20).std()\n",
    "    features['Vol_Term_Structure'] = vol_short / vol_long\n",
    "    \n",
    "    # Contango (vol_short < vol_long) = complacency\n",
    "    # Backwardation (vol_short > vol_long) = fear\n",
    "    \n",
    "    # 6. Speculative Activity (High Range % on low Diff %)\n",
    "    if range_pct_col in df.columns and diff_pct_col in df.columns:\n",
    "        # Large intraday range but small close change = intraday chop/speculation\n",
    "        features['Intraday_Chop'] = (\n",
    "            (df[range_pct_col] > df[range_pct_col].rolling(20).mean() * 1.5) &\n",
    "            (abs(df[diff_pct_col]) < 1.0)\n",
    "        ).astype(int)\n",
    "        \n",
    "        # Trend Day (large range + large directional move)\n",
    "        features['Trend_Day'] = (\n",
    "            (df[range_pct_col] > df[range_pct_col].rolling(20).mean()) &\n",
    "            (abs(df[diff_pct_col]) > df[range_pct_col] * 0.7)  # Close near high/low\n",
    "        ).astype(int)\n",
    "    \n",
    "    # 7. Sentiment Momentum (Rate of change of optimism)\n",
    "    features['Optimism_Score'] = (\n",
    "        (df[close_col] > df[open_col]).astype(int) * 0.4 +  # Green candle\n",
    "        (df[close_col] > df[close_col].shift(1)).astype(int) * 0.3 +  # Higher close\n",
    "        (df[vol_col] < df[vol_col].rolling(20).mean()).astype(int) * 0.3  # Low vol rally\n",
    "    )\n",
    "    \n",
    "    features['Sentiment_Momentum'] = features['Optimism_Score'].diff(3)\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Sentiment Analysis for NEPSE:\n",
    "#\n",
    "# 1. Fear_Index:\n",
    "#    High values (>2.0) indicate asymmetric fear of downside.\n",
    "#    In NEPSE, fear persists longer than euphoria (loss aversion).\n",
    "#    Fear_Index > 3.0 often marks washout bottoms (buy signal).\n",
    "#\n",
    "# 2. Euphoria:\n",
    "#    Three components: Overextended price, expanding volatility, high volume.\n",
    "#    In NEPSE, euphoria readings > 3 consecutive days precede corrections 70% of time.\n",
    "#    Retail participation peaks during euphoria (distribution by institutions).\n",
    "#\n",
    "# 3. Panic:\n",
    "#    Capitulation volume (2x normal) + large decline (-5%).\n",
    "#    NEPSE panic often occurs on political news or regulatory changes.\n",
    "#    Single panic day = opportunity; Three panic days = stay away (systemic risk).\n",
    "#\n",
    "# 4. Intraday_Chop:\n",
    "#    Large daily range but close near open indicates uncertainty.\n",
    "#    High chop + high volume = battle between bulls/bears (avoid).\n",
    "#    High chop + low volume = accumulation/distribution in progress.\n",
    "#\n",
    "# 5. Optimism_Score:\n",
    "#    Composite of three bullish factors. Range 0-1.\n",
    "#    Scores > 0.8 on three consecutive days indicate over-optimism (contrarian sell).\n",
    "#    Scores < 0.2 indicate excessive pessimism (contrarian buy).\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `calculate_sentiment_features` function quantifies market psychology through behavioral proxies, critical for NEPSE where retail sentiment drives short-term price action.\n",
    "\n",
    "**Fear Index Construction:**\n",
    "Combining negative skewness (indicating crash risk) with realized volatility creates a \"Fear Index\" analogous to VIX. In NEPSE, high fear persists longer than in developed markets due to limited hedging mechanisms and loss aversion among retail investors. Fear Index readings above 3.0 historically mark capitulation points where risk-reward favors long positions.\n",
    "\n",
    "**Euphoria Detection:**\n",
    "The function identifies bubble conditions through three simultaneous signals: price extended 10% above moving averages, expanding volatility (increasing uncertainty), and high volume (retail FOMO). In NEPSE's bull markets, euphoria can persist for 5-7 days before sharp corrections. This indicator helps reduce position sizes or take profits during unsustainable rallies.\n",
    "\n",
    "**Intraday Chop:**\n",
    "When daily range percentage exceeds 1.5x the average but the day closes with less than 1% net change (Diff %), it indicates intraday speculation without directional conviction. In NEPSE, this pattern often preceds major moves as market makers accumulate inventory during the chop, then drive price directionally once positioned.\n",
    "\n",
    "---\n",
    "\n",
    "## **14.2 Retail and E-Commerce Features**\n",
    "\n",
    "While financial features focus on price discovery, retail forecasting requires understanding consumer behavior, inventory cycles, and promotional calendars.\n",
    "\n",
    "### **14.2.1 Sales Patterns**\n",
    "\n",
    "Retail sales exhibit distinct patterns: weekly (weekends vs weekdays), monthly (salary cycles), and seasonal (festivals, holidays).\n",
    "\n",
    "```python\n",
    "def calculate_retail_sales_features(df, sales_col='sales', date_col='date'):\n",
    "    \"\"\"\n",
    "    Calculate features for retail sales forecasting.\n",
    "    \n",
    "    Applications: NEPSE retail sector stocks, inventory management,\n",
    "    revenue prediction for listed retail companies.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    df.set_index(date_col, inplace=True)\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Calendar Features\n",
    "    features['Day_of_Week'] = df.index.dayofweek  # 0=Monday, 6=Sunday\n",
    "    features['Is_Weekend'] = features['Day_of_Week'].isin([5, 6]).astype(int)\n",
    "    features['Day_of_Month'] = df.index.day\n",
    "    features['Week_of_Year'] = df.index.isocalendar().week\n",
    "    features['Month'] = df.index.month\n",
    "    features['Quarter'] = df.index.quarter\n",
    "    features['Year'] = df.index.year\n",
    "    \n",
    "    # 2. Salary Cycle Effects (Nepali context: 1st and 15th of month)\n",
    "    features['Is_Salary_Day'] = df.index.day.isin([1, 2, 15, 16]).astype(int)\n",
    "    features['Days_Since_Salary'] = df.index.day.apply(lambda x: min(x % 15, 15 - x % 15))\n",
    "    \n",
    "    # 3. Lag Features (autoregressive)\n",
    "    for lag in [1, 7, 14, 28]:\n",
    "        features[f'Sales_Lag_{lag}d'] = df[sales_col].shift(lag)\n",
    "    \n",
    "    # 4. Rolling Statistics (trend and seasonality)\n",
    "    features['Sales_MA_7'] = df[sales_col].rolling(7).mean()\n",
    "    features['Sales_MA_30'] = df[sales_col].rolling(30).mean()\n",
    "    features['Sales_Std_7'] = df[sales_col].rolling(7).std()\n",
    "    \n",
    "    # 5. Growth Rates\n",
    "    features['Sales_YoY'] = df[sales_col] / df[sales_col].shift(365) - 1\n",
    "    features['Sales_MoM'] = df[sales_col] / df[sales_col].shift(30) - 1\n",
    "    features['Sales_WoW'] = df[sales_col] / df[sales_col].shift(7) - 1\n",
    "    \n",
    "    # 6. Seasonal Decomposition (trend, seasonal, residual)\n",
    "    from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "    decomposition = seasonal_decompose(df[sales_col], model='multiplicative', period=7)\n",
    "    features['Trend'] = decomposition.trend\n",
    "    features['Seasonal'] = decomposition.seasonal\n",
    "    features['Residual'] = decomposition.resid\n",
    "    \n",
    "    return features\n",
    "\n",
    "# Application to NEPSE:\n",
    "# For retail sector stocks (e.g., supermarkets, consumer goods), \n",
    "# these features predict quarterly earnings before announcement.\n",
    "# Sales_Lag_7 captures weekly patterns (Dashain shopping cycles).\n",
    "# Salary_Day effects predict cash flow for financial sector stocks.\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This function generates features for retail demand forecasting, applicable to NEPSE-listed retail companies or supply chain optimization. It captures the **salary cycle effect**—Nepali consumers typically shop on salary days (1st and 15th of each month), creating predictable revenue spikes for retail stocks. The function also performs seasonal decomposition to separate trend growth from cyclical patterns, essential for predicting quarterly earnings surprises in the NEPSE consumer sector.\n",
    "\n",
    "### **14.2.2 Customer Behavior**\n",
    "\n",
    "Customer behavior features segment purchasing patterns into cohorts and lifecycle stages.\n",
    "\n",
    "```python\n",
    "def calculate_customer_behavior_features(transactions_df, customer_col='customer_id',\n",
    "                                        date_col='date', amount_col='amount'):\n",
    "    \"\"\"\n",
    "    Calculate RFM (Recency, Frequency, Monetary) features for customer analytics.\n",
    "    \n",
    "    Applicable to NEPSE: Fintech companies, banks with digital platforms,\n",
    "    retail chains with membership data.\n",
    "    \"\"\"\n",
    "    current_date = transactions_df[date_col].max()\n",
    "    \n",
    "    # Aggregate by customer\n",
    "    customer_features = transactions_df.groupby(customer_col).agg({\n",
    "        date_col: ['max', 'count'],  # Last purchase, Frequency\n",
    "        amount_col: ['sum', 'mean', 'std']  # Monetary values\n",
    "    }).reset_index()\n",
    "    \n",
    "    customer_features.columns = ['Customer_ID', 'Last_Purchase', 'Frequency',\n",
    "                                'Total_Spent', 'Avg_Order_Value', 'Spend_Std']\n",
    "    \n",
    "    # Recency (days since last purchase)\n",
    "    customer_features['Recency'] = (current_date - customer_features['Last_Purchase']).dt.days\n",
    "    \n",
    "    # RFM Scores (1-5 quintiles)\n",
    "    customer_features['R_Score'] = pd.qcut(customer_features['Recency'], 5, labels=[5,4,3,2,1])\n",
    "    customer_features['F_Score'] = pd.qcut(customer_features['Frequency'].rank(method='first'), 5, labels=[1,2,3,4,5])\n",
    "    customer_features['M_Score'] = pd.qcut(customer_features['Total_Spent'], 5, labels=[1,2,3,4,5])\n",
    "    \n",
    "    # Combined RFM Score\n",
    "    customer_features['RFM_Score'] = (customer_features['R_Score'].astype(str) + \n",
    "                                     customer_features['F_Score'].astype(str) +\n",
    "                                     customer_features['M_Score'].astype(str))\n",
    "    \n",
    "    # Customer Segments\n",
    "    def segment_customers(row):\n",
    "        if row['RFM_Score'] in ['555', '554', '544', '545', '454', '455', '445']:\n",
    "            return 'Champions'\n",
    "        elif row['RFM_Score'] in ['543', '444', '435', '355', '354', '345', '344', '335']:\n",
    "            return 'Loyal Customers'\n",
    "        elif row['RFM_Score'] in ['512', '511', '422', '421', '412', '411', '311']:\n",
    "            return 'New Customers'\n",
    "        elif row['RFM_Score'] in ['155', '154', '144', '214', '215', '115', '114']:\n",
    "            return 'At Risk'\n",
    "        else:\n",
    "            return 'Others'\n",
    "    \n",
    "    customer_features['Segment'] = customer_features.apply(segment_customers, axis=1)\n",
    "    \n",
    "    # Lifetime Value Proxy (assuming 2-year horizon)\n",
    "    customer_features['CLV_Proxy'] = (customer_features['Avg_Order_Value'] * \n",
    "                                     customer_features['Frequency'] * 2)\n",
    "    \n",
    "    return customer_features\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The RFM (Recency, Frequency, Monetary) framework segments customers based on purchase history. For NEPSE analysis of banking or fintech stocks, these metrics predict **Customer Lifetime Value (CLV)** and churn risk. High-frequency, recent customers (\"Champions\") indicate strong digital adoption for Nepali banks—a leading indicator of non-interest income growth reported in quarterly financials.\n",
    "\n",
    "### **14.2.3 Inventory Features**\n",
    "\n",
    "Inventory optimization features prevent stockouts and overstock situations.\n",
    "\n",
    "```python\n",
    "def calculate_inventory_features(df, sales_col='sales', stock_col='current_stock',\n",
    "                                lead_time_col='lead_time_days'):\n",
    "    \"\"\"\n",
    "    Calculate inventory management features.\n",
    "    \n",
    "    Critical for NEPSE: Manufacturing sector, trading companies,\n",
    "    supply chain efficiency metrics.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 1. Stock-to-Sales Ratio\n",
    "    features['Stock_Cover_Days'] = df[stock_col] / df[sales_col].rolling(7).mean()\n",
    "    \n",
    "    # 2. Days of Supply\n",
    "    features['Days_of_Supply'] = df[stock_col] / df[sales_col].replace(0, np.nan)\n",
    "    \n",
    "    # 3. Inventory Turnover Proxy (Sales / Average Inventory)\n",
    "    avg_inventory = df[stock_col].rolling(30).mean()\n",
    "    features['Inventory_Turnover'] = df[sales_col].rolling(30).sum() / avg_inventory\n",
    "    \n",
    "    # 4. Stockout Risk\n",
    "    features['Stockout_Risk'] = (features['Days_of_Supply'] < df[lead_time_col]).astype(int)\n",
    "    \n",
    "    # 5. Overstock Alert (inventory > 2x lead time demand)\n",
    "    expected_demand = df[sales_col].rolling(7).mean() * df[lead_time_col]\n",
    "    features['Overstock'] = (df[stock_col] > expected_demand * 2).astype(int)\n",
    "    \n",
    "    # 6. Reorder Point (ROP)\n",
    "    # ROP = (Average daily sales * Lead time) + Safety stock\n",
    "    safety_stock = df[sales_col].rolling(30).std() * 1.65  # 95% service level\n",
    "    features['Reorder_Point'] = (df[sales_col].rolling(7).mean() * df[lead_time_col]) + safety_stock\n",
    "    features['Reorder_Now'] = (df[stock_col] <= features['Reorder_Point']).astype(int)\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Inventory features predict working capital efficiency for NEPSE manufacturing and trading companies. The **Stock-to-Sales Ratio** indicates how many days of inventory are on hand relative to sales velocity. For Nepali companies with supply chain constraints (India border dependence), high stockout risk correlates with production halts and negative earnings surprises, while overstock indicates capital inefficiency and potential write-downs.\n",
    "\n",
    "### **14.2.4 Seasonal Features**\n",
    "\n",
    "Seasonal features capture cyclical demand patterns specific to Nepali calendar events.\n",
    "\n",
    "```python\n",
    "def calculate_nepali_seasonal_features(df, date_col='date'):\n",
    "    \"\"\"\n",
    "    Calculate seasonal features specific to Nepali calendar and festivals.\n",
    "    \n",
    "    Critical for: Retail sales, bank deposits (Dashain withdrawal season),\n",
    "    remittance flows (overseas workers sending money home).\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Nepali months approximation (simplified)\n",
    "    # Actual implementation would use nepali-datetime library\n",
    "    month = df[date_col].dt.month\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # Dashain (September-October) - Major shopping festival\n",
    "    features['Is_Dashain'] = ((month == 9) & (df[date_col].dt.day > 15)) | \\\n",
    "                            ((month == 10) & (df[date_col].dt.day < 15))\n",
    "    \n",
    "    # Tihar (October-November) - Second shopping peak\n",
    "    features['Is_Tihar'] = ((month == 10) & (df[date_col].dt.day > 20)) | \\\n",
    "                          ((month == 11) & (df[date_col].dt.day < 10))\n",
    "    \n",
    "    # Tax Season (Mid-March to Mid-July in Nepal)\n",
    "    features['Is_Tax_Season'] = df[date_col].dt.month.isin([3, 4, 5, 6])\n",
    "    \n",
    "    # Remittance Season (pre-Dashain surge)\n",
    "    features['Remittance_Season'] = ((month == 8) & (df[date_col].dt.day > 15)) | \\\n",
    "                                   ((month == 9) & (df[date_col].dt.day < 30))\n",
    "    \n",
    "    # Agricultural Cycles (planting vs harvest)\n",
    "    # Terai planting: June-July, Harvest: Nov-Dec\n",
    "    # Hills planting: May-June, Harvest: Oct-Nov\n",
    "    features['Planting_Season'] = df[date_col].dt.month.isin([5, 6, 7])\n",
    "    features['Harvest_Season'] = df[date_col].dt.month.isin([10, 11, 12])\n",
    "    \n",
    "    # Quarter-end effects (window dressing by mutual funds)\n",
    "    features['Is_Quarter_End'] = df[date_col].dt.is_quarter_end\n",
    "    \n",
    "    # Pre-weekend effects (Friday high activity for NEPSE Sunday-Thursday week)\n",
    "    features['Is_Thursday'] = df[date_col].dt.dayofweek == 3  # Thursday = last trading day\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Nepali seasonal features account for cultural and agricultural cycles unique to the region. **Dashain** (the biggest festival) drives retail sales spikes and banking liquidity crunches. **Remittance Season** (pre-Dashain) sees massive inflows from overseas workers, impacting NEPSE banking sector deposits and foreign exchange reserves. **Agricultural cycles** affect microfinance and insurance sector stocks, with planting season indicating loan disbursement peaks and harvest season indicating repayment collections. These features provide predictive power for quarterly earnings that Western calendar features miss.\n",
    "\n",
    "---\n",
    "\n",
    "## **14.3 Weather and Climate Features**\n",
    "\n",
    "Weather features are essential for agricultural commodity prediction, hydropower generation forecasting, and insurance risk assessment in NEPSE.\n",
    "\n",
    "### **14.3.1 Temperature Patterns**\n",
    "\n",
    "Temperature affects everything from agricultural yields to energy demand.\n",
    "\n",
    "```python\n",
    "def calculate_temperature_features(temp_series):\n",
    "    \"\"\"\n",
    "    Calculate temperature-based features for time-series prediction.\n",
    "    \n",
    "    NEPSE Applications:\n",
    "    - Tea/Coffee sector stocks (temperature affects yield)\n",
    "    - Insurance sector (extreme heat claims)\n",
    "    - Tourism sector (seasonal demand)\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame(index=temp_series.index)\n",
    "    \n",
    "    # 1. Degree Days (for energy demand prediction)\n",
    "    base_temp = 18  # Base temperature for heating/cooling calculations\n",
    "    \n",
    "    features['Heating_Degree_Days'] = np.maximum(base_temp - temp_series, 0)\n",
    "    features['Cooling_Degree_Days'] = np.maximum(temp_series - base_temp, 0)\n",
    "    \n",
    "    # 2. Extreme Temperature Flags\n",
    "    features['Heat_Wave'] = (temp_series > temp_series.quantile(0.95)).astype(int)\n",
    "    features['Cold_Snap'] = (temp_series < temp_series.quantile(0.05)).astype(int)\n",
    "    \n",
    "    # 3. Temperature Change Velocity\n",
    "    features['Temp_Change_1d'] = temp_series.diff(1)\n",
    "    features['Temp_Change_7d'] = temp_series.diff(7)\n",
    "    \n",
    "    # 4. Accumulated Temperature (growing degree days for agriculture)\n",
    "    # Base 10°C for crop growth\n",
    "    features['Growing_Degree_Days'] = np.maximum(temp_series - 10, 0).cumsum()\n",
    "    \n",
    "    # 5. Temperature Regime\n",
    "    features['Temp_Regime'] = pd.cut(temp_series, \n",
    "                                    bins=[-np.inf, 0, 15, 25, 35, np.inf],\n",
    "                                    labels=['Freezing', 'Cold', 'Mild', 'Warm', 'Hot'])\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Temperature features support prediction of NEPSE agricultural stocks (tea, sugarcane, dairy) where yield correlates with growing degree days. **Heating/Cooling Degree Days** predict energy sector demand—Nepal's electricity consumption spikes during winter heating and summer cooling. **Growing Degree Days** (accumulated heat above 10°C) predict crop maturation rates for agricultural insurance underwriting and commodity price forecasting.\n",
    "\n",
    "### **14.3.2 Pressure Features**\n",
    "\n",
    "Atmospheric pressure systems predict weather changes and extreme events.\n",
    "\n",
    "```python\n",
    "def calculate_pressure_features(pressure_series, temp_series):\n",
    "    \"\"\"\n",
    "    Calculate atmospheric pressure features.\n",
    "    \n",
    "    NEPSE Applications:\n",
    "    - Monsoon prediction (agriculture sector)\n",
    "    - Extreme weather event forecasting (insurance claims)\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 1. Pressure Trend (falling pressure = storm approaching)\n",
    "    features['Pressure_Trend'] = pressure_series.diff(3)  # 3-hour change\n",
    "    features['Storm_Approaching'] = (features['Pressure_Trend'] < -5).astype(int)\n",
    "    \n",
    "    # 2. Pressure Anomaly\n",
    "    pressure_mean = pressure_series.rolling(30*24).mean()  # 30-day rolling\n",
    "    features['Pressure_Anomaly'] = pressure_series - pressure_mean\n",
    "    \n",
    "    # 3. Weather Front Proxy (pressure gradient + temp change)\n",
    "    features['Front_Proxy'] = abs(features['Pressure_Trend']) + abs(temp_series.diff(3))\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "### **14.3.3 Spatial Features**\n",
    "\n",
    "Spatial features aggregate weather across multiple locations relevant to NEPSE sectors.\n",
    "\n",
    "```python\n",
    "def calculate_spatial_weather_features(weather_stations_df):\n",
    "    \"\"\"\n",
    "    Aggregate weather data across multiple stations for regional analysis.\n",
    "    \n",
    "    NEPSE: Aggregate rainfall across major hydropower catchment areas\n",
    "    to predict generation capacity.\n",
    "    \"\"\"\n",
    "    # Group by date, aggregate across stations\n",
    "    daily_agg = weather_stations_df.groupby('date').agg({\n",
    "        'rainfall': ['mean', 'max', 'sum'],\n",
    "        'temperature': ['mean', 'max', 'min'],\n",
    "        'humidity': 'mean'\n",
    "    })\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Catchment area rainfall (sum for runoff calculation)\n",
    "    features['Total_Catchment_Rain'] = daily_agg[('rainfall', 'sum')]\n",
    "    \n",
    "    # Spatial variability (std across stations)\n",
    "    features['Rainfall_Variability'] = weather_stations_df.groupby('date')['rainfall'].std()\n",
    "    \n",
    "    # Extremes (max rainfall at any station)\n",
    "    features['Max_Station_Rain'] = daily_agg[('rainfall', 'max')]\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Spatial aggregation is critical for NEPSE **hydropower sector** analysis. Nepal's electricity generation depends on Himalayan snowmelt and monsoon rainfall across multiple river catchments. By aggregating rainfall across the catchment areas of major hydro plants (Upper Tamakoshi, Kaligandaki), investors can predict quarterly generation capacity and revenue 30-60 days before financial reports.\n",
    "\n",
    "### **14.3.4 Extreme Events**\n",
    "\n",
    "Extreme weather features predict disaster risk for insurance and reconstruction sectors.\n",
    "\n",
    "```python\n",
    "def calculate_extreme_weather_features(weather_df):\n",
    "    \"\"\"\n",
    "    Detect extreme weather events for risk prediction.\n",
    "    \n",
    "    NEPSE Applications:\n",
    "    - Insurance sector (claim reserves)\n",
    "    - Cement/Steel sectors (reconstruction demand post-disaster)\n",
    "    - Agriculture (crop failure prediction)\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 1. Extreme Rainfall (cloudburst detection)\n",
    "    rainfall = weather_df['rainfall']\n",
    "    features['Extreme_Rain'] = (rainfall > rainfall.quantile(0.99)).astype(int)\n",
    "    features['Consecutive_Extreme_Rain'] = features['Extreme_Rain'].rolling(3).sum()\n",
    "    \n",
    "    # 2. Drought Conditions\n",
    "    rolling_rain = rainfall.rolling(30).sum()\n",
    "    features['Drought_Condition'] = (rolling_rain < rolling_rain.quantile(0.1)).astype(int)\n",
    "    \n",
    "    # 3. Flash Flood Risk (intense rain over short period)\n",
    "    rain_intensity = rainfall / 1  # hourly intensity\n",
    "    features['Flash_Flood_Risk'] = (rain_intensity > 50).astype(int)  # 50mm/hour\n",
    "    \n",
    "    # 4. Compound Extremes (heat + drought)\n",
    "    features['Heat_Drought_Combo'] = (\n",
    "        (weather_df['temperature'] > weather_df['temperature'].quantile(0.9)) &\n",
    "        (features['Drought_Condition'] == 1)\n",
    "    ).astype(int)\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Extreme weather features predict **insurance claim spikes** and **reconstruction booms** for NEPSE-listed cement and steel companies. The **2015 earthquake** demonstrated how natural disasters drive construction sector revenues. Monsoon cloudbursts (extreme rain >99th percentile) predict agricultural loan defaults for banking sector analysis. Drought conditions predict poor harvests for fertilizer and seed company demand forecasting.\n",
    "\n",
    "---\n",
    "\n",
    "## **14.4 Healthcare Features**\n",
    "\n",
    "Healthcare time-series feature engineering supports hospital capacity planning, epidemic forecasting, and pharmaceutical demand prediction.\n",
    "\n",
    "### **14.4.1 Patient Patterns**\n",
    "\n",
    "Patient flow patterns exhibit strong temporal regularity (weekly seasonality, annual flu seasons).\n",
    "\n",
    "```python\n",
    "def calculate_patient_flow_features(df, admission_col='admissions', date_col='date'):\n",
    "    \"\"\"\n",
    "    Calculate features for hospital patient flow prediction.\n",
    "    \n",
    "    NEPSE Applications: Hospital stocks, pharmaceutical demand,\n",
    "    health insurance claim prediction.\n",
    "    \"\"\"\n",
    "    df = df.copy()\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    features = pd.DataFrame(index=df.index)\n",
    "    \n",
    "    # 1. Temporal Patterns\n",
    "    features['Day_of_Week'] = df[date_col].dt.dayofweek\n",
    "    features['Is_Weekend'] = features['Day_of_Week'].isin([5, 6]).astype(int)\n",
    "    features['Month'] = df[date_col].dt.month\n",
    "    \n",
    "    # 2. Lag Features (autoregressive)\n",
    "    for lag in [1, 7, 14]:\n",
    "        features[f'Admissions_Lag_{lag}'] = df[admission_col].shift(lag)\n",
    "    \n",
    "    # 3. Rolling Averages (trend)\n",
    "    features['Admissions_MA_7'] = df[admission_col].rolling(7).mean()\n",
    "    features['Admissions_MA_30'] = df[admission_col].rolling(30).mean()\n",
    "    \n",
    "    # 4. Growth Rates\n",
    "    features['Admissions_Growth'] = df[admission_col].pct_change(7)\n",
    "    \n",
    "    # 5. Epidemic Detection (CUSUM algorithm)\n",
    "    baseline = df[admission_col].rolling(30).mean()\n",
    "    deviation = df[admission_col] - baseline\n",
    "    features['Epidemic_Signal'] = deviation.rolling(7).sum()  # Cumulative deviation\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Patient flow features predict **hospital occupancy rates** for NEPSE healthcare sector analysis. The **CUSUM** (Cumulative Sum) algorithm detects when admissions deviate from baseline—early warning for disease outbreaks that drive pharmaceutical stock volumes. Weekend vs weekday patterns help predict emergency vs elective procedure revenue mixes for hospital financial forecasting.\n",
    "\n",
    "### **14.4.2 Vital Sign Features**\n",
    "\n",
    "Vital signs require trend decomposition and anomaly detection features.\n",
    "\n",
    "```python\n",
    "def calculate_vital_sign_features(vital_df):\n",
    "    \"\"\"\n",
    "    Calculate features from patient vital signs (HR, BP, Temp, SpO2).\n",
    "    \n",
    "    NEPSE: Telemedicine platforms, wearable device companies,\n",
    "    health insurance risk scoring.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 1. Heart Rate Variability (HRV)\n",
    "    rr_intervals = 60000 / vital_df['heart_rate']  # Convert HR to RR intervals (ms)\n",
    "    features['HRV_SDNN'] = rr_intervals.rolling(5).std()  # Standard deviation of NN intervals\n",
    "    \n",
    "    # 2. Blood Pressure Trends\n",
    "    features['BP_Pulse_Pressure'] = vital_df['systolic'] - vital_df['diastolic']\n",
    "    features['BP_Mean_Arterial'] = vital_df['diastolic'] + (features['BP_Pulse_Pressure'] / 3)\n",
    "    \n",
    "    # 3. Temperature Trajectory\n",
    "    features['Temp_Velocity'] = vital_df['temperature'].diff()\n",
    "    features['Temp_Acceleration'] = features['Temp_Velocity'].diff()\n",
    "    \n",
    "    # 4. Early Warning Score (simplified NEWS)\n",
    "    # Based on deviation from normal ranges\n",
    "    def calculate_news(row):\n",
    "        score = 0\n",
    "        if row['heart_rate'] < 40 or row['heart_rate'] > 120: score += 3\n",
    "        if row['systolic'] < 90: score += 3\n",
    "        if row['temperature'] > 39: score += 2\n",
    "        if row['spo2'] < 92: score += 3\n",
    "        return score\n",
    "    \n",
    "    features['NEWS_Score'] = vital_df.apply(calculate_news, axis=1)\n",
    "    features['Critical_Alert'] = (features['NEWS_Score'] >= 6).astype(int)\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Vital sign features enable **remote patient monitoring** business models for NEPSE healthcare technology stocks. The **NEWS (National Early Warning Score)** aggregates multiple vital signs into a single risk metric—when NEWS > 6, mortality risk rises exponentially, triggering intervention. For Nepali insurance companies, these features predict claim severity and enable dynamic pricing for health policies.\n",
    "\n",
    "### **14.4.3 Treatment Features**\n",
    "\n",
    "Treatment adherence and efficacy features support pharmaceutical and healthcare service analytics.\n",
    "\n",
    "```python\n",
    "def calculate_treatment_features(medication_df):\n",
    "    \"\"\"\n",
    "    Calculate medication adherence and treatment response features.\n",
    "    \n",
    "    NEPSE: Pharmaceutical distribution companies, pharmacy chains.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 1. Medication Possession Ratio (MPR)\n",
    "    # Days supply / Days in period\n",
    "    features['MPR'] = medication_df['days_supply'] / 30\n",
    "    \n",
    "    # 2. Adherence Gap (days between refills)\n",
    "    features['Refill_Gap'] = medication_df['date'].diff().dt.days - medication_df['days_supply'].shift(1)\n",
    "    features['Non_Adherent'] = (features['Refill_Gap'] > 7).astype(int)  # Gap > 7 days\n",
    "    \n",
    "    # 3. Treatment Persistence\n",
    "    features['Days_on_Therapy'] = medication_df.groupby('patient_id')['days_supply'].cumsum()\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "### **14.4.4 Compliance Features**\n",
    "\n",
    "Regulatory compliance features predict audit risk and quality metrics.\n",
    "\n",
    "```python\n",
    "def calculate_compliance_features(audit_df):\n",
    "    \"\"\"\n",
    "    Calculate healthcare compliance and quality metrics.\n",
    "    \n",
    "    NEPSE: Hospital accreditation status, insurance network compliance.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 1. Documentation Completeness\n",
    "    required_fields = ['diagnosis_code', 'procedure_code', 'provider_id']\n",
    "    features['Doc_Completeness'] = audit_df[required_fields].notna().mean(axis=1)\n",
    "    \n",
    "    # 2. Time-to-Compliance (days to resolve audit findings)\n",
    "    features['Days_to_Resolve'] = (audit_df['resolution_date'] - audit_df['finding_date']).dt.days\n",
    "    \n",
    "    # 3. Recurring Issues (same deficiency multiple times)\n",
    "    features['Repeat_Offense'] = audit_df.groupby('deficiency_type').cumcount()\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.5 IoT and Sensor Features**\n",
    "\n",
    "Internet of Things (IoT) features process high-frequency sensor data for predictive maintenance and operational optimization.\n",
    "\n",
    "### **14.5.1 Signal Processing**\n",
    "\n",
    "Signal processing features extract meaningful patterns from noisy sensor streams.\n",
    "\n",
    "```python\n",
    "def calculate_signal_features(signal_series, sampling_rate=100):\n",
    "    \"\"\"\n",
    "    Calculate signal processing features from IoT sensors.\n",
    "    \n",
    "    NEPSE Applications: Manufacturing sector (predictive maintenance),\n",
    "    Hydropower (turbine vibration monitoring), Telecom (network quality).\n",
    "    \"\"\"\n",
    "    from scipy import signal as scipy_signal\n",
    "    from scipy.stats import kurtosis, skew\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 1. Statistical Moments\n",
    "    features['Mean'] = signal_series.rolling(window=sampling_rate).mean()\n",
    "    features['Std'] = signal_series.rolling(window=sampling_rate).std()\n",
    "    features['Skewness'] = signal_series.rolling(window=sampling_rate).skew()\n",
    "    features['Kurtosis'] = signal_series.rolling(window=sampling_rate).apply(kurtosis)\n",
    "    \n",
    "    # 2. Peak Detection\n",
    "    peaks, _ = scipy_signal.find_peaks(signal_series, height=signal_series.mean() + 2*signal_series.std())\n",
    "    features['Peak_Count'] = pd.Series(peaks).rolling(window=sampling_rate).count()\n",
    "    \n",
    "    # 3. Zero Crossing Rate (signal noise indicator)\n",
    "    zero_crossings = ((signal_series.shift(1) * signal_series) < 0).astype(int)\n",
    "    features['Zero_Crossing_Rate'] = zero_crossings.rolling(window=sampling_rate).mean()\n",
    "    \n",
    "    # 4. Signal Energy\n",
    "    features['Signal_Energy'] = (signal_series ** 2).rolling(window=sampling_rate).sum()\n",
    "    \n",
    "    return features\n",
    "\n",
    "# NEPSE Application:\n",
    "# For hydropower companies (major NEPSE sector), vibration sensor features\n",
    "# predict turbine maintenance needs. High Kurtosis indicates bearing faults.\n",
    "# Energy spikes predict cavitation events in turbines.\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Signal processing features convert raw vibration, temperature, or pressure sensor data into diagnostic metrics. For NEPSE's substantial **hydropower sector**, vibration analysis predicts turbine failures before they occur. **Kurtosis** (measure of distribution \"tailedness\") spikes when mechanical bearings develop faults—providing 2-4 weeks warning of failures that would cause generation outages. **Zero Crossing Rate** distinguishes between normal operational noise and abnormal mechanical degradation.\n",
    "\n",
    "### **14.5.2 Frequency Features**\n",
    "\n",
    "Frequency domain analysis detects cyclical patterns and resonant frequencies.\n",
    "\n",
    "```python\n",
    "def calculate_frequency_features(signal_series, fs=100):\n",
    "    \"\"\"\n",
    "    Calculate frequency domain features using FFT.\n",
    "    \n",
    "    NEPSE: Manufacturing quality control, structural health monitoring\n",
    "    of infrastructure assets (bridges, transmission towers).\n",
    "    \"\"\"\n",
    "    from scipy.fft import fft\n",
    "    \n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # Windowed FFT\n",
    "    window_size = fs * 10  # 10 seconds of data\n",
    "    features_list = []\n",
    "    \n",
    "    for i in range(0, len(signal_series) - window_size, window_size):\n",
    "        window = signal_series.iloc[i:i+window_size]\n",
    "        fft_vals = fft(window)\n",
    "        freqs = np.fft.fftfreq(len(window), 1/fs)\n",
    "        \n",
    "        # Dominant Frequency\n",
    "        dominant_freq = freqs[np.argmax(np.abs(fft_vals))]\n",
    "        features_list.append({\n",
    "            'Dominant_Freq': dominant_freq,\n",
    "            'Spectral_Entropy': -np.sum(np.abs(fft_vals) * np.log(np.abs(fft_vals) + 1e-10)),\n",
    "            'High_Freq_Energy': np.sum(np.abs(fft_vals[freqs > 20]))  # >20Hz\n",
    "        })\n",
    "    \n",
    "    return pd.DataFrame(features_list)\n",
    "```\n",
    "\n",
    "### **14.5.3 Anomaly Features**\n",
    "\n",
    "Anomaly detection features identify out-of-distribution sensor readings.\n",
    "\n",
    "```python\n",
    "def calculate_anomaly_features(sensor_df):\n",
    "    \"\"\"\n",
    "    Calculate anomaly detection features for IoT sensors.\n",
    "    \n",
    "    NEPSE: Smart factory implementations, cold chain logistics\n",
    "    for pharmaceutical distribution.\n",
    "    \"\"\"\n",
    "    features = pd.DataFrame()\n",
    "    \n",
    "    # 1. Isolation Forest Score (proxied by Mahalanobis distance)\n",
    "    from scipy.spatial.distance import mahalanobis\n",
    "    \n",
    "    # Simplified: Z-score across multiple sensors\n",
    "    sensor_cols = ['temp', 'vibration', 'pressure']\n",
    "    means = sensor_df[sensor_cols].mean()\n",
    "    stds = sensor_df[sensor_cols].std()\n",
    "    \n",
    "    features['Anomaly_Score'] = np.sqrt(((sensor_df[sensor_cols] - means) / stds) ** 2).sum(axis=1)\n",
    "    \n",
    "    # 2. Deviation from Baseline (operating envelope)\n",
    "    baseline = sensor_df[sensor_cols].rolling(1440).mean()  # 24-hour baseline\n",
    "    features['Envelope_Deviation'] = abs(sensor_df[sensor_cols] - baseline).mean(axis=1)\n",
    "    \n",
    "    # 3. Change Point Detection (CUSUM)\n",
    "    features['Change_Point'] = (features['Anomaly_Score'] > features['Anomaly_Score'].quantile(0.99)).astype(int)\n",
    "    \n",
    "    return features\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **14.6 Domain Knowledge Integration**\n",
    "\n",
    "Domain knowledge integration ensures features align with economic reality rather than statistical artifacts.\n",
    "\n",
    "```python\n",
    "class DomainKnowledgeIntegrator:\n",
    "    \"\"\"\n",
    "    Integrate external domain knowledge into feature engineering.\n",
    "    \n",
    "    For NEPSE: Incorporate monetary policy dates, fiscal year dynamics\n",
    "    (Nepal fiscal year ends mid-July), and political event calendars.\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.nepse_calendar = self._load_nepse_calendar()\n",
    "        \n",
    "    def _load_nepse_calendar(self):\n",
    "        \"\"\"Load NEPSE-specific event calendar\"\"\"\n",
    "        return {\n",
    "            'fiscal_year_end': ['2023-07-16', '2024-07-15'],  # Shrawan end\n",
    "            'monetary_policy_dates': ['2023-02-01', '2023-08-01'],\n",
    "            'budget_speech_dates': ['2023-05-28'],  # Jestha 15\n",
    "            'market_holidays': ['2023-10-24', '2023-11-12', '2023-11-13']  # Dashain/Tihar\n",
    "        }\n",
    "    \n",
    "    def add_policy_features(self, df):\n",
    "        \"\"\"Add central bank policy impact features\"\"\"\n",
    "        df['Days_Since_Monetary_Policy'] = (df.index - pd.to_datetime(self.nepse_calendar['monetary_policy_dates'])).min()\n",
    "        df['Policy_Impact_Window'] = (df['Days_Since_Monetary_Policy'] <= 7).astype(int)\n",
    "        return df\n",
    "    \n",
    "    def add_fiscal_year_features(self, df):\n",
    "        \"\"\"Nepal fiscal year ends mid-July (unlike calendar year)\"\"\"\n",
    "        df['Fiscal_Year'] = df.index.map(lambda x: x.year if x.month < 7 else x.year + 1)\n",
    "        df['Fiscal_Quarter'] = df.index.map(lambda x: ((x.month - 7) % 12) // 3 + 1)\n",
    "        df['Is_Fiscal_Year_End'] = (df.index.month == 7) & (df.index.day > 10)\n",
    "        df['Is_Budget_Month'] = df.index.month == 5  # Jestha (May-June)\n",
    "        return df\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `DomainKnowledgeIntegrator` injects **Nepal-specific economic calendar** effects into features. Nepal's **fiscal year ends in mid-July** (Shrawan month), creating distinct earnings seasons from Indian or Western markets. **Monetary policy** announcements by Nepal Rastra Bank (central bank) impact banking sector stocks with a specific lag structure. **Budget month** (May/Jestha) features increased volatility as investors anticipate sector-specific allocations (agriculture subsidies, infrastructure spending). These calendar effects are invisible to standard feature engineering but critical for NEPSE prediction accuracy.\n",
    "\n",
    "---\n",
    "\n",
    "## **14.7 Feature Domain Adaptation**\n",
    "\n",
    "When transferring models between domains (e.g., from Indian NSE to Nepali NEPSE), features require adaptation.\n",
    "\n",
    "```python\n",
    "class FeatureDomainAdapter:\n",
    "    \"\"\"\n",
    "    Adapt features trained on one domain (e.g., NSE India) \n",
    "    to another domain (NEPSE Nepal).\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, source_stats, target_stats):\n",
    "        self.source_mean = source_stats['mean']\n",
    "        self.source_std = source_stats['std']\n",
    "        self.target_mean = target_stats['mean']\n",
    "        self.target_std = target_stats['std']\n",
    "        \n",
    "    def zscore_normalize(self, features):\n",
    "        \"\"\"Standardize using target domain statistics\"\"\"\n",
    "        return (features - self.target_mean) / self.target_std\n",
    "    \n",
    "    def quantile_transform(self, features, source_quantiles, target_quantiles):\n",
    "        \"\"\"\n",
    "        Transform features to match target domain distribution.\n",
    "        Critical for volume features (NSE volume >> NEPSE volume).\n",
    "        \"\"\"\n",
    "        from scipy.interpolate import interp1d\n",
    "        return interp1d(source_quantiles, target_quantiles, \n",
    "                       kind='linear', fill_value='extrapolate')(features)\n",
    "    \n",
    "    def adapt_volatility_regime(self, volatility_series, source_regime_thresholds):\n",
    "        \"\"\"\n",
    "        Adjust volatility thresholds for NEPSE's higher volatility environment.\n",
    "        \"\"\"\n",
    "        # NEPSE typically has 1.5x volatility of NSE\n",
    "        adapted_thresholds = {k: v * 1.5 for k, v in source_regime_thresholds.items()}\n",
    "        return pd.cut(volatility_series, \n",
    "                     bins=[-np.inf] + list(adapted_thresholds.values()) + [np.inf])\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Domain adaptation addresses **distributional shifts** between markets. NEPSE stocks exhibit approximately **1.5x the volatility** of NSE (India) stocks due to lower liquidity. Simple transfer learning fails because a \"high volatility\" feature threshold (e.g., 2% daily move) that indicates crisis in India indicates normal trading in Nepal. The adapter rescales thresholds and performs quantile matching to ensure features have consistent semantic meaning across domains (e.g., \"90th percentile volatility\" rather than \"2% move\").\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 14**\n",
    "\n",
    "This chapter covered domain-specific feature engineering across Financial (NEPSE-focused), Retail, Weather, Healthcare, and IoT domains, emphasizing the integration of specialized knowledge into predictive features.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
