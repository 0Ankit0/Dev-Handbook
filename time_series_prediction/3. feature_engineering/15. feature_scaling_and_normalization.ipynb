{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Chapter 15: Feature Scaling and Normalization**\n",
    "\n",
    "## **15.1 Why Scale Features?**\n",
    "\n",
    "Feature scaling transforms features to comparable ranges, essential for machine learning algorithms sensitive to magnitude differences. In NEPSE (Nepal Stock Exchange) prediction systems, scaling addresses fundamental challenges arising from heterogeneous data dimensions.\n",
    "\n",
    "**The Magnitude Problem in NEPSE Data:**\n",
    "Consider a typical NEPSE dataset row:\n",
    "- **Close Price**: Rs. 450.50 (hundreds)\n",
    "- **Volume**: 125,000 shares (hundreds of thousands)\n",
    "- **Turnover**: NPR 56,250,000 (millions)\n",
    "- **Diff %**: 2.35 (units)\n",
    "- **RSI**: 68.5 (tens)\n",
    "\n",
    "Without scaling, distance-based algorithms (KNN, SVM, Neural Networks) interpret turnover (millions) as vastly more important than RSI (tens), regardless of actual predictive power. Gradient-based optimizers (SGD, Adam) converge slowly when features have vastly different scales, requiring different learning rates per feature.\n",
    "\n",
    "**Specific NEPSE Challenges:**\n",
    "1. **Price Heterogeneity**: NEPSE stocks range from Rs. 10 (penny stocks) to Rs. 10,000+ (blue chips like NTC). Raw price levels obscure percentage-based patterns.\n",
    "2. **Volume Variability**: Institutional stocks trade millions of shares daily; illiquid micro-caps trade hundreds. Raw volume creates outliers that dominate models.\n",
    "3. **Indicator Mixing**: Technical indicators (0-100 range) mixed with fundamental ratios (0-50 range) and absolute prices (10-10,000 range) require harmonization.\n",
    "4. **Temporal Drift**: As NEPSE indices rise over years, absolute price levels inflate, making cross-temporal model deployment impossible without scaling.\n",
    "\n",
    "**When Scaling is Critical:**\n",
    "- **Distance-based algorithms**: KNN, K-Means, SVM (RBF kernel), PCA\n",
    "- **Gradient descent**: Neural Networks, Linear/Logistic Regression\n",
    "- **Regularization**: L1/L2 penalties apply uniformly; unscaled features receive inappropriate penalization\n",
    "- **Ensemble methods**: Though tree-based models (Random Forest, XGBoost) are scale-invariant, scaling improves numerical stability\n",
    "\n",
    "**When Scaling is Optional:**\n",
    "- Tree-based models: Decision Trees, Random Forest, Gradient Boosting (scale-invariant splits)\n",
    "- Rule-based systems: If-then logic unaffected by magnitude\n",
    "\n",
    "---\n",
    "\n",
    "## **15.2 Standardization (Z-Score)**\n",
    "\n",
    "Standardization transforms features to have zero mean and unit variance, preserving distribution shape while removing location and scale parameters.\n",
    "\n",
    "**Mathematical Foundation:**\n",
    "$$z = \\frac{x - \\mu}{\\sigma}$$\n",
    "\n",
    "Where $\\mu$ is the population mean and $\\sigma$ is the standard deviation. For NEPSE time-series, we use rolling statistics to prevent lookahead bias.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def calculate_rolling_zscore(df, column, window=252, min_periods=30):\n",
    "    \"\"\"\n",
    "    Calculate rolling Z-Score for NEPSE time-series features.\n",
    "    \n",
    "    Uses expanding window for initial periods to maximize data usage,\n",
    "    then rolling window to maintain stationarity.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    df : pd.DataFrame\n",
    "        NEPSE data with time-series index\n",
    "    column : str\n",
    "        Column to standardize (e.g., 'Close', 'Volume', 'Turnover')\n",
    "    window : int\n",
    "        Rolling window size (default 252 ≈ 1 year of NEPSE trading days)\n",
    "    min_periods : int\n",
    "        Minimum observations required for calculation\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    pd.Series\n",
    "        Z-Score normalized values\n",
    "    \"\"\"\n",
    "    # Calculate rolling statistics\n",
    "    rolling_mean = df[column].rolling(window=window, min_periods=min_periods).mean()\n",
    "    rolling_std = df[column].rolling(window=window, min_periods=min_periods).std()\n",
    "    \n",
    "    # Handle zero standard deviation (constant values)\n",
    "    rolling_std = rolling_std.replace(0, np.nan)\n",
    "    \n",
    "    # Calculate Z-Score\n",
    "    zscore = (df[column] - rolling_mean) / rolling_std\n",
    "    \n",
    "    # Forward fill for initial NaN periods (optional, or use expanding window)\n",
    "    zscore = zscore.fillna(method='ffill')\n",
    "    \n",
    "    return zscore\n",
    "\n",
    "def standardize_nepse_features(df, feature_groups=None):\n",
    "    \"\"\"\n",
    "    Apply appropriate standardization to different NEPSE feature groups.\n",
    "    \n",
    "    Different feature types require different window sizes:\n",
    "    - Price features: Long window (252 days) to capture regime changes\n",
    "    - Volume features: Medium window (63 days ≈ 3 months) for liquidity cycles\n",
    "    - Volatility features: Short window (21 days) for current risk regime\n",
    "    \"\"\"\n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    if feature_groups is None:\n",
    "        feature_groups = {\n",
    "            'price': ['Close', 'Open', 'High', 'Low', 'VWAP'],\n",
    "            'volume': ['Vol', 'Turnover'],\n",
    "            'volatility': ['ATR', 'Range_Pct'],\n",
    "            'technical': ['RSI', 'MACD', 'Stoch_K']  # Already bounded, but scale for consistency\n",
    "        }\n",
    "    \n",
    "    window_map = {\n",
    "        'price': 252,      # Annual cycle\n",
    "        'volume': 63,      # Quarterly liquidity patterns\n",
    "        'volatility': 21,  # Monthly risk regime\n",
    "        'technical': 63   # Quarterly momentum cycles\n",
    "    }\n",
    "    \n",
    "    for group, columns in feature_groups.items():\n",
    "        window = window_map.get(group, 63)\n",
    "        \n",
    "        for col in columns:\n",
    "            if col in df.columns:\n",
    "                # Use expanding window for first 'window' days, then rolling\n",
    "                df_scaled[f'{col}_Z'] = calculate_rolling_zscore(\n",
    "                    df, col, window=window, min_periods=min_periods\n",
    "                )\n",
    "                \n",
    "                # Alternative: Group by Symbol for cross-sectional standardization\n",
    "                # Useful for comparing stocks within same timeframe\n",
    "                df_scaled[f'{col}_Z_XS'] = df.groupby('Symbol')[col].transform(\n",
    "                    lambda x: (x - x.rolling(window).mean()) / x.rolling(window).std()\n",
    "                )\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "# Detailed Explanation:\n",
    "#\n",
    "# 1. Rolling vs Expanding Windows:\n",
    "#    - Rolling(window=252): Uses only last 252 days, adapts to regime changes\n",
    "#    - Expanding: Uses all history since start, stable but slow to adapt\n",
    "#    - For NEPSE, rolling is preferred because market regimes shift (bull/bear cycles)\n",
    "#\n",
    "# 2. Why 252 days?\n",
    "#    - NEPSE trades ~252 days per year (Sunday-Thursday, minus holidays)\n",
    "#    - Annual window captures full business cycle, earnings seasons, and tax year effects\n",
    "#    - Shorter windows (20, 60) create noise; longer windows (500) lag regime shifts\n",
    "#\n",
    "# 3. Cross-Sectional Standardization (Z_XS suffix):\n",
    "#    - Groups by Symbol, calculates Z-score relative to stock's own history\n",
    "#    - Essential for NEPSE because different stocks have different volatility profiles\n",
    "#    - Example: NTC (low vol, Rs 800) and micro-cap (high vol, Rs 50) become comparable\n",
    "#\n",
    "# 4. Handling Zero Std:\n",
    "#    - Some features (like Diff % on holidays) may have zero variance\n",
    "#    - Replace with NaN to avoid division by zero, then forward fill or drop\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "The `calculate_rolling_zscore` function implements time-series aware standardization critical for NEPSE data. Unlike `StandardScaler` from sklearn (which uses full dataset statistics), this function uses **rolling windows** to prevent lookahead bias—essential for financial time-series where future information cannot influence past scaling.\n",
    "\n",
    "**Rolling vs. Expanding:**\n",
    "The function uses a rolling window of 252 days (approximately one NEPSE trading year) rather than expanding statistics. This ensures the scaler adapts to **regime changes**—for example, when NEPSE transitions from a low-volatility bull market to high-volatility bear market, the Z-scores recalibrate using only recent data, preventing the \"ghost\" of old regimes from distorting current signals.\n",
    "\n",
    "**Cross-Sectional Standardization:**\n",
    "The `standardize_nepse_features` function creates two versions of each feature:\n",
    "1. **Time-series Z** (`Close_Z`): Relative to stock's own history (rolling mean/std)\n",
    "2. **Cross-sectional Z** (`Close_Z_XS`): Relative to stock's specific behavior patterns using `groupby('Symbol')`\n",
    "\n",
    "This is crucial for NEPSE because comparing a stable blue-chip (NTC, volatility 15% annually) against a speculative micro-cap (volatility 80%) requires each to be scaled relative to its own baseline. A raw price move of Rs. 10 is significant for a Rs. 50 stock (20%) but noise for a Rs. 1000 stock (1%). Cross-sectional Z-scores normalize these differences.\n",
    "\n",
    "**NEPSE-Specific Considerations:**\n",
    "- **252-day window**: Matches Nepal's fiscal year and captures full earnings cycles\n",
    "- **Min_periods=30**: Ensures at least one month of data before calculating (prevents erratic early values)\n",
    "- **Holiday handling**: NEPSE observes Nepali holidays; pandas rolling automatically handles irregular trading days\n",
    "\n",
    "---\n",
    "\n",
    "## **15.3 Min-Max Normalization**\n",
    "\n",
    "Min-Max scaling transforms features to a fixed range (typically [0, 1] or [-1, 1]), preserving zero values and maintaining the original distribution shape (unlike Z-score which forces normal distribution).\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$x_{scaled} = \\frac{x - x_{min}}{x_{max} - x_{min}}$$\n",
    "\n",
    "For NEPSE bounded indicators (RSI, Stochastic, Williams %R), Min-Max is natural since these already have theoretical bounds (0-100).\n",
    "\n",
    "```python\n",
    "def calculate_rolling_minmax(df, column, window=252, feature_range=(0, 1)):\n",
    "    \"\"\"\n",
    "    Calculate rolling Min-Max scaling for NEPSE features.\n",
    "    \n",
    "    Critical for bounded indicators and neural network inputs.\n",
    "    Handles look-ahead bias by using only past data for min/max calculation.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    feature_range : tuple\n",
    "        Desired output range, default (0, 1)\n",
    "    \"\"\"\n",
    "    min_val, max_val = feature_range\n",
    "    \n",
    "    # Rolling min/max (lookback only)\n",
    "    rolling_min = df[column].rolling(window=window, min_periods=30).min()\n",
    "    rolling_max = df[column].rolling(window=window, min_periods=30).max()\n",
    "    \n",
    "    # Scale to range\n",
    "    range_size = rolling_max - rolling_min\n",
    "    range_size = range_size.replace(0, np.nan)  # Avoid division by zero\n",
    "    \n",
    "    scaled = (df[column] - rolling_min) / range_size\n",
    "    scaled = scaled * (max_val - min_val) + min_val\n",
    "    \n",
    "    return scaled\n",
    "\n",
    "def minmax_scale_nepse_indicators(df):\n",
    "    \"\"\"\n",
    "    Apply Min-Max scaling to bounded NEPSE technical indicators.\n",
    "    \n",
    "    Appropriate for:\n",
    "    - RSI (0-100) → Scale to (0, 1)\n",
    "    - Stochastic Oscillator (0-100)\n",
    "    - Position in Range (0-100)\n",
    "    - Percentile Rank (0-100)\n",
    "    \"\"\"\n",
    "    bounded_indicators = ['RSI', 'Stoch_K', 'Stoch_D', 'Position_52W_Range', \n",
    "                        'Percentile_20', 'Close_Location']\n",
    "    \n",
    "    df_scaled = df.copy()\n",
    "    \n",
    "    for indicator in bounded_indicators:\n",
    "        if indicator in df.columns:\n",
    "            # For bounded indicators, use fixed theoretical bounds\n",
    "            if indicator in ['RSI', 'Stoch_K', 'Stoch_D']:\n",
    "                theoretical_min, theoretical_max = 0, 100\n",
    "            elif 'Percentile' in indicator or 'Position' in indicator:\n",
    "                theoretical_min, theoretical_max = 0, 100\n",
    "            else:\n",
    "                # For others, use rolling min/max\n",
    "                theoretical_min = df[indicator].rolling(252).min()\n",
    "                theoretical_max = df[indicator].rolling(252).max()\n",
    "            \n",
    "            # Scale using theoretical bounds (prevents future data leakage)\n",
    "            df_scaled[f'{indicator}_Norm'] = (df[indicator] - theoretical_min) / \\\n",
    "                                           (theoretical_max - theoretical_min)\n",
    "    \n",
    "    # Special handling for Price data (dynamic range, but bounded recently)\n",
    "    # Use recent 252-day high/low for normalization (support/resistance levels)\n",
    "    df_scaled['Close_Norm'] = calculate_rolling_minmax(df, 'Close', window=252, feature_range=(0, 1))\n",
    "    \n",
    "    return df_scaled\n",
    "\n",
    "def robust_minmax_scaling(df, column, lookback=252, quantile_range=(0.05, 0.95)):\n",
    "    \"\"\"\n",
    "    Robust Min-Max using percentiles instead of min/max to handle outliers.\n",
    "    \n",
    "    Critical for NEPSE volume data which has extreme spikes during news events.\n",
    "    \"\"\"\n",
    "    lower_q = df[column].rolling(lookback).quantile(quantile_range[0])\n",
    "    upper_q = df[column].rolling(lookback).quantile(quantile_range[1])\n",
    "    \n",
    "    clipped = df[column].clip(lower_q, upper_q)\n",
    "    scaled = (clipped - lower_q) / (upper_q - lower_q)\n",
    "    \n",
    "    return scaled\n",
    "\n",
    "# Explanation:\n",
    "#\n",
    "# 1. Theoretical vs Empirical Bounds:\n",
    "#    - RSI theoretically bounded 0-100, so use fixed bounds (0, 100)\n",
    "#    - Price has no theoretical bound, so use rolling 252-day high/low\n",
    "#    - Using fixed bounds for bounded indicators prevents look-ahead bias\n",
    "#\n",
    "# 2. Why Min-Max for Neural Networks?\n",
    "#    - Sigmoid/tanh activations saturate at extremes; inputs should be in [-1, 1] or [0, 1]\n",
    "#    - Z-scores can be (-inf, +inf); outliers cause vanishing gradients\n",
    "#    - Min-Max ensures all inputs fall within active region of activation functions\n",
    "#\n",
    "# 3. Robust Scaling:\n",
    "#    - NEPSE volume can spike 10x normal during IPO announcements or earnings\n",
    "#    - Standard Min-Max compresses normal data into tiny range when outliers exist\n",
    "#    - Using 5th-95th percentiles clips outliers while preserving 90% of data distribution\n",
    "```\n",
    "\n",
    "**Code Explanation:**\n",
    "\n",
    "The `calculate_rolling_minmax` function implements **lookback-only Min-Max scaling**, crucial for preventing data leakage in time-series. Unlike sklearn's `MinMaxScaler` which uses global min/max (including future data), this function uses rolling windows where each day's scaling depends only on the past 252 days.\n",
    "\n",
    "**Theoretical vs. Empirical Bounds:**\n",
    "For bounded indicators like RSI (0-100) or Stochastic Oscillator (0-100), the function uses **theoretical bounds** rather than empirical rolling min/max. This is critical because:\n",
    "1. **Prevents compression**: If NEPSE has a strong bull run and RSI never drops below 40 in a year, rolling Min-Max would scale 40→0 and 80→1, losing the absolute overbought/oversold context. Fixed bounds preserve semantic meaning (70+ always = overbought).\n",
    "2. **Avoids look-ahead**: Theoretical bounds don't require historical data to estimate.\n",
    "\n",
    "**Robust Min-Max:**\n",
    "The `robust_minmax_scaling` function addresses NEPSE's **fat-tailed distributions**—particularly volume data where a single corporate announcement can cause 20x normal trading volume. Using percentiles (5th and 95th) instead of absolute min/max prevents these outliers from compressing the scaling of normal trading days into a tiny range.\n",
    "\n",
    "**NEPSE Application:**\n",
    "For LSTM/GRU neural networks processing NEPSE sequences, Min-Max scaling to [-1, 1] is preferred over Z-score because:\n",
    "- LSTM gates (sigmoid) expect inputs in [0, 1]\n",
    "- Tanh hidden states expect [-1, 1]\n",
    "- Z-scores outside [-3, 3] are common in financial data and cause saturation\n",
    "\n",
    "---\n",
    "\n",
    "## **15.4 Robust Scaling**\n",
    "\n",
    "Robust scaling uses median and interquartile range (IQR) instead of mean and standard deviation, making it resistant to outliers—critical for NEPSE's occasional extreme moves.\n",
    "\n",
    "**Mathematical Formula:**\n",
    "$$x_{robust} = \\frac{x - median}{IQR}$$\n",
    "\n",
    "Where $IQR = Q_3 - Q_1$ (75th percentile - 25th percentile).\n",
    "\n",
    "```python\n",
    "def calculate_rolling_robust_scale(df, column, window=252):\n",
    "    \"\"\"\n",
    "    Calculate robust scaling using median and IQR.\n",
    "    \n",
    "    Outliers (e.g., NEPSE circuit breaker days) have minimal impact.\n",
    "    \"\"\"\n",
    "    # Rolling median (50th percentile)\n",
    "    rolling_median = df[column].rolling(window=window).median()\n",
    "    \n",
    "    # Rolling IQR (75th - 25th percentile)\n",
    "    q75 = df[column].rolling(window=window).quantile(0.75)\n",
    "    q25 = df[column].rolling(window=window).quantile(0.25)\n",
    "    iqr = q75 - q25\n",
    "    \n",
    "    # Avoid division by zero\n",
    "    iqr = iqr.replace(0, np.nan)\n",
    "    \n",
    "    robust_scaled = (df[column] - rolling_median) / iqr\n",
    "    \n",
    "    return robust_scaled\n",
    "\n",
    "def detect_and_scale_outliers(df, column, method='robust', threshold=3):\n",
    "    \"\"\"\n",
    "    Detect outliers using robust scaling, then optionally winsorize.\n",
    "    \n",
    "    NEPSE specific: Circuit breaker limits (4% daily move) create natural outliers\n",
    "    that should be preserved (information content) but scaled appropriately.\n",
    "    \"\"\"\n",
    "    if method == 'robust':\n",
    "        scaled = calculate_rolling_robust_scale(df, column)\n",
    "        # Values > 3 or < -3 are outliers (beyond 3 IQRs)\n",
    "        is_outlier = (abs(scaled) > threshold)\n",
    "        \n",
    "        # Winsorize (clip) extreme outliers to threshold\n",
    "        winsorized = scaled.clip(-threshold, threshold)\n",
    "        \n",
    "        return pd.DataFrame({\n",
    "            f'{column}_Robust': scaled,\n",
    "            f'{column}_Winsorized': winsorized,\n",
    "            'Is_Outlier': is_outlier\n",
    "        })\n",
    "    \n",
    "    elif method == 'zscore':\n",
    "        zscore = (df[column] - df[column].rolling(252).mean()) / df[column].rolling(252).std()\n",
    "        is_outlier = (abs(zscore) > threshold)\n",
    "        return pd.DataFrame({\n",
    "            f'{column}_ZScore': zscore,\n",
    "            'Is_Outlier': is_outlier\n",
    "        })\n",
    "\n",
    "# Comparison for NEPSE Circuit Breaker Scenario:\n",
    "# If a stock hits +4% upper circuit (common in NEPSE):\n",
    "# - Z-Score: Might be 5.0 (extreme outlier, dominates model)\n",
    "# - Robust: Might be 2.5 (within bounds, preserves information)\n",
    "#\n",
    "# This is why Robust Scaling is preferred for NEPSE high-volatility periods\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Robust scaling is essential for NEPSE due to **circuit breaker mechanisms** and low-float manipulation risks that create extreme outliers. When a NEPSE stock hits the 4% daily upper circuit (common for Class A stocks), Z-score scaling might mark this as a 5-sigma event (essentially infinite in model terms), causing the observation to dominate loss functions and gradient updates.\n",
    "\n",
    "**Median vs. Mean:**\n",
    "The median is unaffected by extreme values. If NEPSE has 249 normal days and 3 circuit-breaker days, the median remains representative of typical trading, while the mean shifts toward the outliers.\n",
    "\n",
    "**IQR vs. Std:**\n",
    "The IQR (Interquartile Range) measures spread using the middle 50% of data (Q3-Q1), ignoring the extreme tails. This prevents the \"tail wagging the dog\" where rare volatility spikes inflate the denominator and compress normal variations toward zero.\n",
    "\n",
    "**Winsorization:**\n",
    "The `detect_and_scale_outliers` function clips extreme values to ±3 IQRs. This preserves the information that \"today was extreme\" (value = 3) without allowing the exact magnitude (which might be 10 or 100) to distort the model. For NEPSE, this handles flash crashes and pump-and-dump schemes gracefully.\n",
    "\n",
    "---\n",
    "\n",
    "## **15.5 Power Transformations**\n",
    "\n",
    "Power transformations stabilize variance and reduce skewness, making distributions more Gaussian—beneficial for linear models and neural networks assuming normally distributed inputs.\n",
    "\n",
    "### **15.5.1 Log Transformation**\n",
    "\n",
    "Log transformation compresses large values and expands small ones, useful for right-skewed financial data (prices, market caps, turnover).\n",
    "\n",
    "```python\n",
    "def apply_log_transform(df, columns, offset=1):\n",
    "    \"\"\"\n",
    "    Apply natural log transformation to NEPSE financial data.\n",
    "    \n",
    "    Log transform is appropriate for:\n",
    "    - Prices (exponential growth assumption)\n",
    "    - Turnover (highly skewed, multiplicative effects)\n",
    "    - Volume (exponential distribution)\n",
    "    \n",
    "    Offset handles zero values (e.g., zero volume days).\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        # Add offset to handle zeros (log(0) = -inf)\n",
    "        # For NEPSE, use offset=1 for volume (no trade = 1 share minimum)\n",
    "        df_transformed[f'{col}_Log'] = np.log(df[col] + offset)\n",
    "        \n",
    "        # Alternative: Log1p (log(1+x)) for small values\n",
    "        df_transformed[f'{col}_Log1p'] = np.log1p(df[col])\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# NEPSE Example:\n",
    "# Turnover distribution is highly right-skewed:\n",
    "# - Most days: 1-10 million NPR\n",
    "# - High activity days: 100+ million NPR\n",
    "# - Log transform makes this Gaussian, improving linear model performance\n",
    "```\n",
    "\n",
    "### **15.5.2 Box-Cox Transformation**\n",
    "\n",
    "Box-Cox finds the optimal lambda ($\\lambda$) to transform data to normality.\n",
    "\n",
    "$$y(\\lambda) = \\begin{cases} \\frac{y^\\lambda - 1}{\\lambda} & \\text{if } \\lambda \\neq 0 \\\\ \\log(y) & \\text{if } \\lambda = 0 \\end{cases}$$\n",
    "\n",
    "```python\n",
    "from scipy.stats import boxcox\n",
    "from scipy.special import inv_boxcox\n",
    "\n",
    "def apply_boxcox_rolling(df, column, window=252):\n",
    "    \"\"\"\n",
    "    Apply Box-Cox transformation using rolling window.\n",
    "    \n",
    "    Box-Cox requires positive values, so ensure data > 0.\n",
    "    \"\"\"\n",
    "    # Shift to positive if needed (NEPSE returns can be negative)\n",
    "    min_val = df[column].min()\n",
    "    shift = abs(min_val) + 1 if min_val <= 0 else 0\n",
    "    shifted = df[column] + shift\n",
    "    \n",
    "    # Calculate optimal lambda on rolling basis (computationally expensive)\n",
    "    # For production, use fixed lambda or monthly recalculation\n",
    "    lambdas = shifted.rolling(window=window).apply(\n",
    "        lambda x: boxcox(x)[1] if len(x) == window else np.nan,\n",
    "        raw=True\n",
    "    )\n",
    "    \n",
    "    # Apply transformation (simplified - actual implementation needs vectorization)\n",
    "    transformed = (shifted ** lambdas - 1) / lambdas\n",
    "    \n",
    "    return transformed, lambdas, shift\n",
    "\n",
    "# Note: Box-Cox is rarely used in production NEPSE pipelines due to:\n",
    "# 1. Computational cost of rolling lambda estimation\n",
    "# 2. Difficulty of inverse transform for predictions\n",
    "# 3. Yeo-Johnson is more flexible (handles negative values)\n",
    "```\n",
    "\n",
    "### **15.5.3 Yeo-Johnson Transformation**\n",
    "\n",
    "Yeo-Johnson extends Box-Cox to handle negative values (returns can be negative), making it suitable for NEPSE financial data.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "def apply_yeo_johnson(df, columns):\n",
    "    \"\"\"\n",
    "    Apply Yeo-Johnson transformation to NEPSE features.\n",
    "    \n",
    "    Advantages over Box-Cox:\n",
    "    - Handles negative values (returns, price changes)\n",
    "    - More robust for financial time-series\n",
    "    \"\"\"\n",
    "    transformer = PowerTransformer(method='yeo-johnson', standardize=True)\n",
    "    \n",
    "    # Fit on rolling window (prevent look-ahead)\n",
    "    # For time-series, fit on first N days, transform rolling\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        values = df[col].values.reshape(-1, 1)\n",
    "        \n",
    "        # Rolling application (simplified - production would use expanding window fit)\n",
    "        transformed = transformer.fit_transform(values)\n",
    "        df_transformed[f'{col}_YJ'] = transformed.flatten()\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "# Detailed Explanation:\n",
    "#\n",
    "# Yeo-Johnson automatically selects the power parameter to minimize skewness.\n",
    "# For NEPSE:\n",
    "# - Price levels: Usually λ ≈ 0 (logarithmic)\n",
    "# - Returns: Usually λ ≈ 1 (nearly linear, already normal-ish)\n",
    "# - Volume: Usually λ ≈ 0.3 (between log and sqrt)\n",
    "#\n",
    "# The standardize=True parameter also applies Z-score after transformation,\n",
    "# giving two-stage normalization (shape correction + scale standardization).\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Power transformations address **non-normality** in NEPSE data. Financial data typically exhibits:\n",
    "- **Prices**: Exponential growth (log-normal distribution)\n",
    "- **Volume**: Power-law distribution (few days with massive volume)\n",
    "- **Returns**: Fat tails (leptokurtic), though often approximately normal\n",
    "\n",
    "**Log Transformation:**\n",
    "For NEPSE, log prices are more stationary than raw prices. The log transform converts percentage changes to absolute differences ($\\log(P_t) - \\log(P_{t-1}) \\approx \\frac{P_t - P_{t-1}}{P_{t-1}}$), making ARIMA and linear models more effective. For volume/turnover, log compresses the extreme right tail (IPO days with 100x volume) into a manageable range.\n",
    "\n",
    "**Yeo-Johnson:**\n",
    "Preferred over Box-Cox for NEPSE because it handles negative values (returns can be -5%) without shifting. The transformation automatically selects the power parameter $\\lambda$ that makes the data most Gaussian. For NEPSE turnover data, $\\lambda$ typically converges to ~0.3, indicating a transformation between square root and logarithmic—effectively handling the heavy tails of trading activity without over-compressing normal days.\n",
    "\n",
    "---\n",
    "\n",
    "## **15.6 Quantile Transformation**\n",
    "\n",
    "Quantile transformation maps data to a uniform or normal distribution based on empirical quantiles, robust to outliers and effective for highly skewed distributions.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import QuantileTransformer\n",
    "\n",
    "def apply_quantile_transform(df, columns, n_quantiles=1000, output_distribution='normal'):\n",
    "    \"\"\"\n",
    "    Apply quantile transformation to NEPSE features.\n",
    "    \n",
    "    Maps any distribution to Gaussian (if output_distribution='normal') \n",
    "    or Uniform (if 'uniform').\n",
    "    \n",
    "    Best for: Highly skewed features with outliers (Volume, Turnover, ATR)\n",
    "    \"\"\"\n",
    "    df_transformed = df.copy()\n",
    "    \n",
    "    for col in columns:\n",
    "        # Rolling quantile transform (complex - requires storing quantile function)\n",
    "        # Simplified: Use expanding window for quantile estimation\n",
    "        \n",
    "        values = df[col].values.reshape(-1, 1)\n",
    "        \n",
    "        transformer = QuantileTransformer(\n",
    "            n_quantiles=n_quantiles,\n",
    "            output_distribution=output_distribution,\n",
    "            random_state=42\n",
    "        )\n",
    "        \n",
    "        # For time-series: Fit on first 252 days, apply to all\n",
    "        # In practice, use rolling quantile bins\n",
    "        transformed = transformer.fit_transform(values)\n",
    "        df_transformed[f'{col}_QT'] = transformed.flatten()\n",
    "    \n",
    "    return df_transformed\n",
    "\n",
    "def manual_quantile_binning(df, column, n_bins=10):\n",
    "    \"\"\"\n",
    "    Manual quantile binning for interpretable features.\n",
    "    \n",
    "    Creates discrete bins based on deciles (10-quantiles).\n",
    "    Useful for tree-based models and rule extraction.\n",
    "    \"\"\"\n",
    "    # Rolling quantile bins (time-series aware)\n",
    "    bins = [df[column].rolling(252).quantile(i/n_bins) for i in range(n_bins+1)]\n",
    "    \n",
    "    labels = [f'Q{i+1}' for i in range(n_bins)]\n",
    "    df[f'{column}_Decile'] = pd.cut(df[column], bins=bins, labels=labels)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Explanation:\n",
    "#\n",
    "# Quantile Transformation forces any distribution into Gaussian shape.\n",
    "# For NEPSE Volume (exponentially distributed):\n",
    "# - Raw: 95% of values < 100k, 5% > 1M (extreme skew)\n",
    "# - After QT: Gaussian distribution, outliers preserved but normalized\n",
    "#\n",
    "# Use case: Neural networks that assume Gaussian inputs perform better\n",
    "# with QT than with raw volume data.\n",
    "#\n",
    "# Warning: Destroys linear relationships (rank-preserving but not distance-preserving)\n",
    "# Do not use when linear relationships between features matter.\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Quantile transformation is **non-linear** and **rank-preserving**, making it ideal for NEPSE data with extreme outliers that must be preserved but normalized. Unlike Z-score (which assumes Gaussianity) or Min-Max (which assumes fixed bounds), Quantile Transform learns the empirical distribution and maps it to a Gaussian.\n",
    "\n",
    "**NEPSE Volume Example:**\n",
    "Raw NEPSE volume follows an exponential distribution—most days trade 50,000 shares, but 1% of days trade 5,000,000+ shares (50x average). Z-scoring makes the high-volume days extreme outliers (Z=10+), potentially treated as missing values by algorithms. Quantile transform maps the 99th percentile to ~2.3 (Gaussian), preserving the information that \"this was a high volume day\" without distorting the scale.\n",
    "\n",
    "**Discrete Quantile Binning:**\n",
    "The `manual_quantile_binning` function creates deciles (10 bins) based on rolling quantiles. This is useful for:\n",
    "1. **Interpretability**: \"Today was in the top decile (Q10) for volume\"\n",
    "2. **Tree models**: Discrete bins often split better than continuous values\n",
    "3. **Robustness**: Bin boundaries adapt to regime changes via rolling calculation\n",
    "\n",
    "---\n",
    "\n",
    "## **15.7 Unit Vector Normalization**\n",
    "\n",
    "Unit vector scaling (L2 normalization) scales rows to have unit norm, useful when the magnitude of the sample vector is irrelevant but the direction (relative composition) matters.\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "def apply_unit_normalization(df, feature_groups):\n",
    "    \"\"\"\n",
    "    Apply L2 normalization to groups of features.\n",
    "    \n",
    "    Useful for: Portfolio construction (weight vectors), \n",
    "    Technical indicator composites (trend direction vs magnitude)\n",
    "    \"\"\"\n",
    "    df_normalized = df.copy()\n",
    "    \n",
    "    for group_name, columns in feature_groups.items():\n",
    "        if not all(col in df.columns for col in columns):\n",
    "            continue\n",
    "            \n",
    "        # Extract group\n",
    "        X = df[columns].values\n",
    "        \n",
    "        # L2 normalization: x / sqrt(sum(x^2))\n",
    "        # Each row becomes unit vector\n",
    "        norms = np.sqrt(np.sum(X**2, axis=1))\n",
    "        norms = np.where(norms == 0, 1, norms)  # Avoid division by zero\n",
    "        \n",
    "        X_normalized = X / norms[:, np.newaxis]\n",
    "        \n",
    "        # Store back\n",
    "        for i, col in enumerate(columns):\n",
    "            df_normalized[f'{col}_Unit'] = X_normalized[:, i]\n",
    "    \n",
    "    return df_normalized\n",
    "\n",
    "# NEPSE Application: Multi-factor scoring\n",
    "# Create composite score from Trend, Value, Momentum, Quality\n",
    "# Unit normalize so each factor contributes equally regardless of scale\n",
    "factors = {\n",
    "    'Trend': ['ADX', 'SMA_Slope'],\n",
    "    'Value': ['PE_Ratio', 'PB_Ratio'], \n",
    "    'Momentum': ['RSI', 'MACD'],\n",
    "    'Quality': ['ROE', 'Debt_Equity']\n",
    "}\n",
    "\n",
    "# Without normalization: ADX (0-100) dominates ROE (0-0.3)\n",
    "# With normalization: Each factor has equal weight in composite score\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Unit vector normalization treats each observation (row) as a vector in high-dimensional space and scales it to length 1. This is crucial for **NEPSE multi-factor models** where we combine indicators with different scales (e.g., ADX 0-100, ROE 0-0.30, PE 5-50).\n",
    "\n",
    "**Direction vs. Magnitude:**\n",
    "After unit normalization, the vector indicates the *direction* (which indicators are high/low relative to each other) but not the *magnitude* (how extreme the overall signal is). This is useful for:\n",
    "1. **Relative strength**: Comparing which technical factors are strongest for a given NEPSE stock, regardless of absolute volatility\n",
    "2. **Portfolio weights**: Ensuring position sizing factors sum to 1 (fully invested)\n",
    "3. **Cosine similarity**: Computing similarity between stocks based on factor profiles rather than absolute prices\n",
    "\n",
    "---\n",
    "\n",
    "## **15.8 Scaling for Different Algorithms**\n",
    "\n",
    "Different ML algorithms require different scaling strategies for optimal NEPSE prediction performance.\n",
    "\n",
    "```python\n",
    "def get_scaling_pipeline(algorithm_type):\n",
    "    \"\"\"\n",
    "    Return appropriate scaling strategy based on algorithm.\n",
    "    \n",
    "    Algorithm-specific recommendations for NEPSE data:\n",
    "    \"\"\"\n",
    "    pipelines = {\n",
    "        'linear_regression': {\n",
    "            'scaler': 'StandardScaler',\n",
    "            'reason': 'Assumes Gaussian residuals; standardize for regularization'\n",
    "        },\n",
    "        'neural_network': {\n",
    "            'scaler': 'MinMaxScaler (-1, 1) or QuantileTransformer',\n",
    "            'reason': 'Sigmoid/tanh activations saturate outside [-1, 1] or [0, 1]'\n",
    "        },\n",
    "        'svm_rbf': {\n",
    "            'scaler': 'StandardScaler or RobustScaler',\n",
    "            'reason': 'RBF kernel sensitive to distance; outliers distort kernel matrix'\n",
    "        },\n",
    "        'knn': {\n",
    "            'scaler': 'MinMaxScaler or StandardScaler',\n",
    "            'reason': 'Distance metric dominated by large-scale features'\n",
    "        },\n",
    "        'tree_based': {\n",
    "            'scaler': 'None',\n",
    "            'reason': 'Splits are invariant to monotonic transformations'\n",
    "        },\n",
    "        'pca': {\n",
    "            'scaler': 'StandardScaler',\n",
    "            'reason': 'Maximizes variance along axes; requires unit variance'\n",
    "        },\n",
    "        'lasso_ridge': {\n",
    "            'scaler': 'StandardScaler',\n",
    "            'reason': 'Penalty terms assume comparable feature scales'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    return pipelines.get(algorithm_type, 'StandardScaler')\n",
    "\n",
    "# Implementation for NEPSE Ensemble:\n",
    "def prepare_nepse_data_for_model(df, model_type):\n",
    "    \"\"\"\n",
    "    Prepare NEPSE data with appropriate scaling for specific model type.\n",
    "    \"\"\"\n",
    "    if model_type in ['random_forest', 'xgboost', 'lightgbm']:\n",
    "        # Tree-based: No scaling needed, but clip outliers for stability\n",
    "        return df.clip(df.quantile(0.01), df.quantile(0.99), axis=1)\n",
    "    \n",
    "    elif model_type == 'neural_network':\n",
    "        # LSTM/GRU for NEPSE: Min-Max to [-1, 1]\n",
    "        from sklearn.preprocessing import MinMaxScaler\n",
    "        scaler = MinMaxScaler(feature_range=(-1, 1))\n",
    "        return pd.DataFrame(\n",
    "            scaler.fit_transform(df), \n",
    "            columns=df.columns, \n",
    "            index=df.index\n",
    "        ), scaler\n",
    "    \n",
    "    elif model_type == 'linear_regression':\n",
    "        # Ridge/Lasso for NEPSE factor modeling\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        scaler = StandardScaler()\n",
    "        return pd.DataFrame(\n",
    "            scaler.fit_transform(df),\n",
    "            columns=df.columns,\n",
    "            index=df.index\n",
    "        ), scaler\n",
    "    \n",
    "    elif model_type == 'svm':\n",
    "        # SVM for NEPSE regime classification\n",
    "        from sklearn.preprocessing import RobustScaler\n",
    "        scaler = RobustScaler()\n",
    "        return pd.DataFrame(\n",
    "            scaler.fit_transform(df),\n",
    "            columns=df.columns,\n",
    "            index=df.index\n",
    "        ), scaler\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "This section provides a **decision matrix** for scaling strategies:\n",
    "\n",
    "**Tree-Based Models (XGBoost, LightGBM, Random Forest):**\n",
    "- **No scaling required**: Splits depend on rank order, not absolute values\n",
    "- **But clip outliers**: Extreme values (circuit breakers) can cause numerical instability in gain calculations\n",
    "- **NEPSE tip**: These models often perform best on NEPSE raw data with only outlier clipping\n",
    "\n",
    "**Neural Networks (LSTM for NEPSE time-series):**\n",
    "- **Min-Max [-1, 1]**: Tanh activation outputs [-1, 1]; inputs should match\n",
    "- **Quantile Transform**: If data is highly skewed (volume), use QuantileTransformer first, then Min-Max\n",
    "- **Batch Normalization**: Alternative to manual scaling; learns optimal scaling internally\n",
    "\n",
    "**Linear Models (Ridge/Lasso for NEPSE factor models):**\n",
    "- **StandardScaler essential**: L1/L2 penalties apply equally to all coefficients\n",
    "- **Without scaling**: A feature with scale 1000 (turnover) receives 1/1000th the penalty of scale 1 (RSI), effectively unregularized\n",
    "- **RobustScaler alternative**: If NEPSE data has crash periods (outliers), RobustScaler prevents bias\n",
    "\n",
    "**SVM (for NEPSE regime classification):**\n",
    "- **Standard or Robust**: RBF kernel computes $\\exp(-\\gamma \\|x - x'\\|^2)$; large-scale features dominate distance\n",
    "- **Robust preferred**: NEPSE flash crashes create support vectors that distort the decision boundary if using standard scaling\n",
    "\n",
    "---\n",
    "\n",
    "## **15.9 Scaling in Production Pipelines**\n",
    "\n",
    "Production scaling requires careful handling of train/test splits, online learning, and inverse transformations for predictions.\n",
    "\n",
    "```python\n",
    "class NEPSEScalingPipeline:\n",
    "    \"\"\"\n",
    "    Production-grade scaling pipeline for NEPSE prediction system.\n",
    "    \n",
    "    Handles:\n",
    "    - Time-series aware fitting (no look-ahead bias)\n",
    "    - Online learning (updating scalers with new data)\n",
    "    - Inverse transformations for prediction interpretation\n",
    "    - Feature-specific scaling strategies\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, scaling_config):\n",
    "        self.scalers = {}\n",
    "        self.config = scaling_config\n",
    "        self.fitted = False\n",
    "        \n",
    "    def fit(self, df_train, date_col='Date'):\n",
    "        \"\"\"\n",
    "        Fit scalers on training data up to a specific date.\n",
    "        \"\"\"\n",
    "        # Ensure chronological order\n",
    "        df_train = df_train.sort_values(date_col)\n",
    "        \n",
    "        for feature, params in self.config.items():\n",
    "            if feature not in df_train.columns:\n",
    "                continue\n",
    "                \n",
    "            scaler_type = params['type']\n",
    "            window = params.get('window', 252)\n",
    "            \n",
    "            if scaler_type == 'rolling_zscore':\n",
    "                # Store rolling statistics for transform\n",
    "                self.scalers[feature] = {\n",
    "                    'mean': df_train[feature].rolling(window).mean().iloc[-1],\n",
    "                    'std': df_train[feature].rolling(window).std().iloc[-1],\n",
    "                    'type': 'rolling_zscore',\n",
    "                    'window': window\n",
    "                }\n",
    "            \n",
    "            elif scaler_type == 'minmax':\n",
    "                from sklearn.preprocessing import MinMaxScaler\n",
    "                scaler = MinMaxScaler(feature_range=params.get('range', (0, 1)))\n",
    "                scaler.fit(df_train[[feature]])\n",
    "                self.scalers[feature] = {'scaler': scaler, 'type': 'sklearn'}\n",
    "            \n",
    "            elif scaler_type == 'quantile':\n",
    "                from sklearn.preprocessing import QuantileTransformer\n",
    "                scaler = QuantileTransformer(\n",
    "                    n_quantiles=params.get('n_quantiles', 1000),\n",
    "                    output_distribution=params.get('output', 'normal')\n",
    "                )\n",
    "                scaler.fit(df_train[[feature]])\n",
    "                self.scalers[feature] = {'scaler': scaler, 'type': 'sklearn'}\n",
    "        \n",
    "        self.fitted = True\n",
    "        return self\n",
    "    \n",
    "    def transform(self, df):\n",
    "        \"\"\"\n",
    "        Transform new data using fitted scalers.\n",
    "        \"\"\"\n",
    "        if not self.fitted:\n",
    "            raise ValueError(\"Pipeline not fitted yet\")\n",
    "        \n",
    "        df_scaled = df.copy()\n",
    "        \n",
    "        for feature, scaler_info in self.scalers.items():\n",
    "            if feature not in df.columns:\n",
    "                continue\n",
    "                \n",
    "            if scaler_info['type'] == 'rolling_zscore':\n",
    "                # For rolling zscore, we use the stored statistics\n",
    "                # In production, you'd update these with new data\n",
    "                mean = scaler_info['mean']\n",
    "                std = scaler_info['std']\n",
    "                df_scaled[f'{feature}_scaled'] = (df[feature] - mean) / std\n",
    "            \n",
    "            elif scaler_info['type'] == 'sklearn':\n",
    "                df_scaled[f'{feature}_scaled'] = scaler_info['scaler'].transform(df[[feature]])\n",
    "        \n",
    "        return df_scaled\n",
    "    \n",
    "    def update(self, new_data):\n",
    "        \"\"\"\n",
    "        Update scalers with new data (online learning).\n",
    "        Critical for NEPSE: Market regimes change, scalers must adapt.\n",
    "        \"\"\"\n",
    "        for feature, scaler_info in self.scalers.items():\n",
    "            if feature not in new_data.columns:\n",
    "                continue\n",
    "            \n",
    "            if scaler_info['type'] == 'rolling_zscore':\n",
    "                # Update rolling statistics with exponential decay\n",
    "                alpha = 2 / (scaler_info['window'] + 1)  # EMA decay\n",
    "                new_mean = new_data[feature].mean()\n",
    "                new_std = new_data[feature].std()\n",
    "                \n",
    "                # Exponential moving average update\n",
    "                scaler_info['mean'] = (1 - alpha) * scaler_info['mean'] + alpha * new_mean\n",
    "                scaler_info['std'] = (1 - alpha) * scaler_info['std'] + alpha * new_std\n",
    "            \n",
    "            elif scaler_info['type'] == 'sklearn':\n",
    "                # Partial fit not available for all scalers\n",
    "                # Refit periodically (weekly/monthly) instead\n",
    "                pass\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def inverse_transform_prediction(self, predictions, feature_name):\n",
    "        \"\"\"\n",
    "        Convert scaled predictions back to original units (e.g., stock prices).\n",
    "        \"\"\"\n",
    "        scaler_info = self.scalers.get(feature_name)\n",
    "        \n",
    "        if not scaler_info:\n",
    "            return predictions\n",
    "        \n",
    "        if scaler_info['type'] == 'rolling_zscore':\n",
    "            mean = scaler_info['mean']\n",
    "            std = scaler_info['std']\n",
    "            return (predictions * std) + mean\n",
    "        \n",
    "        elif scaler_info['type'] == 'sklearn':\n",
    "            return scaler_info['scaler'].inverse_transform(predictions.reshape(-1, 1)).flatten()\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "# Usage example for NEPSE production:\n",
    "config = {\n",
    "    'Close': {'type': 'rolling_zscore', 'window': 252},\n",
    "    'Volume': {'type': 'quantile', 'n_quantiles': 1000, 'output': 'normal'},\n",
    "    'RSI': {'type': 'minmax', 'range': (0, 1)}\n",
    "}\n",
    "\n",
    "pipeline = NEPSEScalingPipeline(config)\n",
    "pipeline.fit(train_df)\n",
    "scaled_test = pipeline.transform(test_df)\n",
    "\n",
    "# After model prediction (scaled units)\n",
    "scaled_pred = model.predict(scaled_test)\n",
    "actual_price_pred = pipeline.inverse_transform_prediction(scaled_pred, 'Close')\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "The `NEPSEScalingPipeline` class implements **production-grade scaling** that addresses critical time-series challenges:\n",
    "\n",
    "**No Look-Ahead Bias:**\n",
    "The `fit` method uses only training data (historical) to calculate statistics. Unlike typical ML workflows where `fit_transform` uses global statistics (including future test data), this pipeline respects the temporal boundary—critical for NEPSE backtesting validity.\n",
    "\n",
    "**Online Updating:**\n",
    "The `update` method implements **exponential moving average** updates for rolling statistics. As new NEPSE trading days arrive, the scaler adapts to recent market regimes (volatility changes, new price levels) without requiring full refitting. The decay factor $\\alpha = \\frac{2}{n+1}$ ensures older data gradually loses influence.\n",
    "\n",
    "**Inverse Transform:**\n",
    "Machine learning models predict in scaled space (e.g., Z-scores). The `inverse_transform_prediction` method converts these back to interpretable units (NPR stock prices, actual volume numbers) for trading execution and reporting.\n",
    "\n",
    "**Feature-Specific Strategies:**\n",
    "The config dictionary allows different scaling for different feature types—Z-score for prices (mean-reverting), Quantile for volume (skewed), Min-Max for bounded indicators (RSI).\n",
    "\n",
    "---\n",
    "\n",
    "## **15.10 Inverse Transformation**\n",
    "\n",
    "Inverse transformation converts model outputs back to original units, essential for trading execution and result interpretation.\n",
    "\n",
    "```python\n",
    "def inverse_transform_predictions(scaled_predictions, scaler, method='zscore', \n",
    "                                 original_stats=None):\n",
    "    \"\"\"\n",
    "    Convert scaled predictions back to original NEPSE price/volume units.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    scaled_predictions : np.array\n",
    "        Model output in scaled space (Z-scores or 0-1 range)\n",
    "    scaler : fitted scaler object or dict\n",
    "        Statistics from training data\n",
    "    method : str\n",
    "        'zscore', 'minmax', 'robust', 'log'\n",
    "    original_stats : dict\n",
    "        Required for rolling statistics (mean, std, min, max)\n",
    "    \"\"\"\n",
    "    if method == 'zscore':\n",
    "        mean = original_stats['mean']\n",
    "        std = original_stats['std']\n",
    "        return scaled_predictions * std + mean\n",
    "    \n",
    "    elif method == 'minmax':\n",
    "        min_val = original_stats['min']\n",
    "        max_val = original_stats['max']\n",
    "        return scaled_predictions * (max_val - min_val) + min_val\n",
    "    \n",
    "    elif method == 'robust':\n",
    "        median = original_stats['median']\n",
    "        iqr = original_stats['iqr']\n",
    "        return scaled_predictions * iqr + median\n",
    "    \n",
    "    elif method == 'log':\n",
    "        offset = original_stats.get('offset', 1)\n",
    "        return np.exp(scaled_predictions) - offset\n",
    "\n",
    "# NEPSE-specific inverse transform example:\n",
    "# Model predicts next day's Close price in Z-score terms: +1.5\n",
    "# Current mean (252-day): Rs. 450\n",
    "# Current std: Rs. 25\n",
    "# Prediction: 450 + (1.5 * 25) = Rs. 487.50\n",
    "\n",
    "# Confidence intervals in original units:\n",
    "# 95% CI in Z-space: [prediction - 1.96, prediction + 1.96]\n",
    "# Converted to price: [487.50 - 49, 487.50 + 49] = [438.50, 536.50]\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "Inverse transformation is crucial for **actionable predictions**. A model outputting \"Z-score = +1.5\" is meaningless to traders; converting to \"Predicted Price = Rs. 487.50\" enables execution decisions.\n",
    "\n",
    "**Confidence Intervals:**\n",
    "When models output uncertainty (e.g., Bayesian Neural Networks or Quantile Regression), inverse transformation must be applied to prediction intervals, not just point estimates. If the model predicts Z = 1.5 ± 0.5 (95% CI: 0.5 to 2.5), the price range is:\n",
    "- Lower: $450 + (0.5 \\times 25) = 462.50$\n",
    "- Upper: $450 + (2.5 \\times 25) = 512.50$\n",
    "\n",
    "This Rs. 50 range informs position sizing (wider range = smaller position) and stop-loss placement.\n",
    "\n",
    "---\n",
    "\n",
    "## **15.11 Common Pitfalls**\n",
    "\n",
    "Avoid these critical errors when scaling NEPSE data:\n",
    "\n",
    "```python\n",
    "def demonstrate_scaling_pitfalls():\n",
    "    \"\"\"\n",
    "    Common mistakes in NEPSE feature scaling and their consequences.\n",
    "    \"\"\"\n",
    "    \n",
    "    # PITFALL 1: Look-ahead bias (using future data to scale past)\n",
    "    # WRONG:\n",
    "    global_mean = df['Close'].mean()  # Uses entire dataset including future\n",
    "    global_std = df['Close'].std()\n",
    "    df['Close_Z_Wrong'] = (df['Close'] - global_mean) / global_std\n",
    "    \n",
    "    # CORRECT:\n",
    "    df['Close_Z_Correct'] = df['Close'].rolling(252).apply(\n",
    "        lambda x: (x.iloc[-1] - x.mean()) / x.std()\n",
    "    )\n",
    "    \n",
    "    # PITFALL 2: Different scaling for train vs test\n",
    "    # WRONG: Fit scaler on all data, then split\n",
    "    scaler = StandardScaler()\n",
    "    scaler.fit(df[['Close', 'Volume']])  # Leaks test statistics\n",
    "    train_scaled = scaler.transform(train_df)\n",
    "    test_scaled = scaler.transform(test_df)\n",
    "    \n",
    "    # CORRECT: Fit only on train, transform both\n",
    "    scaler.fit(train_df[['Close', 'Volume']])\n",
    "    train_scaled = scaler.transform(train_df)\n",
    "    test_scaled = scaler.transform(test_df)  # Uses train statistics\n",
    "    \n",
    "    # PITFALL 3: Scaling target variable (Close price) with features\n",
    "    # WRONG: Scale target, predict, inverse transform\n",
    "    # Loss of interpretability and potential data leakage\n",
    "    \n",
    "    # CORRECT: Scale features, leave target in raw units (or log transform only)\n",
    "    # Model learns to predict actual price changes, not scaled prices\n",
    "    \n",
    "    # PITFILL 4: Ignoring regime changes (static scaling)\n",
    "    # WRONG: Use 2020 statistics for 2024 predictions\n",
    "    # NEPSE index moved from 1200 to 2100; old mean is meaningless\n",
    "    \n",
    "    # CORRECT: Rolling/expanding windows, or online updating\n",
    "    \n",
    "    # PITFALL 5: Scaling non-stationary features (prices) without differencing\n",
    "    # WRONG: Scale raw prices (trending series)\n",
    "    # Scaler assumes fixed mean, but NEPSE trends upward over years\n",
    "    \n",
    "    # CORRECT: Scale returns (stationary) or use rolling Z-score\n",
    "    \n",
    "    # PITFALL 6: Zero variance features (illiquid days)\n",
    "    # Some micro-caps have zero volume or flat prices for days\n",
    "    # Division by zero in Z-score\n",
    "    \n",
    "    # CORRECT: Check std > 0, or add epsilon (1e-8) to denominator\n",
    "    \n",
    "    # PITFALL 7: Scaling integer features (categorical encoded as ints)\n",
    "    # Day of week: 0-6. Scaling to Z-score makes Monday = -1.2, Sunday = 1.5\n",
    "    # Loses categorical nature\n",
    "    \n",
    "    # CORRECT: One-hot encode categoricals, don't scale\n",
    "    \n",
    "    pass\n",
    "\n",
    "def validate_scaling_quality(df_original, df_scaled, feature):\n",
    "    \"\"\"\n",
    "    Validate that scaling preserved necessary properties.\n",
    "    \"\"\"\n",
    "    checks = {}\n",
    "    \n",
    "    # Check 1: Rank preservation (Spearman correlation = 1)\n",
    "    from scipy.stats import spearmanr\n",
    "    corr, _ = spearmanr(df_original[feature], df_scaled[f'{feature}_scaled'])\n",
    "    checks['rank_preservation'] = corr > 0.99\n",
    "    \n",
    "    # Check 2: Outlier handling (max Z-score < 10)\n",
    "    max_z = df_scaled[f'{feature}_scaled'].abs().max()\n",
    "    checks['no_extreme_outliers'] = max_z < 10\n",
    "    \n",
    "    # Check 3: No NaN generation\n",
    "    checks['no_missing_values'] = df_scaled[f'{feature}_scaled'].isna().sum() == 0\n",
    "    \n",
    "    # Check 4: Stationarity (scaled should be more stationary)\n",
    "    from statsmodels.tsa.stattools import adfuller\n",
    "    adf_before = adfuller(df_original[feature].dropna())[1]\n",
    "    adf_after = adfuller(df_scaled[f'{feature}_scaled'].dropna())[1]\n",
    "    checks['improved_stationarity'] = adf_after > adf_before\n",
    "    \n",
    "    return checks\n",
    "```\n",
    "\n",
    "**Explanation:**\n",
    "\n",
    "**Pitfall 1: Look-Ahead Bias:**\n",
    "The most dangerous error—using global mean/std that includes future data. In NEPSE backtests, this creates unrealistic performance (you're using \"future knowledge\" of price levels to normalize past data). Always use rolling or expanding windows.\n",
    "\n",
    "**Pitfall 2: Train/Test Leakage:**\n",
    "Fitting the scaler on the full dataset before splitting leaks test set statistics (mean, variance) into the training set. The scaler should only \"see\" training data; test data is transformed using training statistics.\n",
    "\n",
    "**Pitfall 3: Scaling the Target:**\n",
    "If predicting `Close` price, don't Z-score the target variable. The model should predict actual prices (or log prices), not Z-scores. Inverse transforming predictions introduces error and complicates loss functions.\n",
    "\n",
    "**Pitfall 4: Static Scaling:**\n",
    "NEPSE is non-stationary—the index tripled from 2020 to 2024. A scaler fitted on 2020 data (mean = 1200) applied to 2024 data (prices = 2000+) produces Z-scores of +10, treated as outliers. Use rolling windows or online updating.\n",
    "\n",
    "**Pitfall 5: Scaling Trends:**\n",
    "Raw prices have trends (non-stationary). Scaling them preserves the trend in the scaled data, violating assumptions of many algorithms. Scale returns (stationary) or use rolling Z-scores.\n",
    "\n",
    "**Pitfall 6: Zero Variance:**\n",
    "NEPSE micro-caps often have days with zero volume or unchanged prices (flat lines). Standard deviation = 0 causes division by zero. Always add epsilon (`1e-8`) or check for zero variance.\n",
    "\n",
    "**Pitfall 7: Scaling Categoricals:**\n",
    "Encoding \"Day of Week\" as 0-6 then Z-scoring creates nonsensical values (Monday = -1.2). The distances become meaningless (Saturday to Sunday is 1 day, but Z-scored distance might be 2.3). One-hot encode categoricals instead.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 15**\n",
    "\n",
    "This chapter covered feature scaling and normalization strategies essential for NEPSE time-series prediction, including Z-score, Min-Max, Robust, Power transformations, and production pipeline implementation with strict avoidance of look-ahead bias.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
