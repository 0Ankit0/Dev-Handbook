{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 34: Cross\u2011Validation Techniques\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand why standard k\u2011fold cross\u2011validation fails for time\u2011series data\n",
    "- Implement time\u2011series cross\u2011validation using scikit\u2011learn's `TimeSeriesSplit`\n",
    "- Apply blocked cross\u2011validation to handle multiple independent time series\n",
    "- Use nested cross\u2011validation to perform model selection without optimistic bias\n",
    "- Implement purged cross\u2011validation to prevent leakage from overlapping windows\n",
    "- Understand combinatorial purged cross\u2011validation for hyperparameter tuning in financial applications\n",
    "- Apply walk\u2011forward validation as a realistic out\u2011of\u2011sample testing framework\n",
    "- Implement rolling origin evaluation for multi\u2011step forecasting\n",
    "- Choose the appropriate cross\u2011validation strategy based on data characteristics and modeling goals\n",
    "- Avoid common pitfalls that lead to overoptimistic performance estimates\n",
    "\n",
    "---\n",
    "\n",
    "## **34.1 Why Standard Cross\u2011Validation Fails for Time\u2011Series**\n",
    "\n",
    "In standard machine learning, k\u2011fold cross\u2011validation (CV) randomly partitions the data into k folds, trains on k\u20111 folds, and tests on the remaining fold. This works under the assumption that samples are independent and identically distributed (i.i.d.). However, time\u2011series data violates this assumption because observations are temporally dependent. Randomly assigning observations to folds can place future data in the training set and past data in the test set, leading to **look\u2011ahead bias** \u2013 the model is effectively trained on data from the future to predict the past, resulting in overly optimistic performance estimates.\n",
    "\n",
    "For the NEPSE prediction system, using standard k\u2011fold CV would be disastrous: the model could learn patterns from 2023 to predict 2022, which is impossible in reality. Therefore, we must use cross\u2011validation methods that respect the temporal order.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.2 K\u2011Fold Cross\u2011Validation (The Wrong Way)**\n",
    "\n",
    "Let's first demonstrate why standard k\u2011fold CV fails on time\u2011series data.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Load NEPSE data (simplified)\n",
    "df = pd.read_csv('nepse_data.csv')\n",
    "df['Date'] = pd.to_datetime(df['Date'])\n",
    "df = df.sort_values(['Symbol', 'Date']).reset_index(drop=True)\n",
    "\n",
    "# Use a single symbol for simplicity\n",
    "symbol = df['Symbol'].unique()[0]\n",
    "df_stock = df[df['Symbol'] == symbol].copy()\n",
    "\n",
    "# Create a simple feature: lagged return\n",
    "df_stock['Return'] = df_stock['Close'].pct_change()\n",
    "df_stock['Lag1'] = df_stock['Return'].shift(1)\n",
    "df_stock['Target'] = df_stock['Return'].shift(-1)\n",
    "df_ml = df_stock[['Lag1', 'Target']].dropna()\n",
    "\n",
    "X = df_ml[['Lag1']]\n",
    "y = df_ml['Target']\n",
    "\n",
    "# Standard k-fold CV (WRONG for time series)\n",
    "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "scores = cross_val_score(model, X, y, cv=kfold, scoring='neg_mean_squared_error')\n",
    "rmse_scores = np.sqrt(-scores)\n",
    "print(f\"Standard k-fold RMSE: {rmse_scores.mean():.4f} (+/- {rmse_scores.std():.4f})\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`KFold` with `shuffle=True` randomly mixes the data. The resulting RMSE is likely low, but it is an illusion because the model was tested on data that, in some folds, may be earlier than training data. In practice, this model would fail when deployed.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.3 TimeSeriesSplit (Scikit\u2011Learn)**\n",
    "\n",
    "`TimeSeriesSplit` is a cross\u2011validator that provides train/test indices to split time series data into train/test sets that respect temporal order. In each split, the training set consists of the first `k` folds, and the test set is the next fold. The test sets are non\u2011overlapping and always come after the training set.\n",
    "\n",
    "### **34.3.1 How TimeSeriesSplit Works**\n",
    "\n",
    "For example, with `n_splits=5`, the splits are:\n",
    "\n",
    "- Fold 1: train indices [0], test indices [1]\n",
    "- Fold 2: train indices [0,1], test indices [2]\n",
    "- Fold 3: train indices [0,1,2], test indices [3]\n",
    "- Fold 4: train indices [0,1,2,3], test indices [4]\n",
    "- Fold 5: train indices [0,1,2,3,4], test indices [5]\n",
    "\n",
    "The training set expands with each fold, simulating a scenario where we train on all available data up to a point and test on the next period.\n",
    "\n",
    "### **34.3.2 Implementing TimeSeriesSplit**\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "rmse_scores = []\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "    rmse_scores.append(rmse)\n",
    "\n",
    "print(f\"TimeSeriesSplit RMSE: {np.mean(rmse_scores):.4f} (+/- {np.std(rmse_scores):.4f})\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This gives a more realistic estimate of how the model would perform if trained up to a certain date and tested on the subsequent period. Note that the first few folds have very small training sets, so the scores may be unstable; in practice, we might use fewer splits or ensure a minimum training size.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.4 Blocked Cross\u2011Validation**\n",
    "\n",
    "If we have multiple independent time series (e.g., many stocks), we can perform **blocked cross\u2011validation** where we split by time across all series simultaneously, or we can split by series. A common approach is to use a **rolling window** across all series, treating each time point as a block.\n",
    "\n",
    "For example, with daily data for many stocks, we can define blocks of, say, 60 days. We train on the first `k` blocks and test on the next block. This respects the temporal order across all stocks.\n",
    "\n",
    "```python\n",
    "# Assume we have a multi-stock DataFrame with columns: Date, Symbol, Return\n",
    "# We'll create a pivot table: rows = Date, columns = Symbol, values = Return\n",
    "df_pivot = df.pivot(index='Date', columns='Symbol', values='Return').dropna()\n",
    "\n",
    "# Now we have a matrix where each row is a date, each column a stock\n",
    "# We can apply TimeSeriesSplit on the rows\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "for train_idx, test_idx in tscv.split(df_pivot):\n",
    "    train_dates = df_pivot.index[train_idx]\n",
    "    test_dates = df_pivot.index[test_idx]\n",
    "    # Now we can train on all stocks for those dates, and test on subsequent dates\n",
    "    X_train = df_pivot.loc[train_dates]  # features could be lagged values, etc.\n",
    "    y_train = df_pivot.loc[train_dates].shift(-1)  # next day returns (for each stock)\n",
    "    # ... and so on\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Blocked CV treats each time point as an independent observation across series. It is appropriate when we have a panel of time series and we want to evaluate a model that uses features from all stocks (e.g., cross\u2011sectional momentum). The temporal split is still essential.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.5 Nested Cross\u2011Validation**\n",
    "\n",
    "When we need to both tune hyperparameters and evaluate model performance, we must use **nested cross\u2011validation** to avoid optimistic bias. The outer loop estimates generalization performance, while the inner loop performs model selection (e.g., grid search) on the training data of each outer fold.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Outer CV\n",
    "tscv_outer = TimeSeriesSplit(n_splits=3)\n",
    "outer_scores = []\n",
    "\n",
    "for train_idx_outer, test_idx_outer in tscv_outer.split(X):\n",
    "    X_train_outer, X_test_outer = X.iloc[train_idx_outer], X.iloc[test_idx_outer]\n",
    "    y_train_outer, y_test_outer = y.iloc[train_idx_outer], y.iloc[test_idx_outer]\n",
    "    \n",
    "    # Inner CV for hyperparameter tuning\n",
    "    tscv_inner = TimeSeriesSplit(n_splits=3)\n",
    "    param_grid = {'max_depth': [3, 5, 7], 'n_estimators': [50, 100]}\n",
    "    model = RandomForestRegressor(random_state=42)\n",
    "    grid = GridSearchCV(model, param_grid, cv=tscv_inner, scoring='neg_mean_squared_error')\n",
    "    grid.fit(X_train_outer, y_train_outer)\n",
    "    \n",
    "    # Best model on this outer fold\n",
    "    best_model = grid.best_estimator_\n",
    "    y_pred = best_model.predict(X_test_outer)\n",
    "    rmse = np.sqrt(np.mean((y_test_outer - y_pred)**2))\n",
    "    outer_scores.append(rmse)\n",
    "\n",
    "print(f\"Nested CV RMSE: {np.mean(outer_scores):.4f} (+/- {np.std(outer_scores):.4f})\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The outer scores provide an unbiased estimate of the model's performance with hyperparameter tuning. The inner CV uses only the training portion of each outer fold, preventing leakage from the test data into the tuning process.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.6 Purged Cross\u2011Validation**\n",
    "\n",
    "In financial machine learning, Marcos L\u00f3pez de Prado introduced **purged cross\u2011validation** to address the issue of overlapping observations when using features that depend on a lookback window. For example, if we use a 20\u2011day moving average as a feature, the first few test observations may have features that depend on training data (which is allowed), but the test labels might be too close to the training period, causing leakage. Purged CV removes from the training set any data that overlaps in time with the test set's feature lookback window.\n",
    "\n",
    "The idea is to \"purge\" from the training set any observations whose feature window includes any time in the test set. This ensures that no training observation uses data from the test period.\n",
    "\n",
    "Implementation is more complex and requires careful indexing. We'll outline the concept and provide a simplified version.\n",
    "\n",
    "```python\n",
    "def purged_time_series_split(X, y, test_size, gap=0):\n",
    "    \"\"\"\n",
    "    Generator that yields train/test indices with purging.\n",
    "    test_size: number of test samples per fold.\n",
    "    gap: number of samples to purge between train and test.\n",
    "    \"\"\"\n",
    "    n_samples = len(X)\n",
    "    indices = np.arange(n_samples)\n",
    "    for test_start in range(0, n_samples - test_size + 1, test_size):\n",
    "        train_end = test_start - gap\n",
    "        train_indices = indices[:train_end]\n",
    "        test_indices = indices[test_start:test_start + test_size]\n",
    "        if len(train_indices) > 0:\n",
    "            yield train_indices, test_indices\n",
    "\n",
    "# Example usage\n",
    "for train_idx, test_idx in purged_time_series_split(X, y, test_size=50, gap=20):\n",
    "    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    # train and evaluate\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The `gap` parameter ensures that the training set ends `gap` days before the test set starts, purging any potentially overlapping information. The size of the gap should be at least the maximum lookback period used in feature engineering.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.7 Combinatorial Purged Cross\u2011Validation (CPCV)**\n",
    "\n",
    "Combinatorial Purged Cross\u2011Validation (CPCV) is an advanced technique that creates many train/test splits by combining different training and test periods, while purging and embargoing to avoid leakage. It is particularly useful for backtesting trading strategies where we want to evaluate performance over many different market regimes.\n",
    "\n",
    "CPCV is complex to implement from scratch. The `mlfinlab` library (now `mosef`) provides an implementation. We'll outline the idea:\n",
    "\n",
    "- Divide the time series into `N` sequential groups.\n",
    "- Select a number of training groups `k` and test groups `n`.\n",
    "- Generate all combinations of groups that satisfy the temporal order (train groups before test groups).\n",
    "- For each combination, purge any overlapping data and embargo a gap.\n",
    "\n",
    "This yields many backtest paths, providing a robust estimate of performance distribution.\n",
    "\n",
    "For the NEPSE system, CPCV might be overkill, but it's good to know for advanced applications.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.8 Walk\u2011Forward Validation**\n",
    "\n",
    "Walk\u2011forward validation is the most realistic simulation of how a forecasting model would be used in practice. It involves:\n",
    "\n",
    "- Starting with an initial training window.\n",
    "- Forecasting the next `h` steps.\n",
    "- Expanding or rolling the training window to include the most recent actual data.\n",
    "- Repeating until the end of the dataset.\n",
    "\n",
    "This is essentially the same as `TimeSeriesSplit` but with the ability to forecast multiple steps ahead. We can implement it manually.\n",
    "\n",
    "```python\n",
    "def walk_forward_validation(X, y, train_size, test_size, step=1):\n",
    "    \"\"\"\n",
    "    Walk-forward validation for multi-step forecasting.\n",
    "    X, y: arrays (n_samples, ...)\n",
    "    train_size: initial training window size\n",
    "    test_size: number of steps to forecast each iteration\n",
    "    step: how many steps to move forward each time (usually test_size)\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    scores = []\n",
    "    for start in range(0, n - train_size - test_size + 1, step):\n",
    "        train_end = start + train_size\n",
    "        test_end = train_end + test_size\n",
    "        \n",
    "        X_train = X[start:train_end]\n",
    "        y_train = y[start:train_end]\n",
    "        X_test = X[train_end:test_end]\n",
    "        y_test = y[train_end:test_end]\n",
    "        \n",
    "        # Train model (could be retrained each iteration)\n",
    "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "        scores.append(rmse)\n",
    "    return scores\n",
    "\n",
    "# Example\n",
    "scores = walk_forward_validation(X.values, y.values, train_size=500, test_size=50, step=50)\n",
    "print(f\"Walk-forward RMSE: {np.mean(scores):.4f} (+/- {np.std(scores):.4f})\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This mimics a realistic retraining schedule. It is computationally intensive but provides the most trustworthy estimate of out\u2011of\u2011sample performance.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.9 Rolling Origin Evaluation**\n",
    "\n",
    "Rolling origin evaluation (also known as time series cross\u2011validation with a fixed origin) is a variant where the training window expands from a fixed start point, but the test set is always the next `h` steps. This is similar to `TimeSeriesSplit` but with a fixed origin.\n",
    "\n",
    "```python\n",
    "def rolling_origin_evaluation(X, y, test_size, min_train_size):\n",
    "    \"\"\"\n",
    "    Rolling origin evaluation.\n",
    "    min_train_size: minimum training size for first fold.\n",
    "    \"\"\"\n",
    "    n = len(X)\n",
    "    scores = []\n",
    "    for train_end in range(min_train_size, n - test_size + 1):\n",
    "        X_train = X[:train_end]\n",
    "        y_train = y[:train_end]\n",
    "        X_test = X[train_end:train_end+test_size]\n",
    "        y_test = y[train_end:train_end+test_size]\n",
    "        \n",
    "        model = RandomForestRegressor(n_estimators=50, random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        rmse = np.sqrt(np.mean((y_test - y_pred)**2))\n",
    "        scores.append(rmse)\n",
    "    return scores\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This yields many evaluation points and is often used in forecasting competitions. It can be computationally expensive, but it gives a very detailed view of performance over time.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.10 Implementation Strategies**\n",
    "\n",
    "### **34.10.1 Custom Splitters for scikit\u2011learn**\n",
    "\n",
    "We can create custom cross\u2011validator classes that implement `split` and `get_n_splits` methods to use with scikit\u2011learn's `cross_val_score` and `GridSearchCV`.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import BaseCrossValidator\n",
    "\n",
    "class PurgedTimeSeriesSplit(BaseCrossValidator):\n",
    "    def __init__(self, n_splits=5, test_size=None, gap=0):\n",
    "        self.n_splits = n_splits\n",
    "        self.test_size = test_size\n",
    "        self.gap = gap\n",
    "    \n",
    "    def split(self, X, y=None, groups=None):\n",
    "        n_samples = len(X)\n",
    "        indices = np.arange(n_samples)\n",
    "        if self.test_size is None:\n",
    "            self.test_size = n_samples // (self.n_splits + 1)\n",
    "        for i in range(self.n_splits):\n",
    "            test_start = (i+1) * self.test_size\n",
    "            train_end = test_start - self.gap\n",
    "            train_indices = indices[:train_end]\n",
    "            test_indices = indices[test_start:test_start + self.test_size]\n",
    "            if len(train_indices) == 0 or len(test_indices) == 0:\n",
    "                break\n",
    "            yield train_indices, test_indices\n",
    "    \n",
    "    def get_n_splits(self, X=None, y=None, groups=None):\n",
    "        return self.n_splits\n",
    "\n",
    "# Usage\n",
    "purgedsplit = PurgedTimeSeriesSplit(n_splits=5, test_size=50, gap=20)\n",
    "for train_idx, test_idx in purgedsplit.split(X):\n",
    "    # ...\n",
    "```\n",
    "\n",
    "### **34.10.2 Handling Multiple Assets**\n",
    "\n",
    "When you have multiple assets, you can either:\n",
    "\n",
    "- Treat each asset separately and average results (if you want asset\u2011specific models).\n",
    "- Concatenate assets and use a feature that identifies the asset, then apply time\u2011series CV on the combined dataset (ensuring that train/test splits are by time, not by asset). This is valid if the model uses asset\u2011specific features.\n",
    "\n",
    "### **34.10.3 Computational Considerations**\n",
    "\n",
    "Walk\u2011forward and rolling origin evaluations can be slow. Use caching of models or parallel processing where possible. Also, consider reducing the number of folds or using a representative subset of folds.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.11 Choosing the Right Cross\u2011Validation Strategy**\n",
    "\n",
    "| Strategy | When to Use |\n",
    "|----------|-------------|\n",
    "| TimeSeriesSplit | Simple, standard for evaluating one\u2011step ahead forecasts with expanding window. |\n",
    "| Blocked CV | Multiple independent time series, need to test across all series simultaneously. |\n",
    "| Nested CV | When you need to tune hyperparameters without optimistic bias. |\n",
    "| Purged CV | When features have long lookback windows that could overlap test set. |\n",
    "| CPCV | Advanced backtesting of trading strategies with many possible paths. |\n",
    "| Walk\u2011Forward | Most realistic simulation; use for final model evaluation before deployment. |\n",
    "| Rolling Origin | Detailed performance over time; often used in research. |\n",
    "\n",
    "For the NEPSE system, a good starting point is `TimeSeriesSplit` for initial model comparison, and walk\u2011forward validation for final evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.12 Common Pitfalls**\n",
    "\n",
    "- **Using standard k\u2011fold CV:** This will give overly optimistic results and is invalid for time\u2011series.\n",
    "- **Not purging when using lagged features:** Even with time\u2011based splits, the first few test observations may have features that use training data (which is fine), but if the test set is too close to the training set, the model may benefit from short\u2011term autocorrelation. A small gap (embargo) can help.\n",
    "- **Overlapping train/test in multi\u2011step forecasting:** Ensure that when forecasting multiple steps ahead, the test set does not contain data that overlaps with training (e.g., if you predict 5 steps ahead, the last 5 training points should be excluded from evaluation for that horizon).\n",
    "- **Information leakage from scaling:** Always fit scalers on the training fold only, then transform test fold.\n",
    "- **Not accounting for multiple assets:** If you have many stocks, ensure that the same time split is applied to all, and that features are computed per stock without leakage across stocks.\n",
    "\n",
    "---\n",
    "\n",
    "## **34.13 Chapter Summary**\n",
    "\n",
    "In this chapter, we covered the essential cross\u2011validation techniques for time\u2011series forecasting, with applications to the NEPSE dataset.\n",
    "\n",
    "- **Standard k\u2011fold CV** is invalid for time\u2011series due to look\u2011ahead bias.\n",
    "- **TimeSeriesSplit** provides a simple expanding\u2011window CV that respects temporal order.\n",
    "- **Blocked CV** handles multiple independent series.\n",
    "- **Nested CV** enables unbiased hyperparameter tuning.\n",
    "- **Purged CV** removes overlapping observations to prevent leakage.\n",
    "- **Combinatorial Purged CV** is an advanced method for robust backtesting.\n",
    "- **Walk\u2011forward validation** mimics real\u2011world deployment and gives trustworthy performance estimates.\n",
    "- **Rolling origin evaluation** provides many evaluation points.\n",
    "- Implementation strategies and custom splitters make these methods easy to use.\n",
    "- Choosing the right method depends on the data and the goal.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Start with `TimeSeriesSplit` for quick model comparisons.\n",
    "- Use walk\u2011forward validation for final performance assessment.\n",
    "- If using features with long lookback (e.g., 50\u2011day moving average), apply a purge gap equal to that lookback.\n",
    "- Always fit scalers and preprocessors on training folds only.\n",
    "- Report performance across folds to understand stability.\n",
    "\n",
    "In the next chapter, **Chapter 35: Hyperparameter Tuning**, we will explore how to systematically search for the best model parameters using grid search, random search, Bayesian optimization, and more, all within a time\u2011series context.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 34**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='33. time_series_specific_evaluation.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='35. hyperparameter_tuning.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}