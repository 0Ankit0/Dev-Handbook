{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 35: Hyperparameter Tuning\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the role of hyperparameters in machine learning models and their impact on performance\n",
    "- Distinguish between model parameters (learned during training) and hyperparameters (set before training)\n",
    "- Perform manual hyperparameter tuning by systematically varying key parameters\n",
    "- Implement grid search and random search using scikit\u2011learn with time\u2011series cross\u2011validation\n",
    "- Apply Bayesian optimization techniques (e.g., Gaussian Processes, Tree\u2011structured Parzen Estimator) for more efficient tuning\n",
    "- Use libraries like Hyperopt, Optuna, and scikit\u2011optimize for automated hyperparameter optimization\n",
    "- Implement multi\u2011fidelity optimization (e.g., successive halving, Hyperband) to speed up tuning\n",
    "- Understand early stopping in the context of hyperparameter tuning for neural networks\n",
    "- Avoid common pitfalls such as overfitting to the validation set and data leakage during tuning\n",
    "- Develop a practical hyperparameter tuning strategy for the NEPSE prediction system\n",
    "\n",
    "---\n",
    "\n",
    "## **35.1 Introduction to Hyperparameters**\n",
    "\n",
    "In machine learning, we distinguish between two types of parameters:\n",
    "\n",
    "- **Model parameters:** These are learned from the data during training (e.g., weights in a neural network, coefficients in linear regression, split points in a decision tree).\n",
    "- **Hyperparameters:** These are set before training and control the learning process and model architecture (e.g., learning rate, number of trees in a random forest, depth of a tree, regularization strength).\n",
    "\n",
    "Choosing the right hyperparameters is crucial for model performance. Too simple a model (e.g., shallow tree) may underfit; too complex (e.g., deep tree with many leaves) may overfit. Hyperparameter tuning is the process of searching for the combination of hyperparameters that yields the best generalization performance on unseen data.\n",
    "\n",
    "For the NEPSE prediction system, we will encounter hyperparameters in almost every model:\n",
    "\n",
    "- **Linear models:** regularization strength `alpha` in Ridge, Lasso, ElasticNet.\n",
    "- **Tree\u2011based models:** `max_depth`, `min_samples_split`, `n_estimators`, `learning_rate` (for boosting).\n",
    "- **Neural networks:** number of layers, number of units per layer, dropout rate, learning rate, batch size.\n",
    "- **Support vector machines:** `C`, `gamma`, kernel choice.\n",
    "- **Statistical models (ARIMA):** `p`, `d`, `q` orders.\n",
    "\n",
    "Tuning these hyperparameters can significantly improve forecast accuracy, but it must be done carefully to avoid overfitting to the validation data.\n",
    "\n",
    "---\n",
    "\n",
    "## **35.2 Hyperparameter Types**\n",
    "\n",
    "Hyperparameters can be categorized as:\n",
    "\n",
    "- **Model hyperparameters:** related to the architecture (e.g., number of trees, depth).\n",
    "- **Training hyperparameters:** related to the optimization (e.g., learning rate, batch size, number of epochs).\n",
    "- **Regularization hyperparameters:** control overfitting (e.g., dropout rate, L2 penalty).\n",
    "\n",
    "Some hyperparameters are continuous (e.g., learning rate), others are discrete (e.g., number of layers), and some are categorical (e.g., kernel type). This influences the choice of search method.\n",
    "\n",
    "---\n",
    "\n",
    "## **35.3 Manual Tuning**\n",
    "\n",
    "Manual tuning involves changing hyperparameters by hand based on intuition, experience, or trial and error. It is feasible when the number of hyperparameters is small and the search space is well understood. For example, we might try a few values of `max_depth` for a random forest and observe validation performance.\n",
    "\n",
    "```python\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import numpy as np\n",
    "\n",
    "# Assume X_train, y_train are prepared\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "depths = [3, 5, 7, 10]\n",
    "best_depth = None\n",
    "best_score = -np.inf\n",
    "\n",
    "for depth in depths:\n",
    "    scores = []\n",
    "    for train_idx, val_idx in tscv.split(X_train):\n",
    "        X_tr, X_val = X_train.iloc[train_idx], X_train.iloc[val_idx]\n",
    "        y_tr, y_val = y_train.iloc[train_idx], y_train.iloc[val_idx]\n",
    "        \n",
    "        model = RandomForestRegressor(max_depth=depth, n_estimators=100, random_state=42)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        score = model.score(X_val, y_val)  # R\u00b2\n",
    "        scores.append(score)\n",
    "    avg_score = np.mean(scores)\n",
    "    if avg_score > best_score:\n",
    "        best_score = avg_score\n",
    "        best_depth = depth\n",
    "\n",
    "print(f\"Best max_depth: {best_depth} with average R\u00b2: {best_score:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Manual tuning is simple but becomes impractical when the number of hyperparameters grows. It also relies on the practitioner's skill and may miss optimal combinations.\n",
    "\n",
    "---\n",
    "\n",
    "## **35.4 Grid Search**\n",
    "\n",
    "Grid search is an exhaustive search over a manually specified subset of the hyperparameter space. For each combination of hyperparameters, the model is evaluated (usually via cross\u2011validation) and the best combination is selected.\n",
    "\n",
    "### **35.4.1 Grid Search with Time\u2011Series Cross\u2011Validation**\n",
    "\n",
    "Scikit\u2011learn's `GridSearchCV` can be used with a custom cross\u2011validator like `TimeSeriesSplit`.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import GridSearchCV, TimeSeriesSplit\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# Define parameter grid\n",
    "param_grid = {\n",
    "    'max_depth': [3, 5, 7, 10],\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'min_samples_split': [2, 5, 10]\n",
    "}\n",
    "\n",
    "# Create model\n",
    "rf = RandomForestRegressor(random_state=42)\n",
    "\n",
    "# TimeSeriesSplit\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "\n",
    "# Grid search\n",
    "grid_search = GridSearchCV(rf, param_grid, cv=tscv, scoring='neg_mean_squared_error', verbose=1, n_jobs=-1)\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {grid_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-grid_search.best_score_):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Grid search evaluates every combination in the grid. With `n_splits=3` and 4\u00d73\u00d73 = 36 combinations, that's 108 model fits. This can become computationally expensive as the grid grows.\n",
    "\n",
    "**Limitations:**  \n",
    "- Curse of dimensionality: number of combinations grows exponentially with the number of hyperparameters.\n",
    "- Continuous parameters must be discretized, potentially missing the optimum.\n",
    "\n",
    "---\n",
    "\n",
    "## **35.5 Random Search**\n",
    "\n",
    "Random search samples hyperparameter combinations from a distribution. It is more efficient than grid search when some hyperparameters are more important than others, because it explores a wider range of values for each hyperparameter with the same budget.\n",
    "\n",
    "### **35.5.1 Implementing Random Search**\n",
    "\n",
    "Scikit\u2011learn provides `RandomizedSearchCV` which samples a fixed number of combinations from specified distributions.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from scipy.stats import randint, uniform\n",
    "\n",
    "# Define parameter distributions\n",
    "param_dist = {\n",
    "    'max_depth': randint(3, 15),\n",
    "    'n_estimators': randint(50, 300),\n",
    "    'min_samples_split': randint(2, 20),\n",
    "    'max_features': uniform(0.5, 0.5)  # uniform between 0.5 and 1.0\n",
    "}\n",
    "\n",
    "# Random search\n",
    "random_search = RandomizedSearchCV(rf, param_dist, n_iter=50, cv=tscv, \n",
    "                                   scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "random_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {random_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-random_search.best_score_):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We specify distributions (e.g., `randint` for integers, `uniform` for floats). The search runs for `n_iter=50` iterations, which is much less than the full grid (which could be thousands). Random search is often more effective than grid search, especially when only a few hyperparameters matter.\n",
    "\n",
    "---\n",
    "\n",
    "## **35.6 Bayesian Optimization**\n",
    "\n",
    "Bayesian optimization builds a probabilistic model of the objective function (e.g., validation RMSE) and uses it to select the most promising hyperparameters to evaluate next. It balances exploration (trying new areas) and exploitation (focusing on areas known to be good).\n",
    "\n",
    "### **35.6.1 Gaussian Processes**\n",
    "\n",
    "One common approach uses Gaussian Processes to model the objective function. The expected improvement (EI) acquisition function guides the next sample.\n",
    "\n",
    "Libraries like `scikit-optimize` (skopt) provide an easy interface.\n",
    "\n",
    "```python\n",
    "from skopt import BayesSearchCV\n",
    "from skopt.space import Integer, Real\n",
    "\n",
    "# Define search spaces\n",
    "search_spaces = {\n",
    "    'max_depth': Integer(3, 15),\n",
    "    'n_estimators': Integer(50, 300),\n",
    "    'min_samples_split': Integer(2, 20),\n",
    "    'max_features': Real(0.5, 1.0)\n",
    "}\n",
    "\n",
    "# Bayesian search\n",
    "bayes_search = BayesSearchCV(rf, search_spaces, n_iter=30, cv=tscv, \n",
    "                              scoring='neg_mean_squared_error', random_state=42, n_jobs=-1)\n",
    "bayes_search.fit(X_train, y_train)\n",
    "\n",
    "print(f\"Best parameters: {bayes_search.best_params_}\")\n",
    "print(f\"Best CV RMSE: {np.sqrt(-bayes_search.best_score_):.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`BayesSearchCV` from `skopt` implements Bayesian optimization over hyperparameters. It typically finds better configurations faster than random search, especially when the search space is large.\n",
    "\n",
    "### **35.6.2 Tree\u2011structured Parzen Estimator (TPE)**\n",
    "\n",
    "TPE is another Bayesian optimization method, popularized by the Hyperopt library. It models the density of good and bad configurations separately.\n",
    "\n",
    "```python\n",
    "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Define search space\n",
    "space = {\n",
    "    'max_depth': hp.choice('max_depth', [3, 5, 7, 10, 15]),\n",
    "    'n_estimators': hp.choice('n_estimators', [50, 100, 200, 300]),\n",
    "    'min_samples_split': hp.choice('min_samples_split', [2, 5, 10, 20]),\n",
    "    'max_features': hp.uniform('max_features', 0.5, 1.0)\n",
    "}\n",
    "\n",
    "def objective(params):\n",
    "    model = RandomForestRegressor(**params, random_state=42)\n",
    "    # Use TimeSeriesSplit for CV\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=tscv, scoring='neg_mean_squared_error')\n",
    "    rmse = np.sqrt(-np.mean(scores))\n",
    "    return {'loss': rmse, 'status': STATUS_OK}\n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(fn=objective, space=space, algo=tpe.suggest, max_evals=50, trials=trials)\n",
    "print(f\"Best hyperparameters: {best}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Hyperopt allows flexible search spaces and can handle conditional hyperparameters (e.g., different kernel choices). TPE often performs well in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## **35.7 Multi\u2011Fidelity Optimization**\n",
    "\n",
    "Multi\u2011fidelity methods speed up hyperparameter tuning by evaluating unpromising configurations with lower fidelity (e.g., fewer training iterations, smaller subset of data) and only fully evaluating promising ones.\n",
    "\n",
    "### **35.7.1 Successive Halving**\n",
    "\n",
    "Successive halving allocates a budget to a set of configurations, evaluates them, keeps the best half, and increases the budget. This is the basis of **Hyperband**.\n",
    "\n",
    "### **35.7.2 Hyperband**\n",
    "\n",
    "Hyperband combines random search with adaptive resource allocation. It is particularly effective for deep learning where training time is long.\n",
    "\n",
    "```python\n",
    "# Using the `hyperband` library or `keras-tuner` for neural networks\n",
    "# Example with keras-tuner (for neural networks)\n",
    "import keras_tuner as kt\n",
    "\n",
    "def build_model(hp):\n",
    "    model = keras.Sequential()\n",
    "    model.add(layers.Dense(units=hp.Int('units_1', 32, 256, step=32), activation='relu'))\n",
    "    model.add(layers.Dropout(hp.Float('dropout_1', 0, 0.5, step=0.1)))\n",
    "    model.add(layers.Dense(1))\n",
    "    model.compile(optimizer=keras.optimizers.Adam(hp.Float('lr', 1e-4, 1e-2, sampling='log')),\n",
    "                  loss='mse')\n",
    "    return model\n",
    "\n",
    "tuner = kt.Hyperband(build_model, objective='val_loss', max_epochs=50, factor=3, directory='my_dir')\n",
    "tuner.search(X_train, y_train, validation_data=(X_val, y_val), callbacks=[early_stop])\n",
    "best_hps = tuner.get_best_hyperparameters(1)[0]\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Hyperband trains many configurations for a few epochs, then promotes the best ones to longer training. This is efficient for deep learning.\n",
    "\n",
    "---\n",
    "\n",
    "## **35.8 Early Stopping in Tuning**\n",
    "\n",
    "When tuning neural networks, we often use early stopping to prevent overfitting. However, during hyperparameter search, we must be careful: if we stop early based on validation loss, that loss becomes the objective we optimize. This is acceptable as long as we use the same validation set for all trials. But we must still have a separate test set for final evaluation.\n",
    "\n",
    "In `GridSearchCV` or `RandomizedSearchCV`, we can pass an early stopping callback, but the search will use the validation score at the stopping point. This is fine.\n",
    "\n",
    "```python\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "early_stop = EarlyStopping(monitor='val_loss', patience=5)\n",
    "\n",
    "# Inside a custom objective function for Hyperopt, we can include early stopping\n",
    "def objective_nn(params):\n",
    "    model = create_model(params)  # build model with given hyperparameters\n",
    "    history = model.fit(X_train, y_train, validation_data=(X_val, y_val),\n",
    "                        epochs=100, callbacks=[early_stop], verbose=0)\n",
    "    best_val_loss = min(history.history['val_loss'])\n",
    "    return {'loss': best_val_loss, 'status': STATUS_OK}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **35.9 Practical Considerations for NEPSE**\n",
    "\n",
    "### **35.9.1 Time\u2011Series Cross\u2011Validation in Tuning**\n",
    "\n",
    "Always use time\u2011series cross\u2011validation (e.g., `TimeSeriesSplit`) inside the tuning loop. Random shuffling is invalid. With `GridSearchCV`, pass `cv=TimeSeriesSplit()`.\n",
    "\n",
    "### **35.9.2 Nested Cross\u2011Validation for Unbiased Performance**\n",
    "\n",
    "If you want an unbiased estimate of the final model's performance after tuning, use nested CV (as in Chapter 34). The outer loop splits data, inner loop tunes hyperparameters. This gives a realistic estimate of how the model will perform on new data.\n",
    "\n",
    "### **35.9.3 Computational Budget**\n",
    "\n",
    "For the NEPSE dataset (a few thousand rows), random search with 50\u2011100 iterations is often sufficient. Bayesian optimization can reduce that number. For neural networks, use Hyperband to save time.\n",
    "\n",
    "### **35.9.4 Hyperparameter Spaces**\n",
    "\n",
    "Define reasonable ranges based on domain knowledge:\n",
    "\n",
    "- **Tree depth:** For small datasets, depth > 10 may overfit.\n",
    "- **Learning rate:** Log\u2011uniform between 1e-4 and 1e-2.\n",
    "- **Regularization:** Try small values first (e.g., 0.001, 0.01, 0.1).\n",
    "- **Number of estimators:** Start with 100, go up to 500 if not overfitting.\n",
    "\n",
    "### **35.9.5 Avoiding Overfitting to Validation**\n",
    "\n",
    "The more you search, the higher the chance of finding a combination that performs well on the validation set by chance. To mitigate:\n",
    "\n",
    "- Use a separate test set that is never used during tuning.\n",
    "- Use nested CV.\n",
    "- Limit the search space and number of trials.\n",
    "- Report performance on the test set only after final model selection.\n",
    "\n",
    "---\n",
    "\n",
    "## **35.10 Chapter Summary**\n",
    "\n",
    "In this chapter, we covered the essential techniques for hyperparameter tuning in machine learning, with applications to the NEPSE prediction system.\n",
    "\n",
    "- **Hyperparameters** are settings that control model architecture and training; they must be tuned to optimize performance.\n",
    "- **Manual tuning** is simple but limited.\n",
    "- **Grid search** exhaustively evaluates all combinations but is inefficient for many hyperparameters.\n",
    "- **Random search** samples from distributions and is more efficient.\n",
    "- **Bayesian optimization** (Gaussian Processes, TPE) models the objective and focuses on promising regions.\n",
    "- **Multi\u2011fidelity methods** (Hyperband) speed up tuning by early stopping poor configurations.\n",
    "- **Early stopping** can be integrated into tuning for neural networks.\n",
    "- **Practical considerations** include using time\u2011series CV, nested CV for unbiased estimates, and defining sensible search spaces.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- For tree\u2011based models, start with random search (50\u2011100 iterations) using `RandomizedSearchCV` with `TimeSeriesSplit`.\n",
    "- For neural networks, use Hyperband (via `keras\u2011tuner`) to efficiently explore architectures.\n",
    "- Always keep a separate test set for final evaluation after tuning.\n",
    "- Use nested CV if you need an unbiased estimate of the tuned model's performance.\n",
    "- Document the tuning process and the final chosen hyperparameters for reproducibility.\n",
    "\n",
    "In the next chapter, **Chapter 36: Model Interpretation and Explainability**, we will explore methods to understand and explain model predictions, which is crucial for trust and debugging in financial applications.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 35**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='34. cross_validation_techniques.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='36. model_interpretation_and_explainability.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}