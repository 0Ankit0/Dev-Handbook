{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 37: Error Analysis\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the critical role of systematic error analysis in model development and refinement\n",
    "- Identify and visualize systematic error patterns in time\u2011series predictions\n",
    "- Perform residual analysis to diagnose model misspecification, including tests for autocorrelation and heteroscedasticity\n",
    "- Analyze error distributions and their properties (skewness, kurtosis, normality)\n",
    "- Conduct conditional error analysis to determine when and why the model fails (e.g., error as a function of feature values, market regimes)\n",
    "- Detect temporal error patterns such as autocorrelation and volatility clustering\n",
    "- Assess the impact of outliers on model performance using influence measures\n",
    "- Cluster errors to identify distinct regimes of poor performance\n",
    "- Perform root cause analysis to trace errors back to data issues, feature gaps, or model limitations\n",
    "- Develop an error analysis workflow and document findings for iterative model improvement\n",
    "\n",
    "---\n",
    "\n",
    "## **37.1 Introduction to Error Analysis**\n",
    "\n",
    "Error analysis is the systematic investigation of a model's prediction errors to understand their nature, causes, and potential remedies. While evaluation metrics (RMSE, MAE, etc.) give a summary of performance, they do not reveal *why* the model makes mistakes or *when* it is most unreliable. Error analysis fills this gap by examining the residuals (errors) in detail, uncovering patterns that can guide feature engineering, model selection, and data collection.\n",
    "\n",
    "For the NEPSE prediction system, error analysis can answer questions like:\n",
    "\n",
    "- Does the model consistently overpredict on Mondays?\n",
    "- Are errors larger on high\u2011volatility days?\n",
    "- Does the model fail to capture sudden reversals?\n",
    "- Are there specific stocks or time periods where performance degrades?\n",
    "\n",
    "By answering these questions, we can iteratively improve the model.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.2 Residual Analysis Basics**\n",
    "\n",
    "Residuals are the differences between actual and predicted values:  \n",
    "`e\u1d62 = y\u1d62 - \u0177\u1d62`\n",
    "\n",
    "A good model should have residuals that are:\n",
    "\n",
    "- **Unbiased:** Mean close to zero.\n",
    "- **Homoscedastic:** Constant variance over time.\n",
    "- **Uncorrelated:** No autocorrelation.\n",
    "- **Approximately normally distributed** (optional, but useful for inference).\n",
    "\n",
    "We will examine these properties using the NEPSE return predictions from a sample model (e.g., a random forest).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from scipy import stats\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "from statsmodels.stats.stattools import durbin_watson\n",
    "\n",
    "# Assume we have trained a model and have predictions on a test set\n",
    "# For demonstration, we'll create synthetic residuals\n",
    "np.random.seed(42)\n",
    "y_test = np.random.randn(200) * 2  # actual returns\n",
    "y_pred = y_test + np.random.randn(200) * 0.5  # predictions with noise\n",
    "residuals = y_test - y_pred\n",
    "\n",
    "# Basic statistics\n",
    "print(f\"Mean of residuals: {np.mean(residuals):.4f}\")\n",
    "print(f\"Std of residuals: {np.std(residuals):.4f}\")\n",
    "print(f\"Skewness: {stats.skew(residuals):.4f}\")\n",
    "print(f\"Kurtosis: {stats.kurtosis(residuals):.4f}\")\n",
    "\n",
    "# Plot residuals over time (index)\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(residuals)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Test sample index')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Residuals over time')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We compute basic statistics: mean near zero indicates unbiasedness. Skewness and kurtosis describe the shape. A residual plot helps spot trends, clusters, or changing variance.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.3 Systematic Error Patterns**\n",
    "\n",
    "We can look for patterns by plotting residuals against:\n",
    "\n",
    "- Predicted values\n",
    "- Individual features\n",
    "- Time (day of week, month, etc.)\n",
    "\n",
    "### **37.3.1 Residuals vs. Predicted**\n",
    "\n",
    "A random scatter around zero is ideal. A funnel shape suggests heteroscedasticity.\n",
    "\n",
    "```python\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.scatter(y_pred, residuals, alpha=0.5)\n",
    "plt.axhline(y=0, color='r', linestyle='--')\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Residuals vs. Predicted')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **37.3.2 Residuals vs. Features**\n",
    "\n",
    "Plot residuals against each feature to see if errors depend on feature values. For example, if errors are larger when RSI is extreme, the model may not handle overbought/oversold conditions well.\n",
    "\n",
    "```python\n",
    "# Assume we have a DataFrame X_test with feature columns\n",
    "# For demonstration, we'll create a synthetic feature\n",
    "X_test = pd.DataFrame({\n",
    "    'RSI': np.random.uniform(0, 100, 200),\n",
    "    'Lag1': np.random.randn(200)\n",
    "})\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12,4))\n",
    "axes[0].scatter(X_test['RSI'], residuals, alpha=0.5)\n",
    "axes[0].set_xlabel('RSI')\n",
    "axes[0].set_ylabel('Residual')\n",
    "axes[0].axhline(y=0, color='r', linestyle='--')\n",
    "\n",
    "axes[1].scatter(X_test['Lag1'], residuals, alpha=0.5)\n",
    "axes[1].set_xlabel('Lag1')\n",
    "axes[1].set_ylabel('Residual')\n",
    "axes[1].axhline(y=0, color='r', linestyle='--')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **37.3.3 Residuals by Time Categories**\n",
    "\n",
    "If we have datetime information, we can group residuals by day of week, month, etc.\n",
    "\n",
    "```python\n",
    "# Assume we have a test set with dates\n",
    "dates = pd.date_range('2023-01-01', periods=200, freq='B')\n",
    "df_test = pd.DataFrame({'Date': dates, 'Residual': residuals})\n",
    "df_test['DayOfWeek'] = df_test['Date'].dt.dayofweek\n",
    "df_test['Month'] = df_test['Date'].dt.month\n",
    "\n",
    "# Boxplot by day of week\n",
    "df_test.boxplot(column='Residual', by='DayOfWeek')\n",
    "plt.title('Residuals by Day of Week')\n",
    "plt.suptitle('')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "If the median residual is not zero on certain days, the model may have systematic bias related to day\u2011of\u2011week effects.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.4 Residual Distribution Analysis**\n",
    "\n",
    "We can check if residuals are approximately normal using a Q\u2011Q plot and statistical tests.\n",
    "\n",
    "```python\n",
    "# Q-Q plot\n",
    "stats.probplot(residuals, dist=\"norm\", plot=plt)\n",
    "plt.title('Q-Q Plot of Residuals')\n",
    "plt.show()\n",
    "\n",
    "# Shapiro-Wilk test for normality\n",
    "shapiro_stat, shapiro_p = stats.shapiro(residuals)\n",
    "print(f\"Shapiro-Wilk p-value: {shapiro_p:.4f}\")\n",
    "if shapiro_p > 0.05:\n",
    "    print(\"Residuals appear normally distributed (fail to reject H0)\")\n",
    "else:\n",
    "    print(\"Residuals do not appear normally distributed\")\n",
    "```\n",
    "\n",
    "**Note:** Normality is not strictly required for forecasting models, but severe non\u2011normality may indicate model misspecification or outliers.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.5 Temporal Error Patterns**\n",
    "\n",
    "Time\u2011series residuals often exhibit autocorrelation or volatility clustering.\n",
    "\n",
    "### **37.5.1 Autocorrelation**\n",
    "\n",
    "Autocorrelation means that errors are correlated with their own past values. This violates the independence assumption and suggests the model is missing some temporal structure (e.g., an AR term).\n",
    "\n",
    "```python\n",
    "# ACF plot\n",
    "plot_acf(residuals, lags=30)\n",
    "plt.show()\n",
    "\n",
    "# Durbin-Watson test (values near 2 indicate no autocorrelation)\n",
    "dw = durbin_watson(residuals)\n",
    "print(f\"Durbin-Watson statistic: {dw:.4f}\")\n",
    "# If dw << 2, positive autocorrelation; if dw >> 2, negative autocorrelation.\n",
    "```\n",
    "\n",
    "If autocorrelation is present, consider adding lagged features or using an ARIMA model.\n",
    "\n",
    "### **37.5.2 Heteroscedasticity (Volatility Clustering)**\n",
    "\n",
    "In financial returns, volatility tends to cluster: large changes follow large changes. If residuals show periods of high variance followed by low variance, it suggests the model does not capture volatility dynamics.\n",
    "\n",
    "```python\n",
    "# Plot squared residuals\n",
    "plt.figure(figsize=(12,4))\n",
    "plt.plot(residuals**2)\n",
    "plt.title('Squared Residuals (volatility proxy)')\n",
    "plt.show()\n",
    "\n",
    "# Breusch-Pagan test for heteroscedasticity\n",
    "# Need to regress squared residuals on features\n",
    "# For simplicity, we'll use a constant-only model\n",
    "from statsmodels.stats.diagnostic import het_breuschpagan\n",
    "# We need a matrix of predictors (including constant)\n",
    "X_design = sm.add_constant(X_test)  # assuming X_test is a DataFrame of features\n",
    "bp_test = het_breuschpagan(residuals, X_design)\n",
    "print(f\"Breusch-Pagan LM statistic: {bp_test[0]:.4f}, p-value: {bp_test[1]:.4f}\")\n",
    "if bp_test[1] < 0.05:\n",
    "    print(\"Evidence of heteroscedasticity\")\n",
    "else:\n",
    "    print(\"No strong evidence of heteroscedasticity\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "If heteroscedasticity is present, consider models that explicitly model volatility (e.g., GARCH) or use heteroscedasticity\u2011robust standard errors.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.6 Conditional Error Analysis**\n",
    "\n",
    "We can analyze errors conditional on specific events or regimes. For example, we might want to know if errors are larger on days when the market is volatile, or when a stock hits a circuit breaker.\n",
    "\n",
    "### **37.6.1 Error by Market Regime**\n",
    "\n",
    "Define market regimes (e.g., high/low volatility) and compare error distributions.\n",
    "\n",
    "```python\n",
    "# Example: classify days by volatility (e.g., using VIX or rolling std)\n",
    "# For demonstration, we'll create a synthetic volatility measure\n",
    "volatility = np.abs(np.random.randn(200)) + 0.5\n",
    "high_vol = volatility > np.percentile(volatility, 75)\n",
    "low_vol = volatility <= np.percentile(volatility, 75)\n",
    "\n",
    "errors_high = residuals[high_vol]\n",
    "errors_low = residuals[low_vol]\n",
    "\n",
    "print(f\"Mean error (high vol): {np.mean(errors_high):.4f}, std: {np.std(errors_high):.4f}\")\n",
    "print(f\"Mean error (low vol): {np.mean(errors_low):.4f}, std: {np.std(errors_low):.4f}\")\n",
    "\n",
    "# Boxplot comparison\n",
    "plt.boxplot([errors_high, errors_low], labels=['High Vol', 'Low Vol'])\n",
    "plt.ylabel('Residual')\n",
    "plt.title('Error Distribution by Volatility Regime')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "### **37.6.2 Error by Feature Value**\n",
    "\n",
    "We can bin a feature (e.g., RSI) and compute average absolute error per bin.\n",
    "\n",
    "```python\n",
    "bins = pd.cut(X_test['RSI'], bins=10)\n",
    "grouped = pd.DataFrame({'RSI_bin': bins, 'AbsError': np.abs(residuals)}).groupby('RSI_bin').mean()\n",
    "print(grouped)\n",
    "grouped.plot(kind='bar')\n",
    "plt.ylabel('Mean Absolute Error')\n",
    "plt.title('MAE by RSI Bin')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "If MAE is higher for extreme RSI values, the model may not capture mean reversion well.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.7 Outlier Impact Analysis**\n",
    "\n",
    "Outliers can disproportionately affect model performance. We need to identify influential points and decide whether to treat them.\n",
    "\n",
    "### **37.7.1 Cook's Distance**\n",
    "\n",
    "Cook's distance measures the influence of each observation on the regression coefficients. For tree models, we can use a simpler approach: refit the model without each point and observe the change.\n",
    "\n",
    "```python\n",
    "# For linear models, we can use Cook's distance from statsmodels\n",
    "# For tree models, we can use a simple loop\n",
    "def influence_analysis(model, X, y, indices):\n",
    "    \"\"\"\n",
    "    Estimate influence by refitting model without each index.\n",
    "    This is computationally expensive; we may sample a subset.\n",
    "    \"\"\"\n",
    "    original_pred = model.predict(X)\n",
    "    original_rmse = np.sqrt(np.mean((y - original_pred)**2))\n",
    "    influences = []\n",
    "    for idx in indices:\n",
    "        X_loo = np.delete(X, idx, axis=0)\n",
    "        y_loo = np.delete(y, idx)\n",
    "        model_loo = clone(model).fit(X_loo, y_loo)\n",
    "        pred_loo = model_loo.predict(X)\n",
    "        rmse_loo = np.sqrt(np.mean((y - pred_loo)**2))\n",
    "        influences.append(rmse_loo - original_rmse)\n",
    "    return np.array(influences)\n",
    "\n",
    "# Sample a few points to test\n",
    "sample_idx = np.random.choice(len(residuals), size=50, replace=False)\n",
    "influences = influence_analysis(model, X_test.values, y_test, sample_idx)\n",
    "plt.hist(influences, bins=20)\n",
    "plt.xlabel('Change in RMSE when point removed')\n",
    "plt.title('Influence of Points')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Points that cause a large increase in RMSE when removed are highly influential. They may be outliers or important data points. We should examine them to see if they are errors or genuine market events.\n",
    "\n",
    "### **37.7.2 Identifying Outliers in Residuals**\n",
    "\n",
    "We can flag residuals beyond a threshold (e.g., 3 standard deviations).\n",
    "\n",
    "```python\n",
    "threshold = 3 * np.std(residuals)\n",
    "outliers = np.abs(residuals) > threshold\n",
    "print(f\"Number of outliers: {np.sum(outliers)}\")\n",
    "print(f\"Outlier indices: {np.where(outliers)[0]}\")\n",
    "```\n",
    "\n",
    "If outliers are data errors, we may correct them; if they represent extreme but real events (e.g., market crash), we may want the model to capture them, but they may dominate the loss function.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.8 Error Clustering**\n",
    "\n",
    "We can cluster error\u2011prone regions by using features that characterize the context of each error. For example, we can create a dataset where each row is a prediction error, along with the feature values at that time, and then cluster these rows to find groups of similar errors.\n",
    "\n",
    "```python\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Create error context dataset\n",
    "error_context = X_test.copy()\n",
    "error_context['Error'] = residuals\n",
    "error_context['AbsError'] = np.abs(residuals)\n",
    "\n",
    "# Normalize features (excluding error columns)\n",
    "features_for_clustering = error_context[feature_cols].values\n",
    "scaler = StandardScaler()\n",
    "features_scaled = scaler.fit_transform(features_for_clustering)\n",
    "\n",
    "# Cluster the error contexts (e.g., into 3 clusters)\n",
    "kmeans = KMeans(n_clusters=3, random_state=42)\n",
    "clusters = kmeans.fit_predict(features_scaled)\n",
    "error_context['Cluster'] = clusters\n",
    "\n",
    "# Analyze each cluster\n",
    "for cluster in range(3):\n",
    "    cluster_data = error_context[error_context['Cluster'] == cluster]\n",
    "    print(f\"Cluster {cluster}: size {len(cluster_data)}, mean abs error {cluster_data['AbsError'].mean():.4f}\")\n",
    "    print(\"Feature means in this cluster:\")\n",
    "    print(cluster_data[feature_cols].mean())\n",
    "    print()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This helps identify regimes where errors are systematically high. For example, one cluster might have high RSI and low lagged returns, indicating the model fails in overbought conditions.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.9 Root Cause Analysis**\n",
    "\n",
    "Once we identify problematic patterns, we need to trace them back to their causes. Possible root causes include:\n",
    "\n",
    "- **Missing features:** The model lacks a crucial indicator (e.g., news sentiment).\n",
    "- **Feature engineering issues:** Lag windows too short, wrong transformations.\n",
    "- **Data quality problems:** Errors in the raw data (e.g., incorrect prices).\n",
    "- **Model limitations:** The model cannot capture certain non\u2011linearities.\n",
    "- **Temporal shifts:** The market regime has changed (concept drift).\n",
    "\n",
    "We can use SHAP to explain individual large errors.\n",
    "\n",
    "```python\n",
    "# For a large error instance, compute SHAP values\n",
    "large_error_idx = np.argmax(np.abs(residuals))\n",
    "shap_values_instance = explainer.shap_values(X_test.iloc[large_error_idx:large_error_idx+1])\n",
    "shap.force_plot(explainer.expected_value, shap_values_instance[0], X_test.iloc[large_error_idx])\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The force plot shows which features contributed most to the prediction (and thus the error). If the model's prediction was too high, we can see which features pushed it up.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.10 Iterative Improvement Workflow**\n",
    "\n",
    "Error analysis should be an iterative process:\n",
    "\n",
    "1. **Train initial model** and evaluate on test set.\n",
    "2. **Perform error analysis** (as above) to identify patterns.\n",
    "3. **Hypothesize causes** and propose changes (e.g., add new feature, transform feature, change model).\n",
    "4. **Implement changes** and retrain.\n",
    "5. **Validate** that the changes reduce the targeted errors without harming overall performance.\n",
    "6. **Repeat**.\n",
    "\n",
    "This is analogous to the scientific method and is key to building robust models.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.11 Documentation**\n",
    "\n",
    "Document your error analysis findings. For each major error pattern, note:\n",
    "\n",
    "- Description of the pattern\n",
    "- Possible causes\n",
    "- Impact (e.g., how much it degrades performance)\n",
    "- Proposed solution\n",
    "- Whether the solution was implemented and its effect\n",
    "\n",
    "This documentation is valuable for future debugging and for knowledge transfer.\n",
    "\n",
    "---\n",
    "\n",
    "## **37.12 Chapter Summary**\n",
    "\n",
    "In this chapter, we conducted a thorough error analysis of a time\u2011series forecasting model using the NEPSE dataset as an example.\n",
    "\n",
    "- **Residual analysis** provided basic diagnostics: mean, variance, distribution, and plots against predictions and features.\n",
    "- **Temporal patterns** were examined via autocorrelation and heteroscedasticity tests.\n",
    "- **Conditional error analysis** revealed how errors vary with feature values and market regimes.\n",
    "- **Outlier impact analysis** identified influential points.\n",
    "- **Error clustering** grouped error contexts to find systematic failure modes.\n",
    "- **Root cause analysis** used SHAP to explain large errors.\n",
    "- **Iterative improvement workflow** turned insights into actionable model enhancements.\n",
    "- **Documentation** ensured findings are captured and shared.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Regularly perform error analysis after each model iteration.\n",
    "- Look for systematic biases (e.g., day\u2011of\u2011week effects) and address them with additional features.\n",
    "- Check for autocorrelation; if present, consider adding lagged errors (AR terms) or switching to ARIMA.\n",
    "- If errors are heteroscedastic, consider volatility\u2011based features or models that account for changing variance.\n",
    "- Use SHAP to understand individual large errors and guide feature engineering.\n",
    "- Document findings to build institutional knowledge about market behavior and model limitations.\n",
    "\n",
    "In the next chapter, **Chapter 38: From Development to Production**, we will discuss the crucial steps to move a model from a Jupyter notebook to a production environment, covering code organization, testing, and deployment considerations.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 37**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='36. model_interpretation_and_explainability.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='../6. production_systems/38. from_development_to_production.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}