{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 33: Time‑Series Specific Evaluation\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand why standard evaluation metrics must be adapted for time‑series forecasting\n",
    "- Evaluate model performance across multiple forecast horizons (1‑day, 5‑day, etc.)\n",
    "- Compute cumulative forecast error to assess long‑term trend accuracy\n",
    "- Measure directional accuracy (sign prediction) as a complement to magnitude errors\n",
    "- Calculate hit rate to quantify how often predictions are within a tolerance band\n",
    "- Apply economic evaluation metrics (e.g., profitability, Sharpe ratio) to assess real‑world value\n",
    "- Use risk‑adjusted metrics (Sharpe, Sortino, Calmar) to compare models\n",
    "- Compare models against simple benchmarks (naïve, historical mean, etc.)\n",
    "- Perform statistical significance tests (Diebold‑Mariano) to determine if one model is truly better\n",
    "- Develop a robust model comparison framework for time‑series\n",
    "- Adopt best practices for evaluating forecasts in a financial context\n",
    "\n",
    "---\n",
    "\n",
    "## **33.1 Introduction to Time‑Series Specific Evaluation**\n",
    "\n",
    "Evaluating forecasting models in a time‑series context goes beyond simple regression or classification metrics. Financial time series have unique characteristics: autocorrelation, non‑stationarity, and economic significance. A model that achieves a low RMSE may still be useless if it consistently misses turning points or yields negative trading profits. Conversely, a model with higher RMSE might capture directional moves correctly and generate profitable trades.\n",
    "\n",
    "Therefore, we need a suite of evaluation tools that capture different aspects of forecast quality:\n",
    "\n",
    "- **Forecast horizon:** How does accuracy decay as we predict further into the future?\n",
    "- **Cumulative error:** Does the model correctly track the overall trend?\n",
    "- **Directional accuracy:** Does the model get the sign right?\n",
    "- **Hit rate:** Are predictions within an acceptable error band?\n",
    "- **Economic utility:** Can the model be turned into a profitable trading strategy?\n",
    "- **Risk adjustment:** How does the model perform per unit of risk?\n",
    "- **Statistical significance:** Is the improvement over a baseline genuine?\n",
    "\n",
    "We will explore each of these using the NEPSE dataset and example models (e.g., LSTM, ARIMA, naïve forecast).\n",
    "\n",
    "---\n",
    "\n",
    "## **33.2 Forecast Horizon Evaluation**\n",
    "\n",
    "A model may perform well at one‑step‑ahead forecasts but poorly at longer horizons. It is essential to evaluate performance across multiple horizons. For the NEPSE system, we might want to know how accurate our 1‑day, 5‑day, and 20‑day forecasts are.\n",
    "\n",
    "### **33.2.1 Multi‑Step Forecasting and Horizon‑Specific Metrics**\n",
    "\n",
    "We can compute RMSE, MAE, or other metrics separately for each horizon. For example, if we have a model that predicts the next 5 days simultaneously (direct multi‑step), we can evaluate each step individually.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume we have a multi-step forecast model that predicts 5 steps ahead\n",
    "# For demonstration, we'll generate synthetic multi-step predictions\n",
    "np.random.seed(42)\n",
    "n_samples = 100\n",
    "horizons = 5\n",
    "y_true_multi = np.random.randn(n_samples, horizons)  # actual future returns\n",
    "y_pred_multi = y_true_multi + np.random.randn(n_samples, horizons) * 0.5  # predictions with error\n",
    "\n",
    "# Compute RMSE per horizon\n",
    "rmse_per_horizon = np.sqrt(np.mean((y_true_multi - y_pred_multi) ** 2, axis=0))\n",
    "mae_per_horizon = np.mean(np.abs(y_true_multi - y_pred_multi), axis=0)\n",
    "\n",
    "for h in range(horizons):\n",
    "    print(f\"Horizon {h+1}: RMSE = {rmse_per_horizon[h]:.4f}, MAE = {mae_per_horizon[h]:.4f}\")\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, horizons+1), rmse_per_horizon, marker='o', label='RMSE')\n",
    "plt.plot(range(1, horizons+1), mae_per_horizon, marker='s', label='MAE')\n",
    "plt.xlabel('Forecast Horizon (days)')\n",
    "plt.ylabel('Error')\n",
    "plt.title('Forecast Error by Horizon')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We compute errors for each forecast step separately. Typically, error increases with horizon due to accumulating uncertainty. For NEPSE, we might observe that 5‑day ahead RMSE is double the 1‑day RMSE. This information helps in setting realistic expectations and deciding which horizons are reliable enough for trading.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.3 Cumulative Forecast Error**\n",
    "\n",
    "Cumulative forecast error measures how well the model tracks the overall trend over a period. It is especially important for multi‑step forecasts where we care about the cumulative return over a horizon, not just each step individually.\n",
    "\n",
    "For example, if we predict the next 5 daily returns, the cumulative 5‑day return is the sum of those returns (if we use simple returns). We can compare the predicted cumulative return to the actual cumulative return.\n",
    "\n",
    "```python\n",
    "# Compute cumulative returns (assuming simple returns)\n",
    "y_true_cumulative = np.cumprod(1 + y_true_multi / 100, axis=1) - 1  # convert to percentage returns\n",
    "y_pred_cumulative = np.cumprod(1 + y_pred_multi / 100, axis=1) - 1\n",
    "\n",
    "# For each sample, we have a cumulative return at each horizon\n",
    "# We can compute RMSE of cumulative returns per horizon\n",
    "rmse_cumulative = np.sqrt(np.mean((y_true_cumulative - y_pred_cumulative) ** 2, axis=0))\n",
    "\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, horizons+1), rmse_per_horizon, marker='o', label='Step-wise RMSE')\n",
    "plt.plot(range(1, horizons+1), rmse_cumulative, marker='s', label='Cumulative RMSE')\n",
    "plt.xlabel('Horizon (days)')\n",
    "plt.ylabel('RMSE')\n",
    "plt.title('Step-wise vs Cumulative Forecast Error')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Cumulative error often grows faster than step‑wise error because errors accumulate. A model that is good at daily forecasts may still be poor at predicting the 5‑day trend if errors are correlated. This metric helps identify that.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.4 Directional Accuracy**\n",
    "\n",
    "Directional accuracy (also called sign prediction accuracy) measures how often the model correctly predicts whether the return will be positive or negative. It is a classification metric applied to regression forecasts. For a trading strategy, getting the direction right is often more important than the exact magnitude.\n",
    "\n",
    "We can compute directional accuracy by comparing the sign of the predicted return with the sign of the actual return.\n",
    "\n",
    "```python\n",
    "def directional_accuracy(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Compute the proportion of times the sign of prediction matches the sign of actual.\n",
    "    \"\"\"\n",
    "    true_sign = np.sign(y_true)\n",
    "    pred_sign = np.sign(y_pred)\n",
    "    # Handle zero: we can define zero as either positive or negative; here we treat as correct only if both zero\n",
    "    correct = (true_sign == pred_sign)\n",
    "    return np.mean(correct)\n",
    "\n",
    "# For a single horizon (e.g., 1-day ahead)\n",
    "dir_acc_1 = directional_accuracy(y_true_multi[:, 0], y_pred_multi[:, 0])\n",
    "print(f\"1-day directional accuracy: {dir_acc_1:.4f}\")\n",
    "\n",
    "# For multi-step, we can compute per horizon or overall\n",
    "dir_acc_per_horizon = [directional_accuracy(y_true_multi[:, h], y_pred_multi[:, h]) for h in range(horizons)]\n",
    "print(\"Directional accuracy per horizon:\", np.round(dir_acc_per_horizon, 4))\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Directional accuracy above 0.5 indicates the model has some predictive power for the sign. For NEPSE, if we achieve 55% directional accuracy, we might be able to build a profitable trading strategy, depending on transaction costs.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.5 Hit Rate**\n",
    "\n",
    "Hit rate measures the proportion of predictions that fall within a certain tolerance band around the actual value. It is useful when we care about being \"close enough\" rather than exact precision. For example, we might define a hit as `|y_true - y_pred| < threshold`, where threshold could be 0.5% for returns.\n",
    "\n",
    "```python\n",
    "def hit_rate(y_true, y_pred, threshold=0.5):\n",
    "    \"\"\"\n",
    "    Proportion of predictions within threshold (absolute error).\n",
    "    \"\"\"\n",
    "    errors = np.abs(y_true - y_pred)\n",
    "    hits = errors < threshold\n",
    "    return np.mean(hits)\n",
    "\n",
    "# Example with threshold 0.5%\n",
    "hit_rate_1 = hit_rate(y_true_multi[:, 0], y_pred_multi[:, 0], threshold=0.5)\n",
    "print(f\"1-day hit rate (within 0.5%): {hit_rate_1:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Hit rate is intuitive and can be tailored to the application. For a day trader, a threshold of 0.2% might be appropriate; for a swing trader, 1% might be acceptable.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.6 Economic Evaluation Metrics**\n",
    "\n",
    "Ultimately, a forecasting model's value is determined by its economic utility. We can simulate a simple trading strategy based on the model's predictions and compute profit and loss (P&L), Sharpe ratio, maximum drawdown, etc.\n",
    "\n",
    "### **33.6.1 Simple Trading Simulation**\n",
    "\n",
    "Assume we have a model that predicts the next day's return. We can go long if the predicted return is positive, short if negative, and hold for one day. The daily return of the strategy is:\n",
    "\n",
    "`strategy_return = sign(pred) * actual_return`\n",
    "\n",
    "We then compute cumulative returns and risk metrics.\n",
    "\n",
    "```python\n",
    "# Simulate a simple strategy\n",
    "pred_sign = np.sign(y_pred_multi[:, 0])\n",
    "strategy_returns = pred_sign * y_true_multi[:, 0]  # daily returns (%)\n",
    "\n",
    "# Cumulative wealth (starting with 1)\n",
    "cumulative_wealth = np.cumprod(1 + strategy_returns / 100)\n",
    "total_return = (cumulative_wealth[-1] - 1) * 100\n",
    "\n",
    "# Annualized Sharpe ratio (assuming 252 trading days)\n",
    "sharpe = np.sqrt(252) * strategy_returns.mean() / strategy_returns.std()\n",
    "\n",
    "print(f\"Total Return: {total_return:.2f}%\")\n",
    "print(f\"Sharpe Ratio: {sharpe:.4f}\")\n",
    "\n",
    "# Plot cumulative wealth\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(cumulative_wealth, label='Strategy')\n",
    "plt.plot(np.cumprod(1 + y_true_multi[:, 0] / 100), label='Buy & Hold')\n",
    "plt.xlabel('Days')\n",
    "plt.ylabel('Cumulative Wealth')\n",
    "plt.title('Trading Strategy Performance')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This simulation shows whether the model can generate profits after accounting for direction. We compare against a buy‑and‑hold benchmark. If the Sharpe ratio is above 1, the strategy is considered good. For NEPSE, we must also account for transaction costs (brokerage, taxes), which we omit here for simplicity but should be included in a real evaluation.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.7 Risk‑Adjusted Metrics**\n",
    "\n",
    "Risk‑adjusted metrics normalize returns by the risk taken. Common metrics include:\n",
    "\n",
    "- **Sharpe ratio:** (average return) / (standard deviation of returns)\n",
    "- **Sortino ratio:** (average return) / (downside deviation) – focuses on negative volatility.\n",
    "- **Calmar ratio:** (average annual return) / (maximum drawdown)\n",
    "\n",
    "These metrics allow comparison across strategies with different risk profiles.\n",
    "\n",
    "```python\n",
    "def downside_deviation(returns, target=0):\n",
    "    \"\"\"Calculate downside deviation (standard deviation of negative returns).\"\"\"\n",
    "    downside = returns[returns < target]\n",
    "    return np.sqrt(np.mean(downside**2))\n",
    "\n",
    "sharpe = strategy_returns.mean() / strategy_returns.std() * np.sqrt(252)\n",
    "sortino = strategy_returns.mean() / downside_deviation(strategy_returns) * np.sqrt(252)\n",
    "max_drawdown = np.max(np.maximum.accumulate(cumulative_wealth) - cumulative_wealth) / np.maximum.accumulate(cumulative_wealth)\n",
    "calmar = strategy_returns.mean() * 252 / max_drawdown  # annualized return / max drawdown\n",
    "\n",
    "print(f\"Sharpe Ratio: {sharpe:.4f}\")\n",
    "print(f\"Sortino Ratio: {sortino:.4f}\")\n",
    "print(f\"Calmar Ratio: {calmar:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Higher ratios indicate better risk‑adjusted performance. For NEPSE, a Sharpe above 1 is excellent; above 2 is exceptional. However, such high values are rare in efficient markets.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.8 Benchmark Comparison**\n",
    "\n",
    "A model should always be compared against simple benchmarks to ensure it adds value. Common benchmarks for time‑series:\n",
    "\n",
    "- **Naïve forecast (persistence):** predict the last observed value.\n",
    "- **Historical mean:** predict the average of the training period.\n",
    "- **Drift:** predict a linear trend.\n",
    "- **Seasonal naïve:** for seasonal data, predict the value from the same season last year.\n",
    "\n",
    "We already introduced MASE in Chapter 31, which compares MAE to the naïve forecast's MAE. We can also compute relative metrics like **Theil's U**, which compares RMSE to the RMSE of a random walk.\n",
    "\n",
    "```python\n",
    "# Naïve forecast (persistence) for 1-step ahead\n",
    "y_naive = np.roll(y_true_multi[:, 0], 1)  # shift by one, assuming we use previous actual as prediction\n",
    "y_naive[0] = 0  # handle first prediction\n",
    "\n",
    "rmse_model = np.sqrt(mean_squared_error(y_true_multi[:, 0], y_pred_multi[:, 0]))\n",
    "rmse_naive = np.sqrt(mean_squared_error(y_true_multi[:, 0], y_naive))\n",
    "\n",
    "print(f\"Model RMSE: {rmse_model:.4f}\")\n",
    "print(f\"Naïve RMSE: {rmse_naive:.4f}\")\n",
    "print(f\"RMSE ratio (model/naïve): {rmse_model/rmse_naive:.4f}\")\n",
    "if rmse_model < rmse_naive:\n",
    "    print(\"Model beats naïve forecast.\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "If the ratio is less than 1, the model improves upon persistence. For financial returns, beating the naïve forecast is a significant achievement.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.9 Statistical Significance Testing**\n",
    "\n",
    "Even if a model has lower RMSE, we need to know if the improvement is statistically significant. The **Diebold‑Mariano test** is commonly used to compare predictive accuracy of two forecasts. It tests the null hypothesis that the two forecasts have equal predictive accuracy.\n",
    "\n",
    "```python\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "import scipy.stats as stats\n",
    "\n",
    "def diebold_mariano(e1, e2, h=1, method='HLN'):\n",
    "    \"\"\"\n",
    "    Diebold-Mariano test for equal predictive accuracy.\n",
    "    e1, e2: forecast errors from two models.\n",
    "    h: forecast horizon.\n",
    "    method: 'HLN' for Harvey, Leybourne, Newbold adjustment.\n",
    "    \"\"\"\n",
    "    d = e1**2 - e2**2  # squared error loss differential\n",
    "    n = len(d)\n",
    "    # Estimate variance of d using Newey-West (autocorrelation robust)\n",
    "    # Simple version: assume no autocorrelation (for h=1)\n",
    "    var_d = np.var(d, ddof=1) / n\n",
    "    DM = np.mean(d) / np.sqrt(var_d)\n",
    "    # For h>1, we need to account for autocorrelation\n",
    "    # We'll implement a simple version ignoring autocorrelation for demonstration\n",
    "    p_value = 2 * (1 - stats.norm.cdf(np.abs(DM)))\n",
    "    return DM, p_value\n",
    "\n",
    "# Example: compare model vs naïve forecast errors\n",
    "e_model = y_true_multi[:, 0] - y_pred_multi[:, 0]\n",
    "e_naive = y_true_multi[:, 0] - y_naive\n",
    "\n",
    "dm_stat, p_val = diebold_mariano(e_model, e_naive)\n",
    "print(f\"Diebold-Mariano statistic: {dm_stat:.4f}, p-value: {p_val:.4f}\")\n",
    "if p_val < 0.05:\n",
    "    print(\"Reject null: models have different predictive accuracy.\")\n",
    "else:\n",
    "    print(\"Cannot reject null: models may have equal accuracy.\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "A small p‑value (e.g., < 0.05) indicates that the difference in forecast accuracy is statistically significant. This helps avoid concluding superiority based on chance.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.10 Model Comparison Framework**\n",
    "\n",
    "To compare multiple models systematically, we need a framework that includes:\n",
    "\n",
    "1. **Train/validation/test splits** (temporal).\n",
    "2. **Multiple evaluation metrics** (RMSE, MAE, directional accuracy, Sharpe, etc.).\n",
    "3. **Statistical tests** to compare models pairwise.\n",
    "4. **Robustness checks** across different time periods (walk‑forward validation).\n",
    "\n",
    "We can create a table summarizing performance across models and metrics.\n",
    "\n",
    "```python\n",
    "# Example: compare three models (ARIMA, LSTM, Naïve) on multiple metrics\n",
    "models = ['ARIMA', 'LSTM', 'Naïve']\n",
    "metrics = ['RMSE', 'MAE', 'DirAcc', 'Sharpe']\n",
    "results = pd.DataFrame(index=models, columns=metrics)\n",
    "\n",
    "# Assume we have precomputed results\n",
    "results.loc['ARIMA'] = [0.85, 0.62, 0.53, 0.8]\n",
    "results.loc['LSTM'] = [0.82, 0.60, 0.55, 1.1]\n",
    "results.loc['Naïve'] = [0.90, 0.68, 0.50, 0.0]\n",
    "\n",
    "print(results)\n",
    "\n",
    "# Identify best model per metric\n",
    "print(\"\\nBest model per metric:\")\n",
    "for metric in metrics:\n",
    "    best = results[metric].idxmax() if metric in ['DirAcc', 'Sharpe'] else results[metric].idxmin()\n",
    "    print(f\"{metric}: {best}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "A summary table helps in making a final selection. Often, a model that performs well across multiple metrics is preferred.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.11 Evaluation Best Practices**\n",
    "\n",
    "To ensure robust evaluation in a time‑series context, follow these best practices:\n",
    "\n",
    "- **Use walk‑forward validation:** Evaluate models over multiple rolling windows to see performance stability.\n",
    "- **Avoid look‑ahead bias:** All features and targets must be constructed using only past information.\n",
    "- **Report multiple metrics:** No single metric tells the full story.\n",
    "- **Benchmark against simple models:** Always compare to naïve, mean, or ARIMA.\n",
    "- **Consider economic value:** A model with slightly higher RMSE but better directional accuracy may be more profitable.\n",
    "- **Test for statistical significance:** Use Diebold‑Mariano or similar tests when comparing models.\n",
    "- **Check performance across market regimes:** Evaluate separately in bull, bear, and sideways markets.\n",
    "- **Account for transaction costs:** In trading simulations, include realistic costs.\n",
    "- **Document the evaluation procedure:** Ensure reproducibility.\n",
    "\n",
    "---\n",
    "\n",
    "## **33.12 Chapter Summary**\n",
    "\n",
    "In this chapter, we explored evaluation techniques specific to time‑series forecasting, with a focus on financial applications like the NEPSE system.\n",
    "\n",
    "- **Horizon evaluation** reveals how accuracy degrades with longer forecasts.\n",
    "- **Cumulative error** assesses trend tracking.\n",
    "- **Directional accuracy** measures sign prediction, often more important than magnitude.\n",
    "- **Hit rate** gives a flexible \"close enough\" metric.\n",
    "- **Economic metrics** (P&L, Sharpe) connect model performance to real‑world value.\n",
    "- **Risk‑adjusted metrics** (Sortino, Calmar) account for downside risk.\n",
    "- **Benchmark comparison** ensures the model adds value over simple baselines.\n",
    "- **Statistical significance testing** (Diebold‑Mariano) validates improvements.\n",
    "- **Model comparison framework** organizes evaluation across multiple models and metrics.\n",
    "- **Best practices** guide robust, realistic assessment.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Always evaluate on multiple horizons, as short‑term accuracy may not translate to long‑term profitability.\n",
    "- Use directional accuracy and trading simulations to gauge economic value.\n",
    "- Compare against a naïve forecast; if you cannot beat it, your model is useless for trading.\n",
    "- Apply walk‑forward validation to test stability over time.\n",
    "- Consider transaction costs in simulations to avoid overestimating profitability.\n",
    "\n",
    "In the next chapter, **Chapter 34: Cross‑Validation Techniques**, we will delve into advanced resampling methods for time‑series, including purged and embargoed cross‑validation, to obtain more reliable performance estimates.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 33**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
