{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 32: Evaluation Metrics for Classification\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamental metrics used to evaluate classification models\n",
    "- Compute and interpret accuracy, precision, recall, and F1‑score for binary direction predictions\n",
    "- Construct and analyze a confusion matrix to understand error types\n",
    "- Explain the trade‑off between precision and recall and how to adjust decision thresholds\n",
    "- Generate ROC curves and calculate AUC to assess model discrimination\n",
    "- Use precision‑recall curves for imbalanced datasets\n",
    "- Apply log loss to evaluate probabilistic predictions\n",
    "- Handle class imbalance appropriately when choosing metrics\n",
    "- Extend binary metrics to multi‑class classification problems\n",
    "- Select the most suitable metrics for the NEPSE direction prediction task\n",
    "\n",
    "---\n",
    "\n",
    "## **32.1 Introduction to Classification Metrics**\n",
    "\n",
    "In Chapter 19, we defined a binary classification target for the NEPSE prediction system: predicting whether tomorrow's return will be positive (up) or non‑positive (down). After training a classifier (e.g., logistic regression, random forest, or neural network), we need to evaluate its performance. Classification metrics quantify how well the model distinguishes between classes.\n",
    "\n",
    "Unlike regression metrics (MAE, RMSE), classification metrics focus on counts of correct and incorrect predictions. They also allow us to assess different types of errors separately – for example, predicting an up move when the market actually goes down (false positive) versus predicting down when it goes up (false negative). Depending on the trading strategy, one error type may be more costly than the other.\n",
    "\n",
    "We will explore the most common metrics and illustrate them using predictions from a simple classifier on NEPSE data.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.2 Accuracy and Its Limitations**\n",
    "\n",
    "Accuracy is the simplest metric: the proportion of correct predictions among all predictions.\n",
    "\n",
    "**Formula:**  \n",
    "`Accuracy = (TP + TN) / (TP + TN + FP + FN)`\n",
    "\n",
    "where:\n",
    "- TP = True Positives (correctly predicted up)\n",
    "- TN = True Negatives (correctly predicted down)\n",
    "- FP = False Positives (predicted up, actual down)\n",
    "- FN = False Negatives (predicted down, actual up)\n",
    "\n",
    "Accuracy is intuitive and widely used. However, it can be misleading when classes are imbalanced. For example, if the market goes up 70% of the time, a model that always predicts \"up\" would achieve 70% accuracy without any predictive skill. Therefore, we must always consider accuracy in the context of the **baseline** (the proportion of the majority class).\n",
    "\n",
    "### **32.2.1 Computing Accuracy for NEPSE Direction Predictions**\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "\n",
    "# Assume we have prepared data X (features) and y (binary target) from previous chapters\n",
    "# For demonstration, we'll create synthetic predictions\n",
    "np.random.seed(42)\n",
    "y_true = np.random.randint(0, 2, size=1000)  # actual directions\n",
    "y_pred = np.random.randint(0, 2, size=1000)  # random predictions\n",
    "\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Baseline accuracy (predict majority class)\n",
    "baseline = max(y_true.mean(), 1 - y_true.mean())\n",
    "print(f\"Baseline accuracy (always predict majority): {baseline:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`accuracy_score` from scikit‑learn computes the fraction of matching labels. The baseline gives context: if our model's accuracy is only slightly above the baseline, it may not be truly predictive. For NEPSE, if up days constitute 52% of the test period, a model with 53% accuracy is only marginally better than guessing.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.3 Confusion Matrix**\n",
    "\n",
    "A confusion matrix provides a tabular summary of correct and incorrect predictions, broken down by class. It is the foundation for many other metrics.\n",
    "\n",
    "For binary classification, it is a 2×2 matrix:\n",
    "\n",
    "|                | Predicted Down | Predicted Up |\n",
    "|----------------|----------------|--------------|\n",
    "| Actual Down    | TN             | FP           |\n",
    "| Actual Up      | FN             | TP           |\n",
    "\n",
    "### **32.3.2 Generating a Confusion Matrix in Python**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "print(\"Confusion Matrix:\")\n",
    "print(cm)\n",
    "\n",
    "# Visualize\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Down', 'Up'])\n",
    "disp.plot()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The confusion matrix shows, for example, how many actual up days were correctly predicted (TP) and how many were misclassified as down (FN). This breakdown is essential for understanding where the model fails. In trading, we might care more about avoiding false positives (predicting up when it actually goes down) if we are going long, or false negatives if we are shorting.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.4 Precision and Recall**\n",
    "\n",
    "Precision and recall focus on the positive class (usually the class of interest, e.g., \"up\").\n",
    "\n",
    "**Precision** (also called Positive Predictive Value) measures how many of the predicted positive cases are actually positive:  \n",
    "`Precision = TP / (TP + FP)`\n",
    "\n",
    "A high precision means that when the model predicts an up move, it is very likely to be correct. This is important if false positives are costly (e.g., entering a long position that loses money).\n",
    "\n",
    "**Recall** (also called Sensitivity or True Positive Rate) measures how many of the actual positive cases the model captured:  \n",
    "`Recall = TP / (TP + FN)`\n",
    "\n",
    "A high recall means the model catches most of the up moves, missing few. This is important if missing a profit opportunity is costly.\n",
    "\n",
    "### **32.4.1 Computing Precision and Recall**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "These metrics give a more nuanced view than accuracy. For NEPSE, if precision is high but recall is low, the model is very selective: it only predicts up when it is very sure, but it misses many up moves. Conversely, high recall and low precision means it predicts up often, but many of those predictions are wrong.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.5 F1‑Score and F‑Beta Score**\n",
    "\n",
    "F1‑score is the harmonic mean of precision and recall, providing a single metric that balances both.\n",
    "\n",
    "**Formula:**  \n",
    "`F1 = 2 * (precision * recall) / (precision + recall)`\n",
    "\n",
    "It ranges from 0 to 1, with 1 being perfect precision and recall. F1 is useful when you want to balance precision and recall, especially when classes are imbalanced.\n",
    "\n",
    "**F‑Beta score** generalizes F1 by allowing different weights for precision and recall:  \n",
    "`Fβ = (1 + β²) * (precision * recall) / (β² * precision + recall)`\n",
    "\n",
    "- β < 1 weighs precision more (e.g., F0.5)\n",
    "- β > 1 weighs recall more (e.g., F2)\n",
    "\n",
    "### **32.5.1 Computing F1 and F‑Beta**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import f1_score, fbeta_score\n",
    "\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "f05 = fbeta_score(y_true, y_pred, beta=0.5)\n",
    "f2 = fbeta_score(y_true, y_pred, beta=2)\n",
    "\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "print(f\"F0.5 (precision-focused): {f05:.4f}\")\n",
    "print(f\"F2 (recall-focused): {f2:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "If your trading strategy is more sensitive to false positives (e.g., you lose money on wrong long positions), you might prefer F0.5. If missing a profit opportunity hurts more, F2 could be more appropriate.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.6 ROC Curve and AUC**\n",
    "\n",
    "The Receiver Operating Characteristic (ROC) curve plots the True Positive Rate (recall) against the False Positive Rate (FPR = FP / (FP + TN)) at various threshold settings. The Area Under the ROC Curve (AUC) summarizes the curve: AUC = 1 for a perfect model, 0.5 for a random model.\n",
    "\n",
    "ROC curves are useful for comparing models and for choosing a threshold that balances TPR and FPR according to business needs.\n",
    "\n",
    "### **32.6.1 Plotting ROC Curve and Computing AUC**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# For ROC, we need predicted probabilities (not just classes)\n",
    "# Assuming we have a model that can output probabilities\n",
    "# For demonstration, we'll use random probabilities\n",
    "y_scores = np.random.rand(1000)  # simulated probabilities\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_true, y_scores)\n",
    "auc = roc_auc_score(y_true, y_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'ROC curve (AUC = {auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"AUC: {auc:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The ROC curve shows the trade‑off: as we lower the threshold, TPR increases but so does FPR. AUC gives the probability that the model will rank a randomly chosen positive instance higher than a randomly chosen negative one. For NEPSE, an AUC > 0.6 might indicate some predictive power.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.7 Precision‑Recall Curve**\n",
    "\n",
    "When classes are imbalanced, the ROC curve can be overly optimistic because FPR remains low due to the large number of true negatives. The Precision‑Recall (PR) curve focuses on the positive class, plotting precision against recall at different thresholds. The Area Under the PR curve (AUPRC) is a useful metric.\n",
    "\n",
    "### **32.7.1 Plotting PR Curve**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "\n",
    "precision, recall, thresholds = precision_recall_curve(y_true, y_scores)\n",
    "ap = average_precision_score(y_true, y_scores)\n",
    "\n",
    "plt.figure()\n",
    "plt.plot(recall, precision, label=f'PR curve (AP = {ap:.2f})')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Average Precision: {ap:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Average Precision (AP) summarizes the PR curve. For imbalanced datasets (e.g., if up days are rare), PR curves are more informative than ROC.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.8 Log Loss (Cross‑Entropy)**\n",
    "\n",
    "Log loss, or cross‑entropy loss, measures the performance of a classification model where the output is a probability value between 0 and 1. It penalizes false classifications, but also penalizes uncertainty. The formula for binary log loss is:\n",
    "\n",
    "`LogLoss = - (1/n) Σ [yᵢ log(pᵢ) + (1 - yᵢ) log(1 - pᵢ)]`\n",
    "\n",
    "where `pᵢ` is the predicted probability of the positive class.\n",
    "\n",
    "Log loss is sensitive to how confident the model is: being wrong with high confidence is penalized more than being wrong with low confidence.\n",
    "\n",
    "### **32.8.1 Computing Log Loss**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import log_loss\n",
    "\n",
    "# y_scores are predicted probabilities for the positive class\n",
    "logloss = log_loss(y_true, y_scores)\n",
    "print(f\"Log Loss: {logloss:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "A perfect model would have log loss close to 0. A model that predicts the baseline probability (e.g., always 0.5) would have higher log loss. For NEPSE, log loss can be used to compare probability‑based models.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.9 Metrics for Imbalanced Data**\n",
    "\n",
    "In many financial datasets, the classes may be imbalanced (e.g., up days 55%, down days 45%, or more extreme). Standard accuracy can be misleading. We should:\n",
    "\n",
    "- Use precision, recall, F1, and PR curves.\n",
    "- Consider the **balanced accuracy**, which averages recall on each class: `(TPR + TNR) / 2`.\n",
    "- Use **Cohen's Kappa**, which measures agreement with chance.\n",
    "- For probabilistic outputs, use **Brier score** (mean squared error of probabilities).\n",
    "\n",
    "### **32.9.1 Balanced Accuracy and Cohen's Kappa**\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import balanced_accuracy_score, cohen_kappa_score\n",
    "\n",
    "balanced_acc = balanced_accuracy_score(y_true, y_pred)\n",
    "kappa = cohen_kappa_score(y_true, y_pred)\n",
    "\n",
    "print(f\"Balanced Accuracy: {balanced_acc:.4f}\")\n",
    "print(f\"Cohen's Kappa: {kappa:.4f}\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Balanced accuracy gives equal weight to both classes, so it is not inflated by majority class bias. Kappa measures how much better the model is than random guessing, accounting for class imbalance.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.10 Multi‑Class Classification Metrics**\n",
    "\n",
    "If we extend to multi‑class (e.g., strong up, weak up, weak down, strong down), we need metrics that handle multiple classes.\n",
    "\n",
    "- **Macro‑averaging:** Compute metric for each class independently and average (unweighted).\n",
    "- **Micro‑averaging:** Aggregate TP, FP, FN across classes and then compute metric.\n",
    "- **Weighted averaging:** Average weighted by the number of true instances per class.\n",
    "\n",
    "Scikit‑learn provides `classification_report` that includes precision, recall, F1 for each class and their averages.\n",
    "\n",
    "```python\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# Assume y_true_multi and y_pred_multi for 4 classes\n",
    "report = classification_report(y_true_multi, y_pred_multi)\n",
    "print(report)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **32.11 Choosing the Right Metric for NEPSE**\n",
    "\n",
    "For the NEPSE direction prediction task, we recommend a combination:\n",
    "\n",
    "- **Accuracy** – quick baseline, but always compare to majority class baseline.\n",
    "- **Confusion matrix** – to see where errors occur.\n",
    "- **Precision and recall** – depending on the trading strategy:\n",
    "  - If you are long‑only, you want high precision (avoid false positives that lose money).\n",
    "  - If you are short‑only, you want high recall of down moves, which translates to high precision for down class (or high recall for up if you trade both sides?).\n",
    "- **F1‑score** – a balanced view.\n",
    "- **ROC‑AUC** – overall discriminative power.\n",
    "- **Log loss** – if you need well‑calibrated probabilities for position sizing.\n",
    "\n",
    "Always compute these on a temporally separated test set to avoid look‑ahead bias.\n",
    "\n",
    "---\n",
    "\n",
    "## **32.12 Chapter Summary**\n",
    "\n",
    "In this chapter, we covered the essential classification metrics and applied them to the NEPSE direction prediction problem.\n",
    "\n",
    "- **Accuracy** is simple but can be misleading with imbalanced data.\n",
    "- **Confusion matrix** provides a detailed breakdown of errors.\n",
    "- **Precision** and **recall** focus on the positive class and reveal different aspects of performance.\n",
    "- **F1‑score** balances precision and recall.\n",
    "- **ROC curve and AUC** assess discrimination across thresholds.\n",
    "- **Precision‑recall curve** is better for imbalanced datasets.\n",
    "- **Log loss** evaluates probabilistic predictions.\n",
    "- For imbalanced data, use **balanced accuracy**, **Cohen's kappa**, and PR curves.\n",
    "- Multi‑class extensions (macro/micro averaging) allow evaluation of more than two classes.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- For a binary up/down classifier, report precision, recall, and F1 alongside accuracy.\n",
    "- Always compute the majority class baseline to put accuracy in context.\n",
    "- Use ROC‑AUC to compare models; a value above 0.6 may indicate predictive value.\n",
    "- If you plan to use predicted probabilities for position sizing, monitor log loss and calibration.\n",
    "- Tailor metric choice to the trading strategy: if false positives are costly, focus on precision.\n",
    "\n",
    "In the next chapter, **Chapter 33: Time‑Series Specific Evaluation**, we will extend these concepts to the unique aspects of evaluating forecasts over time, including cumulative errors, directional accuracy, hit rate, and economic metrics.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 32**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
