{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 36: Model Interpretation and Explainability\n",
    "\n",
    "## **Learning Objectives**\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand why interpretability is crucial in financial machine learning\n",
    "- Distinguish between global and local interpretability methods\n",
    "- Compute and interpret feature importance using permutation importance and SHAP values\n",
    "- Use tree‑specific importance measures for random forests and gradient boosting\n",
    "- Generate and analyze Partial Dependence Plots (PDP) to understand feature effects\n",
    "- Create Individual Conditional Expectation (ICE) plots to examine heterogeneity\n",
    "- Apply LIME to explain individual predictions from complex models\n",
    "- Understand counterfactual explanations and how they can aid decision‑making\n",
    "- Visualize attention weights in transformer models for time‑series\n",
    "- Implement basic neural network interpretation techniques (saliency maps)\n",
    "- Communicate model insights effectively to non‑technical stakeholders\n",
    "\n",
    "---\n",
    "\n",
    "## **36.1 Introduction to Model Interpretability**\n",
    "\n",
    "In financial applications like the NEPSE prediction system, interpretability is not just a nice‑to‑have—it is often a regulatory requirement and a practical necessity. Traders and risk managers need to trust the model’s decisions, understand why a particular prediction was made, and identify potential failure modes. Interpretability helps with:\n",
    "\n",
    "- **Debugging:** Understanding why a model makes certain errors.\n",
    "- **Fairness:** Ensuring the model does not rely on inappropriate features.\n",
    "- **Regulatory compliance:** Many financial regulations require explanations for automated decisions.\n",
    "- **Stakeholder buy‑in:** Domain experts are more likely to adopt a model they can understand.\n",
    "\n",
    "Interpretability methods can be broadly categorized into:\n",
    "\n",
    "- **Global interpretability:** Explains the overall behavior of the model (e.g., which features are most important on average).\n",
    "- **Local interpretability:** Explains an individual prediction (e.g., why was today’s forecast an up move?).\n",
    "\n",
    "We will explore both types using the NEPSE dataset and various models.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.2 Global vs. Local Interpretability**\n",
    "\n",
    "**Global interpretability** aims to understand the entire model’s decision process. For example, we might want to know which features (lagged returns, volume, RSI) are most influential across all predictions. Methods like feature importance and partial dependence plots fall into this category.\n",
    "\n",
    "**Local interpretability** focuses on a single prediction. For instance, if the model predicts a sharp increase for a particular stock tomorrow, we want to know why. Techniques like LIME and SHAP (when applied locally) provide such explanations.\n",
    "\n",
    "Both perspectives are valuable. Global explanations help validate the model against domain knowledge; local explanations build trust and aid in decision‑making for specific trades.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.3 Feature Importance**\n",
    "\n",
    "Feature importance measures how much each feature contributes to the model’s predictions. Several methods exist, depending on the model type.\n",
    "\n",
    "### **36.3.1 Permutation Importance**\n",
    "\n",
    "Permutation importance is a model‑agnostic method: it measures the drop in model performance when a feature’s values are randomly shuffled. A large drop indicates the feature is important. It works for any model and is implemented in scikit‑learn.\n",
    "\n",
    "```python\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Assume we have X_train, y_train prepared from NEPSE data (as in previous chapters)\n",
    "# Train a random forest\n",
    "model = RandomForestRegressor(n_estimators=100, max_depth=5, random_state=42)\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "# Compute permutation importance on a validation set (temporal)\n",
    "# We'll use a separate validation set X_val, y_val\n",
    "tscv = TimeSeriesSplit(n_splits=3)\n",
    "# For simplicity, we take the last 20% of training as validation\n",
    "val_size = int(0.2 * len(X_train))\n",
    "X_val, y_val = X_train[-val_size:], y_train[-val_size:]\n",
    "X_train_fit = X_train[:-val_size]\n",
    "y_train_fit = y_train[:-val_size]\n",
    "model.fit(X_train_fit, y_train_fit)\n",
    "\n",
    "result = permutation_importance(model, X_val, y_val, n_repeats=10, random_state=42, scoring='neg_mean_squared_error')\n",
    "importance_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance_mean': result.importances_mean,\n",
    "    'importance_std': result.importances_std\n",
    "}).sort_values('importance_mean', ascending=False)\n",
    "\n",
    "print(importance_df.head(10))\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.barh(importance_df['feature'][:10], importance_df['importance_mean'][:10])\n",
    "plt.xlabel('Permutation Importance (increase in MSE)')\n",
    "plt.title('Top 10 Feature Importances (Permutation)')\n",
    "plt.gca().invert_yaxis()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Permutation importance measures the increase in prediction error after shuffling a feature. If shuffling causes a large error increase, the feature is important. The standard deviation across repeats gives a sense of stability. This method is model‑agnostic and can be applied to any estimator.\n",
    "\n",
    "**Note:** For time‑series, we must ensure the validation set is temporally after the training set. Also, when shuffling, we must shuffle within the validation set only, respecting that the validation set is a contiguous block.\n",
    "\n",
    "### **36.3.2 SHAP (SHapley Additive exPlanations)**\n",
    "\n",
    "SHAP values, based on cooperative game theory, provide a unified measure of feature importance. They attribute the prediction to each feature, summing to the difference between the prediction and the expected value (baseline). SHAP works for any model (model‑agnostic) but also has optimized implementations for tree‑based models.\n",
    "\n",
    "#### **Global SHAP Importance**\n",
    "\n",
    "We can average absolute SHAP values across samples to get global importance.\n",
    "\n",
    "```python\n",
    "import shap\n",
    "\n",
    "# For tree models, we can use TreeExplainer\n",
    "explainer = shap.TreeExplainer(model)\n",
    "shap_values = explainer.shap_values(X_val)  # returns array of shape (n_samples, n_features)\n",
    "\n",
    "# Global importance: mean absolute SHAP value per feature\n",
    "shap_importance = np.abs(shap_values).mean(axis=0)\n",
    "shap_df = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': shap_importance\n",
    "}).sort_values('importance', ascending=False)\n",
    "\n",
    "print(shap_df.head(10))\n",
    "\n",
    "# Summary plot\n",
    "shap.summary_plot(shap_values, X_val, feature_names=X_train.columns)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "SHAP values show how much each feature contributed to the prediction, relative to the baseline (average prediction). Positive SHAP values push the prediction higher, negative lower. The summary plot displays feature importance (by mean absolute SHAP) and the direction of the effect (color indicates feature value).\n",
    "\n",
    "#### **Local SHAP Explanations**\n",
    "\n",
    "For a single prediction, SHAP provides a force plot:\n",
    "\n",
    "```python\n",
    "# Explain a single instance (e.g., first in validation set)\n",
    "shap.initjs()\n",
    "shap.force_plot(explainer.expected_value, shap_values[0,:], X_val.iloc[0,:], feature_names=X_train.columns)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The force plot shows which features pushed the prediction higher (red) and lower (blue), and by how much. This is a powerful local explanation tool.\n",
    "\n",
    "### **36.3.3 Tree‑Specific Importance**\n",
    "\n",
    "Tree‑based models (random forest, XGBoost, LightGBM) provide built‑in feature importance based on how often a feature is used for splitting and the improvement in impurity (e.g., Gini importance). However, these can be biased towards high‑cardinality features. Permutation importance or SHAP are often preferred.\n",
    "\n",
    "```python\n",
    "# Built-in importance from random forest\n",
    "importances = model.feature_importances_\n",
    "tree_imp_df = pd.DataFrame({'feature': X_train.columns, 'importance': importances}).sort_values('importance', ascending=False)\n",
    "print(tree_imp_df.head(10))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## **36.4 Partial Dependence Plots (PDP)**\n",
    "\n",
    "Partial Dependence Plots show the marginal effect of one or two features on the predicted outcome, averaging over the other features. They help understand the functional relationship between a feature and the target.\n",
    "\n",
    "For example, we might want to see how the predicted return changes with different values of `RSI`, holding other features constant.\n",
    "\n",
    "```python\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# PDP for one feature\n",
    "PartialDependenceDisplay.from_estimator(model, X_train, ['RSI'], kind='average')\n",
    "plt.show()\n",
    "\n",
    "# PDP for two features (interaction)\n",
    "PartialDependenceDisplay.from_estimator(model, X_train, [('RSI', 'Return_Lag1')], kind='average')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The PDP shows that as RSI increases, the predicted return might decrease (if RSI is used as a mean‑reversion indicator). The 2D PDP can reveal interactions, e.g., the effect of RSI might depend on the lagged return.\n",
    "\n",
    "**Caveat:** PDP assumes features are independent, which may not hold. Also, it averages over the data distribution, which can hide heterogeneity (see ICE plots).\n",
    "\n",
    "---\n",
    "\n",
    "## **36.5 Individual Conditional Expectation (ICE)**\n",
    "\n",
    "ICE plots extend PDP by showing the prediction for each individual sample as the feature varies. They reveal whether the effect is consistent across observations.\n",
    "\n",
    "```python\n",
    "from sklearn.inspection import PartialDependenceDisplay\n",
    "\n",
    "# ICE for one feature\n",
    "PartialDependenceDisplay.from_estimator(model, X_train, ['RSI'], kind='individual', subsample=50)\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Each line represents one observation's predicted outcome as RSI changes. If lines are roughly parallel, the effect is homogeneous; if they cross or have different slopes, there is interaction with other features.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.6 LIME (Local Interpretable Model‑agnostic Explanations)**\n",
    "\n",
    "LIME explains individual predictions by approximating the complex model locally with a simple interpretable model (e.g., linear regression). It perturbs the input, observes the model’s predictions, and fits a weighted linear model in the neighborhood.\n",
    "\n",
    "```python\n",
    "from lime import lime_tabular\n",
    "\n",
    "# Create LIME explainer\n",
    "explainer_lime = lime_tabular.LimeTabularExplainer(\n",
    "    X_train.values,\n",
    "    feature_names=X_train.columns,\n",
    "    mode='regression',\n",
    "    discretize_continuous=False\n",
    ")\n",
    "\n",
    "# Explain a single prediction (e.g., first test instance)\n",
    "i = 0\n",
    "exp = explainer_lime.explain_instance(X_test.iloc[i].values, model.predict, num_features=5)\n",
    "exp.show_in_notebook()\n",
    "exp.as_list()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "LIME produces a list of feature contributions (positive or negative) for that instance, similar to a linear model. It is model‑agnostic and works for any black‑box model. However, it can be unstable (different runs may give slightly different explanations) and the neighborhood definition matters.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.7 Counterfactual Explanations**\n",
    "\n",
    "A counterfactual explanation answers: \"What would need to change in the input to get a different prediction?\" For example, if the model predicts a down move, a counterfactual might show that if RSI were 10 points higher, the prediction would be up.\n",
    "\n",
    "Counterfactuals are intuitive and actionable. They can be generated by optimization: find the smallest change to the input that flips the prediction.\n",
    "\n",
    "```python\n",
    "# Simple counterfactual using `alibi` library (if installed)\n",
    "from alibi.explainers import Counterfactual\n",
    "\n",
    "# Define a prediction function\n",
    "predict_fn = lambda x: model.predict(x)\n",
    "\n",
    "# Initialize counterfactual explainer\n",
    "cf = Counterfactual(predict_fn, shape=(1, X_train.shape[1]), target_proba=0.5, tolerance=0.01)\n",
    "\n",
    "# Explain a single instance\n",
    "explanation = cf.explain(X_test.iloc[0:1].values)\n",
    "print(explanation.cf['X'])\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Counterfactuals provide a minimal perturbation to change the outcome. They are very useful for understanding decision boundaries and for giving actionable advice (e.g., \"if you want the stock to be predicted up, you would need a higher RSI\").\n",
    "\n",
    "---\n",
    "\n",
    "## **36.8 Attention Visualization**\n",
    "\n",
    "For transformer models, attention weights can be visualized to see which time steps the model focuses on when making a prediction. This is particularly insightful for time‑series.\n",
    "\n",
    "If we have a transformer model (e.g., from Chapter 28), we can extract attention weights from a layer and plot them.\n",
    "\n",
    "```python\n",
    "# Assuming we have a trained transformer model with a MultiHeadAttention layer\n",
    "# We need to create a model that outputs attention weights\n",
    "# This is model-specific; here's a generic example\n",
    "\n",
    "# Get attention layer (assuming it's named 'multi_head_attention')\n",
    "attention_layer = model.get_layer('multi_head_attention')\n",
    "\n",
    "# Create a model that outputs attention weights\n",
    "attention_model = tf.keras.Model(inputs=model.input, outputs=attention_layer.output)\n",
    "\n",
    "# For a given input, get attention scores\n",
    "sample_input = X_test_scaled[0:1]  # shape (1, seq_len, features)\n",
    "attention_weights = attention_model(sample_input)  # shape may vary\n",
    "\n",
    "# Average over heads and plot\n",
    "avg_attention = tf.reduce_mean(attention_weights, axis=1)[0]  # (seq_len, seq_len)\n",
    "plt.imshow(avg_attention, cmap='viridis')\n",
    "plt.xlabel('Key time steps')\n",
    "plt.ylabel('Query time steps')\n",
    "plt.colorbar()\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The attention matrix shows for each query time step (y‑axis) how much it attends to each key time step (x‑axis). High values indicate strong influence. This can reveal, for example, that the model focuses on recent days (diagonal) or on specific past events.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.9 Interpreting Neural Networks**\n",
    "\n",
    "For neural networks, several techniques exist:\n",
    "\n",
    "- **Saliency maps:** Compute the gradient of the output with respect to input features. Large gradients indicate sensitivity.\n",
    "- **Activation maximization:** Find inputs that maximally activate a neuron.\n",
    "- **Integrated gradients:** A more robust gradient‑based attribution method.\n",
    "\n",
    "### **Saliency Maps Example**\n",
    "\n",
    "```python\n",
    "import tensorflow as tf\n",
    "\n",
    "# Assume we have a trained Keras model `nn_model`\n",
    "def get_saliency(model, x):\n",
    "    x = tf.convert_to_tensor(x)\n",
    "    with tf.GradientTape() as tape:\n",
    "        tape.watch(x)\n",
    "        pred = model(x)\n",
    "    grads = tape.gradient(pred, x)\n",
    "    return grads.numpy()\n",
    "\n",
    "# Compute saliency for a single sample\n",
    "sample = X_test_scaled[0:1]\n",
    "sal = get_saliency(nn_model, sample)\n",
    "# Average over features if multivariate\n",
    "sal_mean = np.mean(np.abs(sal), axis=-1).flatten()\n",
    "# Plot saliency over time\n",
    "plt.plot(sal_mean)\n",
    "plt.xlabel('Time step')\n",
    "plt.ylabel('Saliency')\n",
    "plt.title('Saliency Map for Prediction')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Saliency shows which time steps most influence the prediction. High values indicate sensitivity; the model would change its prediction if those steps were altered.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.10 Communicating Results to Stakeholders**\n",
    "\n",
    "Explanations are only useful if they can be understood by non‑technical stakeholders (traders, managers, regulators). Tips:\n",
    "\n",
    "- Use visualizations (force plots, PDPs) rather than raw numbers.\n",
    "- Relate explanations to domain concepts (e.g., \"the model thinks RSI is overbought\").\n",
    "- Provide both global and local views.\n",
    "- Be honest about uncertainty and limitations.\n",
    "- Create automated reports that summarize key insights for each trading day.\n",
    "\n",
    "For the NEPSE system, we might produce a daily dashboard showing:\n",
    "\n",
    "- Top features influencing today's prediction.\n",
    "- A PDP showing how the prediction changes with key indicators.\n",
    "- A counterfactual: what would need to change for a different prediction.\n",
    "- Attention heatmap for transformer models.\n",
    "\n",
    "---\n",
    "\n",
    "## **36.11 Chapter Summary**\n",
    "\n",
    "In this chapter, we explored a wide range of model interpretation and explainability techniques, all applied to the NEPSE prediction system.\n",
    "\n",
    "- **Global interpretability** methods like permutation importance, SHAP, and PDP help us understand the model’s overall behavior.\n",
    "- **Local interpretability** methods (LIME, SHAP force plots, counterfactuals) explain individual predictions, which is crucial for trading decisions.\n",
    "- **Tree‑specific importance** is quick but can be biased; permutation and SHAP are more reliable.\n",
    "- **PDP and ICE** reveal feature effects and heterogeneity.\n",
    "- **Attention visualization** gives insight into transformer models.\n",
    "- **Neural network interpretation** via saliency maps shows sensitivity to input time steps.\n",
    "- **Counterfactuals** provide actionable advice.\n",
    "- **Communication** of results to stakeholders is essential for adoption.\n",
    "\n",
    "### **Practical Takeaways for the NEPSE System:**\n",
    "\n",
    "- Start with permutation importance to identify the most influential features globally.\n",
    "- Use SHAP to get both global rankings and local explanations for each prediction.\n",
    "- Generate PDPs for key features (e.g., RSI, lagged returns) to understand their marginal effects.\n",
    "- For individual trades, use LIME or SHAP force plots to explain why a buy/sell signal was generated.\n",
    "- If using transformers, visualize attention to see which past days the model focuses on.\n",
    "- Incorporate these explanations into a dashboard for traders.\n",
    "\n",
    "In the next chapter, **Chapter 37: Error Analysis**, we will learn how to systematically analyze model errors to identify weaknesses and guide improvements.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 36**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
