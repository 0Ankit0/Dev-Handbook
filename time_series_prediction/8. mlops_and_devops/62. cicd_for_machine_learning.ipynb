{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 62: CI/CD for Machine Learning\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the fundamentals of Continuous Integration and Continuous Delivery (CI/CD) and their importance for ML systems\n",
    "- Identify the unique challenges of applying CI/CD to machine learning compared to traditional software\n",
    "- Design and implement automated testing strategies for ML projects, including data tests, model tests, and infrastructure tests\n",
    "- Set up continuous integration pipelines that automatically validate code, data, and models\n",
    "- Build continuous delivery pipelines that deploy models to staging and production environments\n",
    "- Implement continuous deployment strategies for models that pass all validation gates\n",
    "- Apply GitOps principles to manage ML infrastructure and model versions declaratively\n",
    "- Use Infrastructure as Code (IaC) to provision and manage ML environments\n",
    "- Orchestrate end‑to‑end ML pipelines that combine training, validation, and deployment\n",
    "- Adopt best practices for CI/CD in ML to ensure reliable, reproducible, and frequent model updates\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous chapter, we introduced MLOps and its importance for production ML systems. One of the core practices of MLOps is **CI/CD**—Continuous Integration and Continuous Delivery (or Deployment). CI/CD pipelines automate the process of building, testing, and deploying software, enabling teams to deliver changes frequently and reliably.\n",
    "\n",
    "For machine learning systems, CI/CD is both more important and more challenging than for traditional software. An ML system has multiple artifacts: code, data, features, and models. Each of these can change and must be validated. A change in the training data could break the model just as easily as a code change. Therefore, CI/CD for ML must encompass data and model validation alongside traditional software tests.\n",
    "\n",
    "In this chapter, we will explore how to adapt CI/CD principles to machine learning. We will build automated pipelines for the NEPSE prediction system that test data quality, validate model performance, and deploy new models safely. We'll use tools like GitHub Actions, Jenkins, and MLflow, and discuss best practices for each stage.\n",
    "\n",
    "---\n",
    "\n",
    "## 62.1 CI/CD Fundamentals\n",
    "\n",
    "Before diving into ML‑specific aspects, let's review the basic concepts of CI/CD.\n",
    "\n",
    "### 62.1.1 Continuous Integration (CI)\n",
    "\n",
    "**Continuous Integration** is the practice of automatically integrating code changes from multiple contributors into a shared repository several times a day. Each integration is verified by an automated build and tests, allowing teams to detect problems early.\n",
    "\n",
    "Key elements of CI:\n",
    "- Developers frequently merge code to a central repository (e.g., `main` branch).\n",
    "- An automated server (e.g., Jenkins, GitHub Actions) runs a build and tests on every push.\n",
    "- If tests fail, the team is notified and fixes the issue immediately.\n",
    "\n",
    "For ML, CI extends beyond code to include data and model validation.\n",
    "\n",
    "### 62.1.2 Continuous Delivery (CD)\n",
    "\n",
    "**Continuous Delivery** extends CI by ensuring that the software can be released to production at any time. After passing CI, the application is automatically deployed to a staging environment that mirrors production, where further tests (e.g., integration tests, performance tests) are run. The release to production is still a manual decision but can be done with a single click.\n",
    "\n",
    "### 62.1.3 Continuous Deployment\n",
    "\n",
    "**Continuous Deployment** goes one step further: every change that passes all stages of the production pipeline is automatically released to users, with no human intervention. This requires a high level of confidence in the automated testing and deployment process.\n",
    "\n",
    "For ML systems, continuous deployment of models is possible but requires robust validation and monitoring to prevent deploying a bad model.\n",
    "\n",
    "---\n",
    "\n",
    "## 62.2 ML‑Specific CI/CD Challenges\n",
    "\n",
    "ML systems introduce unique challenges for CI/CD:\n",
    "\n",
    "1. **Multiple artifacts**: Besides code, we have data, features, and models. Each must be versioned and tested.\n",
    "2. **Non‑deterministic training**: Model training involves randomness. The same code and data can produce slightly different models. Tests must account for this variability.\n",
    "3. **Data and concept drift**: A model that performed well at training time may degrade in production due to changing data. CI/CD pipelines must include monitoring and trigger retraining.\n",
    "4. **Model validation is complex**: Evaluating a model requires more than just a unit test. It involves comparing against baselines, checking for fairness, and measuring performance on holdout data.\n",
    "5. **Training is expensive**: Running a full training pipeline on every code change may be prohibitively expensive. We need strategies to decide when to retrain.\n",
    "\n",
    "Despite these challenges, CI/CD for ML is achievable and essential for maintaining production systems.\n",
    "\n",
    "---\n",
    "\n",
    "## 62.3 Automated Testing for ML\n",
    "\n",
    "Testing in ML systems must cover multiple layers:\n",
    "\n",
    "- **Code tests**: Unit tests for functions (e.g., feature engineering functions, data loading).\n",
    "- **Data tests**: Validate input data schema, distributions, and quality.\n",
    "- **Model tests**: Verify model performance, consistency, and fairness.\n",
    "- **Infrastructure tests**: Ensure the serving infrastructure works correctly (e.g., API responses).\n",
    "\n",
    "### 62.3.1 Code Tests\n",
    "\n",
    "Standard unit tests for Python functions using `pytest`. For example, test the RSI calculation function.\n",
    "\n",
    "```python\n",
    "# test_features.py\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from features import compute_rsi\n",
    "\n",
    "def test_rsi_calculation():\n",
    "    prices = pd.Series([100, 101, 102, 101, 100, 99, 98])\n",
    "    expected_rsi = pd.Series([np.nan, np.nan, np.nan, np.nan, np.nan, 41.18, 33.33])\n",
    "    result = compute_rsi(prices, period=5)\n",
    "    pd.testing.assert_series_equal(result, expected_rsi, check_less_precise=True)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We test a core feature engineering function with known inputs and expected outputs. This ensures that changes to the code do not break the feature calculation.\n",
    "\n",
    "### 62.3.2 Data Tests\n",
    "\n",
    "Data tests validate that the input data meets expectations. For the NEPSE system, we might check:\n",
    "\n",
    "- No missing values in critical columns.\n",
    "- Prices are positive.\n",
    "- Volume is integer and non‑negative.\n",
    "- Date column is in the correct format and increasing.\n",
    "\n",
    "**Example using `great_expectations`:**\n",
    "\n",
    "```python\n",
    "import great_expectations as ge\n",
    "\n",
    "def test_data_quality(df):\n",
    "    df_ge = ge.from_pandas(df)\n",
    "    \n",
    "    # Expectations\n",
    "    df_ge.expect_column_values_to_not_be_null('Close')\n",
    "    df_ge.expect_column_values_to_be_between('Close', min_value=0)\n",
    "    df_ge.expect_column_values_to_be_between('Volume', min_value=0)\n",
    "    df_ge.expect_column_values_to_match_strftime_format('Date', '%Y-%m-%d')\n",
    "    \n",
    "    # Run validation\n",
    "    results = df_ge.validate()\n",
    "    assert results['success'], \"Data quality checks failed\"\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`great_expectations` allows us to define declarative expectations for data. These can be run in CI to catch data issues early.\n",
    "\n",
    "### 62.3.3 Model Tests\n",
    "\n",
    "Model tests go beyond simple accuracy checks. They might include:\n",
    "\n",
    "- **Performance on holdout set**: Ensure the new model meets a minimum accuracy threshold.\n",
    "- **Comparison with baseline**: New model should outperform the current production model (or a simple heuristic).\n",
    "- **Invariance tests**: The model should not change its prediction for small, semantically meaningless changes (e.g., adding tiny noise).\n",
    "- **Fairness tests**: Ensure the model performs similarly across different groups (if applicable).\n",
    "\n",
    "**Example: Comparing with baseline**\n",
    "\n",
    "```python\n",
    "def test_model_against_baseline(new_model, baseline_model, X_test, y_test):\n",
    "    new_preds = new_model.predict(X_test)\n",
    "    baseline_preds = baseline_model.predict(X_test)\n",
    "    \n",
    "    new_acc = accuracy_score(y_test, new_preds)\n",
    "    baseline_acc = accuracy_score(y_test, baseline_preds)\n",
    "    \n",
    "    assert new_acc > baseline_acc, f\"New model ({new_acc:.3f}) not better than baseline ({baseline_acc:.3f})\"\n",
    "```\n",
    "\n",
    "### 62.3.4 Infrastructure Tests\n",
    "\n",
    "Once a model is deployed, we need to test that the serving infrastructure works correctly. This includes:\n",
    "\n",
    "- API endpoint returns correct HTTP status codes.\n",
    "- Response time is within limits.\n",
    "- Model inference works with expected input formats.\n",
    "\n",
    "**Example using `requests` and `pytest`:**\n",
    "\n",
    "```python\n",
    "import requests\n",
    "\n",
    "def test_prediction_endpoint():\n",
    "    url = \"http://localhost:8000/predict\"\n",
    "    sample_input = {\"features\": [1.2, 3.4, 5.6, 7.8]}\n",
    "    \n",
    "    response = requests.post(url, json=sample_input)\n",
    "    assert response.status_code == 200\n",
    "    data = response.json()\n",
    "    assert 'probability' in data\n",
    "    assert 0 <= data['probability'] <= 1\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 62.4 Continuous Integration for ML\n",
    "\n",
    "A CI pipeline for ML should run automatically on every push to the repository (or on a schedule). A typical ML CI pipeline might include:\n",
    "\n",
    "1. **Lint and format code** (e.g., `black`, `flake8`).\n",
    "2. **Run unit tests** on feature engineering code.\n",
    "3. **Run data validation tests** on a sample of the data.\n",
    "4. **Train a quick model** on a subset of data to ensure the training script runs.\n",
    "5. **Evaluate the model** on a small validation set (quick check).\n",
    "6. **Package the model** and any necessary artifacts.\n",
    "\n",
    "**Example GitHub Actions workflow for ML CI:**\n",
    "\n",
    "```yaml\n",
    "name: ML CI\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main, develop ]\n",
    "  pull_request:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  test:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    \n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    \n",
    "    - name: Install dependencies\n",
    "      run: |\n",
    "        pip install -r requirements.txt\n",
    "        pip install -r requirements-dev.txt  # testing tools\n",
    "    \n",
    "    - name: Lint with flake8\n",
    "      run: flake8 src/ tests/\n",
    "    \n",
    "    - name: Run unit tests\n",
    "      run: pytest tests/unit\n",
    "    \n",
    "    - name: Run data validation\n",
    "      run: python scripts/validate_data.py --sample-size 1000\n",
    "    \n",
    "    - name: Quick model training test\n",
    "      run: python scripts/train_quick.py --max-samples 5000 --max-epochs 2\n",
    "    \n",
    "    - name: Package model (if training succeeded)\n",
    "      run: python scripts/package_model.py\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This workflow runs on every push and pull request. It performs code quality checks, unit tests, data validation, and a quick training test to catch errors early. The quick training uses a subset of data to avoid excessive runtime.\n",
    "\n",
    "---\n",
    "\n",
    "## 62.5 Continuous Delivery for ML\n",
    "\n",
    "Continuous delivery ensures that a validated model can be deployed to production at any time. After CI passes, we typically deploy to a staging environment that mirrors production, run more extensive tests, and then promote to production.\n",
    "\n",
    "**Staging environment tests might include:**\n",
    "\n",
    "- **Full training on all data** (if not too expensive).\n",
    "- **Integration tests** with downstream systems (e.g., databases, dashboards).\n",
    "- **Performance tests** under simulated load.\n",
    "- **Shadow deployment**: Send real traffic to the new model but use predictions for logging only, not for decisions.\n",
    "\n",
    "### 62.5.1 Staging Deployment with MLflow\n",
    "\n",
    "MLflow can help manage model staging. After training, we can register the model in the MLflow Model Registry with the stage \"Staging\".\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"NEPSE_Predictor\",\n",
    "    version=5,\n",
    "    stage=\"Staging\"\n",
    ")\n",
    "```\n",
    "\n",
    "Then, in the staging environment, we can automatically deploy all models in the \"Staging\" stage.\n",
    "\n",
    "### 62.5.2 GitHub Actions for CD\n",
    "\n",
    "We can extend our GitHub Actions workflow to deploy to staging after CI passes on the main branch.\n",
    "\n",
    "```yaml\n",
    "name: ML CD\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "\n",
    "jobs:\n",
    "  deploy-staging:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    # ... install dependencies, etc.\n",
    "    - name: Train full model\n",
    "      run: python scripts/train_full.py\n",
    "    - name: Register model in MLflow\n",
    "      run: python scripts/register_model.py\n",
    "    - name: Deploy to staging\n",
    "      run: python scripts/deploy_staging.py\n",
    "    - name: Run staging tests\n",
    "      run: pytest tests/staging\n",
    "    - name: Promote to production (if manual approval)\n",
    "      run: echo \"Ready for production\"\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This workflow trains a full model, registers it, deploys to staging, and runs tests. After successful tests, it waits for manual approval (or could automatically promote if confidence is high).\n",
    "\n",
    "---\n",
    "\n",
    "## 62.6 Continuous Deployment for ML\n",
    "\n",
    "Continuous deployment automatically promotes models to production after passing all validation gates. This requires a high level of automation and confidence in the testing process.\n",
    "\n",
    "**Gates for auto‑promotion might include:**\n",
    "\n",
    "- Model performance on staging data exceeds current production model by a statistically significant margin.\n",
    "- No degradation in fairness metrics.\n",
    "- Latency and throughput meet SLOs.\n",
    "- Shadow deployment shows no anomalies.\n",
    "\n",
    "**Example: Auto‑promotion script**\n",
    "\n",
    "```python\n",
    "def should_promote(staging_model_id, production_model_id, test_data):\n",
    "    staging_metrics = evaluate_model(staging_model_id, test_data)\n",
    "    production_metrics = evaluate_model(production_model_id, test_data)\n",
    "    \n",
    "    # Check if staging model is significantly better\n",
    "    if staging_metrics['accuracy'] > production_metrics['accuracy'] + 0.01:\n",
    "        # Also check latency\n",
    "        staging_latency = measure_latency(staging_model_id)\n",
    "        if staging_latency < 100:  # milliseconds\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "if should_promote(staging_model, production_model, test_data):\n",
    "    client.transition_model_version_stage(\n",
    "        name=\"NEPSE_Predictor\",\n",
    "        version=staging_version,\n",
    "        stage=\"Production\"\n",
    "    )\n",
    "    # Also update the serving endpoint\n",
    "    update_production_endpoint(staging_model_id)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "The script compares the staging model against the current production model on a holdout dataset. If it meets criteria (better accuracy and acceptable latency), it is automatically promoted to production.\n",
    "\n",
    "---\n",
    "\n",
    "## 62.7 GitOps for ML\n",
    "\n",
    "**GitOps** is a practice where the entire system's desired state is described in Git, and automated processes ensure the actual state matches the desired state. For ML, this means:\n",
    "\n",
    "- Model versions, configurations, and infrastructure definitions are stored in Git.\n",
    "- Changes are made via pull requests.\n",
    "- CI/CD pipelines automatically apply the changes.\n",
    "\n",
    "### 62.7.1 Infrastructure as Code (IaC)\n",
    "\n",
    "Use tools like Terraform, AWS CloudFormation, or Pulumi to define your ML infrastructure (e.g., Kubernetes clusters, S3 buckets, IAM roles). Store these definitions in Git.\n",
    "\n",
    "**Example Terraform snippet for an S3 bucket:**\n",
    "\n",
    "```hcl\n",
    "resource \"aws_s3_bucket\" \"model_artifacts\" {\n",
    "  bucket = \"nepse-model-artifacts\"\n",
    "  acl    = \"private\"\n",
    "  \n",
    "  versioning {\n",
    "    enabled = true\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "When a pull request changes this file, a CI job can run `terraform plan` to show the changes, and after merge, `terraform apply` to enact them.\n",
    "\n",
    "### 62.7.2 Model Versions in Git\n",
    "\n",
    "For smaller projects, you might store model files directly in Git (using Git LFS). More commonly, you store a pointer to the model in a registry (like MLflow), and the registry itself can be backed by Git.\n",
    "\n",
    "**Example: A YAML file describing the desired production model**\n",
    "\n",
    "```yaml\n",
    "# production-model.yaml\n",
    "model_name: NEPSE_Predictor\n",
    "model_version: 5\n",
    "serving_endpoint: https://api.nepse.example.com/predict\n",
    "```\n",
    "\n",
    "A GitOps operator (e.g., Argo CD) could watch this file and ensure the deployed model matches the specified version.\n",
    "\n",
    "---\n",
    "\n",
    "## 62.8 Infrastructure as Code (IaC) for ML\n",
    "\n",
    "IaC is a key enabler of GitOps. It allows you to provision and manage ML infrastructure programmatically, ensuring consistency and reproducibility.\n",
    "\n",
    "### 62.8.1 Terraform for ML Infrastructure\n",
    "\n",
    "Terraform can manage cloud resources for the entire ML pipeline:\n",
    "\n",
    "- **Data storage**: S3 buckets, BigQuery datasets.\n",
    "- **Compute**: SageMaker instances, Kubernetes clusters.\n",
    "- **Networking**: VPCs, security groups.\n",
    "- **IAM roles and policies**.\n",
    "\n",
    "**Example: Provisioning a SageMaker notebook instance**\n",
    "\n",
    "```hcl\n",
    "resource \"aws_sagemaker_notebook_instance\" \"nepse_notebook\" {\n",
    "  name          = \"nepse-data-science\"\n",
    "  role_arn      = aws_iam_role.sagemaker_role.arn\n",
    "  instance_type = \"ml.t3.medium\"\n",
    "  \n",
    "  tags = {\n",
    "    Project = \"NEPSE\"\n",
    "  }\n",
    "}\n",
    "```\n",
    "\n",
    "### 62.8.2 Kubernetes Manifests for Model Serving\n",
    "\n",
    "If you deploy models on Kubernetes, you can define the deployment and service in YAML and store them in Git.\n",
    "\n",
    "```yaml\n",
    "# deployment.yaml\n",
    "apiVersion: apps/v1\n",
    "kind: Deployment\n",
    "metadata:\n",
    "  name: nepse-predictor-v5\n",
    "spec:\n",
    "  replicas: 3\n",
    "  selector:\n",
    "    matchLabels:\n",
    "      app: nepse-predictor\n",
    "      version: v5\n",
    "  template:\n",
    "    metadata:\n",
    "      labels:\n",
    "        app: nepse-predictor\n",
    "        version: v5\n",
    "    spec:\n",
    "      containers:\n",
    "      - name: predictor\n",
    "        image: myregistry/nepse-predictor:v5\n",
    "        ports:\n",
    "        - containerPort: 8000\n",
    "```\n",
    "\n",
    "Updating the version in Git and syncing with the cluster (using Argo CD) deploys the new model.\n",
    "\n",
    "---\n",
    "\n",
    "## 62.9 Pipeline Orchestration\n",
    "\n",
    "CI/CD pipelines for ML often need to orchestrate multiple steps: data validation, training, evaluation, deployment. Tools like Apache Airflow, Prefect, and Kubeflow Pipelines are designed for this.\n",
    "\n",
    "### 62.9.1 Airflow DAG for ML Pipeline\n",
    "\n",
    "An Airflow DAG can define the entire workflow, with dependencies and retries.\n",
    "\n",
    "```python\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "default_args = {\n",
    "    'owner': 'ml-team',\n",
    "    'depends_on_past': False,\n",
    "    'start_date': datetime(2024, 1, 1),\n",
    "    'retries': 1,\n",
    "    'retry_delay': timedelta(minutes=5),\n",
    "}\n",
    "\n",
    "dag = DAG(\n",
    "    'nepse_training_pipeline',\n",
    "    default_args=default_args,\n",
    "    schedule_interval='@weekly',\n",
    "    catchup=False\n",
    ")\n",
    "\n",
    "def validate_data(**context):\n",
    "    # Run data validation\n",
    "    pass\n",
    "\n",
    "def train_model(**context):\n",
    "    # Train model\n",
    "    pass\n",
    "\n",
    "def evaluate_model(**context):\n",
    "    # Evaluate and compare with baseline\n",
    "    pass\n",
    "\n",
    "def deploy_if_better(**context):\n",
    "    # Deploy if evaluation passes\n",
    "    pass\n",
    "\n",
    "validate = PythonOperator(task_id='validate_data', python_callable=validate_data, dag=dag)\n",
    "train = PythonOperator(task_id='train_model', python_callable=train_model, dag=dag)\n",
    "evaluate = PythonOperator(task_id='evaluate_model', python_callable=evaluate_model, dag=dag)\n",
    "deploy = PythonOperator(task_id='deploy_if_better', python_callable=deploy_if_better, dag=dag)\n",
    "\n",
    "validate >> train >> evaluate >> deploy\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This DAG runs weekly. It validates data, trains a model, evaluates it, and deploys only if it meets criteria. Airflow handles scheduling, retries, and monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "## 62.10 Best Practices for ML CI/CD\n",
    "\n",
    "1. **Start small**: Automate one part of the pipeline at a time (e.g., data validation first).\n",
    "2. **Version everything**: Code, data, models, and environments.\n",
    "3. **Test data as rigorously as code**: Data drift can break models.\n",
    "4. **Use staging environments**: Test models in an environment that mirrors production.\n",
    "5. **Monitor deployed models**: CI/CD doesn't stop at deployment; monitor performance and trigger retraining.\n",
    "6. **Keep pipelines fast**: Use incremental training or smaller datasets for quick CI tests.\n",
    "7. **Secure secrets**: Never hard‑code credentials; use secrets managers (e.g., GitHub Secrets, HashiCorp Vault).\n",
    "8. **Document the pipeline**: Make it easy for team members to understand and modify.\n",
    "9. **Have a rollback plan**: Automate the ability to revert to a previous model version.\n",
    "10. **Embrace GitOps**: Use Git as the single source of truth for both code and configuration.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored how to apply CI/CD principles to machine learning systems, using the NEPSE prediction system as a concrete example. We covered:\n",
    "\n",
    "- The fundamentals of CI/CD and their adaptation to ML.\n",
    "- ML‑specific testing strategies: data tests, model tests, infrastructure tests.\n",
    "- Building continuous integration pipelines with GitHub Actions.\n",
    "- Continuous delivery to staging environments and continuous deployment with automatic promotion.\n",
    "- GitOps and Infrastructure as Code for managing ML infrastructure declaratively.\n",
    "- Orchestrating complex ML pipelines with Airflow.\n",
    "- Best practices for reliable and efficient ML CI/CD.\n",
    "\n",
    "By implementing CI/CD for the NEPSE system, we can ensure that model updates are delivered quickly, reliably, and safely. This transforms the project from a static model into a living system that adapts to changing market conditions.\n",
    "\n",
    "In the next chapter, we will discuss **Feature Stores**, a critical component for managing features consistently across training and serving.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 62**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
