{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 65: Data and Model Lineage\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand what data and model lineage means and why it is crucial for governance, debugging, and reproducibility\n",
    "- Distinguish between data lineage (tracking data transformations) and model lineage (tracking model creation and deployment)\n",
    "- Capture lineage information using tools like DVC, MLflow, and custom metadata stores\n",
    "- Implement a simple lineage tracking system for the NEPSE prediction pipeline\n",
    "- Visualise lineage graphs to understand dependencies and impact of changes\n",
    "- Use lineage for impact analysis (e.g., if a source dataset changes, which models are affected?)\n",
    "- Ensure compliance with regulations (e.g., GDPR right to explanation) through lineage\n",
    "- Integrate lineage with experiment tracking and model registries\n",
    "- Recognise the challenges of lineage in complex, distributed systems\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the NEPSE prediction system, we have multiple components: raw data CSV files, feature engineering scripts, trained models, and deployed APIs. If a data quality issue is discovered in the raw data from 2023, which models are affected? If we update a feature definition, which experiment runs used the old definition? Answering these questions requires **lineage**\u2014the ability to trace the relationships between data, code, features, models, and predictions.\n",
    "\n",
    "Lineage is a fundamental aspect of data governance and MLOps. It enables:\n",
    "\n",
    "- **Reproducibility**: Knowing exactly which data and code produced a model.\n",
    "- **Debugging**: Tracing errors back to their source.\n",
    "- **Impact analysis**: Assessing the effect of changes upstream.\n",
    "- **Compliance**: Meeting regulatory requirements for explainability and audit trails.\n",
    "- **Trust**: Providing transparency to stakeholders.\n",
    "\n",
    "In this chapter, we will explore both data lineage and model lineage. We will use tools like DVC for data versioning and MLflow for model tracking, and we will build a simple lineage graph to visualise dependencies. Using the NEPSE system as a running example, we will see how lineage can be captured and used in practice.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.1 Lineage Concepts\n",
    "\n",
    "### 65.1.1 Data Lineage\n",
    "\n",
    "**Data lineage** tracks the origin and transformations of data as it flows through pipelines. For the NEPSE system, this includes:\n",
    "\n",
    "- Raw CSV files from the exchange.\n",
    "- Cleaned and imputed data.\n",
    "- Engineered features (e.g., lagged prices, rolling statistics).\n",
    "- Training and test splits.\n",
    "\n",
    "Each transformation step should be recorded, along with the code version and parameters used. This allows us to trace any feature value back to its source.\n",
    "\n",
    "### 65.1.2 Model Lineage\n",
    "\n",
    "**Model lineage** tracks the creation and deployment of models. It includes:\n",
    "\n",
    "- The training dataset (version) used.\n",
    "- The code and configuration (hyperparameters) that produced the model.\n",
    "- The experiment run that generated the model.\n",
    "- The model version in the registry.\n",
    "- The deployment environment and stage (staging, production).\n",
    "\n",
    "Model lineage helps answer: \"Which model is currently in production, and how was it trained?\"\n",
    "\n",
    "### 65.1.3 Why Both Matter\n",
    "\n",
    "Data and model lineage are intertwined. A model is trained on a specific version of a dataset. If that dataset is updated, the model's performance may change. By linking model lineage to data lineage, we can understand the full picture.\n",
    "\n",
    "For example, if we discover an error in the raw data from a certain period, we can query the lineage to find all models trained on that data and retrain them.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.2 Tools for Lineage\n",
    "\n",
    "Several tools can help capture lineage:\n",
    "\n",
    "- **DVC** (Data Version Control): Versions datasets and pipelines, tracking dependencies between code and data.\n",
    "- **MLflow**: Tracks experiments, models, and can log dataset versions.\n",
    "- **Great Expectations**: Validates data and can store expectations as metadata.\n",
    "- **Apache Atlas**: Enterprise\u2011grade data governance and lineage platform.\n",
    "- **Amundsen**: Data discovery and lineage tool.\n",
    "- **OpenLineage**: Open standard for lineage metadata collection.\n",
    "\n",
    "For the NEPSE system, we will combine DVC for data versioning and pipeline tracking, and MLflow for experiment and model tracking.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.3 Implementing Data Lineage with DVC\n",
    "\n",
    "DVC (Data Version Control) is a tool that brings Git\u2011like versioning to data and machine learning pipelines. It tracks data files and the transformations applied to them.\n",
    "\n",
    "### 65.3.1 Setting Up DVC\n",
    "\n",
    "```bash\n",
    "pip install dvc\n",
    "cd nepse-project\n",
    "git init\n",
    "dvc init\n",
    "```\n",
    "\n",
    "DVC stores metadata in `.dvc` files and a local cache. The actual data can be stored remotely (e.g., S3, GCS).\n",
    "\n",
    "### 65.3.2 Adding Raw Data\n",
    "\n",
    "```bash\n",
    "dvc add data/raw/nepse_2023.csv\n",
    "git add data/raw/nepse_2023.csv.dvc data/raw/.gitignore\n",
    "git commit -m \"Add raw NEPSE 2023 data\"\n",
    "```\n",
    "\n",
    "The `.dvc` file contains a hash of the data file. This hash uniquely identifies the version.\n",
    "\n",
    "### 65.3.3 Defining a Pipeline\n",
    "\n",
    "DVC allows you to define pipelines where each stage takes dependencies and produces outputs. This automatically captures lineage.\n",
    "\n",
    "Create a `dvc.yaml` file:\n",
    "\n",
    "```yaml\n",
    "stages:\n",
    "  clean:\n",
    "    cmd: python scripts/clean_data.py data/raw/nepse_2023.csv data/interim/cleaned.csv\n",
    "    deps:\n",
    "      - scripts/clean_data.py\n",
    "      - data/raw/nepse_2023.csv\n",
    "    outs:\n",
    "      - data/interim/cleaned.csv\n",
    "  features:\n",
    "    cmd: python scripts/feature_engineering.py data/interim/cleaned.csv data/processed/features.csv\n",
    "    deps:\n",
    "      - scripts/feature_engineering.py\n",
    "      - data/interim/cleaned.csv\n",
    "    outs:\n",
    "      - data/processed/features.csv\n",
    "  train:\n",
    "    cmd: python scripts/train_model.py data/processed/features.csv models/model.pkl\n",
    "    deps:\n",
    "      - scripts/train_model.py\n",
    "      - data/processed/features.csv\n",
    "    outs:\n",
    "      - models/model.pkl\n",
    "    metrics:\n",
    "      - metrics/accuracy.json\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Each stage specifies its dependencies (code and input data) and outputs. DVC tracks the hashes of all dependencies and outputs. When you run `dvc repro`, DVC checks if any dependency has changed and re\u2011runs the necessary stages. This ensures reproducibility and provides lineage: we know exactly which data and code produced each output.\n",
    "\n",
    "### 65.3.4 Running the Pipeline\n",
    "\n",
    "```bash\n",
    "dvc repro\n",
    "```\n",
    "\n",
    "This executes the pipeline. DVC records the execution in `dvc.lock`, which captures the exact versions (hashes) of all dependencies and outputs.\n",
    "\n",
    "### 65.3.5 Exploring Lineage with DVC\n",
    "\n",
    "DVC provides commands to show the pipeline graph:\n",
    "\n",
    "```bash\n",
    "dvc dag\n",
    "```\n",
    "\n",
    "This displays a text\u2011based graph of stages. For a visual representation, you can use `dvc dag --dot` and render with Graphviz.\n",
    "\n",
    "To see the history of a data file:\n",
    "\n",
    "```bash\n",
    "dvc log data/processed/features.csv\n",
    "```\n",
    "\n",
    "This shows the Git commits that changed the file, linking to the pipeline stages.\n",
    "\n",
    "### 65.3.6 Remote Storage\n",
    "\n",
    "For collaboration, you can set up a remote storage (e.g., S3 bucket) and push data:\n",
    "\n",
    "```bash\n",
    "dvc remote add -d myremote s3://mybucket/nepse-dvc\n",
    "dvc push\n",
    "```\n",
    "\n",
    "Now the data is versioned and stored centrally.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.4 Integrating Data Lineage with MLflow\n",
    "\n",
    "While DVC tracks data and pipeline versions, MLflow tracks experiments and models. We can link them by logging DVC file hashes or Git commits in MLflow.\n",
    "\n",
    "### 65.4.1 Logging Data Version in MLflow\n",
    "\n",
    "In our training script, we can capture the DVC hash of the features file.\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def get_dvc_hash(filepath):\n",
    "    # Get the DVC hash of a file (if tracked by DVC)\n",
    "    result = subprocess.run(['dvc', 'status', filepath], capture_output=True, text=True)\n",
    "    # Parse output to get hash (simplified)\n",
    "    # Better: use dvc.api to get the hash\n",
    "    import dvc.api\n",
    "    return dvc.api.get_url(filepath)  # returns the resource URL with hash\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # ... other logging ...\n",
    "    \n",
    "    # Log data version\n",
    "    features_hash = get_dvc_hash('data/processed/features.csv')\n",
    "    mlflow.log_param(\"features_data_version\", features_hash)\n",
    "    \n",
    "    # Also log the commit hash of the code\n",
    "    commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()\n",
    "    mlflow.log_param(\"git_commit\", commit)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "By logging the DVC hash of the features file and the Git commit, we create a link between the model and the exact data and code that produced it. If we later need to reproduce the model, we can check out that Git commit and run `dvc checkout` to restore the data.\n",
    "\n",
    "### 65.4.2 Using DVC and MLflow Together\n",
    "\n",
    "A common workflow:\n",
    "\n",
    "1. Data scientists develop features and update the DVC pipeline.\n",
    "2. They run `dvc repro` to update the features file.\n",
    "3. They run training scripts with MLflow tracking, which logs the DVC hash.\n",
    "4. The best model is registered in MLflow Model Registry.\n",
    "5. For deployment, the model is fetched along with its metadata, including the data version.\n",
    "\n",
    "This creates a complete lineage chain.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.5 Model Lineage with MLflow\n",
    "\n",
    "MLflow already captures significant model lineage through its tracking and registry.\n",
    "\n",
    "### 65.5.1 Tracking Run Metadata\n",
    "\n",
    "Each MLflow run records:\n",
    "\n",
    "- Parameters (including data versions, as above).\n",
    "- Metrics.\n",
    "- Tags (e.g., `model_type`, `feature_set`).\n",
    "- Artifacts (model file, plots).\n",
    "- Source code (if using MLflow projects).\n",
    "\n",
    "This metadata forms the model's lineage.\n",
    "\n",
    "### 65.5.2 Model Registry Lineage\n",
    "\n",
    "When a model is registered in the MLflow Model Registry, it retains a link to the source run. You can see which run produced a model version and then drill down into that run's details.\n",
    "\n",
    "```python\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "model_version_details = client.get_model_version(\"NEPSE_Predictor\", 5)\n",
    "run_id = model_version_details.run_id\n",
    "run = client.get_run(run_id)\n",
    "print(run.data.params)\n",
    "```\n",
    "\n",
    "This allows you to trace a deployed model back to its training run and all associated metadata.\n",
    "\n",
    "### 65.5.3 Stage Transitions\n",
    "\n",
    "The registry also records stage transitions (e.g., from \"Staging\" to \"Production\") along with who performed the transition and when. This is part of the model's deployment lineage.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.6 Building a Simple Lineage Graph\n",
    "\n",
    "We can build a simple lineage graph by combining DVC pipeline info and MLflow run info. This graph can help visualise dependencies.\n",
    "\n",
    "### 65.6.1 Extracting DVC Pipeline\n",
    "\n",
    "DVC provides a JSON representation of the pipeline:\n",
    "\n",
    "```bash\n",
    "dvc dag --dot > pipeline.dot\n",
    "```\n",
    "\n",
    "You can convert this to a graph using Graphviz or parse it to extract dependencies.\n",
    "\n",
    "### 65.6.2 Extracting MLflow Lineage\n",
    "\n",
    "MLflow provides a REST API to query runs. We can fetch runs that produced models and link them to data versions.\n",
    "\n",
    "### 65.6.3 Example: Visualising with NetworkX\n",
    "\n",
    "```python\n",
    "import networkx as nx\n",
    "import matplotlib.pyplot as plt\n",
    "import mlflow\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "# Create a graph\n",
    "G = nx.DiGraph()\n",
    "\n",
    "# Add DVC pipeline nodes (simplified: from dvc.lock)\n",
    "# For each stage, add edges from deps to outs.\n",
    "\n",
    "# Add MLflow run nodes\n",
    "client = MlflowClient()\n",
    "runs = client.search_runs(experiment_ids=[\"1\"])\n",
    "for run in runs:\n",
    "    run_id = run.info.run_id\n",
    "    G.add_node(f\"run_{run_id}\", type=\"run\", accuracy=run.data.metrics.get(\"test_accuracy\"))\n",
    "    # Link run to data version (if logged)\n",
    "    data_version = run.data.params.get(\"features_data_version\")\n",
    "    if data_version:\n",
    "        G.add_edge(f\"data_{data_version}\", f\"run_{run_id}\")\n",
    "\n",
    "# Draw\n",
    "pos = nx.spring_layout(G)\n",
    "nx.draw(G, pos, with_labels=True, node_size=3000, node_color='lightblue')\n",
    "plt.show()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This is a simplistic example, but it shows the idea. In practice, lineage graphs can become large, so tools like Apache Atlas or custom graph databases (e.g., Neo4j) are used.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.7 Impact Analysis\n",
    "\n",
    "One of the main uses of lineage is **impact analysis**: if something changes upstream, what downstream artifacts are affected?\n",
    "\n",
    "### 65.7.1 Data Change\n",
    "\n",
    "Suppose we discover that the raw data for March 2023 had a systematic error. We fix it and update the raw CSV. Using DVC, we run:\n",
    "\n",
    "```bash\n",
    "dvc repro\n",
    "```\n",
    "\n",
    "DVC automatically detects which stages depend on the changed raw data and re\u2011runs them, producing new cleaned data, new features, and new models. This ensures all downstream artifacts are consistent.\n",
    "\n",
    "But what about models already in production that were trained on the old data? We need to trace them. Using the lineage graph, we can query all models that used the old data version (by the DVC hash). We can then decide to retrain them or mark them as deprecated.\n",
    "\n",
    "### 65.7.2 Code Change\n",
    "\n",
    "If a feature engineering script changes, DVC will rerun that stage and all downstream stages. MLflow runs that used the old script will still exist in the tracking server, but they are not automatically updated. However, we can query runs by the Git commit of the script and identify which models were affected.\n",
    "\n",
    "### 65.7.3 Example Query\n",
    "\n",
    "Using MLflow, we can search for runs that used a specific data version:\n",
    "\n",
    "```python\n",
    "runs = client.search_runs(\n",
    "    experiment_ids=[\"1\"],\n",
    "    filter_string=\"params.features_data_version = 'abc123'\"\n",
    ")\n",
    "```\n",
    "\n",
    "This returns all runs trained on that data version.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.8 Compliance and Auditing\n",
    "\n",
    "For regulated industries, lineage is essential for audits. Regulators may ask:\n",
    "\n",
    "- Which data was used to train the model that made this prediction?\n",
    "- How was that data processed?\n",
    "- Was the model properly validated before deployment?\n",
    "\n",
    "A lineage system provides answers. By storing all metadata immutably, we can produce an audit trail.\n",
    "\n",
    "### 65.8.1 Storing Lineage for Compliance\n",
    "\n",
    "Consider storing lineage information in an immutable data store (e.g., append\u2011only database, blockchain) to prevent tampering. For each prediction, you might log:\n",
    "\n",
    "- Model version.\n",
    "- Input features (or a hash).\n",
    "- Prediction timestamp.\n",
    "- Output.\n",
    "\n",
    "Then, using the model version, you can trace back to the training run and data.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.9 Challenges and Best Practices\n",
    "\n",
    "### 65.9.1 Challenges\n",
    "\n",
    "- **Scale**: In large organisations with thousands of models and datasets, lineage graphs become huge. Automated tools are necessary.\n",
    "- **Granularity**: How much detail to capture? Too much detail can be overwhelming; too little defeats the purpose.\n",
    "- **Integration**: Different tools (DVC, MLflow, Airflow) have their own lineage representations. Integrating them requires effort.\n",
    "- **Dynamic systems**: In streaming pipelines, data is continuously updated, making lineage more complex.\n",
    "\n",
    "### 65.9.2 Best Practices\n",
    "\n",
    "1. **Start simple**: Begin by capturing data version and code version for each model. Expand as needed.\n",
    "2. **Automate lineage collection**: Integrate with your pipeline tools rather than manually recording.\n",
    "3. **Use unique identifiers**: Use hashes of data files and code commits as universal IDs.\n",
    "4. **Store lineage alongside artifacts**: Keep metadata with the model file (e.g., in MLflow).\n",
    "5. **Regularly audit lineage**: Check that lineage is complete and correct.\n",
    "6. **Plan for retention**: Decide how long to keep lineage information (e.g., for regulatory requirements).\n",
    "7. **Educate the team**: Ensure everyone understands the importance of lineage and follows practices that enable it.\n",
    "\n",
    "---\n",
    "\n",
    "## 65.10 Complete Example: NEPSE Lineage Pipeline\n",
    "\n",
    "Let's put together a complete example for the NEPSE system that captures data and model lineage using DVC and MLflow.\n",
    "\n",
    "**Directory structure:**\n",
    "\n",
    "```\n",
    "nepse-project/\n",
    "\u251c\u2500\u2500 data/\n",
    "\u2502   \u251c\u2500\u2500 raw/\n",
    "\u2502   \u2502   \u2514\u2500\u2500 nepse_2023.csv\n",
    "\u2502   \u251c\u2500\u2500 interim/\n",
    "\u2502   \u2502   \u2514\u2500\u2500 cleaned.csv\n",
    "\u2502   \u2514\u2500\u2500 processed/\n",
    "\u2502       \u2514\u2500\u2500 features.csv\n",
    "\u251c\u2500\u2500 scripts/\n",
    "\u2502   \u251c\u2500\u2500 clean_data.py\n",
    "\u2502   \u251c\u2500\u2500 feature_engineering.py\n",
    "\u2502   \u2514\u2500\u2500 train_model.py\n",
    "\u251c\u2500\u2500 models/\n",
    "\u2502   \u2514\u2500\u2500 model.pkl\n",
    "\u251c\u2500\u2500 metrics/\n",
    "\u2502   \u2514\u2500\u2500 accuracy.json\n",
    "\u251c\u2500\u2500 dvc.yaml\n",
    "\u251c\u2500\u2500 dvc.lock\n",
    "\u2514\u2500\u2500 .gitignore\n",
    "```\n",
    "\n",
    "**dvc.yaml** (as shown earlier).\n",
    "\n",
    "**scripts/train_model.py** (with MLflow and DVC integration):\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import subprocess\n",
    "import json\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import dvc.api\n",
    "\n",
    "def get_dvc_hash(path):\n",
    "    # Using dvc.api to get the file's DVC hash (if tracked)\n",
    "    repo = dvc.api.DVCRepo('.')\n",
    "    try:\n",
    "        rev = repo.get_rev()\n",
    "        url = dvc.api.get_url(path, repo=repo, rev=rev)\n",
    "        # URL includes the hash (e.g., .../file?rev=abc123)\n",
    "        return url\n",
    "    except:\n",
    "        return \"unknown\"\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--data_path', default='data/processed/features.csv')\n",
    "    parser.add_argument('--model_path', default='models/model.pkl')\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--max_depth', type=int, default=10)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Start MLflow run\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_param(\"n_estimators\", args.n_estimators)\n",
    "        mlflow.log_param(\"max_depth\", args.max_depth)\n",
    "        \n",
    "        # Log data version\n",
    "        data_hash = get_dvc_hash(args.data_path)\n",
    "        mlflow.log_param(\"features_data_version\", data_hash)\n",
    "        \n",
    "        # Log code version (Git commit)\n",
    "        commit = subprocess.check_output(['git', 'rev-parse', 'HEAD']).decode().strip()\n",
    "        mlflow.log_param(\"git_commit\", commit)\n",
    "        \n",
    "        # Load data\n",
    "        df = pd.read_csv(args.data_path)\n",
    "        X = df.drop(columns=['target'])\n",
    "        y = df['target']\n",
    "        \n",
    "        # Train/test split (time-based)\n",
    "        split_idx = int(0.8 * len(df))\n",
    "        X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "        y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "        \n",
    "        # Train\n",
    "        model = RandomForestClassifier(n_estimators=args.n_estimators,\n",
    "                                       max_depth=args.max_depth,\n",
    "                                       random_state=42)\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_acc = accuracy_score(y_train, model.predict(X_train))\n",
    "        test_acc = accuracy_score(y_test, model.predict(X_test))\n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "        \n",
    "        # Save model\n",
    "        mlflow.sklearn.log_model(model, \"model\")\n",
    "        \n",
    "        # Also save metrics to file (for DVC)\n",
    "        with open('metrics/accuracy.json', 'w') as f:\n",
    "            json.dump({'test_accuracy': test_acc}, f)\n",
    "        \n",
    "        print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "**Running the pipeline:**\n",
    "\n",
    "```bash\n",
    "dvc repro\n",
    "```\n",
    "\n",
    "This will run the entire pipeline, and the training stage will log to MLflow. The DVC lock file records all dependencies and outputs.\n",
    "\n",
    "**Exploring lineage:**\n",
    "\n",
    "- To see the pipeline graph: `dvc dag`\n",
    "- To see MLflow runs: `mlflow ui`\n",
    "- To find which model version used a particular data hash: use MLflow search.\n",
    "\n",
    "Now we have a complete lineage from raw data to trained model, all captured and queryable.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored the critical topic of data and model lineage. We covered:\n",
    "\n",
    "- The definitions and importance of lineage for reproducibility, debugging, impact analysis, and compliance.\n",
    "- Tools for lineage: DVC for data and pipeline versioning, and MLflow for experiment and model tracking.\n",
    "- How to implement data lineage with DVC pipelines, capturing dependencies and versions.\n",
    "- How to integrate data lineage with MLflow by logging DVC hashes.\n",
    "- Model lineage through MLflow runs and the model registry.\n",
    "- Building simple lineage graphs and performing impact analysis.\n",
    "- Compliance considerations and best practices.\n",
    "- A complete example for the NEPSE system combining DVC and MLflow.\n",
    "\n",
    "With lineage in place, the NEPSE prediction system becomes transparent and auditable. We can trace any prediction back to the data and code that produced it, and we can assess the impact of changes upstream. This is a fundamental requirement for trustworthy machine learning in production.\n",
    "\n",
    "In the next chapter, we will discuss **Model Governance**, which builds on lineage to ensure that models are developed, deployed, and maintained responsibly.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 65**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='64. experiment_tracking.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='66. model_governance.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}