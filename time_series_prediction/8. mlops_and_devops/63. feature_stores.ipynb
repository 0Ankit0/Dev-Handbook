{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 63: Feature Stores\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the concept of a feature store and its role in the MLOps lifecycle\n",
    "- Differentiate between offline and online feature stores and their use cases\n",
    "- Explain the importance of point‑in‑time correctness to avoid data leakage\n",
    "- Design and implement a simple feature store using Feast for the NEPSE prediction system\n",
    "- Define feature sets and register them in a feature registry\n",
    "- In压 features for training (offline) and serve them for real‑time inference (online)\n",
    "- Manage feature versioning and lineage to ensure reproducibility\n",
    "- Integrate a feature store with existing data pipelines and model training scripts\n",
    "- Recognise the benefits of feature stores: reusability, consistency, and reduced training‑serving skew\n",
    "- Evaluate when to adopt a feature store versus simpler alternatives\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous chapters, we engineered features for the NEPSE prediction system—lags, rolling statistics, technical indicators—and used them to train models. However, we faced a common problem: the same features had to be recomputed both during training (on historical data) and during inference (on live data). This duplication of effort often leads to inconsistencies, known as **training‑serving skew**. Moreover, features were tied to specific models, making it difficult to reuse them across different projects or share them among team members.\n",
    "\n",
    "A **feature store** solves these problems by acting as a central repository for features. It provides a consistent way to define, compute, store, and serve features for both training and inference. Feature stores have become a cornerstone of modern MLOps architectures, especially for time‑series and real‑time applications.\n",
    "\n",
    "In this chapter, we will explore the concepts behind feature stores, their architecture, and how to implement one using the open‑source tool **Feast**. Using the NEPSE system as a concrete example, we will see how a feature store can streamline our workflow, ensure consistency, and enable collaboration.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.1 What is a Feature Store?\n",
    "\n",
    "A **feature store** is a data management layer for machine learning. It stores pre‑computed features and makes them available for both training (offline) and serving (online). It also manages metadata about features, such as their definitions, sources, and versions.\n",
    "\n",
    "### 63.1.1 Core Components\n",
    "\n",
    "A typical feature store consists of:\n",
    "\n",
    "- **Offline Store**: A storage system (e.g., data lake, data warehouse) that holds historical feature data, used for training and batch inference. It supports large‑scale retrieval of feature data at specific points in time.\n",
    "- **Online Store**: A low‑latency database (e.g., Redis, DynamoDB) that stores the latest feature values for each entity, used for real‑time inference.\n",
    "- **Feature Registry**: A catalog of all defined features, including their metadata, source data, and transformation logic.\n",
    "- **Serving API**: A service that provides features to models during training and inference, ensuring the correct features are retrieved efficiently.\n",
    "\n",
    "### 63.1.2 Why Use a Feature Store?\n",
    "\n",
    "For the NEPSE system, a feature store offers several benefits:\n",
    "\n",
    "- **Consistency**: The same feature definitions are used for training and serving, eliminating training‑serving skew.\n",
    "- **Reusability**: Features like `RSI_14` or `SMA_20` can be defined once and used by multiple models (e.g., different stocks or different prediction horizons).\n",
    "- **Point‑in‑time correctness**: When joining features for training, we must ensure we only use data available at the prediction time (no look‑ahead). Feature stores handle this automatically.\n",
    "- **Reduced engineering effort**: Data scientists can focus on feature logic, while the feature store handles storage, serving, and scalability.\n",
    "- **Feature discovery and governance**: A central registry allows teams to discover existing features, understand their meaning, and track lineage.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.2 Feature Store Architecture\n",
    "\n",
    "A typical feature store architecture is shown below:\n",
    "\n",
    "```\n",
    "                    ┌─────────────────┐\n",
    "                    │   Data Sources  │\n",
    "                    │ (e.g., Kafka,   │\n",
    "                    │  historical DB) │\n",
    "                    └────────┬────────┘\n",
    "                             │\n",
    "                             ▼\n",
    "                    ┌─────────────────┐\n",
    "                    │  Stream/Batch   │\n",
    "                    │   Processing    │\n",
    "                    │  (e.g., Spark,  │\n",
    "                    │   Flink)        │\n",
    "                    └────────┬────────┘\n",
    "                             │\n",
    "              ┌──────────────┴──────────────┐\n",
    "              │                             │\n",
    "              ▼                             ▼\n",
    "    ┌─────────────────┐             ┌─────────────────┐\n",
    "    │  Offline Store  │             │  Online Store   │\n",
    "    │ (e.g., BigQuery,│             │ (e.g., Redis,   │\n",
    "    │  S3 + Parquet)  │             │  DynamoDB)      │\n",
    "    └────────┬────────┘             └────────┬────────┘\n",
    "             │                               │\n",
    "             │                               │\n",
    "             ▼                               ▼\n",
    "    ┌─────────────────┐             ┌─────────────────┐\n",
    "    │ Training        │             │ Online Serving  │\n",
    "    │ (historical     │             │ (real‑time)     │\n",
    "    │  feature views) │             │                 │\n",
    "    └─────────────────┘             └─────────────────┘\n",
    "```\n",
    "\n",
    "- **Data sources** can be batch (e.g., daily CSV files) or streaming (e.g., Kafka topics). For NEPSE, we might have daily CSV files and a live tick stream.\n",
    "- **Batch/Stream processing** computes features from raw data. For example, a Spark job computes daily rolling statistics and stores them in the offline store, while a Flink job computes real‑time features and updates the online store.\n",
    "- **Offline store** stores historical feature values with timestamps. This is used to generate training datasets.\n",
    "- **Online store** stores the most recent feature values per entity (e.g., per stock) for low‑latency retrieval.\n",
    "- **Training** retrieves features from the offline store, often for a specific time range, to create training data.\n",
    "- **Online serving** retrieves the latest features from the online store during inference.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.3 Point‑in‑Time Correctness\n",
    "\n",
    "One of the most critical aspects of feature stores is ensuring **point‑in‑time correctness**. When training a model, we must join features that were available at the time of prediction. For example, if we want to predict tomorrow's return using today's RSI, we must ensure that the RSI used in training is computed from data up to today, not including tomorrow's prices.\n",
    "\n",
    "Feature stores handle this by storing each feature value with a timestamp (event time) and an entity key (e.g., stock symbol). When creating a training dataset, we specify a set of timestamps (e.g., each trading day) and join the feature values that were valid at those times. This prevents look‑ahead bias.\n",
    "\n",
    "**Example:** Suppose we have a feature `RSI_14` computed daily. For a training example with timestamp `2024-01-01`, we need the RSI value computed using data up to `2024-01-01` (i.e., the RSI for that day, which uses the last 14 days including `2024-01-01`). If we mistakenly used the RSI from `2024-01-02`, we would be leaking future information.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.4 Feast: A Popular Open‑Source Feature Store\n",
    "\n",
    "**Feast** (Feature Store) is an open‑source feature store that works with offline and online stores. It provides:\n",
    "\n",
    "- A Python SDK to define features and feature views.\n",
    "- A registry to store metadata.\n",
    "- Retrieval APIs for training and serving.\n",
    "\n",
    "Feast is designed to be cloud‑agnostic and integrates with various data sources and storage backends.\n",
    "\n",
    "### 63.4.1 Installation\n",
    "\n",
    "```bash\n",
    "pip install feast\n",
    "```\n",
    "\n",
    "### 63.4.2 Defining Features for NEPSE\n",
    "\n",
    "We start by creating a Feast feature repository. This is a directory with configuration and feature definitions.\n",
    "\n",
    "```bash\n",
    "feast init nepse_feast\n",
    "cd nepse_feast\n",
    "```\n",
    "\n",
    "The repository contains `feature_store.yaml` and a `features` directory. We'll define our features in `features.py`.\n",
    "\n",
    "**Example `features.py`:**\n",
    "\n",
    "```python\n",
    "from datetime import timedelta\n",
    "from feast import Entity, FeatureView, Field, FileSource\n",
    "from feast.types import Float32, Int64, String\n",
    "from feast.value_type import ValueType\n",
    "\n",
    "# Define a data source (parquet files on disk or in cloud storage)\n",
    "# For NEPSE, we could store daily feature values in Parquet.\n",
    "nepse_source = FileSource(\n",
    "    path=\"data/nepse_features.parquet\",  # path to historical features\n",
    "    event_timestamp_column=\"event_timestamp\",\n",
    "    created_timestamp_column=\"created_timestamp\",\n",
    ")\n",
    "\n",
    "# Define an entity (the thing we are modelling)\n",
    "stock = Entity(\n",
    "    name=\"stock\",\n",
    "    value_type=ValueType.STRING,\n",
    "    description=\"Stock symbol\",\n",
    "    join_keys=[\"stock\"],\n",
    ")\n",
    "\n",
    "# Define feature views (groupings of features)\n",
    "stock_features = FeatureView(\n",
    "    name=\"stock_daily_features\",\n",
    "    entities=[stock],\n",
    "    ttl=timedelta(days=365),  # how long features are retained\n",
    "    schema=[\n",
    "        Field(name=\"open\", dtype=Float32),\n",
    "        Field(name=\"high\", dtype=Float32),\n",
    "        Field(name=\"low\", dtype=Float32),\n",
    "        Field(name=\"close\", dtype=Float32),\n",
    "        Field(name=\"volume\", dtype=Int64),\n",
    "        Field(name=\"sma_20\", dtype=Float32),\n",
    "        Field(name=\"rsi_14\", dtype=Float32),\n",
    "        Field(name=\"volume_ratio\", dtype=Float32),\n",
    "    ],\n",
    "    source=nepse_source,\n",
    ")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We define a data source (a Parquet file) containing historical features. The `Entity` represents the stock symbol. The `FeatureView` groups features that share the same source and entity. Feast will use this definition to serve features.\n",
    "\n",
    "### 63.4.3 Ingesting Historical Features\n",
    "\n",
    "We need to populate the Parquet file with historical features. This can be done by a separate batch pipeline (e.g., a Spark job or a Python script). The Parquet file should have columns: `stock`, `event_timestamp`, and all feature columns.\n",
    "\n",
    "**Example Python script to create Parquet:**\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# Assume we have a DataFrame with features for each stock and day\n",
    "df = pd.read_csv('nepse_features.csv')\n",
    "df['event_timestamp'] = pd.to_datetime(df['date'])\n",
    "df['created_timestamp'] = datetime.now()\n",
    "df = df.rename(columns={'symbol': 'stock'})\n",
    "# Select only needed columns\n",
    "feature_columns = ['stock', 'event_timestamp', 'created_timestamp',\n",
    "                   'open', 'high', 'low', 'close', 'volume',\n",
    "                   'sma_20', 'rsi_14', 'volume_ratio']\n",
    "df[feature_columns].to_parquet('data/nepse_features.parquet')\n",
    "```\n",
    "\n",
    "### 63.4.4 Applying Feature Definitions\n",
    "\n",
    "After defining the features, we apply them to the Feast registry.\n",
    "\n",
    "```bash\n",
    "feast apply\n",
    "```\n",
    "\n",
    "This registers the entities and feature views in the registry (by default, a local SQLite database). It also sets up the online store (if configured).\n",
    "\n",
    "### 63.4.5 Retrieving Features for Training\n",
    "\n",
    "To create a training dataset, we need a list of entity rows with timestamps. Feast will join the features that were valid at those timestamps.\n",
    "\n",
    "```python\n",
    "from feast import FeatureStore\n",
    "import pandas as pd\n",
    "\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "\n",
    "# Create an entity dataframe: for each stock and day we want features\n",
    "entity_df = pd.DataFrame({\n",
    "    \"stock\": [\"NABIL\", \"NABIL\", \"NTC\", \"NTC\"],\n",
    "    \"event_timestamp\": pd.to_datetime([\"2024-01-01\", \"2024-01-02\", \"2024-01-01\", \"2024-01-02\"])\n",
    "})\n",
    "\n",
    "# Retrieve features\n",
    "training_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"stock_daily_features:open\",\n",
    "        \"stock_daily_features:high\",\n",
    "        \"stock_daily_features:low\",\n",
    "        \"stock_daily_features:close\",\n",
    "        \"stock_daily_features:volume\",\n",
    "        \"stock_daily_features:sma_20\",\n",
    "        \"stock_daily_features:rsi_14\",\n",
    "        \"stock_daily_features:volume_ratio\",\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "print(training_df.head())\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Feast performs a point‑in‑time join: for each row in `entity_df`, it retrieves the feature values that were valid at that timestamp (i.e., the most recent feature value with `event_timestamp <= entity timestamp`, respecting the TTL). This ensures no look‑ahead.\n",
    "\n",
    "### 63.4.6 Online Serving\n",
    "\n",
    "For real‑time inference, we need to push the latest feature values to the online store. Feast supports writing features to an online store (e.g., Redis) via the `materialize` command.\n",
    "\n",
    "First, configure an online store in `feature_store.yaml`. For example, using Redis:\n",
    "\n",
    "```yaml\n",
    "project: nepse\n",
    "provider: local\n",
    "online_store:\n",
    "  type: redis\n",
    "  connection_string: \"localhost:6379\"\n",
    "offline_store:\n",
    "  type: file\n",
    "registry: data/registry.db\n",
    "```\n",
    "\n",
    "Then materialize features from the offline store to the online store:\n",
    "\n",
    "```bash\n",
    "feast materialize 2024-01-01T00:00:00 2024-12-31T00:00:00\n",
    "```\n",
    "\n",
    "This loads all feature values from the offline store within the time range into the online store.\n",
    "\n",
    "Now, during inference, we can retrieve the latest features for a stock:\n",
    "\n",
    "```python\n",
    "features = store.get_online_features(\n",
    "    features=[\n",
    "        \"stock_daily_features:open\",\n",
    "        \"stock_daily_features:high\",\n",
    "        \"stock_daily_features:low\",\n",
    "        \"stock_daily_features:close\",\n",
    "        \"stock_daily_features:volume\",\n",
    "        \"stock_daily_features:sma_20\",\n",
    "        \"stock_daily_features:rsi_14\",\n",
    "        \"stock_daily_features:volume_ratio\",\n",
    "    ],\n",
    "    entity_rows=[{\"stock\": \"NABIL\"}]\n",
    ").to_dict()\n",
    "\n",
    "print(features)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "`get_online_features` fetches the latest feature values for the given entity from the online store (Redis). This is a low‑latency operation suitable for real‑time prediction APIs.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.5 Feature Versioning and Lineage\n",
    "\n",
    "As features evolve (e.g., we change the definition of `rsi_14`), we need to version them. Feast supports versioning through the registry: each feature view has a name and can be updated. However, changing a feature view definition does not retroactively change historical data; it creates a new version. Feast does not automatically handle time‑travel of feature definitions—you must manage this by using different feature view names or by storing the definition alongside the data.\n",
    "\n",
    "For lineage, Feast records metadata: which source each feature comes from, when it was created, etc. This can be exported for auditing.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.6 Integrating with Model Training\n",
    "\n",
    "We can now use the feature store in our NEPSE training script:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from feast import FeatureStore\n",
    "import xgboost as xgb\n",
    "\n",
    "# 1. Get training data\n",
    "store = FeatureStore(repo_path=\".\")\n",
    "entity_df = pd.DataFrame({\n",
    "    \"stock\": [\"NABIL\"] * 500,  # example: last 500 days for NABIL\n",
    "    \"event_timestamp\": pd.date_range(end=\"2024-01-01\", periods=500)\n",
    "})\n",
    "train_df = store.get_historical_features(\n",
    "    entity_df=entity_df,\n",
    "    features=[\n",
    "        \"stock_daily_features:open\",\n",
    "        \"stock_daily_features:high\",\n",
    "        \"stock_daily_features:low\",\n",
    "        \"stock_daily_features:close\",\n",
    "        \"stock_daily_features:volume\",\n",
    "        \"stock_daily_features:sma_20\",\n",
    "        \"stock_daily_features:rsi_14\",\n",
    "        \"stock_daily_features:volume_ratio\",\n",
    "    ]\n",
    ").to_df()\n",
    "\n",
    "# Assume we have a target column (next day return) pre‑joined or in a separate table\n",
    "# For simplicity, we merge with target here\n",
    "target_df = pd.read_csv('nepse_targets.csv')  # columns: stock, date, target\n",
    "train_df = train_df.merge(target_df, left_on=['stock', 'event_timestamp'], right_on=['stock', 'date'])\n",
    "\n",
    "X = train_df[['open', 'high', 'low', 'close', 'volume', 'sma_20', 'rsi_14', 'volume_ratio']]\n",
    "y = train_df['target']\n",
    "\n",
    "# Train model\n",
    "model = xgb.XGBClassifier()\n",
    "model.fit(X, y)\n",
    "\n",
    "# Save model (as before)\n",
    "```\n",
    "\n",
    "For inference in a real‑time API, we would fetch online features and feed them to the model.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.7 Feast in Production\n",
    "\n",
    "For production use, consider:\n",
    "\n",
    "- **Scaling the online store**: Use managed Redis or DynamoDB.\n",
    "- **Streaming updates**: Use a stream processor (e.g., Flink) to compute features and push them to the online store via Feast's `write_to_online_store` method.\n",
    "- **Monitoring**: Track feature retrieval latency and error rates.\n",
    "- **Access control**: Use Feast's support for different projects to separate teams.\n",
    "\n",
    "Feast also integrates with Kubernetes via the Feast Serving component, which provides a gRPC API for low‑latency feature retrieval.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.8 Alternative Feature Stores\n",
    "\n",
    "While Feast is popular and open‑source, other feature stores exist:\n",
    "\n",
    "- **Tecton**: Commercial platform built on Feast, with additional features like automatic feature engineering and monitoring.\n",
    "- **Hopsworks**: Open‑source feature store with integrated ML platform.\n",
    "- **AWS SageMaker Feature Store**: Managed feature store integrated with SageMaker.\n",
    "- **Vertex AI Feature Store**: Google Cloud's managed feature store.\n",
    "\n",
    "The choice depends on your infrastructure and requirements. For the NEPSE system, starting with Feast is a good way to learn the concepts without vendor lock‑in.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.9 Best Practices for Feature Stores\n",
    "\n",
    "1. **Define features early**: Involve data scientists in defining the feature schema.\n",
    "2. **Use consistent naming**: Adopt a naming convention (e.g., `sma_20` for 20‑day simple moving average).\n",
    "3. **Version features carefully**: When changing a feature, consider creating a new feature view rather than modifying an existing one.\n",
    "4. **Monitor feature freshness**: Ensure that the online store is updated promptly; stale features can hurt model performance.\n",
    "5. **Backfill historical data**: Before using a feature store, backfill all historical features to enable training on past data.\n",
    "6. **Secure sensitive features**: If some features are sensitive (e.g., proprietary signals), implement access controls.\n",
    "7. **Test feature retrieval**: Write tests to ensure that features are correctly retrieved for both training and serving.\n",
    "\n",
    "---\n",
    "\n",
    "## 63.10 Implementing a Simple Custom Feature Store\n",
    "\n",
    "If Feast seems too heavy for your initial NEPSE project, you can build a simple custom feature store using a database and a caching layer. For example:\n",
    "\n",
    "- Store historical features in a PostgreSQL table with columns `(stock, date, feature_name, value)`.\n",
    "- For training, query all features for a date range and pivot to wide format.\n",
    "- For online, use Redis with keys like `feature:stock:NABIL` storing a JSON of latest features.\n",
    "\n",
    "However, you'll need to implement point‑in‑time joins and ensure consistency yourself. Feast handles these complexities, making it worth the initial setup.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we introduced the concept of a feature store and its role in MLOps. We covered:\n",
    "\n",
    "- The core components: offline store, online store, feature registry, and serving API.\n",
    "- The importance of point‑in‑time correctness to avoid data leakage.\n",
    "- How to implement a feature store using Feast, with a concrete example for the NEPSE prediction system.\n",
    "- Defining entities, feature views, and sources.\n",
    "- Retrieving features for training (historical) and serving (online).\n",
    "- Feature versioning, lineage, and integration with model training.\n",
    "- Production considerations and best practices.\n",
    "\n",
    "By adopting a feature store, the NEPSE system gains consistency, reusability, and reduced training‑serving skew. Features become a first‑class asset, shared across models and teams. In the next chapter, we will discuss **Experiment Tracking**, another key MLOps practice that helps manage the model development lifecycle.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 63**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
