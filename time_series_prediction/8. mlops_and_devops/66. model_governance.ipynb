{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 66: Model Governance\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the concept of model governance and its importance in regulated industries\n",
    "- Identify the key components of a model governance framework: documentation, approval, risk management, and audit\n",
    "- Create comprehensive model documentation, including model cards and data sheets\n",
    "- Implement approval workflows for model development and deployment\n",
    "- Assess and mitigate risks associated with machine learning models\n",
    "- Establish audit trails to track model changes and decisions\n",
    "- Navigate regulatory compliance requirements (GDPR, financial regulations) relevant to the NEPSE system\n",
    "- Incorporate ethics and fairness checks into the model lifecycle\n",
    "- Adopt best practices for responsible AI governance\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "As the NEPSE prediction system matures and potentially influences real trading decisions, it becomes subject to a range of governance requirements. Model governance is the system of policies, processes, and controls that ensure machine learning models are developed, deployed, and maintained in a responsible, transparent, and compliant manner. It encompasses everything from documentation and approval workflows to risk management and audit trails.\n",
    "\n",
    "In the financial industry, model governance is not optional—it is mandated by regulators. For example, the Federal Reserve's SR 11-7 (in the US) and similar regulations globally require banks to have robust model risk management frameworks. Even if the NEPSE system is for educational or personal use, adopting governance practices is a mark of professionalism and prepares you for real‑world deployment.\n",
    "\n",
    "In this chapter, we will explore the pillars of model governance. We will create model cards for our NEPSE models, establish approval workflows, consider risk and fairness, and set up audit trails. By the end, you will have a blueprint for governing ML models responsibly.\n",
    "\n",
    "---\n",
    "\n",
    "## 66.1 Governance Framework\n",
    "\n",
    "A model governance framework typically includes the following components:\n",
    "\n",
    "- **Policies and Standards**: High‑level principles and specific requirements that all models must meet.\n",
    "- **Roles and Responsibilities**: Clear assignment of who is responsible for development, validation, approval, and monitoring.\n",
    "- **Model Lifecycle Processes**: Defined stages from development to retirement, with gates at each stage.\n",
    "- **Documentation Requirements**: What must be documented at each stage.\n",
    "- **Risk Categorisation**: Models are categorised by risk level (e.g., low, medium, high) with corresponding oversight.\n",
    "- **Validation and Testing**: Independent validation of models before deployment.\n",
    "- **Monitoring and Reporting**: Ongoing performance monitoring and periodic reviews.\n",
    "- **Audit and Compliance**: Ensuring adherence to policies and regulatory requirements.\n",
    "\n",
    "For the NEPSE system, a simplified framework might include:\n",
    "\n",
    "- **Roles**: Data Scientist (develops), ML Engineer (deploys), Risk Owner (approves), Model Validator (independent review).\n",
    "- **Stages**: Development → Validation → Approval → Deployment → Monitoring → Retirement.\n",
    "- **Risk Level**: Initially, treat the system as medium risk (since it could inform trading decisions).\n",
    "\n",
    "---\n",
    "\n",
    "## 66.2 Model Documentation\n",
    "\n",
    "Comprehensive documentation is the foundation of governance. It should capture all aspects of a model's development and intended use.\n",
    "\n",
    "### 66.2.1 What to Document\n",
    "\n",
    "- **Business Objective**: What problem does the model solve? (e.g., predict next‑day price direction for NEPSE stocks)\n",
    "- **Data Sources**: Description of data, including provenance, time period, and any preprocessing.\n",
    "- **Feature Engineering**: List of features and how they are computed.\n",
    "- **Model Architecture**: Type of model (e.g., XGBoost, LSTM), hyperparameters, training algorithm.\n",
    "- **Training and Validation**: How data was split, performance metrics (accuracy, precision, recall, etc.).\n",
    "- **Limitations**: Known weaknesses, such as poor performance during high‑volatility periods.\n",
    "- **Intended Use**: Who should use the model and for what purpose? Who should not?\n",
    "- **Ethical Considerations**: Potential biases, fairness checks.\n",
    "- **Version History**: Changes over time.\n",
    "\n",
    "### 66.2.2 Example Documentation Outline for NEPSE Model\n",
    "\n",
    "```\n",
    "Model Name: NEPSE Daily Direction Predictor v1.2\n",
    "Date: 2025-03-15\n",
    "Author: Data Science Team\n",
    "\n",
    "1. Business Objective\n",
    "   Predict whether the NEPSE index will close higher than the previous day.\n",
    "\n",
    "2. Data Sources\n",
    "   - NEPSE daily OHLCV data from 2015-01-01 to 2024-12-31 (CSV files).\n",
    "   - Manually curated list of corporate actions (dividends, splits) from annual reports.\n",
    "\n",
    "3. Feature Engineering\n",
    "   - Lagged returns (1,2,3,5 days)\n",
    "   - 20-day simple moving average\n",
    "   - 14-day RSI\n",
    "   - Volume ratio (current volume / 20-day average volume)\n",
    "   - Day-of-week indicator (one-hot encoded)\n",
    "   - Fiscal quarter indicator (Nepal-specific)\n",
    "\n",
    "4. Model Architecture\n",
    "   - Algorithm: XGBoost Classifier\n",
    "   - Hyperparameters: n_estimators=200, max_depth=6, learning_rate=0.05, subsample=0.8\n",
    "   - Training objective: binary logistic loss\n",
    "\n",
    "5. Training and Validation\n",
    "   - Training period: 2015-01-01 to 2023-12-31\n",
    "   - Validation period: 2024-01-01 to 2024-06-30\n",
    "   - Test period: 2024-07-01 to 2024-12-31\n",
    "   - Performance (test): Accuracy = 0.62, Precision = 0.64, Recall = 0.59, F1 = 0.61\n",
    "\n",
    "6. Limitations\n",
    "   - Performance drops during election periods (accuracy ~0.52).\n",
    "   - Does not incorporate news sentiment or macroeconomic data.\n",
    "   - Trained only on daily data; not suitable for intraday predictions.\n",
    "\n",
    "7. Intended Use\n",
    "   - Generate trading signals for a quantitative strategy with appropriate risk management.\n",
    "   - Not intended for use as the sole basis for investment decisions.\n",
    "\n",
    "8. Ethical Considerations\n",
    "   - Model was tested for performance across different market cap segments; no significant bias found.\n",
    "   - No personal data used.\n",
    "\n",
    "9. Version History\n",
    "   v1.0: Initial release (2024-01-15)\n",
    "   v1.1: Added fiscal quarter features (2024-06-20)\n",
    "   v1.2: Retrained with additional data through 2024 (2025-03-15)\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 66.3 Model Cards\n",
    "\n",
    "**Model cards** are a structured, transparent format for reporting model information. Introduced by researchers at Google, they provide a standardised way to communicate model details, intended use, and evaluation results. Model cards are particularly useful for sharing models with stakeholders or the public.\n",
    "\n",
    "### 66.3.1 Structure of a Model Card\n",
    "\n",
    "A typical model card includes:\n",
    "\n",
    "- **Model Details**: Name, version, type, date, authors.\n",
    "- **Intended Use**: Primary use cases, out‑of‑scope uses.\n",
    "- **Factors**: Demographic or other groups considered in evaluation.\n",
    "- **Metrics**: Performance measures and how they were computed.\n",
    "- **Evaluation Data**: Description of datasets used for evaluation.\n",
    "- **Training Data**: Description of training data.\n",
    "- **Quantitative Analyses**: Performance breakdowns by segments.\n",
    "- **Ethical Considerations**: Potential biases, risks.\n",
    "- **Caveats and Recommendations**: Known limitations, usage advice.\n",
    "\n",
    "### 66.3.2 Generating a Model Card Programmatically\n",
    "\n",
    "We can create a model card as a Markdown file or JSON using a template. Here's a Python example that generates a model card from MLflow run data.\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def generate_model_card(run_id, model_name, model_version):\n",
    "    # Fetch run data from MLflow (simplified)\n",
    "    import mlflow\n",
    "    client = mlflow.tracking.MlflowClient()\n",
    "    run = client.get_run(run_id)\n",
    "    \n",
    "    # Extract parameters and metrics\n",
    "    params = run.data.params\n",
    "    metrics = run.data.metrics\n",
    "    \n",
    "    # Build model card\n",
    "    card = {\n",
    "        \"model_details\": {\n",
    "            \"name\": model_name,\n",
    "            \"version\": model_version,\n",
    "            \"date\": datetime.now().isoformat(),\n",
    "            \"type\": params.get(\"model_type\", \"XGBoost\"),\n",
    "            \"authors\": [\"Data Science Team\"],\n",
    "        },\n",
    "        \"intended_use\": {\n",
    "            \"primary_uses\": [\"Predict next-day direction of NEPSE index\"],\n",
    "            \"out_of_scope\": [\"Intraday trading\", \"Individual stock prediction\"],\n",
    "        },\n",
    "        \"factors\": {\n",
    "            \"relevant_factors\": [\"Market conditions\", \"Day of week\"],\n",
    "            \"evaluation_groups\": [\"Bull market period\", \"Bear market period\", \"Election period\"],\n",
    "        },\n",
    "        \"metrics\": {\n",
    "            \"accuracy\": metrics.get(\"test_accuracy\"),\n",
    "            \"precision\": metrics.get(\"test_precision\"),\n",
    "            \"recall\": metrics.get(\"test_recall\"),\n",
    "            \"f1\": metrics.get(\"test_f1\"),\n",
    "        },\n",
    "        \"evaluation_data\": {\n",
    "            \"description\": \"NEPSE daily data from 2024-07-01 to 2024-12-31\",\n",
    "            \"size\": int(params.get(\"test_size\", 0)),\n",
    "        },\n",
    "        \"training_data\": {\n",
    "            \"description\": \"NEPSE daily data from 2015-01-01 to 2024-06-30\",\n",
    "            \"size\": int(params.get(\"train_size\", 0)),\n",
    "        },\n",
    "        \"quantitative_analyses\": {\n",
    "            \"accuracy_by_period\": {\n",
    "                \"bull\": 0.65,\n",
    "                \"bear\": 0.58,\n",
    "                \"election\": 0.52,\n",
    "            }\n",
    "        },\n",
    "        \"ethical_considerations\": {\n",
    "            \"bias_assessment\": \"Model performance was consistent across stock sectors.\",\n",
    "            \"data_privacy\": \"No personal data used.\",\n",
    "        },\n",
    "        \"caveats\": [\n",
    "            \"Performance degrades during periods of political instability.\",\n",
    "            \"Model does not account for news sentiment.\",\n",
    "        ],\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(f\"model_card_{model_name}_v{model_version}.json\", \"w\") as f:\n",
    "        json.dump(card, f, indent=2)\n",
    "    \n",
    "    return card\n",
    "\n",
    "# Example usage\n",
    "generate_model_card(\"abc123\", \"NEPSE_Predictor\", 5)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This function pulls information from an MLflow run and combines it with manually entered details to produce a JSON model card. The card can be stored alongside the model in the registry or shared with stakeholders.\n",
    "\n",
    "---\n",
    "\n",
    "## 66.4 Data Sheets\n",
    "\n",
    "Similar to model cards, **data sheets** document the datasets used for training and evaluation. They promote transparency about data provenance, collection methods, and potential biases.\n",
    "\n",
    "### 66.4.1 Content of a Data Sheet\n",
    "\n",
    "- **Dataset Description**: Name, source, intended use.\n",
    "- **Collection Method**: How data was gathered (e.g., API, manual entry).\n",
    "- **Preprocessing**: Cleaning steps, feature engineering.\n",
    "- **Distribution**: Time period, number of samples, feature types.\n",
    "- **Known Issues**: Missing data, errors, biases.\n",
    "- **Recommended Splits**: Training, validation, test.\n",
    "\n",
    "### 66.4.2 Example Data Sheet for NEPSE Data\n",
    "\n",
    "```yaml\n",
    "dataset_name: NEPSE Daily OHLCV\n",
    "version: 2024.1\n",
    "source: Nepal Stock Exchange (public CSV files)\n",
    "collection_method: Downloaded daily from exchange website; scripted download from 2015.\n",
    "time_period: 2015-01-01 to 2024-12-31\n",
    "number_of_samples: ~2500 trading days\n",
    "features:\n",
    "  - Date: trading date\n",
    "  - Open: opening price (float)\n",
    "  - High: daily high (float)\n",
    "  - Low: daily low (float)\n",
    "  - Close: closing price (float)\n",
    "  - Volume: number of shares traded (integer)\n",
    "  - Prev_Close: previous day's close (float)\n",
    "preprocessing:\n",
    "  - Removed rows with zero volume (non‑trading days)\n",
    "  - Forward‑filled missing prices (rare)\n",
    "  - Added derived features (returns, moving averages) separately\n",
    "known_issues:\n",
    "  - Data from 2020-03 to 2020-05 may have increased volatility due to COVID‑19.\n",
    "  - Some older data may have inconsistent formatting (corrected in preprocessing).\n",
    "recommended_splits:\n",
    "  - Train: 2015-01-01 to 2023-12-31\n",
    "  - Validation: 2024-01-01 to 2024-06-30\n",
    "  - Test: 2024-07-01 to 2024-12-31\n",
    "```\n",
    "\n",
    "Data sheets can be stored as YAML or JSON alongside the dataset.\n",
    "\n",
    "---\n",
    "\n",
    "## 66.5 Approval Processes\n",
    "\n",
    "A governance framework requires formal approval at key stages: before deployment, after major changes, and periodically. Approval workflows can be implemented using tools like Jira, ServiceNow, or even Git pull requests with checklists.\n",
    "\n",
    "### 66.5.1 Example Approval Workflow\n",
    "\n",
    "1. **Model Development**: Data scientist creates a model and documents it (model card, data sheet).\n",
    "2. **Model Validation**: An independent validator (or a peer) reviews the model, tests it on holdout data, and checks for compliance. They produce a validation report.\n",
    "3. **Risk Assessment**: The model owner assesses the risk level and proposes mitigations.\n",
    "4. **Approval Committee**: A committee (including risk, compliance, and business representatives) reviews the documentation and validation report. They either approve, request changes, or reject.\n",
    "5. **Deployment**: Upon approval, the model is deployed to production.\n",
    "6. **Post‑Deployment Review**: After a period (e.g., 3 months), a review is conducted to ensure the model is performing as expected.\n",
    "\n",
    "### 66.5.2 Automating Approval with Git and CI/CD\n",
    "\n",
    "We can encode some approval steps in our CI/CD pipeline. For example, a pull request that adds a new model could trigger automated tests and require a review from a designated approver before merging.\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/model_approval.yml\n",
    "name: Model Approval\n",
    "\n",
    "on:\n",
    "  pull_request:\n",
    "    paths:\n",
    "      - 'models/**'\n",
    "      - 'model_cards/**'\n",
    "\n",
    "jobs:\n",
    "  validate:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v2\n",
    "      - name: Run model tests\n",
    "        run: pytest tests/test_model.py\n",
    "      - name: Check model card completeness\n",
    "        run: python scripts/check_model_card.py\n",
    "      - name: Require approval\n",
    "        uses: hmarr/auto-approve-action@v2\n",
    "        if: github.actor == 'approver-team'\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This workflow runs validation tests on any PR that changes models or model cards. It also uses an action that requires a specific team to approve the PR. This ensures that no model is merged without proper review.\n",
    "\n",
    "---\n",
    "\n",
    "## 66.6 Risk Assessment\n",
    "\n",
    "Model risk is the potential for adverse consequences from decisions based on model outputs. For the NEPSE system, risks include:\n",
    "\n",
    "- **Financial loss** if the model's predictions lead to bad trades.\n",
    "- **Reputational damage** if the model is publicly perceived as flawed.\n",
    "- **Regulatory non‑compliance** if the model is used in a regulated activity without proper governance.\n",
    "\n",
    "Risk assessment involves:\n",
    "\n",
    "1. **Identifying risks**: What could go wrong?\n",
    "2. **Assessing likelihood and impact**: How likely is each risk? How severe?\n",
    "3. **Mitigating risks**: What controls can reduce likelihood or impact?\n",
    "4. **Monitoring**: How will we detect if a risk materialises?\n",
    "\n",
    "### 66.6.1 Example Risk Assessment for NEPSE Model\n",
    "\n",
    "| Risk | Likelihood | Impact | Mitigation |\n",
    "|------|------------|--------|------------|\n",
    "| Model predicts incorrectly during high volatility | Medium | High (financial loss) | Implement stop‑loss; use ensemble models; monitor volatility and disable predictions during extreme periods. |\n",
    "| Data feed error (missing prices) | Low | Medium | Use data validation checks; fallback to last known price. |\n",
    "| Model overfits to historical data | Medium | Medium | Use walk‑forward validation; monitor performance decay; retrain regularly. |\n",
    "| Regulatory scrutiny | Low | High | Maintain full documentation; ensure explainability. |\n",
    "\n",
    "This risk assessment should be documented and reviewed periodically.\n",
    "\n",
    "---\n",
    "\n",
    "## 66.7 Audit Trails\n",
    "\n",
    "An audit trail is a chronological record of all activities related to a model: changes to code, data, parameters, approvals, and deployments. This is essential for accountability and for responding to audits.\n",
    "\n",
    "### 66.7.1 What to Log\n",
    "\n",
    "- Model version changes (who, when, why)\n",
    "- Data version changes\n",
    "- Hyperparameter changes\n",
    "- Approval decisions (who approved, when, based on what)\n",
    "- Deployment events\n",
    "- Monitoring alerts and responses\n",
    "\n",
    "### 66.7.2 Implementing Audit Trails\n",
    "\n",
    "We can leverage existing tools:\n",
    "\n",
    "- **Git** for code and configuration changes.\n",
    "- **DVC** for data version changes.\n",
    "- **MLflow** for experiment and model registry changes.\n",
    "- **Airflow** for pipeline run logs.\n",
    "- **Cloud logging** (e.g., AWS CloudTrail) for infrastructure changes.\n",
    "\n",
    "For a unified audit log, we can write a simple logging function that records events to a database or a file.\n",
    "\n",
    "```python\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "def audit_log(event_type, user, details):\n",
    "    log_entry = {\n",
    "        \"timestamp\": datetime.now().isoformat(),\n",
    "        \"event_type\": event_type,\n",
    "        \"user\": user,\n",
    "        \"details\": details\n",
    "    }\n",
    "    # Append to a file (or send to a database)\n",
    "    with open(\"audit.log\", \"a\") as f:\n",
    "        f.write(json.dumps(log_entry) + \"\\n\")\n",
    "\n",
    "# Example\n",
    "audit_log(\"model_approval\", \"alice\", {\"model_name\": \"NEPSE_Predictor\", \"version\": 5, \"decision\": \"approved\"})\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This simple function writes structured logs. In production, you would use a more robust system like ELK stack or a dedicated audit database.\n",
    "\n",
    "---\n",
    "\n",
    "## 66.8 Compliance\n",
    "\n",
    "Depending on the jurisdiction and application, various regulations may apply.\n",
    "\n",
    "### 66.8.1 GDPR (General Data Protection Regulation)\n",
    "\n",
    "If the NEPSE system ever processes personal data (e.g., trader IDs), GDPR applies. Key requirements:\n",
    "\n",
    "- **Right to explanation**: Individuals have the right to an explanation of decisions made by automated systems. This requires interpretability.\n",
    "- **Right to erasure**: Individuals can request deletion of their data. Models may need to be retrained without that data.\n",
    "- **Data protection by design**: Privacy must be embedded into the system.\n",
    "\n",
    "For the NEPSE system, if we only use market data, GDPR may not apply. However, if we expand to include user‑specific data, we must comply.\n",
    "\n",
    "### 66.8.2 Financial Regulations\n",
    "\n",
    "If the model is used for trading, it may fall under regulations like **MiFID II** (Europe) or **SEC rules** (US). These often require:\n",
    "\n",
    "- **Algorithm testing**: Proof that the algorithm has been tested and does not create disorderly trading conditions.\n",
    "- **Record keeping**: All algorithm changes and trading decisions must be logged.\n",
    "- **Disclosure**: Clients must be informed if algorithms are used.\n",
    "\n",
    "For a personal or educational project, these may not apply, but it's good to be aware.\n",
    "\n",
    "### 66.8.3 SOC2\n",
    "\n",
    "SOC2 is an auditing standard for service organisations. It covers security, availability, processing integrity, confidentiality, and privacy. Achieving SOC2 compliance demonstrates that you have appropriate controls. Relevant controls for ML include change management, access control, and monitoring.\n",
    "\n",
    "---\n",
    "\n",
    "## 66.9 Ethics and Fairness\n",
    "\n",
    "Ethical AI goes beyond compliance. It ensures that models do not perpetuate bias, discriminate against groups, or cause harm.\n",
    "\n",
    "### 66.9.1 Fairness in Financial Models\n",
    "\n",
    "For the NEPSE system, fairness might involve checking that the model performs equally well for stocks of different sectors, sizes, or liquidity. If it systematically underperforms for small‑cap stocks, that could be a bias.\n",
    "\n",
    "### 66.9.2 Fairness Metrics\n",
    "\n",
    "Common metrics include:\n",
    "\n",
    "- **Demographic parity**: The proportion of positive predictions should be similar across groups.\n",
    "- **Equal opportunity**: True positive rates should be similar across groups.\n",
    "- **Predictive parity**: Positive predictive value should be similar.\n",
    "\n",
    "We can compute these if we have group labels (e.g., sector).\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "def fairness_metrics(y_true, y_pred, groups):\n",
    "    results = {}\n",
    "    for group in groups.unique():\n",
    "        mask = groups == group\n",
    "        tn, fp, fn, tp = confusion_matrix(y_true[mask], y_pred[mask]).ravel()\n",
    "        tpr = tp / (tp + fn) if (tp+fn)>0 else 0\n",
    "        ppv = tp / (tp + fp) if (tp+fp)>0 else 0\n",
    "        results[group] = {\"TPR\": tpr, \"PPV\": ppv}\n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# Example\n",
    "groups = test_data['sector']  # sector labels\n",
    "fairness_df = fairness_metrics(y_test, y_pred, groups)\n",
    "print(fairness_df)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We compute true positive rate (TPR) and positive predictive value (PPV) for each sector. If they vary widely, the model may be unfair. This should be documented and, if necessary, mitigated (e.g., by reweighting or collecting more data for underperforming groups).\n",
    "\n",
    "### 66.9.3 Ethical Review\n",
    "\n",
    "An ethical review should consider:\n",
    "\n",
    "- **Purpose**: Is the model being used for a beneficial purpose?\n",
    "- **Transparency**: Are users aware they are interacting with an AI?\n",
    "- **Accountability**: Who is responsible if the model causes harm?\n",
    "- **Redress**: Is there a mechanism for users to challenge decisions?\n",
    "\n",
    "For the NEPSE system, if it is used for personal trading, these questions are less critical. But if deployed in a professional context, they must be addressed.\n",
    "\n",
    "---\n",
    "\n",
    "## 66.10 Best Practices\n",
    "\n",
    "Summarising the key best practices for model governance:\n",
    "\n",
    "1. **Document everything**: Use model cards and data sheets.\n",
    "2. **Establish clear roles**: Who develops, validates, approves, deploys.\n",
    "3. **Implement approval workflows**: Formal sign‑offs for high‑risk models.\n",
    "4. **Maintain audit trails**: Record all changes and decisions.\n",
    "5. **Assess risk regularly**: Update risk assessments as the model evolves.\n",
    "6. **Monitor for fairness and bias**: Check performance across groups.\n",
    "7. **Ensure compliance**: Understand and adhere to relevant regulations.\n",
    "8. **Plan for model retirement**: Define criteria for decommissioning.\n",
    "9. **Educate the team**: Ensure everyone understands governance responsibilities.\n",
    "10. **Automate where possible**: Use CI/CD pipelines to enforce governance gates.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored model governance, a critical discipline for responsible AI. We covered:\n",
    "\n",
    "- The components of a governance framework: policies, roles, processes.\n",
    "- Comprehensive model documentation and the use of model cards and data sheets.\n",
    "- Approval workflows and how to implement them with CI/CD.\n",
    "- Risk assessment to identify and mitigate potential harms.\n",
    "- Audit trails to track model changes and decisions.\n",
    "- Compliance with regulations like GDPR and financial rules.\n",
    "- Ethics and fairness, including metrics to detect bias.\n",
    "- Best practices for embedding governance into the ML lifecycle.\n",
    "\n",
    "For the NEPSE prediction system, adopting governance practices ensures that the model is developed and deployed responsibly, with transparency and accountability. Even for a personal project, these habits prepare you for professional environments where governance is mandatory.\n",
    "\n",
    "In the next chapter, we will discuss **Infrastructure as Code**, which helps manage the infrastructure supporting the NEPSE system in a declarative, version‑controlled way.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 66**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
