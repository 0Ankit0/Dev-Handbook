{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 64: Experiment Tracking\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand why experiment tracking is essential for reproducible machine learning\n",
    "- Identify the key components to track: parameters, metrics, artifacts, and environment\n",
    "- Compare popular experiment tracking tools (MLflow, Weights & Biases, Neptune, Comet)\n",
    "- Set up an experiment tracking server and integrate it with your training scripts\n",
    "- Log hyperparameters, metrics, and artifacts for each run\n",
    "- Organise experiments using tags, names, and hierarchical structures\n",
    "- Query and compare runs to select the best model\n",
    "- Integrate experiment tracking with a model registry for production handoff\n",
    "- Apply these practices to the NEPSE prediction system to manage model development\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In the previous chapters, we trained numerous models for the NEPSE prediction system: different algorithms, feature sets, and hyperparameters. Without a systematic way to record these experiments, it quickly becomes impossible to remember which combination produced the best results, let alone reproduce them. **Experiment tracking** solves this problem by providing a central place to log all relevant information about each training run.\n",
    "\n",
    "Experiment tracking is a cornerstone of MLOps. It enables data scientists to:\n",
    "\n",
    "- Keep a historical record of all experiments.\n",
    "- Compare runs to identify the best model.\n",
    "- Share results with colleagues.\n",
    "- Reproduce any past model exactly.\n",
    "- Automate model selection and deployment pipelines.\n",
    "\n",
    "In this chapter, we will explore experiment tracking in depth. We will set up **MLflow** (a popular open\u2011source tool) for the NEPSE project and demonstrate how to log parameters, metrics, and artifacts. We will also discuss other tools and best practices for managing experiments at scale.\n",
    "\n",
    "---\n",
    "\n",
    "## 64.1 Why Track Experiments?\n",
    "\n",
    "Imagine you are developing a model for NEPSE. You try different window sizes for moving averages, different lags, different classifiers, and various hyperparameters. After a week, you have a dozen notebooks, each with slightly different code and results. You find a model that performs well, but you cannot remember exactly which features it used or the random seed. This is the **reproducibility crisis** in machine learning.\n",
    "\n",
    "Experiment tracking addresses this by capturing:\n",
    "\n",
    "- **Code version**: Git commit hash or notebook snapshot.\n",
    "- **Environment**: Python packages and versions.\n",
    "- **Data**: Version or checksum of the dataset used.\n",
    "- **Parameters**: All hyperparameters and configuration options.\n",
    "- **Metrics**: Training and validation scores at each epoch or at the end.\n",
    "- **Artifacts**: Model files, plots, feature importance, etc.\n",
    "\n",
    "With this information, you can:\n",
    "\n",
    "- Recreate any past result.\n",
    "- Compare multiple runs side\u2011by\u2011side.\n",
    "- Share a link to a specific experiment with a colleague.\n",
    "- Automate the selection of the best model for deployment.\n",
    "\n",
    "For the NEPSE system, experiment tracking will save countless hours of manual note\u2011taking and enable confident model selection.\n",
    "\n",
    "---\n",
    "\n",
    "## 64.2 What to Track\n",
    "\n",
    "A comprehensive experiment log should include:\n",
    "\n",
    "### 64.2.1 Parameters\n",
    "\n",
    "All inputs that affect the model's behavior:\n",
    "\n",
    "- Model type (e.g., `random_forest`, `xgboost`).\n",
    "- Hyperparameters (e.g., `n_estimators`, `max_depth`, `learning_rate`).\n",
    "- Feature engineering choices (e.g., `window_sizes`, `include_rsi`).\n",
    "- Data split (e.g., `train_start_date`, `test_end_date`).\n",
    "\n",
    "### 64.2.2 Metrics\n",
    "\n",
    "Quantitative measures of model performance:\n",
    "\n",
    "- Accuracy, precision, recall, F1 (for classification).\n",
    "- Mean Absolute Error, Root Mean Squared Error (for regression).\n",
    "- Training time, inference latency.\n",
    "- Custom metrics like Sharpe ratio or profit from a trading simulation.\n",
    "\n",
    "### 64.2.3 Artifacts\n",
    "\n",
    "Files generated during the run:\n",
    "\n",
    "- Serialized model (e.g., `.pkl` file).\n",
    "- Feature importance plots.\n",
    "- Confusion matrix.\n",
    "- Prediction vs. actual plots.\n",
    "- Training and validation loss curves.\n",
    "\n",
    "### 64.2.4 Metadata\n",
    "\n",
    "Contextual information:\n",
    "\n",
    "- Git commit hash.\n",
    "- Experiment name and description.\n",
    "- Tags (e.g., `\"baseline\"`, `\"feature_set_v2\"`).\n",
    "- Start and end time.\n",
    "- Hostname or environment (e.g., `\"aws_g4dn.xlarge\"`).\n",
    "\n",
    "---\n",
    "\n",
    "## 64.3 Experiment Tracking Tools\n",
    "\n",
    "Several tools are available, ranging from simple home\u2011grown solutions to enterprise platforms.\n",
    "\n",
    "### 64.3.1 MLflow\n",
    "\n",
    "**MLflow** is an open\u2011source platform by Databricks. It provides four main components:\n",
    "\n",
    "- **Tracking**: Log parameters, metrics, and artifacts.\n",
    "- **Projects**: Package code for reproducible runs.\n",
    "- **Models**: Manage and deploy models.\n",
    "- **Model Registry**: Central model store with versioning and stage transitions.\n",
    "\n",
    "MLflow is lightweight, integrates with many libraries, and can be self\u2011hosted or used with a tracking server.\n",
    "\n",
    "### 64.3.2 Weights & Biases (W&B)\n",
    "\n",
    "**W&B** is a commercial platform with a generous free tier. It offers rich visualizations, collaboration features, and tight integration with deep learning frameworks. W&B is particularly popular in the research community.\n",
    "\n",
    "### 64.3.3 Neptune\n",
    "\n",
    "**Neptune** is another commercial platform with a strong focus on experiment tracking and model registry. It provides a clean UI and supports many integrations.\n",
    "\n",
    "### 64.3.4 Comet\n",
    "\n",
    "**Comet** offers experiment tracking, model monitoring, and a model registry. It has a free tier for individuals and academic users.\n",
    "\n",
    "### 64.3.5 Comparison\n",
    "\n",
    "| Tool      | Open Source | Self\u2011Hosted | Free Tier | UI Richness | Integration |\n",
    "|-----------|-------------|-------------|-----------|-------------|-------------|\n",
    "| MLflow    | Yes         | Yes         | Yes       | Basic       | Many        |\n",
    "| W&B       | No          | No          | Yes       | Excellent   | Excellent   |\n",
    "| Neptune   | No          | Yes (enterprise) | Yes (limited) | Good | Good |\n",
    "| Comet     | No          | No          | Yes (limited) | Good | Good |\n",
    "\n",
    "For the NEPSE project, we will use MLflow because it is open source, easy to set up, and covers our needs. You can later migrate to a commercial tool if required.\n",
    "\n",
    "---\n",
    "\n",
    "## 64.4 Setting Up MLflow\n",
    "\n",
    "MLflow can be used locally or with a tracking server. For a single user, local file storage is sufficient. For a team, you should set up a tracking server with a database backend.\n",
    "\n",
    "### 64.4.1 Installation\n",
    "\n",
    "```bash\n",
    "pip install mlflow\n",
    "```\n",
    "\n",
    "### 64.4.2 Local Usage\n",
    "\n",
    "By default, MLflow logs to a local `mlruns` directory. You can start the UI with:\n",
    "\n",
    "```bash\n",
    "mlflow ui\n",
    "```\n",
    "\n",
    "Then open `http://localhost:5000` to view experiments.\n",
    "\n",
    "### 64.4.3 Tracking Server (Optional)\n",
    "\n",
    "For team use, set up a tracking server with a PostgreSQL backend and an artifact store (e.g., S3). This is more involved; refer to the MLflow documentation.\n",
    "\n",
    "---\n",
    "\n",
    "## 64.5 Integrating MLflow with NEPSE Training\n",
    "\n",
    "Let's modify our training script to log everything to MLflow.\n",
    "\n",
    "### 64.5.1 Basic Logging\n",
    "\n",
    "```python\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pandas as pd\n",
    "\n",
    "# Load data (as before)\n",
    "df = pd.read_csv('nepse_features.csv')\n",
    "X = df.drop(columns=['target', 'date', 'symbol'])\n",
    "y = df['target']\n",
    "\n",
    "# Split (simplified, use time\u2011based split in practice)\n",
    "split_idx = int(0.8 * len(X))\n",
    "X_train, X_test = X[:split_idx], X[split_idx:]\n",
    "y_train, y_test = y[:split_idx], y[split_idx:]\n",
    "\n",
    "# Start an MLflow run\n",
    "with mlflow.start_run(run_name=\"random_forest_baseline\"):\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"model_type\", \"RandomForest\")\n",
    "    mlflow.log_param(\"n_estimators\", 100)\n",
    "    mlflow.log_param(\"max_depth\", 10)\n",
    "    mlflow.log_param(\"train_size\", len(X_train))\n",
    "    mlflow.log_param(\"test_size\", len(X_test))\n",
    "    \n",
    "    # Train model\n",
    "    model = RandomForestClassifier(n_estimators=100, max_depth=10, random_state=42)\n",
    "    model.fit(X_train, y_train)\n",
    "    \n",
    "    # Evaluate\n",
    "    train_pred = model.predict(X_train)\n",
    "    test_pred = model.predict(X_test)\n",
    "    train_acc = accuracy_score(y_train, train_pred)\n",
    "    test_acc = accuracy_score(y_test, test_pred)\n",
    "    \n",
    "    # Log metrics\n",
    "    mlflow.log_metric(\"train_accuracy\", train_acc)\n",
    "    mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "    \n",
    "    # Log model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "    \n",
    "    # (Optional) Log feature importance plot\n",
    "    import matplotlib.pyplot as plt\n",
    "    importances = model.feature_importances_\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.barh(X.columns, importances)\n",
    "    plt.xlabel(\"Importance\")\n",
    "    plt.title(\"Feature Importance\")\n",
    "    plt.savefig(\"feature_importance.png\")\n",
    "    mlflow.log_artifact(\"feature_importance.png\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We wrap the training code inside an MLflow run. `log_param` records key\u2011value pairs. `log_metric` records a numeric value (multiple values can be logged per metric over time). `log_model` saves the model in MLflow's format, which includes the environment. `log_artifact` saves any file to the artifact store.\n",
    "\n",
    "### 64.5.2 Logging Hyperparameters Dynamically\n",
    "\n",
    "Often we want to try many hyperparameter combinations. We can use a loop or integrate with a search tool.\n",
    "\n",
    "```python\n",
    "import itertools\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [5, 10, None]\n",
    "}\n",
    "\n",
    "for n_est, depth in itertools.product(param_grid['n_estimators'], param_grid['max_depth']):\n",
    "    with mlflow.start_run(run_name=f\"rf_n{n_est}_d{depth}\"):\n",
    "        mlflow.log_param(\"n_estimators\", n_est)\n",
    "        mlflow.log_param(\"max_depth\", depth)\n",
    "        # ... train and evaluate ...\n",
    "```\n",
    "\n",
    "### 64.5.3 Logging Metrics Per Epoch\n",
    "\n",
    "For iterative models (like neural networks), you can log metrics at each epoch.\n",
    "\n",
    "```python\n",
    "with mlflow.start_run():\n",
    "    for epoch in range(epochs):\n",
    "        loss = train_one_epoch()\n",
    "        val_acc = validate()\n",
    "        mlflow.log_metric(\"loss\", loss, step=epoch)\n",
    "        mlflow.log_metric(\"val_accuracy\", val_acc, step=epoch)\n",
    "```\n",
    "\n",
    "This creates a time series of metrics that can be plotted in the MLflow UI.\n",
    "\n",
    "---\n",
    "\n",
    "## 64.6 Organising Experiments\n",
    "\n",
    "As the number of runs grows, organisation becomes crucial. MLflow provides several mechanisms:\n",
    "\n",
    "### 64.6.1 Experiment Names\n",
    "\n",
    "You can create separate experiments for different projects or phases.\n",
    "\n",
    "```python\n",
    "mlflow.set_experiment(\"NEPSE_RandomForest\")\n",
    "```\n",
    "\n",
    "All runs under that experiment will be grouped together in the UI.\n",
    "\n",
    "### 64.6.2 Tags\n",
    "\n",
    "Add tags to runs for filtering and searching.\n",
    "\n",
    "```python\n",
    "mlflow.set_tag(\"data_version\", \"v2024_01\")\n",
    "mlflow.set_tag(\"feature_set\", \"basic_plus_technical\")\n",
    "mlflow.set_tag(\"model_family\", \"xgboost\")\n",
    "```\n",
    "\n",
    "### 64.6.3 Nested Runs\n",
    "\n",
    "For complex pipelines (e.g., hyperparameter tuning), you can create nested runs. The parent run represents the overall search, and children represent individual trials.\n",
    "\n",
    "```python\n",
    "with mlflow.start_run(run_name=\"HPO_RandomForest\") as parent_run:\n",
    "    for params in param_combinations:\n",
    "        with mlflow.start_run(run_name=f\"trial_{i}\", nested=True):\n",
    "            mlflow.log_params(params)\n",
    "            # ... train ...\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 64.7 Model Registry Integration\n",
    "\n",
    "MLflow includes a **Model Registry** to manage model versions and stages (Staging, Production, Archived). After a successful run, you can register the model.\n",
    "\n",
    "```python\n",
    "mlflow.register_model(f\"runs:/{run.info.run_id}/model\", \"NEPSE_Predictor\")\n",
    "```\n",
    "\n",
    "This adds the model to the registry. You can then transition it to \"Staging\" or \"Production\" via the UI or API.\n",
    "\n",
    "**Example: Promoting a model programmatically**\n",
    "\n",
    "```python\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "client.transition_model_version_stage(\n",
    "    name=\"NEPSE_Predictor\",\n",
    "    version=3,\n",
    "    stage=\"Production\"\n",
    ")\n",
    "```\n",
    "\n",
    "Now, your deployment pipeline can fetch the model in the \"Production\" stage.\n",
    "\n",
    "---\n",
    "\n",
    "## 64.8 Querying and Comparing Runs\n",
    "\n",
    "MLflow provides a Python API to query runs and compare them.\n",
    "\n",
    "```python\n",
    "from mlflow.tracking import MlflowClient\n",
    "\n",
    "client = MlflowClient()\n",
    "experiment = client.get_experiment_by_name(\"NEPSE_RandomForest\")\n",
    "runs = client.search_runs(experiment.experiment_id, order_by=[\"metrics.test_accuracy DESC\"])\n",
    "\n",
    "best_run = runs[0]\n",
    "print(f\"Best run ID: {best_run.info.run_id}\")\n",
    "print(f\"Test accuracy: {best_run.data.metrics['test_accuracy']}\")\n",
    "```\n",
    "\n",
    "You can also load the model from the best run:\n",
    "\n",
    "```python\n",
    "model_uri = f\"runs:/{best_run.info.run_id}/model\"\n",
    "model = mlflow.sklearn.load_model(model_uri)\n",
    "```\n",
    "\n",
    "This is useful for automated model selection pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 64.9 Best Practices for Experiment Tracking\n",
    "\n",
    "1. **Log everything**: Even small changes can affect results. Log all parameters, including data paths and random seeds.\n",
    "2. **Use consistent naming**: Adopt a convention for run names (e.g., `model_featureSet_timestamp`).\n",
    "3. **Tag runs with meaningful labels**: `baseline`, `feature_set_v2`, `production_candidate`.\n",
    "4. **Log code version**: Automatically capture the Git commit hash.\n",
    "   ```python\n",
    "   import subprocess\n",
    "   commit_hash = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode(\"ascii\").strip()\n",
    "   mlflow.log_param(\"git_commit\", commit_hash)\n",
    "   ```\n",
    "5. **Log environment**: Use `mlflow.log_artifact(\"requirements.txt\")` or log the output of `pip freeze`.\n",
    "6. **Log artifacts liberally**: Save plots, confusion matrices, and sample predictions for later inspection.\n",
    "7. **Use nested runs for structured workflows**: HPO, cross\u2011validation.\n",
    "8. **Regularly clean up old runs**: If using a local tracking server, archive or delete runs you no longer need.\n",
    "9. **Integrate with your CI/CD**: Automatically log runs from your training pipelines.\n",
    "\n",
    "---\n",
    "\n",
    "## 64.10 Complete Example: NEPSE with MLflow\n",
    "\n",
    "Let's put it all together in a script that trains an XGBoost model with different hyperparameters and logs everything.\n",
    "\n",
    "```python\n",
    "# train_nepse_with_mlflow.py\n",
    "import mlflow\n",
    "import mlflow.xgboost\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score\n",
    "import xgboost as xgb\n",
    "import argparse\n",
    "import subprocess\n",
    "import json\n",
    "\n",
    "def load_data():\n",
    "    df = pd.read_csv('nepse_features.csv')\n",
    "    # Assume we have a 'target' column (0/1) and a 'date' column for splitting\n",
    "    df['date'] = pd.to_datetime(df['date'])\n",
    "    df = df.sort_values('date')\n",
    "    split_date = '2024-01-01'\n",
    "    train = df[df['date'] < split_date]\n",
    "    test = df[df['date'] >= split_date]\n",
    "    \n",
    "    feature_cols = [c for c in df.columns if c not in ['target', 'date', 'symbol']]\n",
    "    X_train = train[feature_cols]\n",
    "    y_train = train['target']\n",
    "    X_test = test[feature_cols]\n",
    "    y_test = test['target']\n",
    "    return X_train, y_train, X_test, y_test, feature_cols\n",
    "\n",
    "def main():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--learning_rate', type=float, default=0.1)\n",
    "    parser.add_argument('--max_depth', type=int, default=6)\n",
    "    parser.add_argument('--n_estimators', type=int, default=100)\n",
    "    parser.add_argument('--subsample', type=float, default=0.8)\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    # Start MLflow run\n",
    "    mlflow.set_experiment(\"NEPSE_XGBoost\")\n",
    "    with mlflow.start_run():\n",
    "        # Log parameters\n",
    "        mlflow.log_params(vars(args))\n",
    "        \n",
    "        # Log git commit\n",
    "        try:\n",
    "            commit = subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"]).decode().strip()\n",
    "            mlflow.log_param(\"git_commit\", commit)\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        # Load data\n",
    "        X_train, y_train, X_test, y_test, feature_cols = load_data()\n",
    "        mlflow.log_param(\"n_features\", len(feature_cols))\n",
    "        mlflow.log_param(\"train_size\", len(X_train))\n",
    "        mlflow.log_param(\"test_size\", len(X_test))\n",
    "        \n",
    "        # Train model\n",
    "        model = xgb.XGBClassifier(\n",
    "            learning_rate=args.learning_rate,\n",
    "            max_depth=args.max_depth,\n",
    "            n_estimators=args.n_estimators,\n",
    "            subsample=args.subsample,\n",
    "            random_state=42,\n",
    "            use_label_encoder=False,\n",
    "            eval_metric='logloss'\n",
    "        )\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate\n",
    "        train_pred = model.predict(X_train)\n",
    "        test_pred = model.predict(X_test)\n",
    "        train_acc = accuracy_score(y_train, train_pred)\n",
    "        test_acc = accuracy_score(y_test, test_pred)\n",
    "        \n",
    "        mlflow.log_metric(\"train_accuracy\", train_acc)\n",
    "        mlflow.log_metric(\"test_accuracy\", test_acc)\n",
    "        \n",
    "        # Log feature importance\n",
    "        importance = model.feature_importances_\n",
    "        importance_dict = dict(zip(feature_cols, importance.tolist()))\n",
    "        mlflow.log_dict(importance_dict, \"feature_importance.json\")\n",
    "        \n",
    "        # Log model\n",
    "        mlflow.xgboost.log_model(model, \"model\")\n",
    "        \n",
    "        # (Optional) log a sample prediction plot\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.figure(figsize=(10,6))\n",
    "        plt.barh(feature_cols, importance)\n",
    "        plt.xlabel(\"Importance\")\n",
    "        plt.title(\"Feature Importance\")\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(\"importance.png\")\n",
    "        mlflow.log_artifact(\"importance.png\")\n",
    "        \n",
    "        print(f\"Run ID: {mlflow.active_run().info.run_id}\")\n",
    "        print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This script accepts hyperparameters as command\u2011line arguments, logs them, loads the data, trains an XGBoost model, and logs metrics and artifacts. The `log_dict` method saves a JSON file with feature importance. The script can be run multiple times with different arguments, and all runs will be recorded in MLflow.\n",
    "\n",
    "To run a set of experiments, we can use a shell loop:\n",
    "\n",
    "```bash\n",
    "for lr in 0.01 0.05 0.1; do\n",
    "    for depth in 4 6 8; do\n",
    "        python train_nepse_with_mlflow.py --learning_rate $lr --max_depth $depth\n",
    "    done\n",
    "done\n",
    "```\n",
    "\n",
    "Afterwards, we open the MLflow UI to compare the runs and select the best model.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we explored the critical practice of experiment tracking. We covered:\n",
    "\n",
    "- The reasons for tracking experiments: reproducibility, comparison, and collaboration.\n",
    "- The key elements to log: parameters, metrics, artifacts, and metadata.\n",
    "- Popular experiment tracking tools, with a focus on MLflow.\n",
    "- How to integrate MLflow into the NEPSE training pipeline.\n",
    "- Organising experiments with experiments, tags, and nested runs.\n",
    "- Using the MLflow Model Registry to manage model versions.\n",
    "- Querying and comparing runs programmatically.\n",
    "- Best practices to ensure effective experiment tracking.\n",
    "\n",
    "By adopting experiment tracking, the NEPSE prediction system gains a solid foundation for model development. Every experiment is recorded, every result is reproducible, and the path to production is clear. In the next chapter, we will discuss **Data and Model Lineage**, which extends tracking to capture the relationships between data, features, models, and predictions.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 64**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='63. feature_stores.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='65. data_and_model_lineage.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}