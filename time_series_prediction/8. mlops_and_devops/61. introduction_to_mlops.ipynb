{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 61: Introduction to MLOps\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand what MLOps is and why it is essential for deploying and maintaining machine learning systems in production\n",
    "- Distinguish between MLOps and DevOps and appreciate the unique challenges of operationalising ML\n",
    "- Identify the key principles of MLOps: automation, reproducibility, versioning, monitoring, and collaboration\n",
    "- Describe the end\u2011to\u2011end MLOps lifecycle, from data collection to model monitoring\n",
    "- Recognise the typical roles and team structures in an MLOps\u2011enabled organisation\n",
    "- Survey the MLOps tooling landscape and select appropriate tools for different stages of the lifecycle\n",
    "- Assess the maturity of an ML system using MLOps maturity models\n",
    "- Plan the initial steps to introduce MLOps practices in a project like the NEPSE prediction system\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Building a high\u2011accuracy machine learning model for NEPSE stock prediction is a significant achievement, but it is only the beginning. To deliver real value, the model must be deployed into a production environment, integrated with other systems, and maintained over time. This is where **MLOps** (Machine Learning Operations) comes into play.\n",
    "\n",
    "MLOps is a set of practices that combines machine learning, DevOps, and data engineering to reliably and efficiently deploy and maintain ML systems in production. It aims to automate and streamline the entire ML lifecycle, from data preparation to model monitoring and retraining. Without MLOps, ML projects often fail to deliver long\u2011term business value due to technical debt, manual processes, and lack of reproducibility.\n",
    "\n",
    "In this chapter, we will introduce the core concepts of MLOps, its principles, and its lifecycle. We will use the NEPSE prediction system as a concrete example to illustrate how MLOps practices can be applied. By the end, you will have a clear understanding of what it takes to operationalise a time\u2011series prediction system and the first steps to get there.\n",
    "\n",
    "---\n",
    "\n",
    "## 61.1 What is MLOps?\n",
    "\n",
    "MLOps is a discipline that aims to unify ML system development (Dev) and ML system deployment (Ops). It standardises and streamlines the lifecycle of ML models, from inception to retirement, ensuring that models are reliable, scalable, and maintainable in production.\n",
    "\n",
    "### 61.1.1 The Need for MLOps\n",
    "\n",
    "Developing an ML model in a Jupyter notebook is very different from running it in production. In development, the focus is on experimentation: trying different algorithms, features, and hyperparameters. The environment is often isolated, data is static, and reproducibility is limited to the notebook itself.\n",
    "\n",
    "In production, the model must:\n",
    "\n",
    "- Handle varying data volumes and velocities.\n",
    "- Integrate with other systems via APIs.\n",
    "- Be monitored for performance degradation (drift).\n",
    "- Be updated or retrained without downtime.\n",
    "- Scale to meet demand.\n",
    "- Comply with security and regulatory requirements.\n",
    "\n",
    "MLOps provides the practices and tools to bridge this gap, reducing the time from model development to deployment while maintaining quality and governance.\n",
    "\n",
    "### 61.1.2 MLOps vs. DevOps\n",
    "\n",
    "MLOps is inspired by DevOps, which brought similar principles to software development. Both emphasise automation, continuous integration and delivery (CI/CD), testing, and monitoring. However, ML systems introduce additional complexities:\n",
    "\n",
    "- **Data is versioned alongside code**: Models depend on data, which changes over time. Reproducing a model requires the exact data version.\n",
    "- **Experimentation is non\u2011deterministic**: Model training involves randomness; reproducibility requires seeding and logging.\n",
    "- **Model performance degrades over time**: Data drift and concept drift mean models must be monitored and retrained.\n",
    "- **Multiple artifacts**: Besides code, we have datasets, feature definitions, model files, and evaluation metrics.\n",
    "- **Model explainability and fairness**: These are often required for compliance.\n",
    "\n",
    "Thus, MLOps extends DevOps to address these unique challenges.\n",
    "\n",
    "---\n",
    "\n",
    "## 61.2 MLOps Principles\n",
    "\n",
    "The core principles of MLOps are:\n",
    "\n",
    "1. **Automation**: Automate as much of the ML lifecycle as possible, including data validation, training, testing, deployment, and monitoring. This reduces manual errors and speeds up iterations.\n",
    "\n",
    "2. **Reproducibility**: Every model should be reproducible. This means versioning code, data, and environment, and tracking all parameters and metrics.\n",
    "\n",
    "3. **Versioning**: All artifacts (code, data, features, models) must be versioned to enable rollbacks, audits, and collaboration.\n",
    "\n",
    "4. **Testing**: ML systems require testing at multiple levels: data tests (e.g., schema validation), model tests (e.g., accuracy on holdout set), and infrastructure tests (e.g., API response time).\n",
    "\n",
    "5. **Monitoring**: Continuously monitor model performance, data drift, and system health. Set up alerts for anomalies.\n",
    "\n",
    "6. **Collaboration**: Enable data scientists, engineers, and operations teams to work together with shared tools and processes.\n",
    "\n",
    "7. **Continuous Training and Delivery**: Models should be retrained automatically when new data arrives or when performance drops, and new versions should be deployed seamlessly.\n",
    "\n",
    "---\n",
    "\n",
    "## 61.3 The MLOps Lifecycle\n",
    "\n",
    "The MLOps lifecycle encompasses all stages from data collection to model retirement. A typical high\u2011level view is:\n",
    "\n",
    "```\n",
    "Data Collection \u2192 Data Validation \u2192 Feature Engineering \u2192 Model Training \u2192 Model Validation \u2192 Model Deployment \u2192 Model Monitoring \u2192 (Feedback loop back to Data Collection)\n",
    "```\n",
    "\n",
    "Let's break down each stage with the NEPSE system in mind.\n",
    "\n",
    "### 61.3.1 Data Collection and Ingestion\n",
    "\n",
    "Raw data from various sources (e.g., NEPSE CSV files, live feeds) is ingested into a data lake or warehouse. This step must be reliable and handle both batch and streaming data.\n",
    "\n",
    "**MLOps considerations**: Automate data ingestion pipelines, validate schema and freshness, and version the raw data.\n",
    "\n",
    "### 61.3.2 Data Validation\n",
    "\n",
    "Check for data quality issues: missing values, outliers, schema changes. For NEPSE, we might validate that the 'Close' column is numeric and within a reasonable range.\n",
    "\n",
    "**MLOps considerations**: Write automated data tests that run before training or inference.\n",
    "\n",
    "### 61.3.3 Feature Engineering\n",
    "\n",
    "Transform raw data into features (e.g., lags, technical indicators). This step must be consistent between training and serving to avoid training\u2011serving skew.\n",
    "\n",
    "**MLOps considerations**: Use a feature store to define and serve features consistently. Version feature definitions.\n",
    "\n",
    "### 61.3.4 Model Training\n",
    "\n",
    "Train models using the latest features. This may involve hyperparameter tuning and cross\u2011validation.\n",
    "\n",
    "**MLOps considerations**: Automate training runs, track experiments (parameters, metrics), and store model artifacts with metadata.\n",
    "\n",
    "### 61.3.5 Model Validation\n",
    "\n",
    "Evaluate the trained model on a holdout set or using A/B tests. Compare against baseline and check for fairness and explainability.\n",
    "\n",
    "**MLOps considerations**: Define validation gates that must be passed before deployment.\n",
    "\n",
    "### 61.3.6 Model Deployment\n",
    "\n",
    "Deploy the validated model to a serving environment (e.g., REST API, batch job). Use deployment strategies like blue\u2011green or canary to minimise risk.\n",
    "\n",
    "**MLOps considerations**: Automate deployment pipelines (CI/CD). Ensure rollback capability.\n",
    "\n",
    "### 61.3.7 Model Monitoring\n",
    "\n",
    "Monitor the model's performance in production: track prediction drift, data drift, latency, and error rates. Set up alerts.\n",
    "\n",
    "**MLOps considerations**: Log predictions and inputs for later analysis. Use monitoring tools to detect issues.\n",
    "\n",
    "### 61.3.8 Continuous Retraining\n",
    "\n",
    "When performance degrades or new data arrives, automatically trigger retraining. The new model goes through the same validation and deployment process.\n",
    "\n",
    "**MLOps considerations**: Orchestrate retraining pipelines (e.g., Airflow, Kubeflow). Decide on retraining triggers (time\u2011based, drift\u2011based).\n",
    "\n",
    "---\n",
    "\n",
    "## 61.4 Team Structure and Roles\n",
    "\n",
    "MLOps requires collaboration across multiple roles:\n",
    "\n",
    "- **Data Scientists**: Develop models, experiment with features, define validation criteria.\n",
    "- **Data Engineers**: Build and maintain data pipelines, ensure data quality.\n",
    "- **ML Engineers**: Bridge data science and engineering; implement ML pipelines, deploy models, set up monitoring.\n",
    "- **DevOps Engineers**: Manage infrastructure, CI/CD, monitoring, and security.\n",
    "- **Business Stakeholders**: Define requirements, evaluate business impact.\n",
    "\n",
    "In a small team, one person may wear multiple hats. In a larger organisation, these roles are distinct.\n",
    "\n",
    "For the NEPSE project, you might start with a single ML engineer/data scientist, but as the system grows, you'll need dedicated data engineers and DevOps support.\n",
    "\n",
    "---\n",
    "\n",
    "## 61.5 MLOps Tooling Landscape\n",
    "\n",
    "The MLOps ecosystem is vast and rapidly evolving. Tools can be categorised by lifecycle stage:\n",
    "\n",
    "- **Data versioning**: DVC, LakeFS, Delta Lake.\n",
    "- **Feature stores**: Feast, Tecton, Hopsworks.\n",
    "- **Experiment tracking**: MLflow, Weights & Biases, Neptune, Comet.\n",
    "- **Orchestration**: Apache Airflow, Prefect, Dagster, Kubeflow Pipelines.\n",
    "- **Model serving**: TensorFlow Serving, TorchServe, Seldon, BentoML, KServe.\n",
    "- **Monitoring**: Prometheus + Grafana, Evidently, WhyLabs, Arize.\n",
    "- **CI/CD for ML**: Jenkins, GitLab CI, GitHub Actions, Kubeflow.\n",
    "- **End\u2011to\u2011end platforms**: SageMaker, Vertex AI, Azure ML, Databricks.\n",
    "\n",
    "For the NEPSE system, a pragmatic stack might include:\n",
    "\n",
    "- **DVC** or **Git\u2011LFS** for data versioning.\n",
    "- **MLflow** for experiment tracking and model registry.\n",
    "- **Airflow** for orchestrating retraining pipelines.\n",
    "- **FastAPI** for serving (with Docker and Kubernetes).\n",
    "- **Prometheus/Grafana** for monitoring.\n",
    "\n",
    "We will explore many of these tools in later chapters.\n",
    "\n",
    "---\n",
    "\n",
    "## 61.6 MLOps Maturity Models\n",
    "\n",
    "Organisations typically progress through levels of MLOps maturity. A common model is:\n",
    "\n",
    "- **Level 0: Manual Process** \u2013 Data scientists hand over a notebook to engineers who manually deploy. No automation, no monitoring.\n",
    "- **Level 1: ML Pipeline Automation** \u2013 Automated training and deployment pipelines. Models are retrained on a schedule.\n",
    "- **Level 2: CI/CD Pipeline Automation** \u2013 Full CI/CD for ML, including automated testing of data and models. Rapid, reliable deployments.\n",
    "- **Level 3: Automated Operations** \u2013 Models are automatically retrained and deployed based on monitoring triggers. Continuous improvement.\n",
    "\n",
    "The NEPSE project likely starts at Level 0. The goal is to progress through the levels to ensure reliability and efficiency.\n",
    "\n",
    "---\n",
    "\n",
    "## 61.7 Getting Started with MLOps\n",
    "\n",
    "Introducing MLOps to an existing project like NEPSE can be daunting. Start small and iterate:\n",
    "\n",
    "1. **Version control everything**: Put all code (including notebooks) in Git. Use DVC for data.\n",
    "2. **Automate the training pipeline**: Write a script that trains the model, logs parameters and metrics (e.g., with MLflow), and saves the model.\n",
    "3. **Establish a model registry**: Use MLflow or a simple file system to store models with version tags.\n",
    "4. **Set up a deployment pipeline**: Automate the deployment of the model as a REST API using CI/CD (e.g., GitHub Actions).\n",
    "5. **Add monitoring**: Log predictions and inputs, set up basic dashboards.\n",
    "\n",
    "As you gain confidence, add more advanced practices: feature stores, A/B testing, automated retraining.\n",
    "\n",
    "---\n",
    "\n",
    "## 61.8 Example: First MLOps Steps for NEPSE\n",
    "\n",
    "Let's walk through a concrete example of taking the NEPSE prediction model from a notebook to a minimal MLOps setup.\n",
    "\n",
    "### 61.8.1 Versioning Code and Data\n",
    "\n",
    "We create a Git repository for the project. We add a `data/` directory and use DVC to track the CSV files.\n",
    "\n",
    "```bash\n",
    "git init\n",
    "dvc init\n",
    "dvc add data/nepse_raw.csv\n",
    "git add data/nepse_raw.csv.dvc .gitignore\n",
    "git commit -m \"Add raw NEPSE data\"\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "DVC stores a small metadata file in Git, while the actual data file is in a separate cache (e.g., local or cloud). This allows versioning large datasets without bloating the Git repository.\n",
    "\n",
    "### 61.8.2 Automating Training with MLflow\n",
    "\n",
    "We refactor the training code into a Python script `train.py` that uses MLflow to track parameters and metrics.\n",
    "\n",
    "```python\n",
    "# train.py\n",
    "import mlflow\n",
    "import mlflow.sklearn\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--n_estimators', type=int, default=100)\n",
    "parser.add_argument('--max_depth', type=int, default=10)\n",
    "args = parser.parse_args()\n",
    "\n",
    "with mlflow.start_run():\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"n_estimators\", args.n_estimators)\n",
    "    mlflow.log_param(\"max_depth\", args.max_depth)\n",
    "\n",
    "    # Load data (from DVC)\n",
    "    df = pd.read_csv('data/nepse_features.csv')\n",
    "    X = df.drop(columns=['target'])\n",
    "    y = df['target']\n",
    "\n",
    "    # Train\n",
    "    model = RandomForestClassifier(n_estimators=args.n_estimators,\n",
    "                                   max_depth=args.max_depth,\n",
    "                                   random_state=42)\n",
    "    model.fit(X, y)\n",
    "\n",
    "    # Evaluate\n",
    "    preds = model.predict(X)  # simplified; use proper split\n",
    "    acc = accuracy_score(y, preds)\n",
    "    mlflow.log_metric(\"accuracy\", acc)\n",
    "\n",
    "    # Save model\n",
    "    mlflow.sklearn.log_model(model, \"model\")\n",
    "\n",
    "print(\"Training complete. Run 'mlflow ui' to view results.\")\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "We use MLflow to automatically log parameters, metrics, and the model artifact. This creates a run that can be viewed in the MLflow UI.\n",
    "\n",
    "### 61.8.3 Automating with a Script\n",
    "\n",
    "We can now run `python train.py --n_estimators 200 --max_depth 15` and have the result tracked. Next, we could add a shell script to run training with different hyperparameters and compare runs.\n",
    "\n",
    "### 61.8.4 CI/CD with GitHub Actions\n",
    "\n",
    "We set up a GitHub Action that runs training and, if on the main branch, deploys the model. This is a simplified example.\n",
    "\n",
    "```yaml\n",
    "# .github/workflows/train.yml\n",
    "name: Train and Deploy\n",
    "\n",
    "on:\n",
    "  push:\n",
    "    branches: [ main ]\n",
    "  schedule:\n",
    "    - cron: '0 2 * * 0'  # weekly on Sunday\n",
    "\n",
    "jobs:\n",
    "  train:\n",
    "    runs-on: ubuntu-latest\n",
    "    steps:\n",
    "    - uses: actions/checkout@v2\n",
    "    - name: Set up Python\n",
    "      uses: actions/setup-python@v2\n",
    "      with:\n",
    "        python-version: '3.9'\n",
    "    - name: Install dependencies\n",
    "      run: pip install -r requirements.txt\n",
    "    - name: Pull data from DVC\n",
    "      run: dvc pull  # assumes remote storage configured\n",
    "    - name: Train model\n",
    "      run: python train.py\n",
    "    - name: Deploy model\n",
    "      run: python deploy.py  # script that updates the production API\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "This workflow triggers on every push to main and weekly. It trains a new model and, if successful, deploys it. The `deploy.py` script would update, for example, a Kubernetes deployment with a new model version.\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this introductory chapter to MLOps, we covered:\n",
    "\n",
    "- The definition and necessity of MLOps for production ML systems.\n",
    "- The key differences between MLOps and DevOps.\n",
    "- The core principles: automation, reproducibility, versioning, testing, monitoring, collaboration, and continuous training.\n",
    "- The end\u2011to\u2011end MLOps lifecycle with the NEPSE prediction system as a running example.\n",
    "- Typical team structures and roles.\n",
    "- A survey of the MLOps tooling landscape.\n",
    "- MLOps maturity models to assess and guide progress.\n",
    "- Practical first steps to introduce MLOps to a project.\n",
    "\n",
    "MLOps is not a single tool but a culture and set of practices that ensure ML systems deliver long\u2011term value. For the NEPSE system, adopting MLOps principles will transform a one\u2011off experiment into a reliable, maintainable, and continuously improving service.\n",
    "\n",
    "In the next chapter, we will dive deeper into **CI/CD for Machine Learning**, exploring how to build automated pipelines that test and deploy models with confidence.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 61**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='../7. advanced_topics/60. advanced_optimization_techniques.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='62. cicd_for_machine_learning.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}