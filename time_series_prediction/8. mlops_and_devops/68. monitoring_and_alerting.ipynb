{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 68: Monitoring and Alerting\n",
    "\n",
    "## Learning Objectives\n",
    "\n",
    "By the end of this chapter, you will be able to:\n",
    "\n",
    "- Understand the importance of monitoring and alerting for production machine learning systems\n",
    "- Define a monitoring strategy covering system, application, and business metrics for the NEPSE prediction system\n",
    "- Instrument a Python application to expose metrics (counters, gauges, histograms) using Prometheus\n",
    "- Set up Prometheus to scrape metrics and store them as time\u2011series data\n",
    "- Build dashboards in Grafana to visualise key performance indicators\n",
    "- Implement structured logging and aggregate logs using the ELK stack or Loki\n",
    "- Use distributed tracing to debug latency issues in microservices\n",
    "- Design alerting rules to notify the team of anomalies and incidents\n",
    "- Define Service Level Objectives (SLOs) and track error budgets\n",
    "- Apply best practices for on\u2011call rotations and incident response\n",
    "\n",
    "---\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Deploying the NEPSE prediction system is only the beginning. Once live, we need to ensure it continues to function correctly, perform within acceptable limits, and deliver value. **Monitoring** provides visibility into the system's health and behaviour, while **alerting** notifies us when something requires attention. Together, they form the eyes and ears of a production system.\n",
    "\n",
    "In traditional software, monitoring focuses on metrics like CPU usage, request latency, and error rates. For machine learning systems, we must also monitor model\u2011specific concerns: prediction drift, data drift, and feature distribution shifts. A model that is technically responding may still be producing poor predictions because the underlying data has changed.\n",
    "\n",
    "In this chapter, we will build a comprehensive monitoring stack for the NEPSE system. We'll use **Prometheus** for metrics, **Grafana** for dashboards, **Loki** for logs, and **Jaeger** for tracing. We'll also discuss alerting with **Alertmanager** and define SLOs to guide our reliability efforts.\n",
    "\n",
    "---\n",
    "\n",
    "## 68.1 Monitoring Strategy\n",
    "\n",
    "A good monitoring strategy answers three questions:\n",
    "\n",
    "1. **What to monitor?** \u2013 Identify the key indicators of system health and business performance.\n",
    "2. **How to collect data?** \u2013 Instrument the application, set up exporters, and aggregate logs.\n",
    "3. **How to respond?** \u2013 Define alerts, escalation policies, and runbooks.\n",
    "\n",
    "For the NEPSE system, we categorise metrics into three layers:\n",
    "\n",
    "### 68.1.1 System Metrics\n",
    "\n",
    "- **Infrastructure**: CPU, memory, disk, network I/O (per server/container).\n",
    "- **Kubernetes**: Pod status, resource usage, restart counts.\n",
    "- **Database**: Connection pool usage, query latency, replication lag.\n",
    "\n",
    "### 68.1.2 Application Metrics\n",
    "\n",
    "- **Request rate**: Number of prediction requests per second, per endpoint.\n",
    "- **Latency**: Distribution of response times (p50, p95, p99).\n",
    "- **Error rate**: Percentage of failed requests (4xx, 5xx).\n",
    "- **Model inference time**: Time spent in the model itself.\n",
    "- **Feature retrieval time**: Time to fetch features from the online store.\n",
    "\n",
    "### 68.1.3 Business Metrics\n",
    "\n",
    "- **Prediction volume**: Total predictions made, per symbol.\n",
    "- **Prediction distribution**: Histogram of predicted probabilities (for classification).\n",
    "- **Model performance**: When ground truth becomes available (e.g., next day), track accuracy, precision, recall.\n",
    "- **Drift metrics**: Data drift scores (PSI) for each feature.\n",
    "- **User engagement**: If exposed to users, track number of active users, requests per user.\n",
    "\n",
    "### 68.1.4 Alerting Philosophy\n",
    "\n",
    "We follow the **\"alert on symptoms, not causes\"** principle. For example, instead of alerting on high CPU (a cause), we alert on high latency or error rate (symptoms that users experience). However, we still monitor causes for debugging.\n",
    "\n",
    "Alerts should be:\n",
    "\n",
    "- **Actionable**: Something needs to be done.\n",
    "- **Timely**: Not too late.\n",
    "- **Relevant**: Not too noisy.\n",
    "\n",
    "---\n",
    "\n",
    "## 68.2 Instrumenting the Application with Prometheus\n",
    "\n",
    "Prometheus is a popular open\u2011source monitoring system that collects metrics via HTTP scraping. We'll instrument our FastAPI prediction service to expose metrics.\n",
    "\n",
    "### 68.2.1 Installing Prometheus Client\n",
    "\n",
    "```bash\n",
    "pip install prometheus-client\n",
    "```\n",
    "\n",
    "### 68.2.2 Adding Metrics to FastAPI\n",
    "\n",
    "We'll create a `metrics.py` module that defines our metrics and a `/metrics` endpoint.\n",
    "\n",
    "```python\n",
    "# app/metrics.py\n",
    "from prometheus_client import Counter, Histogram, Gauge, generate_latest\n",
    "from fastapi import Response\n",
    "import time\n",
    "\n",
    "# Define metrics\n",
    "PREDICTIONS = Counter('predictions_total', 'Total number of predictions', ['symbol', 'model_version'])\n",
    "PREDICTION_LATENCY = Histogram('prediction_latency_seconds', 'Prediction latency', buckets=[0.005, 0.01, 0.025, 0.05, 0.1, 0.25, 0.5, 1, 2.5])\n",
    "ERRORS = Counter('prediction_errors_total', 'Total prediction errors', ['error_type'])\n",
    "FEATURE_FETCH_LATENCY = Histogram('feature_fetch_latency_seconds', 'Feature fetch latency')\n",
    "MODEL_VERSION = Gauge('model_version_info', 'Model version', ['version'])\n",
    "FEATURE_DRIFT = Gauge('feature_drift_psi', 'Feature drift PSI', ['feature'])\n",
    "\n",
    "# For tracking the current model version\n",
    "def set_model_version(version):\n",
    "    MODEL_VERSION.labels(version=version).set(1)\n",
    "\n",
    "# For updating drift metrics (called periodically)\n",
    "def update_drift_metrics(drift_scores):\n",
    "    for feature, score in drift_scores.items():\n",
    "        FEATURE_DRIFT.labels(feature=feature).set(score)\n",
    "\n",
    "# FastAPI endpoint for Prometheus to scrape\n",
    "@app.get(\"/metrics\")\n",
    "async def get_metrics():\n",
    "    return Response(content=generate_latest(), media_type=\"text/plain\")\n",
    "```\n",
    "\n",
    "### 68.2.3 Instrumenting the Prediction Endpoint\n",
    "\n",
    "Now we use these metrics in our prediction endpoint.\n",
    "\n",
    "```python\n",
    "# app/main.py (simplified)\n",
    "from fastapi import FastAPI, HTTPException\n",
    "import time\n",
    "from .metrics import PREDICTIONS, PREDICTION_LATENCY, ERRORS, FEATURE_FETCH_LATENCY\n",
    "\n",
    "app = FastAPI()\n",
    "\n",
    "@app.post(\"/predict\")\n",
    "async def predict(symbol: str, features: dict):\n",
    "    start_time = time.time()\n",
    "    try:\n",
    "        # Measure feature fetch time\n",
    "        fetch_start = time.time()\n",
    "        # ... fetch features from online store ...\n",
    "        fetch_time = time.time() - fetch_start\n",
    "        FEATURE_FETCH_LATENCY.observe(fetch_time)\n",
    "\n",
    "        # Model inference\n",
    "        inference_start = time.time()\n",
    "        # ... run model ...\n",
    "        prob = 0.75  # dummy\n",
    "        inference_time = time.time() - inference_start\n",
    "\n",
    "        # Record metrics\n",
    "        PREDICTIONS.labels(symbol=symbol, model_version=\"v1.2\").inc()\n",
    "        PREDICTION_LATENCY.observe(time.time() - start_time)\n",
    "\n",
    "        return {\"probability\": prob}\n",
    "    except Exception as e:\n",
    "        ERRORS.labels(error_type=type(e).__name__).inc()\n",
    "        raise HTTPException(status_code=500, detail=str(e))\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- `PREDICTIONS` counts each successful prediction, labelled by symbol and model version.\n",
    "- `PREDICTION_LATENCY` records the total latency.\n",
    "- `FEATURE_FETCH_LATENCY` measures how long it takes to retrieve features.\n",
    "- `ERRORS` counts failures by exception type.\n",
    "\n",
    "---\n",
    "\n",
    "## 68.3 Setting Up Prometheus\n",
    "\n",
    "Prometheus scrapes metrics endpoints at regular intervals. We need to configure it to target our service.\n",
    "\n",
    "### 68.3.1 Prometheus Configuration\n",
    "\n",
    "Create a `prometheus.yml` file:\n",
    "\n",
    "```yaml\n",
    "global:\n",
    "  scrape_interval: 15s\n",
    "  evaluation_interval: 15s\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: 'nepse-predictor'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:8000']  # if running locally\n",
    "    metrics_path: /metrics\n",
    "\n",
    "  - job_name: 'node'\n",
    "    static_configs:\n",
    "      - targets: ['localhost:9100']  # node_exporter for system metrics\n",
    "\n",
    "  - job_name: 'kubernetes-pods'\n",
    "    kubernetes_sd_configs:\n",
    "      - role: pod\n",
    "    relabel_configs:\n",
    "      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_scrape]\n",
    "        action: keep\n",
    "        regex: true\n",
    "      - source_labels: [__meta_kubernetes_pod_annotation_prometheus_io_path]\n",
    "        action: replace\n",
    "        target_label: __metrics_path__\n",
    "        regex: (.+)\n",
    "      - source_labels: [__address__, __meta_kubernetes_pod_annotation_prometheus_io_port]\n",
    "        action: replace\n",
    "        regex: ([^:]+)(?::\\d+)?;(\\d+)\n",
    "        replacement: $1:$2\n",
    "        target_label: __address__\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- The first job scrapes our prediction service directly.\n",
    "- The second job scrapes `node_exporter` for host\u2011level metrics (CPU, memory, etc.).\n",
    "- The third job demonstrates auto\u2011discovery of Kubernetes pods annotated with `prometheus.io/scrape: true`. This is useful in a Kubernetes deployment.\n",
    "\n",
    "### 68.3.2 Running Prometheus\n",
    "\n",
    "Using Docker:\n",
    "\n",
    "```bash\n",
    "docker run -p 9090:9090 -v /path/to/prometheus.yml:/etc/prometheus/prometheus.yml prom/prometheus\n",
    "```\n",
    "\n",
    "Then access the UI at `http://localhost:9090`.\n",
    "\n",
    "### 68.3.3 Key Prometheus Queries\n",
    "\n",
    "Some useful queries for the NEPSE system:\n",
    "\n",
    "- Request rate per second: `rate(predictions_total[5m])`\n",
    "- Error rate: `rate(prediction_errors_total[5m])`\n",
    "- 95th percentile latency: `histogram_quantile(0.95, sum(rate(prediction_latency_seconds_bucket[5m])) by (le))`\n",
    "- Feature fetch latency average: `avg(rate(feature_fetch_latency_seconds_sum[5m]) / rate(feature_fetch_latency_seconds_count[5m]))`\n",
    "- CPU usage (if node_exporter is running): `100 - (avg by (instance) (rate(node_cpu_seconds_total{mode=\"idle\"}[5m])) * 100)`\n",
    "\n",
    "---\n",
    "\n",
    "## 68.4 Visualising with Grafana\n",
    "\n",
    "Grafana connects to Prometheus and provides rich dashboards.\n",
    "\n",
    "### 68.4.1 Installing Grafana\n",
    "\n",
    "```bash\n",
    "docker run -d -p 3000:3000 --name=grafana grafana/grafana\n",
    "```\n",
    "\n",
    "Default login: admin/admin.\n",
    "\n",
    "### 68.4.2 Adding Prometheus Data Source\n",
    "\n",
    "1. Go to Configuration \u2192 Data Sources \u2192 Add data source.\n",
    "2. Select Prometheus.\n",
    "3. Set URL to `http://localhost:9090` (or the Prometheus server address).\n",
    "4. Save & Test.\n",
    "\n",
    "### 68.4.3 Building a Dashboard\n",
    "\n",
    "Create a new dashboard and add panels. Example panels:\n",
    "\n",
    "- **Request Rate** \u2013 Graph of `rate(predictions_total[5m])` by `symbol`.\n",
    "- **Error Rate** \u2013 Graph of `rate(prediction_errors_total[5m])`.\n",
    "- **Latency (p95)** \u2013 Graph using the histogram quantile query.\n",
    "- **Model Version** \u2013 Stat panel showing the current model version (using `model_version_info`).\n",
    "- **Drift Scores** \u2013 Table or gauge of `feature_drift_psi`.\n",
    "- **System Metrics** \u2013 CPU, memory of the pods.\n",
    "\n",
    "Grafana also supports alerting (though we'll use Alertmanager for production).\n",
    "\n",
    "---\n",
    "\n",
    "## 68.5 Log Aggregation\n",
    "\n",
    "Metrics give aggregate numbers, but logs provide detailed events. For the NEPSE system, we want to log:\n",
    "\n",
    "- Every prediction request (with anonymised features, if necessary).\n",
    "- Errors and stack traces.\n",
    "- Model version changes.\n",
    "- Anomaly detection events.\n",
    "\n",
    "### 68.5.1 Structured Logging with JSON\n",
    "\n",
    "We'll use the `structlog` library to output JSON logs, which are easy to ingest into log aggregation systems.\n",
    "\n",
    "```python\n",
    "# app/logging_config.py\n",
    "import structlog\n",
    "import logging\n",
    "\n",
    "structlog.configure(\n",
    "    processors=[\n",
    "        structlog.processors.TimeStamper(fmt=\"iso\"),\n",
    "        structlog.processors.JSONRenderer()\n",
    "    ],\n",
    "    context_class=dict,\n",
    "    logger_factory=structlog.stdlib.LoggerFactory(),\n",
    ")\n",
    "\n",
    "logger = structlog.get_logger()\n",
    "\n",
    "# In prediction endpoint\n",
    "logger.info(\"prediction_request\", symbol=symbol, features=features, model_version=\"v1.2\")\n",
    "```\n",
    "\n",
    "### 68.5.2 Log Aggregation with Loki\n",
    "\n",
    "Loki is a log aggregation system by Grafana Labs, designed to be lightweight and integrated with Grafana. It indexes only metadata, not the full log text, making it cost\u2011effective.\n",
    "\n",
    "**Running Loki with Docker:**\n",
    "\n",
    "```bash\n",
    "docker run -d --name=loki -p 3100:3100 grafana/loki:latest\n",
    "```\n",
    "\n",
    "**Installing Promtail** (log collector) to ship logs from our service:\n",
    "\n",
    "```yaml\n",
    "# promtail-config.yaml\n",
    "server:\n",
    "  http_listen_port: 9080\n",
    "  grpc_listen_port: 0\n",
    "\n",
    "positions:\n",
    "  filename: /tmp/positions.yaml\n",
    "\n",
    "clients:\n",
    "  - url: http://localhost:3100/loki/api/v1/push\n",
    "\n",
    "scrape_configs:\n",
    "  - job_name: nepse-predictor\n",
    "    static_configs:\n",
    "      - targets: [localhost]\n",
    "        labels:\n",
    "          job: nepse-predictor\n",
    "          __path__: /var/log/nepse/*.log   # where our app writes logs\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "Promtail tails log files and sends them to Loki. In Kubernetes, you would run Promtail as a DaemonSet to collect logs from all pods.\n",
    "\n",
    "### 68.5.3 Alternative: ELK Stack\n",
    "\n",
    "If you need full\u2011text search and more advanced analytics, use Elasticsearch, Logstash, and Kibana. Filebeat ships logs to Logstash or directly to Elasticsearch.\n",
    "\n",
    "---\n",
    "\n",
    "## 68.6 Distributed Tracing\n",
    "\n",
    "In a microservices architecture (e.g., prediction service, feature store, model service), a single request may traverse multiple services. Distributed tracing helps understand where time is spent.\n",
    "\n",
    "### 68.6.1 Instrumenting with OpenTelemetry\n",
    "\n",
    "OpenTelemetry is the industry standard for tracing. We'll instrument our FastAPI app.\n",
    "\n",
    "```bash\n",
    "pip install opentelemetry-distro opentelemetry-exporter-jaeger\n",
    "```\n",
    "\n",
    "**Setup:**\n",
    "\n",
    "```python\n",
    "# app/tracing.py\n",
    "from opentelemetry import trace\n",
    "from opentelemetry.exporter.jaeger.thrift import JaegerExporter\n",
    "from opentelemetry.instrumentation.fastapi import FastAPIInstrumentor\n",
    "from opentelemetry.sdk.resources import SERVICE_NAME, Resource\n",
    "from opentelemetry.sdk.trace import TracerProvider\n",
    "from opentelemetry.sdk.trace.export import BatchSpanProcessor\n",
    "\n",
    "def setup_tracing(app):\n",
    "    resource = Resource(attributes={\n",
    "        SERVICE_NAME: \"nepse-predictor\"\n",
    "    })\n",
    "    provider = TracerProvider(resource=resource)\n",
    "    processor = BatchSpanProcessor(\n",
    "        JaegerExporter(\n",
    "            agent_host_name=\"localhost\",\n",
    "            agent_port=6831,\n",
    "        )\n",
    "    )\n",
    "    provider.add_span_processor(processor)\n",
    "    trace.set_tracer_provider(provider)\n",
    "\n",
    "    FastAPIInstrumentor.instrument_app(app)\n",
    "\n",
    "# In main.py\n",
    "from .tracing import setup_tracing\n",
    "setup_tracing(app)\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- The tracer provider is configured to export spans to Jaeger (running locally or in the cluster).\n",
    "- `FastAPIInstrumentor` automatically creates spans for each request, capturing timing and metadata.\n",
    "\n",
    "### 68.6.2 Running Jaeger\n",
    "\n",
    "```bash\n",
    "docker run -d --name jaeger \\\n",
    "  -e COLLECTOR_ZIPKIN_HOST_PORT=:9411 \\\n",
    "  -p 5775:5775/udp \\\n",
    "  -p 6831:6831/udp \\\n",
    "  -p 6832:6832/udp \\\n",
    "  -p 5778:5778 \\\n",
    "  -p 16686:16686 \\\n",
    "  -p 14268:14268 \\\n",
    "  -p 14250:14250 \\\n",
    "  -p 9411:9411 \\\n",
    "  jaegertracing/all-in-one:latest\n",
    "```\n",
    "\n",
    "Access UI at `http://localhost:16686`.\n",
    "\n",
    "### 68.6.3 Creating Custom Spans\n",
    "\n",
    "You can also create custom spans for specific operations, like fetching features:\n",
    "\n",
    "```python\n",
    "from opentelemetry import trace\n",
    "\n",
    "tracer = trace.get_tracer(__name__)\n",
    "\n",
    "with tracer.start_as_current_span(\"fetch_features\") as span:\n",
    "    # fetch features\n",
    "    span.set_attribute(\"symbol\", symbol)\n",
    "    span.set_attribute(\"feature_count\", len(features))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 68.7 Alerting with Alertmanager\n",
    "\n",
    "Prometheus includes Alertmanager for handling alerts. It can group, inhibit, and route alerts to various receivers (email, Slack, PagerDuty).\n",
    "\n",
    "### 68.7.1 Defining Alert Rules\n",
    "\n",
    "Create a file `alerts.yml`:\n",
    "\n",
    "```yaml\n",
    "groups:\n",
    "  - name: nepse_alerts\n",
    "    rules:\n",
    "      - alert: HighErrorRate\n",
    "        expr: rate(prediction_errors_total[5m]) > 0.01\n",
    "        for: 2m\n",
    "        labels:\n",
    "          severity: critical\n",
    "        annotations:\n",
    "          summary: \"High error rate on {{ $labels.job }}\"\n",
    "          description: \"Error rate is {{ $value | printf \\\"%.2f\\\" }} errors/s for job {{ $labels.job }}\"\n",
    "\n",
    "      - alert: HighLatency\n",
    "        expr: histogram_quantile(0.95, sum(rate(prediction_latency_seconds_bucket[5m])) by (le)) > 0.5\n",
    "        for: 5m\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"High latency for predictions\"\n",
    "          description: \"P95 latency is {{ $value | printf \\\"%.2f\\\" }}s for job {{ $labels.job }}\"\n",
    "\n",
    "      - alert: FeatureDriftHigh\n",
    "        expr: feature_drift_psi > 0.25\n",
    "        for: 1h\n",
    "        labels:\n",
    "          severity: warning\n",
    "        annotations:\n",
    "          summary: \"Feature drift detected\"\n",
    "          description: \"Feature {{ $labels.feature }} has PSI {{ $value | printf \\\"%.2f\\\" }}\"\n",
    "```\n",
    "\n",
    "**Explanation:**  \n",
    "- The `HighErrorRate` alert fires if the error rate exceeds 0.01 per second for 2 minutes.\n",
    "- `HighLatency` fires if p95 latency > 0.5s for 5 minutes.\n",
    "- `FeatureDriftHigh` fires if any feature drift PSI exceeds 0.25 for an hour.\n",
    "\n",
    "Include this file in your Prometheus configuration under `rule_files`.\n",
    "\n",
    "### 68.7.2 Configuring Alertmanager\n",
    "\n",
    "Create `alertmanager.yml`:\n",
    "\n",
    "```yaml\n",
    "route:\n",
    "  group_by: ['alertname', 'job']\n",
    "  group_wait: 10s\n",
    "  group_interval: 10s\n",
    "  repeat_interval: 1h\n",
    "  receiver: 'slack'\n",
    "\n",
    "receivers:\n",
    "  - name: 'slack'\n",
    "    slack_configs:\n",
    "      - api_url: 'https://hooks.slack.com/services/...'\n",
    "        channel: '#alerts'\n",
    "        send_resolved: true\n",
    "```\n",
    "\n",
    "Start Alertmanager:\n",
    "\n",
    "```bash\n",
    "docker run -p 9093:9093 -v /path/to/alertmanager.yml:/etc/alertmanager/alertmanager.yml prom/alertmanager\n",
    "```\n",
    "\n",
    "Then configure Prometheus to send alerts to Alertmanager by adding:\n",
    "\n",
    "```yaml\n",
    "alerting:\n",
    "  alertmanagers:\n",
    "    - static_configs:\n",
    "        - targets: ['localhost:9093']\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 68.8 Service Level Objectives (SLOs) and Error Budgets\n",
    "\n",
    "An SLO is a target for a service level indicator (SLI), such as availability or latency. For the NEPSE system, we might define:\n",
    "\n",
    "- **SLI: Availability** \u2013 Proportion of successful prediction requests (HTTP 200) over a 30\u2011day window.\n",
    "- **SLO**: 99.9% availability.\n",
    "- **SLI: Latency** \u2013 Proportion of requests served under 200ms.\n",
    "- **SLO**: 95% of requests under 200ms.\n",
    "\n",
    "An **error budget** is 100% minus the SLO. For a 99.9% availability SLO, the error budget is 0.1%. If we exceed this budget (i.e., availability drops below 99.9%), we should stop deploying new features and focus on reliability.\n",
    "\n",
    "### 68.8.1 Calculating SLOs in Prometheus\n",
    "\n",
    "We can compute the current availability over a rolling 30\u2011day window:\n",
    "\n",
    "```promql\n",
    "(sum(rate(predictions_total[30d])) + sum(rate(prediction_errors_total[30d]))) / sum(rate(predictions_total[30d]))\n",
    "```\n",
    "\n",
    "But availability is usually (successful requests) / (total requests). More accurately:\n",
    "\n",
    "```\n",
    "availability = sum(rate(predictions_total[30d])) / (sum(rate(predictions_total[30d])) + sum(rate(prediction_errors_total[30d])))\n",
    "```\n",
    "\n",
    "We can create a recording rule to pre\u2011compute this metric.\n",
    "\n",
    "### 68.8.2 Alerting on Error Budget Burn\n",
    "\n",
    "Instead of alerting when we drop below the SLO (which means we've already exhausted the budget), we alert when we are burning budget too fast. For example, if we have a 30\u2011day error budget, we can alert if we've used 10% of it in the last hour. This gives early warning.\n",
    "\n",
    "**Burn rate alert example:**\n",
    "\n",
    "```yaml\n",
    "- alert: HighErrorBudgetBurn\n",
    "  expr: (1 - (sum(rate(predictions_total[1h])) / (sum(rate(predictions_total[1h])) + sum(rate(prediction_errors_total[1h]))))) > (0.001 * 0.1)\n",
    "  for: 1h\n",
    "  labels:\n",
    "    severity: page\n",
    "  annotations:\n",
    "    summary: \"High error budget burn rate\"\n",
    "```\n",
    "\n",
    "Here `0.001` is the error budget (0.1%) and `0.1` is 10% of it. So this alerts if the error rate over the last hour exceeds 0.01% (10% of the budget).\n",
    "\n",
    "---\n",
    "\n",
    "## 68.9 Incident Management\n",
    "\n",
    "When an alert fires, an incident begins. A good incident management process includes:\n",
    "\n",
    "- **Detection**: Alert triggers.\n",
    "- **Response**: On\u2011call engineer acknowledges.\n",
    "- **Investigation**: Determine root cause.\n",
    "- **Mitigation**: Fix or workaround.\n",
    "- **Resolution**: Service restored.\n",
    "- **Post\u2011mortem**: Document what happened, why, and how to prevent recurrence.\n",
    "\n",
    "Tools like **PagerDuty** or **Opsgenie** handle on\u2011call rotations, escalations, and incident tracking.\n",
    "\n",
    "### 68.9.1 Runbooks\n",
    "\n",
    "For common alerts, create **runbooks** \u2013 step\u2011by\u2011step guides for investigation and mitigation. For example:\n",
    "\n",
    "**Alert: HighErrorRate**\n",
    "1. Check the logs for recent errors (`kubectl logs ...`).\n",
    "2. Check if the model serving pod is running out of memory.\n",
    "3. Verify the database connection.\n",
    "4. If feature store is down, fall back to a cached model.\n",
    "5. Escalate to the on\u2011call data scientist if needed.\n",
    "\n",
    "---\n",
    "\n",
    "## 68.10 Best Practices for Monitoring and Alerting\n",
    "\n",
    "1. **Start with the four golden signals**: Latency, traffic, errors, saturation.\n",
    "2. **Monitor both infrastructure and application**.\n",
    "3. **Use labels wisely** in Prometheus to allow slicing by service, version, etc.\n",
    "4. **Avoid alert fatigue** \u2013 tune thresholds, use `for` clauses, and resolve quickly.\n",
    "5. **Write runbooks** for every alert.\n",
    "6. **Regularly review alerts** \u2013 archive unused ones, adjust thresholds.\n",
    "7. **Use SLOs to guide priorities** \u2013 reliability vs. feature development.\n",
    "8. **Test your monitoring** \u2013 simulate failures to ensure alerts fire.\n",
    "9. **Secure monitoring endpoints** \u2013 do not expose `/metrics` publicly without authentication.\n",
    "10. **Retain metrics and logs** for at least 30 days (or longer for compliance).\n",
    "\n",
    "---\n",
    "\n",
    "## Chapter Summary\n",
    "\n",
    "In this chapter, we built a comprehensive monitoring and alerting stack for the NEPSE prediction system. We covered:\n",
    "\n",
    "- A monitoring strategy covering system, application, and business metrics.\n",
    "- Instrumenting a FastAPI application with Prometheus metrics (counters, histograms, gauges).\n",
    "- Setting up Prometheus to scrape metrics and query them.\n",
    "- Building dashboards in Grafana to visualise key indicators.\n",
    "- Structured logging with JSON and aggregating logs with Loki.\n",
    "- Distributed tracing with OpenTelemetry and Jaeger.\n",
    "- Defining alerting rules and routing through Alertmanager.\n",
    "- Introducing SLOs and error budgets to guide reliability efforts.\n",
    "- Incident management and runbook best practices.\n",
    "\n",
    "With monitoring and alerting in place, we can sleep soundly knowing that the NEPSE system is being watched, and that we'll be woken only when it truly matters. In the next chapter, we will discuss **Model Drift Detection**, which extends monitoring to the model's performance over time.\n",
    "\n",
    "---\n",
    "\n",
    "**End of Chapter 68**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style='width:100%; display:flex; justify-content:space-between; align-items:center; margin: 1em 0;'>\n",
    "  <a href='67. infrastructure_as_code.ipynb' style='font-weight:bold; font-size:1.05em;'>&larr; Previous</a>\n",
    "  <a href='../TOC.md' style='font-weight:bold; font-size:1.05em; text-align:center;'>Table of Contents</a>\n",
    "  <a href='69. cost_management.ipynb' style='font-weight:bold; font-size:1.05em;'>Next &rarr;</a>\n",
    "</div>\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}