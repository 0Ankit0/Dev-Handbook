{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 5: IDENTIFY (ID) – Asset & Risk Management\n",
    "\n",
    "In Chapter 4, we established the governance structures that define our security strategy—policies that mandate protection, risk appetites that guide our decisions, and supply chain controls that secure our dependencies. However, strategy without intelligence is blind. Before we can protect our organization, before we can detect intrusions, or respond to incidents, we must answer a fundamental question: **What are we protecting?**\n",
    "\n",
    "The **IDENTIFY** function of the NIST CSF 2.0 provides the organizational understanding to manage cybersecurity risk to systems, assets, data, and capabilities. This is not merely an inventory exercise; it is the continuous process of discovering what exists in our ever-changing environments, understanding the business context and criticality of those assets, modeling the threats they face, assessing their vulnerabilities, and quantifying the risks they present to our mission.\n",
    "\n",
    "For developers, this chapter transforms abstract risk into concrete reality. We will learn to discover the APIs we have forgotten, classify the data we process, model the threats against our architectures before we write the first line of code, and prioritize vulnerabilities not by severity alone, but by the business impact of exploitation. We move from \"I think our attack surface is small\" to \"We have 47 public-facing APIs, 12 contain PII, and 3 have critical vulnerabilities requiring patching within 24 hours.\"\n",
    "\n",
    "---\n",
    "\n",
    "## 5.1 Asset Discovery: Data, Systems, Applications, and APIs\n",
    "\n",
    "Asset management is the foundation of all security operations. You cannot patch what you do not know exists, encrypt data you cannot find, or monitor logs from systems that are invisible to you.\n",
    "\n",
    "### 5.1.1 The Asset Inventory Scope\n",
    "Modern asset inventories extend far beyond physical servers. They include:\n",
    "\n",
    "*   **Hardware:** Servers, laptops, mobile devices, IoT sensors, network appliances.\n",
    "*   **Software:** Applications, microservices, serverless functions, databases, middleware.\n",
    "*   **Data:** Structured (databases), unstructured (documents), ephemeral (logs, caches).\n",
    "*   **Network Assets:** Subnets, cloud VPCs, VPN gateways, load balancers.\n",
    "*   **Digital Identities:** Service accounts, API keys, certificates, user accounts.\n",
    "*   **Cloud Resources:** S3 buckets, Lambda functions, Kubernetes clusters, IAM roles.\n",
    "\n",
    "### 5.1.2 Shadow IT and the Ephemeral Challenge\n",
    "**Shadow IT** refers to assets deployed without formal IT approval—often cloud services spun up by business units or developers testing new tools. By 2026, **ephemeral assets** (containers that exist for minutes, serverless functions invoked on-demand) create a dynamic inventory that traditional spreadsheets cannot capture.\n",
    "\n",
    "**Automated Discovery with Cloud APIs:**\n",
    "Continuous discovery is essential. This Python example uses the AWS Resource Groups Tagging API to discover untagged resources (a common indicator of Shadow IT):\n",
    "\n",
    "```python\n",
    "import boto3\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "class CloudAssetDiscovery:\n",
    "    def __init__(self):\n",
    "        self.tagging = boto3.client('resourcegroupstaggingapi')\n",
    "        self.ec2 = boto3.client('ec2')\n",
    "        self.inventory = []\n",
    "    \n",
    "    def discover_untagged_resources(self):\n",
    "        \"\"\"\n",
    "        Identifies resources lacking mandatory security tags (Owner, Environment, DataClassification)\n",
    "        Returns potential Shadow IT or unmanaged assets\n",
    "        \"\"\"\n",
    "        paginator = self.tagging.get_paginator('get_resources')\n",
    "        \n",
    "        for page in paginator.paginate():\n",
    "            for resource in page['ResourceTagMappingList']:\n",
    "                tags = {tag['Key']: tag['Value'] for tag in resource.get('Tags', [])}\n",
    "                \n",
    "                mandatory_tags = ['Owner', 'Environment', 'DataClassification']\n",
    "                missing_tags = [tag for tag in mandatory_tags if tag not in tags]\n",
    "                \n",
    "                if missing_tags:\n",
    "                    self.inventory.append({\n",
    "                        'arn': resource['ResourceARN'],\n",
    "                        'resource_type': resource['ResourceType'],\n",
    "                        'missing_tags': missing_tags,\n",
    "                        'discovered_at': datetime.utcnow().isoformat(),\n",
    "                        'risk_level': 'High' if 'DataClassification' in missing_tags else 'Medium'\n",
    "                    })\n",
    "        \n",
    "        return self.inventory\n",
    "    \n",
    "    def discover_exposed_storage(self):\n",
    "        \"\"\"\n",
    "        Finds S3 buckets that are publicly accessible (high risk data exposure)\n",
    "        \"\"\"\n",
    "        s3 = boto3.client('s3')\n",
    "        exposed_buckets = []\n",
    "        \n",
    "        buckets = s3.list_buckets()['Buckets']\n",
    "        for bucket in buckets:\n",
    "            try:\n",
    "                # Check bucket policy\n",
    "                policy = s3.get_bucket_policy_status(Bucket=bucket['Name'])\n",
    "                if policy['PolicyStatus']['IsPublic']:\n",
    "                    exposed_buckets.append({\n",
    "                        'bucket': bucket['Name'],\n",
    "                        'issue': 'PublicBucketPolicy',\n",
    "                        'remediation': 'Review and restrict bucket policy'\n",
    "                    })\n",
    "            except s3.exceptions.ClientError as e:\n",
    "                if 'NoSuchBucketPolicy' in str(e):\n",
    "                    # Check ACLs if no policy exists\n",
    "                    acl = s3.get_bucket_acl(Bucket=bucket['Name'])\n",
    "                    for grant in acl['Grants']:\n",
    "                        if grant['Grantee'].get('URI') == 'http://acs.amazonaws.com/groups/global/AllUsers':\n",
    "                            exposed_buckets.append({\n",
    "                                'bucket': bucket['Name'],\n",
    "                                'issue': 'PublicACL',\n",
    "                                'remediation': 'Remove public read/write ACLs'\n",
    "                            })\n",
    "        \n",
    "        return exposed_buckets\n",
    "\n",
    "# Usage\n",
    "# scanner = CloudAssetDiscovery()\n",
    "# shadow_assets = scanner.discover_untagged_resources()\n",
    "# print(json.dumps(shadow_assets, indent=2))\n",
    "```\n",
    "\n",
    "### 5.1.3 API Discovery and Inventory\n",
    "In modern microservices architectures, APIs are the primary attack surface. **API sprawl** occurs when services expose endpoints without centralized registration.\n",
    "\n",
    "**OpenAPI Specification (OAS) as Inventory:**\n",
    "Every API should have an OpenAPI (Swagger) specification that serves as its technical contract and security documentation.\n",
    "\n",
    "```yaml\n",
    "# Example OpenAPI 3.0 with security metadata\n",
    "openapi: 3.0.3\n",
    "info:\n",
    "  title: Customer Data API\n",
    "  version: 1.0.0\n",
    "  description: |\n",
    "    **Asset Classification:** Critical\n",
    "    **Data Processed:** PII (Email, Phone)\n",
    "    **Owner:** Platform Team\n",
    "    **Threat Model:** STRIDE Analysis completed 2026-01-15\n",
    "\n",
    "servers:\n",
    "  - url: https://api.example.com/v1\n",
    "\n",
    "security:\n",
    "  - OAuth2: [read:customers, write:customers]\n",
    "\n",
    "paths:\n",
    "  /customers/{id}:\n",
    "    get:\n",
    "      summary: Retrieve customer data\n",
    "      security:\n",
    "        - OAuth2: [read:customers]\n",
    "      parameters:\n",
    "        - name: id\n",
    "          in: path\n",
    "          required: true\n",
    "          schema:\n",
    "            type: string\n",
    "            pattern: '^[0-9a-f]{8}-[0-9a-f]{4}-4[0-9a-f]{3}-[89ab][0-9a-f]{3}-[0-9a-f]{12}$'  # UUID v4 only\n",
    "      responses:\n",
    "        '200':\n",
    "          description: Customer data (Confidential)\n",
    "          headers:\n",
    "            X-Request-ID:\n",
    "              description: For audit tracing\n",
    "              schema:\n",
    "                type: string\n",
    "```\n",
    "\n",
    "**Automated API Discovery:**\n",
    "Tools like **Kiterunner** or **OWASP Amass** can discover APIs through DNS enumeration and brute-forcing common endpoint patterns, helping identify undocumented \"rogue\" endpoints.\n",
    "\n",
    "---\n",
    "\n",
    "## 5.2 Business Context & Impact Analysis\n",
    "\n",
    "Not all assets are equal. A vulnerability in the cafeteria menu app does not equal a vulnerability in the payment processing gateway. **Business Impact Analysis (BIA)** establishes this criticality context.\n",
    "\n",
    "### 5.2.1 Asset Criticality Classification\n",
    "Classify assets based on their importance to business operations:\n",
    "\n",
    "| Tier | Classification | Criteria | Recovery Time Objective (RTO) | Example |\n",
    "|------|---------------|----------|------------------------------|---------|\n",
    "| 1 | **Mission Critical** | Revenue-generating, regulatory compliance, safety-of-life | < 4 hours | Payment gateway, Trading platform |\n",
    "| 2 | **Business Critical** | Supports core operations, high reputational impact | < 24 hours | CRM, Email systems |\n",
    "| 3 | **Business Important** | Productivity tools, limited operational impact | < 72 hours | HR portal, Internal wiki |\n",
    "| 4 | **Routine** | Non-essential, easily replaced | < 1 week | Marketing blog, Test environments |\n",
    "\n",
    "### 5.2.2 Data Classification and Handling\n",
    "Data classification determines encryption requirements, access controls, and retention policies. The **NIST SP 800-60** guidelines provide a federal standard adaptable to industry.\n",
    "\n",
    "**Classification Levels:**\n",
    "1.  **Public:** Approved for public disclosure (marketing materials).\n",
    "2.  **Internal:** Organization use only, but disclosure causes no harm (org charts).\n",
    "3.  **Confidential:** Unauthorized disclosure causes moderate harm (financial data, customer lists).\n",
    "4.  **Restricted/Regulated:** Unauthorized disclosure causes severe harm or violates law (PHI under HIPAA, PCI-DSS data, trade secrets).\n",
    "\n",
    "**Implementation: Automated Data Classification**\n",
    "Use regular expressions and machine learning to scan repositories and databases for unclassified sensitive data:\n",
    "\n",
    "```python\n",
    "import re\n",
    "import hashlib\n",
    "\n",
    "class DataClassifier:\n",
    "    def __init__(self):\n",
    "        # Patterns for PII/PHI detection\n",
    "        self.patterns = {\n",
    "            'SSN': r'\\b\\d{3}-\\d{2}-\\d{4}\\b',\n",
    "            'CREDIT_CARD': r'\\b(?:\\d[ -]*?){13,16}\\b',\n",
    "            'EMAIL': r'\\b[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\\.[A-Z|a-z]{2,}\\b',\n",
    "            'API_KEY': r'\\b(sk|pk)_(live|test)_[a-zA-Z0-9]{24,}\\b',  # Stripe-like keys\n",
    "            'HIPAA_MEDICAL_RECORD': r'\\bMRN:\\s*\\d{6,10}\\b'\n",
    "        }\n",
    "    \n",
    "    def scan_text(self, content, source):\n",
    "        \"\"\"\n",
    "        Scan text content for sensitive data patterns\n",
    "        Returns classification recommendation\n",
    "        \"\"\"\n",
    "        findings = {}\n",
    "        total_severity = 0\n",
    "        \n",
    "        for data_type, pattern in self.patterns.items():\n",
    "            matches = re.findall(pattern, content)\n",
    "            if matches:\n",
    "                # Hash matches for privacy in logs\n",
    "                hashed_matches = [hashlib.sha256(m.encode()).hexdigest()[:8] for m in matches]\n",
    "                findings[data_type] = {\n",
    "                    'count': len(matches),\n",
    "                    'examples': hashed_matches[:3],  # Log only hashes, not actual data\n",
    "                    'severity': self._get_severity(data_type)\n",
    "                }\n",
    "                total_severity += len(matches) * self._get_severity(data_type)\n",
    "        \n",
    "        # Determine classification based on findings\n",
    "        if 'HIPAA_MEDICAL_RECORD' in findings or total_severity > 10:\n",
    "            classification = 'Restricted'\n",
    "        elif 'CREDIT_CARD' in findings or 'SSN' in findings:\n",
    "            classification = 'Confidential'\n",
    "        elif total_severity > 0:\n",
    "            classification = 'Internal'\n",
    "        else:\n",
    "            classification = 'Public'\n",
    "        \n",
    "        return {\n",
    "            'source': source,\n",
    "            'recommended_classification': classification,\n",
    "            'findings': findings,\n",
    "            'requires_encryption': classification in ['Confidential', 'Restricted'],\n",
    "            'retention_years': 7 if classification == 'Restricted' else 3\n",
    "        }\n",
    "    \n",
    "    def _get_severity(self, data_type):\n",
    "        severity_map = {\n",
    "            'HIPAA_MEDICAL_RECORD': 5,\n",
    "            'CREDIT_CARD': 4,\n",
    "            'SSN': 4,\n",
    "            'API_KEY': 3,\n",
    "            'EMAIL': 1\n",
    "        }\n",
    "        return severity_map.get(data_type, 1)\n",
    "\n",
    "# Usage for scanning code repositories\n",
    "# classifier = DataClassifier()\n",
    "# with open('config.py', 'r') as f:\n",
    "#     result = classifier.scan_text(f.read(), 'config.py')\n",
    "```\n",
    "\n",
    "### 5.2.3 Dependency Mapping\n",
    "Understanding **data flow** is as important as knowing the assets themselves. A database may be Restricted, but if a Business Important app has direct access to it, the app's criticality effectively increases.\n",
    "\n",
    "**Data Flow Diagrams (DFD):**\n",
    "Create DFDs showing:\n",
    "*   Data sources (databases, external APIs)\n",
    "*   Processes (microservices, functions)\n",
    "*   Data stores (caches, object storage)\n",
    "*   Trust boundaries (internet, DMZ, internal network)\n",
    "\n",
    "---\n",
    "\n",
    "## 5.3 Threat Modeling Methodologies: STRIDE, PASTA, and LINDDUN\n",
    "\n",
    "Threat modeling is the practice of identifying and eliminating potential threats **before** code is written. It is the ultimate \"shift left\" activity—fixing architectural flaws in design is exponentially cheaper than fixing them in production.\n",
    "\n",
    "### 5.3.1 The STRIDE Framework (Microsoft)\n",
    "STRIDE categorizes threats by the type of attack. It is the most accessible framework for developers.\n",
    "\n",
    "| Threat | Property Violated | Description | Example |\n",
    "|--------|------------------|-------------|---------|\n",
    "| **S**poofing | Authentication | Pretending to be someone/something else | Stolen credentials, forging JWT tokens |\n",
    "| **T**ampering | Integrity | Modifying data or code | SQL injection, modifying request parameters |\n",
    "| **R**epudiation | Non-repudiation | Denying an action took place | Lack of audit logs for financial transactions |\n",
    "| **I**nformation Disclosure | Confidentiality | Exposing data to unauthorized parties | Verbose error messages, IDOR vulnerabilities |\n",
    "| **D**enial of Service | Availability | Disrupting service availability | DDoS, resource exhaustion attacks |\n",
    "| **E**levation of Privilege | Authorization | Gaining unauthorized capabilities | Privilege escalation, broken access control |\n",
    "\n",
    "**Threat Modeling Process:**\n",
    "1.  **Decompose:** Create DFDs showing processes, data stores, data flows, and trust boundaries.\n",
    "2.  **Identify:** For each element, ask STRIDE questions.\n",
    "3.  **Mitigate:** Design controls to eliminate or reduce threats.\n",
    "4.  **Validate:** Verify mitigations work and no new threats introduced.\n",
    "\n",
    "**Tool: pytm (Pythonic Threat Modeling)**\n",
    "```python\n",
    "from pytm import TM, Server, Datastore, Dataflow, Boundary\n",
    "\n",
    "# Create the threat model\n",
    "tm = TM(\"Customer Portal Threat Model\")\n",
    "tm.description = \"Web application handling PII\"\n",
    "tm.isOrdered = True\n",
    "\n",
    "# Define boundaries\n",
    "internet = Boundary(\"Internet\")\n",
    "dmz = Boundary(\"DMZ\")\n",
    "internal = Boundary(\"Internal Network\")\n",
    "\n",
    "# Assets\n",
    "web_server = Server(\"Web Server\")\n",
    "web_server.OS = \"Linux\"\n",
    "web_server.isHardened = True\n",
    "web_server.inBoundary = dmz\n",
    "\n",
    "database = Datastore(\"PostgreSQL DB\")\n",
    "database.OS = \"Linux\"\n",
    "database.isEncryptedAtRest = True\n",
    "database.inBoundary = internal\n",
    "database.storesSensitiveData = True\n",
    "\n",
    "# Data flows\n",
    "user_login = Dataflow(\"User\", web_server, \"Login Credentials\")\n",
    "user_login.protocol = \"HTTPS\"\n",
    "user_login.usesEncryption = True\n",
    "\n",
    "db_query = Dataflow(web_server, database, \"SQL Query\")\n",
    "db_query.protocol = \"PostgreSQL\"\n",
    "db_query.usesEncryption = True\n",
    "db_query.usesTLS = True\n",
    "\n",
    "# Process\n",
    "tm.process()  # Generates threat list based on STRIDE\n",
    "```\n",
    "\n",
    "### 5.3.2 PASTA (Process for Attack Simulation and Threat Analysis)\n",
    "PASTA is a risk-centric framework with seven stages, ideal for applications with high security requirements (financial, critical infrastructure).\n",
    "\n",
    "1.  **Define Objectives:** Business impact, compliance requirements.\n",
    "2.  **Define Technical Scope:** Components, APIs, dependencies.\n",
    "3.  **Application Decomposition:** DFDs, trust boundaries.\n",
    "4.  **Threat Analysis:** Real-world threat intelligence (MITRE ATT&CK mapping).\n",
    "5.  **Vulnerability Detection:** Linking threats to actual vulnerabilities.\n",
    "6.  **Attack Modeling:** Probability and impact scoring.\n",
    "7.  **Risk Analysis:** Countermeasures and residual risk.\n",
    "\n",
    "### 5.3.3 LINDDUN (Privacy Threat Modeling)\n",
    "While STRIDE covers security, LINDDUN specifically addresses privacy threats required by GDPR/CCPA.\n",
    "\n",
    "| Threat | Description | Example |\n",
    "|--------|-------------|---------|\n",
    "| **Li**nkability | Associating data with user | Combining logs to track user across sessions |\n",
    "| **Id**entifiability | Identifying anonymous user | Re-identifying anonymized dataset |\n",
    "| **N**on-repudiation | Denying data sharing consent | User claims they didn't agree to TOS |\n",
    "| **D**etectability | Determining if data exists | Attackers inferring user existence via error messages |\n",
    "| **D**isclosure | Unauthorized data access | Data breach exposing user profiles |\n",
    "| **U**nawareness | Lack of transparency | User unaware of data collection |\n",
    "| **N**on-compliance | Violating regulations | Retaining data beyond legal requirement |\n",
    "\n",
    "---\n",
    "\n",
    "## 5.4 Vulnerability Management: Scanning, Assessment, and Prioritization\n",
    "\n",
    "Vulnerability management is the continuous cycle of identifying, classifying, remediating, and mitigating software vulnerabilities. It bridges the gap between \"potential threats\" (STRIDE) and \"actual weaknesses\" (CVEs).\n",
    "\n",
    "### 5.4.1 The Vulnerability Landscape\n",
    "*   **CVE (Common Vulnerabilities and Exposures):** Global dictionary of publicly known security vulnerabilities (e.g., CVE-2021-44228 for Log4j).\n",
    "*   **CWE (Common Weakness Enumeration):** Categories of software weaknesses (e.g., CWE-89 for SQL Injection).\n",
    "*   **CVSS (Common Vulnerability Scoring System):** Standardized scoring (0-10) of severity. CVSS v4.0 (released 2023) improves on v3.1 by better incorporating threat intelligence and environmental factors.\n",
    "\n",
    "**CVSS v4.0 Metrics:**\n",
    "*   **Base Metrics:** Inherent characteristics (attack vector, complexity, privileges required).\n",
    "*   **Threat Metrics:** Exploit maturity (Attacked, POC, Unreported).\n",
    "*   **Environmental Metrics:** Asset-specific impact (confidentiality/integrity requirements).\n",
    "\n",
    "### 5.4.2 Vulnerability Scanning Types\n",
    "\n",
    "**1. Network Scanning**\n",
    "Discovers open ports, services, and missing OS patches.\n",
    "*   **Tools:** Nessus, OpenVAS, Nmap scripts.\n",
    "\n",
    "**2. Static Application Security Testing (SAST)**\n",
    "Analyzes source code without execution. Finds SQLi, XSS, hardcoded secrets.\n",
    "*   **Tools:** SonarQube, Semgrep, Checkmarx, Bandit (Python).\n",
    "\n",
    "```yaml\n",
    "# Example: Semgrep CI rule for detecting SQL injection (SAST)\n",
    "# .semgrep.yml\n",
    "rules:\n",
    "  - id: python-sql-injection\n",
    "    pattern: |\n",
    "      cursor.execute($X % ...)\n",
    "    message: \"Possible SQL injection detected. Use parameterized queries.\"\n",
    "    languages: [python]\n",
    "    severity: ERROR\n",
    "    metadata:\n",
    "      cwe: \"CWE-89: SQL Injection\"\n",
    "      owasp: \"A03:2021 - Injection\"\n",
    "```\n",
    "\n",
    "**3. Dynamic Application Security Testing (DAST)**\n",
    "Tests running applications by sending malicious payloads.\n",
    "*   **Tools:** OWASP ZAP, Burp Suite.\n",
    "\n",
    "**4. Software Composition Analysis (SCA)**\n",
    "Identifies vulnerable open-source dependencies.\n",
    "*   **Tools:** Snyk, OWASP Dependency-Check, GitHub Dependabot.\n",
    "\n",
    "**5. Container Scanning**\n",
    "Analyzes Docker images for OS-level and application-level vulnerabilities.\n",
    "*   **Tools:** Trivy, Clair, Grype.\n",
    "\n",
    "### 5.4.3 Risk-Based Prioritization (Beyond CVSS)\n",
    "CVSS Base Score alone is insufficient. A \"Critical\" vulnerability in an isolated test system is less urgent than a \"High\" vulnerability in a public-facing payment API.\n",
    "\n",
    "**The Exploit Prediction Scoring System (EPSS):**\n",
    "EPSS (first.org/epss) uses machine learning to predict the probability that a vulnerability will be exploited in the wild within the next 30 days. It helps prioritize the 2% of vulnerabilities that pose 80% of the risk.\n",
    "\n",
    "**Prioritization Formula:**\n",
    "$$ Priority = f(CVSS, EPSS, Asset\\_Criticality, Network\\_Exposure) $$\n",
    "\n",
    "**Implementation: Risk-Based Triage Script**\n",
    "```python\n",
    "import requests\n",
    "from datetime import datetime\n",
    "\n",
    "class RiskBasedPrioritizer:\n",
    "    def __init__(self):\n",
    "        self.epss_api = \"https://api.first.org/data/v1/epss\"\n",
    "    \n",
    "    def get_epss_score(self, cve_id):\n",
    "        \"\"\"Fetch EPSS probability score for a CVE\"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.epss_api}?cve={cve_id}\")\n",
    "            data = response.json()\n",
    "            if data['data']:\n",
    "                return float(data['data'][0]['epss'])\n",
    "            return 0.0\n",
    "        except:\n",
    "            return 0.0\n",
    "    \n",
    "    def calculate_priority(self, vulnerability):\n",
    "        \"\"\"\n",
    "        Calculate priority score (0-100)\n",
    "        vulnerability dict contains: cve, cvss, asset_tier, exposure\n",
    "        \"\"\"\n",
    "        # Normalize CVSS (0-10 -> 0-40 weight)\n",
    "        cvss_weight = vulnerability['cvss'] * 4\n",
    "        \n",
    "        # EPSS probability (0-1 -> 0-30 weight)\n",
    "        epss = self.get_epss_score(vulnerability['cve'])\n",
    "        epss_weight = epss * 30\n",
    "        \n",
    "        # Asset criticality (Tier 1=25, Tier 2=15, Tier 3=5, Tier 4=1)\n",
    "        tier_weights = {'Tier1': 25, 'Tier2': 15, 'Tier3': 5, 'Tier4': 1}\n",
    "        asset_weight = tier_weights.get(vulnerability['asset_tier'], 1)\n",
    "        \n",
    "        # Exposure (Internet=5, Internal=2, Isolated=0)\n",
    "        exposure_weights = {'Internet': 5, 'Internal': 2, 'Isolated': 0}\n",
    "        exposure_weight = exposure_weights.get(vulnerability['exposure'], 0)\n",
    "        \n",
    "        total_score = cvss_weight + epss_weight + asset_weight + exposure_weight\n",
    "        \n",
    "        # Determine SLA\n",
    "        if total_score >= 80:\n",
    "            sla = \"24 Hours\"\n",
    "            priority = \"Critical\"\n",
    "        elif total_score >= 60:\n",
    "            sla = \"72 Hours\"\n",
    "            priority = \"High\"\n",
    "        elif total_score >= 40:\n",
    "            sla = \"7 Days\"\n",
    "            priority = \"Medium\"\n",
    "        else:\n",
    "            sla = \"30 Days\"\n",
    "            priority = \"Low\"\n",
    "        \n",
    "        return {\n",
    "            'cve': vulnerability['cve'],\n",
    "            'total_score': round(total_score, 2),\n",
    "            'priority': priority,\n",
    "            'remediation_sla': sla,\n",
    "            'epss_probability': f\"{epss:.2%}\",\n",
    "            'factors': {\n",
    "                'cvss_contribution': cvss_weight,\n",
    "                'epss_contribution': epss_weight,\n",
    "                'asset_contribution': asset_weight,\n",
    "                'exposure_contribution': exposure_weight\n",
    "            }\n",
    "        }\n",
    "\n",
    "# Example usage\n",
    "# vuln = {\n",
    "#     'cve': 'CVE-2021-44228',\n",
    "#     'cvss': 10.0,\n",
    "#     'asset_tier': 'Tier1',\n",
    "#     'exposure': 'Internet'\n",
    "# }\n",
    "# prioritizer = RiskBasedPrioritizer()\n",
    "# print(prioritizer.calculate_priority(vuln))\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "## 5.5 Risk Assessment & Treatment: Quantitative vs. Qualitative Approaches\n",
    "\n",
    "With assets identified, threats modeled, and vulnerabilities cataloged, we must assess the risk they collectively represent and decide how to treat that risk.\n",
    "\n",
    "### 5.5.1 Qualitative Risk Assessment\n",
    "Uses categorical scales (High/Medium/Low) based on expert judgment. Fast, suitable for initial screenings.\n",
    "\n",
    "**Risk Matrix (5x5):**\n",
    "| Likelihood \\ Impact | Negligible | Minor | Moderate | Major | Catastrophic |\n",
    "|---------------------|------------|-------|----------|-------|--------------|\n",
    "| **Almost Certain** | Medium | High | Critical | Critical | Critical |\n",
    "| **Likely** | Medium | Medium | High | Critical | Critical |\n",
    "| **Possible** | Low | Medium | High | High | Critical |\n",
    "| **Unlikely** | Low | Low | Medium | High | High |\n",
    "| **Rare** | Low | Low | Low | Medium | Medium |\n",
    "\n",
    "### 5.5.2 Quantitative Risk Assessment (FAIR Model)\n",
    "The **Factor Analysis of Information Risk (FAIR)** model provides a framework for understanding, analyzing, and measuring information risk in financial terms ($).\n",
    "\n",
    "**FAIR Terminology:**\n",
    "*   **Loss Event Frequency (LEF):** How often does the threat contact the asset and succeed?\n",
    "    *   Threat Event Frequency (TEF) × Vulnerability (Threat Capability vs Control Strength)\n",
    "*   **Loss Magnitude (LM):** How much does it cost when it happens?\n",
    "    *   Primary Loss (response, replacement) + Secondary Loss (fines, reputation, customer churn).\n",
    "\n",
    "**Calculation:**\n",
    "$$ Risk = Loss Event Frequency × Loss Magnitude $$\n",
    "\n",
    "**Implementation: Simplified FAIR Calculation**\n",
    "```python\n",
    "import numpy as np\n",
    "\n",
    "class FairRiskAnalyzer:\n",
    "    def __init__(self):\n",
    "        # Calibrated estimates (using ranges for Monte Carlo simulation)\n",
    "        self.simulations = 10000\n",
    "    \n",
    "    def calculate_lef(self, threat_contact_freq, probability_of_success):\n",
    "        \"\"\"\n",
    "        Loss Event Frequency = Contact Frequency * Probability of Action * Probability of Success\n",
    "        \"\"\"\n",
    "        # Monte Carlo simulation with triangular distributions\n",
    "        tef = np.random.triangular(\n",
    "            threat_contact_freq['min'],\n",
    "            threat_contact_freq['mode'],\n",
    "            threat_contact_freq['max'],\n",
    "            self.simulations\n",
    "        )\n",
    "        \n",
    "        vuln = np.random.triangular(\n",
    "            probability_of_success['min'],\n",
    "            probability_of_success['mode'],\n",
    "            probability_of_success['max'],\n",
    "            self.simulations\n",
    "        )\n",
    "        \n",
    "        return tef * vuln\n",
    "    \n",
    "    def calculate_lm(self, primary_loss, secondary_loss):\n",
    "        \"\"\"Total Loss Magnitude\"\"\"\n",
    "        pl = np.random.triangular(\n",
    "            primary_loss['min'],\n",
    "            primary_loss['mode'],\n",
    "            primary_loss['max'],\n",
    "            self.simulations\n",
    "        )\n",
    "        \n",
    "        sl = np.random.triangular(\n",
    "            secondary_loss['min'],\n",
    "            secondary_loss['mode'],\n",
    "            secondary_loss['max'],\n",
    "            self.simulations\n",
    "        )\n",
    "        \n",
    "        return pl + sl\n",
    "    \n",
    "    def analyze_risk(self, scenario):\n",
    "        \"\"\"\n",
    "        scenario = {\n",
    "            'tef': {'min': 1, 'mode': 5, 'max': 20},  # times per year\n",
    "            'vuln': {'min': 0.1, 'mode': 0.3, 'max': 0.8},  # probability\n",
    "            'primary_loss': {'min': 10000, 'mode': 50000, 'max': 200000},\n",
    "            'secondary_loss': {'min': 0, 'mode': 100000, 'max': 500000}  # regulatory fines\n",
    "        }\n",
    "        \"\"\"\n",
    "        lef = self.calculate_lef(scenario['tef'], scenario['vuln'])\n",
    "        lm = self.calculate_lm(scenario['primary_loss'], scenario['secondary_loss'])\n",
    "        \n",
    "        annual_loss_exposure = lef * lm\n",
    "        \n",
    "        return {\n",
    "            'ale_mean': np.mean(annual_loss_exposure),\n",
    "            'ale_90th': np.percentile(annual_loss_exposure, 90),\n",
    "            'ale_max': np.max(annual_loss_exposure),\n",
    "            'interpretation': self._interpret_ale(np.mean(annual_loss_exposure))\n",
    "        }\n",
    "    \n",
    "    def _interpret_ale(self, ale):\n",
    "        if ale > 1000000:\n",
    "            return \"Critical: Risk treatment required immediately\"\n",
    "        elif ale > 100000:\n",
    "            return \"High: Senior management attention required\"\n",
    "        elif ale > 10000:\n",
    "            return \"Medium: Risk owner monitoring required\"\n",
    "        else:\n",
    "            return \"Low: Acceptable with routine monitoring\"\n",
    "\n",
    "# Example: Data breach risk for customer database\n",
    "# fair = FairRiskAnalyzer()\n",
    "# result = fair.analyze_risk({\n",
    "#     'tef': {'min': 0.5, 'mode': 2, 'max': 5},  # Attack attempts per year\n",
    "#     'vuln': {'min': 0.05, 'mode': 0.2, 'max': 0.5},  # 20% chance of success (patched systems)\n",
    "#     'primary_loss': {'min': 50000, 'mode': 150000, 'max': 400000},  # Forensics, notification\n",
    "#     'secondary_loss': {'min': 100000, 'mode': 500000, 'max': 2000000}  # GDPR fines\n",
    "# })\n",
    "# print(f\"Annual Loss Exposure (Average): ${result['ale_mean']:,.2f}\")\n",
    "```\n",
    "\n",
    "### 5.5.3 Risk Treatment Options\n",
    "Once risk is quantified, we apply the four treatment strategies from Chapter 1:\n",
    "\n",
    "1.  **Mitigate:** Implement controls to reduce likelihood or impact (firewalls, encryption, training). Most common for high risks.\n",
    "2.  **Transfer:** Cyber insurance, outsourcing, cloud provider shared responsibility. Does not eliminate risk but shifts financial impact.\n",
    "3.  **Accept:** Acknowledge and monitor. Appropriate for low risks or when mitigation cost exceeds potential loss. Requires documented approval.\n",
    "4.  **Avoid:** Discontinue the activity or system. Extreme but effective for unacceptable risks (e.g., shutting down a vulnerable legacy system rather than securing it).\n",
    "\n",
    "**Risk Register Maintenance:**\n",
    "A living document tracking:\n",
    "*   Risk ID, Description, Owner\n",
    "*   Inherent Risk (before controls)\n",
    "*   Control Effectiveness\n",
    "*   Residual Risk (after controls)\n",
    "*   Treatment Strategy\n",
    "*   Target Date for Remediation\n",
    "\n",
    "---\n",
    "\n",
    "### Chapter Summary\n",
    "\n",
    "In this chapter, we executed the **IDENTIFY** function with rigor and precision. We implemented **automated asset discovery** to eliminate shadow IT and map our ephemeral cloud resources, recognizing that comprehensive visibility is the prerequisite for all security operations. We established **business context** through data classification and criticality tiers, ensuring that security efforts align with business value and regulatory requirements. We applied **threat modeling** methodologies—STRIDE for security design, PASTA for risk-centric analysis, and LINDDUN for privacy—to anticipate attacks before code is committed. We implemented a **vulnerability management program** that moves beyond CVSS scores to risk-based prioritization using EPSS and asset criticality, ensuring we patch what matters most first. Finally, we quantified risk using the **FAIR model**, translating technical vulnerabilities into financial terms that drive executive decision-making.\n",
    "\n",
    "Identification gives us the map and the intelligence, but intelligence without action is mere observation. Having cataloged our assets, modeled our threats, and assessed our risks, we must now implement the safeguards—the technical, administrative, and physical controls that constitute the **PROTECT** function. We transition from knowing what to defend to actively building the walls, gates, and guardians that enforce our security policies.\n",
    "\n",
    "**Next Up: Chapter 6: PROTECT (PR) – Safeguards & Implementation**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
